<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-26T11:05:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jjqnmq</id>
    <title>Amoral Gemma3 v2 (more uncensored this time)</title>
    <updated>2025-03-25T18:36:31+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/soob3123/amoral-gemma3-12B-v2"&gt;https://huggingface.co/soob3123/amoral-gemma3-12B-v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Big thanks to the community for testing the initial amoral-gemma3 release! Based on your feedback, I'm excited to share version 2 with significantly fewer refusals in pure assistant mode (no system prompts).&lt;/p&gt; &lt;p&gt;Thanks to mradermacher for the quants!&lt;br /&gt; Quants: &lt;a href="https://huggingface.co/mradermacher/amoral-gemma3-12B-v2-GGUF"&gt;mradermacher/amoral-gemma3-12B-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your test results - particularly interested in refusal rate comparisons with v1. Please share any interesting edge cases you find!&lt;/p&gt; &lt;p&gt;Note: 4B and 27B are coming soon! just wanted to test it out with 12B first!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjqnmq/amoral_gemma3_v2_more_uncensored_this_time/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjqnmq/amoral_gemma3_v2_more_uncensored_this_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjqnmq/amoral_gemma3_v2_more_uncensored_this_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T18:36:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jj6i4m</id>
    <title>Deepseek v3</title>
    <updated>2025-03-25T00:19:31+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jj6i4m/deepseek_v3/"&gt; &lt;img alt="Deepseek v3" src="https://preview.redd.it/xaic503gbqqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=616bfd3de239ef7db7a2416bc9be3a95051f9c0b" title="Deepseek v3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xaic503gbqqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jj6i4m/deepseek_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jj6i4m/deepseek_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T00:19:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjwj88</id>
    <title>Extensive llama.cpp benchmark for quality degradation by quantization</title>
    <updated>2025-03-25T22:36:47+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjwj88/extensive_llamacpp_benchmark_for_quality/"&gt; &lt;img alt="Extensive llama.cpp benchmark for quality degradation by quantization" src="https://b.thumbs.redditmedia.com/zxtAfLGLZhuwSDYwd8Sx2b9cZAMVpqWqyfjm8M9INsc.jpg" title="Extensive llama.cpp benchmark for quality degradation by quantization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;a href="https://arxiv.org/pdf/2503.08188"&gt;paper on RigoChat 2&lt;/a&gt; (Spanish language model) was published. The authors included a test of all llama.cpp quantizations of the model using imatrix on different benchmarks. The graph is on the bottom of page 14, the table on page 15.&lt;/p&gt; &lt;p&gt;According to their results there's barely any relevant degradation for IQ3_XS on a 7B model. It seems to slowly start around IQ3_XXS. The achieved scores should probably be taken with a grain of salt, since it doesn't show the deterioration with the partially broken Q3_K model (compilade just submitted a PR for fixing it and also improving other lower quants). LLaMA 8B was used as a judge model instead of a larger model. This choice was explained in the paper though.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c0kfsy6bxwqe1.png?width=1354&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6b616b6be0ca0e84630e1198f168a6643d556f6"&gt;https://preview.redd.it/c0kfsy6bxwqe1.png?width=1354&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6b616b6be0ca0e84630e1198f168a6643d556f6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjwj88/extensive_llamacpp_benchmark_for_quality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjwj88/extensive_llamacpp_benchmark_for_quality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjwj88/extensive_llamacpp_benchmark_for_quality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T22:36:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjvzk0</id>
    <title>:|</title>
    <updated>2025-03-25T22:13:49+00:00</updated>
    <author>
      <name>/u/muxxington</name>
      <uri>https://old.reddit.com/user/muxxington</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjvzk0/_/"&gt; &lt;img alt=":|" src="https://preview.redd.it/6c6z4mxrtwqe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f53b44b9a4b4608007a917982e63cd139f9da2c" title=":|" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muxxington"&gt; /u/muxxington &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6c6z4mxrtwqe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjvzk0/_/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjvzk0/_/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T22:13:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjl49l</id>
    <title>Qwen?! üëÄ</title>
    <updated>2025-03-25T14:48:34+00:00</updated>
    <author>
      <name>/u/Lowkey_LokiSN</name>
      <uri>https://old.reddit.com/user/Lowkey_LokiSN</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl49l/qwen/"&gt; &lt;img alt="Qwen?! üëÄ" src="https://external-preview.redd.it/pSzn5luA5bEL814Ul3JN__zTnOX5m5uc7UJYJ7zNQ_k.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa05eb3479835a6a18730e50d61faa6a62ffce2b" title="Qwen?! üëÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/gcu4thlvluqe1.png?width=1168&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bc2bbb1a4b8d74ca2d65572c25e1ed2ae19b4db"&gt;Is it what I think it is?!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This was posted as a reply shortly after Qwen2.5-VL-32B-Instruct's announcement&lt;br /&gt; &lt;a href="https://x.com/JustinLin610/status/1904231553183744020"&gt;https://x.com/JustinLin610/status/1904231553183744020&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lowkey_LokiSN"&gt; /u/Lowkey_LokiSN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl49l/qwen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl49l/qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl49l/qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T14:48:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk64d7</id>
    <title>Installation commands for whisper.cpp's talk-llama on Android's termux</title>
    <updated>2025-03-26T07:18:50+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk64d7/installation_commands_for_whispercpps_talkllama/"&gt; &lt;img alt="Installation commands for whisper.cpp's talk-llama on Android's termux" src="https://external-preview.redd.it/MnlnNO1j_5bv5TAC7lTjiCkmmkBFYOpH0cfI5V_na2M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae5685e95d73e7f40e3ed12ad1d509c1c9bf2ff1" title="Installation commands for whisper.cpp's talk-llama on Android's termux" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Whisper.cpp is a project to run openai's speech-to-text models. It uses the same machine learning library as llama.cpp: &lt;strong&gt;ggml&lt;/strong&gt; - maintained by ggerganov and contributors.&lt;/p&gt; &lt;p&gt;In this project exists a simple executable: which you can create and run on any device. This post provides further details for creating and running the executable on Android phones. Here is the example provided in whisper.cpp:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/ggerganov/whisper.cpp/tree/master/examples/talk-llama"&gt;https://github.com/ggerganov/whisper.cpp/tree/master/examples/talk-llama&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Pre-requisites:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Download f-droid from here: &lt;a href="https://f-droid.org"&gt;https://f-droid.org&lt;/a&gt; refresh to update the app list to newest.&lt;/li&gt; &lt;li&gt;Download &amp;quot;Termux&amp;quot; and &amp;quot;termux-api&amp;quot; apps using f-droid. &lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;1. Install Dependencies:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;pkg update # (hit return on all) pkg install termux-api wget git cmake clang x11-repo -y pkg install sdl2 pulseaudio espeak -y # enable Microphone permissions termux-microphone-record -d -f /tmp/audio_recording.wav # records with microphone for 10 seconds &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;2. Build it:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/ggerganov/whisper.cpp cd whisper.cpp cmake -B build -S . -DWHISPER_SDL2=ON cmake --build build --config Release cp build/bin/whisper-talk-llama . cp examples/talk-llama/speak . chmod +x speak touch speak_file wget -c https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-tiny.en.bin wget -c https://huggingface.co/mradermacher/SmolLM-135M-GGUF/resolve/main/SmolLM-135M.Q4_K_M.gguf &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;3. Run with this command:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;pulseaudio --start &amp;amp;&amp;amp; pactl load-module module-sles-source &amp;amp;&amp;amp; ./whisper-talk-llama -c 0 -mw ggml-tiny.en.bin -ml SmolLM-135M.Q4_K_M.gguf -s speak -sf speak_file &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Next steps:&lt;/h1&gt; &lt;p&gt;Try larger models until response time becomes too slow: &lt;code&gt;wget -c&lt;/code&gt; &lt;a href="https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_0.gguf"&gt;&lt;code&gt;https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_0.gguf&lt;/code&gt;&lt;/a&gt; Replace your -ml flag with your model.&lt;/p&gt; &lt;p&gt;You can get the realtime interruption and sentence-wise tts operation by running the glados project in a more proper debian linux environment within termux. There is currently a bug where the models don't download consistently.&lt;/p&gt; &lt;p&gt;Both talk-llama and glados can be run properly while under load. Here's an example where I chat with gemma 1B and play a demanding 3D game.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1jk64d7/video/df8l0ncmgzqe1/player"&gt;https://reddit.com/link/1jk64d7/video/df8l0ncmgzqe1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I hope you benefit from this tutorial. Cancel the process with Ctrl+C, or the phone will keep models in RAM, which uses battery while sleeping.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk64d7/installation_commands_for_whispercpps_talkllama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk64d7/installation_commands_for_whispercpps_talkllama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk64d7/installation_commands_for_whispercpps_talkllama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T07:18:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk69ns</id>
    <title>ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning</title>
    <updated>2025-03-26T07:29:54+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Large Language Models (LLMs) have shown remarkable capabilities in reasoning, exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating reasoning with external search processes remains challenging, especially for complex multi-hop questions requiring multiple retrieval steps. We propose ReSearch, a novel framework that trains LLMs to Reason with Search via reinforcement learning without using any supervised data on reasoning steps. Our approach treats search operations as integral components of the reasoning chain, where when and how to perform searches is guided by text-based thinking, and search results subsequently influence further reasoning. We train ReSearch on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct extensive experiments. Despite being trained on only one dataset, our models demonstrate strong generalizability across various benchmarks. Analysis reveals that ReSearch naturally elicits advanced reasoning capabilities such as reflection and self-correction during the reinforcement learning process.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Code: &lt;a href="https://github.com/Agent-RL/ReSearch"&gt;https://github.com/Agent-RL/ReSearch&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.19470"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk69ns/research_learning_to_reason_with_search_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk69ns/research_learning_to_reason_with_search_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T07:29:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jji2da</id>
    <title>DeepSeek-V3-0324 GGUF - Unsloth</title>
    <updated>2025-03-25T12:22:51+00:00</updated>
    <author>
      <name>/u/Co0k1eGal3xy</name>
      <uri>https://old.reddit.com/user/Co0k1eGal3xy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jk0qjs/178bit_deepseekv30324_230gb_unsloth_dynamic_gguf/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;Official Unsloth Post Here - 1.78bit DeepSeek-V3-0324 - 230GB Unsloth Dynamic GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jk0qjs/178bit_deepseekv30324_230gb_unsloth_dynamic_gguf/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;Official Unsloth Post Here - 1.78bit DeepSeek-V3-0324 - 230GB Unsloth Dynamic GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Available Formats so far;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/e1715170d568c3e938082c5e968a575d9db2b74e/UD-IQ1_S"&gt;UD-IQ1_S (140.2 GB) (Version 1)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/e1715170d568c3e938082c5e968a575d9db2b74e/UD-IQ1_M"&gt;UD-IQ1_M (155.0 GB) (Version 1)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/0bd498bb8e5c8c7b00eb167dfe8ca1b7d3615816/UD-IQ1_S"&gt;UD-IQ1_S (186.2 GB) (Version 2)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/e1715170d568c3e938082c5e968a575d9db2b74e/UD-IQ2_XXS"&gt;UD-IQ2_XXS (196.2 GB) (Version 1)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/a82206f71a2cfdde0dbe36017ab88c0276f6e303/UD-IQ1_M"&gt;UD-IQ1_M (196.5 GB) (Version 2)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/e3029fe3f01a438859f6ba2753992c2747f137ea/UD-IQ2_XXS"&gt;UD-IQ2_XXS (218.6 GB) (Version 2)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/286bc425b9836ea90f3a55bf29f33fb79aca6ed6/UD-Q2_K_XL"&gt;UD-Q2_K_XL (226.6 GB) (Version 1)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/e3029fe3f01a438859f6ba2753992c2747f137ea/Q2_K"&gt;Q2_K (244.0 GB) (Version 1)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/84427dcbdb8db8e3f63920823c28cf8df30852e7/UD-Q2_K_XL"&gt;UD-Q2_K_XL (247.6 GB) (Version 2)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/e3029fe3f01a438859f6ba2753992c2747f137ea/Q3_K_M"&gt;Q3_K_M (319.2 GB)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/e3029fe3f01a438859f6ba2753992c2747f137ea/UD-Q3_K_XL"&gt;UD-Q3_K_XL (320.7 GB)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/e3029fe3f01a438859f6ba2753992c2747f137ea/Q4_K_M"&gt;Q4_K_M (404.3 GB)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/e3029fe3f01a438859f6ba2753992c2747f137ea/UD-Q4_K_XL"&gt;UD-Q4_K_XL (404.9 GB)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/e3029fe3f01a438859f6ba2753992c2747f137ea/Q5_K_M"&gt;Q5_K_M (475.4 GB)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/e3029fe3f01a438859f6ba2753992c2747f137ea/Q6_K"&gt;Q6_K (550.5 GB)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/e3029fe3f01a438859f6ba2753992c2747f137ea/Q8_0"&gt;Q8_0 (712.9 GB)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/e3029fe3f01a438859f6ba2753992c2747f137ea/BF16"&gt;BF16 (1765.3 GB)&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Co0k1eGal3xy"&gt; /u/Co0k1eGal3xy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jji2da/deepseekv30324_gguf_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jji2da/deepseekv30324_gguf_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jji2da/deepseekv30324_gguf_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T12:22:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjxq7r</id>
    <title>Gemini Coder - support for 2.5 Pro with AI Studio has landed!</title>
    <updated>2025-03-25T23:28:44+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjxq7r/gemini_coder_support_for_25_pro_with_ai_studio/"&gt; &lt;img alt="Gemini Coder - support for 2.5 Pro with AI Studio has landed!" src="https://external-preview.redd.it/bxJRNnlXq5IDDRT6Vz7ceiBs6PGN-YNIchXF0QIlGl0.jpg?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b7fdd819da65276924239d95b5dd385b92ae20c" title="Gemini Coder - support for 2.5 Pro with AI Studio has landed!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://marketplace.visualstudio.com/items?itemName=robertpiosik.gemini-coder"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjxq7r/gemini_coder_support_for_25_pro_with_ai_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjxq7r/gemini_coder_support_for_25_pro_with_ai_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T23:28:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjqa9a</id>
    <title>AMD Is Reportedly Bringing Strix Halo To Desktop; CEO Lisa Su Confirms In An Interview.</title>
    <updated>2025-03-25T18:21:29+00:00</updated>
    <author>
      <name>/u/metallicamax</name>
      <uri>https://old.reddit.com/user/metallicamax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://wccftech.com/amd-is-reportedly-bringing-strix-halo-to-desktop/"&gt;https://wccftech.com/amd-is-reportedly-bringing-strix-halo-to-desktop/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is so awesome. You will be able to have up to 96Gb dedicated to Vram.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/metallicamax"&gt; /u/metallicamax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjqa9a/amd_is_reportedly_bringing_strix_halo_to_desktop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjqa9a/amd_is_reportedly_bringing_strix_halo_to_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjqa9a/amd_is_reportedly_bringing_strix_halo_to_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T18:21:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjjv8k</id>
    <title>DeepSeek official communication on X: DeepSeek-V3-0324 is out now!</title>
    <updated>2025-03-25T13:53:11+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjjv8k/deepseek_official_communication_on_x/"&gt; &lt;img alt="DeepSeek official communication on X: DeepSeek-V3-0324 is out now!" src="https://b.thumbs.redditmedia.com/__aOAn3RMb1pB4WQ7nZaRtP8KJ2vjbYZROoq35jWyoc.jpg" title="DeepSeek official communication on X: DeepSeek-V3-0324 is out now!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jjjv8k"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjjv8k/deepseek_official_communication_on_x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjjv8k/deepseek_official_communication_on_x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T13:53:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjpsfp</id>
    <title>Google's new Gemini 2.5 beats all other thinking model as per their claims in their article . What are your views on this?</title>
    <updated>2025-03-25T18:01:43+00:00</updated>
    <author>
      <name>/u/WriedGuy</name>
      <uri>https://old.reddit.com/user/WriedGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking"&gt;https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WriedGuy"&gt; /u/WriedGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjpsfp/googles_new_gemini_25_beats_all_other_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjpsfp/googles_new_gemini_25_beats_all_other_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjpsfp/googles_new_gemini_25_beats_all_other_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T18:01:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjusya</id>
    <title>Deepseek V3 0324 got 38.8% SWE-Bench Verified w/ OpenHands</title>
    <updated>2025-03-25T21:24:35+00:00</updated>
    <author>
      <name>/u/AmbitiousSeaweed101</name>
      <uri>https://old.reddit.com/user/AmbitiousSeaweed101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjusya/deepseek_v3_0324_got_388_swebench_verified_w/"&gt; &lt;img alt="Deepseek V3 0324 got 38.8% SWE-Bench Verified w/ OpenHands" src="https://preview.redd.it/laea7v40lwqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258ada96674ff89666373dd77c431418f63438cc" title="Deepseek V3 0324 got 38.8% SWE-Bench Verified w/ OpenHands" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AmbitiousSeaweed101"&gt; /u/AmbitiousSeaweed101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/laea7v40lwqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjusya/deepseek_v3_0324_got_388_swebench_verified_w/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjusya/deepseek_v3_0324_got_388_swebench_verified_w/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T21:24:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjgje5</id>
    <title>We got competition</title>
    <updated>2025-03-25T10:50:03+00:00</updated>
    <author>
      <name>/u/BlueeWaater</name>
      <uri>https://old.reddit.com/user/BlueeWaater</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgje5/we_got_competition/"&gt; &lt;img alt="We got competition" src="https://preview.redd.it/bamkfj1yftqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c43810bb7e5d8ea7891aeebc79e47a801d562c8" title="We got competition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlueeWaater"&gt; /u/BlueeWaater &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bamkfj1yftqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgje5/we_got_competition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgje5/we_got_competition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T10:50:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk857w</id>
    <title>Chonkie, the "no-nonsense RAG chunking library" just vanished from GitHub</title>
    <updated>2025-03-26T09:56:52+00:00</updated>
    <author>
      <name>/u/SK33LA</name>
      <uri>https://old.reddit.com/user/SK33LA</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using chonkie at work, and today we were looking for its docs. Then we realized that the GitHub repository was either deleted or marked as private, their website is down, and I couldn't find any mention of this on reddit or linkedin. Was I really the only one using it? I don't think so.&lt;/p&gt; &lt;p&gt;I still found the library on pypi, &lt;a href="https://github.com/GPTim/chonkie"&gt;here&lt;/a&gt; a GH repository with the latest pushed version 0.5.1&lt;/p&gt; &lt;p&gt;Does anyone have any news about what happened?&lt;/p&gt; &lt;p&gt;Original GH repository: &lt;a href="https://github.com/chonkie-ai/chonkie"&gt;Page not found ¬∑ GitHub&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SK33LA"&gt; /u/SK33LA &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk857w/chonkie_the_nononsense_rag_chunking_library_just/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk857w/chonkie_the_nononsense_rag_chunking_library_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk857w/chonkie_the_nononsense_rag_chunking_library_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T09:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjuu78</id>
    <title>New DeepSeek V3 (significant improvement) and Gemini 2.5 Pro (SOTA) Tested in long context</title>
    <updated>2025-03-25T21:25:54+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjuu78/new_deepseek_v3_significant_improvement_and/"&gt; &lt;img alt="New DeepSeek V3 (significant improvement) and Gemini 2.5 Pro (SOTA) Tested in long context" src="https://preview.redd.it/ks0djm85lwqe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7c523a2ac3957a50567391d0e0d6a09816702e7" title="New DeepSeek V3 (significant improvement) and Gemini 2.5 Pro (SOTA) Tested in long context" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ks0djm85lwqe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjuu78/new_deepseek_v3_significant_improvement_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjuu78/new_deepseek_v3_significant_improvement_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T21:25:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjgi8y</id>
    <title>Deepseek V3 0324 is now the best non-reasoning model (across both open and closed source) according to Artificial Analisys.</title>
    <updated>2025-03-25T10:47:48+00:00</updated>
    <author>
      <name>/u/kristaller486</name>
      <uri>https://old.reddit.com/user/kristaller486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgi8y/deepseek_v3_0324_is_now_the_best_nonreasoning/"&gt; &lt;img alt="Deepseek V3 0324 is now the best non-reasoning model (across both open and closed source) according to Artificial Analisys." src="https://preview.redd.it/4hh6ys9gftqe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e01ab49fd276d31a93696fb2a9a9f51d5ad35c7" title="Deepseek V3 0324 is now the best non-reasoning model (across both open and closed source) according to Artificial Analisys." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kristaller486"&gt; /u/kristaller486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4hh6ys9gftqe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgi8y/deepseek_v3_0324_is_now_the_best_nonreasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgi8y/deepseek_v3_0324_is_now_the_best_nonreasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T10:47:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk5udi</id>
    <title>Jensen Huang on GPUs - Computerphile</title>
    <updated>2025-03-26T06:58:20+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk5udi/jensen_huang_on_gpus_computerphile/"&gt; &lt;img alt="Jensen Huang on GPUs - Computerphile" src="https://external-preview.redd.it/phUXsHRgF1E2nsygi2QqQyxnIknoI3fD9D7Q_h5m5rs.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=879ede20cdcf5fe1c140d267b9146bf0d41f481a" title="Jensen Huang on GPUs - Computerphile" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=G6R7UOFx1bw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk5udi/jensen_huang_on_gpus_computerphile/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk5udi/jensen_huang_on_gpus_computerphile/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T06:58:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjsiiw</id>
    <title>Mario game made by new a Gemini pro 2.5 in couple minutes - best version I ever saw. Even great physics!</title>
    <updated>2025-03-25T19:51:51+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjsiiw/mario_game_made_by_new_a_gemini_pro_25_in_couple/"&gt; &lt;img alt="Mario game made by new a Gemini pro 2.5 in couple minutes - best version I ever saw. Even great physics!" src="https://external-preview.redd.it/N283c2VudGQ0d3FlMV2EuLTbyq8GSEyVM5EFne5QcU-eiwqTnkibTrWsMAGW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=749ce5ef623cae53e2677afd031ff951b8a489ce" title="Mario game made by new a Gemini pro 2.5 in couple minutes - best version I ever saw. Even great physics!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/955pvmtd4wqe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjsiiw/mario_game_made_by_new_a_gemini_pro_25_in_couple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjsiiw/mario_game_made_by_new_a_gemini_pro_25_in_couple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T19:51:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk57au</id>
    <title>How I adapted a 1B function calling LLM for fast routing and agent hand -off scenarios in a framework agnostic way.</title>
    <updated>2025-03-26T06:10:37+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk57au/how_i_adapted_a_1b_function_calling_llm_for_fast/"&gt; &lt;img alt="How I adapted a 1B function calling LLM for fast routing and agent hand -off scenarios in a framework agnostic way." src="https://preview.redd.it/f5ex9ltz6zqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d8be8b19f9bc0bb0f4e9c01fe98d94cdbb16ebf" title="How I adapted a 1B function calling LLM for fast routing and agent hand -off scenarios in a framework agnostic way." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You might have heard a thing or two about agents. Things that have high level goals and usually run in a loop to complete a said task - the trade off being latency for some powerful automation work&lt;/p&gt; &lt;p&gt;Well if you have been building with agents then you know that users can switch between them.Mid context and expect you to get the routing and agent hand off scenarios right. So now you are focused on not only working on the goals of your agent you are also working on thus pesky work on fast, contextual routing and hand off &lt;/p&gt; &lt;p&gt;Well I just adapted Arch-Function a SOTA function calling LLM that can make precise tools calls for common agentic scenarios to support routing to more coarse-grained or high-level agent definitions&lt;/p&gt; &lt;p&gt;The project can be found here: &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt; and the models are listed in the README. &lt;/p&gt; &lt;p&gt;Happy bulking üõ†Ô∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f5ex9ltz6zqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk57au/how_i_adapted_a_1b_function_calling_llm_for_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk57au/how_i_adapted_a_1b_function_calling_llm_for_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T06:10:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjv68r</id>
    <title>Aider - A new Gemini pro 2.5 just ate sonnet 3.7 thinking like a snack ;-)</title>
    <updated>2025-03-25T21:39:40+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjv68r/aider_a_new_gemini_pro_25_just_ate_sonnet_37/"&gt; &lt;img alt="Aider - A new Gemini pro 2.5 just ate sonnet 3.7 thinking like a snack ;-)" src="https://preview.redd.it/vnkynyqrnwqe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b966146c75053316ab0b7e51083981dd658f31bd" title="Aider - A new Gemini pro 2.5 just ate sonnet 3.7 thinking like a snack ;-)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vnkynyqrnwqe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjv68r/aider_a_new_gemini_pro_25_just_ate_sonnet_37/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjv68r/aider_a_new_gemini_pro_25_just_ate_sonnet_37/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T21:39:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjvo4e</id>
    <title>we are just 3 months into 2025</title>
    <updated>2025-03-25T22:00:40+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjvo4e/we_are_just_3_months_into_2025/"&gt; &lt;img alt="we are just 3 months into 2025" src="https://b.thumbs.redditmedia.com/UkF78GvO1l_Iu4ZikUoUTSJvE6F25Fvn1d1yTgP75FU.jpg" title="we are just 3 months into 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/sijszr0lrwqe1.png?width=832&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e3073891e4d9e2650e53ef7f9aa6cd393d23c81"&gt;https://preview.redd.it/sijszr0lrwqe1.png?width=832&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e3073891e4d9e2650e53ef7f9aa6cd393d23c81&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjvo4e/we_are_just_3_months_into_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjvo4e/we_are_just_3_months_into_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjvo4e/we_are_just_3_months_into_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T22:00:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk0qjs</id>
    <title>1.78bit DeepSeek-V3-0324 - 230GB Unsloth Dynamic GGUF</title>
    <updated>2025-03-26T01:51:16+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk0qjs/178bit_deepseekv30324_230gb_unsloth_dynamic_gguf/"&gt; &lt;img alt="1.78bit DeepSeek-V3-0324 - 230GB Unsloth Dynamic GGUF" src="https://b.thumbs.redditmedia.com/VQUhjwmdzkKwJU-pPDHGMZROeUu65PNj2UGT0ZKAjUg.jpg" title="1.78bit DeepSeek-V3-0324 - 230GB Unsloth Dynamic GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey r/LocalLLaMA! We're back again to release DeepSeek-V3-0324 (671B) dynamic quants in &lt;strong&gt;1.78-bit and more GGUF formats&lt;/strong&gt; so you can run them locally. All GGUFs are at &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/67rxi0wo3yqe1.gif"&gt;https://i.redd.it/67rxi0wo3yqe1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We initially provided the &lt;strong&gt;1.58-bit version&lt;/strong&gt;, which you can still use but its outputs weren't the best. So, we found it necessary to upcast to 1.78-bit by increasing the down proj size to achieve much better performance.&lt;/p&gt; &lt;p&gt;To ensure the best tradeoff between accuracy and size, we do &lt;strong&gt;not to quantize all layers&lt;/strong&gt;, but selectively quantize e.g. the MoE layers to lower bit, and leave attention and other layers in 4 or 6bit. This time we also added 3.5 + 4.5-bit dynamic quants.&lt;/p&gt; &lt;p&gt;Read our Guide on How To Run the GGUFs on llama.cpp: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-v3-0324-locally"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-v3-0324-locally&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also found that if you use convert all layers to 2-bit (standard 2-bit GGUF), the model is still very bad, producing endless loops, gibberish and very poor code. Our Dynamic 2.51-bit quant largely solves this issue. The same applies for 1.78-bit however is it recommended to use our 2.51 version for best results.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model uploads:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;MoE Bits&lt;/th&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Disk Size&lt;/th&gt; &lt;th align="left"&gt;HF Link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1.78bit (prelim)&lt;/td&gt; &lt;td align="left"&gt;IQ1_S&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;151GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-IQ1_S"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1.93bit (prelim)&lt;/td&gt; &lt;td align="left"&gt;IQ1_M&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;178GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-IQ1_M"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.42-bit (prelim)&lt;/td&gt; &lt;td align="left"&gt;IQ2_XXS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;203GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-IQ2_XXS"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;2.71-bit (best)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Q2_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;231GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-Q2_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3.5-bit&lt;/td&gt; &lt;td align="left"&gt;Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;321GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-Q3_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4.5-bit&lt;/td&gt; &lt;td align="left"&gt;Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;406GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-Q4_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;For recommended settings:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Temperature of 0.3&lt;/strong&gt; (Maybe 0.0 for coding as &lt;a href="https://api-docs.deepseek.com/quick_start/parameter_settings"&gt;seen here&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Min_P of 0.00 (optional, but 0.01 works well, llama.cpp default is 0.1)&lt;/li&gt; &lt;li&gt;Chat template: &lt;code&gt;&amp;lt;ÔΩúUserÔΩú&amp;gt;Create a simple playable Flappy Bird Game in Python. Place the final game inside of a markdown section.&amp;lt;ÔΩúAssistantÔΩú&amp;gt;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;A BOS token of &lt;code&gt;&amp;lt;ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú&amp;gt;&lt;/code&gt; is auto added during tokenization (do NOT add it manually!)&lt;/li&gt; &lt;li&gt;DeepSeek mentioned using a &lt;strong&gt;system prompt&lt;/strong&gt; as well (optional) - it's in Chinese: &lt;code&gt;ËØ•Âä©Êâã‰∏∫DeepSeek ChatÔºåÁî±Ê∑±Â∫¶Ê±ÇÁ¥¢ÂÖ¨Âè∏ÂàõÈÄ†„ÄÇ\n‰ªäÂ§©ÊòØ3Êúà24Êó•ÔºåÊòüÊúü‰∏Ä„ÄÇ&lt;/code&gt; which translates to: &lt;code&gt;The assistant is DeepSeek Chat, created by DeepSeek.\nToday is Monday, March 24th.&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;For KV cache quantization, use 8bit, NOT 4bit - we found it to do noticeably worse.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I suggest people to run the 2.71bit for now - the other other bit quants (listed as prelim) are still processing.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# !pip install huggingface_hub hf_transfer import os os.environ[&amp;quot;HF_HUB_ENABLE_HF_TRANSFER&amp;quot;] = &amp;quot;1&amp;quot; from huggingface_hub import snapshot_download snapshot_download( repo_id = &amp;quot;unsloth/DeepSeek-V3-0324-GGUF&amp;quot;, local_dir = &amp;quot;unsloth/DeepSeek-V3-0324-GGUF&amp;quot;, allow_patterns = [&amp;quot;*UD-Q2_K_XL*&amp;quot;], # Dynamic 2.7bit (230GB) ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I did both the Flappy Bird and Heptagon test (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j7r47l/i%5C_just%5C_made%5C_an%5C_animation%5C_of%5C_a%5C_ball%5C_bouncing/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1j7r47l/i\_just\_made\_an\_animation\_of\_a\_ball\_bouncing/&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk0qjs/178bit_deepseekv30324_230gb_unsloth_dynamic_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk0qjs/178bit_deepseekv30324_230gb_unsloth_dynamic_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk0qjs/178bit_deepseekv30324_230gb_unsloth_dynamic_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T01:51:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjorwd</id>
    <title>I think we‚Äôre going to need a bigger bank account.</title>
    <updated>2025-03-25T17:20:34+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjorwd/i_think_were_going_to_need_a_bigger_bank_account/"&gt; &lt;img alt="I think we‚Äôre going to need a bigger bank account." src="https://preview.redd.it/zr3syf8mdvqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=27187b0a5f34d831c3e26fc2978cc6ab6324cf35" title="I think we‚Äôre going to need a bigger bank account." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zr3syf8mdvqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjorwd/i_think_were_going_to_need_a_bigger_bank_account/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjorwd/i_think_were_going_to_need_a_bigger_bank_account/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T17:20:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk7cka</id>
    <title>Plenty 3090 FE's for sale in the Netherlands</title>
    <updated>2025-03-26T08:56:31+00:00</updated>
    <author>
      <name>/u/jwestra</name>
      <uri>https://old.reddit.com/user/jwestra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk7cka/plenty_3090_fes_for_sale_in_the_netherlands/"&gt; &lt;img alt="Plenty 3090 FE's for sale in the Netherlands" src="https://preview.redd.it/3bxpnick00re1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7295d7a24ea4033f0e4d96d4cdbf0b662770a23a" title="Plenty 3090 FE's for sale in the Netherlands" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jwestra"&gt; /u/jwestra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3bxpnick00re1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk7cka/plenty_3090_fes_for_sale_in_the_netherlands/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk7cka/plenty_3090_fes_for_sale_in_the_netherlands/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T08:56:31+00:00</published>
  </entry>
</feed>
