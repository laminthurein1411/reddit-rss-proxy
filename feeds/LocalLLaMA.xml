<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-03T20:48:34+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ig8ve3</id>
    <title>Americans can distill models too</title>
    <updated>2025-02-02T21:55:59+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLLaMA, I'm a TTS model trainer and a US citizen. Last month, I put out a &lt;a href="https://huggingface.co/posts/hexgrad/418806998707773"&gt;call for synthetic training data&lt;/a&gt;, that call was answered with well over a hundred hours of audio in various languages, and the resulting model &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;Kokoro-82M&lt;/a&gt; has since been upgraded/delivered. Happy customers all around.&lt;/p&gt; &lt;p&gt;The current model mostly excels at &lt;em&gt;reading long texts&lt;/em&gt; and has some glaring limitations, especially on short texts. It's also been described as relatively flat and emotionless. Nevertheless, it is currently the most-liked &lt;a href="https://huggingface.co/models?pipeline_tag=text-to-speech&amp;amp;sort=likes"&gt;TTS model&lt;/a&gt; and &lt;a href="https://huggingface.co/spaces?sort=likes&amp;amp;search=tts"&gt;TTS space&lt;/a&gt; on Hugging Face thanks to people smashing that like button.&lt;/p&gt; &lt;p&gt;Now, I'm considering making another call for crowdsourced data, except this time with a focus on only ChatGPT Advanced Voice Mode text/audio pairs, likely just in English, spanning whatever emotions people can prompt out of it. If successful, it could result in a substantially better &lt;em&gt;conversational&lt;/em&gt; model within the same size class, albeit more limited on voices and languages.&lt;/p&gt; &lt;p&gt;There are many things to consider:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Top priority would be given to paying ChatGPT subscribers, $20 and $200, but in practice free AVM audio would likely be admitted as well. This is because the paying subscribers would be least likely to be using a quantized and/or distilled AVM product.&lt;/li&gt; &lt;li&gt;Ideally I could maximally open source any voicepack derived from the AVM data, which means the people contributing audio would have to do it for ideological reasons, and couldn't be compensated with an &amp;quot;exclusive voicepack&amp;quot;. Also, any sponsorships I receive are directed at GPU compute, and both on principle + potential legal liability, I cannot financially compensate people who give me synthetic data.&lt;/li&gt; &lt;li&gt;As far as ToS goes, this distillation strategy rests on the fact that I am not the one obtaining the data, others are. Obviously, I do not agree with the OpenAI ToS or feel bound by it because I don't use any of their products. Feel free to comment on how dumb this strategy is.&lt;/li&gt; &lt;li&gt;I have skimmed Part 2 of the US Copyright Office's Report on AI. I still see no copyright protection on synthetic data of this nature, but any lawyers (real or wannabe) can chime in here with the default prefix of IANL.&lt;/li&gt; &lt;li&gt;I do not wish to be sued, and I'm also deeply allergic to .50 caliber bullets. Jokes aside, I think OpenAI likely has bigger whales to fry, than some guy training 82M param speech models.&lt;/li&gt; &lt;li&gt;Why do it: these small TTS models are (relatively) cheap to train, especially compared to LLMs, and the total utility they offer might exceed their cost, at least for now, until Zucc drops Llama 4 multimodal or DeepSeek puts up a good audio model, etc.&lt;/li&gt; &lt;li&gt;The scale of data I am looking for is at least 10 hours per voice/emotion, but label quality also matters. Each audio file would have to be fished out one-by-one, since there are no API calls for AVM.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I understand this is LocalLLaMA and people here are likely very pro-open-weights, pro-open-source, and therefore anti-OpenAI. But putting aside any feelings you might have about various sides of history, (A) how do we generally feel about building a model this way and (B) do we think enough people would answer the call?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig8ve3/americans_can_distill_models_too/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig8ve3/americans_can_distill_models_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig8ve3/americans_can_distill_models_too/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T21:55:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1igr151</id>
    <title>LLM Based Mapping</title>
    <updated>2025-02-03T15:02:14+00:00</updated>
    <author>
      <name>/u/ranoutofusernames__</name>
      <uri>https://old.reddit.com/user/ranoutofusernames__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, just wanted to share the open-source repo of a project I shared in the past.&lt;/p&gt; &lt;p&gt;Godview is a map that works entirely off of large language models. You can search for places or ask questions and it will plot the results and give you addresses and websites (when applicable). It also has a &amp;quot;discover&amp;quot; page where you can click on any point on the map and get the model to tell you about it. Very simple implementation but it's gotten a lot of use so figured I'd share it. It runs locally if you have Ollama running. Just cycle through the models button until it hits &amp;quot;Local&amp;quot; instead of the cloud providers.&lt;/p&gt; &lt;p&gt;Link to repo: &lt;a href="https://github.com/space0blaster/godview"&gt;https://github.com/space0blaster/godview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you'd like to try out the free demo/cloud version: &lt;a href="https://godview.ai"&gt;https://godview.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ranoutofusernames__"&gt; /u/ranoutofusernames__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igr151/llm_based_mapping/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igr151/llm_based_mapping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igr151/llm_based_mapping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T15:02:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ignwxh</id>
    <title>Cursor now supports deepseek v3 and r1 models</title>
    <updated>2025-02-03T12:27:03+00:00</updated>
    <author>
      <name>/u/Available-Stress8598</name>
      <uri>https://old.reddit.com/user/Available-Stress8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ignwxh/cursor_now_supports_deepseek_v3_and_r1_models/"&gt; &lt;img alt="Cursor now supports deepseek v3 and r1 models" src="https://b.thumbs.redditmedia.com/Ugm-o5reKn35RsdVj7A8-79UAJyFEluwtKhDUSaoGas.jpg" title="Cursor now supports deepseek v3 and r1 models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/1t2mirr83xge1.png?width=849&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6ea465bb381d350dac2fe65c04c009d8a98a7ef"&gt;https://preview.redd.it/1t2mirr83xge1.png?width=849&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6ea465bb381d350dac2fe65c04c009d8a98a7ef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Since Deepseek is open source, you don't need Cursor Pro to use it, we don't need to worry about rate limits anymore.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available-Stress8598"&gt; /u/Available-Stress8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ignwxh/cursor_now_supports_deepseek_v3_and_r1_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ignwxh/cursor_now_supports_deepseek_v3_and_r1_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ignwxh/cursor_now_supports_deepseek_v3_and_r1_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T12:27:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1igte22</id>
    <title>GitHub - openorch/openorch: A language-agnostic, distributed backend platform for AI, microservices, and beyond.</title>
    <updated>2025-02-03T16:41:39+00:00</updated>
    <author>
      <name>/u/crufter</name>
      <uri>https://old.reddit.com/user/crufter</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crufter"&gt; /u/crufter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/openorch/openorch"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igte22/github_openorchopenorch_a_languageagnostic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igte22/github_openorchopenorch_a_languageagnostic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T16:41:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1igvpu5</id>
    <title>Imatrix gguf vs regular GGUF? Which one to use? Why?</title>
    <updated>2025-02-03T18:14:33+00:00</updated>
    <author>
      <name>/u/No_Expert1801</name>
      <uri>https://old.reddit.com/user/No_Expert1801</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I donâ€™t know the difference can someone please tell me, and which one should be used?&lt;/p&gt; &lt;p&gt;16gb VRAM user incase that matters &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Expert1801"&gt; /u/No_Expert1801 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igvpu5/imatrix_gguf_vs_regular_gguf_which_one_to_use_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igvpu5/imatrix_gguf_vs_regular_gguf_which_one_to_use_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igvpu5/imatrix_gguf_vs_regular_gguf_which_one_to_use_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T18:14:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1igdnx2</id>
    <title>Ok I admit it, Browser Use is insane (using gemini 2.0 flash-exp default) [https://github.com/browser-use/browser-use]</title>
    <updated>2025-02-03T01:38:56+00:00</updated>
    <author>
      <name>/u/teddybear082</name>
      <uri>https://old.reddit.com/user/teddybear082</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igdnx2/ok_i_admit_it_browser_use_is_insane_using_gemini/"&gt; &lt;img alt="Ok I admit it, Browser Use is insane (using gemini 2.0 flash-exp default) [https://github.com/browser-use/browser-use]" src="https://preview.redd.it/evlscivtvtge1.gif?width=640&amp;amp;crop=smart&amp;amp;s=8e17dd7562de9086f58eb97b4363b79f94ad14a3" title="Ok I admit it, Browser Use is insane (using gemini 2.0 flash-exp default) [https://github.com/browser-use/browser-use]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teddybear082"&gt; /u/teddybear082 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/evlscivtvtge1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igdnx2/ok_i_admit_it_browser_use_is_insane_using_gemini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igdnx2/ok_i_admit_it_browser_use_is_insane_using_gemini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T01:38:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1igcpwz</id>
    <title>Kokoro TTS 1.0</title>
    <updated>2025-02-03T00:52:05+00:00</updated>
    <author>
      <name>/u/zxyzyxz</name>
      <uri>https://old.reddit.com/user/zxyzyxz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcpwz/kokoro_tts_10/"&gt; &lt;img alt="Kokoro TTS 1.0" src="https://external-preview.redd.it/PSxCcCk18RpMpFh_Tgc1ycbd0zsabOZK7av3YdT9fA4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b75663383244e2aa5f5fcf0207756c5dc28fb51b" title="Kokoro TTS 1.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zxyzyxz"&gt; /u/zxyzyxz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcpwz/kokoro_tts_10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igcpwz/kokoro_tts_10/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T00:52:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1igpfgp</id>
    <title>whats the difference between uncensored and abliterated models</title>
    <updated>2025-02-03T13:47:25+00:00</updated>
    <author>
      <name>/u/Traveling_Pirate2190</name>
      <uri>https://old.reddit.com/user/Traveling_Pirate2190</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im quite new to llms and ai but i was searching around and i found some models are uncensored and some models are ablitirated i tried both and they simply performed the task aka asking the old &amp;quot;how to create a ied question&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Traveling_Pirate2190"&gt; /u/Traveling_Pirate2190 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igpfgp/whats_the_difference_between_uncensored_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igpfgp/whats_the_difference_between_uncensored_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igpfgp/whats_the_difference_between_uncensored_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T13:47:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig2cm2</id>
    <title>mistral-small-24b-instruct-2501 is simply the best model ever made.</title>
    <updated>2025-02-02T17:25:29+00:00</updated>
    <author>
      <name>/u/hannibal27</name>
      <uri>https://old.reddit.com/user/hannibal27</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Itâ€™s the only truly good model that can run locally on a normal machine. I'm running it on my M3 36GB and it performs fantastically with 18 TPS (tokens per second). It responds to everything precisely for day-to-day use, serving me as well as ChatGPT does.&lt;/p&gt; &lt;p&gt;For the first time, I see a local model actually delivering satisfactory results. Does anyone else think so?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hannibal27"&gt; /u/hannibal27 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T17:25:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1igf1vi</id>
    <title>Phi 4 is so underrated</title>
    <updated>2025-02-03T02:49:57+00:00</updated>
    <author>
      <name>/u/jeremyckahn</name>
      <uri>https://old.reddit.com/user/jeremyckahn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a GPU poor pleb with but a humble M4 Mac mini (24 GB RAM), my local LLM options are limited. As such, I've found Phi 4 (&lt;a href="https://huggingface.co/unsloth/phi-4-GGUF"&gt;Q8, Unsloth variant&lt;/a&gt;) to be an extremely capable model for my hardware. My use cases are general knowledge questions and coding prompts. It's at least as good as GPT 3.5 in my experience and sets me on the right direction more often then not. I can't speak to benchmarks because I don't really understand (or frankly care about) any of them. It's just a good model for the things I need a model for.&lt;/p&gt; &lt;p&gt;And no, Microsoft isn't paying me. I'm just a fan. ðŸ™‚&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jeremyckahn"&gt; /u/jeremyckahn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igf1vi/phi_4_is_so_underrated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igf1vi/phi_4_is_so_underrated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igf1vi/phi_4_is_so_underrated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T02:49:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1igr55c</id>
    <title>Training deepseek r1 to trade stocks</title>
    <updated>2025-02-03T15:07:11+00:00</updated>
    <author>
      <name>/u/ExaminationNo8522</name>
      <uri>https://old.reddit.com/user/ExaminationNo8522</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like everyone else on the internet, I was really fascinated by deepseek's abilities, but the thing that got me the most was how they trained deepseek-r1-zero. Essentially, it just seemed to boil down to: &amp;quot;feed the machine an objective reward function, and train it a whole bunch, letting it think a variable amount&amp;quot;. So I thought: hey, you can use stock prices going up and down as an objective reward function kinda? &lt;/p&gt; &lt;p&gt;Anyways, so I used huggingface's open-r1 to write a version of deepseek that aims to maximize short-term stock prediction, by acting as a &amp;quot;stock analyst&amp;quot; of sort, offering buy and sell recommendations based on some signals I scraped for each company. All the code and colab and discussion is at &lt;a href="https://2084.substack.com/p/2084-deepstock-can-you-train-deepseek"&gt;2084: Deepstock - can you train deepseek to do stock trading?&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Training it rn over the next week, my goal is to get it to do better than random, altho getting it to that point is probably going to take a ton of compute. (Anyone got any spare?)&lt;/p&gt; &lt;p&gt;Thoughts on how I should expand this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExaminationNo8522"&gt; /u/ExaminationNo8522 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igr55c/training_deepseek_r1_to_trade_stocks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igr55c/training_deepseek_r1_to_trade_stocks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igr55c/training_deepseek_r1_to_trade_stocks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T15:07:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1iggetv</id>
    <title>Make your Mistral Small 3 24B Think like R1-distilled models</title>
    <updated>2025-02-03T04:01:42+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"&gt; &lt;img alt="Make your Mistral Small 3 24B Think like R1-distilled models" src="https://external-preview.redd.it/P8lS0kk6BFe2IEo6TxCZd1LVwksc34IkzGTVx_SCc8w.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d74dbe4f1d67cc8b587db9aa01762f26e269bcf" title="Make your Mistral Small 3 24B Think like R1-distilled models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been seeing a lot of posts about the Mistral Small 3 24B model, and I remember having this CoT system prompt in my collection. I might as well try it out on this new model. I haven't used it for a long time since I switched to R1-distilled-32b. &lt;/p&gt; &lt;p&gt;I'm not the original writer of this prompt; I've rewritten some parts of it, and I can't remember where I got it from.&lt;/p&gt; &lt;p&gt;System prompt: &lt;a href="https://pastebin.com/sVMrgZBp"&gt;https://pastebin.com/sVMrgZBp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is just an experiment. I doubt it will actually make your model smarter in a noticeable way, this is not a replacement of Mistral's furture reasoning models&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/d1geatbckuge1.gif"&gt;https://i.redd.it/d1geatbckuge1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/hyrryecnkuge1.gif"&gt;https://i.redd.it/hyrryecnkuge1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T04:01:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1igo6c9</id>
    <title>Don't forget to optimize your hardware! (Windows)</title>
    <updated>2025-02-03T12:41:48+00:00</updated>
    <author>
      <name>/u/rpwoerk</name>
      <uri>https://old.reddit.com/user/rpwoerk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igo6c9/dont_forget_to_optimize_your_hardware_windows/"&gt; &lt;img alt="Don't forget to optimize your hardware! (Windows)" src="https://b.thumbs.redditmedia.com/mq5j1Xjh-aMGbb1yZh8muOtGDY8qezCGISvJPZlb6kM.jpg" title="Don't forget to optimize your hardware! (Windows)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rpwoerk"&gt; /u/rpwoerk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1igo6c9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igo6c9/dont_forget_to_optimize_your_hardware_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igo6c9/dont_forget_to_optimize_your_hardware_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T12:41:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig6e6t</id>
    <title>DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt.</title>
    <updated>2025-02-02T20:12:17+00:00</updated>
    <author>
      <name>/u/Qaxar</name>
      <uri>https://old.reddit.com/user/Qaxar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"&gt; &lt;img alt="DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt." src="https://external-preview.redd.it/Er7i7V1ka8BO-MpGkuLs0Jmvu0-6GTVfn9JqY2PTKfM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd56ea2fa742541be1366b6615889d6a52f560b3" title="DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We knew R1 was good, but not that good. All the cries of CCP censorship are meaningless when it's trivial to bypass its guard rails.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qaxar"&gt; /u/Qaxar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/rohanpaul_ai/status/1886025249273339961?t=Wpp2kGJKVSZtSAOmTJjh0g&amp;amp;s=19"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T20:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iggwff</id>
    <title>Mistral, Qwen, Deepseek</title>
    <updated>2025-02-03T04:28:53+00:00</updated>
    <author>
      <name>/u/Stargazer-8989</name>
      <uri>https://old.reddit.com/user/Stargazer-8989</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Aren't you noticing a pattern? Companies outside the USA are releasing models like Mistral AI, Qwen, and DeepSeek - reliable models that are made accessible, smaller and open-source, compared to most US-based companies &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stargazer-8989"&gt; /u/Stargazer-8989 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggwff/mistral_qwen_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggwff/mistral_qwen_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iggwff/mistral_qwen_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T04:28:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1igwkpw</id>
    <title>I trained a tinystories model from scratch for educational purposes, how cooked? (1M-parameters)</title>
    <updated>2025-02-03T18:48:54+00:00</updated>
    <author>
      <name>/u/THE--GRINCH</name>
      <uri>https://old.reddit.com/user/THE--GRINCH</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igwkpw/i_trained_a_tinystories_model_from_scratch_for/"&gt; &lt;img alt="I trained a tinystories model from scratch for educational purposes, how cooked? (1M-parameters)" src="https://preview.redd.it/mdy6shrszyge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cae5a6b70dd9f4b9a4cda0016978c57391368c6" title="I trained a tinystories model from scratch for educational purposes, how cooked? (1M-parameters)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/THE--GRINCH"&gt; /u/THE--GRINCH &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mdy6shrszyge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igwkpw/i_trained_a_tinystories_model_from_scratch_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igwkpw/i_trained_a_tinystories_model_from_scratch_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T18:48:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1igpedw</id>
    <title>Mistral Small 3: Redefining Expectations â€“ Performance Beyond Its Size (Feels Like a 70B Model!)</title>
    <updated>2025-02-03T13:45:54+00:00</updated>
    <author>
      <name>/u/Vishnu_One</name>
      <uri>https://old.reddit.com/user/Vishnu_One</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igpedw/mistral_small_3_redefining_expectations/"&gt; &lt;img alt="Mistral Small 3: Redefining Expectations â€“ Performance Beyond Its Size (Feels Like a 70B Model!)" src="https://b.thumbs.redditmedia.com/65ozbJ1ALEQgiuu_PZ9iyOKM1ciEEq2a4swxnh3Ta_k.jpg" title="Mistral Small 3: Redefining Expectations â€“ Performance Beyond Its Size (Feels Like a 70B Model!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ðŸš€ Hold onto your hats, folks! Mistral Small 3 is here to blow your minds! This isn't just another small model â€“ it's a powerhouse that feels like you're wielding a 70B beast! I've thrown every complex question I could think of at it, and the results are mind-blowing. From coding conundrums to deep language understanding, this thing is breaking barriers left and right.&lt;/p&gt; &lt;p&gt;I dare you to try it out and share your experiences here. Let's see what crazy things we can make Mistral Small 3 do! Who else is ready to have their expectations redefined? ðŸ¤¯&lt;br /&gt; This is Q4_K_M just 14GB&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/fdqvgbm9gxge1.gif"&gt;https://i.redd.it/fdqvgbm9gxge1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Prompt&lt;/p&gt; &lt;p&gt;Create an interactive web page that animates the Sun and the planets in our Solar System. The animation should include the following features:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Sun&lt;/strong&gt; : A central, bright yellow circle representing the Sun.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Planets&lt;/strong&gt; : Eight planets (Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune) orbiting around the Sun with realistic relative sizes and distances.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Orbits&lt;/strong&gt; : Visible elliptical orbits for each planet to show their paths around the Sun.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Animation&lt;/strong&gt; : Smooth orbital motion for all planets, with varying speeds based on their actual orbital periods.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Labels&lt;/strong&gt; : Clickable labels for each planet that display additional information when hovered over or clicked (e.g., name, distance from the Sun, orbital period).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interactivity&lt;/strong&gt; : Users should be able to pause and resume the animation using buttons.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Ensure the design is visually appealing with a dark background to enhance the visibility of the planets and their orbits. Use CSS for styling and JavaScript for the animation logic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vishnu_One"&gt; /u/Vishnu_One &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igpedw/mistral_small_3_redefining_expectations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igpedw/mistral_small_3_redefining_expectations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igpedw/mistral_small_3_redefining_expectations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T13:45:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ign0lz</id>
    <title>DeepSeek-R1 never ever relaxes...</title>
    <updated>2025-02-03T11:30:28+00:00</updated>
    <author>
      <name>/u/IrisColt</name>
      <uri>https://old.reddit.com/user/IrisColt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ign0lz/deepseekr1_never_ever_relaxes/"&gt; &lt;img alt="DeepSeek-R1 never ever relaxes..." src="https://external-preview.redd.it/P8lS0kk6BFe2IEo6TxCZd1LVwksc34IkzGTVx_SCc8w.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d74dbe4f1d67cc8b587db9aa01762f26e269bcf" title="DeepSeek-R1 never ever relaxes..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I was testing DeepSeek-R1 with a math problem I found in a textbook for 9-year-olds &lt;strong&gt;(yes, really)&lt;/strong&gt;, and the model managed to crack it.&lt;/p&gt; &lt;p&gt;The problem was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Find two 3-digit palindromic numbers that add up to a 4-digit palindromic number. Note: the first digit of any of these numbers can't be 0.&amp;quot;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ml5hnng3rwge1.jpg?width=1800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1456610eeff8d8b9a122d86fbb44967f84f682d9"&gt;R1 starts thinking...&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now, hereâ€™s where it gets interesting. R1 thought for a bit, found the correct answer in its &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; block, then went ahead to output itâ€”but made a mistake.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/77bke6q1swge1.jpg?width=1800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d6eac07677fe576be9e699776a2134cba1d15c62"&gt;R1 makes a mistake...&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Before even finishing its response, it caught its own error, backtracked, and corrected itself on the fly outside of the&lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; block.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yc3zjamsswge1.jpg?width=1800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=903d42998593e95a68ff32006b7bac6335df9f1e"&gt;R1 corrects itself...&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j8vgvxn3twge1.jpg?width=1800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b189fce4a099ed9182b315c2164a1071a4a32104"&gt;R1's final answer.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/0Ayv77LN"&gt;DeepSeek-R1 complete answer.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Regarding the problem, &lt;strong&gt;no other LLM solved it, except for&lt;/strong&gt; &lt;a href="https://pastebin.com/YCRR521W"&gt;&lt;strong&gt;OpenAI o1&lt;/strong&gt;&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;So now Iâ€™m wonderingâ€”&lt;strong&gt;what's holding them back?&lt;/strong&gt; Is it the tokenizer's weaknesses? The sampling parameters (even when all where at the recommended settings they failed)? Or maybe, just maybe, non-thinking LLMs are really that bad at math? &lt;/p&gt; &lt;p&gt;Would love to hear thoughts on this.&lt;/p&gt; &lt;p&gt;Unsuccessful attemps by other models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://pastebin.com/r8VKHrcA"&gt;chatgpt-4o-latest-20241120&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pastebin.com/tXc7wGVz"&gt;claude-3-5-sonnet-20241022&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pastebin.com/zGzQJ8B5"&gt;phi-4&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pastebin.com/vt54UFBe"&gt;amazon-nova-pro-v1.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pastebin.com/eSN4y6E0"&gt;gemini-exp-1206&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pastebin.com/jVj1KcMF"&gt;llama-3.1-405b-instruct-bf16&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pastebin.com/ZRLfhEfU"&gt;qwen-max-2025-01-25&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IrisColt"&gt; /u/IrisColt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ign0lz/deepseekr1_never_ever_relaxes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ign0lz/deepseekr1_never_ever_relaxes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ign0lz/deepseekr1_never_ever_relaxes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T11:30:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1igcvol</id>
    <title>I built a silent speech recognition tool that reads your lips in real-time and types whatever you mouth - runs 100% locally!</title>
    <updated>2025-02-03T01:00:09+00:00</updated>
    <author>
      <name>/u/tycho_brahes_nose_</name>
      <uri>https://old.reddit.com/user/tycho_brahes_nose_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcvol/i_built_a_silent_speech_recognition_tool_that/"&gt; &lt;img alt="I built a silent speech recognition tool that reads your lips in real-time and types whatever you mouth - runs 100% locally!" src="https://external-preview.redd.it/bnIwMGoyaXludGdlMVL1KlPwXSM4mwFtLRlx6KM67CArRsK705RfUy_x1msn.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b064a65e7251b4b07e096a39fc4d698d7f457b36" title="I built a silent speech recognition tool that reads your lips in real-time and types whatever you mouth - runs 100% locally!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tycho_brahes_nose_"&gt; /u/tycho_brahes_nose_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dh90m1iyntge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcvol/i_built_a_silent_speech_recognition_tool_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igcvol/i_built_a_silent_speech_recognition_tool_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T01:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1igtj17</id>
    <title>Look at me I still exist you know !</title>
    <updated>2025-02-03T16:47:21+00:00</updated>
    <author>
      <name>/u/prumf</name>
      <uri>https://old.reddit.com/user/prumf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igtj17/look_at_me_i_still_exist_you_know/"&gt; &lt;img alt="Look at me I still exist you know !" src="https://preview.redd.it/eilyf564eyge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1fe49e465e7665c5325457d9ea69e89e44a737c" title="Look at me I still exist you know !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When you have to use ads because DeepSeek-R1 is bankrupting you. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prumf"&gt; /u/prumf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eilyf564eyge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igtj17/look_at_me_i_still_exist_you_know/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igtj17/look_at_me_i_still_exist_you_know/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T16:47:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1igpkc8</id>
    <title>I built a Linux Distro to run Nvidia GPUs for AI</title>
    <updated>2025-02-03T13:53:56+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igpkc8/i_built_a_linux_distro_to_run_nvidia_gpus_for_ai/"&gt; &lt;img alt="I built a Linux Distro to run Nvidia GPUs for AI" src="https://b.thumbs.redditmedia.com/pCPtLbPVUS7barizEgMp7xXMDTS6YQUYF470-tBLf_o.jpg" title="I built a Linux Distro to run Nvidia GPUs for AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I wanted to share a project Iâ€™ve been working on - I built a minimalist Linux distro called &lt;strong&gt;Sbnb Linux&lt;/strong&gt; thatâ€™s super easy to get up and running with Nvidia GPUs.&lt;/p&gt; &lt;p&gt;Hereâ€™s the cool part:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can boot straight from a USB flash drive, no installation needed.&lt;/li&gt; &lt;li&gt;Itâ€™s got all the tools to spin up a virtual machine and attach your Nvidia GPU using a low-overhead &lt;code&gt;vfio-pci&lt;/code&gt; setup.&lt;/li&gt; &lt;li&gt;Once thatâ€™s done, you can easily run AI like DeepSeek R1 in the VM using &lt;strong&gt;ollama&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But wait, thereâ€™s more! The bare metal server and VM are connected through &lt;strong&gt;Tailscale tunnels&lt;/strong&gt;, so you can SSH into them from anywhere using OAuth (Google, etc.).&lt;/p&gt; &lt;p&gt;If anyoneâ€™s curious to give it a shot, all you need is a USB flash drive and about 30 minutes to get up and running. The instructions are here: &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-NVIDIA.md"&gt;GitHub Link&lt;/a&gt;. If you run into any issues, drop a message below and Iâ€™ll be happy to help out!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As a fun weekend project, my kids and I built a beast of a home server powered by an AMD EPYC 7C13 (3rd gen). I posted all the nerdy details and costs over on &lt;a href="/r/homelab"&gt;r/homelab&lt;/a&gt; if you're into that kind of thing: &lt;a href="https://www.reddit.com/r/homelab/comments/1hmnnwg/built_a_powerful_and_silent_amd_epyc_home_server/"&gt;link here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1igpkc8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igpkc8/i_built_a_linux_distro_to_run_nvidia_gpus_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igpkc8/i_built_a_linux_distro_to_run_nvidia_gpus_for_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T13:53:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1igtfqv</id>
    <title>OpenAI Deep Research is working hard on my report, ETA 1-2 weeks</title>
    <updated>2025-02-03T16:43:33+00:00</updated>
    <author>
      <name>/u/PerformanceRound7913</name>
      <uri>https://old.reddit.com/user/PerformanceRound7913</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igtfqv/openai_deep_research_is_working_hard_on_my_report/"&gt; &lt;img alt="OpenAI Deep Research is working hard on my report, ETA 1-2 weeks" src="https://b.thumbs.redditmedia.com/myh4Jhb6qbGKuvnrpcHaCQ0x1M3fspzE5ffGwEcPW4I.jpg" title="OpenAI Deep Research is working hard on my report, ETA 1-2 weeks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/rifxki2edyge1.jpg?width=2090&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a17db6c9919482f611818bb1d2d2bbdee8d1d80c"&gt;https://preview.redd.it/rifxki2edyge1.jpg?width=2090&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a17db6c9919482f611818bb1d2d2bbdee8d1d80c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PerformanceRound7913"&gt; /u/PerformanceRound7913 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igtfqv/openai_deep_research_is_working_hard_on_my_report/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igtfqv/openai_deep_research_is_working_hard_on_my_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igtfqv/openai_deep_research_is_working_hard_on_my_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T16:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1igc6r0</id>
    <title>20 yrs in jail or $1 million for downloading Chinese models proposed at congress</title>
    <updated>2025-02-03T00:26:00+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.hawley.senate.gov/wp-content/uploads/2025/01/Hawley-Decoupling-Americas-Artificial-Intelligence-Capabilities-from-China-Act.pdf"&gt;https://www.hawley.senate.gov/wp-content/uploads/2025/01/Hawley-Decoupling-Americas-Artificial-Intelligence-Capabilities-from-China-Act.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Seriously stop giving your money to these anti open companies and encourage everyone and anyone you know to do the same, don't let your company use their products. Anthrophic and OpenAI are the worse. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igc6r0/20_yrs_in_jail_or_1_million_for_downloading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igc6r0/20_yrs_in_jail_or_1_million_for_downloading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igc6r0/20_yrs_in_jail_or_1_million_for_downloading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T00:26:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1igq9ud</id>
    <title>Jokes aside, which is your favorite local tts model and why?</title>
    <updated>2025-02-03T14:27:05+00:00</updated>
    <author>
      <name>/u/iaseth</name>
      <uri>https://old.reddit.com/user/iaseth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igq9ud/jokes_aside_which_is_your_favorite_local_tts/"&gt; &lt;img alt="Jokes aside, which is your favorite local tts model and why?" src="https://preview.redd.it/ao50vnrwoxge1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=298e2e3c1ed5241562b49a3f4402fcd6c67afae4" title="Jokes aside, which is your favorite local tts model and why?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iaseth"&gt; /u/iaseth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ao50vnrwoxge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igq9ud/jokes_aside_which_is_your_favorite_local_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igq9ud/jokes_aside_which_is_your_favorite_local_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T14:27:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1igpwzl</id>
    <title>Paradigm shift?</title>
    <updated>2025-02-03T14:10:33+00:00</updated>
    <author>
      <name>/u/RetiredApostle</name>
      <uri>https://old.reddit.com/user/RetiredApostle</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igpwzl/paradigm_shift/"&gt; &lt;img alt="Paradigm shift?" src="https://preview.redd.it/gre7z74ylxge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a0d8456581d7b4608dca287eb41e0f6185e191a" title="Paradigm shift?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RetiredApostle"&gt; /u/RetiredApostle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gre7z74ylxge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igpwzl/paradigm_shift/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igpwzl/paradigm_shift/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T14:10:33+00:00</published>
  </entry>
</feed>
