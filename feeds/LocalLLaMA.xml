<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-24T12:27:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jilbkw</id>
    <title>Modifying Large Language Model Post-Training for Diverse Creative Writing</title>
    <updated>2025-03-24T07:40:03+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;As creative writing tasks do not have singular correct answers, large language models (LLMs) trained to perform these tasks should be able to generate diverse valid outputs. However, LLM post-training often focuses on improving generation quality but neglects to facilitate output diversity. Hence, in creative writing generation, we investigate post-training approaches to promote both output diversity and quality. Our core idea is to include deviation -- the degree of difference between a training sample and all other samples with the same prompt -- in the training objective to facilitate learning from rare high-quality instances. By adopting our approach to direct preference optimization (DPO) and odds ratio preference optimization (ORPO), we demonstrate that we can promote the output diversity of trained models while minimally decreasing quality. Our best model with 8B parameters could achieve on-par diversity as a human-created dataset while having output quality similar to the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We further validate our approaches with a human evaluation, an ablation, and a comparison to an existing diversification approach, DivPO.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.17126"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jilbkw/modifying_large_language_model_posttraining_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jilbkw/modifying_large_language_model_posttraining_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T07:40:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jingxo</id>
    <title>How to estimate how much VRAM is needed to load a model and x amount of text?</title>
    <updated>2025-03-24T10:26:44+00:00</updated>
    <author>
      <name>/u/blaher123</name>
      <uri>https://old.reddit.com/user/blaher123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to understand how to estimate how much text I can load into x amount of VRAM when using llama.cpp in python.&lt;/p&gt; &lt;p&gt;For example how much text can I fit in to a 40gb A100 using a 5gb llama 3.2 model?&lt;/p&gt; &lt;p&gt;As I understand it first you have to load the model itself in memory so thats 5gb leaving 35gb for the text. How much text can be stored per gb? I'm aware that any storage space after the 128k token context of llama3.2 is not used?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blaher123"&gt; /u/blaher123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jingxo/how_to_estimate_how_much_vram_is_needed_to_load_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jingxo/how_to_estimate_how_much_vram_is_needed_to_load_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jingxo/how_to_estimate_how_much_vram_is_needed_to_load_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T10:26:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji8o7p</id>
    <title>Quantization Method Matters: MLX Q2 vs GGUF Q2_K: MLX ruins the model performance whereas GGUF keeps it useable</title>
    <updated>2025-03-23T20:19:33+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji8o7p/quantization_method_matters_mlx_q2_vs_gguf_q2_k/"&gt; &lt;img alt="Quantization Method Matters: MLX Q2 vs GGUF Q2_K: MLX ruins the model performance whereas GGUF keeps it useable" src="https://external-preview.redd.it/ZHI0enNwOW16aHFlMU0viwrkIp2qoHODjO1xeyQmexy85eAy-zLVHbokf46f.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81e60a85697fe81f6d46a7a028cc13e36dea4682" title="Quantization Method Matters: MLX Q2 vs GGUF Q2_K: MLX ruins the model performance whereas GGUF keeps it useable" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3ijpyp9mzhqe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji8o7p/quantization_method_matters_mlx_q2_vs_gguf_q2_k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji8o7p/quantization_method_matters_mlx_q2_vs_gguf_q2_k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T20:19:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jif6oa</id>
    <title>Second Me: Local trained Open-source alternative to centralized AI that preserves your autonomy</title>
    <updated>2025-03-24T01:17:15+00:00</updated>
    <author>
      <name>/u/DontPlayMeLikeAFool</name>
      <uri>https://old.reddit.com/user/DontPlayMeLikeAFool</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,I wanted to share our Python-based open-source project &lt;a href="https://github.com/Mindverse/Second-Me"&gt;Second Me&lt;/a&gt;. We've created a framework that lets you build and train a personalized AI representation of yourself.Technical highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hierarchical Memory Modeling with three-layer structure (L0-L2)&lt;/li&gt; &lt;li&gt;Me-alignment system using reinforcement learning&lt;/li&gt; &lt;li&gt;Outperforms leading RAG systems by 37% in personalization tests&lt;/li&gt; &lt;li&gt;Decentralized architecture for AI-to-AI interaction&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Python codebase is well-documented and contributions are welcome! We're particularly interested in expanding the role-play capabilities and improving the memory modeling system.If you're interested in AI, identity, or decentralized AI systems, we'd love your feedback and stars!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DontPlayMeLikeAFool"&gt; /u/DontPlayMeLikeAFool &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jif6oa/second_me_local_trained_opensource_alternative_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jif6oa/second_me_local_trained_opensource_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jif6oa/second_me_local_trained_opensource_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T01:17:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji75t5</id>
    <title>Mistral 24b</title>
    <updated>2025-03-23T19:15:30+00:00</updated>
    <author>
      <name>/u/Illustrious-Dot-6888</name>
      <uri>https://old.reddit.com/user/Illustrious-Dot-6888</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First time using Mistral 24b today. Man, how good this thing is! And fast too!Finally a model that translates perfectly. This is a keeper.ü§ó&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Dot-6888"&gt; /u/Illustrious-Dot-6888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji75t5/mistral_24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji75t5/mistral_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji75t5/mistral_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T19:15:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji5mbg</id>
    <title>Are there any attempts at CPU-only LLM architectures? I know Nvidia doesn't like it, but the biggest threat to their monopoly is AI models that don't need that much GPU compute</title>
    <updated>2025-03-23T18:10:50+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically the title. I know of this post &lt;a href="https://github.com/flawedmatrix/mamba-ssm"&gt;https://github.com/flawedmatrix/mamba-ssm&lt;/a&gt; that optimizes MAMBA for CPU-only devices, but other than that, I don't know of any other effort.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji5mbg/are_there_any_attempts_at_cpuonly_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji5mbg/are_there_any_attempts_at_cpuonly_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji5mbg/are_there_any_attempts_at_cpuonly_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T18:10:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jioxgl</id>
    <title>DeepSeek V3 Minor Update?</title>
    <updated>2025-03-24T11:59:33+00:00</updated>
    <author>
      <name>/u/Cheap_Ship6400</name>
      <uri>https://old.reddit.com/user/Cheap_Ship6400</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jioxgl/deepseek_v3_minor_update/"&gt; &lt;img alt="DeepSeek V3 Minor Update?" src="https://b.thumbs.redditmedia.com/c_od9mincjeFa1u5CXICmGv0ARkit1y9Wm_rKGsXsvQ.jpg" title="DeepSeek V3 Minor Update?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zuoj2qhllmqe1.png?width=1290&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=281e75adeeaf43d451216ce83d12503601cd9b2a"&gt;https://preview.redd.it/zuoj2qhllmqe1.png?width=1290&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=281e75adeeaf43d451216ce83d12503601cd9b2a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Translation of the image:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;DeepSeek Assistant @ DeepSeek: (DeepSeek's official bot)&lt;/p&gt; &lt;p&gt;„ÄêAnnouncement„ÄëThe DeepSeek V3 model has completed a minor version upgrade. You are welcome to try it out on the official website, app, or mini-program (with Deep Thinking disabled). The API interface and usage methods remain unchanged.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My experience:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;It's giving me major DeepSeek R1 vibes. The output's way more unpredictable, plus throwing in fancy emojis. Futhermore, it seems like new V3 is more like Claude when it comes to code and whipping up SVGs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheap_Ship6400"&gt; /u/Cheap_Ship6400 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jioxgl/deepseek_v3_minor_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jioxgl/deepseek_v3_minor_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jioxgl/deepseek_v3_minor_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T11:59:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhwr2p</id>
    <title>Next Gemma versions wishlist</title>
    <updated>2025-03-23T11:00:25+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I'm Omar from the Gemma team. Few months ago, we &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hchoyy/open_models_wishlist/"&gt;asked for user feedback &lt;/a&gt;and incorporated it into Gemma 3: longer context, a smaller model, vision input, multilinguality, and so on, while doing a nice lmsys jump! We also made sure to collaborate with OS maintainers to have decent support at day-0 in your favorite tools, including vision in llama.cpp!&lt;/p&gt; &lt;p&gt;Now, it's time to look into the future. What would you like to see for future Gemma versions? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhwr2p/next_gemma_versions_wishlist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhwr2p/next_gemma_versions_wishlist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhwr2p/next_gemma_versions_wishlist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T11:00:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji0fwh</id>
    <title>Qwq gets bad reviews because it's used wrong</title>
    <updated>2025-03-23T14:25:50+00:00</updated>
    <author>
      <name>/u/Far_Buyer_7281</name>
      <uri>https://old.reddit.com/user/Far_Buyer_7281</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title says it all, Loaded up with these parameters in ollama:&lt;/p&gt; &lt;p&gt;temperature 0.6&lt;br /&gt; top_p 0.95&lt;br /&gt; top_k 40&lt;br /&gt; repeat_penalty 1&lt;br /&gt; num_ctx 16,384 &lt;/p&gt; &lt;p&gt;Using a logic that does &lt;strong&gt;&lt;em&gt;not&lt;/em&gt;&lt;/strong&gt; feed the thinking proces into the context,&lt;br /&gt; Its the best local modal available right now, &lt;strong&gt;&lt;em&gt;I think I will die on this hill.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;But you can proof me wrong, tell me about a task or prompt another model can do better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far_Buyer_7281"&gt; /u/Far_Buyer_7281 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji0fwh/qwq_gets_bad_reviews_because_its_used_wrong/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji0fwh/qwq_gets_bad_reviews_because_its_used_wrong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji0fwh/qwq_gets_bad_reviews_because_its_used_wrong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T14:25:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jio4ev</id>
    <title>Is anybody here talking about this? Is it legit?</title>
    <updated>2025-03-24T11:10:37+00:00</updated>
    <author>
      <name>/u/Fitzroyah</name>
      <uri>https://old.reddit.com/user/Fitzroyah</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jio4ev/is_anybody_here_talking_about_this_is_it_legit/"&gt; &lt;img alt="Is anybody here talking about this? Is it legit?" src="https://preview.redd.it/isaf3uplemqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad264082ae379e37ddde4ad660c0b792b5321d1f" title="Is anybody here talking about this? Is it legit?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Disclaimer: I am not an engineer. I am a finance student, so most stuff here goes over my head, but I love seeing all you smart people develop for open source. Please correct me if I am missunderstanding anything.&lt;/p&gt; &lt;p&gt;The dev Taelin posted some days ago on X about him achieving extreme performance gains in program synthesis, mentioning above 70x speed increases.&lt;/p&gt; &lt;p&gt;IF this is true, and thats a big IF, doesnt that mean that AI coding will be 100x better pretty soon, if this could be implemented? These kinds of performance gains in math/reasoning capabilities would be huge, no?&lt;/p&gt; &lt;p&gt;Would appreciate if anybody who has braincells could take a look at this. Thanks for the help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fitzroyah"&gt; /u/Fitzroyah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/isaf3uplemqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jio4ev/is_anybody_here_talking_about_this_is_it_legit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jio4ev/is_anybody_here_talking_about_this_is_it_legit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T11:10:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji7oh6</id>
    <title>Q2 models are utterly useless. Q4 is the minimum quantization level that doesn't ruin the model (at least for MLX). Example with Mistral Small 24B at Q2 ‚Üì</title>
    <updated>2025-03-23T19:37:18+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji7oh6/q2_models_are_utterly_useless_q4_is_the_minimum/"&gt; &lt;img alt="Q2 models are utterly useless. Q4 is the minimum quantization level that doesn't ruin the model (at least for MLX). Example with Mistral Small 24B at Q2 ‚Üì" src="https://external-preview.redd.it/MGNib2hqNDBzaHFlMduLIsGKeKAIqIhMG9hPFCUJocOn0o4QCk_kODmlK36c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=606d07cb0b1252363afb0bafccfd8b42f1c705d6" title="Q2 models are utterly useless. Q4 is the minimum quantization level that doesn't ruin the model (at least for MLX). Example with Mistral Small 24B at Q2 ‚Üì" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ns6gqa40shqe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji7oh6/q2_models_are_utterly_useless_q4_is_the_minimum/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji7oh6/q2_models_are_utterly_useless_q4_is_the_minimum/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T19:37:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jiilot</id>
    <title>jukofyork/DeepSeek-R1-DRAFT-0.5B-GGUF ¬∑ Hugging Face</title>
    <updated>2025-03-24T04:24:30+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jiilot/jukofyorkdeepseekr1draft05bgguf_hugging_face/"&gt; &lt;img alt="jukofyork/DeepSeek-R1-DRAFT-0.5B-GGUF ¬∑ Hugging Face" src="https://external-preview.redd.it/s10KkW_R4qMdLDisGcqbfEBZS2Ye7-xQcXtqJrlwkdQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e1d39d630b0703ef51c2c41d065cb21b81b2ceb" title="jukofyork/DeepSeek-R1-DRAFT-0.5B-GGUF ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/jukofyork/DeepSeek-R1-DRAFT-0.5B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jiilot/jukofyorkdeepseekr1draft05bgguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jiilot/jukofyorkdeepseekr1draft05bgguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T04:24:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jie6oo</id>
    <title>Mistral small draft model</title>
    <updated>2025-03-24T00:27:09+00:00</updated>
    <author>
      <name>/u/frivolousfidget</name>
      <uri>https://old.reddit.com/user/frivolousfidget</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jie6oo/mistral_small_draft_model/"&gt; &lt;img alt="Mistral small draft model" src="https://external-preview.redd.it/fGQq4SyEYUq9b_wFHpxbLnoYjtgYQA70SDrLvYPMmkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c1cb6b0de0e00a235e6f56fd99854ec7f7e0180" title="Mistral small draft model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was browsing hugging face and found this model, made a 4bit mlx quants and it actually seems to work really well! 60.7% accepted tokens in a coding test!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frivolousfidget"&gt; /u/frivolousfidget &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/alamios/Mistral-Small-3.1-DRAFT-0.5B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jie6oo/mistral_small_draft_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jie6oo/mistral_small_draft_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T00:27:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jilv1g</id>
    <title>Experimental Support for GPU (Vulkan) in Distributed Llama</title>
    <updated>2025-03-24T08:23:10+00:00</updated>
    <author>
      <name>/u/b4rtaz</name>
      <uri>https://old.reddit.com/user/b4rtaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jilv1g/experimental_support_for_gpu_vulkan_in/"&gt; &lt;img alt="Experimental Support for GPU (Vulkan) in Distributed Llama" src="https://external-preview.redd.it/r8ksxQA0c7npTwIVOsKGIuivjE_pYt6Knh2HhI9gds4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2fee55aba182e87af585c45a0193224135c7fd30" title="Experimental Support for GPU (Vulkan) in Distributed Llama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/b4rtaz"&gt; /u/b4rtaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/b4rtaz/distributed-llama/releases/tag/v0.13.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jilv1g/experimental_support_for_gpu_vulkan_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jilv1g/experimental_support_for_gpu_vulkan_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T08:23:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jiewjn</id>
    <title>Possible Llama 4 prototypes on Chatbot Arena</title>
    <updated>2025-03-24T01:02:40+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There currently is an unusually large number of anonymous Llama/Meta models randomly appearing on &lt;a href="https://lmarena.ai/"&gt;Chatbot Arena&lt;/a&gt; Battle and it's fair to assume assuming that all or most of them are test versions of Llama 4. Most appear to have image input capabilities and some have a different feel than others. Anybody tested them?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;aurora&lt;/code&gt; -&amp;gt; Developed by MetaAI, image-enabled.&lt;/li&gt; &lt;li&gt;&lt;code&gt;ertiga&lt;/code&gt; -&amp;gt; Llama, developed by MetaAI, image-enabled.&lt;/li&gt; &lt;li&gt;&lt;code&gt;pinnacle&lt;/code&gt; -&amp;gt; Llama, developed by MetaAI, image-enabled.&lt;/li&gt; &lt;li&gt;&lt;code&gt;rhea&lt;/code&gt; -&amp;gt; Claims to be Llama 3, a friendly assistant created by Meta AI.&lt;/li&gt; &lt;li&gt;&lt;code&gt;solaris&lt;/code&gt; -&amp;gt; Llama model, image-enabled.&lt;/li&gt; &lt;li&gt;&lt;code&gt;sparrow&lt;/code&gt; -&amp;gt; LLaMA (Large Language Model Application), made by Meta&lt;/li&gt; &lt;li&gt;&lt;code&gt;spectra&lt;/code&gt; -&amp;gt; No name disclosed, but created by MetaAI. Image-enabled.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jiewjn/possible_llama_4_prototypes_on_chatbot_arena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jiewjn/possible_llama_4_prototypes_on_chatbot_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jiewjn/possible_llama_4_prototypes_on_chatbot_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T01:02:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji525h</id>
    <title>Since its release I've gone through all three phases of QwQ acceptance</title>
    <updated>2025-03-23T17:47:23+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji525h/since_its_release_ive_gone_through_all_three/"&gt; &lt;img alt="Since its release I've gone through all three phases of QwQ acceptance" src="https://preview.redd.it/8qv1c0xd8hqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61fdacdcc23fd66a96010f24d3c0bec601ad7eed" title="Since its release I've gone through all three phases of QwQ acceptance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qv1c0xd8hqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji525h/since_its_release_ive_gone_through_all_three/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji525h/since_its_release_ive_gone_through_all_three/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T17:47:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jijga9</id>
    <title>FanFic-Illustrator: A 3B Reasoning Model that Transforms Your Stories into Perfect Illustration Prompts</title>
    <updated>2025-03-24T05:20:23+00:00</updated>
    <author>
      <name>/u/dahara111</name>
      <uri>https://old.reddit.com/user/dahara111</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm excited to share FanFic-Illustrator, a specialized 3B reasoning model that bridges creative writing and AI image generation. This model analyzes your stories (original or fan fiction) and suggests optimal illustration scenes with perfectly crafted prompts for image generation models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes FanFic-Illustrator special:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Converts narrative text into optimized Danbooru tags for image generation (particularly tuned for [animagine-xl-4.0 opt](&lt;a href="https://huggingface.co/cagliostrolab/animagine-xl-4.0"&gt;https://huggingface.co/cagliostrolab/animagine-xl-4.0&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Shows its reasoning process so you understand why certain scenes and elements were chosen&lt;/li&gt; &lt;li&gt;Supports multilingual input (primarily Japanese, with good handling of English and Chinese)&lt;/li&gt; &lt;li&gt;Allows control over output category/tendency by specifying content categories and providing prioritized tag sets&lt;/li&gt; &lt;li&gt;Lightweight at just 3B parameters, based on Qwen2.5-3B-Instruct&lt;/li&gt; &lt;li&gt;Trained using Unsloth (GPTO) for efficient reinforcement learning.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;FanFic-Illustrator bridges an important gap in the AI creative pipeline - Danbooru tags (special terms like &amp;quot;1girl&amp;quot;, &amp;quot;solo&amp;quot;, &amp;quot;looking at viewer&amp;quot;, etc.) are widely used in open-weight image generation AI but can be challenging for newcomers to master. This model handles the complexity for you, converting natural language stories into effective prompt structures.&lt;/p&gt; &lt;p&gt;I expect this to create powerful synergies with creative writing LLMs, allowing for end-to-end story-to-illustration workflows.&lt;/p&gt; &lt;p&gt;model&lt;br /&gt; &lt;a href="https://huggingface.co/webbigdata/FanFic-Illustrator"&gt;https://huggingface.co/webbigdata/FanFic-Illustrator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gguf model with sample script&lt;br /&gt; &lt;a href="https://huggingface.co/webbigdata/FanFic-Illustrator_gguf"&gt;https://huggingface.co/webbigdata/FanFic-Illustrator_gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Free Colab sample&lt;br /&gt; &lt;a href="https://github.com/webbigdata-jp/python_sample/blob/main/FanFic_Illustrator_demo.ipynb"&gt;https://github.com/webbigdata-jp/python_sample/blob/main/FanFic_Illustrator_demo.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This first release is fully open-source under the Apache-2.0 license. I created it because I thought it would be technically interesting and fill a genuine need. While I'm primarily sharing it with the community to see how people use it and gather feedback for improvements, I'm also curious about potential applications people might discover. If you find innovative ways to use this in your projects or workflows, I'd love to hear about them!&lt;/p&gt; &lt;p&gt;During development, I discovered that creative text-to-illustration conversion tools like this lack established benchmarks, making objective evaluation particularly challenging. To accurately measure user experience and output quality, we may need to build entirely new evaluation criteria and testing methodologies. This challenge extends beyond technical issues, as the very definition of a 'good illustration suggestion' is inherently subjective. Community feedback will be invaluable in overcoming these hurdles and guiding future improvements.&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dahara111"&gt; /u/dahara111 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jijga9/fanficillustrator_a_3b_reasoning_model_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jijga9/fanficillustrator_a_3b_reasoning_model_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jijga9/fanficillustrator_a_3b_reasoning_model_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T05:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jiook5</id>
    <title>LLMs on a Steam Deck in Docker</title>
    <updated>2025-03-24T11:45:14+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jiook5/llms_on_a_steam_deck_in_docker/"&gt; &lt;img alt="LLMs on a Steam Deck in Docker" src="https://external-preview.redd.it/cHhsYnZ6bnVrbXFlMWk0BDd4-QDMiZx1kTrcST9W7IN4hLEz2IEbJGuZDar2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c9af1e809a2ed2bd77854137b66776def0b0e69" title="LLMs on a Steam Deck in Docker" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vdpn00oukmqe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jiook5/llms_on_a_steam_deck_in_docker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jiook5/llms_on_a_steam_deck_in_docker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T11:45:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1jip611</id>
    <title>Deepseek releases new V3 checkpoint (V3-0324)</title>
    <updated>2025-03-24T12:12:24+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jip611/deepseek_releases_new_v3_checkpoint_v30324/"&gt; &lt;img alt="Deepseek releases new V3 checkpoint (V3-0324)" src="https://external-preview.redd.it/L_MDAztp6gi49dQUv9vk2IeXw1OjSoBT_ooENnggvOg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94d961b0b48a76bd398ef8e9a387f6a5087e577d" title="Deepseek releases new V3 checkpoint (V3-0324)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-0324"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jip611/deepseek_releases_new_v3_checkpoint_v30324/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jip611/deepseek_releases_new_v3_checkpoint_v30324/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T12:12:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jioxj4</id>
    <title>Announcing TeapotLLM- an open-source ~800M model for hallucination-resistant Q&amp;A and document extraction, running entirely on CPU.</title>
    <updated>2025-03-24T11:59:40+00:00</updated>
    <author>
      <name>/u/zakerytclarke</name>
      <uri>https://old.reddit.com/user/zakerytclarke</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jioxj4/announcing_teapotllm_an_opensource_800m_model_for/"&gt; &lt;img alt="Announcing TeapotLLM- an open-source ~800M model for hallucination-resistant Q&amp;amp;A and document extraction, running entirely on CPU." src="https://external-preview.redd.it/3n5L3e5awOcQcEl4Nf4qqSOgiuX7eJUHzj2ZLltzndc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbd66c6cac4856e8128e418bbe7a6f3172256e6d" title="Announcing TeapotLLM- an open-source ~800M model for hallucination-resistant Q&amp;amp;A and document extraction, running entirely on CPU." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zakerytclarke"&gt; /u/zakerytclarke &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/teapotai/teapotllm#evaluation"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jioxj4/announcing_teapotllm_an_opensource_800m_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jioxj4/announcing_teapotllm_an_opensource_800m_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T11:59:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jinm8p</id>
    <title>I took your guys advice and made a React Reasoning UI model! It has a new reasoning structure and uses state, for component generation! TESSA-T1 (on Huggingface, from the creator of UIGEN)</title>
    <updated>2025-03-24T10:37:08+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jinm8p/i_took_your_guys_advice_and_made_a_react/"&gt; &lt;img alt="I took your guys advice and made a React Reasoning UI model! It has a new reasoning structure and uses state, for component generation! TESSA-T1 (on Huggingface, from the creator of UIGEN)" src="https://external-preview.redd.it/aHhhb2FrcWQ1bXFlMRJZCnWIqpqA-PWEKDHLhCPlKPFJgchtistQtNSdyEex.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22a548c77157d133a621e7c05b10b3d59179dee1" title="I took your guys advice and made a React Reasoning UI model! It has a new reasoning structure and uses state, for component generation! TESSA-T1 (on Huggingface, from the creator of UIGEN)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! Thanks to you guys a few weeks ago, my UIGEN models were trending on HF, with over 15k+ downloads. Because of that, I had a lot of very nice people reach out to me, offering free compute and resources. So I was able to make a better model!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/Tessa-T1-14B"&gt;Tessa-T1-14B&lt;/a&gt; is a reasoning model built on Qwen2.5 Coder. You can find all the size variants here: &lt;a href="https://huggingface.co/collections/Tesslate/tessa-t1-react-reasoning-model-67e0fb72ca23e04473885c0e"&gt;(32B, 14B, 7B, 3B)&lt;/a&gt;. It follows State, useref, useffect and a lot of react libraries like router. In the upcoming weeks I'll be releasing with shadcn. This model can be used in a multi-agent system to generate components or pages and make them work together.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The reasoning comes from a custom finetuned model but is more geared towards UI generation. You can tell this by how it backtracks and thinks about different design principles as the thought process. (Gestalt, etc)&lt;/li&gt; &lt;li&gt;The reasoning bounces between code and not code, and tries its best to check itself before generating.&lt;/li&gt; &lt;li&gt;For those who need it: &lt;a href="https://huggingface.co/Tesslate/Tessa-T1-14B-Q8_0-GGUF"&gt;GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;I had a lot of fun with this model. Just playing around with it and experimenting was really fun and unexpected.&lt;/li&gt; &lt;li&gt;Its very sensitive to temperature and chat template. I recommend the default parameters in LMSTUDIO.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Not just that, I'm also launching an update to &lt;a href="https://huggingface.co/collections/Tesslate/uigen-t15-reasoning-model-67e0fc3605add0af7c427c75"&gt;UIGEN-T1.5&lt;/a&gt;! Its a UI reasoning model that generates html css js tailwind, but I've upgraded the graphics a little bit. (You can check the model card for examples). This is part of my new model training pipeline (which will be available to the public once ready) where I can get data from unstructured sources and use it to create reasoning.&lt;/p&gt; &lt;p&gt;As always, I‚Äôd love to hear your feedback and see how you‚Äôre using it. Happy experimenting! (real question is can someone make a spinning balls demo on this).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/idvt1kqd5mqe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jinm8p/i_took_your_guys_advice_and_made_a_react/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jinm8p/i_took_your_guys_advice_and_made_a_react/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T10:37:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jiljpe</id>
    <title>MSI again teases GeForce RTX 5080 with 24GB memory</title>
    <updated>2025-03-24T07:57:54+00:00</updated>
    <author>
      <name>/u/regunakyle</name>
      <uri>https://old.reddit.com/user/regunakyle</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jiljpe/msi_again_teases_geforce_rtx_5080_with_24gb_memory/"&gt; &lt;img alt="MSI again teases GeForce RTX 5080 with 24GB memory" src="https://external-preview.redd.it/5BWhRWZ8DGaRwq5UYh8SMDqb1b2NpYJnFNdVg0eHXd4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=523d28119527a66cb2d6ee26c55fae8708a7c6bf" title="MSI again teases GeForce RTX 5080 with 24GB memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/regunakyle"&gt; /u/regunakyle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/msi-again-teases-geforce-rtx-5080-with-24gb-memory"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jiljpe/msi_again_teases_geforce_rtx_5080_with_24gb_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jiljpe/msi_again_teases_geforce_rtx_5080_with_24gb_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T07:57:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jifvny</id>
    <title>I made a diagram and explanation of how transformers work</title>
    <updated>2025-03-24T01:52:51+00:00</updated>
    <author>
      <name>/u/Cromulent123</name>
      <uri>https://old.reddit.com/user/Cromulent123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jifvny/i_made_a_diagram_and_explanation_of_how/"&gt; &lt;img alt="I made a diagram and explanation of how transformers work" src="https://b.thumbs.redditmedia.com/fprSteMLUKc-khfg243EAMYIKDnFkQHn_2xMr_C2w7o.jpg" title="I made a diagram and explanation of how transformers work" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cromulent123"&gt; /u/Cromulent123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jifvny"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jifvny/i_made_a_diagram_and_explanation_of_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jifvny/i_made_a_diagram_and_explanation_of_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T01:52:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jijyx2</id>
    <title>I don't understand what an LLM exactly is anymore</title>
    <updated>2025-03-24T05:57:34+00:00</updated>
    <author>
      <name>/u/surveypoodle</name>
      <uri>https://old.reddit.com/user/surveypoodle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About a year ago when LLMs were kind of new, the most intuitive explanation I found was that it is predicting the next word or token, appending that to the input and repeating, and that the prediction itself is based on pretrainedf weights which comes from large amount of texts.&lt;/p&gt; &lt;p&gt;Now I'm seeing audio generation, image generation, image classification, segmentation and all kinds of things also under LLMs so I'm not sure what exactly is going on. Did an LLM suddenly become more generalized?&lt;/p&gt; &lt;p&gt;As an example, [SpatialLM](&lt;a href="https://manycore-research.github.io/SpatialLM/"&gt;https://manycore-research.github.io/SpatialLM/&lt;/a&gt;) says it processes 3D point cloud data and understands 3D scenes. I don't understand what this has anything to do with language models.&lt;/p&gt; &lt;p&gt;Can someone explain?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/surveypoodle"&gt; /u/surveypoodle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jijyx2/i_dont_understand_what_an_llm_exactly_is_anymore/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jijyx2/i_dont_understand_what_an_llm_exactly_is_anymore/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jijyx2/i_dont_understand_what_an_llm_exactly_is_anymore/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T05:57:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jig5re</id>
    <title>Meta released a paper last month that seems to have gone under the radar. ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization. This is a better solution than BitNet and means if Meta wanted (for 10% extra compute) they could give us extremely performant 2-bit models.</title>
    <updated>2025-03-24T02:07:26+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jig5re/meta_released_a_paper_last_month_that_seems_to/"&gt; &lt;img alt="Meta released a paper last month that seems to have gone under the radar. ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization. This is a better solution than BitNet and means if Meta wanted (for 10% extra compute) they could give us extremely performant 2-bit models." src="https://b.thumbs.redditmedia.com/9hRP5bjRzlFUKNIF0QROoq6Vx5TN7YGbabV11IZeJ3M.jpg" title="Meta released a paper last month that seems to have gone under the radar. ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization. This is a better solution than BitNet and means if Meta wanted (for 10% extra compute) they could give us extremely performant 2-bit models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/pdf/2502.02631"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jig5re/meta_released_a_paper_last_month_that_seems_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jig5re/meta_released_a_paper_last_month_that_seems_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T02:07:26+00:00</published>
  </entry>
</feed>
