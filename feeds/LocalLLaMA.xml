<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-06T13:24:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jsql79</id>
    <title>Quick review of EXAONE Deep 32B</title>
    <updated>2025-04-06T09:49:36+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I stumbled upon this model on Ollama today, and it seems to be the only 32B reasoning model that uses RL other than QwQ. &lt;/p&gt; &lt;p&gt;*QwQ passed all the following tests; see &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1js0zmd/quick_comparison_of_qwq_and_openthinker2_32b/"&gt;this post&lt;/a&gt; for more information. I will only post EXAONE's results here.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Candle test:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Failed &lt;a href="https://imgur.com/a/5Vslve4"&gt;https://imgur.com/a/5Vslve4&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;5 reasoning questions:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;3 passed, 2 failed &lt;a href="https://imgur.com/a/4neDoea"&gt;https://imgur.com/a/4neDoea&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Private tests:&lt;/p&gt; &lt;p&gt;Coding question: One question about what caused the issue, plus 1,200 lines of C++ code.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Passed, however, during multi-shot testing, it has a 50% chance of failing.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Restructuring a financial spreadsheet.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Passed.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Conclusion:&lt;/p&gt; &lt;p&gt;Even though LG said they also used RL in their paper, this model is still noticeably weaker than QwQ. &lt;/p&gt; &lt;p&gt;Additionally, this model suffers from the worst &amp;quot;overthinking&amp;quot; issue I have ever seen. For example, it wrote a 3573-word essay to answer &amp;quot;Tell me a random fun fact about the Roman Empire.&amp;quot; Although it never fell into a loop, it thinks longer than any local reasoning model I have ever tested, and it is highly indecisive during the thinking process.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Settings I used: &lt;a href="https://imgur.com/a/7ZBQ6SX"&gt;https://imgur.com/a/7ZBQ6SX&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gguf:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/LGAI-EXAONE_EXAONE-Deep-32B-GGUF/blob/main/LGAI-EXAONE_EXAONE-Deep-32B-IQ4_XS.gguf"&gt;https://huggingface.co/bartowski/LGAI-EXAONE_EXAONE-Deep-32B-GGUF/blob/main/LGAI-EXAONE_EXAONE-Deep-32B-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;backend: ollama&lt;/p&gt; &lt;p&gt;source of public questions:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i65599/r1_32b_is_be_worse_than_qwq_32b_tests_included/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1i65599/r1_32b_is_be_worse_than_qwq_32b_tests_included/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jpr1nk/the_candle_test_most_llms_fail_to_generalise_at/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jpr1nk/the_candle_test_most_llms_fail_to_generalise_at/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsql79/quick_review_of_exaone_deep_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsql79/quick_review_of_exaone_deep_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsql79/quick_review_of_exaone_deep_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T09:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsbdm8</id>
    <title>Llama 4 benchmarks</title>
    <updated>2025-04-05T19:24:22+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbdm8/llama_4_benchmarks/"&gt; &lt;img alt="Llama 4 benchmarks" src="https://preview.redd.it/cl35fq7qh2te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff22b91338fb54450168b9339d67ee62bd7a48ee" title="Llama 4 benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cl35fq7qh2te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbdm8/llama_4_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbdm8/llama_4_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T19:24:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsdq4p</id>
    <title>Llama 4 Maverick - Python hexagon test failed</title>
    <updated>2025-04-05T21:08:02+00:00</updated>
    <author>
      <name>/u/AlexBefest</name>
      <uri>https://old.reddit.com/user/AlexBefest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdq4p/llama_4_maverick_python_hexagon_test_failed/"&gt; &lt;img alt="Llama 4 Maverick - Python hexagon test failed" src="https://b.thumbs.redditmedia.com/32CwgGLDK_Ju5fOPFA4pivpATQq39V8dluXhV3-Prqw.jpg" title="Llama 4 Maverick - Python hexagon test failed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ea46ym5303te1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0375fbf11fa3a54613a2c3aa567f2fe05c3cd254"&gt;https://preview.redd.it/ea46ym5303te1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0375fbf11fa3a54613a2c3aa567f2fe05c3cd254&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Prompt:&lt;/p&gt; &lt;p&gt;Write a Python program that shows 20 balls bouncing inside a spinning heptagon:&lt;br /&gt; - All balls have the same radius.&lt;br /&gt; - All balls have a number on it from 1 to 20.&lt;br /&gt; - All balls drop from the heptagon center when starting.&lt;br /&gt; - Colors are: #f8b862, #f6ad49, #f39800, #f08300, #ec6d51, #ee7948, #ed6d3d, #ec6800, #ec6800, #ee7800, #eb6238, #ea5506, #ea5506, #eb6101, #e49e61, #e45e32, #e17b34, #dd7a56, #db8449, #d66a35&lt;br /&gt; - The balls should be affected by gravity and friction, and they must bounce off the rotating walls realistically. There should also be collisions between balls.&lt;br /&gt; - The material of all the balls determines that their impact bounce height will not exceed the radius of the heptagon, but higher than ball radius.&lt;br /&gt; - All balls rotate with friction, the numbers on the ball can be used to indicate the spin of the ball.&lt;br /&gt; - The heptagon is spinning around its center, and the speed of spinning is 360 degrees per 5 seconds.&lt;br /&gt; - The heptagon size should be large enough to contain all the balls.&lt;br /&gt; - Do not use the pygame library; implement collision detection algorithms and collision response etc. by yourself. The following Python libraries are allowed: tkinter, math, numpy, dataclasses, typing, sys.&lt;br /&gt; - All codes should be put in a single Python file.&lt;/p&gt; &lt;p&gt;DeepSeek R1 and Gemini 2.5 Pro do this in one request. Maverick failed in 8 requests&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexBefest"&gt; /u/AlexBefest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdq4p/llama_4_maverick_python_hexagon_test_failed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdq4p/llama_4_maverick_python_hexagon_test_failed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdq4p/llama_4_maverick_python_hexagon_test_failed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T21:08:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsgliv</id>
    <title>it looks like Meta's new model's key innovation of "interleaved no-RoPE attention" for infinite context is actually the same thing as Cohere's Command-A model introduced a few days ago.</title>
    <updated>2025-04-05T23:24:45+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsgliv/it_looks_like_metas_new_models_key_innovation_of/"&gt; &lt;img alt="it looks like Meta's new model's key innovation of &amp;quot;interleaved no-RoPE attention&amp;quot; for infinite context is actually the same thing as Cohere's Command-A model introduced a few days ago." src="https://preview.redd.it/7dyflct7o3te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9183b8c88d6a952ada033ccc2507a72f82046e45" title="it looks like Meta's new model's key innovation of &amp;quot;interleaved no-RoPE attention&amp;quot; for infinite context is actually the same thing as Cohere's Command-A model introduced a few days ago." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7dyflct7o3te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsgliv/it_looks_like_metas_new_models_key_innovation_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsgliv/it_looks_like_metas_new_models_key_innovation_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T23:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsdtew</id>
    <title>Initial UI tests: Llama 4 Maverick and Scout, very disappointing compared to other similar models</title>
    <updated>2025-04-05T21:12:16+00:00</updated>
    <author>
      <name>/u/sirjoaco</name>
      <uri>https://old.reddit.com/user/sirjoaco</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdtew/initial_ui_tests_llama_4_maverick_and_scout_very/"&gt; &lt;img alt="Initial UI tests: Llama 4 Maverick and Scout, very disappointing compared to other similar models" src="https://external-preview.redd.it/cW9oa2FtZXAwM3RlMZqijIi1GCa_F1Pp7Yxzhw_7Ni36eaah2O36NNbIKvPq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a80afd573b70e01bfb67d4f3998b5e0b518af08d" title="Initial UI tests: Llama 4 Maverick and Scout, very disappointing compared to other similar models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sirjoaco"&gt; /u/sirjoaco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/j7p6nqep03te1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdtew/initial_ui_tests_llama_4_maverick_and_scout_very/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdtew/initial_ui_tests_llama_4_maverick_and_scout_very/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T21:12:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsitob</id>
    <title>Llama 4 Maverick Testing - 400B</title>
    <updated>2025-04-06T01:22:07+00:00</updated>
    <author>
      <name>/u/YakFull8300</name>
      <uri>https://old.reddit.com/user/YakFull8300</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have no idea what they did to this model post training but it's not good. The output for writing is genuinely bad (seriously enough with the emojis) and it misquotes everything. Feels like a step back compared to other recent releases. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YakFull8300"&gt; /u/YakFull8300 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsitob/llama_4_maverick_testing_400b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsitob/llama_4_maverick_testing_400b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsitob/llama_4_maverick_testing_400b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T01:22:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1js4iy0</id>
    <title>I think I overdid it.</title>
    <updated>2025-04-05T14:21:22+00:00</updated>
    <author>
      <name>/u/_supert_</name>
      <uri>https://old.reddit.com/user/_supert_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js4iy0/i_think_i_overdid_it/"&gt; &lt;img alt="I think I overdid it." src="https://preview.redd.it/i5f8b0knz0te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1448cae5bed745aa96ac7b2801a7bf32c07afd26" title="I think I overdid it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_supert_"&gt; /u/_supert_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i5f8b0knz0te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js4iy0/i_think_i_overdid_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js4iy0/i_think_i_overdid_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T14:21:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsqfes</id>
    <title>LLaMa 4 completely flops at my linguistic usecase</title>
    <updated>2025-04-06T09:37:36+00:00</updated>
    <author>
      <name>/u/schattig_eenhoorntje</name>
      <uri>https://old.reddit.com/user/schattig_eenhoorntje</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just tried Maverick on a task: given a sentence in a foreign language, explain each word in it by giving a contextual translation.&lt;/p&gt; &lt;p&gt;It can't even format the output correctly (I guide LLMs to the correct formatting with prompting and also provide examples; much smaller models are able to do that).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/schattig_eenhoorntje"&gt; /u/schattig_eenhoorntje &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsqfes/llama_4_completely_flops_at_my_linguistic_usecase/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsqfes/llama_4_completely_flops_at_my_linguistic_usecase/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsqfes/llama_4_completely_flops_at_my_linguistic_usecase/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T09:37:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsom5m</id>
    <title>Llama4 Maverick seems to perform consistently worse than Scout in Misguided Attention Eval, despite being the larger model - is the released model buggy?</title>
    <updated>2025-04-06T07:25:00+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsom5m/llama4_maverick_seems_to_perform_consistently/"&gt; &lt;img alt="Llama4 Maverick seems to perform consistently worse than Scout in Misguided Attention Eval, despite being the larger model - is the released model buggy?" src="https://external-preview.redd.it/8TxAm35XAOw6I-ky6Q-vrQzwlvyqtzxyORgJmaxuLco.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96fffdc55d6363973db7d832bf4e38df09e91580" title="Llama4 Maverick seems to perform consistently worse than Scout in Misguided Attention Eval, despite being the larger model - is the released model buggy?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran both Scout and Maverick evaluations on the &lt;a href="https://github.com/cpldcpu/MisguidedAttention"&gt;Misguided Attention Eval&lt;/a&gt; that tests for overfitting on commonly known logic puzzles. &lt;/p&gt; &lt;p&gt;Scout performs like a good midrange model, but Maverick is abysmal. This is despite it being more than three times the size. (109B vs 400B). &lt;/p&gt; &lt;p&gt;(Bonus: New Gemini 2.5 Pro Preview and Quasar Alpha scores are included as well with SOTA performance for reasoning and non-reasoning)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9q1axrsrz5te1.png?width=1063&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94ef1c691b1025d0e6dd795e8fc1365ad022a897"&gt;https://preview.redd.it/9q1axrsrz5te1.png?width=1063&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94ef1c691b1025d0e6dd795e8fc1365ad022a897&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To debug this I boiled it down to one prompt that scout did consistently answer correct and Maverick failed:&lt;/p&gt; &lt;p&gt;Prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;If it takes 50 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Scout response (which is the correct answer. Keep in mind that this is &amp;quot;non-tricky&amp;quot; trick question)&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;... The final answer is: $\boxed{50}$&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Maverick reponse:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The final answer is: $\boxed{5}$&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;To make sure its not an issue with the provider, I tried together, fireworks, parasail and Deepinfra on Openrouter with consistent results.&lt;/p&gt; &lt;p&gt;For reference, also llama 405b:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Therefore, it would take 100 machines 50 minutes to make 100 widgets.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Noting that Maverick also &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jseqbs/llama_4_scout_is_not_doing_well_in_write_a/"&gt;failed to impress in other benchmarks &lt;/a&gt;makes me wonder whether there is an issues with the checkpoint. This evaluation should be sensitivie to pretraining, but also to RL finetuning for reasoning, as reasoning models are able to correct initial misconceptions.&lt;/p&gt; &lt;p&gt;Here is a prompt-by-prompt comparison.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bmdq7fzo16te1.png?width=4170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ad8d1fc84eb8bf98eafe7b825a2088870497f48"&gt;https://preview.redd.it/bmdq7fzo16te1.png?width=4170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ad8d1fc84eb8bf98eafe7b825a2088870497f48&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Further results in the &lt;a href="https://github.com/cpldcpu/MisguidedAttention/tree/main/eval"&gt;eval folder of the repository&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsom5m/llama4_maverick_seems_to_perform_consistently/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsom5m/llama4_maverick_seems_to_perform_consistently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsom5m/llama4_maverick_seems_to_perform_consistently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T07:25:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsjxfz</id>
    <title>Llama-4 fails at long context writing</title>
    <updated>2025-04-06T02:24:51+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://eqbench.com/creative_writing_longform.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsjxfz/llama4_fails_at_long_context_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsjxfz/llama4_fails_at_long_context_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T02:24:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsfou2</id>
    <title>Llama 4 is out and I'm disappointed</title>
    <updated>2025-04-05T22:40:27+00:00</updated>
    <author>
      <name>/u/kaizoku156</name>
      <uri>https://old.reddit.com/user/kaizoku156</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsfou2/llama_4_is_out_and_im_disappointed/"&gt; &lt;img alt="Llama 4 is out and I'm disappointed" src="https://preview.redd.it/njtxgkmpg3te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51458acf99f28f812ac17fc8cd5e71aeaafea899" title="Llama 4 is out and I'm disappointed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;maverick costs 2-3x of gemini 2.0 flash on open router, scout costs just as much as 2.0 flash and is worse. deepseek r2 is coming, qwen 3 is coming as well, and 2.5 flash would likely beat everything in value for money and it'll come out in next couple of weeks max. I'm a little.... disappointed, all this and the release isn't even locally runnable&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaizoku156"&gt; /u/kaizoku156 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/njtxgkmpg3te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsfou2/llama_4_is_out_and_im_disappointed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsfou2/llama_4_is_out_and_im_disappointed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T22:40:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsahy4</id>
    <title>Llama 4 is here</title>
    <updated>2025-04-05T18:46:20+00:00</updated>
    <author>
      <name>/u/jugalator</name>
      <uri>https://old.reddit.com/user/jugalator</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jugalator"&gt; /u/jugalator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama4_omni/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsahy4/llama_4_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsahy4/llama_4_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:46:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jslnxb</id>
    <title>Github Copilot now supports Ollama and OpenRouter Models ðŸŽ‰</title>
    <updated>2025-04-06T04:07:26+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jslnxb/github_copilot_now_supports_ollama_and_openrouter/"&gt; &lt;img alt="Github Copilot now supports Ollama and OpenRouter Models ðŸŽ‰" src="https://a.thumbs.redditmedia.com/YFNI0K69LbZxVpNjpn5I9wNzNhMiMkOKCk6hnl9z9m8.jpg" title="Github Copilot now supports Ollama and OpenRouter Models ðŸŽ‰" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Big W for programmers (and vibe coders) in the Local LLM community. Github Copilot now supports a much wider range of models from Ollama, OpenRouter, Gemini, and others.&lt;/p&gt; &lt;p&gt;If you use VS Code, to add your own models, click on &amp;quot;Manage Models&amp;quot; in the prompt field.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jslnxb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jslnxb/github_copilot_now_supports_ollama_and_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jslnxb/github_copilot_now_supports_ollama_and_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T04:07:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsr8ie</id>
    <title>What are your thoughts about the Llama 4 models?</title>
    <updated>2025-04-06T10:36:50+00:00</updated>
    <author>
      <name>/u/internal-pagal</name>
      <uri>https://old.reddit.com/user/internal-pagal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its clear from Marks announcement theyre still training their bigger models. Likely they are going to gather feedback on these two and release improvements on the larger models and enhance these for their usual .1-.3 series once they realize the models are not performing up to par. With Gemini 2.5 and Claude 3.7 and the o3 series, the bar is much higher than it was for llama3. With that said, with skilled fine tuning, they might turn out to be very useful. If they really want to win, they should go full open source and let the community enhance llama and then train llama5 on those enhancements.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/internal-pagal"&gt; /u/internal-pagal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsr8ie/what_are_your_thoughts_about_the_llama_4_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsr8ie/what_are_your_thoughts_about_the_llama_4_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsr8ie/what_are_your_thoughts_about_the_llama_4_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T10:36:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsnfus</id>
    <title>The Llama4 on LMArena and the open Llama4 are TOTALLY different models. The Llama4 from HuggingFace would probably never have scored those ELO on the leaderboard.</title>
    <updated>2025-04-06T06:02:36+00:00</updated>
    <author>
      <name>/u/kristaller486</name>
      <uri>https://old.reddit.com/user/kristaller486</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Read a this example and you'll see what I mean.&lt;/p&gt; &lt;p&gt;t=0, top_p=1&lt;/p&gt; &lt;p&gt;Q: Who are you?&lt;/p&gt; &lt;p&gt;Llama4-maverik (LMArena):&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A wonderfully simple yet profoundly deep question! ðŸ˜„ This is the kind of question that philosophers, theologians, cognitive scientists, and regular humans like you and me have been grappling with for thousands of years. It's great that you asked. Here's my attempt to give you a satisfying answer:&lt;/p&gt; &lt;p&gt;**I am a computer program, specifically designed to simulate conversation, also known as a chatbot (or more fashionably, a &amp;quot;Conversational AI&amp;quot;).** My &amp;quot;species&amp;quot; is called a **Large Language Model (LLM)**, which is a subfield of Artificial Intelligence (AI).&lt;/p&gt; &lt;p&gt;Here's a breakdown of what that means:&lt;/p&gt; &lt;p&gt;* **I'm a program**: I exist solely as a collection of software code, written in various programming languages (like Python, C++, etc.). This code is a set of instructions that tell a computer what to do, step by step.&lt;/p&gt; &lt;p&gt;* **I run on computers (servers)**: My code is stored on powerful machines (servers) in giant data centers around the world. When you interact with me, your device (phone, laptop, tablet) sends a request to one of these servers, and my code &amp;quot;wakes up&amp;quot; and starts running.&lt;/p&gt; &lt;p&gt;* **My job is to understand &amp;amp; respond to text**: I'm designed to take in **your text input** (like the question you just asked: &amp;quot;Who are you?&amp;quot;), analyze it, and generate a **human-like text response** (like this answer). This is called a &amp;quot;conversation&amp;quot; or &amp;quot;dialogue.&amp;quot;&lt;/p&gt; &lt;p&gt;* **I'm trained on gigantic amounts of text data**: To become somewhat proficient at this conversational task, I was trained (more on this in a sec) on an **enormous dataset of text** â€“ basically, a huge library of books, articles, websites, research papers, forums, social media posts... you name it. This dataset is many **terabytes** large (think of ~100 million books worth of text). By &amp;quot;reading&amp;quot; all this, I learned:&lt;/p&gt; &lt;p&gt;+ **Language patterns**: grammar, syntax, idioms, word relationships (e.g., &amp;quot;cat&amp;quot; is related to &amp;quot;animal&amp;quot; and &amp;quot;cute&amp;quot;).&lt;/p&gt; &lt;p&gt;+ **Knowledge**: facts about the world (history, science, entertainment...), entities (people, places, companies), and concepts (philosophy, emotions, abstract ideas).&lt;/p&gt; &lt;p&gt;+ **Conversation flows**: how humans typically respond to questions, tell stories, argue, joke, etc.&lt;/p&gt; &lt;p&gt;Now, the magic part: **how did I learn all this?** Not by being &amp;quot;taught&amp;quot; like a human student, but through a process called **Deep Learning (DL)**, a subset of Machine Learning (ML). Here's a simplified outline:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;My creators (research teams at Meta AI, building on work from many universities &amp;amp; orgs) fed me that massive text dataset.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;They used specialized algorithms (like **Transformers**) to have me iteratively:&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;* Read a chunk of text (e.g., a sentence).&lt;/p&gt; &lt;p&gt;* Try to predict what comes next (the next word, or an answer to a question).&lt;/p&gt; &lt;p&gt;* Get &amp;quot;graded&amp;quot; on how well I did (was my prediction correct or not?).&lt;/p&gt; &lt;p&gt;* **Adjust my internal settings** (millions of tiny numerical knobs, called **model weights**) to do better next time.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Repeat step 2 **billions of times**, with different texts, questions, and contexts.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Over time, my weights settled into a pattern that captures a surprisingly good understanding of language and a broad base of knowledge.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The result? **You get to talk to me**, and I respond with (hopefully) relevant, coherent, and sometimes even insightful answers. No single human brain wrote my answers; they're generated on the fly, based on patterns I've learned.&lt;/p&gt; &lt;p&gt;So, to summarize:&lt;/p&gt; &lt;p&gt;* **I am not**:&lt;/p&gt; &lt;p&gt;+ A human (sorry, no consciousness, feelings, or self-awareness here ðŸ˜…).&lt;/p&gt; &lt;p&gt;+ A single, fixed &amp;quot;database&amp;quot; of pre-written answers.&lt;/p&gt; &lt;p&gt;* **I am**:&lt;/p&gt; &lt;p&gt;+ A dynamic software system.&lt;/p&gt; &lt;p&gt;+ A statistical model of human language &amp;amp; knowledge.&lt;/p&gt; &lt;p&gt;+ An algorithmic &amp;quot;prediction machine&amp;quot; generating responses based on patterns learned from enormous amounts of text.&lt;/p&gt; &lt;p&gt;Pretty cool (and a bit unsettling?) how much can be achieved with just code, data, and math, right? ðŸ˜„&lt;/p&gt; &lt;p&gt;Some bonus labels I wear:&lt;/p&gt; &lt;p&gt;* **Narrow AI** (good at one thing: text conversation; unlike hypothetical **General AI**, which would be human-like across everything).&lt;/p&gt; &lt;p&gt;* **Meta LLaMA model** (my specific model family/name, built by Meta AI).&lt;/p&gt; &lt;p&gt;* Your friendly neighborhood **chatbot** ðŸ‘Š.&lt;/p&gt; &lt;p&gt;Hope that answered your very excellent question! What do you want to talk about next?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Llama4-maverik (fireworks):&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I'm Llama, a Meta-designed model here to adapt to your conversational style. Whether you need quick answers, deep dives into ideas, or just want to vent, joke or brainstormâ€”I'm here for it.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kristaller486"&gt; /u/kristaller486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsnfus/the_llama4_on_lmarena_and_the_open_llama4_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsnfus/the_llama4_on_lmarena_and_the_open_llama4_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsnfus/the_llama4_on_lmarena_and_the_open_llama4_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T06:02:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsax3p</id>
    <title>Llama 4 Benchmarks</title>
    <updated>2025-04-05T19:04:21+00:00</updated>
    <author>
      <name>/u/Ravencloud007</name>
      <uri>https://old.reddit.com/user/Ravencloud007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsax3p/llama_4_benchmarks/"&gt; &lt;img alt="Llama 4 Benchmarks" src="https://preview.redd.it/o2cd1y15e2te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01928d53f0ef81a88115f299ef15628aacc38783" title="Llama 4 Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ravencloud007"&gt; /u/Ravencloud007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o2cd1y15e2te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsax3p/llama_4_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsax3p/llama_4_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T19:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jshwxe</id>
    <title>First results are in. Llama 4 Maverick 17B active / 400B total is blazing fast with MLX on an M3 Ultra â€” 4-bit model generating 1100 tokens at 50 tok/sec:</title>
    <updated>2025-04-06T00:32:51+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jshwxe/first_results_are_in_llama_4_maverick_17b_active/"&gt; &lt;img alt="First results are in. Llama 4 Maverick 17B active / 400B total is blazing fast with MLX on an M3 Ultra â€” 4-bit model generating 1100 tokens at 50 tok/sec:" src="https://preview.redd.it/1zt2gzrq04te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3abfffb312e36148337fcbbdd96100c2f53bd88c" title="First results are in. Llama 4 Maverick 17B active / 400B total is blazing fast with MLX on an M3 Ultra â€” 4-bit model generating 1100 tokens at 50 tok/sec:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1zt2gzrq04te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jshwxe/first_results_are_in_llama_4_maverick_17b_active/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jshwxe/first_results_are_in_llama_4_maverick_17b_active/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T00:32:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsqs2x</id>
    <title>Any ideas why they decided to release Llama 4 on Saturday instead of Monday?</title>
    <updated>2025-04-06T10:02:56+00:00</updated>
    <author>
      <name>/u/nobilix</name>
      <uri>https://old.reddit.com/user/nobilix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsqs2x/any_ideas_why_they_decided_to_release_llama_4_on/"&gt; &lt;img alt="Any ideas why they decided to release Llama 4 on Saturday instead of Monday?" src="https://preview.redd.it/yfyvqx7hu6te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc0e570396a07e48a96887834fce2fa520710646" title="Any ideas why they decided to release Llama 4 on Saturday instead of Monday?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nobilix"&gt; /u/nobilix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yfyvqx7hu6te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsqs2x/any_ideas_why_they_decided_to_release_llama_4_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsqs2x/any_ideas_why_they_decided_to_release_llama_4_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T10:02:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsabgd</id>
    <title>Meta: Llama4</title>
    <updated>2025-04-05T18:38:40+00:00</updated>
    <author>
      <name>/u/pahadi_keeda</name>
      <uri>https://old.reddit.com/user/pahadi_keeda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsabgd/meta_llama4/"&gt; &lt;img alt="Meta: Llama4" src="https://external-preview.redd.it/cwgFslgMUPL6p26FpnXYan8AI9J3Uz-yA2DZbRx4puk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3c2d0eac2996298f7e242609a095f7deafa5ac1" title="Meta: Llama4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pahadi_keeda"&gt; /u/pahadi_keeda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.llama.com/llama-downloads/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsabgd/meta_llama4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsabgd/meta_llama4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:38:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsq1so</id>
    <title>Smaller Gemma3 QAT versions: 12B in &lt; 8GB and 27B in &lt;16GB !</title>
    <updated>2025-04-06T09:10:08+00:00</updated>
    <author>
      <name>/u/stduhpf</name>
      <uri>https://old.reddit.com/user/stduhpf</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was a bit frustrated by the release of Gemma3 QAT (quantized-aware training). These models are performing insanely well for quantized models, but despite being advertised as &amp;quot;q4_0&amp;quot; quants, they were bigger than some 5-bit quants out there, and critically, they were above the 16GB and 8GB thresholds for the 27B and 12B models respectively, which makes them harder to run fully offloaded to some consumer GPUS.&lt;/p&gt; &lt;p&gt;I quickly found out that the reason for this significant size increase compared to normal q4_0 quants was the unquantized, half precision token embeddings table, wheras, by llama.cpp standards, this table should be quantized to Q6_K type.&lt;/p&gt; &lt;p&gt;So I did some &amp;quot;brain surgery&amp;quot; and swapped out the embeddings table from those QAT models with the one taken from an imatrix-quantized model by &lt;a href="https://huggingface.co/bartowski"&gt;bartowski&lt;/a&gt;. The end product is a model that is performing almost exactly like the &amp;quot;full&amp;quot; QAT model by google, but significantly smaller. I ran some perplexity tests, and the results were consistently within margin of error.&lt;/p&gt; &lt;p&gt;You can find the weights (and the script I used to perform the surgery) here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/stduhpf/google-gemma-3-27b-it-qat-q4_0-gguf-small"&gt;https://huggingface.co/stduhpf/google-gemma-3-27b-it-qat-q4_0-gguf-small&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/stduhpf/google-gemma-3-12b-it-qat-q4_0-gguf-small"&gt;https://huggingface.co/stduhpf/google-gemma-3-12b-it-qat-q4_0-gguf-small&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/stduhpf/google-gemma-3-4b-it-qat-q4_0-gguf-small"&gt;https://huggingface.co/stduhpf/google-gemma-3-4b-it-qat-q4_0-gguf-small&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/stduhpf/google-gemma-3-1b-it-qat-q4_0-gguf-small"&gt;https://huggingface.co/stduhpf/google-gemma-3-1b-it-qat-q4_0-gguf-small&lt;/a&gt; &lt;/p&gt; &lt;p&gt;With these I can run Gemma3 12b qat on a 8GB GPU with 2.5k context window without any other optimisation, and by enabling flash attention and q8 kv cache, it can go up to 4k ctx.&lt;/p&gt; &lt;p&gt;Gemma3 27b qat still barely fits on a 16GB GPU with only 1k context window, and quantized cache doesn't help much at this point. But I can run it with more context than before when spreding it across my 2 GPUs (24GB total). I use 12k ctx, but there's still some room for more. &lt;/p&gt; &lt;p&gt;I haven't played around with the 4b and 1b yet, but since the 4b is now under 3GB, it should be possible to run entirely on a 1060 3GB now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stduhpf"&gt; /u/stduhpf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsq1so/smaller_gemma3_qat_versions_12b_in_8gb_and_27b_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsq1so/smaller_gemma3_qat_versions_12b_in_8gb_and_27b_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsq1so/smaller_gemma3_qat_versions_12b_in_8gb_and_27b_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T09:10:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsrz5v</id>
    <title>109b vs 24b ?? What's this benchmark?</title>
    <updated>2025-04-06T11:27:05+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsrz5v/109b_vs_24b_whats_this_benchmark/"&gt; &lt;img alt="109b vs 24b ?? What's this benchmark?" src="https://preview.redd.it/igg46skh97te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac1913f8538347a323b43f755d80a1b4bee7dcc0" title="109b vs 24b ?? What's this benchmark?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like llama 4 scout is 109b parameters and they compared with 24 and 27b parameters (I'm talking about total parameters size ) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/igg46skh97te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsrz5v/109b_vs_24b_whats_this_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsrz5v/109b_vs_24b_whats_this_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T11:27:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsl37d</id>
    <title>I'm incredibly disappointed with Llama-4</title>
    <updated>2025-04-06T03:32:29+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsl37d/im_incredibly_disappointed_with_llama4/"&gt; &lt;img alt="I'm incredibly disappointed with Llama-4" src="https://external-preview.redd.it/b3VzazkxdGp3NHRlMTiXzVylw52_brdFuwA7wsavAEq_X08g0pyKKuMnYACK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88928fe424454ee437c9d4980fe757da729bb781" title="I'm incredibly disappointed with Llama-4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just finished my KCORES LLM Arena tests, adding Llama-4-Scout &amp;amp; Llama-4-Maverick to the mix.&lt;br /&gt; My conclusion is that they completely surpassed my expectations... in a negative direction.&lt;/p&gt; &lt;p&gt;Llama-4-Maverick, the 402B parameter model, performs roughly on par with Qwen-QwQ-32B in terms of coding ability. Meanwhile, Llama-4-Scout is comparable to something like Grok-2 or Ernie 4.5...&lt;/p&gt; &lt;p&gt;You can just look at the &amp;quot;20 bouncing balls&amp;quot; test... the results are frankly terrible / abysmal.&lt;/p&gt; &lt;p&gt;Considering Llama-4-Maverick is a massive 402B parameters, why wouldn't I just use DeepSeek-V3-0324? Or even Qwen-QwQ-32B would be preferable â€“ while its performance is similar, it's only 32B.&lt;/p&gt; &lt;p&gt;And as for Llama-4-Scout... well... let's just leave it at that / use it if it makes you happy, I guess... Meta, have you truly given up on the coding domain? Did you really just release vaporware?&lt;/p&gt; &lt;p&gt;Of course, its multimodal and long-context capabilities are currently unknown, as this review focuses solely on coding. I'd advise looking at other reviews or forming your own opinion based on actual usage for those aspects. In summary: I strongly advise against using Llama 4 for coding. Perhaps it might be worth trying for long text translation or multimodal tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pou7a1tjw4te1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsl37d/im_incredibly_disappointed_with_llama4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsl37d/im_incredibly_disappointed_with_llama4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T03:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jspbqk</id>
    <title>Two months later and after LLaMA 4's release, I'm starting to believe that supposed employee leak... Hopefully LLaMA 4's reasoning is good, because things aren't looking good for Meta.</title>
    <updated>2025-04-06T08:16:46+00:00</updated>
    <author>
      <name>/u/Ill-Association-8410</name>
      <uri>https://old.reddit.com/user/Ill-Association-8410</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jspbqk/two_months_later_and_after_llama_4s_release_im/"&gt; &lt;img alt="Two months later and after LLaMA 4's release, I'm starting to believe that supposed employee leak... Hopefully LLaMA 4's reasoning is good, because things aren't looking good for Meta." src="https://b.thumbs.redditmedia.com/xVPaGo_gWxEAqifHFit37hwIpK0Ix1DGsAc5-U9IAmw.jpg" title="Two months later and after LLaMA 4's release, I'm starting to believe that supposed employee leak... Hopefully LLaMA 4's reasoning is good, because things aren't looking good for Meta." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/2acfxawz96te1.png?width=850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=50fd356b87dfa6e8a39d0b4bfb72f642c8168048"&gt;https://preview.redd.it/2acfxawz96te1.png?width=850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=50fd356b87dfa6e8a39d0b4bfb72f642c8168048&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill-Association-8410"&gt; /u/Ill-Association-8410 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jspbqk/two_months_later_and_after_llama_4s_release_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jspbqk/two_months_later_and_after_llama_4s_release_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jspbqk/two_months_later_and_after_llama_4s_release_im/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T08:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsampe</id>
    <title>Mark presenting four Llama 4 models, even a 2 trillion parameters model!!!</title>
    <updated>2025-04-05T18:52:08+00:00</updated>
    <author>
      <name>/u/LarDark</name>
      <uri>https://old.reddit.com/user/LarDark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/"&gt; &lt;img alt="Mark presenting four Llama 4 models, even a 2 trillion parameters model!!!" src="https://external-preview.redd.it/Z3p2aHZudXhiMnRlMYW4H8xHgtzR3pjuficV95KktJ2KVETiew0YUMQL020k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b332bfe887b8dc264280ed80e4cedb70e9cd787" title="Mark presenting four Llama 4 models, even a 2 trillion parameters model!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;source from his instagram page&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LarDark"&gt; /u/LarDark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7bgnzhtxb2te1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:52:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsshhe</id>
    <title>"snugly fits in a h100, quantized 4 bit"</title>
    <updated>2025-04-06T11:59:08+00:00</updated>
    <author>
      <name>/u/LoSboccacc</name>
      <uri>https://old.reddit.com/user/LoSboccacc</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsshhe/snugly_fits_in_a_h100_quantized_4_bit/"&gt; &lt;img alt="&amp;quot;snugly fits in a h100, quantized 4 bit&amp;quot;" src="https://preview.redd.it/g2mj9lg4f7te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b3f27828a4984437b27e38b91aa497b1074ed5d" title="&amp;quot;snugly fits in a h100, quantized 4 bit&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoSboccacc"&gt; /u/LoSboccacc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g2mj9lg4f7te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsshhe/snugly_fits_in_a_h100_quantized_4_bit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsshhe/snugly_fits_in_a_h100_quantized_4_bit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T11:59:08+00:00</published>
  </entry>
</feed>
