<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-10T01:34:01+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kib12b</id>
    <title>Thoughts on this quantization method of MoE models?</title>
    <updated>2025-05-09T05:34:05+00:00</updated>
    <author>
      <name>/u/robiinn</name>
      <uri>https://old.reddit.com/user/robiinn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kib12b/thoughts_on_this_quantization_method_of_moe_models/"&gt; &lt;img alt="Thoughts on this quantization method of MoE models?" src="https://external-preview.redd.it/iwtMDqkXEXKi0BmJebYdDaWS7Vt-fNowwAcgrSOYkKA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8cff707c7f608172fe260dd8a1231cb649934cb" title="Thoughts on this quantization method of MoE models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;del&gt;Hi, this started with this thought I got after I saw the pruning strategy (&lt;a href="https://huggingface.co/kalomaze/Qwen3-16B-A3B/discussions/6#681770f3335c1c862165ddc0"&gt;https://huggingface.co/kalomaze/Qwen3-16B-A3B/discussions/6#681770f3335c1c862165ddc0&lt;/a&gt;) to prune based on how often the experts are activated. This technique creates an expert-wise quantization, currently based on their normalized (across the layer) activation rate.&lt;/del&gt; &lt;/p&gt; &lt;p&gt;&lt;del&gt;As a concept, I edited llama.cpp to change a bit of how it quantizes the models (hopefully correct). I will update the README file with new information when needed. What's great is that to run the model, you do not have to edit any files and works with existing code.&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;You can find it here:&lt;/del&gt;&lt;br /&gt; &lt;del&gt;&lt;a href="https://huggingface.co/RDson/Qwen3-30B-A3B-By-Expert-Quantization-GGUF"&gt;https://huggingface.co/RDson/Qwen3-30B-A3B-By-Expert-Quantization-GGUF&lt;/a&gt;&lt;/del&gt; &lt;del&gt;I will be uploading more quants to try out.&lt;/del&gt;&lt;/p&gt; &lt;p&gt;Edit: After further investigation into how the layers in tensors are stored, it seems like this is currently not possible. It would require a lot of rewriting the llama.cpp code which would need to be merged etc,. There was a misunderstanding of how I thought it works and how it actually works. Howerver, this is still an interesting topic to potentially explore further in the future, or with another library. I will not be exploring this any further, for now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robiinn"&gt; /u/robiinn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/RDson/Qwen3-30B-A3B-By-Expert-Quantization-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kib12b/thoughts_on_this_quantization_method_of_moe_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kib12b/thoughts_on_this_quantization_method_of_moe_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T05:34:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kij0g1</id>
    <title>Llama.cpp runner tool with multiconfig-swapping (llama-swap style) and LM Studio / Ollama backend proxying</title>
    <updated>2025-05-09T13:51:37+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kij0g1/llamacpp_runner_tool_with_multiconfigswapping/"&gt; &lt;img alt="Llama.cpp runner tool with multiconfig-swapping (llama-swap style) and LM Studio / Ollama backend proxying" src="https://external-preview.redd.it/f1Ev5p_LWGUZaamDb5GPtSwxWaSFPZhS9YDmQC3jEZc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b7784b006f5a1648aa5e93da3479442e8e047d4" title="Llama.cpp runner tool with multiconfig-swapping (llama-swap style) and LM Studio / Ollama backend proxying" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share a tool that I vibe-coded myself out of necessity. Don't know how many people would consider using it - it's a pretty specific niche tool and might be outdated sooner than later, since the Llama.cpp people are already working on a swap/admin backend on the server. However, I had a few use-cases that I couldn't get done with anything else.&lt;/p&gt; &lt;p&gt;So, if you are a:&lt;/p&gt; &lt;p&gt;* IntelliJ AI Assistant user frustrated that you can't run a raw llama.cpp backend model&lt;br /&gt; * GitHub Copilot user who doesn't like Ollama, but would want to serve local models&lt;br /&gt; * ik_llama.cpp fan that can't connect it to modern assistants because it doesn't accept the tool calls&lt;br /&gt; * General llama.cpp fan who wants to swap out a few custom configs&lt;br /&gt; * LM Studio fan who nevertheless would want to run their Qwen3 30B with &amp;quot;-ot (up_exps|down_exps)=CPU&amp;quot; and has no idea when it'll be supported&lt;/p&gt; &lt;p&gt;this is something for you. &lt;/p&gt; &lt;p&gt;I made a simple Python tool with a very rudimentary PySide6 frontend that runs two proxies:&lt;br /&gt; * one proxy on port 11434 translates requests from Ollama format, forwards them to the Llama.cpp server, then translates the response back from Ollama format into OpenAI-compatible and sends it back&lt;br /&gt; * the other proxy on port 1234 serves the simple OpenAI-compatible proxy, but with a twist - it exposes LM Studio specific endpoints, especially the one for listing available models&lt;br /&gt; Both endpoints support streaming, both endpoints will load the necessary config when asked for a specific model.&lt;/p&gt; &lt;p&gt;This allows your local llama.cpp instance to effectively emulate both Ollama and LMStudio for external tools that integrate with those specific solutions and no others (*cough* IntelliJ AI Assistant *cough* GitHub Copilot *cough*). &lt;/p&gt; &lt;p&gt;I vibe-coded this thing with my Aider/Roo and my free Gemini queries, so don't expect the code to be very beatiful - but as far as I've tested it locally (both Linux and Windows) it gets the job done. Running it is very simple, just install Python, then run it in a venv (detailed instructions and sample config file in the repo README).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/pwilkin/llama-runner"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kij0g1/llamacpp_runner_tool_with_multiconfigswapping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kij0g1/llamacpp_runner_tool_with_multiconfigswapping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T13:51:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kivks1</id>
    <title>how to fine-tune LLM without using a self-hosted machine?</title>
    <updated>2025-05-09T22:47:57+00:00</updated>
    <author>
      <name>/u/jamesftf</name>
      <uri>https://old.reddit.com/user/jamesftf</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on your personal experience, what's the best way to fine-tune an LLM on specific industry knowledge and writing style/tone? &lt;/p&gt; &lt;p&gt;Can I fine-tune without my self-hosted machine? Can I do this on the cloud by using OpenAI or Anthropic, for example?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jamesftf"&gt; /u/jamesftf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kivks1/how_to_finetune_llm_without_using_a_selfhosted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kivks1/how_to_finetune_llm_without_using_a_selfhosted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kivks1/how_to_finetune_llm_without_using_a_selfhosted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T22:47:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1khwxal</id>
    <title>The Great Quant Wars of 2025</title>
    <updated>2025-05-08T18:09:08+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khwxal/the_great_quant_wars_of_2025/"&gt; &lt;img alt="The Great Quant Wars of 2025" src="https://external-preview.redd.it/pLzmanaXtc-d2wPrXsO5AlWp5Ge-yDl-Jn3J0rfCGf0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3dbb86fedceccc3df6be89669facd3c1e394bb7d" title="The Great Quant Wars of 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;The Great Quant Wars of 2025&lt;/h1&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;All things leave behind them the Obscurity... and go forward to embrace the Brightness...&amp;quot; â€” Dao De Jing #42&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;tl;dr;&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Q: Who provides the best GGUFs now?&lt;/li&gt; &lt;li&gt;A: They're all pretty good.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;Skip down if you just want graphs and numbers comparing various Qwen3-30B-A3B GGUF quants.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Background&lt;/h1&gt; &lt;p&gt;It's been well over a year since &lt;strong&gt;TheBloke&lt;/strong&gt; uploaded his last quant to huggingface. The LLM landscape has changed markedly since then with many new models being released monthly, new inference engines targeting specific hardware optimizations, and ongoing evolution of quantization algorithims. Our community continues to grow and diversify at an amazing rate.&lt;/p&gt; &lt;p&gt;Fortunately, many folks and organizations have kindly stepped-up to keep the quants cooking so we can all find an LLM sized just right to fit on our home rigs. Amongst them &lt;strong&gt;bartowski&lt;/strong&gt;, and &lt;strong&gt;unsloth&lt;/strong&gt; (Daniel and Michael's start-up company), have become the new &amp;quot;household names&amp;quot; for providing a variety of GGUF quantizations for popular model releases and even all those wild creative fine-tunes! (There are many more including team &lt;strong&gt;mradermacher&lt;/strong&gt; and too many to list everyone, sorry!)&lt;/p&gt; &lt;p&gt;Until recently most GGUF style quants' recipes were &amp;quot;static&amp;quot; meaning that all the tensors and layers were quantized the same e.g. &lt;code&gt;Q8_0&lt;/code&gt; or with consistent patterns defined in llama.cpp's code. So all quants of a given size were mostly the same regardless of who cooked and uploaded it to huggingface.&lt;/p&gt; &lt;p&gt;Things began to change over a year ago with major advancements like importance matrix quantizations by &lt;a href="https://github.com/ggml-org/llama.cpp/pull/4861"&gt;ikawrakow in llama.cpp PR#4861&lt;/a&gt; as well as new quant types (like the perennial favorite &lt;a href="https://github.com/ggml-org/llama.cpp/pull/5747"&gt;IQ4_XS&lt;/a&gt;) which have become the mainstay for users of llama.cpp, ollama, koboldcpp, lmstudio, etc. The entire GGUF ecosystem owes a big thanks to not just to &lt;code&gt;ggerganov&lt;/code&gt; but also &lt;code&gt;ikawrakow&lt;/code&gt; (as well as the many more contributors).&lt;/p&gt; &lt;p&gt;Very recently &lt;strong&gt;unsloth&lt;/strong&gt; introduced a few changes to their quantization methodology that combine different imatrix calibration texts and context lengths along with making some tensors/layers different sizes than the regular llama.cpp code (they had a &lt;a href="https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/discussions/1#68160cf38812c2d5767f6dbd"&gt;public fork with their branch&lt;/a&gt;, but have to update and re-push due to upstream changes). They have named this change in standard methodology &lt;em&gt;Unsloth Dynamic 2.0 GGUFs&lt;/em&gt; as part of their start-up company's marketing strategy.&lt;/p&gt; &lt;p&gt;Around the same time &lt;strong&gt;bartowski&lt;/strong&gt; has been experimenting with different imatrix calibration texts and opened a PR to llama.cpp modifying the default tensor/layer quantization recipes. I myself began experimenting with custom &amp;quot;dynamic&amp;quot; quantization recipes using ikawrakow's latest SOTA quants like &lt;code&gt;iq4_k&lt;/code&gt; which to-date only work on his &lt;a href="https://github.com/ikawrakow/ik_llama.cpp/"&gt;ik_llama.cpp&lt;/a&gt; fork.&lt;/p&gt; &lt;p&gt;While this is great news for all GGUF enjoyers, the friendly competition and additional options have led to some confusion and I dare say some &amp;quot;tribalism&amp;quot;. &lt;em&gt;(If part of your identity as a person depends on downloading quants from only one source, I suggest you google: &amp;quot;Nan Yar?&amp;quot;)&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;So how can you, dear reader, decide which is the best quant of a given model for you to download? &lt;strong&gt;unsloth&lt;/strong&gt; already did a &lt;a href="https://unsloth.ai/blog/dynamic-v2"&gt;great blog post&lt;/a&gt; discussing their own benchmarks and metrics. Open a tab to check out &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kgo7d4/qwen330ba3b_ggufs_mmlupro_benchmark_comparison_q6/"&gt;u/AaronFeng47's many other benchmarks&lt;/a&gt;. And finally, &lt;em&gt;this post&lt;/em&gt; contains &lt;em&gt;even more&lt;/em&gt; metrics and benchmarks. The best answer I have is &lt;em&gt;&amp;quot;Nullius in verba&lt;/em&gt;, (Latin for &amp;quot;take nobody's word for it&amp;quot;) â€” even &lt;em&gt;my&lt;/em&gt; word!&lt;/p&gt; &lt;p&gt;Unfortunately, this means there is no one-size-fits-all rule, &amp;quot;X&amp;quot; is &lt;em&gt;not always&lt;/em&gt; better than &amp;quot;Y&amp;quot;, and if you want to min-max-optimize your LLM for your specific use case on your specific hardware you probably will have to experiment and &lt;em&gt;think critically&lt;/em&gt;. If you don't care too much, then pick the any of biggest quants that fit on your rig for the desired context length and you'll be fine because: &lt;em&gt;they're all pretty good&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;And with that, let's dive into the Qwen3-30B-A3B benchmarks below!&lt;/p&gt; &lt;h1&gt;Quick Thanks&lt;/h1&gt; &lt;p&gt;Shout out to Wendell and the &lt;strong&gt;Level1Techs&lt;/strong&gt; crew, the &lt;a href="https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826"&gt;L1T Forums&lt;/a&gt;, and the &lt;a href="https://www.youtube.com/@Level1Techs"&gt;L1T YouTube Channel&lt;/a&gt;! &lt;strong&gt;BIG thanks&lt;/strong&gt; for providing &lt;strong&gt;BIG hardware&lt;/strong&gt; expertise and access to run these experiments and make great quants available to the community!!!&lt;/p&gt; &lt;h1&gt;Appendix&lt;/h1&gt; &lt;p&gt;&lt;a href="https://gist.github.com/ubergarm/0f9663fd56fc181a00ec9f634635eb38"&gt;Check out this gist&lt;/a&gt; for supporting materials including methodology, raw data, benchmark definitions, and further references.&lt;/p&gt; &lt;h1&gt;Graphs&lt;/h1&gt; &lt;p&gt;ðŸ‘ˆ Qwen3-30B-A3B Benchmark Suite Graphs&lt;/p&gt; &lt;p&gt;Note &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; mode was &lt;em&gt;disabled&lt;/em&gt; for these tests to speed up benchmarking.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nnwulswpllze1.png?width=2136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=20248cbdc258e26fbf6316347dba9b3bb56dec6e"&gt;https://preview.redd.it/nnwulswpllze1.png?width=2136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=20248cbdc258e26fbf6316347dba9b3bb56dec6e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9d2ljgorllze1.png?width=1878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9121b64573866009c5b54249f108e4ac9cf46d33"&gt;https://preview.redd.it/9d2ljgorllze1.png?width=1878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9121b64573866009c5b54249f108e4ac9cf46d33&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ðŸ‘ˆ Qwen3-30B-A3B Perplexity and KLD Graphs&lt;/p&gt; &lt;p&gt;Using the &lt;code&gt;BF16&lt;/code&gt; as baseline for KLD stats. Also note the perplexity was lowest (&amp;quot;best&amp;quot;) for models other than the &lt;code&gt;bf16&lt;/code&gt; which is not typically the case unless there was possibly some QAT going on. As such, the chart is relative to the lowest perplexity score: &lt;code&gt;PPL/min(PPL)-1&lt;/code&gt; plus a small eps for scaling.&lt;/p&gt; &lt;h1&gt;Perplexity&lt;/h1&gt; &lt;p&gt;&lt;code&gt;wiki.test.raw&lt;/code&gt; (lower is &amp;quot;better&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/do90cb6ullze1.png?width=1101&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7e82d94611e285d97f63242ac626ff8d04df643a"&gt;https://preview.redd.it/do90cb6ullze1.png?width=1101&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7e82d94611e285d97f63242ac626ff8d04df643a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ubergarm-kdl-test-corpus.txt&lt;/code&gt; (lower is &amp;quot;better&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9h35expvllze1.png?width=1101&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0aad74e7cf28898c7bcab2dda0fe52e49d8b59d4"&gt;https://preview.redd.it/9h35expvllze1.png?width=1101&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0aad74e7cf28898c7bcab2dda0fe52e49d8b59d4&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;KLD Stats&lt;/h1&gt; &lt;p&gt;(lower is &amp;quot;better&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l2h30sjxllze1.png?width=1005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d348f191c72184474d25ee2b58c2d36ad8dc2743"&gt;https://preview.redd.it/l2h30sjxllze1.png?width=1005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d348f191c72184474d25ee2b58c2d36ad8dc2743&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Î”p Stats&lt;/h1&gt; &lt;p&gt;(lower is &amp;quot;better&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5nc43lfzllze1.png?width=1005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=045e9a78337f640484b3b912af8bcdb7a2f4cf7c"&gt;https://preview.redd.it/5nc43lfzllze1.png?width=1005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=045e9a78337f640484b3b912af8bcdb7a2f4cf7c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ðŸ‘ˆ Qwen3-235B-A22B Perplexity and KLD Graphs&lt;/p&gt; &lt;p&gt;Not as many data points here but just for comparison. Keep in mind the &lt;code&gt;Q8_0&lt;/code&gt; was the baseline for KLD stats given I couldn't easily run the full &lt;code&gt;BF16&lt;/code&gt;.&lt;/p&gt; &lt;h1&gt;Perplexity&lt;/h1&gt; &lt;p&gt;&lt;code&gt;wiki.test.raw&lt;/code&gt; (lower is &amp;quot;better&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dglqaj81mlze1.png?width=1034&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1acda8b080355256e19266ca6e5fe4441fdcac4d"&gt;https://preview.redd.it/dglqaj81mlze1.png?width=1034&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1acda8b080355256e19266ca6e5fe4441fdcac4d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ubergarm-kdl-test-corpus.txt&lt;/code&gt; (lower is &amp;quot;better&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s105wls3mlze1.png?width=1111&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=495f9563157ff5378771eb09fd4c0d730fe584b1"&gt;https://preview.redd.it/s105wls3mlze1.png?width=1111&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=495f9563157ff5378771eb09fd4c0d730fe584b1&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;KLD Stats&lt;/h1&gt; &lt;p&gt;(lower is &amp;quot;better&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i82q3f56mlze1.png?width=965&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b5cf9e555ad98a33a01f0d03e5bd3736491cc82"&gt;https://preview.redd.it/i82q3f56mlze1.png?width=965&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b5cf9e555ad98a33a01f0d03e5bd3736491cc82&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Î”p Stats&lt;/h1&gt; &lt;p&gt;(lower is &amp;quot;better&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/quuvxb28mlze1.png?width=948&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ee54d044e9b7aa13de2d06dbd92d18d8f2f46b7"&gt;https://preview.redd.it/quuvxb28mlze1.png?width=948&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ee54d044e9b7aa13de2d06dbd92d18d8f2f46b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ðŸ‘ˆ Qwen3-30B-A3B Speed llama-sweep-bench Graphs&lt;/p&gt; &lt;h1&gt;Inferencing Speed&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp/pull/225"&gt;llama-sweep-bench&lt;/a&gt; is a great speed benchmarking tool to see how performance varies with longer context length (kv cache).&lt;/p&gt; &lt;p&gt;&lt;em&gt;llama.cpp&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ugld2hpamlze1.png?width=3404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b5e4d656438b0fe0157376eb3226ba59c9783c48"&gt;https://preview.redd.it/ugld2hpamlze1.png?width=3404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b5e4d656438b0fe0157376eb3226ba59c9783c48&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;ik_llama.cpp&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;NOTE: Keep in mind ik's fork is faster than mainline llama.cpp for many architectures and configurations especially only-CPU, hybrid-CPU+GPU, and DeepSeek MLA cases.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l32ulaadmlze1.png?width=3404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b7e2cd45efce9855cb93ddb4eaa999d678763e7"&gt;https://preview.redd.it/l32ulaadmlze1.png?width=3404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b7e2cd45efce9855cb93ddb4eaa999d678763e7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khwxal/the_great_quant_wars_of_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khwxal/the_great_quant_wars_of_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khwxal/the_great_quant_wars_of_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T18:09:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kien12</id>
    <title>How to improve RAG?</title>
    <updated>2025-05-09T09:52:24+00:00</updated>
    <author>
      <name>/u/AsleepCommittee7301</name>
      <uri>https://old.reddit.com/user/AsleepCommittee7301</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im finishing a degree in Computer Science and currently im an intern (at least in spain is part of the degree)&lt;/p&gt; &lt;p&gt;I have a proyect that is about retreiving information from large documents (some of them PDFs from 30 to 120 pages), so surely context wont let me upload it all (and if it could, it would be expensive from a resource perspective)&lt;/p&gt; &lt;p&gt;I &amp;quot;allways&amp;quot; work with documents on a similar format, but the content may change a lot from document to document, right now i have used the PDF index to make Dynamic chunks (that also have parent-son relationships to adjust scores example: if a parent section 1.0 is important, probably 1.1 will be, or vice versa)&lt;/p&gt; &lt;p&gt;The chunking works pretty well, but the problem is when i retrieve them, right now im using GraphRag (so i can take more advantage of the relationships) and giving the node score with part cosine similarity and part BM25, also semantic relationships betweem node edges)&lt;/p&gt; &lt;p&gt;I also have an agent to make the query a more rag apropiate one (removing useless information on searches)&lt;/p&gt; &lt;p&gt;But it still only &amp;quot;Kinda&amp;quot; works, i thought on a reranker for the top-k nodes or something like that, but since im just starting and this proyect is somewhat my thesis id gladly take some advide from some more experienced people :D.&lt;/p&gt; &lt;p&gt;Ty all in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AsleepCommittee7301"&gt; /u/AsleepCommittee7301 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kien12/how_to_improve_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kien12/how_to_improve_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kien12/how_to_improve_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T09:52:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kivcl0</id>
    <title>Does anyone actually use Browser Use in production?</title>
    <updated>2025-05-09T22:37:04+00:00</updated>
    <author>
      <name>/u/SameBuddy8941</name>
      <uri>https://old.reddit.com/user/SameBuddy8941</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title. EDIT: (and other than Manus) Tried using the hosted/cloud version and it took &lt;strong&gt;5 minutes&lt;/strong&gt; to generate 9 successive failure steps (with 0 progress from steps 1 to 9) for a fairly simple use case (filling out an online form). Anthropic Computer Use on the other hand actually works for this use case every time, succeeding in &lt;strong&gt;2-3 minutes&lt;/strong&gt; for comparable cost.&lt;/p&gt; &lt;p&gt;Maybe some people are getting good performance by forking and adapting, but I'm wondering why this repo has so many stars and if I'm doing something wrong trying to use the OOTB version&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SameBuddy8941"&gt; /u/SameBuddy8941 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kivcl0/does_anyone_actually_use_browser_use_in_production/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kivcl0/does_anyone_actually_use_browser_use_in_production/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kivcl0/does_anyone_actually_use_browser_use_in_production/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T22:37:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kima1i</id>
    <title>Qwen introduced new web dev tool on app and website for frontend one line prompt to make web pages I tried and absolute insane</title>
    <updated>2025-05-09T16:08:55+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kima1i/qwen_introduced_new_web_dev_tool_on_app_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kima1i/qwen_introduced_new_web_dev_tool_on_app_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kima1i/qwen_introduced_new_web_dev_tool_on_app_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T16:08:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kim8re</id>
    <title>Considering a 9950X for a CPU only Qwen 3 30B A3B..</title>
    <updated>2025-05-09T16:07:26+00:00</updated>
    <author>
      <name>/u/My_Unbiased_Opinion</name>
      <uri>https://old.reddit.com/user/My_Unbiased_Opinion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Considering upgrading my general use server. It's not just an LLM rig, but hosts heavily modded Minecraft and other games servers. I'm considering throwing in a 9950X on it. &lt;/p&gt; &lt;p&gt;What tokens per second and prompt processing speed would I expect with a 32K context length? 128K context? Considering DDR5 6000 or 6200MT/s.&lt;/p&gt; &lt;p&gt;I tried looking online and couldn't really find good data for the 9950X on faster models like 30B A3B. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/My_Unbiased_Opinion"&gt; /u/My_Unbiased_Opinion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kim8re/considering_a_9950x_for_a_cpu_only_qwen_3_30b_a3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kim8re/considering_a_9950x_for_a_cpu_only_qwen_3_30b_a3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kim8re/considering_a_9950x_for_a_cpu_only_qwen_3_30b_a3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T16:07:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kivbm1</id>
    <title>Are general/shared Rag's a thing</title>
    <updated>2025-05-09T22:35:44+00:00</updated>
    <author>
      <name>/u/nocgeek</name>
      <uri>https://old.reddit.com/user/nocgeek</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im in the process of training my first rag based on some documentation it made me wonder why I had not seen specialized rags for example A linux , Docker or Windows Powershell that you could connect to for specific questions in that domain? Do these exist and i have just not seen them or is it a training data issue or something else that i am missing? I have seen this in image generators via Lora's. i would love to read peoples thoughts on this even if it is something i am totally wrong about. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nocgeek"&gt; /u/nocgeek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kivbm1/are_generalshared_rags_a_thing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kivbm1/are_generalshared_rags_a_thing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kivbm1/are_generalshared_rags_a_thing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T22:35:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kiuexc</id>
    <title>Offloading a 4B LLM to APU, only uses 50% of one CPU core. 21 t/s using Vulkan</title>
    <updated>2025-05-09T21:53:45+00:00</updated>
    <author>
      <name>/u/magnus-m</name>
      <uri>https://old.reddit.com/user/magnus-m</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiuexc/offloading_a_4b_llm_to_apu_only_uses_50_of_one/"&gt; &lt;img alt="Offloading a 4B LLM to APU, only uses 50% of one CPU core. 21 t/s using Vulkan" src="https://external-preview.redd.it/fdN9WJ_uOoSe2kS4aVe8FKGbL-hWzkFf2z6KEoDuAL0.png?width=140&amp;amp;height=45&amp;amp;crop=140:45,smart&amp;amp;auto=webp&amp;amp;s=11aa966ec508af0a4f847488166485a34b1b4962" title="Offloading a 4B LLM to APU, only uses 50% of one CPU core. 21 t/s using Vulkan" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you don't use the iGPU of your CPU, you can run a small LLM on it almost without taking a toll of the CPU. &lt;/p&gt; &lt;p&gt;Running llama.cpp server on a AMD Ryzen with a APU only uses 50 % utilization of one CPU when offloading all layers to the iGPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;: Gemma 3 4B Q4 fully offloaded to the iGPU.&lt;br /&gt; &lt;strong&gt;System&lt;/strong&gt;: AMD 7 8845HS, DDR5 5600, llama.cpp with Vulkan backend. Ubuntu.&lt;br /&gt; &lt;strong&gt;Performance:&lt;/strong&gt; 21 tokens/sec sustained throughput&lt;br /&gt; &lt;strong&gt;CPU Usage:&lt;/strong&gt; Just ~50% of one core&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x4n0p7n7vtze1.png?width=1098&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=642e18d7eeefeb932e2e26b39d600d4bfcbfd2e3"&gt;https://preview.redd.it/x4n0p7n7vtze1.png?width=1098&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=642e18d7eeefeb932e2e26b39d600d4bfcbfd2e3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feels like a waste not to utilize the iGPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/magnus-m"&gt; /u/magnus-m &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiuexc/offloading_a_4b_llm_to_apu_only_uses_50_of_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiuexc/offloading_a_4b_llm_to_apu_only_uses_50_of_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kiuexc/offloading_a_4b_llm_to_apu_only_uses_50_of_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T21:53:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki831c</id>
    <title>User asked computer controlling AI for "a ball bouncing inside the screen", the AI showed them porn...</title>
    <updated>2025-05-09T02:39:12+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I guess, the AI delivered... ðŸ¤£&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/smolagents/computer-agent/discussions/6"&gt;https://huggingface.co/spaces/smolagents/computer-agent/discussions/6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki831c/user_asked_computer_controlling_ai_for_a_ball/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki831c/user_asked_computer_controlling_ai_for_a_ball/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ki831c/user_asked_computer_controlling_ai_for_a_ball/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T02:39:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kimo3j</id>
    <title>Hardware to run 32B models at great speeds</title>
    <updated>2025-05-09T16:24:55+00:00</updated>
    <author>
      <name>/u/Saayaminator</name>
      <uri>https://old.reddit.com/user/Saayaminator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently have a PC with a 7800x3d, 32GB of DDR5-6000 and an RTX3090. I am interested in running 32B models with at least 32k context loaded and great speeds. To that end, I thought about getting a second RTX3090 because you can find some acceptable prices for it. Would that be the best option? Any alternatives at a &amp;lt;1000$ budget?&lt;/p&gt; &lt;p&gt;Ideally I would also like to be able to run the larger MoE models at acceptable speeds (decent prompt processing/tft, tg like 15+ t/s). But for that I would probably need a Linux server. Ideally with a good upgrade path. Then I would have a higher budget, like 5k. Can you have decent power efficiency for such a build? I am only interested in inference &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Saayaminator"&gt; /u/Saayaminator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kimo3j/hardware_to_run_32b_models_at_great_speeds/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kimo3j/hardware_to_run_32b_models_at_great_speeds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kimo3j/hardware_to_run_32b_models_at_great_speeds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T16:24:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kihrpt</id>
    <title>Best model to have</title>
    <updated>2025-05-09T12:54:17+00:00</updated>
    <author>
      <name>/u/Obvious_Cell_1515</name>
      <uri>https://old.reddit.com/user/Obvious_Cell_1515</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to have a model installed locally for &amp;quot;doomsday prep&amp;quot; (no imminent threat to me just because i can). Which open source model should i keep installed, i am using LM Studio and there are so many models at this moment and i havent kept up with all the new ones releasing so i have no idea. Preferably a uncensored model if there is a latest one which is very good&lt;/p&gt; &lt;p&gt;Sorry, I should give my hardware specifications. Ryzen 5600, Amd RX 580 gpu, 16gigs ram, SSD.&lt;/p&gt; &lt;p&gt;The gemma-3-12b-it-qat model runs good on my system if that helps&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Obvious_Cell_1515"&gt; /u/Obvious_Cell_1515 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kihrpt/best_model_to_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kihrpt/best_model_to_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kihrpt/best_model_to_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T12:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kis38u</id>
    <title>GLM-4-32B-0414 one shot of a Pong game with AI opponent that gets stressed as the game progresses, leading to more mistakes!</title>
    <updated>2025-05-09T20:13:07+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kis38u/glm432b0414_one_shot_of_a_pong_game_with_ai/"&gt; &lt;img alt="GLM-4-32B-0414 one shot of a Pong game with AI opponent that gets stressed as the game progresses, leading to more mistakes!" src="https://external-preview.redd.it/qF4fn45-cPqJu4NhTUl8Bwm3SF8Y_jJRcYSsoPgQW40.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5a6eefa6189a89c27706a25a4e0620dbdb8b6ae" title="GLM-4-32B-0414 one shot of a Pong game with AI opponent that gets stressed as the game progresses, leading to more mistakes!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Code &amp;amp; play at jsfiddle &lt;a href="https://jsfiddle.net/jzsyenqm/"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nidzls3bdtze1.png?width=849&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=100ec8cc31bb165ed64331a9888721d3915bed93"&gt;https://preview.redd.it/nidzls3bdtze1.png?width=849&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=100ec8cc31bb165ed64331a9888721d3915bed93&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kis38u/glm432b0414_one_shot_of_a_pong_game_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kis38u/glm432b0414_one_shot_of_a_pong_game_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kis38u/glm432b0414_one_shot_of_a_pong_game_with_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T20:13:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki9u9d</id>
    <title>Sam Altman: OpenAI plans to release an open-source model this summer</title>
    <updated>2025-05-09T04:19:19+00:00</updated>
    <author>
      <name>/u/zan-max</name>
      <uri>https://old.reddit.com/user/zan-max</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki9u9d/sam_altman_openai_plans_to_release_an_opensource/"&gt; &lt;img alt="Sam Altman: OpenAI plans to release an open-source model this summer" src="https://external-preview.redd.it/ajdlMmxzcGNsb3plMbWgh0ga0DeDYWGdPekBwNb0wJ3u2lc2Xz7BD3amRjfR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=544abdd728657d68f07df0719bb55b0d05a32eb6" title="Sam Altman: OpenAI plans to release an open-source model this summer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sam Altman stated during today's Senate testimony that OpenAI is planning to release an open-source model this summer.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://www.youtube.com/watch?v=jOqTg1W_F5Q"&gt;https://www.youtube.com/watch?v=jOqTg1W_F5Q&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zan-max"&gt; /u/zan-max &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0cbh8rpcloze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki9u9d/sam_altman_openai_plans_to_release_an_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ki9u9d/sam_altman_openai_plans_to_release_an_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T04:19:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1kifny6</id>
    <title>IÂ´ve made a Local alternative to "DeepSite" called "LocalSite" - lets you create Web Pages and components like Buttons, etc. with Local LLMs via Ollama and LM Studio</title>
    <updated>2025-05-09T10:59:48+00:00</updated>
    <author>
      <name>/u/Fox-Lopsided</name>
      <uri>https://old.reddit.com/user/Fox-Lopsided</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kifny6/ive_made_a_local_alternative_to_deepsite_called/"&gt; &lt;img alt="IÂ´ve made a Local alternative to &amp;quot;DeepSite&amp;quot; called &amp;quot;LocalSite&amp;quot; - lets you create Web Pages and components like Buttons, etc. with Local LLMs via Ollama and LM Studio" src="https://external-preview.redd.it/cmZiOG5hYWFscXplMUKfOkFzHX-zyyu_0TBeY7g7ib_F1_WyOhrWr9oB-6Wv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=821152f80c82d31fa5be0a732eef101ba76712de" title="IÂ´ve made a Local alternative to &amp;quot;DeepSite&amp;quot; called &amp;quot;LocalSite&amp;quot; - lets you create Web Pages and components like Buttons, etc. with Local LLMs via Ollama and LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some of you may know the HuggingFace Space from &amp;quot;enzostvs&amp;quot; called &amp;quot;DeepSite&amp;quot; which lets you create Web Pages via Text Prompts with DeepSeek V3. I really liked the concept of it, and since Local LLMs have been getting pretty good at coding these days (GLM-4, Qwen3, UIGEN-T2), i decided to create a Local alternative that lets you use Local LLMs via Ollama and LM Studio to do the same as DeepSite locally.&lt;/p&gt; &lt;p&gt;You can also add Cloud LLM Providers via OpenAI Compatible APIs.&lt;/p&gt; &lt;p&gt;Watch the video attached to see it in action, where GLM-4-9B created a pretty nice pricing page for me!&lt;/p&gt; &lt;p&gt;Feel free to check it out and do whatever you want with it:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/weise25/LocalSite-ai"&gt;https://github.com/weise25/LocalSite-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to know what you guys think.&lt;/p&gt; &lt;p&gt;The development of this was heavily supported with Agentic Coding via Augment Code and also a little help from Gemini 2.5 Pro.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fox-Lopsided"&gt; /u/Fox-Lopsided &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/paflnbaalqze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kifny6/ive_made_a_local_alternative_to_deepsite_called/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kifny6/ive_made_a_local_alternative_to_deepsite_called/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T10:59:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kitq9v</id>
    <title>If you had a Blackwell DGX (B200) - what would you run?</title>
    <updated>2025-05-09T21:22:51+00:00</updated>
    <author>
      <name>/u/backnotprop</name>
      <uri>https://old.reddit.com/user/backnotprop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nvidia.com/en-us/data-center/dgx-b200/"&gt;x8 180GB cards&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would like to know what would you run on a single card?&lt;/p&gt; &lt;p&gt;What would you distribute?&lt;/p&gt; &lt;p&gt;...for any cool, fun, scientific, absurd, etc use case. We are serving models with tabbyapi (support for cuda12.8, others are behind). But we don't just have to serve endpoints.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/backnotprop"&gt; /u/backnotprop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kitq9v/if_you_had_a_blackwell_dgx_b200_what_would_you_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kitq9v/if_you_had_a_blackwell_dgx_b200_what_would_you_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kitq9v/if_you_had_a_blackwell_dgx_b200_what_would_you_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T21:22:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kimq0g</id>
    <title>4B Polish language model based on Qwen3 architecture</title>
    <updated>2025-05-09T16:27:02+00:00</updated>
    <author>
      <name>/u/Significant_Focus134</name>
      <uri>https://old.reddit.com/user/Significant_Focus134</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;I just released the first version of a 4B Polish language model based on the Qwen3 architecture:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/piotr-ai/polanka_4b_v0.1_qwen3_gguf"&gt;https://huggingface.co/piotr-ai/polanka_4b_v0.1_qwen3_gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I did continual pretraining of the Qwen3 4B Base model on a single RTX 4090 for around 10 days.&lt;/p&gt; &lt;p&gt;The dataset includes high-quality upsampled Polish content.&lt;/p&gt; &lt;p&gt;To keep the original modelâ€™s strengths, I used a mixed dataset: multilingual, math, code, synthetic, and instruction-style data.&lt;/p&gt; &lt;p&gt;The checkpoint was trained on ~1.4B tokens.&lt;/p&gt; &lt;p&gt;It runs really fast on a laptop (thanks to GGUF + llama.cpp).&lt;/p&gt; &lt;p&gt;Let me know what you think or if you run any tests!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant_Focus134"&gt; /u/Significant_Focus134 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kimq0g/4b_polish_language_model_based_on_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kimq0g/4b_polish_language_model_based_on_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kimq0g/4b_polish_language_model_based_on_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T16:27:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki7tg7</id>
    <title>Don't Offload GGUF Layers, Offload Tensors! 200%+ Gen Speed? Yes Please!!!</title>
    <updated>2025-05-09T02:24:58+00:00</updated>
    <author>
      <name>/u/skatardude10</name>
      <uri>https://old.reddit.com/user/skatardude10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Inspired by:&lt;/strong&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/&lt;/a&gt; but applied to any other model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Bottom line:&lt;/strong&gt; I am running a QwQ merge at IQ4_M size that used to run at 3.95 Tokens per second, with 59 of 65 layers offloaded to GPU. By selectively restricting certain FFN tensors to stay on the CPU, I've saved a ton of space on the GPU, now offload all 65 of 65 layers to the GPU and run at 10.61 Tokens per second. Why is this not standard?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This is ONLY relevant if you have some layers on CPU and CANNOT offload ALL layers to GPU due to VRAM constraints. If you already offload all layers to GPU, you're ahead of the game. &lt;em&gt;But&lt;/em&gt; maybe this could allow you to run larger models at acceptable speeds that would otherwise have been too slow for your liking.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Idea:&lt;/strong&gt; With llama.cpp and derivatives like koboldcpp, you offload entire LAYERS typically. Layers are comprised of various attention tensors, feed forward network (FFN) tensors, gates and outputs. Within each transformer layer, from what I gather, attention tensors are GPU heavy and smaller benefiting from parallelization, while FFN tensors are VERY LARGE tensors that use more basic matrix multiplication that can be done on CPU. You can use the --overridetensors flag in koboldcpp or -ot in llama.cpp to selectively keep certain TENSORS on the cpu.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How-To:&lt;/strong&gt; Upfront, here's an example...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;10.61 TPS vs 3.95 TPS&lt;/strong&gt; using the same amount of VRAM, &lt;em&gt;just offloading tensors&lt;/em&gt; instead of &lt;em&gt;entire layers:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python ~/koboldcpp/koboldcpp.py --threads 10 --usecublas --contextsize 40960 --flashattention --port 5000 --model ~/Downloads/MODELNAME.gguf --gpulayers 65 --quantkv 1 --overridetensors &amp;quot;\.[13579]\.ffn_up|\.[1-3][13579]\.ffn_up=CPU&amp;quot; ... [18:44:54] CtxLimit:39294/40960, Amt:597/2048, Init:0.24s, Process:68.69s (563.34T/s), Generate:56.27s (10.61T/s), Total:124.96s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Offloading layers baseline:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python ~/koboldcpp/koboldcpp.py --threads 6 --usecublas --contextsize 40960 --flashattention --port 5000 --model ~/Downloads/MODELNAME.gguf --gpulayers 59 --quantkv 1 ... [18:53:07] CtxLimit:39282/40960, Amt:585/2048, Init:0.27s, Process:69.38s (557.79T/s), Generate:147.92s (3.95T/s), Total:217.29s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More details on how to? Use regex to match certain FFN layers to target for selectively NOT offloading to GPU as the commands above show.&lt;/p&gt; &lt;p&gt;In my examples above, I targeted FFN up layers because mine were mostly IQ4_XS while my FFN down layers were selectively quantized between IQ4_XS and Q5-Q8, which means those larger tensors vary in size a lot. This is beside the point of this post, but would come into play if you are just going to selectively restrict offloading every/every other/every third FFN_X tensor while assuming they are all the same size with something like Unsloth's Dynamic 2.0 quants that keep certain tensors at higher bits if you were doing math. Realistically though, you're selectively restricting certain tensors from offloading to save GPU space and how you do that doesn't matter all that much as long as you are hitting your VRAM target with your overrides. For example, when I tried to optimize for having every other Q4 FFN tensor stay on CPU versus every third regardless of tensor quant that, included many Q6 and Q8 tensors, to reduce computation load from the higher bit tensors, I only gained 0.4 tokens/second.&lt;/p&gt; &lt;p&gt;So, really how to?? Look at your GGUF's model info. For example, let's use: &lt;a href="https://huggingface.co/MaziyarPanahi/QwQ-32B-GGUF/tree/main?show_file_info=QwQ-32B.Q3_K_M.gguf"&gt;https://huggingface.co/MaziyarPanahi/QwQ-32B-GGUF/tree/main?show_file_info=QwQ-32B.Q3_K_M.gguf&lt;/a&gt; and look at all the layers and all the tensors in each layer.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Tensor&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;blk.1.ffn_down.weight&lt;/td&gt; &lt;td align="left"&gt;[27 648, 5 120]&lt;/td&gt; &lt;td align="left"&gt;Q5_K&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;blk.1.ffn_gate.weight&lt;/td&gt; &lt;td align="left"&gt;[5 120, 27 648]&lt;/td&gt; &lt;td align="left"&gt;Q3_K&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;blk.1.ffn_norm.weight&lt;/td&gt; &lt;td align="left"&gt;[5 120]&lt;/td&gt; &lt;td align="left"&gt;F32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;blk.1.ffn_up.weight&lt;/td&gt; &lt;td align="left"&gt;[5 120, 27 648]&lt;/td&gt; &lt;td align="left"&gt;Q3_K&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;In this example, overriding tensors ffn_down at a higher Q5 to CPU would save more space on your GPU that fnn_up or fnn_gate at Q3. My regex from above only targeted ffn_up on layers 1-39, every other layer, to squeeze every last thing I could onto the GPU. I also alternated which ones I kept on CPU thinking maybe easing up on memory bottlenecks but not sure if that helps. &lt;strong&gt;Remember&lt;/strong&gt; to set threads equivalent to -1 of your total CPU &lt;em&gt;CORE count&lt;/em&gt; to optimize CPU inference (12C/24T), --threads 11 is good.&lt;/p&gt; &lt;p&gt;Either way, seeing QwQ run on my card at over double the speed now is INSANE and figured I would share so you guys look into this too. Offloading entire layers uses the same amount of memory as offloading specific tensors, but sucks way more. This way, offload everything to your GPU except the big layers that work well on CPU. Is this common knowledge?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Future:&lt;/strong&gt; I would love to see llama.cpp and others be able to automatically, selectively restrict offloading heavy CPU efficient tensors to the CPU rather than whole layers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skatardude10"&gt; /u/skatardude10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T02:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kiqqgh</id>
    <title>Local AI Radio Station (uses ACE)</title>
    <updated>2025-05-09T19:15:02+00:00</updated>
    <author>
      <name>/u/MustBeSomethingThere</name>
      <uri>https://old.reddit.com/user/MustBeSomethingThere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiqqgh/local_ai_radio_station_uses_ace/"&gt; &lt;img alt="Local AI Radio Station (uses ACE)" src="https://external-preview.redd.it/eHhjdmw5ZzAwdHplMZ8dC80fbuf6S0WKAY4O-4KfqUEFi7xvJoV20v06EMJ_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a6b9b7eb3c33b930707066322e1a03d91e6ddeb" title="Local AI Radio Station (uses ACE)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/PasiKoodaa/ACE-Step-RADIO"&gt;https://github.com/PasiKoodaa/ACE-Step-RADIO&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Probably works without gaps on 24GB VRAM. I have only tested it on 12GB. It would be very easy to also add radio hosts (for example DIA).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MustBeSomethingThere"&gt; /u/MustBeSomethingThere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fratbag00tze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiqqgh/local_ai_radio_station_uses_ace/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kiqqgh/local_ai_radio_station_uses_ace/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T19:15:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kivw6w</id>
    <title>Webollama: A sleek web interface for Ollama, making local LLM management and usage simple. WebOllama provides an intuitive UI to manage Ollama models, chat with AI, and generate completions.</title>
    <updated>2025-05-09T23:02:50+00:00</updated>
    <author>
      <name>/u/phantagom</name>
      <uri>https://old.reddit.com/user/phantagom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kivw6w/webollama_a_sleek_web_interface_for_ollama_making/"&gt; &lt;img alt="Webollama: A sleek web interface for Ollama, making local LLM management and usage simple. WebOllama provides an intuitive UI to manage Ollama models, chat with AI, and generate completions." src="https://external-preview.redd.it/y0EgI2XLTqKHcjAaim03gc_zVfisCy4KdfNRmAX06uU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05cff2b86886c237a89577608b4cb69c4870fc6c" title="Webollama: A sleek web interface for Ollama, making local LLM management and usage simple. WebOllama provides an intuitive UI to manage Ollama models, chat with AI, and generate completions." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phantagom"&gt; /u/phantagom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/dkruyt/webollama"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kivw6w/webollama_a_sleek_web_interface_for_ollama_making/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kivw6w/webollama_a_sleek_web_interface_for_ollama_making/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T23:02:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kigmfo</id>
    <title>Make Qwen3 Think like Gemini 2.5 Pro</title>
    <updated>2025-05-09T11:55:12+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kigmfo/make_qwen3_think_like_gemini_25_pro/"&gt; &lt;img alt="Make Qwen3 Think like Gemini 2.5 Pro" src="https://external-preview.redd.it/MZqi7CsqO_RyJH6OHbxt3tHe5kTNCKiSqlBbGI5rWyk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08149ac025081d5f8b32a770ddb6097e77e7f25c" title="Make Qwen3 Think like Gemini 2.5 Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So when I was reading Apriel-Nemotron-15b-Thinker's README, I saw this:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We ensure the model starts with &lt;code&gt;Here are my reasoning steps:\n&lt;/code&gt; during all our evaluations.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And this reminds me that I can do the same thing to Qwen3 and make it think step by step like Gemini 2.5. So I wrote an open WebUI function that always starts the assistant message with &lt;code&gt;&amp;lt;think&amp;gt;\nMy step by step thinking process went something like this:\n1.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And it actually worksâ€”now Qwen3 will think with 1. 2. 3. 4. 5.... just like Gemini 2.5.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;\&lt;/em&gt;This is just a small experiment; it doesn't magically enhance the model's intelligence, but rather encourages it to think in a different format.&lt;/strong&gt;*&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u35xvz8fkqze1.png?width=2266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b24bbe37f5dab6affa1cdde41d5ede56487219ef"&gt;https://preview.redd.it/u35xvz8fkqze1.png?width=2266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b24bbe37f5dab6affa1cdde41d5ede56487219ef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/AaronFeng753/Qwen3-Gemini2.5"&gt;https://github.com/AaronFeng753/Qwen3-Gemini2.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kigmfo/make_qwen3_think_like_gemini_25_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kigmfo/make_qwen3_think_like_gemini_25_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kigmfo/make_qwen3_think_like_gemini_25_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T11:55:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kinwuu</id>
    <title>One transistor modelling one neuron - Nature publication</title>
    <updated>2025-05-09T17:15:59+00:00</updated>
    <author>
      <name>/u/Important-Damage-173</name>
      <uri>https://old.reddit.com/user/Important-Damage-173</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's an exciting Nature paper that finds out the fact that it is possible to model a neuron on a single transistor. For reference: humans have 100 Billion neurons in their brains, the Apple M3 chip has 187 Billion.&lt;/p&gt; &lt;p&gt;Now look, this does not mean that you will be running a superhuman on a pc by end of year (since a synapse also requires a full transistor) but I expect things to radically change in terms of new processors in the next few years. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.nature.com/articles/s41586-025-08742-4"&gt;https://www.nature.com/articles/s41586-025-08742-4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Important-Damage-173"&gt; /u/Important-Damage-173 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kinwuu/one_transistor_modelling_one_neuron_nature/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kinwuu/one_transistor_modelling_one_neuron_nature/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kinwuu/one_transistor_modelling_one_neuron_nature/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T17:15:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kiwbs8</id>
    <title>Where is grok2?</title>
    <updated>2025-05-09T23:24:20+00:00</updated>
    <author>
      <name>/u/gzzhongqi</name>
      <uri>https://old.reddit.com/user/gzzhongqi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I remember Elon Musk specifically said on live Grok2 will be open-weighted once Grok3 is officially stable and running. Now even Grok3.5 is about to be released, so where is the Grok2 they promoised? Any news on that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gzzhongqi"&gt; /u/gzzhongqi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiwbs8/where_is_grok2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiwbs8/where_is_grok2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kiwbs8/where_is_grok2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T23:24:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kipwyo</id>
    <title>Vision support in llama-server just landed!</title>
    <updated>2025-05-09T18:39:48+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kipwyo/vision_support_in_llamaserver_just_landed/"&gt; &lt;img alt="Vision support in llama-server just landed!" src="https://external-preview.redd.it/CP6J3J5fdX2KpZfgtlXLbxjm3T5vBWcf3_9VTbBGdw8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84391c4a85576c89e482f93847f1374edea2bc37" title="Vision support in llama-server just landed!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12898"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kipwyo/vision_support_in_llamaserver_just_landed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kipwyo/vision_support_in_llamaserver_just_landed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T18:39:48+00:00</published>
  </entry>
</feed>
