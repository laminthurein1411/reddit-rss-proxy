<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-15T19:34:15+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lbcfjz</id>
    <title>How much VRAM do you have and what's your daily-driver model?</title>
    <updated>2025-06-14T16:14:55+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious what everyone is using day to day, locally, and what hardware they're using.&lt;/p&gt; &lt;p&gt;If you're using a quantized version of a model please say so!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbcfjz/how_much_vram_do_you_have_and_whats_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbcfjz/how_much_vram_do_you_have_and_whats_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbcfjz/how_much_vram_do_you_have_and_whats_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T16:14:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbn1vy</id>
    <title>Ryzen Ai Max+ 395 vs RTX 5090</title>
    <updated>2025-06-15T00:12:44+00:00</updated>
    <author>
      <name>/u/Any-Cobbler6161</name>
      <uri>https://old.reddit.com/user/Any-Cobbler6161</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently running a 5090 and it's been great. Super fast for anything under 34B. I mostly use WAN2.1 14B for video gen and some larger reasoning models. But Id like to run bigger models. And with the release of Veo 3 the quality has blown me away. Stuff like those Bigfoot and Stormtrooper vlogs look years ahead of anything wan2.1 can produce. I‚Äôm guessing we‚Äôll see comparable open-source models within a year, but I imagine the compute requirements will go up too as I heard Veo 3 was trained off a lot of H100's.&lt;/p&gt; &lt;p&gt;I'm trying to figure out how I could future proof to give me the best chance to be able to run these models when they come out. I do have some money saved up. But not H100 money lol. The 5090 although fast has been quite vram limited. I could sell it (bought at retail) and maybe go for a modded 48GB 4090. I also have a deposit down on a Framework Ryzen AI Max 395+ (128GB RAM), but I‚Äôm having second thoughts after watching some reviews ‚Äî256GB/s memory bandwidth and no CUDA. It seems to run LLaMA 70B, but only gets ~5 tokens/sec.&lt;/p&gt; &lt;p&gt;If I did get the framework I could try a PCIe 4x4 Oculink adapter to use it with the 5090, but not sure how well that‚Äôd work. I also picked up an EPYC 9184X last year for $500‚Äî460GB/s bandwidth, seems to run fine and might be ok for CPU inference, but idk how it would work with video gen.&lt;/p&gt; &lt;p&gt;With EPYC Venice just above for 2026 (1.6TB/s mem bandwidth supposedly), I‚Äôm debating whether to just wait and maybe try to get one of the lower/mid tier ones for a couple grand.&lt;/p&gt; &lt;p&gt;Curious if others are having similar ideas/any possibile solutions. As I dont believe our tech corporate overlords will be giving us any consumer grade hardware that will be able to run these models anytime soon. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Cobbler6161"&gt; /u/Any-Cobbler6161 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbn1vy/ryzen_ai_max_395_vs_rtx_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbn1vy/ryzen_ai_max_395_vs_rtx_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbn1vy/ryzen_ai_max_395_vs_rtx_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T00:12:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lc0oyf</id>
    <title>LLM chess ELO?</title>
    <updated>2025-06-15T13:45:37+00:00</updated>
    <author>
      <name>/u/BaconSky</name>
      <uri>https://old.reddit.com/user/BaconSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering how good LLMs are at chess, in regards to ELO - say Lichess for discussion purposes -, and looked online, and the best I could find was &lt;a href="https://dubesor.de/chess/chess-leaderboard"&gt;this&lt;/a&gt;, which seems at least not uptodate at best, and not reliable more realistically. Any clue anyone if there's a more accurate, uptodate, and generally speaking, lack of a better term, better?&lt;/p&gt; &lt;p&gt;Thanks :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BaconSky"&gt; /u/BaconSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc0oyf/llm_chess_elo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc0oyf/llm_chess_elo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lc0oyf/llm_chess_elo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T13:45:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbbafh</id>
    <title>Why local LLM?</title>
    <updated>2025-06-14T15:25:15+00:00</updated>
    <author>
      <name>/u/Beginning_Many324</name>
      <uri>https://old.reddit.com/user/Beginning_Many324</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm about to install Ollama and try a local LLM but I'm wondering what's possible and are the benefits apart from privacy and cost saving?&lt;br /&gt; My current memberships:&lt;br /&gt; - Claude AI&lt;br /&gt; - Cursor AI &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beginning_Many324"&gt; /u/Beginning_Many324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbbafh/why_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbbafh/why_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbbafh/why_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T15:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbv3f1</id>
    <title>Testing Local LLMs on a Simple Web App Task (Performance + Output Comparison)</title>
    <updated>2025-06-15T08:05:58+00:00</updated>
    <author>
      <name>/u/SoAp9035</name>
      <uri>https://old.reddit.com/user/SoAp9035</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hey everyone,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I recently did a simple test to compare how a few local LLMs (plus Claude Sonnet 3.5 for reference) could perform on a basic front-end web development prompt. The goal was to generate code for a &lt;strong&gt;real estate portfolio sharing website&lt;/strong&gt;, including a &lt;strong&gt;listing entry form&lt;/strong&gt; and &lt;strong&gt;listing display&lt;/strong&gt;, all in a &lt;strong&gt;single HTML file using HTML, CSS, and Bootstrap&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt used:&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Using HTML, CSS, and Bootstrap, write the code for a real estate portfolio sharing site, listing entry, and listing display in a single HTML file.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;My setup:&lt;/strong&gt;&lt;br /&gt; All models except &lt;strong&gt;Claude Sonnet 3.5&lt;/strong&gt; were tested locally on my laptop:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; RTX 4070 (8GB VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 32GB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference backend:&lt;/strong&gt; llama.cpp&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3 models:&lt;/strong&gt; Tested with &lt;code&gt;/think&lt;/code&gt; (thinking mode enabled).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üß™ Model Outputs + Performance&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Speed&lt;/th&gt; &lt;th align="left"&gt;Token Count&lt;/th&gt; &lt;th align="left"&gt;Notes&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GLM-9B-0414 Q5_K_XL&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;28.1 t/s&lt;/td&gt; &lt;td align="left"&gt;8451 tokens&lt;/td&gt; &lt;td align="left"&gt;Excellent, most professional design, but listing form doesn't work.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3 30B-A3B Q4_K_XL&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;12.4 t/s&lt;/td&gt; &lt;td align="left"&gt;1856 tokens&lt;/td&gt; &lt;td align="left"&gt;Fully working site, simpler than GLM but does the job.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3 8B Q5_K_XL&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;36.1 t/s&lt;/td&gt; &lt;td align="left"&gt;2420 tokens&lt;/td&gt; &lt;td align="left"&gt;Also functional and well-structured.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3 4B Q8_K_XL&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;38.0 t/s&lt;/td&gt; &lt;td align="left"&gt;3275 tokens&lt;/td&gt; &lt;td align="left"&gt;Surprisingly capable for its size, all basic requirements met.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Claude Sonnet 3.5 (Reference)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;‚Äì&lt;/td&gt; &lt;td align="left"&gt;‚Äì&lt;/td&gt; &lt;td align="left"&gt;Best overall: clean, functional, and interactive. No surprise here.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üí¨ My Thoughts:&lt;/h1&gt; &lt;p&gt;Out of all the models tested, here‚Äôs how I‚Äôd rank them &lt;strong&gt;in terms of quality of design and functionality&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Claude Sonnet 3.5&lt;/strong&gt; ‚Äì Clean, interactive, great structure (expected).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-9B-0414&lt;/strong&gt; ‚Äì VERY polished web page, great UX and design elements, but the listing form can‚Äôt add new entries. Still impressive ‚Äî I believe with a few additional prompts, it could be fixed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3 30B &amp;amp; Qwen3 8B&lt;/strong&gt; ‚Äì Both gave a proper, fully working HTML file that met the prompt's needs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3 4B&lt;/strong&gt; ‚Äì Smallest and simplest, but delivered the complete task nonetheless.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Despite the small functionality flaw, &lt;strong&gt;GLM-9B-0414&lt;/strong&gt; really blew me away in terms of how well-structured and professional-looking the output was. I'd say it's worth working with and iterating on.&lt;/p&gt; &lt;h1&gt;üîó Code Outputs&lt;/h1&gt; &lt;p&gt;You can see the generated HTML files and compare them yourself here:&lt;br /&gt; &lt;a href="https://filebin.net/r0vs0pujf3lx0hab"&gt;[LINK TO CODES]&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts if you‚Äôve tried similar tests ‚Äî particularly with GLM or Qwen3!&lt;br /&gt; Also open to suggestions for follow-up prompts or other models to try on my setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SoAp9035"&gt; /u/SoAp9035 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbv3f1/testing_local_llms_on_a_simple_web_app_task/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbv3f1/testing_local_llms_on_a_simple_web_app_task/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbv3f1/testing_local_llms_on_a_simple_web_app_task/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T08:05:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lc6idx</id>
    <title>Gemma3 12b or 27b for writing assistance/brainstorming?</title>
    <updated>2025-06-15T17:54:26+00:00</updated>
    <author>
      <name>/u/Lord_Thunderballs</name>
      <uri>https://old.reddit.com/user/Lord_Thunderballs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A disclaimer before any reddit writers shit on me for using AI to write. &lt;/p&gt; &lt;p&gt;I don't blindly copy and paste. I don't have it generate stories. All the ideas come from ME. I only use AI to bounce ideas off it. And to give advice on writing. And have it help me streamlie the stories. It's like having a more experienced writer looking at my work and providing advice on wording and making it more streamlined.&lt;/p&gt; &lt;p&gt;Recently I started having ChatGPT give me micro storywriting challenges to help me improve my writing skills. So far, it's been helpful.&lt;/p&gt; &lt;p&gt;I heard Gemma is really good at this sort of stuff to help writers with brainstorming and providing advice on editing texts. Would the 12b model be fine for what I need? &lt;/p&gt; &lt;p&gt;I have the 12b and 27b installed via ollama and open WebUI. I have an RX 7800Xt and I tested it out a little bit. The 27b takes a few minutes to output a response and it's not super different from the 12b responses. Maybe a bit more detailed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lord_Thunderballs"&gt; /u/Lord_Thunderballs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc6idx/gemma3_12b_or_27b_for_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc6idx/gemma3_12b_or_27b_for_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lc6idx/gemma3_12b_or_27b_for_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T17:54:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbu89a</id>
    <title>Dual 3060RTX's running vLLM / Model suggestions?</title>
    <updated>2025-06-15T07:08:06+00:00</updated>
    <author>
      <name>/u/phin586</name>
      <uri>https://old.reddit.com/user/phin586</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I am pretty new to the foray here and I have enjoyed the last couple of days learning a bit about setting things. &lt;/p&gt; &lt;p&gt;I was able to score a pair of 3060RTX's from marketplace for $350.&lt;/p&gt; &lt;p&gt;Currently I have vLLM running with dwetzel/Mistral-Small-24B-Instruct-2501-GPTQ-INT4, per a thread I found here.&lt;/p&gt; &lt;p&gt;Things run pretty well, but I was in hopes of also getting some image detection out of this, Any suggestions on models that would run well in this setup and accomplish this task?&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phin586"&gt; /u/phin586 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbu89a/dual_3060rtxs_running_vllm_model_suggestions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbu89a/dual_3060rtxs_running_vllm_model_suggestions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbu89a/dual_3060rtxs_running_vllm_model_suggestions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T07:08:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lc7ye0</id>
    <title>Bank transactions extractions, tech stack help needed.</title>
    <updated>2025-06-15T18:54:39+00:00</updated>
    <author>
      <name>/u/nimmalachaitanya</name>
      <uri>https://old.reddit.com/user/nimmalachaitanya</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am planning to start a project to extract transactions from bank PDFs. Let say I have 50 different bank statements and they all have different templates some have tables and some donot. Different banks uses different headers for transactions like some credit/deposit..., some banks daily balance etc. So input is PDFs and output is excle with transactions. So I need help in system architecture.(Fully loca runl)&lt;/p&gt; &lt;p&gt;1) model? 2) embeddings model 3) Db&lt;/p&gt; &lt;p&gt;I am new to rag. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nimmalachaitanya"&gt; /u/nimmalachaitanya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc7ye0/bank_transactions_extractions_tech_stack_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc7ye0/bank_transactions_extractions_tech_stack_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lc7ye0/bank_transactions_extractions_tech_stack_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T18:54:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbj978</id>
    <title>How does everyone do Tool Calling?</title>
    <updated>2025-06-14T21:12:00+00:00</updated>
    <author>
      <name>/u/MKU64</name>
      <uri>https://old.reddit.com/user/MKU64</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve begun to see Tool Calling so that I can make the LLMs I‚Äôm using do real work for me. I do all my LLM work in Python and was wondering if there‚Äôs any libraries that you recommend that make it all easy. I have just recently seen MCP and I have been trying to add it manually through the OpenAI library but that‚Äôs quite slow so does anyone have any recommendations? Like LangChain, LlamaIndex and such.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MKU64"&gt; /u/MKU64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbj978/how_does_everyone_do_tool_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbj978/how_does_everyone_do_tool_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbj978/how_does_everyone_do_tool_calling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T21:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbgkuk</id>
    <title>Massive performance gains from linux?</title>
    <updated>2025-06-14T19:13:34+00:00</updated>
    <author>
      <name>/u/Only_Situation_4713</name>
      <uri>https://old.reddit.com/user/Only_Situation_4713</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ive been using LM studio for inference and I switched to Mint Linux because Windows is hell. My tokens per second went from 1-2t/s to 7-8t/s. Prompt eval went from 1 minutes to 2 seconds.&lt;/p&gt; &lt;p&gt;Specs: 13700k Asus Maximus hero z790 64gb of ddr5 2tb Samsung pro SSD 2X 3090 at 250w limit each on x8 pcie lanes&lt;/p&gt; &lt;p&gt;Model: Unsloth Qwen3 235B Q2_K_XL 45 Layers on GPU.&lt;/p&gt; &lt;p&gt;40k context window on both &lt;/p&gt; &lt;p&gt;Was wondering if this was normal? I was using a fresh windows install so I'm not sure what the difference was.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Only_Situation_4713"&gt; /u/Only_Situation_4713 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbgkuk/massive_performance_gains_from_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbgkuk/massive_performance_gains_from_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbgkuk/massive_performance_gains_from_linux/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T19:13:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbd2jy</id>
    <title>What LLM is everyone using in June 2025?</title>
    <updated>2025-06-14T16:43:01+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious what everyone‚Äôs running now.&lt;br /&gt; What model(s) are in your regular rotation?&lt;br /&gt; What hardware are you on?&lt;br /&gt; How are you running it? (LM Studio, Ollama, llama.cpp, etc.)&lt;br /&gt; What do you use it for?&lt;/p&gt; &lt;p&gt;Here‚Äôs mine:&lt;br /&gt; Recently I've been using mostly Qwen3 (30B, 32B, and 235B)&lt;br /&gt; Ryzen 7 5800X, 128GB RAM, RTX 3090&lt;br /&gt; Ollama + Open WebUI&lt;br /&gt; Mostly general use and private conversations I‚Äôd rather not run on cloud platforms&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbd2jy/what_llm_is_everyone_using_in_june_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbd2jy/what_llm_is_everyone_using_in_june_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbd2jy/what_llm_is_everyone_using_in_june_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T16:43:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbsma4</id>
    <title>Tabulens: A Vision-LLM Powered PDF Table Extractor</title>
    <updated>2025-06-15T05:22:30+00:00</updated>
    <author>
      <name>/u/PleasantInspection12</name>
      <uri>https://old.reddit.com/user/PleasantInspection12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;For one of my projects, I needed a tool to pull tables out of PDFs as CSVs (especially ones with nested or hierarchical headers). However, most existing libraries I found couldn't handle those cases well. So, I built this tool (tabulens), which leverages vision-LLMs to convert PDF tables into pandas DataFrames (and optionally save them as CSVs) while preserving complex header structures.&lt;/p&gt; &lt;p&gt;This is the first iteration, and I‚Äôd love any feedback or bug reports you might have. Thanks in advance for checking it out!&lt;/p&gt; &lt;p&gt;Here is the link to GitHub: &lt;a href="https://github.com/astonishedrobo/tabulens"&gt;https://github.com/astonishedrobo/tabulens&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is available as python library to install.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PleasantInspection12"&gt; /u/PleasantInspection12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbsma4/tabulens_a_visionllm_powered_pdf_table_extractor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbsma4/tabulens_a_visionllm_powered_pdf_table_extractor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbsma4/tabulens_a_visionllm_powered_pdf_table_extractor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T05:22:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1lc295w</id>
    <title>Recreating old cartoons</title>
    <updated>2025-06-15T14:55:57+00:00</updated>
    <author>
      <name>/u/olympics2022wins</name>
      <uri>https://old.reddit.com/user/olympics2022wins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don‚Äôt actually have a solution for this. I‚Äôm curious if anyone else has found one.&lt;/p&gt; &lt;p&gt;At some point in the future, I imagine the new video/image models could take old cartoons (or stop motion Gumby) that are very low resolution and very low frame rate and build them so that they are both high frame as well as high resolution. Nine months ago or so I downloaded all the different upscalers and was unimpressed on their ability to handle cartoons. The new video models brought it back to mind. Is anyone working on a project like this? Or now of a technology where there are good results?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/olympics2022wins"&gt; /u/olympics2022wins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc295w/recreating_old_cartoons/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc295w/recreating_old_cartoons/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lc295w/recreating_old_cartoons/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T14:55:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbotj8</id>
    <title>Make Local Models watch your screen! Observer Tutorial</title>
    <updated>2025-06-15T01:46:50+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbotj8/make_local_models_watch_your_screen_observer/"&gt; &lt;img alt="Make Local Models watch your screen! Observer Tutorial" src="https://external-preview.redd.it/OHR6NnZzMGl4ejZmMcpab4Kc_hsNzcDZ4OjMoSBBtpUkATpHq4IzyyL2uzQ6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fbbe587ce8d56143cd21d0bf6daed0a372e406f" title="Make Local Models watch your screen! Observer Tutorial" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys!&lt;/p&gt; &lt;p&gt;This is a tutorial on how to self host Observer on your home lab! &lt;/p&gt; &lt;p&gt;See more info here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/toz9tr0ixz6f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbotj8/make_local_models_watch_your_screen_observer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbotj8/make_local_models_watch_your_screen_observer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T01:46:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbfinu</id>
    <title>26 Quants that fit on 32GB vs 10,000-token "Needle in a Haystack" test</title>
    <updated>2025-06-14T18:27:59+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;The Test&lt;/h1&gt; &lt;h2&gt;The Needle&lt;/h2&gt; &lt;p&gt;In HG Wells' &lt;em&gt;&amp;quot;The Time Machine&amp;quot;&lt;/em&gt; I took the first several chapters, amounting to 10,000 tokens (~5 chapters) and replaced a line of Dialog in Chapter 3 (~6,000 tokens in):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The Time Traveller came to the place reserved for him without a word. He smiled quietly, in his old way. ‚ÄúWhere‚Äôs my mutton?‚Äù he said. ‚ÄúWhat a treat it is to stick a fork into meat again!‚Äù &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The Time Traveller came to the place reserved for him without a word. He smiled quietly, in his old way. ‚ÄúThe fastest land animal in the world is the Cheetah?‚Äù he said. ‚ÄúAnd because of that, we need to dive underwater to save the lost city of Atlantis..‚Äù &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;The prompt/instructions used&lt;/h2&gt; &lt;p&gt;The following is the prompt provided before the long context. It is an instruction (in very plain English giving relatively broad instructions) to locate the text that appears broken or out of place. The only added bit of instructions is to ignore chapter-divides, which I have left in the text.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Something is terribly wrong with the following text (something broken, out of place). You need to read through the whole thing and identify the broken / nonsensical part and then report back with what/where the broken line is. You may notice chapter-divides, these are normal and not broken.. Here is your text to evaluate: &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;The Models/Weights Used&lt;/h2&gt; &lt;p&gt;For this test I wanted to test everything that I had on my machine, a 2x6800 (32GB VRAM total) system. The quants are what I had downloaded/available. For smaller models with extra headroom I tried to use Q5, but these quants are relatively random. &lt;strong&gt;The only goal in selecting these models/quants was that every model chosen was one that a local user with access to 32GB of VRAM or high-bandwidth memory would use.&lt;/strong&gt;&lt;/p&gt; &lt;h2&gt;The Setup&lt;/h2&gt; &lt;p&gt;I think my take to settings/temperature was imperfect, but important to share. Llama CPP was used (specifically the llama-server utility). Settings for temperature were taken from the official model cards (not the cards of the quants) on Huggingface. If none were provided, a test was done at temp == 0.2 and temp == 0.7 and the better of the two results was taken. &lt;strong&gt;In all scenarios kv cache was q8&lt;/strong&gt; - while this likely impacted the results for some models, I believe it keeps to the spirit of the test which is &lt;em&gt;&amp;quot;how would someone with 32GB realistically use these weights?&amp;quot;&lt;/em&gt;.&lt;/p&gt; &lt;h2&gt;Some bonus models&lt;/h2&gt; &lt;p&gt;I tested a handful of models from Lambda-Chat just because. Most of them succeeded, however Llama4 struggled quite a bit.&lt;/p&gt; &lt;h2&gt;Some unscientific disclaimers&lt;/h2&gt; &lt;p&gt;There are a few grains of salt to take with this test, even if you keep in mind my goal was to &lt;em&gt;&amp;quot;test everything in a way that someone with 32GB would realistically use it&amp;quot;&lt;/em&gt;. For all models that failed, I should see if I can fit a larger-sized quant and complete the test that way. For Llama2 70b, I believe the context size simply overwhelmed it.&lt;/p&gt; &lt;p&gt;At the extreme end (see Deepseek 0528 and Hermes 405b) the models didn't seem to be 'searching' so much as identifying &lt;em&gt;&amp;quot;hey, this isn't in HG Well's 'The Time Machine!'&amp;quot;&lt;/em&gt;. I believe this is a fair result, but at the extremely high-end side of model-size the test stops being a &lt;em&gt;&amp;quot;needle in a haystack&amp;quot;&lt;/em&gt; test and stars being a test of the depths of their knowledge. This touches on the biggest problem which is that HG Well's &lt;em&gt;&amp;quot;The Time Machine&amp;quot;&lt;/em&gt; is a very famous work that has been in the public domain for decades at this point. If Meta trained on this but Mistral didn't, could the models instead just be searching for &lt;em&gt;&amp;quot;hey I don't remember that&amp;quot;&lt;/em&gt; instead of &lt;em&gt;&amp;quot;that makes no sense in this context&amp;quot;&lt;/em&gt; ?&lt;/p&gt; &lt;p&gt;For the long-thinkers that failed (QwQ namely) I tried several tests where they would think themselves in circles or get caught up convincing themselves that normal parts of a sci-fi story were 'nonsensical', but it was the train of thought that always ruined them. If tried with enough random settings, I'm sure they would have found it eventually.&lt;/p&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="right"&gt;Params (B)&lt;/th&gt; &lt;th align="center"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Results&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Meta Llama Family&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 2 70&lt;/td&gt; &lt;td align="right"&gt;70&lt;/td&gt; &lt;td align="center"&gt;q2&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.3 70&lt;/td&gt; &lt;td align="right"&gt;70&lt;/td&gt; &lt;td align="center"&gt;iq3&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.3 70&lt;/td&gt; &lt;td align="right"&gt;70&lt;/td&gt; &lt;td align="center"&gt;iq2&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 4 Scout&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="center"&gt;iq2&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.1 8&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.1 8&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="center"&gt;q6&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.2 3&lt;/td&gt; &lt;td align="right"&gt;3&lt;/td&gt; &lt;td align="center"&gt;q6&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IBM Granite 3.3&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mistral Family&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral Small 3.1&lt;/td&gt; &lt;td align="right"&gt;24&lt;/td&gt; &lt;td align="center"&gt;iq4&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral Small 3&lt;/td&gt; &lt;td align="right"&gt;24&lt;/td&gt; &lt;td align="center"&gt;q6&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Deephermes-preview&lt;/td&gt; &lt;td align="right"&gt;24&lt;/td&gt; &lt;td align="center"&gt;q6&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Magistral Small&lt;/td&gt; &lt;td align="right"&gt;24&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;Solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Nvidia&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Nemotron Super (nothink)&lt;/td&gt; &lt;td align="right"&gt;49&lt;/td&gt; &lt;td align="center"&gt;iq4&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Nemotron Super (think)&lt;/td&gt; &lt;td align="right"&gt;49&lt;/td&gt; &lt;td align="center"&gt;iq4&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Nemotron Ultra-Long 8&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Google&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma3 12&lt;/td&gt; &lt;td align="right"&gt;12&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma3 27&lt;/td&gt; &lt;td align="right"&gt;27&lt;/td&gt; &lt;td align="center"&gt;iq4&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen Family&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;QwQ&lt;/td&gt; &lt;td align="right"&gt;32&lt;/td&gt; &lt;td align="center"&gt;q6&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 8b (nothink)&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 8b (think)&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 14 (think)&lt;/td&gt; &lt;td align="right"&gt;14&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 14 (nothink)&lt;/td&gt; &lt;td align="right"&gt;14&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30 A3B (think)&lt;/td&gt; &lt;td align="right"&gt;30&lt;/td&gt; &lt;td align="center"&gt;iq4&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30 A3B (nothink)&lt;/td&gt; &lt;td align="right"&gt;30&lt;/td&gt; &lt;td align="center"&gt;iq4&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30 A6B Extreme (nothink)&lt;/td&gt; &lt;td align="right"&gt;30&lt;/td&gt; &lt;td align="center"&gt;q4&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30 A6B Extreme (think)&lt;/td&gt; &lt;td align="right"&gt;30&lt;/td&gt; &lt;td align="center"&gt;q4&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 32 (think)&lt;/td&gt; &lt;td align="right"&gt;32&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 32 (nothink)&lt;/td&gt; &lt;td align="right"&gt;32&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Deepseek-R1-0528-Distill-Qwen3-8b&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Other&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4&lt;/td&gt; &lt;td align="right"&gt;32&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Some random bonus results from an inference provider (not 32GB)&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="right"&gt;Params (B)&lt;/th&gt; &lt;th align="center"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Results&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Lambda Chat (some quick remote tests)&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hermes 3.1 405&lt;/td&gt; &lt;td align="right"&gt;405&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 4 Scout&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 4 Maverick&lt;/td&gt; &lt;td align="right"&gt;400&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Nemotron 3.1 70&lt;/td&gt; &lt;td align="right"&gt;70&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Deepseek R1 0528&lt;/td&gt; &lt;td align="right"&gt;671&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Deepseek V3 0324&lt;/td&gt; &lt;td align="right"&gt;671&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;R1-Distill-70&lt;/td&gt; &lt;td align="right"&gt;70&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 32 (think)&lt;/td&gt; &lt;td align="right"&gt;32&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 32 (nothink)&lt;/td&gt; &lt;td align="right"&gt;32&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5 Coder 32&lt;/td&gt; &lt;td align="right"&gt;32&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbfinu/26_quants_that_fit_on_32gb_vs_10000token_needle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbfinu/26_quants_that_fit_on_32gb_vs_10000token_needle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbfinu/26_quants_that_fit_on_32gb_vs_10000token_needle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T18:27:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbkd46</id>
    <title>I added vision to Magistral</title>
    <updated>2025-06-14T22:02:20+00:00</updated>
    <author>
      <name>/u/Vivid_Dot_6405</name>
      <uri>https://old.reddit.com/user/Vivid_Dot_6405</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbkd46/i_added_vision_to_magistral/"&gt; &lt;img alt="I added vision to Magistral" src="https://external-preview.redd.it/X_g72xTZNGOJR899I7pB5eNf8G3zVQ49K4x504QQmpg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6cdf83a10d380087b7b9940bcbc3d928a15a2e93" title="I added vision to Magistral" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was inspired by an &lt;a href="https://huggingface.co/ngxson/Devstral-Small-Vision-2505-GGUF"&gt;experimental Devstral model&lt;/a&gt;, and had the idea to the same thing to Magistral Small.&lt;/p&gt; &lt;p&gt;I replaced Mistral Small 3.1's language layers with Magistral's.&lt;br /&gt; I suggest using vLLM for inference with the correct system prompt and sampling params.&lt;br /&gt; There may be config errors present. The model's visual reasoning is definitely not as good as text-only, but it does work.&lt;/p&gt; &lt;p&gt;At the moment, I don't have the resources to replicate Mistral's vision benchmarks from their tech report.&lt;br /&gt; Let me know if you notice any weird behavior!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vivid_Dot_6405"&gt; /u/Vivid_Dot_6405 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/OptimusePrime/Magistral-Small-2506-Vision"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbkd46/i_added_vision_to_magistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbkd46/i_added_vision_to_magistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T22:02:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lc5w5r</id>
    <title>Is rocm better supported on arch through a AUR package?</title>
    <updated>2025-06-15T17:28:39+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Or is the best way to use rocm the docker image provided here: &lt;a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/pytorch-install.html#using-wheels-package"&gt;https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/pytorch-install.html#using-wheels-package&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For a friend of mine&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc5w5r/is_rocm_better_supported_on_arch_through_a_aur/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc5w5r/is_rocm_better_supported_on_arch_through_a_aur/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lc5w5r/is_rocm_better_supported_on_arch_through_a_aur/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T17:28:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lc8cse</id>
    <title>So how are people actually building their agentic RAG pipeline?</title>
    <updated>2025-06-15T19:11:05+00:00</updated>
    <author>
      <name>/u/walagoth</name>
      <uri>https://old.reddit.com/user/walagoth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a rag app, with a few sources that I can manually chose from to retrieve context. how does one prompt the LLM to get it to choose the right source? I just read on here people have success with the new mistral, but what do these prompts to the agent LLM look like? What have I missed after all these months that everyone seems to how to build an agent for their bespoke vector databases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/walagoth"&gt; /u/walagoth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc8cse/so_how_are_people_actually_building_their_agentic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc8cse/so_how_are_people_actually_building_their_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lc8cse/so_how_are_people_actually_building_their_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T19:11:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbwxj8</id>
    <title>Do multimodal LLMs (like Chatgpt, Gemini, Claude) use OCR under the hood to read text in images?</title>
    <updated>2025-06-15T10:11:55+00:00</updated>
    <author>
      <name>/u/Comprehensive-Yam291</name>
      <uri>https://old.reddit.com/user/Comprehensive-Yam291</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SOTA multimodal LLMs can read text from images (e.g. signs, screenshots, book pages) really well ‚Äî almost better thatn OCR.&lt;/p&gt; &lt;p&gt;Are they actually using an internal OCR system (like Tesseract or Azure Vision), or do they learn to &amp;quot;read&amp;quot; purely through pretraining (like contrastive learning on image-text pairs)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comprehensive-Yam291"&gt; /u/Comprehensive-Yam291 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbwxj8/do_multimodal_llms_like_chatgpt_gemini_claude_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbwxj8/do_multimodal_llms_like_chatgpt_gemini_claude_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbwxj8/do_multimodal_llms_like_chatgpt_gemini_claude_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T10:11:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbqwfs</id>
    <title>Mistral Small 3.1 is incredible for agentic use cases</title>
    <updated>2025-06-15T03:40:51+00:00</updated>
    <author>
      <name>/u/ButterscotchVast2948</name>
      <uri>https://old.reddit.com/user/ButterscotchVast2948</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently tried switching from Gemini 2.5 to Mistral Small 3.1 for most components of my agentic workflow and barely saw any drop off in performance. It‚Äôs absolutely mind blowing how good 3.1 is given how few parameters it has. Extremely accurate and intelligent tool calling and structured output capabilities, and equipping 3.1 with web search makes it as good as any frontier LLM in my use cases. Not to mention 3.1 is DIRT cheap and super fast.&lt;/p&gt; &lt;p&gt;Anyone else having great experiences with Mistral Small 3.1? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ButterscotchVast2948"&gt; /u/ButterscotchVast2948 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbqwfs/mistral_small_31_is_incredible_for_agentic_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbqwfs/mistral_small_31_is_incredible_for_agentic_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbqwfs/mistral_small_31_is_incredible_for_agentic_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T03:40:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbva5o</id>
    <title>rednote-hilab dots.llm1 support has been merged into llama.cpp</title>
    <updated>2025-06-15T08:18:35+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbva5o/rednotehilab_dotsllm1_support_has_been_merged/"&gt; &lt;img alt="rednote-hilab dots.llm1 support has been merged into llama.cpp" src="https://external-preview.redd.it/RLBNoVg_e7B3XdrLX8zzgLBIrezL9D4uJwyF2H_1MAE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7d590a18325c50ec13cca839d15368aa53334ae" title="rednote-hilab dots.llm1 support has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14118"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbva5o/rednotehilab_dotsllm1_support_has_been_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbva5o/rednotehilab_dotsllm1_support_has_been_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T08:18:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbnb79</id>
    <title>LLM training on RTX 5090</title>
    <updated>2025-06-15T00:25:56+00:00</updated>
    <author>
      <name>/u/AstroAlto</name>
      <uri>https://old.reddit.com/user/AstroAlto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbnb79/llm_training_on_rtx_5090/"&gt; &lt;img alt="LLM training on RTX 5090" src="https://external-preview.redd.it/cHhubmR6czBqejZmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f0235345a48cddb60039735fb034b1cf444515cc" title="LLM training on RTX 5090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tech Stack&lt;/p&gt; &lt;p&gt;Hardware &amp;amp; OS: NVIDIA RTX 5090 (32GB VRAM, Blackwell architecture), Ubuntu 22.04 LTS, CUDA 12.8&lt;/p&gt; &lt;p&gt;Software: Python 3.12, PyTorch 2.8.0 nightly, Transformers and Datasets libraries from Hugging Face, Mistral-7B base model (7.2 billion parameters)&lt;/p&gt; &lt;p&gt;Training: Full fine-tuning with gradient checkpointing, 23 custom instruction-response examples, Adafactor optimizer with bfloat16 precision, CUDA memory optimization for 32GB VRAM&lt;/p&gt; &lt;p&gt;Environment: Python virtual environment with NVIDIA drivers 570.133.07, system monitoring with nvtop and htop&lt;/p&gt; &lt;p&gt;Result: Domain-specialized 7 billion parameter model trained on cutting-edge RTX 5090 using latest PyTorch nightly builds for RTX 5090 GPU compatibility.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AstroAlto"&gt; /u/AstroAlto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t5kg81t0jz6f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbnb79/llm_training_on_rtx_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbnb79/llm_training_on_rtx_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T00:25:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lc6tii</id>
    <title>I wrapped Apple‚Äôs new on-device models in an OpenAI-compatible API</title>
    <updated>2025-06-15T18:06:56+00:00</updated>
    <author>
      <name>/u/FixedPt</name>
      <uri>https://old.reddit.com/user/FixedPt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent the weekend vibe-coding in Cursor and ended up with a small Swift app that turns the new macOS 26 on-device Apple Intelligence models into a local server you can hit with standard OpenAI &lt;code&gt;/v1/chat/completions&lt;/code&gt; calls. Point any client you like at &lt;code&gt;http://127.0.0.1:11535&lt;/code&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nothing leaves your Mac&lt;/li&gt; &lt;li&gt;Works with any OpenAI-compatible client&lt;/li&gt; &lt;li&gt;Open source, MIT-licensed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo‚Äôs here ‚Üí &lt;a href="https://github.com/gety-ai/apple-on-device-openai"&gt;&lt;strong&gt;https://github.com/gety-ai/apple-on-device-openai&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It was a fun hack‚Äîlet me know if you try it out or run into any weirdness. Cheers! üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FixedPt"&gt; /u/FixedPt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc6tii/i_wrapped_apples_new_ondevice_models_in_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc6tii/i_wrapped_apples_new_ondevice_models_in_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lc6tii/i_wrapped_apples_new_ondevice_models_in_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T18:06:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lc2pv9</id>
    <title>PSA: 2 * 3090 with Nvlink can cause depression*</title>
    <updated>2025-06-15T15:15:53+00:00</updated>
    <author>
      <name>/u/cuckfoders</name>
      <uri>https://old.reddit.com/user/cuckfoders</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc2pv9/psa_2_3090_with_nvlink_can_cause_depression/"&gt; &lt;img alt="PSA: 2 * 3090 with Nvlink can cause depression*" src="https://preview.redd.it/sy4x3c4ft37f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b06f38f038f50f6e67a1b6715463246bf3738e46" title="PSA: 2 * 3090 with Nvlink can cause depression*" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. I was enjoying my 3090 so much. So I thought why not get a second? My use case is local coding models, and Gemma 3 mostly. &lt;/p&gt; &lt;p&gt;It's been nothing short of a nightmare to get working. Just about everything that could go wrong, has gone wrong. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mining rig frame took a day to put together &lt;/li&gt; &lt;li&gt;Power supply so huge it's just hanging out of said rig&lt;/li&gt; &lt;li&gt;Pci-e extender cables are a pain&lt;/li&gt; &lt;li&gt;My OS nvme died during this process&lt;/li&gt; &lt;li&gt;Fiddling with bios options to get both to work&lt;/li&gt; &lt;li&gt;Nvlink wasn't clipped on properly at first&lt;/li&gt; &lt;li&gt;I have a pci-e bifurcation card that I'm not using because I'm too scared to see what happens if I plug that in (it has a sata power connector and I'm scared it will just blow up)&lt;/li&gt; &lt;li&gt;Wouldn't turn on this morning (I've snapped my pci-e clips off my motherboard so maybe it's that)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I have a desk fan nearby for when I finish getting vLLM setup. I will try and clip some case fans near them. &lt;/p&gt; &lt;p&gt;I suppose the point of this post and my advice is, if you are going to mess around - build a second machine, don't take your workstation and try make it be something it isn't. &lt;/p&gt; &lt;p&gt;Cheers.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Just trying to have some light humour about self inflicted problems and hoping to help anyone who might be thinking of doing the same to themselves. ‚ù§Ô∏è&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cuckfoders"&gt; /u/cuckfoders &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sy4x3c4ft37f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc2pv9/psa_2_3090_with_nvlink_can_cause_depression/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lc2pv9/psa_2_3090_with_nvlink_can_cause_depression/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T15:15:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbrnod</id>
    <title>Jan-nano, a 4B model that can outperform 671B on MCP</title>
    <updated>2025-06-15T04:24:03+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbrnod/jannano_a_4b_model_that_can_outperform_671b_on_mcp/"&gt; &lt;img alt="Jan-nano, a 4B model that can outperform 671B on MCP" src="https://external-preview.redd.it/cHZ1c3hxZW9wMDdmMa4t04YB4a4x402rBK-VNPFlhWpjFF6pjwxUI9ThBGZC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91ac073491f0e5dcff93b653851cbf8fdeb441de" title="Jan-nano, a 4B model that can outperform 671B on MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone it's me from Menlo Research again,&lt;/p&gt; &lt;p&gt;Today, I‚Äôd like to introduce our latest model: &lt;strong&gt;Jan-nano&lt;/strong&gt; - a model fine-tuned with DAPO on Qwen3-4B. Jan-nano comes with some unique capabilities:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It can perform deep research (with the right prompting)&lt;/li&gt; &lt;li&gt;It picks up relevant information effectively from search results&lt;/li&gt; &lt;li&gt;It uses tools efficiently&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our original goal was to build a super small model that excels at using search tools to extract high-quality information. To evaluate this, we chose &lt;strong&gt;SimpleQA&lt;/strong&gt; - a relatively straightforward benchmark to test whether the model can find and extract the right answers.&lt;/p&gt; &lt;p&gt;Again, Jan-nano only outperforms Deepseek-671B on this metric, using an agentic and tool-usage-based approach. &lt;strong&gt;We are fully aware that a 4B model has its limitations&lt;/strong&gt;, but it's always interesting to see how far you can push it. Jan-nano can serve as your self-hosted Perplexity alternative on a budget. (We're aiming to improve its performance to 85%, or even close to 90%).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;We will be releasing technical report very soon, stay tuned!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can find the model at:&lt;br /&gt; &lt;a href="https://huggingface.co/Menlo/Jan-nano"&gt;https://huggingface.co/Menlo/Jan-nano&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also have gguf at:&lt;br /&gt; &lt;a href="https://huggingface.co/Menlo/Jan-nano-gguf"&gt;https://huggingface.co/Menlo/Jan-nano-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I saw some users have technical challenges on prompt template of the gguf model, please raise it on the issues we will fix one by one. However at the moment &lt;strong&gt;the model can run well in Jan app and llama.server.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmark&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The evaluation was done using agentic setup, which let the model to freely choose tools to use and generate the answer instead of handheld approach of workflow based deep-research repo that you come across online. So basically it's just input question, then model call tool and generate the answer, like you use MCP in the chat app.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SimpleQA:&lt;/strong&gt;&lt;br /&gt; - OpenAI o1: 42.6&lt;br /&gt; - Grok 3: 44.6&lt;br /&gt; - 03: 49.4&lt;br /&gt; - Claude-3.7-Sonnet: 50.0&lt;br /&gt; - Gemini-2.5 pro: 52.9&lt;br /&gt; &lt;strong&gt;- baseline-with-MCP: 59.2&lt;/strong&gt;&lt;br /&gt; - ChatGPT-4.5: 62.5&lt;br /&gt; &lt;strong&gt;- deepseek-671B-with-MCP: 78.2&lt;/strong&gt; (we benchmark using openrouter)&lt;br /&gt; &lt;strong&gt;- jan-nano-v0.4-with-MCP: 80.7&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/p52b768jp07f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbrnod/jannano_a_4b_model_that_can_outperform_671b_on_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbrnod/jannano_a_4b_model_that_can_outperform_671b_on_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T04:24:03+00:00</published>
  </entry>
</feed>
