<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-22T19:21:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jgap0q</id>
    <title>SpatialLM: A large language model designed for spatial understanding</title>
    <updated>2025-03-21T06:43:28+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgap0q/spatiallm_a_large_language_model_designed_for/"&gt; &lt;img alt="SpatialLM: A large language model designed for spatial understanding" src="https://external-preview.redd.it/Z2F4NmRpYWFvenBlMV9xklPr-alq2N0OOZexCtU6lC7spKP7fvQP_oR6XFl8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94d9dd664854a15924490e41428c31c299e3851e" title="SpatialLM: A large language model designed for spatial understanding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9hvol38aozpe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgap0q/spatiallm_a_large_language_model_designed_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgap0q/spatiallm_a_large_language_model_designed_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T06:43:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhes9x</id>
    <title>Unsloth Fine-Tune Dataset Consequences</title>
    <updated>2025-03-22T18:13:50+00:00</updated>
    <author>
      <name>/u/AlienFlip</name>
      <uri>https://old.reddit.com/user/AlienFlip</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am following the &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B"&gt;Unsloth Gemma3 Notebook&lt;/a&gt;.ipynb)&lt;/p&gt; &lt;p&gt;The dataset which I am fine-tuning to consists of this sort of structure:&lt;/p&gt; &lt;p&gt;&lt;code&gt;dataset.json:&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[ {'conversations': [ { 'content': '...?', 'role': 'user' }, { 'content': '...', 'role': 'assistant' }, { 'content': '...?', 'role': 'user' }, { 'content': '...', 'role': 'assistant' } ]}, {'conversations': [ { 'content': '...?', 'role': 'user' }, { 'content': '...', 'role': 'assistant' } ]}, ... ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I.e. there is a mix of long and short conversations.&lt;/p&gt; &lt;p&gt;What sort of impact will this have on the quality of the fine-tuned model, and why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlienFlip"&gt; /u/AlienFlip &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhes9x/unsloth_finetune_dataset_consequences/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhes9x/unsloth_finetune_dataset_consequences/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhes9x/unsloth_finetune_dataset_consequences/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T18:13:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh9d2p</id>
    <title>Anyone have any luck buying GPUs from Alibaba? (not aliexpress)</title>
    <updated>2025-03-22T14:14:31+00:00</updated>
    <author>
      <name>/u/LanceThunder</name>
      <uri>https://old.reddit.com/user/LanceThunder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was looking around at cards on Alibaba and they sort of look almost legit. The sellers have been on there for a long time and have decent reviews. its a huge success full site so there has to be at least some legit GPU sellers, right? But the prices range from &amp;quot;slightly low&amp;quot; to &amp;quot;too good to be true&amp;quot;. is there any way to buy from that site without getting burned or taking big risks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LanceThunder"&gt; /u/LanceThunder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh9d2p/anyone_have_any_luck_buying_gpus_from_alibaba_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh9d2p/anyone_have_any_luck_buying_gpus_from_alibaba_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh9d2p/anyone_have_any_luck_buying_gpus_from_alibaba_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T14:14:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgzpdb</id>
    <title>What are you using local LLMs for? How do they compare to the big tech offerings?</title>
    <updated>2025-03-22T03:43:40+00:00</updated>
    <author>
      <name>/u/TedHoliday</name>
      <uri>https://old.reddit.com/user/TedHoliday</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m just curious what all people are using local LLMs for. For me personally, I use Claude daily at work I like the idea of running an LLM locally, but I know it would be less accurate on my single PC with one single RTX 4090. &lt;/p&gt; &lt;p&gt;I like the idea of not being subject to the constantly changing pricing models and worrying about how many tokens Iâ€™ve used up, but I feel like even like 5% more accurate code is worth it due to the time it can save.&lt;/p&gt; &lt;p&gt;So Iâ€™m just curious what people are using them for, and how are they now compared to the big players (and with what hardware)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TedHoliday"&gt; /u/TedHoliday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgzpdb/what_are_you_using_local_llms_for_how_do_they/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgzpdb/what_are_you_using_local_llms_for_how_do_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgzpdb/what_are_you_using_local_llms_for_how_do_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T03:43:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh8rk7</id>
    <title>Great performance even quantize to q8q4 for gemma 3 4B</title>
    <updated>2025-03-22T13:44:59+00:00</updated>
    <author>
      <name>/u/Robert__Sinclair</name>
      <uri>https://old.reddit.com/user/Robert__Sinclair</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just finished quantizing gemma 3 4B and I find it great even when heavily quantized like the &amp;quot;q8q4&amp;quot; version.&lt;/p&gt; &lt;p&gt;If you have a memory constrained system or just want CPU inference or perhaps on mobile devices, give it a try: &lt;a href="https://huggingface.co/ZeroWw/gemma-3-4b-it-abliterated-GGUF"&gt;ZeroWw/gemma-3-4b-it-abliterated-GGUF Â· Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Robert__Sinclair"&gt; /u/Robert__Sinclair &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh8rk7/great_performance_even_quantize_to_q8q4_for_gemma/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh8rk7/great_performance_even_quantize_to_q8q4_for_gemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh8rk7/great_performance_even_quantize_to_q8q4_for_gemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T13:44:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jha1vl</id>
    <title>Local LoRA + RAG Academic Writing Setup â€“ Build Check Before I Pull the Trigger</title>
    <updated>2025-03-22T14:46:16+00:00</updated>
    <author>
      <name>/u/wobbley-boots</name>
      <uri>https://old.reddit.com/user/wobbley-boots</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, just chasing a bit of feedback while I'm finalising a build. I'm setting up a local AI writing system to automate the structure and style of academic work. Iâ€™m not training it to learn knowledge or reason, just to mimic how I write using a dataset of my own essays and theses (formatted in JSONL). Iâ€™ll be fine-tuning a small model like Phi-2 or OpenLLaMA 3B using LoRA or QLoRA, and keeping that completely separate from a RAG setup that pulls content from a chunked academic library (~100+ PDFs split into 5KB txt files). The idea is to feed it the right research chunks, and have it paraphrase in my voice without hallucinating or plagiarising. Itâ€™s basically a local ghostwriter with me in the driverâ€™s seat.&lt;/p&gt; &lt;p&gt;Iâ€™m building this on an i9-14900KF with 96GB DDR5-5600 (2x48GB Corsair Vengeance), an MSI MAG Z790 Tomahawk WiFi board, RTX 3070 8GB, DeepCool AK620 Digital air cooler, Samsung 980 Pro 1TB SSD, and decent airflow (6-fan white case). Everything will run locally with CPU offloading where needed. No full-model training, no 13B model insanityâ€”just stable overnight LoRA fine-tunes and section-by-section writing using a RAG-fed workflow.&lt;/p&gt; &lt;p&gt;Just wondering if this sounds like a balanced setup for what Iâ€™m doingâ€”fine-tuning small models locally and generating paraphrased academic content from chunked research via RAG. Any issues I should expect with the 2x48GB RAM setup on Z790, or LoRA/QLoRA performance on this sort of hardware? Appreciate any real-world experience or heads-ups before I finalise it. Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wobbley-boots"&gt; /u/wobbley-boots &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jha1vl/local_lora_rag_academic_writing_setup_build_check/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jha1vl/local_lora_rag_academic_writing_setup_build_check/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jha1vl/local_lora_rag_academic_writing_setup_build_check/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T14:46:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgl41s</id>
    <title>Tencent introduces Hunyuan-T1, their large reasoning model. Competing with DeepSeek-R1!</title>
    <updated>2025-03-21T16:36:11+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgl41s/tencent_introduces_hunyuant1_their_large/"&gt; &lt;img alt="Tencent introduces Hunyuan-T1, their large reasoning model. Competing with DeepSeek-R1!" src="https://preview.redd.it/vcb57bt1m2qe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=374682829fe92002bc36926e45cf71896aada6ea" title="Tencent introduces Hunyuan-T1, their large reasoning model. Competing with DeepSeek-R1!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to their blog post here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vcb57bt1m2qe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgl41s/tencent_introduces_hunyuant1_their_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgl41s/tencent_introduces_hunyuant1_their_large/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T16:36:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhaiyk</id>
    <title>(Update) Generative AI project template (it now includes Ollama)</title>
    <updated>2025-03-22T15:07:33+00:00</updated>
    <author>
      <name>/u/aminedjeghri</name>
      <uri>https://old.reddit.com/user/aminedjeghri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;For those interested in a project template that integrates generative AI, Streamlit, UV, CI/CD, automatic documentation, and more, Iâ€™ve updated my template to now include &lt;strong&gt;Ollama&lt;/strong&gt;. It even includes tests in CI/CD for a small model (Qwen 2.5 with 0.5B parameters).&lt;/p&gt; &lt;p&gt;Hereâ€™s the GitHub project:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/AmineDjeghri/generative-ai-project-template"&gt;Generative AI Project Template&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Engineering tools&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- [x] Use UV to manage packages&lt;/p&gt; &lt;p&gt;- [x] pre-commit hooks: use ``ruff`` to ensure the code quality &amp;amp; ``detect-secrets`` to scan the secrets in the code.&lt;/p&gt; &lt;p&gt;- [x] Logging using loguru (with colors)&lt;/p&gt; &lt;p&gt;- [x] Pytest for unit tests&lt;/p&gt; &lt;p&gt;- [x] Dockerized project (Dockerfile &amp;amp; docker-compose).&lt;/p&gt; &lt;p&gt;- [x] Streamlit (frontend) &amp;amp; FastAPI (backend)&lt;/p&gt; &lt;p&gt;- [x] Make commands to handle everything for you: install, run, test&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AI tools&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- [x] LLM running locally with Ollama or in the cloud with any LLM provider (LiteLLM)&lt;/p&gt; &lt;p&gt;- [x] Information extraction and Question answering from documents&lt;/p&gt; &lt;p&gt;- [x] Chat to test the AI system&lt;/p&gt; &lt;p&gt;- [x] Efficient async code using asyncio.&lt;/p&gt; &lt;p&gt;- [x] AI Evaluation framework: using Promptfoo, Ragas &amp;amp; more...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CI/CD &amp;amp; Maintenance tools&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- [x] CI/CD pipelines: ``.github/workflows`` for GitHub (Testing the AI system, local models with Ollama and the dockerized app)&lt;/p&gt; &lt;p&gt;- [x] Local CI/CD pipelines: GitHub Actions using ``github act``&lt;/p&gt; &lt;p&gt;- [x] GitHub Actions for deploying to GitHub Pages with mkdocs gh-deploy&lt;/p&gt; &lt;p&gt;- [x] Dependabot ``.github/dependabot.yml`` for automatic dependency and security updates&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Documentation tools&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- [x] Wiki creation and setup of documentation website using Mkdocs&lt;/p&gt; &lt;p&gt;- [x] GitHub Pages deployment using mkdocs gh-deploy plugin&lt;/p&gt; &lt;p&gt;Feel free to check it out, contribute, or use it for your own AI projects! Let me know if you have any questions or feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aminedjeghri"&gt; /u/aminedjeghri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhaiyk/update_generative_ai_project_template_it_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhaiyk/update_generative_ai_project_template_it_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhaiyk/update_generative_ai_project_template_it_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T15:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhacpq</id>
    <title>AI-powered Resume Tailoring application using Ollama and Langchain</title>
    <updated>2025-03-22T15:00:05+00:00</updated>
    <author>
      <name>/u/Maleficent-Penalty50</name>
      <uri>https://old.reddit.com/user/Maleficent-Penalty50</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhacpq/aipowered_resume_tailoring_application_using/"&gt; &lt;img alt="AI-powered Resume Tailoring application using Ollama and Langchain" src="https://external-preview.redd.it/cDU5MTExOHA5OXFlMZy3YkGsqoarT07V-vPPMaS_PHa1WWz4P1Vvd0jG5Jtl.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b9882bd7bc18a11f534952ffaa2854a71a1a195" title="AI-powered Resume Tailoring application using Ollama and Langchain" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent-Penalty50"&gt; /u/Maleficent-Penalty50 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/29hof18p99qe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhacpq/aipowered_resume_tailoring_application_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhacpq/aipowered_resume_tailoring_application_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T15:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgio2g</id>
    <title>Qwen 3 is coming soon!</title>
    <updated>2025-03-21T14:53:25+00:00</updated>
    <author>
      <name>/u/themrzmaster</name>
      <uri>https://old.reddit.com/user/themrzmaster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/36878"&gt;https://github.com/huggingface/transformers/pull/36878&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/themrzmaster"&gt; /u/themrzmaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgio2g/qwen_3_is_coming_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgio2g/qwen_3_is_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgio2g/qwen_3_is_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T14:53:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh1m45</id>
    <title>Why Do I Feel Poor Each Time I Decide to Buy a New GPU Even Though I Make More Money?</title>
    <updated>2025-03-22T05:40:29+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean for God sake, this curse has been haunting me for decades now. The first time I bought a GPU with my own money, I had to dream for it for months, saving money every month for my scholarship. When I went to buy my dream GPU, prices increased and I ended up buying a mid-range NVIDIA card (I had to buy other PC component which were expensive). Then years later I got busy with work and had Playstation, so I didn't really need a good PC, couple with the fact that laptop prices were getting cheaper and performant, I just didn't need to build a new rig.&lt;/p&gt; &lt;p&gt;Fast forward a few year, and my old dream to create my own games came back strong, and I decided to learn (seriously this time) 3D modeling and rendering. There is just something satisfying fooling untrained (or trained) eyes looking at a CGI production and thinking it's real.&lt;br /&gt; That's when I decided to build a new PC. Alas, the new age of crypto reaches its peak and yeah.. shortage of GPUs. Then, I felt poor again even after my several years of work and money saving.&lt;/p&gt; &lt;p&gt;Then COVID hits, and an RTX3090 cost $4000, if you get your hand on one. I bought multiple parts from different countries just to minimize my spending, and I felt very poor.&lt;/p&gt; &lt;p&gt;Which brings me to today. I want to build a new rig from my new passion; tinkering with AI. Alas, I have the money to buy any GPU I want, but my damn rational brain isn't allowing me!!! It's too expensive.. Am I insane? An RTX5090 at a price equivalent to a second hand car is NOT A SMART PURCHASE. And, it only comes with 32GB of VRAM. I'd still run the same models my now old 3090 can run...&lt;/p&gt; &lt;p&gt;In short, no matter how much my income increases over the years, I will always feel poor when I want to buy an new GPU ðŸ˜­ðŸ˜­ðŸ˜­&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh1m45/why_do_i_feel_poor_each_time_i_decide_to_buy_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh1m45/why_do_i_feel_poor_each_time_i_decide_to_buy_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh1m45/why_do_i_feel_poor_each_time_i_decide_to_buy_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T05:40:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhe3lq</id>
    <title>gemma3 vision</title>
    <updated>2025-03-22T17:44:22+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ok im gonna write in all lower case because the post keeps getting auto modded. its almost like local llama encourage low effort post. super annoying. imagine there was a fully compliant gemma3 vision model, wouldn't that be nice?&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/X-Ray_Alpha"&gt;https://huggingface.co/SicariusSicariiStuff/X-Ray_Alpha&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhe3lq/gemma3_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhe3lq/gemma3_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhe3lq/gemma3_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T17:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh01fi</id>
    <title>Can someone ELI5 what makes NVIDIA a monopoly in AI race?</title>
    <updated>2025-03-22T04:02:06+00:00</updated>
    <author>
      <name>/u/Trysem</name>
      <uri>https://old.reddit.com/user/Trysem</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I heard somewhere it's cuda,then why some other companies like AMD is not making something like cuda of their own?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trysem"&gt; /u/Trysem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh01fi/can_someone_eli5_what_makes_nvidia_a_monopoly_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh01fi/can_someone_eli5_what_makes_nvidia_a_monopoly_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh01fi/can_someone_eli5_what_makes_nvidia_a_monopoly_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T04:02:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh0ovc</id>
    <title>MoshiVis by kyutai - first open-source real-time speech model that can talk about images</title>
    <updated>2025-03-22T04:40:57+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh0ovc/moshivis_by_kyutai_first_opensource_realtime/"&gt; &lt;img alt="MoshiVis by kyutai - first open-source real-time speech model that can talk about images" src="https://external-preview.redd.it/Y3ptd2t6NGE3NnFlMax15wl1W2mX3SQ6hWixr4c-XUrnbjt3Ig1vm4pgUatm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53210f67e2cb9507383ccec4a8ff094869da2fcb" title="MoshiVis by kyutai - first open-source real-time speech model that can talk about images" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/v86w8w4a76qe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh0ovc/moshivis_by_kyutai_first_opensource_realtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh0ovc/moshivis_by_kyutai_first_opensource_realtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T04:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgp6sw</id>
    <title>China modified 4090s with 48gb sold cheaper than RTX 5090 - water cooled around 3400 usd</title>
    <updated>2025-03-21T19:25:39+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgp6sw/china_modified_4090s_with_48gb_sold_cheaper_than/"&gt; &lt;img alt="China modified 4090s with 48gb sold cheaper than RTX 5090 - water cooled around 3400 usd" src="https://b.thumbs.redditmedia.com/Uv9b5l37Z3HDbLOdsI_RvWZEDLPnFUNe8L0-bY4NmCE.jpg" title="China modified 4090s with 48gb sold cheaper than RTX 5090 - water cooled around 3400 usd" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jgp6sw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgp6sw/china_modified_4090s_with_48gb_sold_cheaper_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgp6sw/china_modified_4090s_with_48gb_sold_cheaper_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T19:25:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhf6x3</id>
    <title>Has anyone switched from remote models (claude, etc.) models to local? Meaning did your investment pay off?</title>
    <updated>2025-03-22T18:31:34+00:00</updated>
    <author>
      <name>/u/TumbleweedDeep825</name>
      <uri>https://old.reddit.com/user/TumbleweedDeep825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Obviously a 70b or 32b model won't be as good as Claude API, on the other hand, many are spending $10 to $30+ per day on the API, so it could be a lot cheaper.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TumbleweedDeep825"&gt; /u/TumbleweedDeep825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhf6x3/has_anyone_switched_from_remote_models_claude_etc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhf6x3/has_anyone_switched_from_remote_models_claude_etc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhf6x3/has_anyone_switched_from_remote_models_claude_etc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T18:31:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh3i7k</id>
    <title>1.5B surprises o1-preview math benchmarks with this new finding</title>
    <updated>2025-03-22T07:59:05+00:00</updated>
    <author>
      <name>/u/Different-Olive-8745</name>
      <uri>https://old.reddit.com/user/Different-Olive-8745</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh3i7k/15b_surprises_o1preview_math_benchmarks_with_this/"&gt; &lt;img alt="1.5B surprises o1-preview math benchmarks with this new finding" src="https://external-preview.redd.it/v81uCWR00P0A7u3BP_mTIasdD33pY9M4769VjQUIiSw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff86f87a266a03096a160a58098c8ced38e6b00c" title="1.5B surprises o1-preview math benchmarks with this new finding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Olive-8745"&gt; /u/Different-Olive-8745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2503.16219"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh3i7k/15b_surprises_o1preview_math_benchmarks_with_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh3i7k/15b_surprises_o1preview_math_benchmarks_with_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T07:59:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh4r72</id>
    <title>Deepseek (the website) now has a optout like the others, earlier they didn't have.</title>
    <updated>2025-03-22T09:34:02+00:00</updated>
    <author>
      <name>/u/Yes_but_I_think</name>
      <uri>https://old.reddit.com/user/Yes_but_I_think</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4r72/deepseek_the_website_now_has_a_optout_like_the/"&gt; &lt;img alt="Deepseek (the website) now has a optout like the others, earlier they didn't have." src="https://b.thumbs.redditmedia.com/EtrziXc9-Pm757HjCIYTYkRO79iwGVPKkOldgIO-yAM.jpg" title="Deepseek (the website) now has a optout like the others, earlier they didn't have." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vooy8j4gn7qe1.png?width=1042&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a82a68facaa4f88b976ef720f67abb57811b25b7"&gt;https://preview.redd.it/vooy8j4gn7qe1.png?width=1042&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a82a68facaa4f88b976ef720f67abb57811b25b7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Yes_but_I_think"&gt; /u/Yes_but_I_think &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4r72/deepseek_the_website_now_has_a_optout_like_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4r72/deepseek_the_website_now_has_a_optout_like_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4r72/deepseek_the_website_now_has_a_optout_like_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T09:34:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhbxr9</id>
    <title>Token impact by long-Chain-of-Thought Reasoning Models</title>
    <updated>2025-03-22T16:10:20+00:00</updated>
    <author>
      <name>/u/dubesor86</name>
      <uri>https://old.reddit.com/user/dubesor86</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhbxr9/token_impact_by_longchainofthought_reasoning/"&gt; &lt;img alt="Token impact by long-Chain-of-Thought Reasoning Models" src="https://preview.redd.it/hxrz73n2l9qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be6c35234bc628ab1e2c263ab5a9a084397d8793" title="Token impact by long-Chain-of-Thought Reasoning Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dubesor86"&gt; /u/dubesor86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hxrz73n2l9qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhbxr9/token_impact_by_longchainofthought_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhbxr9/token_impact_by_longchainofthought_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T16:10:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh6j47</id>
    <title>ðŸš€ Running vLLM with 2 GPUs on my home server - automated in minutes!</title>
    <updated>2025-03-22T11:39:27+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6j47/running_vllm_with_2_gpus_on_my_home_server/"&gt; &lt;img alt="ðŸš€ Running vLLM with 2 GPUs on my home server - automated in minutes!" src="https://b.thumbs.redditmedia.com/ScRkwidkM9zx6RUmuOk3MXdlYXgdVEQFsbGekaQlRYs.jpg" title="ðŸš€ Running vLLM with 2 GPUs on my home server - automated in minutes!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve got vLLM running on a dual-GPU home server, complete with my Sbnb Linux distro tailored for AI, Grafana GPU utilization dashboards, and automated benchmarking - all set up in just a few minutes thanks to Ansible.&lt;/p&gt; &lt;p&gt;If youâ€™re into LLMs, home labs, or automation, I put together a detailed how-to here: ðŸ”— &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-VLLM.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-VLLM.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to help if anyone wants to get started!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jh6j47"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6j47/running_vllm_with_2_gpus_on_my_home_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6j47/running_vllm_with_2_gpus_on_my_home_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T11:39:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh4s2h</id>
    <title>LLama.cpp smillar speed but in pure Rust, local LLM inference alternatives.</title>
    <updated>2025-03-22T09:35:49+00:00</updated>
    <author>
      <name>/u/LewisJin</name>
      <uri>https://old.reddit.com/user/LewisJin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a long time, every time I want to run a LLM locally, the only choice is llama.cpp or other tools with magical optimization. However, llama.cpp is not always easy to set up especially when it comes to a new model and new architecture. Without help from the community, you can hardly convert a new model into GGUF. Even if you can, it is still very hard to make it work in llama.cpp.&lt;/p&gt; &lt;p&gt;Now, we can have an alternative way to infer LLM locally with maximum speed. And it's in pure Rust! No C++ needed. With pyo3 you can still call it with python, but Rust is easy enough, right?&lt;/p&gt; &lt;p&gt;I made a minimal example the same as llama.cpp chat cli. It runs 6 times faster than using pytorch, based on the Candle framework.Check it out:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lucasjinreal/Crane"&gt;https://github.com/lucasjinreal/Crane&lt;/a&gt;&lt;/p&gt; &lt;p&gt;next I would adding Spark-TTS and &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;Orpheus-TTS&lt;/a&gt; support, if you interested in Rust and fast inference, please join to develop with rust!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LewisJin"&gt; /u/LewisJin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4s2h/llamacpp_smillar_speed_but_in_pure_rust_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4s2h/llamacpp_smillar_speed_but_in_pure_rust_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4s2h/llamacpp_smillar_speed_but_in_pure_rust_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T09:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgqmlr</id>
    <title>"If we confuse users enough, they will overpay"</title>
    <updated>2025-03-21T20:26:10+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqmlr/if_we_confuse_users_enough_they_will_overpay/"&gt; &lt;img alt="&amp;quot;If we confuse users enough, they will overpay&amp;quot;" src="https://preview.redd.it/epfkc4xxq3qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f18b9505527bc8ed40557544a084be28952fd9b" title="&amp;quot;If we confuse users enough, they will overpay&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/epfkc4xxq3qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqmlr/if_we_confuse_users_enough_they_will_overpay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqmlr/if_we_confuse_users_enough_they_will_overpay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T20:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh7c6e</id>
    <title>My 4x3090 eGPU collection</title>
    <updated>2025-03-22T12:28:25+00:00</updated>
    <author>
      <name>/u/Threatening-Silence-</name>
      <uri>https://old.reddit.com/user/Threatening-Silence-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh7c6e/my_4x3090_egpu_collection/"&gt; &lt;img alt="My 4x3090 eGPU collection" src="https://b.thumbs.redditmedia.com/tuwbOdIfpLg-K_qo2ArzC4oMvVIIdECI4tmNxUjTuKA.jpg" title="My 4x3090 eGPU collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 3 more 3090s ready to hook up to the 2nd Thunderbolt port in the back when I get the UT4g docks in. &lt;/p&gt; &lt;p&gt;Will need to find an area with more room though ðŸ˜…&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Threatening-Silence-"&gt; /u/Threatening-Silence- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jh7c6e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh7c6e/my_4x3090_egpu_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh7c6e/my_4x3090_egpu_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T12:28:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhdpjk</id>
    <title>Fallen Gemma3 4B 12B 27B - An unholy trinity with no positivity! For users, mergers and cooks!</title>
    <updated>2025-03-22T17:27:35+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not a complete decensor tune, but it should be absent of positivity.&lt;/p&gt; &lt;p&gt;Vision works.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Fallen-Gemma3-4B-v1"&gt;https://huggingface.co/TheDrummer/Fallen-Gemma3-4B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Fallen-Gemma3-12B-v1"&gt;https://huggingface.co/TheDrummer/Fallen-Gemma3-12B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Fallen-Gemma3-27B-v1"&gt;https://huggingface.co/TheDrummer/Fallen-Gemma3-27B-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhdpjk/fallen_gemma3_4b_12b_27b_an_unholy_trinity_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhdpjk/fallen_gemma3_4b_12b_27b_an_unholy_trinity_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhdpjk/fallen_gemma3_4b_12b_27b_an_unholy_trinity_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T17:27:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh6lsx</id>
    <title>OpenAI released GPT-4.5 and O1 Pro via their API and it looks like a weird decision.</title>
    <updated>2025-03-22T11:44:18+00:00</updated>
    <author>
      <name>/u/lessis_amess</name>
      <uri>https://old.reddit.com/user/lessis_amess</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6lsx/openai_released_gpt45_and_o1_pro_via_their_api/"&gt; &lt;img alt="OpenAI released GPT-4.5 and O1 Pro via their API and it looks like a weird decision." src="https://preview.redd.it/x942twbra8qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79683f47809a02571ff90500acb5d28a046d6940" title="OpenAI released GPT-4.5 and O1 Pro via their API and it looks like a weird decision." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;O1 Pro costs 33 times more than Claude 3.7 Sonnet, yet in many cases delivers less capability. GPT-4.5 costs 25 times more and itâ€™s an old model with a cut-off date from November.&lt;/p&gt; &lt;p&gt;Why release old, overpriced models to developers who care most about cost efficiency?&lt;/p&gt; &lt;p&gt;This isn't an accident.&lt;/p&gt; &lt;p&gt;It's anchoring.&lt;/p&gt; &lt;p&gt;Anchoring works by establishing an initial reference point. Once that reference exists, subsequent judgments revolve around it.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Show something expensive.&lt;/li&gt; &lt;li&gt;Show something less expensive.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The second thing seems like a bargain.&lt;/p&gt; &lt;p&gt;The expensive API models reset our expectations. For years, AI got cheaper while getting smarter. OpenAI wants to break that pattern. They're saying high intelligence costs money. Big models cost money. They're claiming they don't even profit from these prices.&lt;/p&gt; &lt;p&gt;When they release their next frontier model at a &amp;quot;lower&amp;quot; price, you'll think it's reasonable. But it will still cost more than what we paid before this reset. The new &amp;quot;cheap&amp;quot; will be expensive by last year's standards.&lt;/p&gt; &lt;p&gt;OpenAI claims these models lose money. Maybe. But they're conditioning the market to accept higher prices for whatever comes next. The API release is just the first move in a longer game.&lt;/p&gt; &lt;p&gt;This was not a confused move. Itâ€™s smart business. (i'm VERY happy we have open-source)&lt;/p&gt; &lt;p&gt;&lt;a href="https://ivelinkozarev.substack.com/p/the-pricing-of-gpt-45-and-o1-pro"&gt;https://ivelinkozarev.substack.com/p/the-pricing-of-gpt-45-and-o1-pro&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lessis_amess"&gt; /u/lessis_amess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x942twbra8qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6lsx/openai_released_gpt45_and_o1_pro_via_their_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6lsx/openai_released_gpt45_and_o1_pro_via_their_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T11:44:18+00:00</published>
  </entry>
</feed>
