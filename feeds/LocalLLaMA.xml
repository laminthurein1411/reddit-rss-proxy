<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-17T00:27:34+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jcdsat</id>
    <title>A tip to make QwQ less verbose</title>
    <updated>2025-03-16T04:37:40+00:00</updated>
    <author>
      <name>/u/Aggressive-Stop-9091</name>
      <uri>https://old.reddit.com/user/Aggressive-Stop-9091</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my experience, QwQ tends to overthink because it's fine-tuned to interpret the writer's intentions. One effective way to minimize this is by providing examples. QwQ is an excellent few-shot learner that doesnt merely copy the examples, but also and when given a few well-crafted examples, it can generate a more articulate prompt than I initially wrote (which I then included in subsequent generations). Yes, I know this is prompt engineering 101, but what I find interesting about QwQ is that, unlike most local models I've tried, it doesn't get fixated on wording or style. Instead, it focuses on understanding the 'bigger picture' in the examples, like it had some sort 'meta learning'. For instance, I was working on condensing a research paper into a highly engaging and conversational format. The model when provided examples was able to outline what I wanted on its own, based on my instruction and the examples:&lt;/p&gt; &lt;p&gt;Hook: Why can't you stop scrolling TikTok?&lt;/p&gt; &lt;p&gt;Problem: Personalized content triggers brain regions linked to attention and reward.&lt;/p&gt; &lt;p&gt;Mechanism: DMN activation, VTA activity, reduced self-control regions coupling.&lt;/p&gt; &lt;p&gt;Outcome: Compulsive use, especially in those with low self-control.&lt;/p&gt; &lt;p&gt;Significance: Algorithm exploits neural pathways, need for understanding tech addiction.&lt;/p&gt; &lt;p&gt;Needless to say, it doesn't always work perfectly, but in my experience, it significantly improves the output. (The engine I use is ExLlama, and I follow the recommended settings for the model.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aggressive-Stop-9091"&gt; /u/Aggressive-Stop-9091 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcdsat/a_tip_to_make_qwq_less_verbose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcdsat/a_tip_to_make_qwq_less_verbose/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcdsat/a_tip_to_make_qwq_less_verbose/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T04:37:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcvni3</id>
    <title>Tool calls DURING reasoning?</title>
    <updated>2025-03-16T21:01:57+00:00</updated>
    <author>
      <name>/u/RandomRobot01</name>
      <uri>https://old.reddit.com/user/RandomRobot01</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone aware of any models that can perform one or more tool/function calls DURING the reasoning process? I am just curious as I have been thinking about it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomRobot01"&gt; /u/RandomRobot01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcvni3/tool_calls_during_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcvni3/tool_calls_during_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcvni3/tool_calls_during_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T21:01:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcwbim</id>
    <title>How vision llm works? What model actually see?</title>
    <updated>2025-03-16T21:31:41+00:00</updated>
    <author>
      <name>/u/uti24</name>
      <uri>https://old.reddit.com/user/uti24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So my question is: What does an LLM actually &amp;quot;see&amp;quot; in an image that I upload?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Does it just extract a general concept of the image using a vision transformer, meaning it has only limited information?&lt;/li&gt; &lt;li&gt;Or is the image loaded into memory the whole time, allowing the LLM to analyze any part of it?&lt;/li&gt; &lt;li&gt;Or does it rely on the output of a separate perceptron that detects objects and features, providing only a structured list rather than a full visual understanding?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The reason I ask is that LLMs seem to lack real spatial awareness when dealing with images.&lt;/p&gt; &lt;p&gt;For example, if I provide an image of a black cat on a brown table and then ask the LLM to recreate it using JavaScript and Canvas - just with simple shapes but maintaining accurate positions: it fails. Instead of correctly placing objects in the right locations and sizes, it only captures the &lt;em&gt;concept&lt;/em&gt; of the image.&lt;/p&gt; &lt;p&gt;Iâ€™m not talking about detailed image reconstructionâ€”I'd be happy if the LLM could just represent objects as bounding boxes in the correct positions with proper(is) scale. But it seems incapable of doing that.&lt;/p&gt; &lt;p&gt;I've tested this with ChatGPT, Grok, and Gemma 3 27B, and the results are similar: they draw concept of the image I gave originally, without any details. And I tried to convince llm to draw features where they should be on the canvas, llm just don't understand.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uti24"&gt; /u/uti24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcwbim/how_vision_llm_works_what_model_actually_see/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcwbim/how_vision_llm_works_what_model_actually_see/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcwbim/how_vision_llm_works_what_model_actually_see/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T21:31:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jch5go</id>
    <title>Unvibe: Generate code that pass Unit-Tests with Qwen-coder 7B</title>
    <updated>2025-03-16T08:41:53+00:00</updated>
    <author>
      <name>/u/inkompatible</name>
      <uri>https://old.reddit.com/user/inkompatible</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jch5go/unvibe_generate_code_that_pass_unittests_with/"&gt; &lt;img alt="Unvibe: Generate code that pass Unit-Tests with Qwen-coder 7B" src="https://external-preview.redd.it/AoihjkMZZrKFb6oZh0_uoP159L4b86Wv3R2wyjBYumc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a409d905bf15acbd562decd0487e21014e17c40f" title="Unvibe: Generate code that pass Unit-Tests with Qwen-coder 7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkompatible"&gt; /u/inkompatible &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://claudio.uk/posts/unvibe.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jch5go/unvibe_generate_code_that_pass_unittests_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jch5go/unvibe_generate_code_that_pass_unittests_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T08:41:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcxaia</id>
    <title>RTX PRO 6000 X Blackwell 96GB 'Gaming/Virtual Production' performance leaked</title>
    <updated>2025-03-16T22:15:36+00:00</updated>
    <author>
      <name>/u/chillinewman</name>
      <uri>https://old.reddit.com/user/chillinewman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcxaia/rtx_pro_6000_x_blackwell_96gb_gamingvirtual/"&gt; &lt;img alt="RTX PRO 6000 X Blackwell 96GB 'Gaming/Virtual Production' performance leaked" src="https://b.thumbs.redditmedia.com/KGezX9Oi_egcbfGjz1XTDz7SUxPp6LonddiDEQuXvXI.jpg" title="RTX PRO 6000 X Blackwell 96GB 'Gaming/Virtual Production' performance leaked" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chillinewman"&gt; /u/chillinewman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jcxaia"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcxaia/rtx_pro_6000_x_blackwell_96gb_gamingvirtual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcxaia/rtx_pro_6000_x_blackwell_96gb_gamingvirtual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T22:15:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc9meu</id>
    <title>Who's still running ancient models?</title>
    <updated>2025-03-16T00:44:12+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had to take a pause from my experiments today, gemma3, mistralsmall, phi4, qwq, qwen, etc and marvel at how good they are for their size. A year ago most of us thought that we needed 70B to kick ass. 14-32B is punching super hard. I'm deleting my Q2/Q3 llama405B, and deepseek dyanmic quants.&lt;/p&gt; &lt;p&gt;I'm going to re-download guanaco, dolphin-llama2, vicuna, wizardLM, nous-hermes-llama2, etc&lt;br /&gt; For old times sake. It's amazing how far we have come and how fast. Some of these are not even 2 years old! Just a year plus! I'm going to keep some ancient model and run them so I can remember and don't forget and to also have more appreciation for what we have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc9meu/whos_still_running_ancient_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc9meu/whos_still_running_ancient_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jc9meu/whos_still_running_ancient_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T00:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcchtq</id>
    <title>Baidu releases X1, a (closed?) model that matches R1 and ERNIE 4.5, that matches GPT 4.5</title>
    <updated>2025-03-16T03:20:25+00:00</updated>
    <author>
      <name>/u/ortegaalfredo</name>
      <uri>https://old.reddit.com/user/ortegaalfredo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Baidu_Inc/status/1901094083508220035"&gt;https://x.com/Baidu_Inc/status/1901094083508220035&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ortegaalfredo"&gt; /u/ortegaalfredo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcchtq/baidu_releases_x1_a_closed_model_that_matches_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcchtq/baidu_releases_x1_a_closed_model_that_matches_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcchtq/baidu_releases_x1_a_closed_model_that_matches_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T03:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcsvys</id>
    <title>How much does flash attention affect intelligence in reasoning models like QwQ</title>
    <updated>2025-03-16T19:01:45+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcsvys/how_much_does_flash_attention_affect_intelligence/"&gt; &lt;img alt="How much does flash attention affect intelligence in reasoning models like QwQ" src="https://b.thumbs.redditmedia.com/Jz6Gd8vqdtutQtHNWxINBKeN_53yYOJwyOU1ALk_7TU.jpg" title="How much does flash attention affect intelligence in reasoning models like QwQ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/oaw93jp5n3pe1.png?width=701&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=563287c572752a94e3c3236af0085231cf1856f9"&gt;https://preview.redd.it/oaw93jp5n3pe1.png?width=701&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=563287c572752a94e3c3236af0085231cf1856f9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Im using QwQ in LM Studio (yes i know abliteration degrades intelligence slightly too but I'm not too worried about that) and flash attention drastically improve memory use and speed to an unbelievable extent but my instinct says surely that big of memory improvement comes with pretty decent intelligence loss, right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcsvys/how_much_does_flash_attention_affect_intelligence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcsvys/how_much_does_flash_attention_affect_intelligence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcsvys/how_much_does_flash_attention_affect_intelligence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T19:01:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcggwb</id>
    <title>Qwen2 72b VL is actually really impressive. It's not perfect, but for a local model I'm certainly impressed (more info in comments)</title>
    <updated>2025-03-16T07:48:53+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcggwb/qwen2_72b_vl_is_actually_really_impressive_its/"&gt; &lt;img alt="Qwen2 72b VL is actually really impressive. It's not perfect, but for a local model I'm certainly impressed (more info in comments)" src="https://preview.redd.it/1t90zqok80pe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e53739cdf1ff5da3fd4597e535c09945ad40c21" title="Qwen2 72b VL is actually really impressive. It's not perfect, but for a local model I'm certainly impressed (more info in comments)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1t90zqok80pe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcggwb/qwen2_72b_vl_is_actually_really_impressive_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcggwb/qwen2_72b_vl_is_actually_really_impressive_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T07:48:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcm5p2</id>
    <title>OCR + LLM for Invoice Extraction</title>
    <updated>2025-03-16T14:04:40+00:00</updated>
    <author>
      <name>/u/JumpyHouse</name>
      <uri>https://old.reddit.com/user/JumpyHouse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m starting to get a bit frustrated. Iâ€™m trying to develop a mobile application for an academic project involving invoice information extraction. Since this is a non-commercial project, Iâ€™m not allowed to use paid solutions like Google Vision or Azure AI Vision. So far, Iâ€™ve studied several possibilities, with the best being SuryaOCR/Marker for data extraction and Qwen 2.5 14B for data interpretation, along with some minor validation through RegEx.&lt;/p&gt; &lt;p&gt;Iâ€™m also limited in terms of options because I have an RX 6700 XT with 12GB of VRAM and canâ€™t run Hugging Face models due to the lack of support for my GPU. Iâ€™ve also tried a few Vision models like Llama 3.2 Vision and various OCR solutions like PaddleOCR , PyTesseract and EasyOCR and they all came short due to the lack of layout detection.&lt;/p&gt; &lt;p&gt;I wanted to ask if any of you have faced a similar situation and if you have any ideas or tips because Iâ€™m running out of options for data extraction. The invoices are predominantly Portuguese, so many OCR models end up lacking support for the layout detection.&lt;/p&gt; &lt;p&gt;Thank you in advance.ðŸ«¡&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JumpyHouse"&gt; /u/JumpyHouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcm5p2/ocr_llm_for_invoice_extraction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcm5p2/ocr_llm_for_invoice_extraction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcm5p2/ocr_llm_for_invoice_extraction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T14:04:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jctlbt</id>
    <title>Best Model under 15B parameters 2025</title>
    <updated>2025-03-16T19:31:47+00:00</updated>
    <author>
      <name>/u/AZ_1010</name>
      <uri>https://old.reddit.com/user/AZ_1010</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im looking for a model that can be used as a reliable daily driver and handle variety of use cases . Especially for my application (instruction following) where i generate medical reports based on output from other models (CNNs etc). I currently have an rx7600s laptop with 16gb ram running on vulkan llama.cpp, would appreciate to know which models performed the best for you :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AZ_1010"&gt; /u/AZ_1010 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jctlbt/best_model_under_15b_parameters_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jctlbt/best_model_under_15b_parameters_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jctlbt/best_model_under_15b_parameters_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T19:31:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcxb9w</id>
    <title>R2R v3.5.0 Release Notes</title>
    <updated>2025-03-16T22:16:28+00:00</updated>
    <author>
      <name>/u/docsoc1</name>
      <uri>https://old.reddit.com/user/docsoc1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're excited to announce R2R v3.5.0, featuring our new Deep Research API and significant improvements to our RAG capabilities.&lt;/p&gt; &lt;h2&gt;ðŸš€ Highlights&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Deep Research API: Multi-step reasoning system that fetches data from your knowledge base and the internet to deliver comprehensive, context-aware answers&lt;/li&gt; &lt;li&gt;Enhanced RAG Agent: More robust with new web search and scraping capabilities&lt;/li&gt; &lt;li&gt;Real-time Streaming: Server-side event streaming for visibility into the agent's thinking process and tool usage ## âœ¨ Key Features ### Research Capabilities&lt;/li&gt; &lt;li&gt;Research Agent: Specialized mode with advanced reasoning and computational tools&lt;/li&gt; &lt;li&gt;Extended Thinking: Toggle reasoning capabilities with optimized Claude model support&lt;/li&gt; &lt;li&gt;Improved Citations: Real-time citation identification with precise source attribution ### New Tools&lt;/li&gt; &lt;li&gt;Web Tools: Search external APIs and scrape web pages for up-to-date information&lt;/li&gt; &lt;li&gt;Research Tools: Reasoning, critique, and Python execution for complex analysis&lt;/li&gt; &lt;li&gt;RAG Tool: Leverage underlying RAG capabilities within the research agent ## ðŸ’¡ Usage Examples ### Basic RAG Mode ```python response = client.retrieval.agent( query=&amp;quot;What does deepseek r1 imply for the future of AI?&amp;quot;, generation_config={ &amp;quot;model&amp;quot;: &amp;quot;anthropic/claude-3-7-sonnet-20250219&amp;quot;, &amp;quot;extended_thinking&amp;quot;: True, &amp;quot;thinking_budget&amp;quot;: 4096, &amp;quot;temperature&amp;quot;: 1, &amp;quot;max_tokens_to_sample&amp;quot;: 16000, &amp;quot;stream&amp;quot;: True }, rag_tools=[&amp;quot;search_file_descriptions&amp;quot;, &amp;quot;search_file_knowledge&amp;quot;, &amp;quot;get_file_content&amp;quot;, &amp;quot;web_search&amp;quot;, &amp;quot;web_scrape&amp;quot;], mode=&amp;quot;rag&amp;quot; )&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Process the streaming events&lt;/h1&gt; &lt;p&gt;for event in response: if isinstance(event, ThinkingEvent): print(f&amp;quot;ðŸ§  Thinking: {event.data.delta.content[0].payload.value}&amp;quot;) elif isinstance(event, ToolCallEvent): print(f&amp;quot;ðŸ”§ Tool call: {event.data.name}({event.data.arguments})&amp;quot;) elif isinstance(event, ToolResultEvent): print(f&amp;quot;ðŸ“Š Tool result: {event.data.content[:60]}...&amp;quot;) elif isinstance(event, CitationEvent): print(f&amp;quot;ðŸ“‘ Citation: {event.data}&amp;quot;) elif isinstance(event, MessageEvent): print(f&amp;quot;ðŸ’¬ Message: {event.data.delta.content[0].payload.value}&amp;quot;) elif isinstance(event, FinalAnswerEvent): print(f&amp;quot;âœ… Final answer: {event.data.generated_answer[:100]}...&amp;quot;) print(f&amp;quot; Citations: {len(event.data.citations)} sources referenced&amp;quot;) ```&lt;/p&gt; &lt;h3&gt;Research Mode&lt;/h3&gt; &lt;p&gt;&lt;code&gt;python response = client.retrieval.agent( query=&amp;quot;Analyze the philosophical implications of DeepSeek R1&amp;quot;, generation_config={ &amp;quot;model&amp;quot;: &amp;quot;anthropic/claude-3-opus-20240229&amp;quot;, &amp;quot;extended_thinking&amp;quot;: True, &amp;quot;thinking_budget&amp;quot;: 8192, &amp;quot;temperature&amp;quot;: 0.2, &amp;quot;max_tokens_to_sample&amp;quot;: 32000, &amp;quot;stream&amp;quot;: True }, research_tools=[&amp;quot;rag&amp;quot;, &amp;quot;reasoning&amp;quot;, &amp;quot;critique&amp;quot;, &amp;quot;python_executor&amp;quot;], mode=&amp;quot;research&amp;quot; ) &lt;/code&gt;&lt;/p&gt; &lt;p&gt;For more details, visit our &lt;a href="https://github.com/SciPhi-AI/R2R/"&gt;Github&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/docsoc1"&gt; /u/docsoc1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcxb9w/r2r_v350_release_notes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcxb9w/r2r_v350_release_notes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcxb9w/r2r_v350_release_notes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T22:16:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcx3gh</id>
    <title>A dataset of 7k flux-generated hands with various finger counts â€“ great for training/testing VLMs on finger counting task</title>
    <updated>2025-03-16T22:06:46+00:00</updated>
    <author>
      <name>/u/taesiri</name>
      <uri>https://old.reddit.com/user/taesiri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcx3gh/a_dataset_of_7k_fluxgenerated_hands_with_various/"&gt; &lt;img alt="A dataset of 7k flux-generated hands with various finger counts â€“ great for training/testing VLMs on finger counting task" src="https://external-preview.redd.it/ERLssQL8T7-JL3OKLYdPbyrlra2Ql3KExBAqXUCjlS4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bed78071d6ce48dd87ba242c0a51c3442cbe679e" title="A dataset of 7k flux-generated hands with various finger counts â€“ great for training/testing VLMs on finger counting task" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taesiri"&gt; /u/taesiri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/datasets/taesiri/FluxHands-FingerCount"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcx3gh/a_dataset_of_7k_fluxgenerated_hands_with_various/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcx3gh/a_dataset_of_7k_fluxgenerated_hands_with_various/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T22:06:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcvvj6</id>
    <title>Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching</title>
    <updated>2025-03-16T21:12:03+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very similar to chain of draft but more thorough &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.05179"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcvvj6/sketchofthought_efficient_llm_reasoning_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcvvj6/sketchofthought_efficient_llm_reasoning_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T21:12:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcmyuc</id>
    <title>Gemma 3 Models Tested : Comparing 1B, 4B, 12B, and 27B Versions</title>
    <updated>2025-03-16T14:42:54+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=CURb2tJBpIA"&gt;https://www.youtube.com/watch?v=CURb2tJBpIA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR: No surprises here, performance increases with size. A bit disappointed to see 1b struggling so much with instruction following, but not surprised. I wonder what 1b is useful for? Any use cases that you have found for it?&lt;/p&gt; &lt;p&gt;The 12b is pretty decent though. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcmyuc/gemma_3_models_tested_comparing_1b_4b_12b_and_27b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcmyuc/gemma_3_models_tested_comparing_1b_4b_12b_and_27b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcmyuc/gemma_3_models_tested_comparing_1b_4b_12b_and_27b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T14:42:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcwpef</id>
    <title>Do you feel 70B (quantized) is the deal breaker for complex role play</title>
    <updated>2025-03-16T21:49:07+00:00</updated>
    <author>
      <name>/u/pcpLiu</name>
      <uri>https://old.reddit.com/user/pcpLiu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently Iâ€™m trying dozens of models &amp;lt;= 70B, all quantized for role play scenarios. &lt;/p&gt; &lt;p&gt;Base models are llama , qwen, mistral. And many fine tunes and distilled ones based on them.&lt;/p&gt; &lt;p&gt;Pure anecdotal observations: once the model parameter # &amp;gt;= 70B. Thereâ€™s some magical quality lifting. &lt;/p&gt; &lt;p&gt;Itâ€™s hard to say this in quantitative way. when I used different models under same prompt + same rp ideas, those 70b models made me feel like Iâ€™m doing it with real human beings, Especially in out of character brainstorming. &lt;/p&gt; &lt;p&gt;Itâ€™s not about individual sentencesâ€™ qualities. But the whole vibe. Not like 70B models are more literal or have a big vocabulary. &lt;/p&gt; &lt;p&gt;For example, qwen 32b distilled by DeepSeek R1 is def smart enough but it cannot follow my instructions to give human-ish responses. Taking out of the RP context, its output is good but just not like a human. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pcpLiu"&gt; /u/pcpLiu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcwpef/do_you_feel_70b_quantized_is_the_deal_breaker_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcwpef/do_you_feel_70b_quantized_is_the_deal_breaker_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcwpef/do_you_feel_70b_quantized_is_the_deal_breaker_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T21:49:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcjrp2</id>
    <title>MetaStone-L1 ---The lightweight reasoning model launched by Yuanshi Zhisuan</title>
    <updated>2025-03-16T11:51:44+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcjrp2/metastonel1_the_lightweight_reasoning_model/"&gt; &lt;img alt="MetaStone-L1 ---The lightweight reasoning model launched by Yuanshi Zhisuan" src="https://external-preview.redd.it/9esdwOtFZ9-xPU4Z6uQ-hTei0HrHXFK8YzAB8QTzGNo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b862f612fd291a426e5b068d932bb8c71a97d4c2" title="MetaStone-L1 ---The lightweight reasoning model launched by Yuanshi Zhisuan" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MetaStone-L1 is the lite reasoning model of the MetaStone series, which aims to enhance the performance in hard downstream tasks.&lt;/p&gt; &lt;p&gt;On core reasoning benchmarks including mathematics and code, MetaStone-L1-7B achieved SOTA results in the parallel-level models, and it also achieved the comparable results as the API models such as Claude-3.5-Sonnet-1022 and GPT4o-0513.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/21s0h0i8i1pe1.png?width=2626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=83367c2fdfa32018cc402076222f7d2c2060c41d"&gt;https://preview.redd.it/21s0h0i8i1pe1.png?width=2626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=83367c2fdfa32018cc402076222f7d2c2060c41d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This repo contains the MetaStone-L1-7B model, which is trained based on DeepSeek-R1-Distill-Qwen-7B by GRPO&lt;/p&gt; &lt;p&gt;Optimization tips for specific tasks: For math problems, you can add a hint like &amp;quot;Please reason step by step and put your final answer in \\boxed{}.&amp;quot; For programming problems, add specific formatting requirements to further improve the reasoning effect of the model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/MetaStoneTec/MetaStone-L1-7B"&gt;https://huggingface.co/MetaStoneTec/MetaStone-L1-7B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcjrp2/metastonel1_the_lightweight_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcjrp2/metastonel1_the_lightweight_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcjrp2/metastonel1_the_lightweight_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T11:51:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcufi6</id>
    <title>Improvements to Kokoro TTS v1.0</title>
    <updated>2025-03-16T20:07:36+00:00</updated>
    <author>
      <name>/u/Professional-Bear857</name>
      <uri>https://old.reddit.com/user/Professional-Bear857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I've spent some time trying to improve the output of this model, since the voice output always seemed inconsistent to me when I convert epubs to audiobooks. I thought I would share the updated kokoro-tts python script. To me, it now sounds a lot more natural then before. There are no additional dependencies so if you want to try it then just rename your older file and put this in its place, and then run it. I am running it with this command line:&lt;/p&gt; &lt;p&gt;python kokoro-tts test.epub --format mp3 --speed 1.0&lt;/p&gt; &lt;p&gt;File link (change the file / extension to 'kokoro-tts' and then run it as normal - I had to upload it as a .txt, which is why you need to change the file including its extension to 'kokoro-tts'). The model version I'm using is v1.0.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/user-attachments/files/19274795/kokoro-tts1.txt"&gt;https://github.com/user-attachments/files/19274795/kokoro-tts1.txt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EDIT: Just realised there are multiple files / versions of Kokoro TTS. Here is the original script / model that I am using:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/nazdridoy/kokoro-tts"&gt;https://github.com/nazdridoy/kokoro-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Additional EDIT: It is possible to improve the quality a bit more by changing the below. This will use a bit more vram if you're creating audiobooks on a gpu (~5gb from ~3gb). I'm not sure how well this script performs on a cpu, the original was slow on a cpu, and so I would imagine the new kokoro-tts file will be as well.&lt;/p&gt; &lt;p&gt;def chunk_text(text, chunk_size=1200): to def chunk_text(text, chunk_size=5000):&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Bear857"&gt; /u/Professional-Bear857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcufi6/improvements_to_kokoro_tts_v10/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcufi6/improvements_to_kokoro_tts_v10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcufi6/improvements_to_kokoro_tts_v10/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T20:07:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jchrro</id>
    <title>Top 5 Model Recommendations for Newbie with 24GB</title>
    <updated>2025-03-16T09:29:41+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Itâ€™s only March, but thereâ€™s already been incredible progress in open-weight LLMs this year.&lt;/p&gt; &lt;p&gt;Hereâ€™s my top 5 recommendation for a beginner with 24GB VRAM (32GB for Mac) to try out. The list is from smallest to biggest.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Phi-4 14B for speed&lt;/li&gt; &lt;li&gt;Mistral Small 24B for RAG (only 32k context but best compromise length/quality IMHO)&lt;/li&gt; &lt;li&gt;Gemma 3 27B for general use&lt;/li&gt; &lt;li&gt;Qwen2.5 Coder 32B for coding (older than rest but still best)&lt;/li&gt; &lt;li&gt;QWQ 32B for reasoning (better than distilled deepseek-r1-qwen-32b)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hoping Llama 4 will earn a spot soon!&lt;/p&gt; &lt;p&gt;What's your recommendation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jchrro/top_5_model_recommendations_for_newbie_with_24gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jchrro/top_5_model_recommendations_for_newbie_with_24gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jchrro/top_5_model_recommendations_for_newbie_with_24gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T09:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcbt5l</id>
    <title>These guys never rest!</title>
    <updated>2025-03-16T02:41:35+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcbt5l/these_guys_never_rest/"&gt; &lt;img alt="These guys never rest!" src="https://preview.redd.it/4hmgoyhlsyoe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=455f74ad35ad822af5cb2fe29f909a6835248ce7" title="These guys never rest!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4hmgoyhlsyoe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcbt5l/these_guys_never_rest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcbt5l/these_guys_never_rest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T02:41:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcre0y</id>
    <title>RTX 3060 vs RTX 3090: LLM Performance on 7B, 14B, 32B, 70B Models</title>
    <updated>2025-03-16T17:58:30+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcre0y/rtx_3060_vs_rtx_3090_llm_performance_on_7b_14b/"&gt; &lt;img alt="RTX 3060 vs RTX 3090: LLM Performance on 7B, 14B, 32B, 70B Models" src="https://external-preview.redd.it/5KAhHFD5rwW2nNDpKI_LcYCfDf4tB7OyGvSX5OWqLeA.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2a690ac8c11dc8c1aceccb4206ce1d8bb8f32874" title="RTX 3060 vs RTX 3090: LLM Performance on 7B, 14B, 32B, 70B Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/VGyKwi9Rfhk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcre0y/rtx_3060_vs_rtx_3090_llm_performance_on_7b_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcre0y/rtx_3060_vs_rtx_3090_llm_performance_on_7b_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T17:58:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jctquk</id>
    <title>Introducing Mochi, a finetuned version of Moshi.</title>
    <updated>2025-03-16T19:38:29+00:00</updated>
    <author>
      <name>/u/SovietWarBear17</name>
      <uri>https://old.reddit.com/user/SovietWarBear17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/DavidBrowne17/Muchi"&gt;https://huggingface.co/DavidBrowne17/Muchi&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I finetuned a version of Moshi, using a modified version of this repo &lt;a href="https://github.com/yangdongchao/RSTnet"&gt;https://github.com/yangdongchao/RSTnet&lt;/a&gt; it still has some of the issues with intelligence but it seems better to me. Using that repo we can also finetune new moshi style models using other smarter LLMs than the helium model that moshi is based on. There is no moat.&lt;/p&gt; &lt;p&gt;Edit: Renamed to Muchi as there is already an AI named Mochi&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SovietWarBear17"&gt; /u/SovietWarBear17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jctquk/introducing_mochi_a_finetuned_version_of_moshi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jctquk/introducing_mochi_a_finetuned_version_of_moshi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jctquk/introducing_mochi_a_finetuned_version_of_moshi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T19:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcqy6c</id>
    <title>We have Deep Research at home</title>
    <updated>2025-03-16T17:39:04+00:00</updated>
    <author>
      <name>/u/atineiatte</name>
      <uri>https://old.reddit.com/user/atineiatte</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcqy6c/we_have_deep_research_at_home/"&gt; &lt;img alt="We have Deep Research at home" src="https://external-preview.redd.it/NA_JTAjwBAYLbzLjIgJ3Q_k4TmFsR5MWHCoiYKiIQJ8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff9bd51d7c05f78cbad725a12ad69bc6ff6fe2ec" title="We have Deep Research at home" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atineiatte"&gt; /u/atineiatte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/atineiatte/deep-research-at-home"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcqy6c/we_have_deep_research_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcqy6c/we_have_deep_research_at_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T17:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jct1lk</id>
    <title>PR for native Windows support was just submitted to vLLM</title>
    <updated>2025-03-16T19:08:31+00:00</updated>
    <author>
      <name>/u/Nextil</name>
      <uri>https://old.reddit.com/user/Nextil</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;User SystemPanic just &lt;a href="https://github.com/vllm-project/vllm/pull/14891"&gt;submitted a PR&lt;/a&gt; to the vLLM repo adding native Windows support. Before now it was only possible to run on Linux/WSL. This should make it significantly easier to run new models (especially VLMs) on Windows. No builds that I can see but it includes build instructions. The patched repo is &lt;a href="https://github.com/SystemPanic/vllm-windows/tree/vllm-windows"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The PR mentions submitting a FlashInfer PR adding Windows support, but that doesn't appear to have been done as of writing so it might not be possible to build just yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nextil"&gt; /u/Nextil &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jct1lk/pr_for_native_windows_support_was_just_submitted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jct1lk/pr_for_native_windows_support_was_just_submitted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jct1lk/pr_for_native_windows_support_was_just_submitted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T19:08:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcx69i</id>
    <title>Text an LLM at +61493035885</title>
    <updated>2025-03-16T22:10:18+00:00</updated>
    <author>
      <name>/u/benkaiser</name>
      <uri>https://old.reddit.com/user/benkaiser</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a basic service running on an old Android phone + cheap prepaid SIM card to allow people to send a text and receive a response from Llama 3.1 8B. I felt the need when we recently lost internet access during a tropical cyclone but SMS was still working.&lt;/p&gt; &lt;p&gt;Full details in the blog post: &lt;a href="https://benkaiser.dev/text-an-llm/"&gt;https://benkaiser.dev/text-an-llm/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/benkaiser"&gt; /u/benkaiser &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcx69i/text_an_llm_at_61493035885/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcx69i/text_an_llm_at_61493035885/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcx69i/text_an_llm_at_61493035885/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T22:10:18+00:00</published>
  </entry>
</feed>
