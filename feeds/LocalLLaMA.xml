<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-11T19:48:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j8kd3x</id>
    <title>Created an open-source alternative to Manus AI!</title>
    <updated>2025-03-11T06:18:58+00:00</updated>
    <author>
      <name>/u/ComfortableArm121</name>
      <uri>https://old.reddit.com/user/ComfortableArm121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone’s talking about Manus AI (an agent that can research, browse, code, and automate tasks.)&lt;br /&gt; But it's only available with an invite code!&lt;/p&gt; &lt;p&gt;Our &lt;a href="https://github.com/The-Pocket-World/PocketManus"&gt;opensource project, PocketManus, &lt;/a&gt;combines &lt;a href="https://github.com/The-Pocket-World/Pocket-Flow-Framework"&gt;Pocketflow Framework &lt;/a&gt;and &lt;a href="https://github.com/mannaandpoem/OpenManus"&gt;OpenManus&lt;/a&gt; to execute actions.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AI break down complex tasks into Pocketflow Nodes&lt;/li&gt; &lt;li&gt;AI creates detailed execution strategies and interact with tools&lt;/li&gt; &lt;li&gt;Tools / Tool agents interface with external services and APIs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Real-World Capabilities&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Autonomous research, coding, and web browsing&lt;/li&gt; &lt;li&gt;Supports top LLMs (easily integrated with GPT-4O, Claude 3.7, Gemini, Mistral, DeepSeek , Qwen, Ollama, Groq more)&lt;/li&gt; &lt;li&gt;Simple Setup. No restrictions. No invites. No paywalls. Just powerful multi-agent collaboration.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here's a video of PocketManus in action: &lt;a href="https://x.com/helenaeverley/status/1899221716464959855"&gt;https://x.com/helenaeverley/status/1899221716464959855&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComfortableArm121"&gt; /u/ComfortableArm121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8kd3x/created_an_opensource_alternative_to_manus_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8kd3x/created_an_opensource_alternative_to_manus_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8kd3x/created_an_opensource_alternative_to_manus_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T06:18:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8xbwc</id>
    <title>How does Lovable technically work behind the scenes?</title>
    <updated>2025-03-11T18:08:46+00:00</updated>
    <author>
      <name>/u/rodbarest</name>
      <uri>https://old.reddit.com/user/rodbarest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it a &amp;quot;just&amp;quot; an smart system prompt? Is it a fine -tuned model with custom tools?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rodbarest"&gt; /u/rodbarest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8xbwc/how_does_lovable_technically_work_behind_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8xbwc/how_does_lovable_technically_work_behind_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8xbwc/how_does_lovable_technically_work_behind_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T18:08:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8xlo9</id>
    <title>Fairydreaming's very instructive post on threadripper 7000 ram bandwidth comparaison</title>
    <updated>2025-03-11T18:20:04+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/threadripper/s/miR4V5udCC"&gt;https://www.reddit.com/r/threadripper/s/miR4V5udCC&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8xlo9/fairydreamings_very_instructive_post_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8xlo9/fairydreamings_very_instructive_post_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8xlo9/fairydreamings_very_instructive_post_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T18:20:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1j87eum</id>
    <title>QwQ 32B can do it if you coach it 2 times</title>
    <updated>2025-03-10T19:41:49+00:00</updated>
    <author>
      <name>/u/DrVonSinistro</name>
      <uri>https://old.reddit.com/user/DrVonSinistro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j87eum/qwq_32b_can_do_it_if_you_coach_it_2_times/"&gt; &lt;img alt="QwQ 32B can do it if you coach it 2 times" src="https://external-preview.redd.it/bGFmOXk2NDIxeG5lMalrzKbbY1wxsyua5vTpp1g3RTatq_ecPpvEXRJ-_J8E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3bc2c544369e356552a9d78fa1f23bdc00fdf6c3" title="QwQ 32B can do it if you coach it 2 times" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrVonSinistro"&gt; /u/DrVonSinistro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6wn0l7421xne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j87eum/qwq_32b_can_do_it_if_you_coach_it_2_times/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j87eum/qwq_32b_can_do_it_if_you_coach_it_2_times/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T19:41:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8lp6d</id>
    <title>RubyLLM 1.0</title>
    <updated>2025-03-11T08:00:24+00:00</updated>
    <author>
      <name>/u/crmne</name>
      <uri>https://old.reddit.com/user/crmne</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! I just released RubyLLM 1.0, a library that makes working with AI feel natural and Ruby-like.&lt;/p&gt; &lt;p&gt;While building a RAG application for business documents, I wanted an AI library that felt like Ruby: elegant, expressive, and focused on developer happiness.&lt;/p&gt; &lt;h2&gt;What makes it different?&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Beautiful interfaces&lt;/strong&gt; &lt;code&gt;ruby chat = RubyLLM.chat embedding = RubyLLM.embed(&amp;quot;Ruby is elegant&amp;quot;) image = RubyLLM.paint(&amp;quot;a sunset over mountains&amp;quot;) &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Works with multiple providers through one API&lt;/strong&gt; ```ruby&lt;/p&gt; &lt;h1&gt;Start with GPT&lt;/h1&gt; &lt;p&gt;chat = RubyLLM.chat(model: 'gpt-4o-mini')&lt;/p&gt; &lt;h1&gt;Switch to Claude? No problem&lt;/h1&gt; &lt;p&gt;chat.with_model('claude-3-5-sonnet') ```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Streaming that makes sense&lt;/strong&gt; &lt;code&gt;ruby chat.ask &amp;quot;Write a story&amp;quot; do |chunk| print chunk.content # Same chunk format for all providers end &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rails integration that just works&lt;/strong&gt; &lt;code&gt;ruby class Chat &amp;lt; ApplicationRecord acts_as_chat end &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tools without the JSON Schema pain&lt;/strong&gt; ```ruby class Search &amp;lt; RubyLLM::Tool description &amp;quot;Searches our database&amp;quot; param :query, desc: &amp;quot;The search query&amp;quot;&lt;/p&gt; &lt;p&gt;def execute(query:) Document.search(query).map(&amp;amp;:title) end end ```&lt;/p&gt; &lt;p&gt;It supports vision, PDFs, audio, and more - all with minimal dependencies.&lt;/p&gt; &lt;p&gt;Check it out at &lt;a href="https://github.com/crmne/ruby_llm"&gt;https://github.com/crmne/ruby_llm&lt;/a&gt; or &lt;code&gt;gem install ruby_llm&lt;/code&gt;&lt;/p&gt; &lt;p&gt;What do you think? I'd love your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crmne"&gt; /u/crmne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8lp6d/rubyllm_10/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8lp6d/rubyllm_10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8lp6d/rubyllm_10/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T08:00:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8i5s2</id>
    <title>Why doesn't Groq Sell its LPUs? By Extension, Why doesn't Google do that?</title>
    <updated>2025-03-11T03:57:03+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When Groq first announced and demoed its LPUs cluster, I was so excited. I believed that finally we get HW that's cost effective. But, it seems the company is not interested in selling its HW at all. &lt;/p&gt; &lt;p&gt;And I DON'T UNDERSTAND THE LOGIC BEHIND such a decision. Does is have something to do with Google since the founder of Groq are ex-Google engineers who worked and developed Googles TPUs?&lt;/p&gt; &lt;p&gt;Why doesn't Google sell its own TPUs? I think now is the right time to enter the HW market.&lt;/p&gt; &lt;p&gt;Can someone shed some light on this topic, please?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8i5s2/why_doesnt_groq_sell_its_lpus_by_extension_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8i5s2/why_doesnt_groq_sell_its_lpus_by_extension_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8i5s2/why_doesnt_groq_sell_its_lpus_by_extension_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T03:57:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8iqns</id>
    <title>Hello world :)</title>
    <updated>2025-03-11T04:30:38+00:00</updated>
    <author>
      <name>/u/No-Abalone1029</name>
      <uri>https://old.reddit.com/user/No-Abalone1029</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8iqns/hello_world/"&gt; &lt;img alt="Hello world :)" src="https://preview.redd.it/hy3131ghnzne1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57704da9936e283da78771a5685c4f870a144bfe" title="Hello world :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA rtx 3060 12gb vram, hyte revolt 3 asrock b760 w wifi intel i5 16gb t-force vulcan ram&lt;/p&gt; &lt;p&gt;$1k. what do we think, and what should I do for my first project?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Abalone1029"&gt; /u/No-Abalone1029 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hy3131ghnzne1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8iqns/hello_world/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8iqns/hello_world/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T04:30:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8odgh</id>
    <title>File Researcher agent with RAG tool for AI coder</title>
    <updated>2025-03-11T11:18:09+00:00</updated>
    <author>
      <name>/u/Grigorij_127</name>
      <uri>https://old.reddit.com/user/Grigorij_127</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8odgh/file_researcher_agent_with_rag_tool_for_ai_coder/"&gt; &lt;img alt="File Researcher agent with RAG tool for AI coder" src="https://external-preview.redd.it/hu7RNfd8Gcq7ktLCiOwfW0yJZvkEjpnNYMhmBOHlZQE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32f20ef79a84e7fe219fbd9b01414f7f239848ee" title="File Researcher agent with RAG tool for AI coder" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ta8cv7rrl1oe1.png?width=965&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=03272d494ace1b04c3441a2c811927335bac336a"&gt;https://preview.redd.it/ta8cv7rrl1oe1.png?width=965&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=03272d494ace1b04c3441a2c811927335bac336a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey,&lt;/p&gt; &lt;p&gt;I want to share how we enchanced the File Researcher agent for our AI coder.&lt;/p&gt; &lt;p&gt;Now it can search for files in codebase using semantic search in addition to classical folder-traversal approach.&lt;/p&gt; &lt;p&gt;All files are described by AI, same as every function inside of that files (we chunking code by functions) and uploaded to vector database. Researcher uses retrieval mechanism with reranker.&lt;/p&gt; &lt;p&gt;Search efficiency seems to increase much.&lt;/p&gt; &lt;p&gt;Works with local models.&lt;/p&gt; &lt;p&gt;Please check out new agent in my project Clean Coder (&lt;a href="https://github.com/Grigorij-Dudnik/Clean-Coder-AI"&gt;https://github.com/Grigorij-Dudnik/Clean-Coder-AI&lt;/a&gt;), leave your feedback and stars.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Grigorij_127"&gt; /u/Grigorij_127 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8odgh/file_researcher_agent_with_rag_tool_for_ai_coder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8odgh/file_researcher_agent_with_rag_tool_for_ai_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8odgh/file_researcher_agent_with_rag_tool_for_ai_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T11:18:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8r970</id>
    <title>Large gap between OpenAI o1 model and DeepSeek R1 visible in ZebraLogic X-Large puzzle performance: https://arxiv.org/pdf/2502.01100</title>
    <updated>2025-03-11T13:52:35+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8r970/large_gap_between_openai_o1_model_and_deepseek_r1/"&gt; &lt;img alt="Large gap between OpenAI o1 model and DeepSeek R1 visible in ZebraLogic X-Large puzzle performance: https://arxiv.org/pdf/2502.01100" src="https://preview.redd.it/5vzdlpy4e2oe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43e63261dc53f305b460eca70393df35dafd5a0c" title="Large gap between OpenAI o1 model and DeepSeek R1 visible in ZebraLogic X-Large puzzle performance: https://arxiv.org/pdf/2502.01100" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5vzdlpy4e2oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8r970/large_gap_between_openai_o1_model_and_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8r970/large_gap_between_openai_o1_model_and_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T13:52:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8r4w0</id>
    <title>Mac Studio M3 Ultra review are out</title>
    <updated>2025-03-11T13:47:09+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There is little actual benchmarks for LLMs though. I found:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=s6wt83TU_B4"&gt;https://www.youtube.com/watch?v=s6wt83TU_B4&lt;/a&gt; running LMStudio with deepseekv2.5&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=J4qwuCXyAcU"&gt;https://www.youtube.com/watch?v=J4qwuCXyAcU&lt;/a&gt; testing R1 at Q4 MLX at 18t/s and I the other graph I would say is ollama so Q4_K_M at 16t/s.&lt;/p&gt; &lt;p&gt;I would say those are token generation and not prompt processing. And at low context size.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8r4w0/mac_studio_m3_ultra_review_are_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8r4w0/mac_studio_m3_ultra_review_are_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8r4w0/mac_studio_m3_ultra_review_are_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T13:47:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8paig</id>
    <title>Draft model for QwQ32B for LMstudio</title>
    <updated>2025-03-11T12:13:29+00:00</updated>
    <author>
      <name>/u/QuotableMorceau</name>
      <uri>https://old.reddit.com/user/QuotableMorceau</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone aware of any usable draft models for QwQ32B in the range 0.5B-1.5B, what work for speculative decoding with LMStudio.&lt;br /&gt; Or maybe of a workflow to generate one that matches the vocabulary in QwQ ?&lt;/p&gt; &lt;p&gt;With the tweaks from Unsloth people I finally managed to get the model to think less, but generation is still too slow (5-6tk/s) on my setup, so like 15 minutes to get initial response :)&lt;/p&gt; &lt;p&gt;UPDATE: AdEmotional1944 pointed to this model : &lt;a href="https://huggingface.co/mradermacher/QwQ-0.5B-GGUF"&gt;https://huggingface.co/mradermacher/QwQ-0.5B-GGUF&lt;/a&gt; , it works like a charm.&lt;br /&gt; My speed increased to 7-8tk/s :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuotableMorceau"&gt; /u/QuotableMorceau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8paig/draft_model_for_qwq32b_for_lmstudio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8paig/draft_model_for_qwq32b_for_lmstudio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8paig/draft_model_for_qwq32b_for_lmstudio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T12:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8pbyh</id>
    <title>Dual NVidia RTX 3090 GPU server I have built</title>
    <updated>2025-03-11T12:15:46+00:00</updated>
    <author>
      <name>/u/Outrageous-Win-3244</name>
      <uri>https://old.reddit.com/user/Outrageous-Win-3244</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8pbyh/dual_nvidia_rtx_3090_gpu_server_i_have_built/"&gt; &lt;img alt="Dual NVidia RTX 3090 GPU server I have built" src="https://external-preview.redd.it/o1T-IJ2bDkIslBf4rn-VWBMLnYG5E8SOL2y4rR1T_kc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=19030bf8faea8150053dace2be005778a01b9d2f" title="Dual NVidia RTX 3090 GPU server I have built" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/wu41a4oiy1oe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c020f8dd3889cc468d45e8eb1d8145abeaa31f11"&gt;https://preview.redd.it/wu41a4oiy1oe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c020f8dd3889cc468d45e8eb1d8145abeaa31f11&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have written an article about what I have learnt during the build. The article can be found here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://ozeki-ai-server.com/p_8665-ai-server-2-nvidia-rtx-3090.html"&gt;https://ozeki-ai-server.com/p_8665-ai-server-2-nvidia-rtx-3090.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would like to share with you what I have learn't when I built this Dual NVidia RTX 3090 GPU server for AI&lt;/p&gt; &lt;p&gt;What was the goal&lt;/p&gt; &lt;p&gt;I have built this AI server to be able to run the LLama 3.1 70B parameter AI model locally for AI chat, the Qwen 2.5 AI model for coding, and to do AI image generation with the Flux model. This AI server is also answering VoIP phone calls, e-mails and is conducting WhatsApp chats.&lt;/p&gt; &lt;p&gt;Overall evaluation&lt;/p&gt; &lt;p&gt;This setup is excellent for small organizations where the number of users are below 10. Such a server offers the ability to work with most AI models and to create great automated services.&lt;/p&gt; &lt;p&gt;Hardware configuration&lt;/p&gt; &lt;p&gt;CPU Intel Core i9 14900K RAM 192GB DDR5 6000Mhz RAM Storage 2x4TB Nvme SSD (Samsung 990 pro) CPU cooler ARCTIC Liquid Freezer III 360 GPU cooling Air cooled system (1 unit between GPUs) GPU 2xNvidia RTX 3090 Founders Edition 24Gb Vram Case Antex Performance 1FT White full tower (8 card slots!) Motherboard Asus Rog Maximus z790 dark hero PSU Corsair AX1500i Operating system Windows 11 pro&lt;/p&gt; &lt;p&gt;What have I have learnt when I have built this server&lt;/p&gt; &lt;p&gt;CPU: The Intel Core i9 14900K CPU is the same CPU as the Intel Core i9 13900K, they have only changed the name. Every parameter is the same, the performance is the same. Although I ended up using the 14900K, I have picked a 13900K for other builds. Originally I have purchased the Intel Core i9 14900KF CPU, which I had to replace to Intel Core i9 14900K. The difference between the two CPUs is that the Intel Core i9 14900KF does not have a built in GPU. This was a problem, because serving the computer screen reduced the amount of GPU RAM I had for AI models. By plugging in the monitor to the on-board Hdmi slot of the GPU built into the 14900K CPU, all of the GPU ram of the Nvidia video cards became available for AI execution.&lt;/p&gt; &lt;p&gt;CPU cooling: Air cooling was not sufficient for the CPU. I had to replace the original CPU cooler with a water cooler, because the CPU always shut down under high load when it was air cooled.&lt;/p&gt; &lt;p&gt;RAM: I have used 4 RAM slots in this system, and I have discovered that this setup is slower than if I use only 2. A system with 2x48GB DDR5 modules will achieve higher RAM speed because the RAM can be overclocked to higher speed offered by the XMP memory profiles in the bios. I ended up keeping the 4 modules because I had done some memory intensive work (analyzing LLM files around 70GB in size, which had to fit into the RAM twice). Unless you want to do RAM intensive work you don't need 4x48GB RAM. Most of the work is done by the GPU, so system memory is rarely used. In other builds I went for 2x48GB instead of 4x48GB RAM.&lt;/p&gt; &lt;p&gt;SSD: I have used a RAID0 in this system. The RAID0 configuration in bios gave me a single drive of 8TB (the capacity of the two 4TB SSDs were added together). The performance was faster when loading large models. Windows installation was a bit more difficult, because a driver had to be loaded during installation. The RAID0 array lost its content during a bios reset and I had to reinstall the system. In following builds I have used a single 4TB SSD and did not setup a RAID0 array.&lt;/p&gt; &lt;p&gt;Case: A full tower case had to be selected that had 8 card slots in the back. It was difficult to find a suitable one, as most pc cases only have 7 card slots, which is not enough to place two air-cooled GPUs in it. The case I have selected is beautiful, but it is also very heavy because of the glass panels and the thicker steel framing. Although it is difficult to move this case around, I like it very much.&lt;/p&gt; &lt;p&gt;GPU: I have tested this system with 2 Nvidia RTX4090 and 2 Nvidia RTX3090 GPUs. The 2 Nvidia RTX3090 GPUs offered nearly the same speed as 2 Nvidia RTX4090 when I have ran AI models on them. For GPUs I have also learnt that, it is much better to have 1 GPU with large VRAM then 2 GPUs. An Nvidia RTX A6000 with 48GB Vram is a better choice then 2 Nvidia RTX3090 with 2x24GB. A single GPU will consume less power, it will be easier to cool it down, it is easier to select a mother board and a case for it, plus the number of PCIe lanes in the i9 14900k CPU only allows 1 GPU to run at it's full potential.&lt;/p&gt; &lt;p&gt;GPU cooling: Each Nvidia RTX3090 FE GPU takes up 3 slots. 1 slot is needed between them for cooling and 1 slot is needed below the second one for cooling. I have also learnt, that air cooling is sufficient for this setup. Water cooling is more complicated, more expensive and is a pain when you want to replace the GPUs.&lt;/p&gt; &lt;p&gt;Mother board: It is important to pick a motherboard with exactly 4 spaces of the PCIe slots in between, so it is possible to fit the two GPUs in a way to have one unit of cooling space in between. The speed of the PCIe ports must be investigated before choosing a motherboard. The motherboard I have picked for this setup (Asus Rog Maximus z790 dark hero) might not be the best choice. It was way more expensive than similar offerings, plus when I put an NVME ssd in to the first NVMe slot, the speed of the second (PCIe slot used for the second GPU) degraded greatly. It is also worth mentioning that it is very hard to get replacement wifi 7 antennas for this motherboard because it uses a proprietary antenna connector. In other builds I have used &amp;quot;MSI MAG Z790 TOMAHAWK WiFi LGA 1700 ATX&amp;quot; which gave me similar performance with less pain.&lt;/p&gt; &lt;p&gt;PSU: The Corsair AX1500i PSU was sufficient. This PSU is quiet and has a great USB interface with a Windows app that allow me to monitor power consumption on all ports. I have also used Corsair AX1600i in similar setups, which gave me more overhead. I have also used EVGA Supernove G+ 2000W in other builds, which I did not like much, as it did not offer a management port, and the fan was very noisy.&lt;/p&gt; &lt;p&gt;Case cooling: I had 3 fans on the top for the water coller, 3 in the front of the case 1 in the back. This was sufficient. The cooling profile could be adjusted in the Bios to keep the system quiet.&lt;/p&gt; &lt;p&gt;OS: Originally I have installed Windows 11 Home edition and have learn't that it is only able to handle 128GB RAM. &lt;/p&gt; &lt;p&gt;Software: I have installed Ozeki AI Server on it for running the AI models. Ozeki AI Server is the best local AI execution framework. It is much faster then other Python based solutions.&lt;/p&gt; &lt;p&gt;I had to upgrade the system to Windows 11 Professional to be able to use the 192GB RAM and to be able to access the server remotely through Remote Desktop.&lt;/p&gt; &lt;p&gt;Key takeaway&lt;/p&gt; &lt;p&gt;This system offers 48GB of GPU RAM and sufficient speed to run high quality AI models. I strongly recommend this setup as a first server.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Win-3244"&gt; /u/Outrageous-Win-3244 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8pbyh/dual_nvidia_rtx_3090_gpu_server_i_have_built/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8pbyh/dual_nvidia_rtx_3090_gpu_server_i_have_built/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8pbyh/dual_nvidia_rtx_3090_gpu_server_i_have_built/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T12:15:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8z5gj</id>
    <title>Question from a noobie : is it easy to fine-tune a model ?</title>
    <updated>2025-03-11T19:23:39+00:00</updated>
    <author>
      <name>/u/enzo_ghll</name>
      <uri>https://old.reddit.com/user/enzo_ghll</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everybody,&lt;/p&gt; &lt;p&gt;I'm a newbie in this field, i'm currently running Qwen2.5 with my MacBook Air M2.&lt;/p&gt; &lt;p&gt;I wanted to know if finetuning a model is easy ? I'm not a dev at all, i saw Unsloth in Hugging Face but I don't really understand what I should do. &lt;/p&gt; &lt;p&gt;My goal is to make the model more efficient, train it on my language (French) and my datas, if possible.&lt;/p&gt; &lt;p&gt;Is it possible ?&lt;/p&gt; &lt;p&gt;+ What are some tips and tricks that you wished to know earlier ? &lt;/p&gt; &lt;p&gt;Thx !!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/enzo_ghll"&gt; /u/enzo_ghll &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8z5gj/question_from_a_noobie_is_it_easy_to_finetune_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8z5gj/question_from_a_noobie_is_it_easy_to_finetune_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8z5gj/question_from_a_noobie_is_it_easy_to_finetune_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T19:23:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8766b</id>
    <title>New rig who dis</title>
    <updated>2025-03-10T19:31:29+00:00</updated>
    <author>
      <name>/u/MotorcyclesAndBizniz</name>
      <uri>https://old.reddit.com/user/MotorcyclesAndBizniz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8766b/new_rig_who_dis/"&gt; &lt;img alt="New rig who dis" src="https://b.thumbs.redditmedia.com/0XSP2n-GAI5n3Op8qnPsulZZgY7u_Dk_E6IZd3L-Ixg.jpg" title="New rig who dis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPU: 6x 3090 FE via 6x PCIe 4.0 x4 Oculink&lt;br /&gt; CPU: AMD 7950x3D&lt;br /&gt; MoBo: B650M WiFi&lt;br /&gt; RAM: 192GB DDR5 @ 4800MHz&lt;br /&gt; NIC: 10Gbe&lt;br /&gt; NVMe: Samsung 980 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MotorcyclesAndBizniz"&gt; /u/MotorcyclesAndBizniz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j8766b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8766b/new_rig_who_dis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8766b/new_rig_who_dis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T19:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8i9rc</id>
    <title>NVLINK improves dual RTX 3090 inference performance by nearly 50%</title>
    <updated>2025-03-11T04:02:55+00:00</updated>
    <author>
      <name>/u/hp1337</name>
      <uri>https://old.reddit.com/user/hp1337</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8i9rc/nvlink_improves_dual_rtx_3090_inference/"&gt; &lt;img alt="NVLINK improves dual RTX 3090 inference performance by nearly 50%" src="https://external-preview.redd.it/vlUgVTNeRZS9-bbTFaZ7ayYcVwvIPXEw74izW1rJLuI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ba6a9bb89057dbbff64dd02958275d4ac3df306" title="NVLINK improves dual RTX 3090 inference performance by nearly 50%" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hp1337"&gt; /u/hp1337 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://himeshp.blogspot.com/2025/03/vllm-performance-benchmarks-4x-rtx-3090.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8i9rc/nvlink_improves_dual_rtx_3090_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8i9rc/nvlink_improves_dual_rtx_3090_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T04:02:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8vjv6</id>
    <title>Factorio Learning Environment – Agents Build Factories</title>
    <updated>2025-03-11T16:57:06+00:00</updated>
    <author>
      <name>/u/finallyifoundvalidUN</name>
      <uri>https://old.reddit.com/user/finallyifoundvalidUN</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://jackhopkins.github.io/factorio-learning-environment/"&gt;https://jackhopkins.github.io/factorio-learning-environment/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/finallyifoundvalidUN"&gt; /u/finallyifoundvalidUN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8vjv6/factorio_learning_environment_agents_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8vjv6/factorio_learning_environment_agents_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8vjv6/factorio_learning_environment_agents_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T16:57:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8mtsc</id>
    <title>I created an Open Source Perplexity-Style Unified Search for Your Distributed Second Brain</title>
    <updated>2025-03-11T09:29:29+00:00</updated>
    <author>
      <name>/u/stealthanthrax</name>
      <uri>https://old.reddit.com/user/stealthanthrax</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8mtsc/i_created_an_open_source_perplexitystyle_unified/"&gt; &lt;img alt="I created an Open Source Perplexity-Style Unified Search for Your Distributed Second Brain" src="https://external-preview.redd.it/ZWY0aWR0OGY0MW9lMZgswa6t9U1L_elgvz9f8oIRyRLGhWXXy5P6nPcZPdz9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02250a73a7824bb867a8348c19f1edaa5ad03c80" title="I created an Open Source Perplexity-Style Unified Search for Your Distributed Second Brain" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stealthanthrax"&gt; /u/stealthanthrax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/q4apht8f41oe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8mtsc/i_created_an_open_source_perplexitystyle_unified/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8mtsc/i_created_an_open_source_perplexitystyle_unified/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T09:29:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8v0wk</id>
    <title>Kokoro Voice Composer (generate new voices + TTS)</title>
    <updated>2025-03-11T16:35:28+00:00</updated>
    <author>
      <name>/u/al4sdair</name>
      <uri>https://old.reddit.com/user/al4sdair</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8v0wk/kokoro_voice_composer_generate_new_voices_tts/"&gt; &lt;img alt="Kokoro Voice Composer (generate new voices + TTS)" src="https://external-preview.redd.it/uM9Sn9FDN3MK3bSThOTPvi96U67JUdhcE60zJX8XsG0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cdda1f9e601d1b7049f6d7d58a497aa9d625d0c3" title="Kokoro Voice Composer (generate new voices + TTS)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/al4sdair"&gt; /u/al4sdair &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/alasdairforsythe/kokoro-voice-composer"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8v0wk/kokoro_voice_composer_generate_new_voices_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8v0wk/kokoro_voice_composer_generate_new_voices_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T16:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8ibs2</id>
    <title>Don't underestimate the power of local models executing recursive agent workflows. (mistral-small)</title>
    <updated>2025-03-11T04:06:03+00:00</updated>
    <author>
      <name>/u/LocoMod</name>
      <uri>https://old.reddit.com/user/LocoMod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8ibs2/dont_underestimate_the_power_of_local_models/"&gt; &lt;img alt="Don't underestimate the power of local models executing recursive agent workflows. (mistral-small)" src="https://external-preview.redd.it/aHB6YWN6MG1pem5lMRehscSTBN6MsWNS82nQXiny-IBLyecHf_sStrTrfL-k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe0eebd0390412c4dbf32e51fec56621c4f2ca18" title="Don't underestimate the power of local models executing recursive agent workflows. (mistral-small)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LocoMod"&gt; /u/LocoMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/51m4yx0mizne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8ibs2/dont_underestimate_the_power_of_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8ibs2/dont_underestimate_the_power_of_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T04:06:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8wfsk</id>
    <title>Reka Flash 3 and the infamous spinning hexagon prompt</title>
    <updated>2025-03-11T17:33:00+00:00</updated>
    <author>
      <name>/u/Lowkey_LokiSN</name>
      <uri>https://old.reddit.com/user/Lowkey_LokiSN</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8wfsk/reka_flash_3_and_the_infamous_spinning_hexagon/"&gt; &lt;img alt="Reka Flash 3 and the infamous spinning hexagon prompt" src="https://external-preview.redd.it/1wLtzmIJNY8IXPRc2HGItEr1OuV-7ei5csHuGx1DeYc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0494b48062ab05674239b1ffd75a741fd0ea172" title="Reka Flash 3 and the infamous spinning hexagon prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ran the following prompt with the 3bit MLX version of the new &lt;a href="https://huggingface.co/RekaAI/reka-flash-3"&gt;Reka Flash 3&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;Create a pygame script with a spinning hexagon and a bouncing ball confined within. Handle collision detection, gravity and ball physics as good as you possibly can.&lt;/p&gt; &lt;p&gt;I DID NOT expect the result to be as clean as it turned out to be. Of all the models under 10GB that I've tested with the same prompt, this(3bit quant!) one's clearly the winner!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1j8wfsk/video/ved8j31vi3oe1/player"&gt;https://reddit.com/link/1j8wfsk/video/ved8j31vi3oe1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lowkey_LokiSN"&gt; /u/Lowkey_LokiSN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8wfsk/reka_flash_3_and_the_infamous_spinning_hexagon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8wfsk/reka_flash_3_and_the_infamous_spinning_hexagon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8wfsk/reka_flash_3_and_the_infamous_spinning_hexagon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T17:33:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8mrju</id>
    <title>Alibaba just dropped R1-Omni!</title>
    <updated>2025-03-11T09:24:29+00:00</updated>
    <author>
      <name>/u/Optifnolinalgebdirec</name>
      <uri>https://old.reddit.com/user/Optifnolinalgebdirec</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alibaba just dropped R1-Omni! Redefining emotional intelligence with Omni-Multimodal Emotion Recognition and Reinforcement Learning!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Optifnolinalgebdirec"&gt; /u/Optifnolinalgebdirec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8mrju/alibaba_just_dropped_r1omni/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8mrju/alibaba_just_dropped_r1omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8mrju/alibaba_just_dropped_r1omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T09:24:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8uvg0</id>
    <title>New Reasoning model (Reka Flash 3 - 21B)</title>
    <updated>2025-03-11T16:29:11+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8uvg0/new_reasoning_model_reka_flash_3_21b/"&gt; &lt;img alt="New Reasoning model (Reka Flash 3 - 21B)" src="https://preview.redd.it/fgldu1ml73oe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f085204b9fc6819966a9114f4e794afbed28a54f" title="New Reasoning model (Reka Flash 3 - 21B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fgldu1ml73oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8uvg0/new_reasoning_model_reka_flash_3_21b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8uvg0/new_reasoning_model_reka_flash_3_21b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T16:29:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8tfh5</id>
    <title>Reka Flash 3, New Open Source 21B Model</title>
    <updated>2025-03-11T15:29:02+00:00</updated>
    <author>
      <name>/u/DreamGenAI</name>
      <uri>https://old.reddit.com/user/DreamGenAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tweet: &lt;a href="https://x.com/RekaAILabs/status/1899481289495031825"&gt;https://x.com/RekaAILabs/status/1899481289495031825&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/RekaAI/reka-flash-3"&gt;https://huggingface.co/RekaAI/reka-flash-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://www.reka.ai/news/introducing-reka-flash"&gt;https://www.reka.ai/news/introducing-reka-flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DreamGenAI"&gt; /u/DreamGenAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8tfh5/reka_flash_3_new_open_source_21b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8tfh5/reka_flash_3_new_open_source_21b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8tfh5/reka_flash_3_new_open_source_21b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T15:29:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8r2nr</id>
    <title>M3 Ultra 512GB does 18T/s with Deepseek R1 671B Q4 (DAVE2D REVIEW)</title>
    <updated>2025-03-11T13:44:15+00:00</updated>
    <author>
      <name>/u/AliNT77</name>
      <uri>https://old.reddit.com/user/AliNT77</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8r2nr/m3_ultra_512gb_does_18ts_with_deepseek_r1_671b_q4/"&gt; &lt;img alt="M3 Ultra 512GB does 18T/s with Deepseek R1 671B Q4 (DAVE2D REVIEW)" src="https://external-preview.redd.it/Z3KKrFryWMuFPZGHYHDmgzf48KaEB5A-Ze6pFibC3lk.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=992e2d56bcc2473a9ea6913ceadc30c7eb46bb1f" title="M3 Ultra 512GB does 18T/s with Deepseek R1 671B Q4 (DAVE2D REVIEW)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AliNT77"&gt; /u/AliNT77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=J4qwuCXyAcU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8r2nr/m3_ultra_512gb_does_18ts_with_deepseek_r1_671b_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8r2nr/m3_ultra_512gb_does_18ts_with_deepseek_r1_671b_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T13:44:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8u90g</id>
    <title>New Gemma models on 12th of March</title>
    <updated>2025-03-11T16:03:39+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8u90g/new_gemma_models_on_12th_of_march/"&gt; &lt;img alt="New Gemma models on 12th of March" src="https://preview.redd.it/8qfnwj7433oe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed4ac1bb57e9292b5685c7637a5bd9e4ac889d7c" title="New Gemma models on 12th of March" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;X pos&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qfnwj7433oe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8u90g/new_gemma_models_on_12th_of_march/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8u90g/new_gemma_models_on_12th_of_march/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T16:03:39+00:00</published>
  </entry>
</feed>
