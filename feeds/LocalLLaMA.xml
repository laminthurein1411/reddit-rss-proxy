<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-02T08:39:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jpbptf</id>
    <title>tried a bunch of open models with goose</title>
    <updated>2025-04-02T00:40:54+00:00</updated>
    <author>
      <name>/u/lifelonglearn3r</name>
      <uri>https://old.reddit.com/user/lifelonglearn3r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpbptf/tried_a_bunch_of_open_models_with_goose/"&gt; &lt;img alt="tried a bunch of open models with goose" src="https://external-preview.redd.it/8ypLw4iI5hM69IbWKzJkC6YTV7o1sva9BZJYBWMD5KY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd7ebb67fdbf8351f9a6eedeebf6f17a1be026ca" title="tried a bunch of open models with goose" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey all, been lurking forever and finally have something hopefully worth sharing. I've been messing with different models in Goose (open source AI agent by Block, similar to Aider) and ran some benchmarking that might be interesting. I tried out qwen series, qwq, deepseek-chat-v3 latest checkpoint, llama3, and the leading closed models also.&lt;/p&gt; &lt;p&gt;For models that don't support native tool calling (deepseek-r1, gemma3, phi4) which is needed for agent use cases, I built a &amp;quot;toolshim&amp;quot; for Goose which uses a local ollama model to interpret responses from the primary model into the right tool calls. It's usable but the performance is unsurprisingly subpar compared to models specifically fine-tuned for tool calling. Has anyone had any success with other approaches for getting these models to successfully use tools?&lt;/p&gt; &lt;p&gt;I ran 8 pretty simple tasks x3 times for each model to get the overall rankings:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create file&lt;/li&gt; &lt;li&gt;List files&lt;/li&gt; &lt;li&gt;Search/replace in file&lt;/li&gt; &lt;li&gt;Build flappy bird&lt;/li&gt; &lt;li&gt;Creating a wikipedia-stylized page&lt;/li&gt; &lt;li&gt;Data analysis on a CSV&lt;/li&gt; &lt;li&gt;Restaurant research on web&lt;/li&gt; &lt;li&gt;Blogpost summarization&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Here are the results:&lt;/h1&gt; &lt;p&gt;|Rank|Model|Average Eval Score|Inference Provider|&lt;/p&gt; &lt;p&gt;|-----|-----|-----|-----|&lt;/p&gt; &lt;p&gt;|1|claude-3-5-sonnet-2|1.00|databricks (bedrock)|&lt;/p&gt; &lt;p&gt;|2|claude-3-7-sonnet|0.94|databricks (bedrock)|&lt;/p&gt; &lt;p&gt;|3|claude-3-5-haiku|0.91|databricks (bedrock)|&lt;/p&gt; &lt;p&gt;|4|o1|0.81|databricks (bedrock)|&lt;/p&gt; &lt;p&gt;|4|gpt-4o|0.81|databricks (bedrock)|&lt;/p&gt; &lt;p&gt;|6|qwen2.5-coder:32b|0.8|ollama|&lt;/p&gt; &lt;p&gt;|7|o3-mini|0.79|databricks (bedrock)|&lt;/p&gt; &lt;p&gt;|8|qwq|0.77|ollama|&lt;/p&gt; &lt;p&gt;|9|gpt-4o-mini|0.74|databricks (bedrock)|&lt;/p&gt; &lt;p&gt;|10|deepseek-chat-v3-0324|0.73|openrouter|&lt;/p&gt; &lt;p&gt;|11|gpt-4-5-preview|0.67|databricks|&lt;/p&gt; &lt;p&gt;|12|qwen2.5:32b|0.64|ollama|&lt;/p&gt; &lt;p&gt;|13|qwen2.5:14b|0.62|ollama|&lt;/p&gt; &lt;p&gt;|14|qwen2.5-coder:14b|0.51|ollama|&lt;/p&gt; &lt;p&gt;|15|deepseek-r1-toolshim-mistral-nemo*|0.48|openrouter|&lt;/p&gt; &lt;p&gt;|16|llama3.3:70b-instruct-q4_K_M|0.47|ollama|&lt;/p&gt; &lt;p&gt;|17|phi4-toolshim-mistral-nemo*|0.46|ollama|&lt;/p&gt; &lt;p&gt;|18|phi4-mistral-nemo|0.45|ollama|&lt;/p&gt; &lt;p&gt;|19|gemma3:27b-toolshim-mistral-nemo*|0.43|ollama|&lt;/p&gt; &lt;p&gt;|20|deepseek-r1-toolshim-qwen2.5-coder7b*|0.42|openrouter|&lt;/p&gt; &lt;p&gt;|21|llama3.3:70b-instruct-q8_0|0.41|ollama|&lt;/p&gt; &lt;p&gt;|22|deepseek-r1:14b-toolshim-mistral-nemo*|0.37|openrouter|&lt;/p&gt; &lt;p&gt;|23|deepseek-r1-distill-llama-70b-toolshim-mistral-nemo*|0.36|ollama|&lt;/p&gt; &lt;p&gt;|24|phi4-toolshim-qwen2.5-coder7b*|0.3|ollama|&lt;/p&gt; &lt;p&gt;|25|mistral-nemo|0.27|ollama|&lt;/p&gt; &lt;p&gt;|26|deepseek-r1-distill-llama-70b-toolshim-qwen2.5-coder7b*|0.26|openrouter|&lt;/p&gt; &lt;p&gt;|27|llama3.2|0.25|ollama|&lt;/p&gt; &lt;p&gt;|28|gemma3:27b-toolshim-qwen2.5-coder7b*|0.24|ollama|&lt;/p&gt; &lt;p&gt;|29|deepseek-r1:14b-toolshim-qwen2.5-coder7b*|0.22|ollama|&lt;/p&gt; &lt;p&gt;|29|gemma3:12b-toolshim-qwen2.5-coder7b*|0.22|ollama|&lt;/p&gt; &lt;p&gt;|31|mistral|0.17|ollama|&lt;/p&gt; &lt;p&gt;|32|gemma3:12b-toolshim-mistral-nemo*|0.15|ollama|&lt;/p&gt; &lt;p&gt;I'm pretty excited about Qwen/QwQ/Deepseek-chat from these rankings! I'm impressed with the 32B model size performance although the tasks I tried are admittedly simple.&lt;/p&gt; &lt;p&gt;Here are some screenshots and gifs comparing some of the results across the models:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v36hanhlgbse1.png?width=1898&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4522686b361aced31272dd7335b2873420716421"&gt;Claude 3.7 Sonnet&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7usml71qgbse1.png?width=2144&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=59ccdb513735841b521b49de257a58afe91d50d6"&gt;deepseek-chat-v3-0324&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h94udhotgbse1.png?width=2144&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a5ecc853cab4c97cb391340c18052ed23343ac93"&gt;qwen2.5-coder:32b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n6j18kyxgbse1.png?width=2144&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=607bfd82876c3d79b611359c8d255a487b2d0b77"&gt;deepseek-r1 70B with mistral-nemo as the tool interpreter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/lslc1mg8hbse1.gif"&gt;deepseek-chat-v3-0324&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/6fr1c0oehbse1.gif"&gt;qwq&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/hitdcjaihbse1.gif"&gt;qwen2.5-coder:32b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/asn7ovnmhbse1.gif"&gt;deepseek-r1 with mistral-nemo tool interpreter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;here's the full blogpost about it I wrote with more results: &lt;a href="https://block.github.io/goose/blog/2025/03/31/goose-benchmark"&gt;https://block.github.io/goose/blog/2025/03/31/goose-benchmark&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lifelonglearn3r"&gt; /u/lifelonglearn3r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpbptf/tried_a_bunch_of_open_models_with_goose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpbptf/tried_a_bunch_of_open_models_with_goose/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpbptf/tried_a_bunch_of_open_models_with_goose/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T00:40:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp9hu6</id>
    <title>Why isn't the whole industry focusing on online-learning?</title>
    <updated>2025-04-01T22:58:03+00:00</updated>
    <author>
      <name>/u/unraveleverything</name>
      <uri>https://old.reddit.com/user/unraveleverything</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLMs (currently) have no memory. You will always be able to tell LLMs from humans because LLMs are stateless. Right now you basically have a bunch of hacks like system prompts and RAG that tries to make it resemble something its not. &lt;/p&gt; &lt;p&gt;So what about concurrent multi-(Q)LoRA serving? Tell me why there's seemingly no research in this direction? &amp;quot;AGI&amp;quot; to me seems as simple as freezing the base weights, then training 1-pass over the context for memory. Like say your goal is to understand a codebase. Just train a LoRA on 1 pass through that codebase? First you give it the folder/file structure then the codebase. Tell me why this woudn't work. Then 1 node can handle multiple concurrent users and by storing 1 small LoRA for each user.&lt;/p&gt; &lt;p&gt;&lt;a href="https://gitingest.com/microsoft/LoRA"&gt;Ex&lt;/a&gt;: ``` Directory structure: ‚îî‚îÄ‚îÄ microsoft-lora/ ‚îú‚îÄ‚îÄ README.md ‚îú‚îÄ‚îÄ LICENSE.md ‚îú‚îÄ‚îÄ SECURITY.md ‚îú‚îÄ‚îÄ setup.py ‚îú‚îÄ‚îÄ examples/ ‚îÇ ‚îú‚îÄ‚îÄ NLG/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ README.md ...&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;h1&gt;File: README.md&lt;/h1&gt; &lt;h1&gt;LoRA: Low-Rank Adaptation of Large Language Models&lt;/h1&gt; &lt;p&gt;This repo contains the source code of the Python package &lt;code&gt;loralib&lt;/code&gt; and several examples of how to integrate it with PyTorch models, such as those in Hugging Face. We only support PyTorch for now. See our paper for a detailed description of LoRA. ...&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;h1&gt;File: LICENSE.md&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;MIT License Copyright (c) Microsoft Corporation. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the &amp;quot;Software&amp;quot;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;... ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unraveleverything"&gt; /u/unraveleverything &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9hu6/why_isnt_the_whole_industry_focusing_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9hu6/why_isnt_the_whole_industry_focusing_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9hu6/why_isnt_the_whole_industry_focusing_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T22:58:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1joyigi</id>
    <title>GemmaCoder3-12b: Fine-Tuning Gemma 3 for Code Reasoning</title>
    <updated>2025-04-01T15:32:33+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joyigi/gemmacoder312b_finetuning_gemma_3_for_code/"&gt; &lt;img alt="GemmaCoder3-12b: Fine-Tuning Gemma 3 for Code Reasoning" src="https://external-preview.redd.it/gH2ta8Ny0Bg1Qm8qZdfZlafv4Sz_L1pzxh-y3yKJtZ8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2914dd3fff11503f8f5f868b03abfbe2d8a5ee73" title="GemmaCoder3-12b: Fine-Tuning Gemma 3 for Code Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/burtenshaw/google-gemma3-gemma-code"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joyigi/gemmacoder312b_finetuning_gemma_3_for_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joyigi/gemmacoder312b_finetuning_gemma_3_for_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T15:32:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp5g2l</id>
    <title>Is a multimodal focused release from openai the best for us?</title>
    <updated>2025-04-01T20:08:29+00:00</updated>
    <author>
      <name>/u/AryanEmbered</name>
      <uri>https://old.reddit.com/user/AryanEmbered</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp5g2l/is_a_multimodal_focused_release_from_openai_the/"&gt; &lt;img alt="Is a multimodal focused release from openai the best for us?" src="https://preview.redd.it/w31a75fy5ase1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf8159e2e72f93f2c1e7edfd8a2bb4a73c81275c" title="Is a multimodal focused release from openai the best for us?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like with the exception of Qwen 2.5 7b(11b) audio, we have seen almost no real progress in multimodality so far in open models.&lt;/p&gt; &lt;p&gt;It seems gippty 4o mini can now do advanced voice mode as well. &lt;/p&gt; &lt;p&gt;They keep saying its a model that can run on your hardware, and 4omini is estimated to be less than a 20B model consider how badly it gets mogged by mistral smol and others. &lt;/p&gt; &lt;p&gt;It would be great if we can get a shittier 4o mini but with all the features intact like audio and image output. (A llamalover can dream)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AryanEmbered"&gt; /u/AryanEmbered &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w31a75fy5ase1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp5g2l/is_a_multimodal_focused_release_from_openai_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp5g2l/is_a_multimodal_focused_release_from_openai_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T20:08:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpi1a7</id>
    <title>What are the options for local high quality text to speech?</title>
    <updated>2025-04-02T06:23:16+00:00</updated>
    <author>
      <name>/u/idleWizard</name>
      <uri>https://old.reddit.com/user/idleWizard</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It doesn't have to be real time. I just care for consistent voices&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/idleWizard"&gt; /u/idleWizard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpi1a7/what_are_the_options_for_local_high_quality_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpi1a7/what_are_the_options_for_local_high_quality_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpi1a7/what_are_the_options_for_local_high_quality_text/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T06:23:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpi1kg</id>
    <title>Why there's no working vision mistral small gguf?</title>
    <updated>2025-04-02T06:23:48+00:00</updated>
    <author>
      <name>/u/kweglinski</name>
      <uri>https://old.reddit.com/user/kweglinski</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama don't even have official support for mistral small. There are user made ggufs that (mostly) work great for text but none works for image properly. When I test with mistral API it produces decent outputs for image but the local ggufs are completely hallucinating on vision.&lt;/p&gt; &lt;p&gt;I like mistral more than gemma3 for my usecases but lack of image makes me sad. &lt;/p&gt; &lt;p&gt;p.s. don't get me wrong, gemma is great, it's just my own preference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kweglinski"&gt; /u/kweglinski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpi1kg/why_theres_no_working_vision_mistral_small_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpi1kg/why_theres_no_working_vision_mistral_small_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpi1kg/why_theres_no_working_vision_mistral_small_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T06:23:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpi5dl</id>
    <title>Are there any TTS with different speaking styles such as Story, News, Narrator ect..? or any good voice clones which does not sound robotic..?</title>
    <updated>2025-04-02T06:31:34+00:00</updated>
    <author>
      <name>/u/udappk_metta</name>
      <uri>https://old.reddit.com/user/udappk_metta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently have Kokoro TTS. Orpheus TTS, XTTS and i have tried SpearkTTS, Zonos tts, STyle TTS, F5 TTS but i couldn't find anything which is less robotic or does not stutter.. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/udappk_metta"&gt; /u/udappk_metta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpi5dl/are_there_any_tts_with_different_speaking_styles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpi5dl/are_there_any_tts_with_different_speaking_styles/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpi5dl/are_there_any_tts_with_different_speaking_styles/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T06:31:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1josy27</id>
    <title>An idea: an LLM trapped in the past</title>
    <updated>2025-04-01T11:09:36+00:00</updated>
    <author>
      <name>/u/Vehnum</name>
      <uri>https://old.reddit.com/user/Vehnum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone ever thought to make an LLM trained on data from before a certain year/time?&lt;/p&gt; &lt;p&gt;For example, an LLM trained on data only from 2010 or prior.&lt;/p&gt; &lt;p&gt;I thought it was an interesting concept but I don‚Äôt know if it had been thought of or done before. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vehnum"&gt; /u/Vehnum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1josy27/an_idea_an_llm_trapped_in_the_past/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1josy27/an_idea_an_llm_trapped_in_the_past/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1josy27/an_idea_an_llm_trapped_in_the_past/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T11:09:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp77z1</id>
    <title>Arch-Function-Chat (1B/3B/7B) - Device friendly, family of fast LLMs for function calling scenarios now trained to chat.</title>
    <updated>2025-04-01T21:20:18+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on feedback from users and the developer community that used Arch-Function (our previous gen) model, I am excited to share our latest work: &lt;a href="https://huggingface.co/katanemo/Arch-Function-Chat-3B"&gt;Arch-Function-Chat&lt;/a&gt; A collection of fast, device friendly LLMs that achieve performance on-par with GPT-4 on function calling, now trained to chat.&lt;/p&gt; &lt;p&gt;These LLMs have three additional training objectives.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Be able to refine and clarify the user request. This means to ask for required function parameters, clarify ambiguous input (e.g., &amp;quot;Transfer $500&amp;quot; without specifying accounts, can be ‚ÄúTransfer from‚Äù and ‚ÄúTransfer to‚Äù)&lt;/li&gt; &lt;li&gt;Accurately maintain context in two specific scenarios: &lt;ol&gt; &lt;li&gt;Progressive information disclosure such as in multi-turn conversations where information is revealed gradually (i.e., the model asks info of multiple parameters and the user only answers one or two instead of all the info)&lt;/li&gt; &lt;li&gt;Context switch where the model must infer missing parameters from context (e.g., &amp;quot;Check the weather&amp;quot; should prompt for location if not provided) and maintains context between turns (e.g., &amp;quot;What about tomorrow?&amp;quot; after a weather query but still in the middle of clarification)&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Respond to the user based on executed tools results. For common function calling scenarios where the response of the execution is all that's needed to complete the user request, Arch-Function-Chat can interpret and respond to the user via chat. Note, parallel and multiple function calling was already supported so if the model needs to respond based on multiple tools call it still can.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Of course the 3B model will now be the primary LLM used in &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;. Hope you all like the work üôè. Happy building!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp77z1/archfunctionchat_1b3b7b_device_friendly_family_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp77z1/archfunctionchat_1b3b7b_device_friendly_family_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp77z1/archfunctionchat_1b3b7b_device_friendly_family_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T21:20:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp9emz</id>
    <title>Easy Whisper UI for Windows</title>
    <updated>2025-04-01T22:54:08+00:00</updated>
    <author>
      <name>/u/mehtabmahir</name>
      <uri>https://old.reddit.com/user/mehtabmahir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made an easy to use UI for Whisper on windows. It is completely made with C++ and has Vulkan support for all gpus. I posted it here recently, but I've since made several major improvements. Please let me know your results, the installer should handle absolutely everything for you!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mehtabmahir/easy-whisper-ui"&gt;https://github.com/mehtabmahir/easy-whisper-ui&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehtabmahir"&gt; /u/mehtabmahir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9emz/easy_whisper_ui_for_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9emz/easy_whisper_ui_for_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9emz/easy_whisper_ui_for_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T22:54:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpcmv2</id>
    <title>Real-Time Introspective Compression for Transformers</title>
    <updated>2025-04-02T01:25:57+00:00</updated>
    <author>
      <name>/u/dicklesworth</name>
      <uri>https://old.reddit.com/user/dicklesworth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpcmv2/realtime_introspective_compression_for/"&gt; &lt;img alt="Real-Time Introspective Compression for Transformers" src="https://external-preview.redd.it/nSUCPw-OePlsDcFeEZ-RExxT1yhTaclOUN7asZP7pyQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9274e777d90eb0a2fcfd188a0baead19fa50ecbf" title="Real-Time Introspective Compression for Transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently started thinking about what a shame it is that LLMs have no way of directly accessing their own internal states, and how potentially useful that would be if they could. One thing led to the next, and I ended up developing those ideas a lot further.&lt;/p&gt; &lt;p&gt;Transformers today discard internal states after each token, losing valuable information. There's no rollback, introspection, or replaying of their reasoning. Saving every activation isn't practical; it would require way too much space (hundreds of megabytes at least).&lt;/p&gt; &lt;p&gt;The insight here is that transformer activations aren't randomly scattered in high-dimensional space. Instead, they form structured, lower-dimensional manifolds shaped by architecture, language structure, and learned tasks. It's all sitting on a paper-thin membrane in N-space!&lt;/p&gt; &lt;p&gt;This suggested a neat analogy: just like video games save compact states (player location, inventory, progress flags) instead of full frames, transformers could efficiently save &amp;quot;thought states,&amp;quot; reconstructable at any time. Reload your saved game, for LLMs!&lt;/p&gt; &lt;p&gt;Here's the approach: attach a small sidecar model alongside a transformer to compress its internal states into compact latent codes. These codes can later be decoded to reconstruct the hidden states and attention caches. The trick is to compress stuff a LOT, but not be TOO lossy.&lt;/p&gt; &lt;p&gt;What new capabilities would this enable? Transformers could rewind their thoughts, debug errors at the latent level, or explore alternative decision paths. RL agents could optimize entire thought trajectories instead of just outputs. A joystick for the brain if you will.&lt;/p&gt; &lt;p&gt;This leads naturally to the concept of a rewindable reasoning graph, where each compressed state is a node. Models could precisely backtrack, branch into alternate reasoning paths, and debug the causes of errors internally. Like a thoughtful person can (hopefully!).&lt;/p&gt; &lt;p&gt;Longer-term, it suggests something bigger: a metacognitive operating system for transformers, enabling AI to practice difficult reasoning tasks repeatedly, refine cognitive strategies, and transfer learned skills across domains. Learning from learning, if you will.&lt;/p&gt; &lt;p&gt;Ultimately, the core shift is moving transformers from stateless text generators into cognitive systems capable of reflective self-improvement. It's a fundamentally new way for AI to become better at thinking.&lt;/p&gt; &lt;p&gt;For fun, I wrote it up and formatted it as a fancy academic-looking paper, which you can read here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Dicklesworthstone/llm_introspective_compression_and_metacognition/main/introspective_compression_for_llms.pdf"&gt;https://raw.githubusercontent.com/Dicklesworthstone/llm_introspective_compression_and_metacognition/main/introspective_compression_for_llms.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dicklesworth"&gt; /u/dicklesworth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Dicklesworthstone/llm_introspective_compression_and_metacognition"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpcmv2/realtime_introspective_compression_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpcmv2/realtime_introspective_compression_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T01:25:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1joyl9t</id>
    <title>New GGUF quants of V3-0324</title>
    <updated>2025-04-01T15:35:49+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joyl9t/new_gguf_quants_of_v30324/"&gt; &lt;img alt="New GGUF quants of V3-0324" src="https://external-preview.redd.it/VVDuLhNJdXUv9Ha7btms0J33I6ffqYD7axOIbyejSC4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb260f4a149e4b5107b97b86ee6df9cf84939894" title="New GGUF quants of V3-0324" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I cooked up these fresh new quants on &lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;ikawrakow/ik_llama.cpp&lt;/a&gt; supporting 32k+ context in under 24GB VRAM with MLA with highest quality tensors for attention/dense layers/shared experts.&lt;/p&gt; &lt;p&gt;Good both for CPU+GPU or CPU only rigs with optimized repacked quant flavours to get the most out of your RAM.&lt;/p&gt; &lt;p&gt;&lt;em&gt;NOTE&lt;/em&gt;: These quants only work with &lt;code&gt;ik_llama.cpp&lt;/code&gt; fork and won't work with mainline llama.cpp, ollama, lm studio, koboldcpp, etc.&lt;/p&gt; &lt;p&gt;Shout out to &lt;a href="https://www.youtube.com/c/level1techs"&gt;level1techs&lt;/a&gt; for supporting this research on some sweet hardware rigs!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/DeepSeek-V3-0324-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joyl9t/new_gguf_quants_of_v30324/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joyl9t/new_gguf_quants_of_v30324/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T15:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpihhv</id>
    <title>I made an open source react component generator that runs locally with good design! It uses Tessa-T1 in the backend.</title>
    <updated>2025-04-02T06:56:51+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpihhv/i_made_an_open_source_react_component_generator/"&gt; &lt;img alt="I made an open source react component generator that runs locally with good design! It uses Tessa-T1 in the backend." src="https://external-preview.redd.it/MWoxcGU1ZTFiZHNlMX9PqpBj2YAnX76IEjgEKiQJ551Zajgf2-v9_Sq13wrH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1f6d84bd871ba7c9c49d636f3f90ba90227f893" title="I made an open source react component generator that runs locally with good design! It uses Tessa-T1 in the backend." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can get the code here: &lt;a href="https://github.com/TesslateAI/Builder"&gt;https://github.com/TesslateAI/Builder&lt;/a&gt; (The main branch is integrated with Gemini if you are looking for a more robust solution that generates multiple independent pages of code).&lt;/p&gt; &lt;p&gt;The underlying local LLM is Tessa-T1 &lt;a href="https://huggingface.co/collections/Tesslate/tessa-t1-react-reasoning-model-67e0fb72ca23e04473885c0e"&gt;https://huggingface.co/collections/Tesslate/tessa-t1-react-reasoning-model-67e0fb72ca23e04473885c0e&lt;/a&gt; running on LM Studio on a 4090 (q8 at 34/s)&lt;/p&gt; &lt;p&gt;Tessa-T1 uses special UI based reasoning to help it excel in the UI/UX domain. It was trained via a custom model. For example:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Okay, so we're aiming for a Twitter-like interface, but with a dark theme. Let's break down how we'd approach building this in React using Tailwind CSS. Initially, when thinking about the overall structure of our app, I'm envisioning two main sections side-by-side ‚Äì like any typical split layout: one for the sidebar and one for the content itself. This immediately suggests a flex container to hold these elements horizontally.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And ofc, GGUF &lt;a href="https://huggingface.co/Tesslate/Tessa-T1-14B-Q8_0-GGUF"&gt;HERE&lt;/a&gt; (and quants are available by Bartowski). As always, please let me know if you have any suggestions or what else (models, agents, etc) you would like to see!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ad7gb1e1bdse1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpihhv/i_made_an_open_source_react_component_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpihhv/i_made_an_open_source_react_component_generator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T06:56:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jph6p2</id>
    <title>Multi-Token Attention</title>
    <updated>2025-04-02T05:25:11+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This &amp;quot;single token attention&amp;quot; bottlenecks the amount of information used in distinguishing a relevant part from the rest of the context. To address this issue, we propose a new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other's attention weights for more precise attention. As a result, our method can locate relevant context using richer, more nuanced information that can exceed a single vector's capacity. Through extensive evaluations, we demonstrate that MTA achieves enhanced performance on a range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where our method's ability to leverage richer information proves particularly beneficial.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2504.00927"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jph6p2/multitoken_attention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jph6p2/multitoken_attention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T05:25:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpez1o</id>
    <title>Qwen2.5-VL-32B and Mistral small tested against close source competitors</title>
    <updated>2025-04-02T03:12:42+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, so put a lot of time and burnt a ton of tokens testing this, so hope you all find it useful. TLDR - Qwen and Mistral beat all GPT models by a wide margin. Qwen even beat Gemini to come in a close second behind sonnet. Mistral is the smallest of the lot and still does better than 4-o. Qwen is surprisingly good - 32b is just as good if not better than 72. Cant wait for Qwen 3, we might have a new leader, sonnet needs to watch its back....&lt;/p&gt; &lt;p&gt;You dont have to watch the whole thing, links to full evals in the video description. Timestamp to just the results if you are not interested in understing the test setup in the description as well.&lt;/p&gt; &lt;p&gt;I welcome your feedback...&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/ZTJmjhMjlpM"&gt;https://youtu.be/ZTJmjhMjlpM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpez1o/qwen25vl32b_and_mistral_small_tested_against/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpez1o/qwen25vl32b_and_mistral_small_tested_against/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpez1o/qwen25vl32b_and_mistral_small_tested_against/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T03:12:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1joqnp0</id>
    <title>Top reasoning LLMs failed horribly on USA Math Olympiad (maximum 5% score)</title>
    <updated>2025-04-01T08:28:37+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joqnp0/top_reasoning_llms_failed_horribly_on_usa_math/"&gt; &lt;img alt="Top reasoning LLMs failed horribly on USA Math Olympiad (maximum 5% score)" src="https://preview.redd.it/lbaxwpako6se1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fe2ebb66027a7c9112a0c9566eaf397ca2d5a18" title="Top reasoning LLMs failed horribly on USA Math Olympiad (maximum 5% score)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need to share something that‚Äôs blown my mind today. I just came across &lt;a href="https://arxiv.org/abs/2503.21934v1"&gt;this paper &lt;/a&gt;evaluating state-of-the-art LLMs (like O3-MINI, Claude 3.7, etc.) on the 2025 USA Mathematical Olympiad (USAMO). And let me tell you‚Äîthis is &lt;em&gt;wild&lt;/em&gt; .&lt;/p&gt; &lt;h1&gt;The Results&lt;/h1&gt; &lt;p&gt;These models were tested on &lt;strong&gt;six proof-based math problems&lt;/strong&gt; from the 2025 USAMO. Each problem was scored out of 7 points, with a max total score of 42. Human experts graded their solutions rigorously.&lt;/p&gt; &lt;p&gt;The highest average score achieved by &lt;strong&gt;any model&lt;/strong&gt; ? &lt;strong&gt;Less than 5%.&lt;/strong&gt; Yes, you read that right: &lt;strong&gt;5%.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Even worse, when these models tried grading their own work (e.g., O3-MINI and Claude 3.7), they consistently &lt;strong&gt;overestimated their scores&lt;/strong&gt; , inflating them by up to &lt;strong&gt;20x&lt;/strong&gt; compared to human graders.&lt;/p&gt; &lt;h1&gt;Why This Matters&lt;/h1&gt; &lt;p&gt;These models have been trained on &lt;strong&gt;all the math data imaginable&lt;/strong&gt; ‚ÄîIMO problems, USAMO archives, textbooks, papers, etc. They‚Äôve seen it all. Yet, they struggle with tasks requiring deep logical reasoning, creativity, and rigorous proofs.&lt;/p&gt; &lt;p&gt;Here are some key issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Logical Failures&lt;/strong&gt; : Models made unjustified leaps in reasoning or labeled critical steps as &amp;quot;trivial.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lack of Creativity&lt;/strong&gt; : Most models stuck to the same flawed strategies repeatedly, failing to explore alternatives.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grading Failures&lt;/strong&gt; : Automated grading by LLMs inflated scores dramatically, showing they can't even evaluate their own work reliably.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Given that billions of dollars have been poured into investments on these models with the hope of it can &amp;quot;generalize&amp;quot; and do &amp;quot;crazy lift&amp;quot; in human knowledge, this result is shocking. Given the models here are probably trained on all Olympiad data previous (USAMO, IMO ,... anything)&lt;/p&gt; &lt;p&gt;Link to the paper: &lt;a href="https://arxiv.org/abs/2503.21934v1"&gt;https://arxiv.org/abs/2503.21934v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lbaxwpako6se1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joqnp0/top_reasoning_llms_failed_horribly_on_usa_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joqnp0/top_reasoning_llms_failed_horribly_on_usa_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T08:28:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jotzue</id>
    <title>Just upgraded my RTX 3060 with 192GB of VRAM</title>
    <updated>2025-04-01T12:09:43+00:00</updated>
    <author>
      <name>/u/Wrong_User_Logged</name>
      <uri>https://old.reddit.com/user/Wrong_User_Logged</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jotzue/just_upgraded_my_rtx_3060_with_192gb_of_vram/"&gt; &lt;img alt="Just upgraded my RTX 3060 with 192GB of VRAM" src="https://a.thumbs.redditmedia.com/0HRndElj4m4MdUTfleIWN0cAk58xxJsG5xvLYGxCDg0.jpg" title="Just upgraded my RTX 3060 with 192GB of VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Soldered in some extra memory chips I had lying around. Runs now Deepseek R1 with 1.6 bits at 8 t/s.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rzmtxp5gs7se1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=68bbae0f177ee26b9e9dd5d80ced43ca1ab364b8"&gt;https://preview.redd.it/rzmtxp5gs7se1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=68bbae0f177ee26b9e9dd5d80ced43ca1ab364b8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong_User_Logged"&gt; /u/Wrong_User_Logged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jotzue/just_upgraded_my_rtx_3060_with_192gb_of_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jotzue/just_upgraded_my_rtx_3060_with_192gb_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jotzue/just_upgraded_my_rtx_3060_with_192gb_of_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T12:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp5y5a</id>
    <title>Different LLM models make different sounds from the GPU when doing inference</title>
    <updated>2025-04-01T20:28:29+00:00</updated>
    <author>
      <name>/u/vibjelo</name>
      <uri>https://old.reddit.com/user/vibjelo</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibjelo"&gt; /u/vibjelo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://bsky.app/profile/victor.earth/post/3llrphluwb22p"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp5y5a/different_llm_models_make_different_sounds_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp5y5a/different_llm_models_make_different_sounds_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T20:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpa1ep</id>
    <title>I got tired of guessing what blackbox AI coding tools were sending as prompt context... so I built a transparent local open-source coding tool</title>
    <updated>2025-04-01T23:22:03+00:00</updated>
    <author>
      <name>/u/wwwillchen</name>
      <uri>https://old.reddit.com/user/wwwillchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpa1ep/i_got_tired_of_guessing_what_blackbox_ai_coding/"&gt; &lt;img alt="I got tired of guessing what blackbox AI coding tools were sending as prompt context... so I built a transparent local open-source coding tool" src="https://external-preview.redd.it/bjBlY3dlMHYyYnNlMeuto_4yHK9N3Xzvw4yI_cUvoQNTs3J3u5A3WEOq9BHN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a88ce2fe0beae410002f53b85b4dc4272dd04a98" title="I got tired of guessing what blackbox AI coding tools were sending as prompt context... so I built a transparent local open-source coding tool" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Cursor &amp;amp; GitHub Copilot and found it frustrating that I couldn't see what prompts were actually being sent.&lt;/p&gt; &lt;p&gt;For example, I have no idea why I got wildly different results when I sent the same prompt to Cursor vs ChatGPT with o3-mini, where the Cursor response was much shorter (and also incorrect) compared to ChatGPT's.&lt;/p&gt; &lt;p&gt;So, I've built a new open-source AI coding tool Dyad that runs locally: &lt;a href="https://github.com/dyad-sh/dyad"&gt;https://github.com/dyad-sh/dyad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It just got a new LLM debugging page that shows exactly what‚Äôs being sent to the model, so you can finally understand why the LLM is responding the way it does.&lt;/p&gt; &lt;p&gt;More demos of the tool here: &lt;a href="https://dyad.sh/"&gt;https://dyad.sh/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think. Is this useful?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wwwillchen"&gt; /u/wwwillchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8xw67g0v2bse1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpa1ep/i_got_tired_of_guessing_what_blackbox_ai_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpa1ep/i_got_tired_of_guessing_what_blackbox_ai_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T23:22:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1joy1g9</id>
    <title>You can now check if your Laptop/ Rig can run a GGUF directly from Hugging Face! ü§ó</title>
    <updated>2025-04-01T15:13:10+00:00</updated>
    <author>
      <name>/u/vaibhavs10</name>
      <uri>https://old.reddit.com/user/vaibhavs10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joy1g9/you_can_now_check_if_your_laptop_rig_can_run_a/"&gt; &lt;img alt="You can now check if your Laptop/ Rig can run a GGUF directly from Hugging Face! ü§ó" src="https://external-preview.redd.it/cjl0NGVwNTJwOHNlMcYNeeStsI4th9K4vfQkpXTEQka5SvAFbcRXwVJ4maQB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d009dc08fd59bef372f2ca0785fa2ef200fe3ea8" title="You can now check if your Laptop/ Rig can run a GGUF directly from Hugging Face! ü§ó" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaibhavs10"&gt; /u/vaibhavs10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0bo4dp52p8se1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joy1g9/you_can_now_check_if_your_laptop_rig_can_run_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joy1g9/you_can_now_check_if_your_laptop_rig_can_run_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T15:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp9tfh</id>
    <title>ü™øQwerky-72B and 32B : Training large attention free models, with only 8 GPU's</title>
    <updated>2025-04-01T23:12:05+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9tfh/qwerky72b_and_32b_training_large_attention_free/"&gt; &lt;img alt="ü™øQwerky-72B and 32B : Training large attention free models, with only 8 GPU's" src="https://preview.redd.it/hzuxqeqn2bse1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=687e079083a01b404da217fc45bd385974523d62" title="ü™øQwerky-72B and 32B : Training large attention free models, with only 8 GPU's" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hzuxqeqn2bse1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9tfh/qwerky72b_and_32b_training_large_attention_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9tfh/qwerky72b_and_32b_training_large_attention_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T23:12:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpdre9</id>
    <title>I made it! 90 t/s on my iPhone with llama1b fp16</title>
    <updated>2025-04-02T02:19:03+00:00</updated>
    <author>
      <name>/u/darkolorin</name>
      <uri>https://old.reddit.com/user/darkolorin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We completely rewrite the inference engine and did some tricks. This is a summarization with llama 3.2 1b float16. So most of the times we do much faster than MLX. lmk in comments if you wanna test the inference and I‚Äôll post a link. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkolorin"&gt; /u/darkolorin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fh2xne3uzbse1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpdre9/i_made_it_90_ts_on_my_iphone_with_llama1b_fp16/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpdre9/i_made_it_90_ts_on_my_iphone_with_llama1b_fp16/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T02:19:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp1555</id>
    <title>DeepMind will delay sharing research to remain competitive</title>
    <updated>2025-04-01T17:17:47+00:00</updated>
    <author>
      <name>/u/mayalihamur</name>
      <uri>https://old.reddit.com/user/mayalihamur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;a href="https://archive.ph/tkuum"&gt;recent report&lt;/a&gt; in Financial Times claims that Google's DeepMind &amp;quot;has been holding back the release of its world-renowned research&amp;quot; to remain competitive. Accordingly the company will adopt a six-month embargo policy &amp;quot;before strategic papers related to generative AI are released&amp;quot;. &lt;/p&gt; &lt;p&gt;In an interesting statement, a DeepMind researcher said he could &amp;quot;not imagine us putting out the transformer papers for general use now&amp;quot;. Considering the impact of the DeepMind's transformer research on the development of LLMs, just think where we would have been now if they held back the research. The report also claims that some DeepMind staff left the company as their careers would be negatively affected if they are not allowed to publish their research. &lt;/p&gt; &lt;p&gt;I don't have any knowledge about the current impact of DeepMind's open research contributions. But just a couple of months ago we have been talking about the potential contributions the DeepSeek release will make. But as it gets competitive it looks like the big players are slowly becoming &lt;del&gt;Open&lt;/del&gt;ClosedAIs. &lt;/p&gt; &lt;p&gt;Too bad, let's hope that this won't turn into a general trend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mayalihamur"&gt; /u/mayalihamur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T17:17:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpi0n9</id>
    <title>KTransformers Now Supports Multi-Concurrency and Runs 40 Tokens/s of DeepSeek-R1 Q4/FP8 on MRDIMM-8800</title>
    <updated>2025-04-02T06:22:05+00:00</updated>
    <author>
      <name>/u/CombinationNo780</name>
      <uri>https://old.reddit.com/user/CombinationNo780</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpi0n9/ktransformers_now_supports_multiconcurrency_and/"&gt; &lt;img alt="KTransformers Now Supports Multi-Concurrency and Runs 40 Tokens/s of DeepSeek-R1 Q4/FP8 on MRDIMM-8800" src="https://external-preview.redd.it/02ytj9SuhUQUk607vUmaEPEVgYFEBjKWtivCx0rWwKk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=26792f11e8a9b5c5d3f26c92c937264cb15d0fec" title="KTransformers Now Supports Multi-Concurrency and Runs 40 Tokens/s of DeepSeek-R1 Q4/FP8 on MRDIMM-8800" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, it's been a while since our last update. &lt;/p&gt; &lt;p&gt;We've been hard at work completely refactoring KTransformers to add the highly desired multi-concurrency support. This effort involved over 10,000 lines of code updates and took longer than we expected.&lt;/p&gt; &lt;p&gt;Drawing inspiration from the excellent architecture of sglang, we have implemented high-performance asynchronous concurrent scheduling in C++, including features like &lt;strong&gt;continuous batching, chunked prefill,&lt;/strong&gt; and more. Thanks to GPU sharing in concurrent scenarios and the efficient flashinfer lib, overall throughput has also improved to a certain extent.&lt;/p&gt; &lt;p&gt;Also, with support from Intel, we tested KTransformers v0.2.4 on the latest Xeon6 + MRDIMM-8800 platform. By increasing concurrency, the total output throughput increased &lt;strong&gt;from 17 tokens/s to 40 tokens/s.&lt;/strong&gt; We observed that the bottleneck has now shifted to the GPU. Using a higher-end GPU than the 4090D could further improve performance.&lt;/p&gt; &lt;p&gt;The following is a demonstration and you can find more infomation from &lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/balance-serve.md"&gt;https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/balance-serve.md&lt;/a&gt; :&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/10g65zko0dse1.jpg?width=2560&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d2af5901cb0f773d315bfdfb324bb3c8ecf61a72"&gt;https://preview.redd.it/10g65zko0dse1.jpg?width=2560&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d2af5901cb0f773d315bfdfb324bb3c8ecf61a72&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After this huge refactoring, we can now start working on merging the AMX part and open sourcing it. We are sure that this will happen in April.&lt;/p&gt; &lt;p&gt;Finally, we greatly thank the local LLaMa community for your support. We now have over 13K GitHub stars and are widely deployed in many scenarios. KTransformers is a project that grew from the localLLaMa community, and we hope to see what you want next.&lt;/p&gt; &lt;p&gt;Stay tuned!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombinationNo780"&gt; /u/CombinationNo780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpi0n9/ktransformers_now_supports_multiconcurrency_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpi0n9/ktransformers_now_supports_multiconcurrency_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpi0n9/ktransformers_now_supports_multiconcurrency_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T06:22:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpbnih</id>
    <title>Qwen3 will be released in the second week of April</title>
    <updated>2025-04-02T00:37:43+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Exclusive from Huxiu: Alibaba is set to release its new model, Qwen3, in the second week of April 2025. This will be Alibaba's most significant model product in the first half of 2025, coming approximately seven months after the release of Qwen2.5 at the Yunqi Computing Conference in September 2024.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://m.huxiu.com/article/4187485.html"&gt;https://m.huxiu.com/article/4187485.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpbnih/qwen3_will_be_released_in_the_second_week_of_april/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpbnih/qwen3_will_be_released_in_the_second_week_of_april/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpbnih/qwen3_will_be_released_in_the_second_week_of_april/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T00:37:43+00:00</published>
  </entry>
</feed>
