<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-07T15:50:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kgh8zw</id>
    <title>I was shocked how Qwen3-235b-a22b is really good at math</title>
    <updated>2025-05-06T22:09:20+00:00</updated>
    <author>
      <name>/u/Surealistic_Sight</name>
      <uri>https://old.reddit.com/user/Surealistic_Sight</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello and I was searching for a ‚ÄúFree Math AI‚Äù and I am also a user of Qwen, besides DeepSeek and I don‚Äôt use ChatGPT anymore since a year. &lt;/p&gt; &lt;p&gt;But yeah, when I tried the strongest model from Qwen with some Math questions from the 2024 Austrian state exam (Matura). I was quite shocked how it correctly answered. I used also the Exam solutions PDF from the 2024 Matura and they were pretty correct. &lt;/p&gt; &lt;p&gt;I used thinking and the maximum Thinking budget of 38,912 tokens on their Website.&lt;/p&gt; &lt;p&gt;I know that Math and AI is always a topic for itself, because AI does more prediction than thinking, but I am really positive that LLMs could do really almost perfect Math in the Future.&lt;/p&gt; &lt;p&gt;I first thought with their claim that it excels in Math was a (marketing) lie, but I am confident to say is that can do math.&lt;/p&gt; &lt;p&gt;So, what do you think and do you also use this model to solve your math questions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Surealistic_Sight"&gt; /u/Surealistic_Sight &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgh8zw/i_was_shocked_how_qwen3235ba22b_is_really_good_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgh8zw/i_was_shocked_how_qwen3235ba22b_is_really_good_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgh8zw/i_was_shocked_how_qwen3235ba22b_is_really_good_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T22:09:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg7768</id>
    <title>Nvidia to drop CUDA support for Maxwell, Pascal, and Volta GPUs with the next major Toolkit release</title>
    <updated>2025-05-06T15:22:04+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/nvidia-to-drop-cuda-support-for-maxwell-pascal-and-volta-gpus-with-the-next-major-toolkit-release"&gt;https://www.tomshardware.com/pc-components/gpus/nvidia-to-drop-cuda-support-for-maxwell-pascal-and-volta-gpus-with-the-next-major-toolkit-release&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg7768/nvidia_to_drop_cuda_support_for_maxwell_pascal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg7768/nvidia_to_drop_cuda_support_for_maxwell_pascal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg7768/nvidia_to_drop_cuda_support_for_maxwell_pascal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T15:22:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgxzsz</id>
    <title>2x RTX 3060 vs 1x RTX 5060 Ti ‚Äî Need Advice!</title>
    <updated>2025-05-07T13:56:38+00:00</updated>
    <author>
      <name>/u/mr_house7</name>
      <uri>https://old.reddit.com/user/mr_house7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm planning a GPU upgrade and could really use some advice. I‚Äôm considering either:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;2x RTX 3060 (12GB VRAM each)&lt;/strong&gt; or&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1x RTX 5060 Ti&lt;/strong&gt; &lt;strong&gt;(16 VRAM)&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My current motherboard is a &lt;strong&gt;Micro-ATX MSI B550M PRO-VDH&lt;/strong&gt;, and I‚Äôm wondering a few things:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;How hard is it to run a 2x GPU setup&lt;/strong&gt; in general? For AI workloads.&lt;/li&gt; &lt;li&gt;Will my motherboard even support both GPUs functionally (&lt;strong&gt;Micro-ATX MSI B550M PRO-VDH&lt;/strong&gt;)?&lt;/li&gt; &lt;li&gt;From a performance and compatibility perspective, &lt;strong&gt;which setup would you recommend&lt;/strong&gt;?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I‚Äôm mainly using the system for AI/deep learning experiments and light gaming.&lt;/p&gt; &lt;p&gt;Any insights or personal experiences would be really appreciated. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_house7"&gt; /u/mr_house7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgxzsz/2x_rtx_3060_vs_1x_rtx_5060_ti_need_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgxzsz/2x_rtx_3060_vs_1x_rtx_5060_ti_need_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgxzsz/2x_rtx_3060_vs_1x_rtx_5060_ti_need_advice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T13:56:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgltqs</id>
    <title>Huawei Atlas 300I 32GB</title>
    <updated>2025-05-07T01:48:31+00:00</updated>
    <author>
      <name>/u/kruzibit</name>
      <uri>https://old.reddit.com/user/kruzibit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saw the Huawei Altas 300I 32GB version is now about USD265 on China Taobao.&lt;/p&gt; &lt;p&gt;Parameters&lt;/p&gt; &lt;p&gt;Atlas 300I Inference Card Model: 3000/3010&lt;/p&gt; &lt;p&gt;Form Factor: Half-height half-length PCIe standard card&lt;/p&gt; &lt;p&gt;AI Processor: Ascend Processor&lt;/p&gt; &lt;p&gt;Memory: LPDDR4X, 32 GB, total bandwidth 204.8 GB/s&lt;/p&gt; &lt;p&gt;Encoding/ Decoding:&lt;/p&gt; &lt;p&gt;‚Ä¢ H.264 hardware decoding, 64-channel 1080p 30 FPS (8-channel 3840 x 2160 @ 60 FPS)&lt;/p&gt; &lt;p&gt;‚Ä¢ H.265 hardware decoding, 64-channel 1080p 30 FPS (8-channel 3840 x 2160 @ 60 FPS)&lt;/p&gt; &lt;p&gt;‚Ä¢ H.264 hardware encoding, 4-channel 1080p 30 FPS &lt;/p&gt; &lt;p&gt;‚Ä¢ H.265 hardware encoding, 4-channel 1080p 30 FPS&lt;/p&gt; &lt;p&gt;‚Ä¢ JPEG decoding: 4-channel 1080p 256 FPS; encoding: 4-channel 1080p 64 FPS; maximum resolution: 8192 x 4320&lt;/p&gt; &lt;p&gt;‚Ä¢ PNG decoding: 4-channel 1080p 48 FPS; maximum resolution: 4096 x 2160&lt;/p&gt; &lt;p&gt;PCIe: PCIe x16 Gen3.0&lt;/p&gt; &lt;p&gt;Power Consumption Maximum: 67 W| |Operating&lt;/p&gt; &lt;p&gt;Temperature: 0¬∞C to 55¬∞C (32¬∞F to +131¬∞F)&lt;/p&gt; &lt;p&gt;Dimensions (W x D): 169.5 mm x 68.9 mm (6.67 in. x 2.71 in.)&lt;/p&gt; &lt;p&gt;Wonder how is the support. According to their website, can run 4 of them together.&lt;/p&gt; &lt;p&gt;Anyone has any idea?&lt;/p&gt; &lt;p&gt;There is a link on the 300i Duo that has 96GB tested against 4090. It is in chinese though.&lt;/p&gt; &lt;p&gt;&lt;a href="https://m.bilibili.com/video/BV1xB3TenE4s"&gt;https://m.bilibili.com/video/BV1xB3TenE4s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Running Ubuntu and llama3-hf. 4090 220t/s, 300i duo 150t/s&lt;/p&gt; &lt;p&gt;Found this on github: &lt;a href="https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md"&gt;https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kruzibit"&gt; /u/kruzibit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgltqs/huawei_atlas_300i_32gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgltqs/huawei_atlas_300i_32gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgltqs/huawei_atlas_300i_32gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T01:48:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgjkgf</id>
    <title>Blazing fast ASR / STT on Apple Silicon</title>
    <updated>2025-05-06T23:55:15+00:00</updated>
    <author>
      <name>/u/bio_risk</name>
      <uri>https://old.reddit.com/user/bio_risk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted about NVIDIAs updated ASR model a few days ago, hoping someone would be motivated to create an MLX version. &lt;/p&gt; &lt;p&gt;My internet pleas were answered by: &lt;a href="https://github.com/senstella/parakeet-mlx"&gt;https://github.com/senstella/parakeet-mlx&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Even on my old M1 8GB Air, it transcribed 11 minutes of audio in 14 seconds. Almost 60x real-time.&lt;/p&gt; &lt;p&gt;And this comes with top leader board WER: &lt;a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard"&gt;https://huggingface.co/spaces/hf-audio/open_asr_leaderboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bio_risk"&gt; /u/bio_risk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgjkgf/blazing_fast_asr_stt_on_apple_silicon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgjkgf/blazing_fast_asr_stt_on_apple_silicon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgjkgf/blazing_fast_asr_stt_on_apple_silicon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T23:55:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kggif3</id>
    <title>We now have local computer-use! M3 Pro 18GB running both UI-TARS-1.5-7B-6bit and a macOS sequoia VM entirely locally using MLX and c/ua at ~30second/action</title>
    <updated>2025-05-06T21:38:03+00:00</updated>
    <author>
      <name>/u/a6oo</name>
      <uri>https://old.reddit.com/user/a6oo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kggif3/we_now_have_local_computeruse_m3_pro_18gb_running/"&gt; &lt;img alt="We now have local computer-use! M3 Pro 18GB running both UI-TARS-1.5-7B-6bit and a macOS sequoia VM entirely locally using MLX and c/ua at ~30second/action" src="https://external-preview.redd.it/YTl4dGg5ZXVkOHplMdxnn65NNKVAPJFD0pCsNWgZyolHWVTVVUjy0pasvAbK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=194b04cbcdd57149e60f1ab30370907ce3659d86" title="We now have local computer-use! M3 Pro 18GB running both UI-TARS-1.5-7B-6bit and a macOS sequoia VM entirely locally using MLX and c/ua at ~30second/action" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a6oo"&gt; /u/a6oo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6okp9ioq38ze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kggif3/we_now_have_local_computeruse_m3_pro_18gb_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kggif3/we_now_have_local_computeruse_m3_pro_18gb_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T21:38:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgupcy</id>
    <title>Gemini 2.5 Pro 05-06 (IO Edition)</title>
    <updated>2025-05-07T11:12:09+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgupcy/gemini_25_pro_0506_io_edition/"&gt; &lt;img alt="Gemini 2.5 Pro 05-06 (IO Edition)" src="https://external-preview.redd.it/AcvY2iQydAkuLrR9B5tut99az-M6OzZYxQpuRtv_6NM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b24b4187cb1c776988e7af64e34ada19e1c9936b" title="Gemini 2.5 Pro 05-06 (IO Edition)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kgupcy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgupcy/gemini_25_pro_0506_io_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgupcy/gemini_25_pro_0506_io_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T11:12:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgmxla</id>
    <title>Jorney of increasing Pre Processing T/s on DeepSeek Q2_K_XL with ~120GB VRAM and ~140GB RAM (7800X3D, 6000Mhz), from 39 t/s to 66 t/s to 100 t/s to 126 t/s, thanks to PCI-E 5.0 and MLA+FA PR.</title>
    <updated>2025-05-07T02:46:09+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgmxla/jorney_of_increasing_pre_processing_ts_on/"&gt; &lt;img alt="Jorney of increasing Pre Processing T/s on DeepSeek Q2_K_XL with ~120GB VRAM and ~140GB RAM (7800X3D, 6000Mhz), from 39 t/s to 66 t/s to 100 t/s to 126 t/s, thanks to PCI-E 5.0 and MLA+FA PR." src="https://b.thumbs.redditmedia.com/vXP6OTmop0Tb5Fc4au0iHpEyeWiz25oLP_d3hor924k.jpg" title="Jorney of increasing Pre Processing T/s on DeepSeek Q2_K_XL with ~120GB VRAM and ~140GB RAM (7800X3D, 6000Mhz), from 39 t/s to 66 t/s to 100 t/s to 126 t/s, thanks to PCI-E 5.0 and MLA+FA PR." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there guys, hope you're doing okay. Sorry for the typo in the title! &lt;strong&gt;Journey.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I did a post some days ago about my setup and some models &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Setup is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 7 7800X3D&lt;/li&gt; &lt;li&gt;192GB DDR5 6000Mhz at CL30 (overclocked and adjusted resistances to make it stable)&lt;/li&gt; &lt;li&gt;RTX 5090 MSI Vanguard LE SOC, flashed to Gigabyte Aorus Master VBIOS.&lt;/li&gt; &lt;li&gt;RTX 4090 ASUS TUF, flashed to Galax HoF VBIOS.&lt;/li&gt; &lt;li&gt;RTX 4090 Gigabyte Gaming OC, flashed to Galax HoF VBIOS.&lt;/li&gt; &lt;li&gt;RTX A6000 (Ampere)&lt;/li&gt; &lt;li&gt;AM5 MSI Carbon X670E&lt;/li&gt; &lt;li&gt;Running at X8 5.0 (5090) / X8 4.0 (4090) / X4 4.0 (4090) / X4 4.0 (A6000), all from CPU lanes (using M2 to PCI-E adapters)&lt;/li&gt; &lt;li&gt;Fedora 41-42 (believe me, I tried these on Windows and multiGPU is just borked there)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So, first running with 4.0 X8&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m '/GGUFs/DeepSeek-V3-0324-UD-Q2_K_XL-merged.gguf' -c 32768 --no-mmap --no-warmup -ngl 999 -ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; -ot &amp;quot;blk.(7|8|9|10).ffn.=CUDA1&amp;quot; -ot &amp;quot;blk.(11|12|13|14|15).ffn.=CUDA2&amp;quot; -ot &amp;quot;blk.(16|17|18|19|20|21|22|23|24|25).ffn.=CUDA3&amp;quot; -ot &amp;quot;ffn.*=CPU&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I was getting&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 38919.92 ms / 1528 tokens ( 25.47 ms per token, 39.26 tokens per second)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;eval time = 57175.47 ms / 471 tokens ( 121.39 ms per token, 8.24 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;So I noticed that the GPU 0 (4090 at X8 4.0) was getting saturated at 13 GiB/s. So as someone suggested on the issues &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD/discussions/2"&gt;https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD/discussions/2&lt;/a&gt;, his GPU was getting saturated at 26 GiB/s, which is the speed that the 5090 does at X8 5.0.&lt;/p&gt; &lt;p&gt;So this was the first step, I did&lt;/p&gt; &lt;p&gt;export CUDA_VISIBLE_DEVICES=2,0,1,3&lt;/p&gt; &lt;p&gt;This is (5090 X8 5.0, 4090 X8 4.0, 4090 X4 4.0, A6000 X4 4.0).&lt;/p&gt; &lt;p&gt;So this was the first step to increase the model speed.&lt;/p&gt; &lt;p&gt;And with the same command I got&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 49257.75 ms / 3252 tokens ( 15.15 ms per token, 66.02 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval time = 46322.14 ms / 436 tokens ( 106.24 ms per token, 9.41 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;So a huge increase in performance, thanks to just changing the device that does PP. Now, take in mind now the 5090 gets saturated at 26-27 GiB/s. I tried at X16 5.0 but I got max 28-29 GiB/s, so I think there is a limit somewhere or it can't use more.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8qxogl0oy9ze1.png?width=674&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43dac669c618f3ca51911e2a4a72859990d33b9b"&gt;5.0 X8 getting saturated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So, then, I was checking PRs and found this one: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/13306"&gt;https://github.com/ggml-org/llama.cpp/pull/13306&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This PR lets you use MLA (which takes 16K ctx from 80GB to 2GB), and then, FA, which reduces the buffer sizes on each GPU from 4.4GB to 400 MB!&lt;/p&gt; &lt;p&gt;So, running:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m '/GGUFs/DeepSeek-V3-0324-UD-Q2_K_XL-merged.gguf' -c 32768 --no-mmap --no-warmup -v -ngl 99 --override-tensor 'blk\.([0-7])\..*_exps\.=CUDA0' --override-tensor 'blk\.([8-9]|1[0-1])\..*_exps\.=CUDA1' --override-tensor 'blk\.(1[2-6])\..*_exps\.=CUDA2' --override-tensor 'blk\.(1[7-9]|2[0-6])\..*_exps\.=CUDA3' -fa --override-tensor 'blk\..*_exps\.=CPU' -mg 0 --ubatch-size 1024&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I got&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 34965.38 ms / 3565 tokens ( 9.81 ms per token, 101.96 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval time = 45389.59 ms / 416 tokens ( 109.11 ms per token, 9.17 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;So, we have went about 1t/s more on generation speed, but we have increased PP performance by 54%. This uses a bit, bit more VRAM but still perfectly to use 32K, 64K or even 128K (GPUs have about 8GB left)&lt;/p&gt; &lt;p&gt;Then, I went ahead and increased ubatch again, to 1536. So running the same command as above, but changing --ubatch-size from 1024 to 1536, I got these speeds.&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 28097.73 ms / 3565 tokens ( 7.88 ms per token, 126.88 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval time = 43426.93 ms / 404 tokens ( 107.49 ms per token, 9.30 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This is an 25.7% increase over -ub 1024, 92.4% increase over -ub 512 and 225% increase over -ub 512 and PCI-E X8 4.0.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This makes this model really usable! So now I'm even tempted to test Q3_K_XL! Q2_K_XL is 250GB and Q3_K_XL is 296GB, which should fit in 320GB total memory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgmxla/jorney_of_increasing_pre_processing_ts_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgmxla/jorney_of_increasing_pre_processing_ts_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgmxla/jorney_of_increasing_pre_processing_ts_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T02:46:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgpujo</id>
    <title>ik_llama and ktransformers are fast, but they completely break OpenAI style tool calling and structured responses</title>
    <updated>2025-05-07T05:35:46+00:00</updated>
    <author>
      <name>/u/texasdude11</name>
      <uri>https://old.reddit.com/user/texasdude11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been testing local LLM frameworks like &lt;strong&gt;ik_llama&lt;/strong&gt; and &lt;strong&gt;ktransformers&lt;/strong&gt; because they offer great performance on large moe models like Qwen3-235B and DeepSeek-V3-0324 685billion parameters.&lt;/p&gt; &lt;p&gt;But there‚Äôs a serious issue I haven‚Äôt seen enough people talk about them breaking OpenAI-compatible features like tool calling and structured JSON responses. Even though they expose a &lt;code&gt;/v1/chat/completions&lt;/code&gt; endpoint and claim OpenAI compatibility, neither &lt;code&gt;ik_llama&lt;/code&gt; nor &lt;code&gt;ktransformers&lt;/code&gt; properly handle: the tools or function field in a request or emitting valid JSON when expected&lt;/p&gt; &lt;p&gt;To work around this, I wrote a local wrapper that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;intercepts chat completions&lt;/li&gt; &lt;li&gt;enriches prompts with tool metadata&lt;/li&gt; &lt;li&gt;parses and transforms the output into OpenAI-compatible responses&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This lets me continue using fast backends while preserving tool calling logic.&lt;br /&gt; If anyone else is hitting this issue: how are you solving it?&lt;/p&gt; &lt;p&gt;I‚Äôm curious if others are patching the backend, modifying prompts, or intercepting responses like I am. Happy to share details if people are interested in the wrapper.&lt;/p&gt; &lt;p&gt;If you want to make use of my hack here is the repo for it:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Teachings/FastAgentAPI"&gt;https://github.com/Teachings/FastAgentAPI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also did a walkthrough of how to set it up:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=JGo9HfkzAmc"&gt;https://www.youtube.com/watch?v=JGo9HfkzAmc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/texasdude11"&gt; /u/texasdude11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgpujo/ik_llama_and_ktransformers_are_fast_but_they/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgpujo/ik_llama_and_ktransformers_are_fast_but_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgpujo/ik_llama_and_ktransformers_are_fast_but_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T05:35:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh0hcd</id>
    <title>Cracking 40% on SWE-bench verified with open source models &amp; agents &amp; open-source synth data</title>
    <updated>2025-05-07T15:39:46+00:00</updated>
    <author>
      <name>/u/klieret</name>
      <uri>https://old.reddit.com/user/klieret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh0hcd/cracking_40_on_swebench_verified_with_open_source/"&gt; &lt;img alt="Cracking 40% on SWE-bench verified with open source models &amp;amp; agents &amp;amp; open-source synth data" src="https://preview.redd.it/4lwtc2sgpdze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f581dfebc0968cbf87949bad4b08918a6afa989" title="Cracking 40% on SWE-bench verified with open source models &amp;amp; agents &amp;amp; open-source synth data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We all know that finetuning &amp;amp; RL work great for getting great LMs for agents -- the problem is where to get the training data!&lt;/p&gt; &lt;p&gt;We've generated 50k+ task instances for 128 popular GitHub repositories, then trained our own LM for SWE-agent. The result? We achieve 40% pass@1 on SWE-bench Verified -- a new SoTA among open source models.&lt;/p&gt; &lt;p&gt;We've open-sourced &lt;em&gt;everything&lt;/em&gt;, and we're excited to see what you build with it! This includes the agent (SWE-agent), the framework used to generate synthetic task instances (SWE-smith), and our fine-tuned LM (SWE-agent-LM-32B)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klieret"&gt; /u/klieret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4lwtc2sgpdze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh0hcd/cracking_40_on_swebench_verified_with_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh0hcd/cracking_40_on_swebench_verified_with_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T15:39:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgqw08</id>
    <title>Qwen3-235B-A22B and Qwen3-14B rank 2nd and 4th on Kagi‚Äôs LLM benchmark</title>
    <updated>2025-05-07T06:45:56+00:00</updated>
    <author>
      <name>/u/Shamp0oo</name>
      <uri>https://old.reddit.com/user/Shamp0oo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgqw08/qwen3235ba22b_and_qwen314b_rank_2nd_and_4th_on/"&gt; &lt;img alt="Qwen3-235B-A22B and Qwen3-14B rank 2nd and 4th on Kagi‚Äôs LLM benchmark" src="https://external-preview.redd.it/Hw7kJv_JneYm55PcK7LkdRzAPhO5o4gr4OcKc4__3Nw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ad6cf9e97cf2006fdfe2b75e079d91ccb83a824" title="Qwen3-235B-A22B and Qwen3-14B rank 2nd and 4th on Kagi‚Äôs LLM benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shamp0oo"&gt; /u/Shamp0oo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://help.kagi.com/kagi/ai/llm-benchmark.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgqw08/qwen3235ba22b_and_qwen314b_rank_2nd_and_4th_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgqw08/qwen3235ba22b_and_qwen314b_rank_2nd_and_4th_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T06:45:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgwxeo</id>
    <title>Faster open webui title generation for Qwen3 models</title>
    <updated>2025-05-07T13:08:26+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you use Qwen3 in Open WebUI, by default, WebUI will use Qwen3 for title generation with reasoning turned on, which is really unnecessary for this simple task.&lt;/p&gt; &lt;p&gt;Simply adding &amp;quot;/no_think&amp;quot; to the end of the title generation prompt can fix the problem.&lt;/p&gt; &lt;p&gt;Even though they &lt;strong&gt;&lt;em&gt;&amp;quot;hide&amp;quot;&lt;/em&gt;&lt;/strong&gt; the title generation prompt for some reason, you can search their GitHub to find all of their default prompts. Here is the title generation one with &amp;quot;/no_think&amp;quot; added to the end of it:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;By the way are there any good webui alternative to this one? I tried librechat but it's not friendly to local inference.&lt;/p&gt; &lt;/blockquote&gt; &lt;pre&gt;&lt;code&gt;### Task: Generate a concise, 3-5 word title with an emoji summarizing the chat history. ### Guidelines: - The title should clearly represent the main theme or subject of the conversation. - Use emojis that enhance understanding of the topic, but avoid quotation marks or special formatting. - Write the title in the chat's primary language; default to English if multilingual. - Prioritize accuracy over excessive creativity; keep it clear and simple. ### Output: JSON format: { &amp;quot;title&amp;quot;: &amp;quot;your concise title here&amp;quot; } ### Examples: - { &amp;quot;title&amp;quot;: &amp;quot;üìâ Stock Market Trends&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;üç™ Perfect Chocolate Chip Recipe&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;Evolution of Music Streaming&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;Remote Work Productivity Tips&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;Artificial Intelligence in Healthcare&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;üéÆ Video Game Development Insights&amp;quot; } ### Chat History: &amp;lt;chat_history&amp;gt; {{MESSAGES:END:2}} &amp;lt;/chat_history&amp;gt; /no_think &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And here is a faster one with chat history limited to 2k tokens to improve title generation speed:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;### Task: Generate a concise, 3-5 word title with an emoji summarizing the chat history. ### Guidelines: - The title should clearly represent the main theme or subject of the conversation. - Use emojis that enhance understanding of the topic, but avoid quotation marks or special formatting. - Write the title in the chat's primary language; default to English if multilingual. - Prioritize accuracy over excessive creativity; keep it clear and simple. ### Output: JSON format: { &amp;quot;title&amp;quot;: &amp;quot;your concise title here&amp;quot; } ### Examples: - { &amp;quot;title&amp;quot;: &amp;quot;üìâ Stock Market Trends&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;üç™ Perfect Chocolate Chip Recipe&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;Evolution of Music Streaming&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;Remote Work Productivity Tips&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;Artificial Intelligence in Healthcare&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;üéÆ Video Game Development Insights&amp;quot; } ### Chat History: &amp;lt;chat_history&amp;gt; {{prompt:start:1000}} {{prompt:end:1000}} &amp;lt;/chat_history&amp;gt; /no_think &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgwxeo/faster_open_webui_title_generation_for_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgwxeo/faster_open_webui_title_generation_for_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgwxeo/faster_open_webui_title_generation_for_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T13:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgs1z7</id>
    <title>3090+3060+3060 llama.cpp benchmarks / tips</title>
    <updated>2025-05-07T08:10:14+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgs1z7/309030603060_llamacpp_benchmarks_tips/"&gt; &lt;img alt="3090+3060+3060 llama.cpp benchmarks / tips" src="https://a.thumbs.redditmedia.com/DbTGh_0uq0Zr27C4QY6dD_OTncYdCY186kdH1HyuqI8.jpg" title="3090+3060+3060 llama.cpp benchmarks / tips" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Building LocalLlama Machine ‚Äì Episode 3: Performance Optimizations&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In the previous episode, I had all three GPUs mounted directly in the motherboard slots. Now, I‚Äôve &lt;strong&gt;moved one 3090 onto a riser&lt;/strong&gt; to make it a bit happier. Let‚Äôs use this setup for benchmarking.&lt;/p&gt; &lt;p&gt;Some people ask whether it's allowed to mix different GPUs, in this tutorial, I‚Äôll explain how to handle that topic.&lt;/p&gt; &lt;p&gt;First, let‚Äôs try some smaller models. In the first screenshot, you can see the results for Qwen3 8B and Qwen3 14B. These models are small enough to fit entirely inside a 3090, so the 3060s are not needed. If we disable them, we see a performance boost: from &lt;strong&gt;48 to 82&lt;/strong&gt; tokens per second, and from &lt;strong&gt;28 to 48&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Next, we switch to &lt;strong&gt;Qwen3 32B&lt;/strong&gt;. This model is larger, and to run it in Q8, you need more than a single 3090. However, in &lt;code&gt;llama.cpp&lt;/code&gt;, we can control how the tensors are split. For example, we can allocate more memory on the first card and less on the second and third. These values are discovered experimentally for each model, so your optimal settings may vary. If the values are incorrect, the model won't load, for instance, it might try to allocate 26GB on a 24GB GPU.&lt;/p&gt; &lt;p&gt;We can improve performance from the default &lt;strong&gt;13.0&lt;/strong&gt; tokens per second to &lt;strong&gt;15.6&lt;/strong&gt; by adjusting the tensor split. Furthermore, we can go even higher, to &lt;strong&gt;16.4 tokens per second&lt;/strong&gt;, by using the &amp;quot;row&amp;quot; split mode. This mode was broken in &lt;code&gt;llama.cpp&lt;/code&gt; until recently, so make sure you're using the latest version of the code.&lt;/p&gt; &lt;p&gt;Now let‚Äôs try &lt;strong&gt;Nemotron 49B&lt;/strong&gt;. I really like this model, though I can't run it fully in Q8 yet, that‚Äôs a good excuse to buy another 3090! For now, let's use Q6. With some tuning, we can go &lt;strong&gt;from 12.4 to 14.1 tokens per second&lt;/strong&gt;. Not bad.&lt;/p&gt; &lt;p&gt;Then we move on to a 70B model. I'm using &lt;strong&gt;DeepSeek-R1-Distill-Llama-70B&lt;/strong&gt; in Q4. We start at &lt;strong&gt;10.3&lt;/strong&gt; tokens per second and improve to &lt;strong&gt;12.1&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Gemma3 27B&lt;/strong&gt; is a different case. With optimized tensor split values, we boost performance from 14.9 to &lt;strong&gt;18.9 tokens per second&lt;/strong&gt;. However, using &lt;code&gt;sm&lt;/code&gt; row mode slightly decreases the speed to 18.5.&lt;/p&gt; &lt;p&gt;Finally, we see similar behavior with &lt;strong&gt;Mistral Small 24B&lt;/strong&gt; (why is it called Llama 13B?). Performance goes from 18.8 to &lt;strong&gt;28.2 tokens per second&lt;/strong&gt; with tensor split, but again, &lt;code&gt;sm&lt;/code&gt; row mode reduces it slightly to 26.1.&lt;/p&gt; &lt;p&gt;So, you‚Äôll need to experiment with your favorite models and your specific setup, but now you know the direction to take on your journey. Good luck!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kgs1z7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgs1z7/309030603060_llamacpp_benchmarks_tips/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgs1z7/309030603060_llamacpp_benchmarks_tips/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T08:10:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgxhdt</id>
    <title>Ollama vs Llama.cpp on 2x3090 and M3Max using qwen3-30b</title>
    <updated>2025-05-07T13:33:49+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone.&lt;/p&gt; &lt;p&gt;This is a comparison test between Ollama and Llama.cpp on 2 x RTX-3090 and M3-Max with 64GB using qwen3:30b-a3b-q8_0.&lt;/p&gt; &lt;p&gt;VLLM, SGLang Exllama don't support rtx3090 with this particular Qwen MoE architecture yet. I ran a separate &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ke26sl/another_attempt_to_measure_speed_for_qwen3_moe_on/"&gt;benchmark with rtx-4090 on VLLM and SGLang&lt;/a&gt; here. This was primarily to compare Ollama and Llama.cpp.&lt;/p&gt; &lt;h3&gt;Metrics&lt;/h3&gt; &lt;p&gt;To ensure consistency, I used a custom Python script that sends requests to the server via the OpenAI-compatible API. Metrics were calculated as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Time to First Token (TTFT): Measured from the start of the streaming request to the first streaming event received.&lt;/li&gt; &lt;li&gt;Prompt Processing Speed (PP): Number of prompt tokens divided by TTFT.&lt;/li&gt; &lt;li&gt;Token Generation Speed (TG): Number of generated tokens divided by (total duration - TTFT).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The displayed results were truncated to two decimal places, but the calculations used full precision. I made the script to prepend 40% new material in the beginning of next longer prompt to avoid caching effect.&lt;/p&gt; &lt;p&gt;Here's my script for anyone interest. &lt;a href="https://github.com/chigkim/prompt-test"&gt;https://github.com/chigkim/prompt-test&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It uses OpenAI API, so it should work in variety setup. Also, this tests one request at a time, so multiple parallel requests could result in higher throughput in different tests.&lt;/p&gt; &lt;h3&gt;Setup&lt;/h3&gt; &lt;p&gt;Both use the same q8_0 model from Ollama library with flash attention. I'm sure you can optimize more, but I copied the flags from Ollama log in order to keep it consistent, so both use the exactly same flags when loading the model.&lt;/p&gt; &lt;p&gt;&lt;code&gt;./build/bin/llama-server --model ~/.ollama/models/blobs/sha256... --ctx-size 36000 --batch-size 512 --n-gpu-layers 49 --verbose --threads 24 --flash-attn --parallel 1 --tensor-split 25,24 --port 11434&lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama.cpp: Commit 2f54e34&lt;/li&gt; &lt;li&gt;Ollama: 0.6.8&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each row in the results represents a test (a specific combination of machine, engine, and prompt length). There are 4 tests per prompt length.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Setup 1: 2xRTX3090, Llama.cpp&lt;/li&gt; &lt;li&gt;Setup 2: 2xRTX3090, Ollama&lt;/li&gt; &lt;li&gt;Setup 3: M3Max, Llama.cpp&lt;/li&gt; &lt;li&gt;Setup 4: M3Max, Ollama&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Result&lt;/h3&gt; &lt;p&gt;Please zoom in to see the graph better.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img xcmmuk1bycze1...&lt;/em&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Machine&lt;/th&gt; &lt;th&gt;Engine&lt;/th&gt; &lt;th&gt;Prompt Tokens&lt;/th&gt; &lt;th&gt;PP/s&lt;/th&gt; &lt;th&gt;TTFT&lt;/th&gt; &lt;th&gt;Generated Tokens&lt;/th&gt; &lt;th&gt;TG/s&lt;/th&gt; &lt;th&gt;Duration&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;702&lt;/td&gt; &lt;td&gt;1663.57&lt;/td&gt; &lt;td&gt;0.42&lt;/td&gt; &lt;td&gt;1419&lt;/td&gt; &lt;td&gt;82.19&lt;/td&gt; &lt;td&gt;17.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;702&lt;/td&gt; &lt;td&gt;1595.04&lt;/td&gt; &lt;td&gt;0.44&lt;/td&gt; &lt;td&gt;1430&lt;/td&gt; &lt;td&gt;77.41&lt;/td&gt; &lt;td&gt;18.91&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;702&lt;/td&gt; &lt;td&gt;289.53&lt;/td&gt; &lt;td&gt;2.42&lt;/td&gt; &lt;td&gt;1485&lt;/td&gt; &lt;td&gt;55.60&lt;/td&gt; &lt;td&gt;29.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;702&lt;/td&gt; &lt;td&gt;288.32&lt;/td&gt; &lt;td&gt;2.43&lt;/td&gt; &lt;td&gt;1440&lt;/td&gt; &lt;td&gt;55.78&lt;/td&gt; &lt;td&gt;28.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;959&lt;/td&gt; &lt;td&gt;1768.00&lt;/td&gt; &lt;td&gt;0.54&lt;/td&gt; &lt;td&gt;1210&lt;/td&gt; &lt;td&gt;81.47&lt;/td&gt; &lt;td&gt;15.39&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;959&lt;/td&gt; &lt;td&gt;1723.07&lt;/td&gt; &lt;td&gt;0.56&lt;/td&gt; &lt;td&gt;1279&lt;/td&gt; &lt;td&gt;74.82&lt;/td&gt; &lt;td&gt;17.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;959&lt;/td&gt; &lt;td&gt;458.40&lt;/td&gt; &lt;td&gt;2.09&lt;/td&gt; &lt;td&gt;1337&lt;/td&gt; &lt;td&gt;55.28&lt;/td&gt; &lt;td&gt;26.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;959&lt;/td&gt; &lt;td&gt;459.38&lt;/td&gt; &lt;td&gt;2.09&lt;/td&gt; &lt;td&gt;1302&lt;/td&gt; &lt;td&gt;55.44&lt;/td&gt; &lt;td&gt;25.57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1306&lt;/td&gt; &lt;td&gt;1752.04&lt;/td&gt; &lt;td&gt;0.75&lt;/td&gt; &lt;td&gt;1108&lt;/td&gt; &lt;td&gt;80.95&lt;/td&gt; &lt;td&gt;14.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1306&lt;/td&gt; &lt;td&gt;1725.06&lt;/td&gt; &lt;td&gt;0.76&lt;/td&gt; &lt;td&gt;1209&lt;/td&gt; &lt;td&gt;73.83&lt;/td&gt; &lt;td&gt;17.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1306&lt;/td&gt; &lt;td&gt;455.39&lt;/td&gt; &lt;td&gt;2.87&lt;/td&gt; &lt;td&gt;1213&lt;/td&gt; &lt;td&gt;54.84&lt;/td&gt; &lt;td&gt;24.99&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1306&lt;/td&gt; &lt;td&gt;458.06&lt;/td&gt; &lt;td&gt;2.85&lt;/td&gt; &lt;td&gt;1213&lt;/td&gt; &lt;td&gt;54.96&lt;/td&gt; &lt;td&gt;24.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1774&lt;/td&gt; &lt;td&gt;1763.32&lt;/td&gt; &lt;td&gt;1.01&lt;/td&gt; &lt;td&gt;1330&lt;/td&gt; &lt;td&gt;80.44&lt;/td&gt; &lt;td&gt;17.54&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1774&lt;/td&gt; &lt;td&gt;1823.88&lt;/td&gt; &lt;td&gt;0.97&lt;/td&gt; &lt;td&gt;1370&lt;/td&gt; &lt;td&gt;78.26&lt;/td&gt; &lt;td&gt;18.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1774&lt;/td&gt; &lt;td&gt;320.44&lt;/td&gt; &lt;td&gt;5.54&lt;/td&gt; &lt;td&gt;1281&lt;/td&gt; &lt;td&gt;54.10&lt;/td&gt; &lt;td&gt;29.21&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1774&lt;/td&gt; &lt;td&gt;321.45&lt;/td&gt; &lt;td&gt;5.52&lt;/td&gt; &lt;td&gt;1281&lt;/td&gt; &lt;td&gt;54.26&lt;/td&gt; &lt;td&gt;29.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;2584&lt;/td&gt; &lt;td&gt;1776.17&lt;/td&gt; &lt;td&gt;1.45&lt;/td&gt; &lt;td&gt;1522&lt;/td&gt; &lt;td&gt;79.39&lt;/td&gt; &lt;td&gt;20.63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;2584&lt;/td&gt; &lt;td&gt;1851.35&lt;/td&gt; &lt;td&gt;1.40&lt;/td&gt; &lt;td&gt;1118&lt;/td&gt; &lt;td&gt;75.08&lt;/td&gt; &lt;td&gt;16.29&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;2584&lt;/td&gt; &lt;td&gt;445.47&lt;/td&gt; &lt;td&gt;5.80&lt;/td&gt; &lt;td&gt;1321&lt;/td&gt; &lt;td&gt;52.86&lt;/td&gt; &lt;td&gt;30.79&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;2584&lt;/td&gt; &lt;td&gt;447.47&lt;/td&gt; &lt;td&gt;5.77&lt;/td&gt; &lt;td&gt;1359&lt;/td&gt; &lt;td&gt;53.00&lt;/td&gt; &lt;td&gt;31.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;3557&lt;/td&gt; &lt;td&gt;1832.97&lt;/td&gt; &lt;td&gt;1.94&lt;/td&gt; &lt;td&gt;1500&lt;/td&gt; &lt;td&gt;77.61&lt;/td&gt; &lt;td&gt;21.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;3557&lt;/td&gt; &lt;td&gt;1928.76&lt;/td&gt; &lt;td&gt;1.84&lt;/td&gt; &lt;td&gt;1653&lt;/td&gt; &lt;td&gt;70.17&lt;/td&gt; &lt;td&gt;25.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;3557&lt;/td&gt; &lt;td&gt;444.32&lt;/td&gt; &lt;td&gt;8.01&lt;/td&gt; &lt;td&gt;1481&lt;/td&gt; &lt;td&gt;51.34&lt;/td&gt; &lt;td&gt;36.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;3557&lt;/td&gt; &lt;td&gt;442.89&lt;/td&gt; &lt;td&gt;8.03&lt;/td&gt; &lt;td&gt;1430&lt;/td&gt; &lt;td&gt;51.52&lt;/td&gt; &lt;td&gt;35.79&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;4739&lt;/td&gt; &lt;td&gt;1773.28&lt;/td&gt; &lt;td&gt;2.67&lt;/td&gt; &lt;td&gt;1279&lt;/td&gt; &lt;td&gt;76.60&lt;/td&gt; &lt;td&gt;19.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;4739&lt;/td&gt; &lt;td&gt;1910.52&lt;/td&gt; &lt;td&gt;2.48&lt;/td&gt; &lt;td&gt;1877&lt;/td&gt; &lt;td&gt;71.85&lt;/td&gt; &lt;td&gt;28.60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;4739&lt;/td&gt; &lt;td&gt;421.06&lt;/td&gt; &lt;td&gt;11.26&lt;/td&gt; &lt;td&gt;1472&lt;/td&gt; &lt;td&gt;49.97&lt;/td&gt; &lt;td&gt;40.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;4739&lt;/td&gt; &lt;td&gt;420.51&lt;/td&gt; &lt;td&gt;11.27&lt;/td&gt; &lt;td&gt;1316&lt;/td&gt; &lt;td&gt;50.16&lt;/td&gt; &lt;td&gt;37.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;6520&lt;/td&gt; &lt;td&gt;1760.68&lt;/td&gt; &lt;td&gt;3.70&lt;/td&gt; &lt;td&gt;1435&lt;/td&gt; &lt;td&gt;73.77&lt;/td&gt; &lt;td&gt;23.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;6520&lt;/td&gt; &lt;td&gt;1897.12&lt;/td&gt; &lt;td&gt;3.44&lt;/td&gt; &lt;td&gt;1781&lt;/td&gt; &lt;td&gt;68.85&lt;/td&gt; &lt;td&gt;29.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;6520&lt;/td&gt; &lt;td&gt;418.03&lt;/td&gt; &lt;td&gt;15.60&lt;/td&gt; &lt;td&gt;1998&lt;/td&gt; &lt;td&gt;47.56&lt;/td&gt; &lt;td&gt;57.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;6520&lt;/td&gt; &lt;td&gt;417.70&lt;/td&gt; &lt;td&gt;15.61&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;47.81&lt;/td&gt; &lt;td&gt;57.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;9101&lt;/td&gt; &lt;td&gt;1714.65&lt;/td&gt; &lt;td&gt;5.31&lt;/td&gt; &lt;td&gt;1528&lt;/td&gt; &lt;td&gt;70.17&lt;/td&gt; &lt;td&gt;27.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;9101&lt;/td&gt; &lt;td&gt;1881.13&lt;/td&gt; &lt;td&gt;4.84&lt;/td&gt; &lt;td&gt;1801&lt;/td&gt; &lt;td&gt;68.09&lt;/td&gt; &lt;td&gt;31.29&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;9101&lt;/td&gt; &lt;td&gt;250.25&lt;/td&gt; &lt;td&gt;36.37&lt;/td&gt; &lt;td&gt;1941&lt;/td&gt; &lt;td&gt;36.29&lt;/td&gt; &lt;td&gt;89.86&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;9101&lt;/td&gt; &lt;td&gt;244.02&lt;/td&gt; &lt;td&gt;37.30&lt;/td&gt; &lt;td&gt;1941&lt;/td&gt; &lt;td&gt;35.55&lt;/td&gt; &lt;td&gt;91.89&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;12430&lt;/td&gt; &lt;td&gt;1591.33&lt;/td&gt; &lt;td&gt;7.81&lt;/td&gt; &lt;td&gt;1001&lt;/td&gt; &lt;td&gt;66.74&lt;/td&gt; &lt;td&gt;22.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;12430&lt;/td&gt; &lt;td&gt;1805.88&lt;/td&gt; &lt;td&gt;6.88&lt;/td&gt; &lt;td&gt;1284&lt;/td&gt; &lt;td&gt;64.01&lt;/td&gt; &lt;td&gt;26.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;12430&lt;/td&gt; &lt;td&gt;280.46&lt;/td&gt; &lt;td&gt;44.32&lt;/td&gt; &lt;td&gt;1291&lt;/td&gt; &lt;td&gt;39.89&lt;/td&gt; &lt;td&gt;76.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;12430&lt;/td&gt; &lt;td&gt;278.79&lt;/td&gt; &lt;td&gt;44.58&lt;/td&gt; &lt;td&gt;1502&lt;/td&gt; &lt;td&gt;39.82&lt;/td&gt; &lt;td&gt;82.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;17078&lt;/td&gt; &lt;td&gt;1546.35&lt;/td&gt; &lt;td&gt;11.04&lt;/td&gt; &lt;td&gt;1028&lt;/td&gt; &lt;td&gt;63.55&lt;/td&gt; &lt;td&gt;27.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;17078&lt;/td&gt; &lt;td&gt;1722.15&lt;/td&gt; &lt;td&gt;9.92&lt;/td&gt; &lt;td&gt;1100&lt;/td&gt; &lt;td&gt;59.36&lt;/td&gt; &lt;td&gt;28.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;17078&lt;/td&gt; &lt;td&gt;270.38&lt;/td&gt; &lt;td&gt;63.16&lt;/td&gt; &lt;td&gt;1461&lt;/td&gt; &lt;td&gt;34.89&lt;/td&gt; &lt;td&gt;105.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;17078&lt;/td&gt; &lt;td&gt;270.49&lt;/td&gt; &lt;td&gt;63.14&lt;/td&gt; &lt;td&gt;1673&lt;/td&gt; &lt;td&gt;34.28&lt;/td&gt; &lt;td&gt;111.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;23658&lt;/td&gt; &lt;td&gt;1429.31&lt;/td&gt; &lt;td&gt;16.55&lt;/td&gt; &lt;td&gt;1039&lt;/td&gt; &lt;td&gt;58.46&lt;/td&gt; &lt;td&gt;34.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;23658&lt;/td&gt; &lt;td&gt;1586.04&lt;/td&gt; &lt;td&gt;14.92&lt;/td&gt; &lt;td&gt;1041&lt;/td&gt; &lt;td&gt;53.90&lt;/td&gt; &lt;td&gt;34.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;23658&lt;/td&gt; &lt;td&gt;241.20&lt;/td&gt; &lt;td&gt;98.09&lt;/td&gt; &lt;td&gt;1681&lt;/td&gt; &lt;td&gt;28.04&lt;/td&gt; &lt;td&gt;158.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;23658&lt;/td&gt; &lt;td&gt;240.64&lt;/td&gt; &lt;td&gt;98.31&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;27.70&lt;/td&gt; &lt;td&gt;170.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;33525&lt;/td&gt; &lt;td&gt;1293.65&lt;/td&gt; &lt;td&gt;25.91&lt;/td&gt; &lt;td&gt;1311&lt;/td&gt; &lt;td&gt;52.92&lt;/td&gt; &lt;td&gt;50.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;33525&lt;/td&gt; &lt;td&gt;1441.12&lt;/td&gt; &lt;td&gt;23.26&lt;/td&gt; &lt;td&gt;1418&lt;/td&gt; &lt;td&gt;49.76&lt;/td&gt; &lt;td&gt;51.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;33525&lt;/td&gt; &lt;td&gt;217.15&lt;/td&gt; &lt;td&gt;154.38&lt;/td&gt; &lt;td&gt;1453&lt;/td&gt; &lt;td&gt;23.91&lt;/td&gt; &lt;td&gt;215.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;33525&lt;/td&gt; &lt;td&gt;219.68&lt;/td&gt; &lt;td&gt;152.61&lt;/td&gt; &lt;td&gt;1522&lt;/td&gt; &lt;td&gt;23.84&lt;/td&gt; &lt;td&gt;216.44&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgxhdt/ollama_vs_llamacpp_on_2x3090_and_m3max_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgxhdt/ollama_vs_llamacpp_on_2x3090_and_m3max_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgxhdt/ollama_vs_llamacpp_on_2x3090_and_m3max_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T13:33:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgo7d4</id>
    <title>Qwen3-30B-A3B GGUFs MMLU-PRO benchmark comparison - Q6_K / Q5_K_M / Q4_K_M / Q3_K_M</title>
    <updated>2025-05-07T03:56:08+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgo7d4/qwen330ba3b_ggufs_mmlupro_benchmark_comparison_q6/"&gt; &lt;img alt="Qwen3-30B-A3B GGUFs MMLU-PRO benchmark comparison - Q6_K / Q5_K_M / Q4_K_M / Q3_K_M" src="https://external-preview.redd.it/luDTORHovWSvyyKGyVuQUU_AS82WswbZpoHOp59s5cs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80203fde524d99b74a2b8e4185b0d45043a2a35e" title="Qwen3-30B-A3B GGUFs MMLU-PRO benchmark comparison - Q6_K / Q5_K_M / Q4_K_M / Q3_K_M" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;MMLU-PRO 0.25 subset(3003 questions), 0 temp, No Think, Q8 KV Cache&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-Q6_K / Q5_K_M / Q4_K_M / Q3_K_M&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The entire benchmark took &lt;strong&gt;10 hours 32 minutes 19 seconds&lt;/strong&gt;.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I wanted to test unsloth dynamic ggufs as well, but ollama still can't run those ggufs properly, and yes I downloaded v0.6.8, lm studio can run them but doesn't support batching. So I only tested _K_M ggufs&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n8uisayb8aze1.png?width=445&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e2ef9b9f7f01091787bc58917ea58a7fe07d814"&gt;https://preview.redd.it/n8uisayb8aze1.png?width=445&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e2ef9b9f7f01091787bc58917ea58a7fe07d814&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rlopilhc8aze1.png?width=1123&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=972522557abddeafa03ea3033ef2f3e05e396038"&gt;https://preview.redd.it/rlopilhc8aze1.png?width=1123&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=972522557abddeafa03ea3033ef2f3e05e396038&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sqzkrdkd8aze1.png?width=2003&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f19a8a0d4d6ee9552209ff8da9b0f9f3d51923a"&gt;https://preview.redd.it/sqzkrdkd8aze1.png?width=2003&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f19a8a0d4d6ee9552209ff8da9b0f9f3d51923a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s35vihde8aze1.png?width=1235&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9261ee5820639594e218ed77edff47a3ea4dcb8d"&gt;https://preview.redd.it/s35vihde8aze1.png?width=1235&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9261ee5820639594e218ed77edff47a3ea4dcb8d&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Q8 KV Cache / No kv cache quant&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/te4noxve8aze1.png?width=2005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c14e380265fe29f0805bf030dada4d452f5e86a"&gt;https://preview.redd.it/te4noxve8aze1.png?width=2005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c14e380265fe29f0805bf030dada4d452f5e86a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gxppkzef8aze1.png?width=1125&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9acc93a5c253f2a9401c3641322231179c3190a"&gt;https://preview.redd.it/gxppkzef8aze1.png?width=1125&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9acc93a5c253f2a9401c3641322231179c3190a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ggufs: &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgo7d4/qwen330ba3b_ggufs_mmlupro_benchmark_comparison_q6/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgo7d4/qwen330ba3b_ggufs_mmlupro_benchmark_comparison_q6/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgo7d4/qwen330ba3b_ggufs_mmlupro_benchmark_comparison_q6/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T03:56:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgdmz6</id>
    <title>The real reason OpenAI bought WindSurf</title>
    <updated>2025-05-06T19:40:33+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgdmz6/the_real_reason_openai_bought_windsurf/"&gt; &lt;img alt="The real reason OpenAI bought WindSurf" src="https://preview.redd.it/knqgtodvs7ze1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b31b8bf514ff9c2407608d699ee65ce7c164f986" title="The real reason OpenAI bought WindSurf" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who don‚Äôt know, today it was announced that OpenAI bought WindSurf, the AI-assisted IDE, for 3 billion USD. Previously, they tried to buy Cursor, the leading company that offers AI-assisted IDE, but didn‚Äôt agree on the details (probably on the price). Therefore, they settled for the second biggest player in terms of market share, WindSurf.&lt;/p&gt; &lt;p&gt;Why?&lt;/p&gt; &lt;p&gt;A lot of people question whether this is a wise move from OpenAI considering that these companies have limited innovation, since they don‚Äôt own the models and their IDE is just a fork of VS code.&lt;/p&gt; &lt;p&gt;Many argued that the reason for this purchase is to acquire the market position, the user base, since these platforms are already established with a big number of users.&lt;/p&gt; &lt;p&gt;I disagree in some degree. It‚Äôs not about the users per se, it‚Äôs about the training data they create. It doesn‚Äôt even matter which model users choose to use inside the IDE, Gemini2.5, Sonnet3.7, doesn‚Äôt really matter. There is a huge market that will be created very soon, and that‚Äôs coding agents. Some rumours suggest that OpenAI would sell them for 10k USD a month! These kind of agents/models need the exact kind of data that these AI-assisted IDEs collect. &lt;/p&gt; &lt;p&gt;Therefore, they paid the 3 billion to buy the training data they‚Äôd need to train their future coding agent models.&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/knqgtodvs7ze1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgdmz6/the_real_reason_openai_bought_windsurf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgdmz6/the_real_reason_openai_bought_windsurf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T19:40:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg9jkq</id>
    <title>New SOTA music generation model</title>
    <updated>2025-05-06T16:56:14+00:00</updated>
    <author>
      <name>/u/topiga</name>
      <uri>https://old.reddit.com/user/topiga</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg9jkq/new_sota_music_generation_model/"&gt; &lt;img alt="New SOTA music generation model" src="https://external-preview.redd.it/N2dybzhkY2h6NnplMUATahysLltY5LFjwkyeKdWeoWJNo8-MZQBD68gR6Fn5.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54585ce0978377749da79c9379f3519be85eac15" title="New SOTA music generation model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ace-step is a multilingual 3.5B parameters music generation model. They released training code, LoRa training code and will release more stuff soon.&lt;/p&gt; &lt;p&gt;It supports 19 languages, instrumental styles, vocal techniques, and more.&lt;/p&gt; &lt;p&gt;I‚Äôm pretty exited because it‚Äôs really good, I never heard anything like it.&lt;/p&gt; &lt;p&gt;Project website: &lt;a href="https://ace-step.github.io/"&gt;https://ace-step.github.io/&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/ace-step/ACE-Step"&gt;https://github.com/ace-step/ACE-Step&lt;/a&gt;&lt;br /&gt; HF: &lt;a href="https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B"&gt;https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/topiga"&gt; /u/topiga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gf0uynfhz6ze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg9jkq/new_sota_music_generation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg9jkq/new_sota_music_generation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T16:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgzskq</id>
    <title>Mistral-Medium 3 (unfortunately no local support so far)</title>
    <updated>2025-05-07T15:12:17+00:00</updated>
    <author>
      <name>/u/pier4r</name>
      <uri>https://old.reddit.com/user/pier4r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzskq/mistralmedium_3_unfortunately_no_local_support_so/"&gt; &lt;img alt="Mistral-Medium 3 (unfortunately no local support so far)" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Mistral-Medium 3 (unfortunately no local support so far)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pier4r"&gt; /u/pier4r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/mistral-medium-3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzskq/mistralmedium_3_unfortunately_no_local_support_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzskq/mistralmedium_3_unfortunately_no_local_support_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T15:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgu4qg</id>
    <title>Qwen3-235B Q6_K ktransformers at 56t/s prefill 4.5t/s decode on Xeon 3175X (384GB DDR4-3400) and RTX 4090</title>
    <updated>2025-05-07T10:37:38+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgu4qg/qwen3235b_q6_k_ktransformers_at_56ts_prefill_45ts/"&gt; &lt;img alt="Qwen3-235B Q6_K ktransformers at 56t/s prefill 4.5t/s decode on Xeon 3175X (384GB DDR4-3400) and RTX 4090" src="https://preview.redd.it/1ijx9ffv8cze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42da044e5a349add8ff7c3733ec1c4e706676161" title="Qwen3-235B Q6_K ktransformers at 56t/s prefill 4.5t/s decode on Xeon 3175X (384GB DDR4-3400) and RTX 4090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1ijx9ffv8cze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgu4qg/qwen3235b_q6_k_ktransformers_at_56ts_prefill_45ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgu4qg/qwen3235b_q6_k_ktransformers_at_56ts_prefill_45ts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T10:37:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgzey8</id>
    <title>Run FLUX.1 losslessly on a GPU with 20GB VRAM</title>
    <updated>2025-05-07T14:57:14+00:00</updated>
    <author>
      <name>/u/arty_photography</name>
      <uri>https://old.reddit.com/user/arty_photography</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've released &lt;strong&gt;losslessly compressed versions&lt;/strong&gt; of the &lt;strong&gt;12B FLUX.1-dev&lt;/strong&gt; and &lt;strong&gt;FLUX.1-schnell&lt;/strong&gt; models using &lt;strong&gt;DFloat11&lt;/strong&gt;, a compression method that applies entropy coding to BFloat16 weights. This reduces model size by &lt;strong&gt;~30%&lt;/strong&gt; &lt;em&gt;without changing outputs&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;This brings the models down from &lt;strong&gt;24GB to ~16.3GB&lt;/strong&gt;, enabling them to run on a &lt;strong&gt;single GPU with 20GB or more of VRAM&lt;/strong&gt;, with only a &lt;strong&gt;few seconds of extra overhead per image&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;üîó Downloads &amp;amp; Resources&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Compressed FLUX.1-dev&lt;/strong&gt;: &lt;a href="https://huggingface.co/DFloat11/FLUX.1-dev-DF11"&gt;huggingface.co/DFloat11/FLUX.1-dev-DF11&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compressed FLUX.1-schnell&lt;/strong&gt;: &lt;a href="https://huggingface.co/DFloat11/FLUX.1-schnell-DF11"&gt;huggingface.co/DFloat11/FLUX.1-schnell-DF11&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Example Code&lt;/strong&gt;: &lt;a href="https://github.com/LeanModels/DFloat11/tree/master/examples/flux.1"&gt;github.com/LeanModels/DFloat11/tree/master/examples/flux.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compressed LLMs (Qwen 3, Gemma 3, etc.)&lt;/strong&gt;: &lt;a href="https://huggingface.co/DFloat11"&gt;huggingface.co/DFloat11&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Research Paper&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2504.11651"&gt;arxiv.org/abs/2504.11651&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Feedback welcome&lt;/strong&gt;! Let me know if you try them out or run into any issues!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arty_photography"&gt; /u/arty_photography &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzey8/run_flux1_losslessly_on_a_gpu_with_20gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzey8/run_flux1_losslessly_on_a_gpu_with_20gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzey8/run_flux1_losslessly_on_a_gpu_with_20gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T14:57:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgzwe9</id>
    <title>New mistral model benchmarks</title>
    <updated>2025-05-07T15:16:25+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzwe9/new_mistral_model_benchmarks/"&gt; &lt;img alt="New mistral model benchmarks" src="https://preview.redd.it/hrtrvrvnmdze1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a47a4a215c33b3670819e5b09e20d25a73074d7" title="New mistral model benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hrtrvrvnmdze1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzwe9/new_mistral_model_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzwe9/new_mistral_model_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T15:16:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgrab2</id>
    <title>Self-improving AI unlocked?</title>
    <updated>2025-05-07T07:13:24+00:00</updated>
    <author>
      <name>/u/FeathersOfTheArrow</name>
      <uri>https://old.reddit.com/user/FeathersOfTheArrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Absolute Zero: Reinforced Self-play Reasoning with Zero Data&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Abstract:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. &lt;strong&gt;Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision&lt;/strong&gt;, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, &lt;strong&gt;we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples&lt;/strong&gt;. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2505.03335"&gt;Paper&lt;/a&gt; &lt;a href="https://x.com/AndrewZ45732491/status/1919920459748909288"&gt;Thread&lt;/a&gt; &lt;a href="https://github.com/LeapLabTHU/Absolute-Zero-Reasoner"&gt;GitHub&lt;/a&gt; &lt;a href="https://huggingface.co/papers/2505.03335"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeathersOfTheArrow"&gt; /u/FeathersOfTheArrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrab2/selfimproving_ai_unlocked/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrab2/selfimproving_ai_unlocked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrab2/selfimproving_ai_unlocked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T07:13:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgt8m5</id>
    <title>nanoVLM: A minimal Vision-Language Model with a LLaMA-style decoder ‚Äî now open source</title>
    <updated>2025-05-07T09:37:59+00:00</updated>
    <author>
      <name>/u/zKingFrist</name>
      <uri>https://old.reddit.com/user/zKingFrist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all ‚Äî we just open-sourced &lt;strong&gt;nanoVLM&lt;/strong&gt;, a lightweight Vision-Language Model (VLM) built from scratch in &lt;strong&gt;pure PyTorch&lt;/strong&gt;, with a &lt;strong&gt;LLaMA-style decoder&lt;/strong&gt;. It's designed to be simple, hackable, and easy to train ‚Äî the full model is just ~750 lines of code.&lt;/p&gt; &lt;p&gt;Why it's interesting:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Achieves &lt;strong&gt;35.3% on MMStar&lt;/strong&gt; with only &lt;strong&gt;6 hours of training on a single H100,&lt;/strong&gt; matching SmolVLM-256M performance ‚Äî but using 100x fewer GPU hours.&lt;/li&gt; &lt;li&gt;Can be trained in a &lt;strong&gt;free Google Colab notebook&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Great for learning, prototyping, or building your own VLMs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Architecture:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vision encoder: &lt;strong&gt;SigLiP-ViT&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Language decoder: &lt;strong&gt;LLaMA-style&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Modality projector connecting the two&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Inspired by nanoGPT, this is like the VLM version ‚Äî compact and easy to understand. Would love to see someone try running this on local hardware or mixing it with other projects.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/huggingface/nanoVLM"&gt;https://github.com/huggingface/nanoVLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zKingFrist"&gt; /u/zKingFrist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgt8m5/nanovlm_a_minimal_visionlanguage_model_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgt8m5/nanovlm_a_minimal_visionlanguage_model_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgt8m5/nanovlm_a_minimal_visionlanguage_model_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T09:37:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kguqmd</id>
    <title>Apriel-Nemotron-15b-Thinker - o1mini level with MIT licence (Nvidia &amp; Servicenow)</title>
    <updated>2025-05-07T11:14:13+00:00</updated>
    <author>
      <name>/u/Temporary-Size7310</name>
      <uri>https://old.reddit.com/user/Temporary-Size7310</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kguqmd/aprielnemotron15bthinker_o1mini_level_with_mit/"&gt; &lt;img alt="Apriel-Nemotron-15b-Thinker - o1mini level with MIT licence (Nvidia &amp;amp; Servicenow)" src="https://external-preview.redd.it/EuRAXuyDLNOAu2-1ktV_-X31N5aAiqTZTPHiWEhPj-E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d4cc697d8897122cd61888ad7f02b892afd49c4" title="Apriel-Nemotron-15b-Thinker - o1mini level with MIT licence (Nvidia &amp;amp; Servicenow)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Service now and Nvidia brings a new 15B thinking model with comparable performance with 32B&lt;br /&gt; Model: &lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker"&gt;https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker&lt;/a&gt; (MIT licence)&lt;br /&gt; It looks very promising (resumed by Gemini) : &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Efficiency:&lt;/strong&gt; Claimed to be half the size of some SOTA models (like QWQ-32b, EXAONE-32b) and consumes significantly fewer tokens (~40% less than QWQ-32b) for comparable tasks, directly impacting VRAM requirements and inference costs for local or self-hosted setups.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning/Enterprise:&lt;/strong&gt; Reports strong performance on benchmarks like MBPP, BFCL, Enterprise RAG, IFEval, and Multi-Challenge. The focus on Enterprise RAG is notable for business-specific applications.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Coding:&lt;/strong&gt; Competitive results on coding tasks like MBPP and HumanEval, important for development workflows.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Academic:&lt;/strong&gt; Holds competitive scores on academic reasoning benchmarks (AIME, AMC, MATH, GPQA) relative to its parameter count.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; We need to test it&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary-Size7310"&gt; /u/Temporary-Size7310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kguqmd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kguqmd/aprielnemotron15bthinker_o1mini_level_with_mit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kguqmd/aprielnemotron15bthinker_o1mini_level_with_mit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T11:14:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgrjor</id>
    <title>New ""Open-Source"" Video generation model</title>
    <updated>2025-05-07T07:32:32+00:00</updated>
    <author>
      <name>/u/topiga</name>
      <uri>https://old.reddit.com/user/topiga</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrjor/new_opensource_video_generation_model/"&gt; &lt;img alt="New &amp;quot;&amp;quot;Open-Source&amp;quot;&amp;quot; Video generation model" src="https://external-preview.redd.it/ZHdlOHlodmQ5YnplMXyf8-rvm1C__Q4bDL3gJBkjO_bjkyMUPsobX80FiZpA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6db55c8236c9c875dc6ec641feb87d228687bd65" title="New &amp;quot;&amp;quot;Open-Source&amp;quot;&amp;quot; Video generation model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LTX-Video is the first DiT-based video generation model that can generate high-quality videos in &lt;em&gt;real-time&lt;/em&gt;. It can generate 30 FPS videos at 1216√ó704 resolution, faster than it takes to watch them. The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos with realistic and diverse content.&lt;/p&gt; &lt;p&gt;The model supports text-to-image, image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.&lt;/p&gt; &lt;p&gt;To be honest, I don't view it as open-source, not even open-weight. The license is weird, not a license we know of, and there's &amp;quot;Use Restrictions&amp;quot;. By doing so, it is NOT open-source.&lt;br /&gt; Yes, the restrictions are honest, and I invite you to read them, &lt;a href="https://static.lightricks.com/legal/LTXV-13b-0.9.7-dev.pdf"&gt;here is an example&lt;/a&gt;, but I think they're just doing this to protect themselves.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Lightricks/LTX-Video"&gt;https://github.com/Lightricks/LTX-Video&lt;/a&gt;&lt;br /&gt; HF: &lt;a href="https://huggingface.co/Lightricks/LTX-Video"&gt;https://huggingface.co/Lightricks/LTX-Video&lt;/a&gt; (FP8 coming soon)&lt;br /&gt; Documentation: &lt;a href="https://www.lightricks.com/ltxv-documentation"&gt;https://www.lightricks.com/ltxv-documentation&lt;/a&gt;&lt;br /&gt; Tweet: &lt;a href="https://x.com/LTXStudio/status/1919751150888239374"&gt;https://x.com/LTXStudio/status/1919751150888239374&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/topiga"&gt; /u/topiga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i4ioviud9bze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrjor/new_opensource_video_generation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrjor/new_opensource_video_generation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T07:32:32+00:00</published>
  </entry>
</feed>
