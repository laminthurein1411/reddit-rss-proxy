<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-02T03:40:57+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kzsa70</id>
    <title>China is leading open source</title>
    <updated>2025-05-31T08:35:25+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"&gt; &lt;img alt="China is leading open source" src="https://preview.redd.it/6stw9ivzw24f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87af4f2951867765dd0c43808b34253b587103b5" title="China is leading open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6stw9ivzw24f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T08:35:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1l17m9g</id>
    <title>SAGA Update: Autonomous Novel Writing with Deep KG &amp; Semantic Context - Now Even More Advanced!</title>
    <updated>2025-06-02T03:12:03+00:00</updated>
    <author>
      <name>/u/MariusNocturnum</name>
      <uri>https://old.reddit.com/user/MariusNocturnum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A couple of weeks ago, I shared an early version of SAGA (Semantic And Graph-enhanced Authoring), my project for autonomous novel generation. Thanks to some great initial feedback and a lot of focused development, I'm excited to share a significantly advanced version!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is SAGA?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SAGA, powered by its NANA (Next-gen Autonomous Narrative Architecture) engine, is designed to write entire novels. It's not just about stringing words together; it employs a team of specialized AI agents that handle planning, drafting, comprehensive evaluation, continuity checking, and intelligent revision. The core idea is to combine the creative power of local LLMs with the structured knowledge of a Neo4j graph database and the coherence provided by semantic embeddings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's New &amp;amp; Improved Since Last Time?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SAGA has undergone substantial enhancements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Deep Neo4j Integration:&lt;/strong&gt; Moved from a simpler DB to a full Neo4j backend. This allows for much richer tracking of characters, world-building, plot points, and dynamic relationships. It includes a robust schema with constraints and a vector index for semantic searches.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Hybrid Context Generation:&lt;/strong&gt; For each chapter, SAGA now generates a &amp;quot;hybrid context&amp;quot; by: &lt;ul&gt; &lt;li&gt; Performing &lt;strong&gt;semantic similarity searches&lt;/strong&gt; (via Ollama embeddings) on past chapter content stored in Neo4j to maintain narrative flow and tone.&lt;/li&gt; &lt;li&gt; Extracting &lt;strong&gt;key reliable facts&lt;/strong&gt; directly from the Neo4j knowledge graph to ensure the LLM adheres to established canon.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Advanced Revision Logic:&lt;/strong&gt; The revision process is now more sophisticated, capable of &lt;strong&gt;patch-based revisions&lt;/strong&gt; for targeted fixes or full chapter rewrites when necessary.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Sophisticated Evaluation &amp;amp; Continuity:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt; The &lt;code&gt;ComprehensiveEvaluatorAgent&lt;/code&gt; assesses drafts on multiple axes (plot, theme, depth, consistency).&lt;/li&gt; &lt;li&gt; A dedicated &lt;code&gt;WorldContinuityAgent&lt;/code&gt; performs focused checks against the KG and world-building data to catch inconsistencies.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Provisional Data Handling:&lt;/strong&gt; The system now explicitly tracks whether data is &amp;quot;provisional&amp;quot; (e.g., from an unrevised draft), allowing for better canon management.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Markdown for User Input:&lt;/strong&gt; You can now seed your story using a &lt;code&gt;user_story_elements.md&lt;/code&gt; file with &lt;code&gt;[Fill-in]&lt;/code&gt; placeholders, making initial setup more intuitive.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Text De-duplication:&lt;/strong&gt; Added a step to help reduce repetitive phrasing or content in generated drafts.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Performance &amp;amp; Stability:&lt;/strong&gt; Lots of under-the-hood improvements. SAGA can now generate a batch of 3 chapters (each ~13K+ tokens of narrative) in about 11 minutes on my setup, including all the planning, evaluation, and KG updates.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Core Architecture Still Intact:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The agentic pipeline remains central:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;strong&gt;Initial Setup:&lt;/strong&gt; Parses user markdown or generates plot, characters, and world-building; pre-populates Neo4j.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Chapter Loop:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Plan:&lt;/strong&gt; &lt;code&gt;PlannerAgent&lt;/code&gt; details scenes.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Context:&lt;/strong&gt; Hybrid semantic &amp;amp; KG context is built.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Draft:&lt;/strong&gt; &lt;code&gt;DraftingAgent&lt;/code&gt; writes the chapter.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Evaluate:&lt;/strong&gt; &lt;code&gt;ComprehensiveEvaluatorAgent&lt;/code&gt; &amp;amp; &lt;code&gt;WorldContinuityAgent&lt;/code&gt; scrutinize the draft.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Revise:&lt;/strong&gt; &lt;code&gt;ChapterRevisionLogic&lt;/code&gt; applies fixes.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Finalize &amp;amp; Update KG:&lt;/strong&gt; &lt;code&gt;KGMaintainerAgent&lt;/code&gt; summarizes, embeds, saves the chapter to Neo4j, and extracts/merges new knowledge back into the graph and agent state.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why This Approach?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The goal is to create narratives that are not only creative but also &lt;em&gt;coherent&lt;/em&gt; and &lt;em&gt;consistent&lt;/em&gt; over tens of thousands of tokens. The graph database acts as the story's long-term memory and source of truth, while semantic embeddings help maintain flow and relevance.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current Performance Example:&lt;/strong&gt; Using local GGUF models (Qwen3 14B for narration/planning, smaller Qwen3s for other tasks), SAGA generates: * &lt;strong&gt;3 chapters&lt;/strong&gt; (each ~13,000+ tokens of narrative) * In approximately &lt;strong&gt;11 minutes&lt;/strong&gt; * This includes all planning, context generation, evaluation, and knowledge graph updates.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Check it out &amp;amp; Get Involved:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/Lanerra/saga"&gt;https://github.com/Lanerra/saga&lt;/a&gt; (The README has been updated with detailed setup instructions!)&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Setup:&lt;/strong&gt; You'll need Python, Ollama (for embeddings), an OpenAI-API compatible LLM server, and Neo4j (Docker setup provided).&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Reset Script:&lt;/strong&gt; &lt;code&gt;reset_neo4j.py&lt;/code&gt; is still there to easily clear the database and start fresh.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Inspect KG:&lt;/strong&gt; The &lt;code&gt;inspect_kg.py&lt;/code&gt; script mentioned previously has been replaced by direct Neo4j browser interaction (which is much more powerful for visualization).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm really proud of how far SAGA has come and believe it's pushing into some interesting territory for AI-assisted storytelling. I'd love for you all to try it out, see what kind of sagas NANA can spin up for you, and share your thoughts, feedback, or any issues you encounter.&lt;/p&gt; &lt;p&gt;What kind of stories will you create?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MariusNocturnum"&gt; /u/MariusNocturnum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l17m9g/saga_update_autonomous_novel_writing_with_deep_kg/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l17m9g/saga_update_autonomous_novel_writing_with_deep_kg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l17m9g/saga_update_autonomous_novel_writing_with_deep_kg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T03:12:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1l17sdd</id>
    <title>Memory Layer Compatible with Local Llama</title>
    <updated>2025-06-02T03:21:11+00:00</updated>
    <author>
      <name>/u/OneEither8511</name>
      <uri>https://old.reddit.com/user/OneEither8511</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a open-sourced remote personal memory vault that works with MCP compatible clients. You can just say &amp;quot;remember X, Y, Z.&amp;quot; and then retrieve it later. You can store documents, and I am working on integrations with Obsidian and such. Looking for contributors to make this compatible with local llama.&lt;/p&gt; &lt;p&gt;I want this to be the catch all for who you are. And will be able to personalize the conversation for your personality. Would love any and all support with this and check it out if you're interested.&lt;/p&gt; &lt;p&gt;&lt;a href="http://jeanmemory.com"&gt;jeanmemory.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OneEither8511"&gt; /u/OneEither8511 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l17sdd/memory_layer_compatible_with_local_llama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l17sdd/memory_layer_compatible_with_local_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l17sdd/memory_layer_compatible_with_local_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T03:21:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1l13j9b</id>
    <title>How are you selecting LLMs?</title>
    <updated>2025-06-01T23:46:43+00:00</updated>
    <author>
      <name>/u/KVT_BK</name>
      <uri>https://old.reddit.com/user/KVT_BK</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l13j9b/how_are_you_selecting_llms/"&gt; &lt;img alt="How are you selecting LLMs?" src="https://b.thumbs.redditmedia.com/__xCGsKDTmvYKW2kxQ00rS17U-PKTSxzgfJdGNwD1Eo.jpg" title="How are you selecting LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Below is my Desktop config&lt;/p&gt; &lt;p&gt;CPU : I9-13900KF &lt;/p&gt; &lt;p&gt;RAM : 64GB DDR4 &lt;/p&gt; &lt;p&gt;GPU: NVIDIA GeForce RTX 4070 Ti with 12GB Dedicated GPU and 32GB Shared GPU. Overall, Task Manager shows my GPU Memory as 44GB.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xljtz6jqhe4f1.png?width=791&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6bfe83e00013b28e950b09b9c8d48a0e89e00f41"&gt;https://preview.redd.it/xljtz6jqhe4f1.png?width=791&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6bfe83e00013b28e950b09b9c8d48a0e89e00f41&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Q1 : While selecting a model should I be considering Dedicated GPU only or Total GPU memory which add shared GPU memory and Dedicated GPU Memory ?&lt;/p&gt; &lt;p&gt;When I run deepseek-r1:32B with Q4 quantization, its eval rate is too slow at 4.56 tokens/s. I feel Its due to model getting offloaded to CPU. Q2: Correct me if I am wrong.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/iur5oenlie4f1.png?width=497&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80efa14ab71d5d9c1e6bd8bc1d2c73130e0de88e"&gt;https://preview.redd.it/iur5oenlie4f1.png?width=497&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80efa14ab71d5d9c1e6bd8bc1d2c73130e0de88e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f6d5vv5zhe4f1.png?width=748&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f567193124bae72b3ff633af3f506c42c35ab482"&gt;https://preview.redd.it/f6d5vv5zhe4f1.png?width=748&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f567193124bae72b3ff633af3f506c42c35ab482&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/330wfl4wie4f1.png?width=351&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a174d5656e1f77f38e340ff3b9c0cf32f81886a"&gt;https://preview.redd.it/330wfl4wie4f1.png?width=351&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a174d5656e1f77f38e340ff3b9c0cf32f81886a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am using local LLMs for 2 use cases 1. Coding 2. General reasoning&lt;/p&gt; &lt;p&gt;Q3: How are you selecting which model to use for Coding and General Reasoning for your hardware?&lt;/p&gt; &lt;p&gt;Q4: Within coding, are you using anything smaller model for auto completions vs Full code agents?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KVT_BK"&gt; /u/KVT_BK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l13j9b/how_are_you_selecting_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l13j9b/how_are_you_selecting_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l13j9b/how_are_you_selecting_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T23:46:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0vwc1</id>
    <title>Would a laptop iGPU + 64GB RAM be good for anything, speed wise?</title>
    <updated>2025-06-01T18:20:15+00:00</updated>
    <author>
      <name>/u/ArsenicBismuth</name>
      <uri>https://old.reddit.com/user/ArsenicBismuth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;VRAM is a big limiting factor for a lot of bigger models for most of consumer GPU. So, I was wondering if my iGPU (Ryzen 5 5600H) would be capable for running some models locally using RAM?&lt;/p&gt; &lt;p&gt;Or would you think a M2 mac machine with similar RAM would be significantly better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArsenicBismuth"&gt; /u/ArsenicBismuth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0vwc1/would_a_laptop_igpu_64gb_ram_be_good_for_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0vwc1/would_a_laptop_igpu_64gb_ram_be_good_for_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0vwc1/would_a_laptop_igpu_64gb_ram_be_good_for_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T18:20:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0mo90</id>
    <title>Introducing an open source cross-platform graphical interface LLM client</title>
    <updated>2025-06-01T11:26:40+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0mo90/introducing_an_open_source_crossplatform/"&gt; &lt;img alt="Introducing an open source cross-platform graphical interface LLM client" src="https://external-preview.redd.it/asw6R0ibq6fWJLI0jTiqq5MWe_ZOda7dhXjccGwW8KM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=20d6c7b5164323d63cf76761c30754520702828d" title="Introducing an open source cross-platform graphical interface LLM client" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cherry Studio is a desktop client that supports for multiple LLM providers, available on Windows, Mac and Linux.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/CherryHQ/cherry-studio"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0mo90/introducing_an_open_source_crossplatform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0mo90/introducing_an_open_source_crossplatform/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T11:26:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0x0q8</id>
    <title>3x Modded 4090 48GB or RTX Pro 6000?</title>
    <updated>2025-06-01T19:05:50+00:00</updated>
    <author>
      <name>/u/sNullp</name>
      <uri>https://old.reddit.com/user/sNullp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can source them for about the same price. I've heard there is an efficiency hit on multi card with those modded 4090. But 3 card has 144GB vram vs RTX Pro's 96GB. &lt;del&gt;And power consumption is comparable.&lt;/del&gt; Which route should I choose?&lt;/p&gt; &lt;p&gt;Edit: power consumption is obviously not comparable. I don't know what I was thinking. But it is in a colo environment so doesn't matter much for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sNullp"&gt; /u/sNullp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0x0q8/3x_modded_4090_48gb_or_rtx_pro_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0x0q8/3x_modded_4090_48gb_or_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0x0q8/3x_modded_4090_48gb_or_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T19:05:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0wfln</id>
    <title>Is multiple m3 ultras the move instead of 1 big one?</title>
    <updated>2025-06-01T18:42:06+00:00</updated>
    <author>
      <name>/u/AcceptableBridge7616</name>
      <uri>https://old.reddit.com/user/AcceptableBridge7616</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am seriously considering investing in a sizable m3 ultra mac studio. Looking through some of the benchmarks, it seems the m3ultra's do well but not as well in prompt processing speed. The comparisons from the 60 core to the 80 core seem to show a (surprisingly?) big boost from going up in gpu size. Given the low power usage, I think just getting more than 1 is a real option. However, I couldn't really find any comparisons comparing chained configurations, though I have seen videos of people doing it especially with the previous model. If you are in the ~10k price range, I think it's worth considering different combos:&lt;/p&gt; &lt;p&gt;one 80 core, 512gb ram- ~$9.4k&lt;/p&gt; &lt;p&gt;two 60 core, 256gb ram each - ~ $11k&lt;/p&gt; &lt;p&gt;two 60 core, 1 256gb ram, 1 96gb ram ~ $9.6k&lt;/p&gt; &lt;p&gt;three 60 core, 96gb ram each ~$12k&lt;/p&gt; &lt;p&gt;Are you losing much performance by spreading things across 2 machines? I think the biggest issue will be the annoyance of administering 2+ boxes. Having different sized boxes many even more annoying. Anyone have any experience with this who can comment? Obviously the best setup is use case dependent but I am trying to understand what I might not be taking into account here...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AcceptableBridge7616"&gt; /u/AcceptableBridge7616 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0wfln/is_multiple_m3_ultras_the_move_instead_of_1_big/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0wfln/is_multiple_m3_ultras_the_move_instead_of_1_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0wfln/is_multiple_m3_ultras_the_move_instead_of_1_big/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T18:42:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0ylj8</id>
    <title>Pure vs. merged - and a modern leaderboard</title>
    <updated>2025-06-01T20:12:09+00:00</updated>
    <author>
      <name>/u/jaggzh</name>
      <uri>https://old.reddit.com/user/jaggzh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably been discussion about this, but I've noticed the trained-in quirks of models diminish with merged models. (Can't tell with abliterated since the only ones I've used are also mergers). Quirks include stubbornness in personality, desire consistency, to suck with certain formatting, etc. &lt;/p&gt; &lt;p&gt;Yet we have no leaderboard [that I know of] that evaluates them anymore. Most leaderboards now are quite crippled in filtering, let alone finding open models.&lt;/p&gt; &lt;p&gt;I'm trying to think of a way we could come up with basic low-energy-use community-based testing. It doesn't need to be exhaustive -- some small subsets of test types would likely satisfy for open against various mergers. &lt;/p&gt; &lt;p&gt;People can establish tests for honoring instruct, basic accuracies, math, function-calling, whatever. (Models bad at something tend to show it quite rapidly in my own experience.)&lt;/p&gt; &lt;p&gt;Being community-based (&amp;quot;crowd-sourced&amp;quot;), the system could cross-reference users' results to give a ranking reliability. Users can be get some type of reliability as well (perhaps a rank/algorithm we work on over time), to try to mitigate weirdos manipulating results (but one climbing high fraudulently would gain popularity and, thus, higher criticisms.&lt;/p&gt; &lt;p&gt;Also, since the turnover of models is quite rapid, I'm not sure if there's much risk in the system just not being that perfect anyway.&lt;/p&gt; &lt;p&gt;(It should, though, have some proper filtering and sorting in the results though!)&lt;/p&gt; &lt;p&gt;What do you all think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jaggzh"&gt; /u/jaggzh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0ylj8/pure_vs_merged_and_a_modern_leaderboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0ylj8/pure_vs_merged_and_a_modern_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0ylj8/pure_vs_merged_and_a_modern_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T20:12:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1l106wk</id>
    <title>Any LLM benchmarks yet for the GMKTek EVO-X2 AMD Ryzen AI Max+ PRO 395?</title>
    <updated>2025-06-01T21:17:46+00:00</updated>
    <author>
      <name>/u/StartupTim</name>
      <uri>https://old.reddit.com/user/StartupTim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any LLM benchmarks yet for the GMKTek Evo-X2 AMD Ryzen AI Max+ PRO 395? &lt;/p&gt; &lt;p&gt;I'd love to see latest benchmarks with ollama doing 30 to 100 GB models and maybe a lineup vs 4xxx and 5xxx Nvidia GPUs.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StartupTim"&gt; /u/StartupTim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l106wk/any_llm_benchmarks_yet_for_the_gmktek_evox2_amd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l106wk/any_llm_benchmarks_yet_for_the_gmktek_evox2_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l106wk/any_llm_benchmarks_yet_for_the_gmktek_evox2_amd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T21:17:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0rvqr</id>
    <title>Old dual socket Xeon server with tons of RAM viable for LLM inference?</title>
    <updated>2025-06-01T15:35:57+00:00</updated>
    <author>
      <name>/u/jojokingxp</name>
      <uri>https://old.reddit.com/user/jojokingxp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was looking into maybe getting a used 2 socket Lga 3647 board and some Xeons wit loads of (RAM 256GB+). I don't need insane speeds, but it shouldn't take hours either. &lt;/p&gt; &lt;p&gt;It seems a lot more affordable per GB than Apple silicon and of course VRAM, but I feel like it might be too slow to really be viable or just plain not worth it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jojokingxp"&gt; /u/jojokingxp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0rvqr/old_dual_socket_xeon_server_with_tons_of_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0rvqr/old_dual_socket_xeon_server_with_tons_of_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0rvqr/old_dual_socket_xeon_server_with_tons_of_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T15:35:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0xubg</id>
    <title>Toolcalling in the reasoning trace as an alternative to agentic frameworks</title>
    <updated>2025-06-01T19:40:13+00:00</updated>
    <author>
      <name>/u/ExaminationNo8522</name>
      <uri>https://old.reddit.com/user/ExaminationNo8522</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://2084.substack.com/p/deep-reasoning-with-tools-toolcalling"&gt;Deep Reasoning With Tools: Toolcalling in the reasoning trace&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey, so I was working on training reasoning models to do interesting things, when I started wanting them to be more dynamic: not just predict based on static information but actively search the data space to get information. Thus I built this toolset to integrate toolcalling into the reasoning trace of the AI models, since then I could do wayyy more complex RL training to allow it to do stuff like reconciliation of accounts, or more complex trading. However, as I built it, I realized that its actually a nice alternative to traditional agentic frameworks - you don't have discrete steps so it can run as long or as short as you want, and it can be invoked with a single command versus having to handle multiple steps. Thoughts? What other weirder agentic frameworks have y'all seen?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExaminationNo8522"&gt; /u/ExaminationNo8522 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0xubg/toolcalling_in_the_reasoning_trace_as_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0xubg/toolcalling_in_the_reasoning_trace_as_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0xubg/toolcalling_in_the_reasoning_trace_as_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T19:40:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l12cmi</id>
    <title>Playing generated games of Atari Style PingPong and Space Invaders, thanks to Qwen 3 8b! (Original non Deepseek version) This small model continues to amaze.</title>
    <updated>2025-06-01T22:51:37+00:00</updated>
    <author>
      <name>/u/c64z86</name>
      <uri>https://old.reddit.com/user/c64z86</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l12cmi/playing_generated_games_of_atari_style_pingpong/"&gt; &lt;img alt="Playing generated games of Atari Style PingPong and Space Invaders, thanks to Qwen 3 8b! (Original non Deepseek version) This small model continues to amaze." src="https://external-preview.redd.it/fORXcgKVkaTCLfSuQUzrXrubR0RAsGHr5swRFkIXzZY.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9570e8b102def9be8ddff84ea19c251af5011698" title="Playing generated games of Atari Style PingPong and Space Invaders, thanks to Qwen 3 8b! (Original non Deepseek version) This small model continues to amaze." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/c64z86"&gt; /u/c64z86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/ar_kFDHGbhQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l12cmi/playing_generated_games_of_atari_style_pingpong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l12cmi/playing_generated_games_of_atari_style_pingpong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T22:51:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0qp75</id>
    <title>App-Use : Create virtual desktops for AI agents to focus on specific apps.</title>
    <updated>2025-06-01T14:46:57+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0qp75/appuse_create_virtual_desktops_for_ai_agents_to/"&gt; &lt;img alt="App-Use : Create virtual desktops for AI agents to focus on specific apps." src="https://external-preview.redd.it/ejV6cmV3ODZ3YjRmMYsTHh_R0WswrUJBBa-0t3y7YsS9UlwJcbvZWkm9vo2Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e17e2eb7db050248c2423a2e18a718dcda868c6" title="App-Use : Create virtual desktops for AI agents to focus on specific apps." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;App-Use lets you scope agents to just the apps they need. Instead of full desktop access, say &amp;quot;only work with Safari and Notes&amp;quot; or &amp;quot;just control iPhone Mirroring&amp;quot; - visual isolation without new processes for perfectly focused automation.&lt;/p&gt; &lt;p&gt;Running computer-use on the entire desktop often causes agent hallucinations and loss of focus when they see irrelevant windows and UI elements. App-Use solves this by creating composited views where agents only see what matters, dramatically improving task completion accuracy&lt;/p&gt; &lt;p&gt;Currently macOS-only (Quartz compositing engine). &lt;/p&gt; &lt;p&gt;Read the full guide: &lt;a href="https://trycua.com/blog/app-use"&gt;https://trycua.com/blog/app-use&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/v0fcznj6wb4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0qp75/appuse_create_virtual_desktops_for_ai_agents_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0qp75/appuse_create_virtual_desktops_for_ai_agents_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T14:46:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0y4ep</id>
    <title>A Privacy-Focused Perplexity That Runs Locally on all your devices - iPhone, Android, iPad!</title>
    <updated>2025-06-01T19:52:02+00:00</updated>
    <author>
      <name>/u/Ssjultrainstnict</name>
      <uri>https://old.reddit.com/user/Ssjultrainstnict</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt; community!&lt;/p&gt; &lt;p&gt;Following up on my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ku1444/a_privacyfocused_perplexity_that_runs_locally_on/"&gt;previous post&lt;/a&gt;- the response has been incredible! Thank you to everyone who tried it out, left reviews, and provided feedback.&lt;/p&gt; &lt;p&gt;Based on your requests, I'm excited to announce that &lt;strong&gt;MyDeviceAI is now available on iPad and Android&lt;/strong&gt;!&lt;/p&gt; &lt;h1&gt;iPad Support&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Full native iPad experience with optimized UI&lt;/li&gt; &lt;li&gt;Same lightning-fast local processing with M-series chips&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Android Release&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Available as APK on GitHub releases (v1.2)&lt;/li&gt; &lt;li&gt;Download link: &lt;a href="https://github.com/navedmerchant/MyDeviceAI/releases"&gt;https://github.com/navedmerchant/MyDeviceAI/releases&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Same core features: local AI, SearXNG integration, complete privacy&lt;/li&gt; &lt;li&gt;Works across a wide range of Android devices&lt;/li&gt; &lt;li&gt;Runs on CPU only for now, working on getting Adreno GPU support in llama.rn&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What's Next?&lt;/h1&gt; &lt;p&gt;I'm continuing to work on improvements based on your suggestions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ability to select a larger model for powerful supported devices (Qwen 3 4b)&lt;/li&gt; &lt;li&gt;Ability to add images and documents to the chat for supported devices (QwenVL support)&lt;/li&gt; &lt;li&gt;Advanced speech mode on device&lt;/li&gt; &lt;li&gt;Enhanced personalization features&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Download Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;iOS/iPad&lt;/strong&gt;: &lt;a href="https://apps.apple.com/us/app/mydeviceai/id6736578281?platform=ipad"&gt;MyDeviceAI on App Store&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Android&lt;/strong&gt;: &lt;a href="https://github.com/navedmerchant/MyDeviceAI/releases"&gt;GitHub Releases v1.2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Source Code&lt;/strong&gt;: &lt;a href="https://github.com/navedmerchant/MyDeviceAI"&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you've been waiting for Android support or want to try it on iPad, now's your chance! As always, everything remains 100% free, open source, and completely private.&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts on the new platforms, and please consider leaving a review if MyDeviceAI has been useful for you. Your support helps tremendously with continued development!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ssjultrainstnict"&gt; /u/Ssjultrainstnict &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y4ep/a_privacyfocused_perplexity_that_runs_locally_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y4ep/a_privacyfocused_perplexity_that_runs_locally_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y4ep/a_privacyfocused_perplexity_that_runs_locally_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T19:52:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0m8r0</id>
    <title>104k-Token Prompt in a 110k-Token Context with DeepSeek-R1-0528-UD-IQ1_S â€“ Benchmark &amp; Impressive Results</title>
    <updated>2025-06-01T11:00:46+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;The Prompts:&lt;/strong&gt; 1. &lt;a href="https://thireus.com/REDDIT/DeepSeek_Runescape_Massive_Prompt.txt"&gt;https://thireus.com/REDDIT/DeepSeek_Runescape_Massive_Prompt.txt&lt;/a&gt; (Firefox: View -&amp;gt; Repair Text Encoding) 2. &lt;a href="https://thireus.com/REDDIT/DeepSeek_Dipiloblop_Massive_Prompt.txt"&gt;https://thireus.com/REDDIT/DeepSeek_Dipiloblop_Massive_Prompt.txt&lt;/a&gt; (Firefox: View -&amp;gt; Repair Text Encoding)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Commands (on Windows):&lt;/strong&gt; &lt;code&gt; perl -pe 's/\n/\\n/' DeepSeek_Runescape_Massive_Prompt.txt | CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0,2,1 ~/llama-b5355-bin-win-cuda12.4-x64/llama-cli -m DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf -t 36 --ctx-size 110000 -ngl 62 --flash-attn --main-gpu 0 --no-mmap --mlock -ot &amp;quot;.ffn_(up|down)_exps.=CPU&amp;quot; --simple-io &lt;/code&gt; &lt;code&gt; perl -pe 's/\n/\\n/' DeepSeek_Dipiloblop_Massive_Prompt.txt | CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0,2,1 ~/llama-b5355-bin-win-cuda12.4-x64/llama-cli -m DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf -t 36 --ctx-size 110000 -ngl 62 --flash-attn --main-gpu 0 --no-mmap --mlock -ot &amp;quot;.ffn_(up|down)_exps.=CPU&amp;quot; --simple-io &lt;/code&gt; - Tips: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kysms8"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kysms8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Answers (first time I see a model provide such a good answer):&lt;/strong&gt; - &lt;a href="https://thireus.com/REDDIT/DeepSeek_Runescape_Massive_Prompt_Answer.txt"&gt;https://thireus.com/REDDIT/DeepSeek_Runescape_Massive_Prompt_Answer.txt&lt;/a&gt; - &lt;a href="https://thireus.com/REDDIT/DeepSeek_Dipiloblop_Massive_Prompt_Answer.txt"&gt;https://thireus.com/REDDIT/DeepSeek_Dipiloblop_Massive_Prompt_Answer.txt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Hardware:&lt;/strong&gt; &lt;code&gt; i9-7980XE - 4.2Ghz on all cores 256GB DDR4 F4-3200C14Q2-256GTRS - XMP enabled 1x 5090 (x16) 1x 3090 (x16) 1x 3090 (x8) Prime-X299-A-II &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The benchmark results:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Runescape: ``` llama_perf_sampler_print: sampling time = 608.32 ms / 106524 runs ( 0.01 ms per token, 175112.36 tokens per second) llama_perf_context_print: load time = 190451.73 ms llama_perf_context_print: prompt eval time = 5188938.33 ms / 104276 tokens ( 49.76 ms per token, 20.10 tokens per second) llama_perf_context_print: eval time = 577349.77 ms / 2248 runs ( 256.83 ms per token, 3.89 tokens per second) llama_perf_context_print: total time = 5768493.07 ms / 106524 tokens&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 608.32 ms / 106524 runs ( 0.01 ms per token, 175112.36 tokens per second) llama_perf_context_print: load time = 190451.73 ms llama_perf_context_print: prompt eval time = 5188938.33 ms / 104276 tokens ( 49.76 ms per token, 20.10 tokens per second) llama_perf_context_print: eval time = 577349.77 ms / 2248 runs ( 256.83 ms per token, 3.89 tokens per second) llama_perf_context_print: total time = 5768493.22 ms / 106524 tokens &lt;code&gt; Dipiloblop: &lt;/code&gt; llama_perf_sampler_print: sampling time = 534.36 ms / 106532 runs ( 0.01 ms per token, 199364.47 tokens per second) llama_perf_context_print: load time = 177215.16 ms llama_perf_context_print: prompt eval time = 5101404.01 ms / 104586 tokens ( 48.78 ms per token, 20.50 tokens per second) llama_perf_context_print: eval time = 500475.72 ms / 1946 runs ( 257.18 ms per token, 3.89 tokens per second) llama_perf_context_print: total time = 5603899.16 ms / 106532 tokens&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 534.36 ms / 106532 runs ( 0.01 ms per token, 199364.47 tokens per second) llama_perf_context_print: load time = 177215.16 ms llama_perf_context_print: prompt eval time = 5101404.01 ms / 104586 tokens ( 48.78 ms per token, 20.50 tokens per second) llama_perf_context_print: eval time = 500475.72 ms / 1946 runs ( 257.18 ms per token, 3.89 tokens per second) llama_perf_context_print: total time = 5603899.32 ms / 106532 tokens ```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Sampler (default values were used, DeepSeek recommends temp 0.6, but 0.8 was used):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Runescape: &lt;code&gt; sampler seed: 3756224448 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 110080 top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampler chain: logits -&amp;gt; logit-bias -&amp;gt; penalties -&amp;gt; dry -&amp;gt; top-n-sigma -&amp;gt; top-k -&amp;gt; typical -&amp;gt; top-p -&amp;gt; min-p -&amp;gt; xtc -&amp;gt; temp-ext -&amp;gt; dist &lt;/code&gt; Dipiloblop: &lt;code&gt; sampler seed: 1633590497 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 110080 top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampler chain: logits -&amp;gt; logit-bias -&amp;gt; penalties -&amp;gt; dry -&amp;gt; top-n-sigma -&amp;gt; top-k -&amp;gt; typical -&amp;gt; top-p -&amp;gt; min-p -&amp;gt; xtc -&amp;gt; temp-ext -&amp;gt; dist &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The questions:&lt;/strong&gt; 1. Would 1x RTX PRO 6000 Blackwell or even 2x RTX PRO 6000 Blackwell significantly improve these metrics without any other hardware upgrade? (knowing that there would still be CPU offloading) 2. Would a different CPU, motherboard and RAM improve these metrics? 3. How to significantly improve prompt processing speed?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt; - Comparative results with Qwen3-235B-A22B-128K-UD-Q3_K_XL are here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1l0m8r0/comment/mvg5ke9/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1l0m8r0/comment/mvg5ke9/&lt;/a&gt; - I've compiled the latest llama.cpp with Blackwell support (&lt;a href="https://github.com/Thireus/llama.cpp/releases/tag/b5565"&gt;https://github.com/Thireus/llama.cpp/releases/tag/b5565&lt;/a&gt;) and now get slightly better speeds than shared before: 21.71 tokens per second (pp) + 4.36 tokens per second - I've been using the GGUF version from 2 days ago sha256: 0e2df082b88088470a761421d48a391085c238a66ea79f5f006df92f0d7d7193, see &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/commit/ff13ed80e2c95ebfbcf94a8d6682ed989fb6961b"&gt;https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/commit/ff13ed80e2c95ebfbcf94a8d6682ed989fb6961b&lt;/a&gt; - The newest GGUF version results may differ (which I have not tested).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0m8r0/104ktoken_prompt_in_a_110ktoken_context_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0m8r0/104ktoken_prompt_in_a_110ktoken_context_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0m8r0/104ktoken_prompt_in_a_110ktoken_context_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T11:00:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0n5ta</id>
    <title>Which is the best uncensored model?</title>
    <updated>2025-06-01T11:55:48+00:00</updated>
    <author>
      <name>/u/BoJackHorseMan53</name>
      <uri>https://old.reddit.com/user/BoJackHorseMan53</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to learn ethical hacking. Tried dolphin-mistral-r1 it did answer but it's answers were bad.&lt;/p&gt; &lt;p&gt;Are there any good uncensored models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoJackHorseMan53"&gt; /u/BoJackHorseMan53 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0n5ta/which_is_the_best_uncensored_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0n5ta/which_is_the_best_uncensored_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0n5ta/which_is_the_best_uncensored_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T11:55:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0v8yq</id>
    <title>I made a simple tool to test/compare your local LLMs on AIME 2024</title>
    <updated>2025-06-01T17:54:01+00:00</updated>
    <author>
      <name>/u/EntropyMagnets</name>
      <uri>https://old.reddit.com/user/EntropyMagnets</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0v8yq/i_made_a_simple_tool_to_testcompare_your_local/"&gt; &lt;img alt="I made a simple tool to test/compare your local LLMs on AIME 2024" src="https://external-preview.redd.it/-Bks8K2_TljN7hLY0DvxIu9Ncpa8BzunHNO4VODMSAA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e1cf6849b57a4c81ac6a807fbf541e56f6b4544" title="I made a simple tool to test/compare your local LLMs on AIME 2024" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made &lt;a href="https://github.com/Belluxx/LocalAIME"&gt;LocalAIME&lt;/a&gt; a simple tool that tests one or many LLMs locally or trough API (you can use any OpenAI-compatible API) on AIME 2024.&lt;/p&gt; &lt;p&gt;It is pretty useful for testing different quants of the same model or the same quant of different providers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r0xk016htc4f1.png?width=4900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5fbfc8a2d435ef0fe50a7ed0dab250cdc03e6f2c"&gt;Performance of some models i tested for each AIME 2024 problem&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think about it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EntropyMagnets"&gt; /u/EntropyMagnets &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0v8yq/i_made_a_simple_tool_to_testcompare_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0v8yq/i_made_a_simple_tool_to_testcompare_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0v8yq/i_made_a_simple_tool_to_testcompare_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T17:54:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1l13fqa</id>
    <title>How are people running dual GPU these days?</title>
    <updated>2025-06-01T23:42:00+00:00</updated>
    <author>
      <name>/u/admiralamott</name>
      <uri>https://old.reddit.com/user/admiralamott</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 4080 but was considering getting a 3090 for LLM models. I've never ran a dual set up before because I read like 6 years ago that it isn't used anymore. But clearly people are doing it so is that still going on? How does it work? Will it only offload to 1 gpu and then to the RAM, or can it offload to one GPU and then to the second one if it needs more? How do I know if my PC can do it? It's down to the motherboard right? (Sorry I am so behind rn) I'm also using ollama with OpenWebUI if that helps.&lt;/p&gt; &lt;p&gt;Thank you for your time :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/admiralamott"&gt; /u/admiralamott &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l13fqa/how_are_people_running_dual_gpu_these_days/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l13fqa/how_are_people_running_dual_gpu_these_days/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l13fqa/how_are_people_running_dual_gpu_these_days/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T23:42:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0p3et</id>
    <title>Let's build a production level Small Language Model (SLM) from scratch | 3 hour workshop</title>
    <updated>2025-06-01T13:34:12+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0p3et/lets_build_a_production_level_small_language/"&gt; &lt;img alt="Let's build a production level Small Language Model (SLM) from scratch | 3 hour workshop" src="https://external-preview.redd.it/3QugVQO6P_Q3v0881CbP7ispW7LV5z9hQhVFGV8ZV58.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64639bca07382b454fb4ec613939209217564782" title="Let's build a production level Small Language Model (SLM) from scratch | 3 hour workshop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/z6y0w621jb4f1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8491a9d59f58f3082afceaa6c737005c740ad38b"&gt;https://preview.redd.it/z6y0w621jb4f1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8491a9d59f58f3082afceaa6c737005c740ad38b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I made a 3 hour workshop showing how to build an SLM from scratch. &lt;/p&gt; &lt;p&gt;Watch it here: &lt;a href="https://youtu.be/pOFcwcwtv3k?si=1UI4uCdw_HLbdQgX"&gt;https://youtu.be/pOFcwcwtv3k?si=1UI4uCdw_HLbdQgX&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here is what I cover in the workshop: &lt;/p&gt; &lt;p&gt;(a) Download a dataset with 1million+ samples&lt;/p&gt; &lt;p&gt;(b) Pre-process and tokenize the dataset&lt;/p&gt; &lt;p&gt;(c) Divide the dataset into input-target pairs&lt;/p&gt; &lt;p&gt;(d) Assemble the SLM architecture: tokenization layer, attention layer, transformer block, output layer and everything in between&lt;/p&gt; &lt;p&gt;(e) Pre-train the entire SLM&lt;/p&gt; &lt;p&gt;(f) Run inference and generate new text from your trained SLM!&lt;/p&gt; &lt;p&gt;This is not a toy project. &lt;/p&gt; &lt;p&gt;It's a production-level project with an extensive dataset. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0p3et/lets_build_a_production_level_small_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0p3et/lets_build_a_production_level_small_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0p3et/lets_build_a_production_level_small_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T13:34:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1581z</id>
    <title>Which model are you using? June'25 edition</title>
    <updated>2025-06-02T01:09:13+00:00</updated>
    <author>
      <name>/u/Ok_Influence505</name>
      <uri>https://old.reddit.com/user/Ok_Influence505</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As proposed previously from this &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jxu0f7/we_should_have_a_monthly_which_models_are_you/"&gt;post&lt;/a&gt;, it's time for another monthly check-in on the latest models and their applications. The goal is to keep everyone updated on recent releases and discover hidden gems that might be flying under the radar.&lt;/p&gt; &lt;p&gt;With new models like DeepSeek-R1-0528, Claude 4 dropping recently, I'm curious to see how these stack up against established options. Have you tested any of the latest releases? How do they compare to what you were using before?&lt;/p&gt; &lt;p&gt;So, let start a discussion on what models (both proprietary and open-weights) are use using (or stop using ;) ) for different purposes (coding, writing, creative writing etc.).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Influence505"&gt; /u/Ok_Influence505 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1581z/which_model_are_you_using_june25_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1581z/which_model_are_you_using_june25_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1581z/which_model_are_you_using_june25_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T01:09:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l13tv3</id>
    <title>Who is getting paid to work doing this rather than just hobby dabbling..what was your path?</title>
    <updated>2025-06-02T00:00:47+00:00</updated>
    <author>
      <name>/u/bornfree4ever</name>
      <uri>https://old.reddit.com/user/bornfree4ever</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really enjoy hacking together LLM scripts and ideas. but how do I get paid doing it??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bornfree4ever"&gt; /u/bornfree4ever &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l13tv3/who_is_getting_paid_to_work_doing_this_rather/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l13tv3/who_is_getting_paid_to_work_doing_this_rather/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l13tv3/who_is_getting_paid_to_work_doing_this_rather/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T00:00:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0q2zk</id>
    <title>DeepSeek-R1-0528-UD-Q6-K-XL on 10 Year Old Hardware</title>
    <updated>2025-06-01T14:19:51+00:00</updated>
    <author>
      <name>/u/Simusid</name>
      <uri>https://old.reddit.com/user/Simusid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't expect anything useful in this post. I did it just to see if it was possible. This was on a 10+ year old system with a 6th generation i5 with 12gb of RAM. My ssd is nearly full so I had to mount an external 8TB USB drive to store the 560GB model. At least it is USB-3.&lt;/p&gt; &lt;p&gt;I made an 800GB swap file and enabled it, then launched llama-cli with a simple prompt and went to bed. I half expected that the model might not even have fully loaded when I got up but it was already part way through the response.&lt;/p&gt; &lt;p&gt;With no GPU, it seems to be about seven minutes per token.&lt;/p&gt; &lt;p&gt;Edit - I've named this system TreeBeard&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Simusid"&gt; /u/Simusid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0q2zk/deepseekr10528udq6kxl_on_10_year_old_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0q2zk/deepseekr10528udq6kxl_on_10_year_old_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0q2zk/deepseekr10528udq6kxl_on_10_year_old_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T14:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0zsv7</id>
    <title>25L Portable NV-linked Dual 3090 LLM Rig</title>
    <updated>2025-06-01T21:01:58+00:00</updated>
    <author>
      <name>/u/Special-Wolverine</name>
      <uri>https://old.reddit.com/user/Special-Wolverine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0zsv7/25l_portable_nvlinked_dual_3090_llm_rig/"&gt; &lt;img alt="25L Portable NV-linked Dual 3090 LLM Rig" src="https://b.thumbs.redditmedia.com/SCLEVQyCptjUTsrbRVb6jCIJUk1CuEOO-Ud355bse9Q.jpg" title="25L Portable NV-linked Dual 3090 LLM Rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Main point of portability is because The workplace of the coworker I built this for is truly offline, with no potential for LAN or wifi, so to download new models and update the system periodically I need to go pick it up from him and take it home. &lt;/p&gt; &lt;p&gt;WARNING - these components don't fit if you try to copy this build. The bottom GPU is resting on the Arctic p12 slim fans at the bottom of the case and pushing up on the GPU. Also the top arctic p14 Max fans don't have mounting points for half of their screw holes, and are in place by being very tightly wedged against the motherboard, case, and PSU. Also, there 's probably way too much pressure on the pcie cables coming off the gpus when you close the glass. Also I had to daisy chain the PCIE cables because the Corsair RM 1200e only has four available on the PSU side and these particular EVGA 3090s require 3x 8pin power. Allegedly it just enforces a hardware power limit to 300 w but you should make it a little bit more safe by also enforcing the 300W power limit in Nvidia -SMI To make sure that the cards don't try to pull 450W through 300W pipes. Could have fit a bigger PSU, but then I wouldn't get that front fan which is probably crucial.&lt;/p&gt; &lt;p&gt;All that being said, with a 300w power limit applied to both gpus in a silent fan profile, this rig has surprisingly good temperatures and noise levels considering how compact it is. &lt;/p&gt; &lt;p&gt;During Cinebench 24 with both gpus being 100% utilized, the CPU runs at 63 C and both gpus at 67 Celsius somehow with almost zero gap between them and the glass closed. All the while running at about 37 to 40 decibels from 1 meter away. &lt;/p&gt; &lt;p&gt;Prompt processing and inference - the gpus run at about 63 C, CPU at 55 C, and decibels at 34. &lt;/p&gt; &lt;p&gt;Again, I don't understand why the temperatures for both are almost the same, when logically the top GPU should be much hotter. The only gap between the two gpus is the size of one of those little silicone rubber DisplayPort caps wedged into the end, right between where the pcie power cables connect to force the GPUs apart a little.&lt;/p&gt; &lt;p&gt;Everything but the case, CPU cooler, and PSU was bought used on Facebook Marketplace&lt;/p&gt; &lt;p&gt;&lt;a href="https://pcpartpicker.com/list/nQXzgn"&gt;PCPartPicker Part List&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Item&lt;/th&gt; &lt;th align="left"&gt;Price&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/qtvqqs/amd-ryzen-7-5800x-38-ghz-8-core-processor-100-100000063wof"&gt;AMD Ryzen 7 5800X 3.8 GHz 8-Core Processor&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$160.54 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU Cooler&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/HbDQzy/id-cooling-frozn-a720-black-986-cfm-cpu-cooler-frozn-a720-black"&gt;ID-COOLING FROZN A720 BLACK 98.6 CFM CPU Cooler&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$69.98 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/CLkgXL/asus-rog-strix-x570-e-gaming-atx-am4-motherboard-rog-strix-x570-e-gaming"&gt;Asus ROG Strix X570-E Gaming ATX AM4 Motherboard&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$559.00 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/6rrcCJ/corsair-memory-cmk32gx4m2b3200c16"&gt;Corsair Vengeance LPX 32 GB (2 x 16 GB) DDR4-3200 CL16 Memory&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$81.96 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/DDWBD3/samsung-980-pro-1-tb-m2-2280-nvme-solid-state-drive-mz-v8p1t0bam"&gt;Samsung 980 Pro 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$149.99 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Video Card&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/PG848d/evga-geforce-rtx-3090-24-gb-ftw3-ultra-gaming-video-card-24g-p5-3987-kr"&gt;EVGA FTW3 ULTRA GAMING GeForce RTX 3090 24 GB Video Card&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$750.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Video Card&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/PG848d/evga-geforce-rtx-3090-24-gb-ftw3-ultra-gaming-video-card-24g-p5-3987-kr"&gt;EVGA FTW3 ULTRA GAMING GeForce RTX 3090 24 GB Video Card&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$750.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Custom&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;NVlink SLI bridge&lt;/td&gt; &lt;td align="left"&gt;$90.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Custom&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Mechanic Master c34plus&lt;/td&gt; &lt;td align="left"&gt;$200.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Custom&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Corsair RM1200e&lt;/td&gt; &lt;td align="left"&gt;$210.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Custom&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2x Arctic p14 max, 3x p12, 3x p12 slim&lt;/td&gt; &lt;td align="left"&gt;$60.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Prices include shipping, taxes, rebates, and discounts&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;$3081.47&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Generated by &lt;a href="https://pcpartpicker.com"&gt;PCPartPicker&lt;/a&gt; 2025-06-01 16:48 EDT-0400&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special-Wolverine"&gt; /u/Special-Wolverine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1l0zsv7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0zsv7/25l_portable_nvlinked_dual_3090_llm_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0zsv7/25l_portable_nvlinked_dual_3090_llm_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T21:01:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0y0wp</id>
    <title>Allowing LLM to ponder in Open WebUI</title>
    <updated>2025-06-01T19:47:52+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y0wp/allowing_llm_to_ponder_in_open_webui/"&gt; &lt;img alt="Allowing LLM to ponder in Open WebUI" src="https://external-preview.redd.it/dHd6NjY5c2JkZDRmMbDY_eAdKP8QUXyZwc-4j2cel9Olwb9ejqufCbXqijwB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d96e6747cff63170125fef17cdbcf53af47bbb3f" title="Allowing LLM to ponder in Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What is this?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A completely superficial way of letting LLM to ponder a bit before making its conversation turn. The process is streamed to an artifact within Open WebUI.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/av/harbor/blob/main/boost/src/modules/ponder.py"&gt;Code&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uoeptbsbdd4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y0wp/allowing_llm_to_ponder_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y0wp/allowing_llm_to_ponder_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T19:47:52+00:00</published>
  </entry>
</feed>
