<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-27T01:33:56+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jkrwyj</id>
    <title>dora-cli - cli tool for semantic search</title>
    <updated>2025-03-27T00:58:40+00:00</updated>
    <author>
      <name>/u/ranoutofusernames__</name>
      <uri>https://old.reddit.com/user/ranoutofusernames__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Local peeps, sharing this CLI &lt;a href="https://github.com/space0blaster/dora-cli"&gt;tool&lt;/a&gt; I wrote last weekend for using semantic search on your local files. It uses a super simple recursive (sorry NASA) crawler and embeds paths so you can use natural language to retrieve files and folder. It's a CLI version of the &lt;a href="https://github.com/space0blaster/dora"&gt;desktop app&lt;/a&gt; I released a couple months ago. Uses local Ollama for inference and ChromaDB for vector storage.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/space0blaster/dora-cli"&gt;https://github.com/space0blaster/dora-cli&lt;/a&gt; &lt;/p&gt; &lt;p&gt;License: MIT&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ranoutofusernames__"&gt; /u/ranoutofusernames__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkrwyj/doracli_cli_tool_for_semantic_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkrwyj/doracli_cli_tool_for_semantic_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkrwyj/doracli_cli_tool_for_semantic_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T00:58:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jklk5y</id>
    <title>MacBook Air M4/32gb Benchmarks</title>
    <updated>2025-03-26T20:18:19+00:00</updated>
    <author>
      <name>/u/The_flight_guy</name>
      <uri>https://old.reddit.com/user/The_flight_guy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got my M4 MacBook Air today and figured I‚Äôd share some benchmark figures. In order of parameters/size:&lt;/p&gt; &lt;p&gt;Phi4-mini (3.8b)- 34 t/s, Gemma3 (4b)- 35 t/s, Granite 3.2 (8b)- 18 t/s, Llama 3.1 (8b)- 20 t/s, Gemma3 (12b)- 13 t/s, Phi4 (14b)- 11 t/s, Gemma (27b)- 6 t/s, QWQ (32b)- 4 t/s&lt;/p&gt; &lt;p&gt;Let me know if you are curious about a particular model that I didn‚Äôt test!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_flight_guy"&gt; /u/The_flight_guy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jklk5y/macbook_air_m432gb_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jklk5y/macbook_air_m432gb_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jklk5y/macbook_air_m432gb_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T20:18:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkhhum</id>
    <title>What are the technical details behind recent improvements in image gen?</title>
    <updated>2025-03-26T17:32:38+00:00</updated>
    <author>
      <name>/u/West-Code4642</name>
      <uri>https://old.reddit.com/user/West-Code4642</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this isn't related to the current batch of local models (maybe in the future), but what are some of the technical details behind the improvements in recent image generators like OpenAI's native image gen or Gemini's? Or is it completely unknown at the moment? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/West-Code4642"&gt; /u/West-Code4642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkhhum/what_are_the_technical_details_behind_recent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkhhum/what_are_the_technical_details_behind_recent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkhhum/what_are_the_technical_details_behind_recent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T17:32:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk97sp</id>
    <title>Fin-R1:A Specialized Large Language Model for Financial Reasoning and Decision-Making</title>
    <updated>2025-03-26T11:08:59+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk97sp/finr1a_specialized_large_language_model_for/"&gt; &lt;img alt="Fin-R1:A Specialized Large Language Model for Financial Reasoning and Decision-Making" src="https://external-preview.redd.it/KYF-69e6kbGPdV1zCvq_5UYTDAc8xutzlnroB8C0qjQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c787cc38ce9acd3a889f5eb94343d6bba3f4c51b" title="Fin-R1:A Specialized Large Language Model for Financial Reasoning and Decision-Making" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fin-R1 is a large financial reasoning language model designed to tackle key challenges in financial AI, including fragmented data, inconsistent reasoning logic, and limited business generalization. It delivers state-of-the-art performance by utilizing a two-stage training process‚ÄîSFT and RL‚Äîon the high-quality Fin-R1-Data dataset. With a compact 7B parameter scale, it achieves scores of 85.0 in ConvFinQA and 76.0 in FinQA, outperforming larger models. Future work aims to enhance financial multimodal capabilities, strengthen regulatory compliance, and expand real-world applications, driving innovation in fintech while ensuring efficient and intelligent financial decision-making.&lt;/p&gt; &lt;p&gt;The reasoning abilities of Fin-R1 in financial scenarios were evaluated through a comparative analysis against several state-of-the-art models, including DeepSeek-R1, Fin-R1-SFT, and various Qwen and Llama-based architectures. Despite its compact 7B parameter size, Fin-R1 achieved a notable average score of 75.2, ranking second overall. It outperformed all models of similar scale and exceeded DeepSeek-R1-Distill-Llama-70B by 8.7 points. Fin-R1 ranked highest in FinQA and ConvFinQA with scores of 76.0 and 85.0, respectively, demonstrating strong financial reasoning and cross-task generalization, particularly in benchmarks like Ant_Finance, TFNS, and Finance-Instruct-500K.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h3ykrngjn0re1.png?width=617&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bb2dd12be4e245ce360cbb2d4aa48265958f9dd"&gt;https://preview.redd.it/h3ykrngjn0re1.png?width=617&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bb2dd12be4e245ce360cbb2d4aa48265958f9dd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/lbr6y8kun0re1.gif"&gt;https://i.redd.it/lbr6y8kun0re1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p1hgmlwwn0re1.png?width=1207&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=579c66b858a8b13260e56cdcf3d181fb6d3a6e91"&gt;https://preview.redd.it/p1hgmlwwn0re1.png?width=1207&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=579c66b858a8b13260e56cdcf3d181fb6d3a6e91&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SUFE-AIFLM-Lab/Fin-R1"&gt;HuggingFace (only Chinese)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2503.16252"&gt;Paper &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SUFE-AIFLM-Lab/Fin-R1/blob/main/README_en.md"&gt;HuggingFace (eng)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk97sp/finr1a_specialized_large_language_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk97sp/finr1a_specialized_large_language_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk97sp/finr1a_specialized_large_language_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T11:08:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjorwd</id>
    <title>I think we‚Äôre going to need a bigger bank account.</title>
    <updated>2025-03-25T17:20:34+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjorwd/i_think_were_going_to_need_a_bigger_bank_account/"&gt; &lt;img alt="I think we‚Äôre going to need a bigger bank account." src="https://preview.redd.it/zr3syf8mdvqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=27187b0a5f34d831c3e26fc2978cc6ab6324cf35" title="I think we‚Äôre going to need a bigger bank account." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zr3syf8mdvqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjorwd/i_think_were_going_to_need_a_bigger_bank_account/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjorwd/i_think_were_going_to_need_a_bigger_bank_account/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T17:20:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jke2i6</id>
    <title>LlamaCon 2025 Registration Opens</title>
    <updated>2025-03-26T15:11:01+00:00</updated>
    <author>
      <name>/u/danmaruchi</name>
      <uri>https://old.reddit.com/user/danmaruchi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jke2i6/llamacon_2025_registration_opens/"&gt; &lt;img alt="LlamaCon 2025 Registration Opens" src="https://preview.redd.it/b6wvdtwev1re1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4be375de33f1ceba195dfe3aebb4af13be42d95a" title="LlamaCon 2025 Registration Opens" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After registering for email updates at &lt;a href="https://www.llama.com/events/llamacon/signup/"&gt;https://www.llama.com/events/llamacon/signup/&lt;/a&gt;, I received an email to register to attend in-person today.&lt;/p&gt; &lt;p&gt;Date &amp;amp; Time: April 29, 2025 9:30AM - 6PM&lt;/p&gt; &lt;p&gt;Location: Meta HQ, Menlo Park, CA&lt;/p&gt; &lt;p&gt;From what I see, parts of it will be live-streamed, but I don‚Äôt think there‚Äôs an option to attend online.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danmaruchi"&gt; /u/danmaruchi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b6wvdtwev1re1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jke2i6/llamacon_2025_registration_opens/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jke2i6/llamacon_2025_registration_opens/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T15:11:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk0qjs</id>
    <title>1.78bit DeepSeek-V3-0324 - 230GB Unsloth Dynamic GGUF</title>
    <updated>2025-03-26T01:51:16+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk0qjs/178bit_deepseekv30324_230gb_unsloth_dynamic_gguf/"&gt; &lt;img alt="1.78bit DeepSeek-V3-0324 - 230GB Unsloth Dynamic GGUF" src="https://b.thumbs.redditmedia.com/VQUhjwmdzkKwJU-pPDHGMZROeUu65PNj2UGT0ZKAjUg.jpg" title="1.78bit DeepSeek-V3-0324 - 230GB Unsloth Dynamic GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey r/LocalLLaMA! We're back again to release DeepSeek-V3-0324 (671B) dynamic quants in &lt;strong&gt;1.78-bit and more GGUF formats&lt;/strong&gt; so you can run them locally. All GGUFs are at &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/67rxi0wo3yqe1.gif"&gt;https://i.redd.it/67rxi0wo3yqe1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We initially provided the &lt;strong&gt;1.58-bit version&lt;/strong&gt;, which you can still use but its outputs weren't the best. So, we found it necessary to upcast to 1.78-bit by increasing the down proj size to achieve much better performance.&lt;/p&gt; &lt;p&gt;To ensure the best tradeoff between accuracy and size, we do &lt;strong&gt;not to quantize all layers&lt;/strong&gt;, but selectively quantize e.g. the MoE layers to lower bit, and leave attention and other layers in 4 or 6bit. This time we also added 3.5 + 4.5-bit dynamic quants.&lt;/p&gt; &lt;p&gt;Read our Guide on How To Run the GGUFs on llama.cpp: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-v3-0324-locally"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-v3-0324-locally&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also found that if you use convert all layers to 2-bit (standard 2-bit GGUF), the model is still very bad, producing endless loops, gibberish and very poor code. Our Dynamic 2.51-bit quant largely solves this issue. The same applies for 1.78-bit however is it recommended to use our 2.51 version for best results.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model uploads:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;MoE Bits&lt;/th&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Disk Size&lt;/th&gt; &lt;th align="left"&gt;HF Link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1.78bit (prelim)&lt;/td&gt; &lt;td align="left"&gt;IQ1_S&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;151GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-IQ1_S"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1.93bit (prelim)&lt;/td&gt; &lt;td align="left"&gt;IQ1_M&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;178GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-IQ1_M"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.42-bit (prelim)&lt;/td&gt; &lt;td align="left"&gt;IQ2_XXS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;203GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-IQ2_XXS"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;2.71-bit (best)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Q2_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;231GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-Q2_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3.5-bit&lt;/td&gt; &lt;td align="left"&gt;Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;321GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-Q3_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4.5-bit&lt;/td&gt; &lt;td align="left"&gt;Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;406GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-Q4_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;For recommended settings:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Temperature of 0.3&lt;/strong&gt; (Maybe 0.0 for coding as &lt;a href="https://api-docs.deepseek.com/quick_start/parameter_settings"&gt;seen here&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Min_P of 0.00 (optional, but 0.01 works well, llama.cpp default is 0.1)&lt;/li&gt; &lt;li&gt;Chat template: &lt;code&gt;&amp;lt;ÔΩúUserÔΩú&amp;gt;Create a simple playable Flappy Bird Game in Python. Place the final game inside of a markdown section.&amp;lt;ÔΩúAssistantÔΩú&amp;gt;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;A BOS token of &lt;code&gt;&amp;lt;ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú&amp;gt;&lt;/code&gt; is auto added during tokenization (do NOT add it manually!)&lt;/li&gt; &lt;li&gt;DeepSeek mentioned using a &lt;strong&gt;system prompt&lt;/strong&gt; as well (optional) - it's in Chinese: &lt;code&gt;ËØ•Âä©Êâã‰∏∫DeepSeek ChatÔºåÁî±Ê∑±Â∫¶Ê±ÇÁ¥¢ÂÖ¨Âè∏ÂàõÈÄ†„ÄÇ\n‰ªäÂ§©ÊòØ3Êúà24Êó•ÔºåÊòüÊúü‰∏Ä„ÄÇ&lt;/code&gt; which translates to: &lt;code&gt;The assistant is DeepSeek Chat, created by DeepSeek.\nToday is Monday, March 24th.&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;For KV cache quantization, use 8bit, NOT 4bit - we found it to do noticeably worse.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I suggest people to run the 2.71bit for now - the other other bit quants (listed as prelim) are still processing.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# !pip install huggingface_hub hf_transfer import os os.environ[&amp;quot;HF_HUB_ENABLE_HF_TRANSFER&amp;quot;] = &amp;quot;1&amp;quot; from huggingface_hub import snapshot_download snapshot_download( repo_id = &amp;quot;unsloth/DeepSeek-V3-0324-GGUF&amp;quot;, local_dir = &amp;quot;unsloth/DeepSeek-V3-0324-GGUF&amp;quot;, allow_patterns = [&amp;quot;*UD-Q2_K_XL*&amp;quot;], # Dynamic 2.7bit (230GB) ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I did both the Flappy Bird and Heptagon test (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j7r47l/i%5C_just%5C_made%5C_an%5C_animation%5C_of%5C_a%5C_ball%5C_bouncing/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1j7r47l/i\_just\_made\_an\_animation\_of\_a\_ball\_bouncing/&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk0qjs/178bit_deepseekv30324_230gb_unsloth_dynamic_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk0qjs/178bit_deepseekv30324_230gb_unsloth_dynamic_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk0qjs/178bit_deepseekv30324_230gb_unsloth_dynamic_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T01:51:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkotcs</id>
    <title>Delving deep into Llama.cpp and exploiting Llama.cpp's Heap Maze, from Heap-Overflow to Remote-Code Execution.</title>
    <updated>2025-03-26T22:36:54+00:00</updated>
    <author>
      <name>/u/FitItem2633</name>
      <uri>https://old.reddit.com/user/FitItem2633</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://retr0.blog/blog/llama-rpc-rce"&gt;https://retr0.blog/blog/llama-rpc-rce&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FitItem2633"&gt; /u/FitItem2633 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkotcs/delving_deep_into_llamacpp_and_exploiting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkotcs/delving_deep_into_llamacpp_and_exploiting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkotcs/delving_deep_into_llamacpp_and_exploiting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T22:36:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jke93s</id>
    <title>I tested the new DeepSeek V3 (0324) vs Claude 3.7 Sonnet in a 250k Token Codebase...</title>
    <updated>2025-03-26T15:19:00+00:00</updated>
    <author>
      <name>/u/marvijo-software</name>
      <uri>https://old.reddit.com/user/marvijo-software</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used Aider to test the coding skills of the new DeepSeek V3 (0324) vs Claude 3.7 Sonnet and boy did DeepSeek deliver. DeepSeek V3 is now in an MIT license and as always, is open weights. GOAT. I tested their Tool Use abilities, using Cline MCP servers (Brave Search and Puppeteer), their frontend bug fixing skills using Aider on a Vite + React Fullstack app. Some TLDR findings:&lt;/p&gt; &lt;p&gt;- They rank the same in tool use, which is a huge improvement from the previous DeepSeek V3&lt;/p&gt; &lt;p&gt;- DeepSeek holds its ground very well against 3.7 Sonnet in almost all coding tasks, backend and frontend&lt;/p&gt; &lt;p&gt;- To watch them in action: &lt;a href="https://youtu.be/MuvGAD6AyKE"&gt;https://youtu.be/MuvGAD6AyKE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- DeepSeek still degrades a lot in inference speed once its context increases&lt;/p&gt; &lt;p&gt;- 3.7 Sonnet feels weaker than 3.5 in many larger codebase edits&lt;/p&gt; &lt;p&gt;- You need to actively manage context (Aider is best for this) using /add and /tokens in order to take advantage of DeepSeek. Not for cost of course, but for speed because it's slower with more context&lt;/p&gt; &lt;p&gt;- Aider's new /context feature was released after the video, would love to see how efficient and Agentic it is vs Cline/RooCode&lt;/p&gt; &lt;p&gt;- If you blacklist slow providers in OpenRouter, you actually get decent speeds with DeepSeek&lt;/p&gt; &lt;p&gt;What are your impressions of DeepSeek? I'm about to test it against the new proclaimed king, Gemini 2.5 Pro (Exp) and will release findings later&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marvijo-software"&gt; /u/marvijo-software &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jke93s/i_tested_the_new_deepseek_v3_0324_vs_claude_37/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jke93s/i_tested_the_new_deepseek_v3_0324_vs_claude_37/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jke93s/i_tested_the_new_deepseek_v3_0324_vs_claude_37/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T15:19:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkkcd2</id>
    <title>Multi modality is currently terrible in open source</title>
    <updated>2025-03-26T19:28:54+00:00</updated>
    <author>
      <name>/u/Unusual_Guidance2095</name>
      <uri>https://old.reddit.com/user/Unusual_Guidance2095</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don‚Äôt know if anyone else feels this way, but currently it seems that multimodal large language models are our best shot at a‚Äúworld model‚Äú (I‚Äôm using the term loosely, of course) and that in open source it‚Äôs currently terrible&lt;/p&gt; &lt;p&gt;A truly Multimodal large language model can replace virtually all models that we think of as AI :&lt;/p&gt; &lt;p&gt;Text to image (image generation) Image to text (image captioning, bounding box generation, object detection) Text to text (standard LLM) Audio to text (transcription) Text to audio (text to speech, music generation) Audio to audio (speech assistant) Image to image (image editing, temporal video generation, image segmentation, image upscaling) Not to mention all sorts of combinations : image and audio to image and audio (film continuation) audio to image (speech assistant that can generate images) image to audio (voice descriptions of images, sound generation for films, perhaps sign language interpretation) etc.&lt;/p&gt; &lt;p&gt;We‚Äôve seen time and time again that in AI having more domains in your training data makes your model better. Our best translation models today are LLM‚Äôs because they understand language more generally and we can give it specific requests ‚Äúmake this formal‚Äù ‚Äúmake this happy sounding‚Äù that no other translations software can do and they develop skills we don‚Äôt have to explicitly train for, we‚Äôve seen with the release of Gemini a few months ago how good its image editing capabilities are and no current model that I know of does image editing at all (let alone be good at it) again other than multimodal LLMs. Who knows what else it can do: visual reasoning by generating images so that it doesn‚Äôt fail the weird spatial benchmarks, etc.?&lt;/p&gt; &lt;p&gt;Yet no company has been able or even trying to replicate the success of either open AI 4o nor Gemini and every time someone releases a new ‚Äúomni‚Äù model it‚Äôs always missing something: modalities, a unified architecture so that all modalities are embedded in the same latent space so that all the above is possible, and it‚Äôs so irritating. QWEN for example doesn‚Äôt support any of the things that 4o voice can do: speak faster, slower, (theoretically) voice imitation, singing, background noise generation not to mention it‚Äôs not great on any of the text benchmarks either. There was the beyond disappointing Sesame model as well&lt;/p&gt; &lt;p&gt;At this point, I‚Äôm wondering if the close source companies do truly have a moat and it‚Äôs this specifically&lt;/p&gt; &lt;p&gt;Of course I‚Äôm not against specialized models and more explainable pipelines composed of multiple models, clearly it works very well for Waymo self driving, coding copilot, and should be used there but I‚Äôm wondering now if we will ever get a good omnimodal model&lt;/p&gt; &lt;p&gt;Sorry for the rant I just keep getting excited and then disappointed time and time again now probably up to 20 times by every subsequent multimodal model release and I‚Äôve been waiting years since the original 4o announcement for any good model that lives up to a quarter of my expectations&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unusual_Guidance2095"&gt; /u/Unusual_Guidance2095 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkkcd2/multi_modality_is_currently_terrible_in_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkkcd2/multi_modality_is_currently_terrible_in_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkkcd2/multi_modality_is_currently_terrible_in_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T19:28:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkhlk6</id>
    <title>Mismatch between official DeepSeek-V3.1 livebench score and my local test results.</title>
    <updated>2025-03-26T17:36:50+00:00</updated>
    <author>
      <name>/u/zjuwyz</name>
      <uri>https://old.reddit.com/user/zjuwyz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Livebench official website has reported 66.86 average for deepseek-v3-0324, which is significantly lower than results from my runs.&lt;br /&gt; I've run the tests 3 times. Here're the results:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;DeepSeek official API, --max-tokens 8192: average 70.2&lt;/li&gt; &lt;li&gt;Thirdparty provider, no extra flags: average 69.7&lt;/li&gt; &lt;li&gt;Thirdparty provider --max-tokens 16384 and --force-temperature 0.3: average 70.0&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Yes I'm using 2024-11-25 checkpoint as shown in the images.&lt;br /&gt; Could anybody please double check to see if I made any mistakes?&lt;/p&gt; &lt;p&gt;EDIT: could be the influence of the private 30% of tests. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jkhlk6/comment/mjvqooj/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jkhlk6/comment/mjvqooj/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zjuwyz"&gt; /u/zjuwyz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkhlk6/mismatch_between_official_deepseekv31_livebench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkhlk6/mismatch_between_official_deepseekv31_livebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkhlk6/mismatch_between_official_deepseekv31_livebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T17:36:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk96ei</id>
    <title>Ling: A new MoE model series - including Ling-lite, Ling-plus and Ling-Coder-lite. Instruct + Base models available. MIT License.</title>
    <updated>2025-03-26T11:06:27+00:00</updated>
    <author>
      <name>/u/Many_SuchCases</name>
      <uri>https://old.reddit.com/user/Many_SuchCases</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk96ei/ling_a_new_moe_model_series_including_linglite/"&gt; &lt;img alt="Ling: A new MoE model series - including Ling-lite, Ling-plus and Ling-Coder-lite. Instruct + Base models available. MIT License." src="https://external-preview.redd.it/to4DeErJUd6nuRuuLqu9lyq844P83bUoZfIxhOq-ba0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=89c5f9dbe9205ff855bef1629d1286873f325f34" title="Ling: A new MoE model series - including Ling-lite, Ling-plus and Ling-Coder-lite. Instruct + Base models available. MIT License." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Ling Lite and Ling Plus:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ling is a MoE LLM provided and open-sourced by InclusionAI. We introduce two different sizes, which are Ling-Lite and Ling-Plus. Ling-Lite has 16.8 billion parameters with 2.75 billion activated parameters, while Ling-Plus has 290 billion parameters with 28.8 billion activated parameters. Both models demonstrate impressive performance compared to existing models in the industry.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ling Coder Lite:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ling-Coder-Lite is a MoE LLM provided and open-sourced by InclusionAI, which has 16.8 billion parameters with 2.75 billion activated parameters. Ling-Coder-Lite performs impressively on coding tasks compared to existing models in the industry. Specifically, Ling-Coder-Lite further pre-training from an intermediate checkpoint of Ling-Lite, incorporating an additional 3 trillion tokens. This extended pre-training significantly boosts the coding abilities of Ling-Lite, while preserving its strong performance in general language tasks. More details are described in the technique report &lt;a href="https://huggingface.co/papers/2503.17793"&gt;Ling-Coder-TR&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hugging Face:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/inclusionAI/ling-67c51c85b34a7ea0aba94c32"&gt;https://huggingface.co/collections/inclusionAI/ling-67c51c85b34a7ea0aba94c32&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2503.05139"&gt;https://arxiv.org/abs/2503.05139&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/inclusionAI/Ling"&gt;https://github.com/inclusionAI/Ling&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note 1:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I would really recommend reading the paper, there's a section called &amp;quot;Bitter Lessons&amp;quot; which covers some of the problems someone might run into making models from scratch. It was insightful to read.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note 2:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I am not affiliated.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some benchmarks (more in the paper):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ling-Lite:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dbqo9n1in0re1.png?width=1661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0bb829ad58a67e12305675b519dfd6cca8354d6"&gt;https://preview.redd.it/dbqo9n1in0re1.png?width=1661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0bb829ad58a67e12305675b519dfd6cca8354d6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ling-Plus:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rexwjtuin0re1.png?width=1661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01cc607e77117c025817add398ddc5329337275c"&gt;https://preview.redd.it/rexwjtuin0re1.png?width=1661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01cc607e77117c025817add398ddc5329337275c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ling-Coder-Lite:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eka1kg0fp0re1.png?width=1661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d069b0069e96400737104c065c3851628543f8b4"&gt;https://preview.redd.it/eka1kg0fp0re1.png?width=1661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d069b0069e96400737104c065c3851628543f8b4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Many_SuchCases"&gt; /u/Many_SuchCases &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk96ei/ling_a_new_moe_model_series_including_linglite/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk96ei/ling_a_new_moe_model_series_including_linglite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk96ei/ling_a_new_moe_model_series_including_linglite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T11:06:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkejpp</id>
    <title>gemini-2.5-pro-exp-03-25 takes no.1 spot on Livebench</title>
    <updated>2025-03-26T15:31:37+00:00</updated>
    <author>
      <name>/u/windxp1</name>
      <uri>https://old.reddit.com/user/windxp1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkejpp/gemini25proexp0325_takes_no1_spot_on_livebench/"&gt; &lt;img alt="gemini-2.5-pro-exp-03-25 takes no.1 spot on Livebench" src="https://a.thumbs.redditmedia.com/DVJOFbOiCP1HhpPBeXJU6UGPZGm6qOHJUghrP3Mvsa4.jpg" title="gemini-2.5-pro-exp-03-25 takes no.1 spot on Livebench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/p8isc1phy1re1.png?width=1018&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=422f9256def988fe4fee25d3a2e5d6c78be45874"&gt;https://preview.redd.it/p8isc1phy1re1.png?width=1018&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=422f9256def988fe4fee25d3a2e5d6c78be45874&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Its free on aistudio with 50 req/day&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/windxp1"&gt; /u/windxp1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkejpp/gemini25proexp0325_takes_no1_spot_on_livebench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkejpp/gemini25proexp0325_takes_no1_spot_on_livebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkejpp/gemini25proexp0325_takes_no1_spot_on_livebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T15:31:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkl8ul</id>
    <title>Megastructure made by new gemini 2.5 Pro one shot</title>
    <updated>2025-03-26T20:05:32+00:00</updated>
    <author>
      <name>/u/KillyOnTerra</name>
      <uri>https://old.reddit.com/user/KillyOnTerra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkl8ul/megastructure_made_by_new_gemini_25_pro_one_shot/"&gt; &lt;img alt="Megastructure made by new gemini 2.5 Pro one shot" src="https://external-preview.redd.it/Y3BoMjhvYnliM3JlMXCwHJYcCFS2at4U0624bh9xAyv9yXDKiCS8t1hTdo0t.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e0f520cb053129adb6695eed4de9b9adaf60706" title="Megastructure made by new gemini 2.5 Pro one shot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see alot of people testing ai with 2D games but I wanted to see how it handles 3D. &lt;/p&gt; &lt;p&gt;Prompt: make an enormous megastructure in unity using c# make it complex and interesting. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KillyOnTerra"&gt; /u/KillyOnTerra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ej6ukleyb3re1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkl8ul/megastructure_made_by_new_gemini_25_pro_one_shot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkl8ul/megastructure_made_by_new_gemini_25_pro_one_shot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T20:05:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jke5e5</id>
    <title>V3.1 on livebench</title>
    <updated>2025-03-26T15:14:31+00:00</updated>
    <author>
      <name>/u/nknnr</name>
      <uri>https://old.reddit.com/user/nknnr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jke5e5/v31_on_livebench/"&gt; &lt;img alt="V3.1 on livebench" src="https://preview.redd.it/u6go7pw0w1re1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e8ce1c39f759069e5c705eb541c28ff5d91aee6" title="V3.1 on livebench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nknnr"&gt; /u/nknnr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u6go7pw0w1re1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jke5e5/v31_on_livebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jke5e5/v31_on_livebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T15:14:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkcd5l</id>
    <title>üò≤ DeepSeek-V3-4bit &gt;20tk/s, &lt;200w on M3 Ultra 512GB, MLX</title>
    <updated>2025-03-26T13:56:26+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This might be the best and most user-friendly way to run DeepSeek-V3 on consumer hardware, possibly the most affordable too.&lt;/p&gt; &lt;p&gt;It sounds like you can finally run a GPT-4o level model locally at home, possibly with even better quality.&lt;/p&gt; &lt;p&gt;&lt;a href="https://venturebeat.com/ai/deepseek-v3-now-runs-at-20-tokens-per-second-on-mac-studio-and-thats-a-nightmare-for-openai/"&gt;https://venturebeat.com/ai/deepseek-v3-now-runs-at-20-tokens-per-second-on-mac-studio-and-thats-a-nightmare-for-openai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Update:&lt;/p&gt; &lt;p&gt;I'm not sure if there's difference between v3 and r1, but here's a result with 13k context from &lt;a href="/u/ifioravanti"&gt;/u/ifioravanti&lt;/a&gt; with DeepSeek R1 671B 4bit using MLX.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;- Prompt: 13140 tokens, 59.562 tokens-per-sec - Generation: 720 tokens, 6.385 tokens-per-sec - Peak memory: 491.054 GB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j9vjf1/deepseek_r1_671b_q4_m3_ultra_512gb_with_mlx/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1j9vjf1/deepseek_r1_671b_q4_m3_ultra_512gb_with_mlx/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That's about 3.5 minutes of prompt processing 13k tokens. Your subsequent chat will go faster with prompt caching. Obviously it depends on your usage and speed tolerance, but 6.385tk/s is not too bad IMO.&lt;/p&gt; &lt;p&gt;You can purchase it on a monthly plan, with $1,531.10 upfront payment, test it for 14 days, and get a refund if you're not happy. lol&lt;/p&gt; &lt;p&gt;In 2020, if someone had said that within five years, a $10k computer could look at a simple text instruction and generate fully runnable code for a basic arcade game in just minutes at home, no one would have believed it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkcd5l/deepseekv34bit_20tks_200w_on_m3_ultra_512gb_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkcd5l/deepseekv34bit_20tks_200w_on_m3_ultra_512gb_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkcd5l/deepseekv34bit_20tks_200w_on_m3_ultra_512gb_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T13:56:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkkqs3</id>
    <title>Free Search: Making Search Free 4 All</title>
    <updated>2025-03-26T19:45:09+00:00</updated>
    <author>
      <name>/u/Far-Celebration-470</name>
      <uri>https://old.reddit.com/user/Far-Celebration-470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üëã Hi all! &lt;/p&gt; &lt;p&gt;For any AI agent, internet search üîé is an important tool. However, with APIs like Tavily and Exa, it becomes really difficult to keep up with the cost. In some cases, these Internet APIs cost more than the LLM. &lt;/p&gt; &lt;p&gt;To solve, this, I am making a playwright wrapper API on top of publicly available searXNG instances. This will enable agent applications to fetch internet results for free. &lt;/p&gt; &lt;p&gt;Currently, I have set up a basic GitHub repo, and I will continue developing advanced search features, such as image search üñºÔ∏è &lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/HanzlaJavaid/Free-Search/tree/main"&gt;https://github.com/HanzlaJavaid/Free-Search/tree/main&lt;/a&gt; &lt;/p&gt; &lt;p&gt;üöÄ Try the deployed version: &lt;a href="https://freesearch.replit.app/docs"&gt;https://freesearch.replit.app/docs&lt;/a&gt; &lt;/p&gt; &lt;p&gt;If you find this useful, consider starring ‚≠êÔ∏è the GitHub repository to support further development!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Celebration-470"&gt; /u/Far-Celebration-470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkkqs3/free_search_making_search_free_4_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkkqs3/free_search_making_search_free_4_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkkqs3/free_search_making_search_free_4_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T19:45:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk7cka</id>
    <title>Plenty 3090 FE's for sale in the Netherlands</title>
    <updated>2025-03-26T08:56:31+00:00</updated>
    <author>
      <name>/u/jwestra</name>
      <uri>https://old.reddit.com/user/jwestra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk7cka/plenty_3090_fes_for_sale_in_the_netherlands/"&gt; &lt;img alt="Plenty 3090 FE's for sale in the Netherlands" src="https://preview.redd.it/3bxpnick00re1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7295d7a24ea4033f0e4d96d4cdbf0b662770a23a" title="Plenty 3090 FE's for sale in the Netherlands" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jwestra"&gt; /u/jwestra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3bxpnick00re1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk7cka/plenty_3090_fes_for_sale_in_the_netherlands/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk7cka/plenty_3090_fes_for_sale_in_the_netherlands/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T08:56:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkbh4f</id>
    <title>Google releases TxGemma, open models for therapeutic applications</title>
    <updated>2025-03-26T13:13:38+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkbh4f/google_releases_txgemma_open_models_for/"&gt; &lt;img alt="Google releases TxGemma, open models for therapeutic applications" src="https://external-preview.redd.it/w4gdQx4Lq4f5EYpFmMZH_AWSFA12WrreFd38e_AppFM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4444c30b578119b10e027ae74a8e53550f6800b" title="Google releases TxGemma, open models for therapeutic applications" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! We're excited to share TxGemma! &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gemma 2-based model for multiple therapeutic tasks &lt;ul&gt; &lt;li&gt;Classification (will molecule cross blood-brain barrier)&lt;/li&gt; &lt;li&gt;Regression (drug's binding affinity)&lt;/li&gt; &lt;li&gt;Generation (given product of some reaction, generate reactant set)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;2B, 9B, and 27B, with 27B being SOTA for many tasks, including versus single-task models&lt;/li&gt; &lt;li&gt;Chat version for general reasoning, to answer questions and engage in discussions&lt;/li&gt; &lt;li&gt;Fine-tunable with transformers, with an example notebook&lt;/li&gt; &lt;li&gt;Agentic-Tx for agentic systems, powered with Gemini, and using TxGemma as a tool&lt;/li&gt; &lt;li&gt;Models on HF: &lt;a href="https://huggingface.co/collections/google/txgemma-release-67dd92e931c857d15e4d1e87"&gt;https://huggingface.co/collections/google/txgemma-release-67dd92e931c857d15e4d1e87&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/introducing-txgemma-open-models-improving-therapeutics-development/?linkId=13647386"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkbh4f/google_releases_txgemma_open_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkbh4f/google_releases_txgemma_open_models_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T13:13:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkl761</id>
    <title>OpenAI announces that they are adopting MCP</title>
    <updated>2025-03-26T20:03:37+00:00</updated>
    <author>
      <name>/u/MeltingHippos</name>
      <uri>https://old.reddit.com/user/MeltingHippos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI has announced support for MCP in the Agents SDK and said they will soon add support to their desktop app and to their Responses API!&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/OpenAIDevs/status/1904957755829481737"&gt;https://x.com/OpenAIDevs/status/1904957755829481737&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MeltingHippos"&gt; /u/MeltingHippos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkl761/openai_announces_that_they_are_adopting_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkl761/openai_announces_that_they_are_adopting_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkl761/openai_announces_that_they_are_adopting_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T20:03:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkgv2f</id>
    <title>Qwen releases Qwen/Qwen2.5-Omni-7B</title>
    <updated>2025-03-26T17:07:13+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgv2f/qwen_releases_qwenqwen25omni7b/"&gt; &lt;img alt="Qwen releases Qwen/Qwen2.5-Omni-7B" src="https://external-preview.redd.it/8SmAxGhIQPYbKQ360sskPqKhJl5vPSWEfB2CyOiyRq8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be40495e2b1d57173ebf46c043544693d2bbcf52" title="Qwen releases Qwen/Qwen2.5-Omni-7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgv2f/qwen_releases_qwenqwen25omni7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgv2f/qwen_releases_qwenqwen25omni7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T17:07:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jke5wg</id>
    <title>M3 Ultra Mac Studio 512GB prompt and write speeds for Deepseek V3 671b gguf q4_K_M, for those curious</title>
    <updated>2025-03-26T15:15:08+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For anyone curious, here's the gguf numbers for Deepseek V3 q4_K_M (the older V3, not the newest one from this week). I loaded it up last night and tested some prompts:&lt;/p&gt; &lt;p&gt;M3 Ultra Mac Studio 512GB Deepseek V3 671b q4_K_M gguf without Flash Attention&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:8102/16384, Amt:902/4000, Init:0.04s, Process:792.65s (9.05T/s), Generate:146.21s (6.17T/s), Total:938.86s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note above: normally I run in debugmode to get the ms per token, but forgot to enable it this time. Comes out to &lt;strong&gt;about 110ms per token for prompt processing&lt;/strong&gt;, and &lt;strong&gt;about 162ms per token for prompt response&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;M3 Ultra Mac Studio 512GB Deepseek V3 671b q4_K_M gguf with Flash Attention On&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:7847/16384, Amt:647/4000, Init:0.04s, Process:793.14s (110.2ms/T = 9.08T/s), Generate:103.81s (160.5ms/T = 6.23T/s), Total:896.95s (0.72T/s) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In comparison, here is Llama 3.3 70b q8 with Flash Attention On&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:6293/16384, Amt:222/800, Init:0.07s, Process:41.22s (8.2ms/T = 121.79T/s), Generate:35.71s (160.8ms/T = 6.22T/s), Total:76.92s (2.89T/s &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jke5wg/m3_ultra_mac_studio_512gb_prompt_and_write_speeds/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jke5wg/m3_ultra_mac_studio_512gb_prompt_and_write_speeds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jke5wg/m3_ultra_mac_studio_512gb_prompt_and_write_speeds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T15:15:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkd8ik</id>
    <title>Notes on Deepseek v3 0324: Finally, the Sonnet 3.5 at home!</title>
    <updated>2025-03-26T14:35:21+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I believe we finally have the Claude 3.5 Sonnet at home. &lt;/p&gt; &lt;p&gt;With a release that was very Deepseek-like, the Whale bros released an updated Deepseek v3 with a significant boost in reasoning abilities. &lt;/p&gt; &lt;p&gt;This time, it's a proper MIT license, unlike the original model with a custom license, a 641GB, 685b model. With a knowledge cut-off date of July'24.&lt;br /&gt; But the significant difference is a massive boost in reasoning abilities. It's a base model, but the responses are similar to how a CoT model will think. And I believe RL with GRPO has a lot to do with it.&lt;/p&gt; &lt;p&gt;The OG model matched GPT-4o, and with this upgrade, it's on par with Claude 3.5 Sonnet; though you still may find Claude to be better at some edge cases, the gap is negligible.&lt;/p&gt; &lt;p&gt;To know how good it is compared to Claude Sonnets, I ran a few prompts, &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here are some observations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Deepseek v3 0324 understands &lt;strong&gt;user intention better&lt;/strong&gt; than before; I'd say it's better than Claude 3.7 Sonnet base and thinking. 3.5 is still better at this (perhaps the best)&lt;/li&gt; &lt;li&gt;Again, in raw quality &lt;strong&gt;code generation&lt;/strong&gt;, it is better than 3.7, on par with 3.5, and sometimes better.&lt;/li&gt; &lt;li&gt;Great at &lt;strong&gt;reasoning&lt;/strong&gt;, much better than any and all non-reasoning models available right now.&lt;/li&gt; &lt;li&gt;Better at the &lt;strong&gt;instruction following&lt;/strong&gt; than 3,7 Sonnet but below 3.5 Sonnet.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For raw capability in real-world tasks, 3.5 &amp;gt;= v3 &amp;gt; 3.7&lt;/p&gt; &lt;p&gt;For a complete analysis and commentary, check out this blog post: &lt;a href="https://composio.dev/blog/deepseek-v3-0324-the-sonnet-3-5-at-home/"&gt;Deepseek v3 0324: The Sonnet 3.5 at home&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's crazy that there's no similar hype as the OG release for such a massive upgrade. They missed naming it v3.5, or else it would've wiped another bunch of billions from the market. It might be the time Deepseek hires good marketing folks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I‚Äôd love to hear about your experience with the new DeepSeek-V3 (0324). How do you like it, and how would you compare it to Claude 3.5 Sonnet?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkd8ik/notes_on_deepseek_v3_0324_finally_the_sonnet_35/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkd8ik/notes_on_deepseek_v3_0324_finally_the_sonnet_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkd8ik/notes_on_deepseek_v3_0324_finally_the_sonnet_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T14:35:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkix1t</id>
    <title>China may effectively ban at least some Nvidia GPUs. What will Nvidia do with all those GPUs if they can't sell them in China?</title>
    <updated>2025-03-26T18:30:23+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nvidia has made cut down versions of Nvidia GPUs for China that duck under the US export restrictions to China. But it looks like China may effectively ban those Nvidia GPUs in China because they are so power hungry. They violate China's green laws. That's a pretty big market for Nvidia. What will Nvidia do with all those GPUs if they can't sell the in China?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.investopedia.com/beijing-enforcement-of-energy-rules-could-hit-nvidia-china-business-report-says-11703513"&gt;https://www.investopedia.com/beijing-enforcement-of-energy-rules-could-hit-nvidia-china-business-report-says-11703513&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkix1t/china_may_effectively_ban_at_least_some_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkix1t/china_may_effectively_ban_at_least_some_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkix1t/china_may_effectively_ban_at_least_some_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T18:30:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkgvxn</id>
    <title>Qwen 2.5 Omni 7B is out</title>
    <updated>2025-03-26T17:08:12+00:00</updated>
    <author>
      <name>/u/Lowkey_LokiSN</name>
      <uri>https://old.reddit.com/user/Lowkey_LokiSN</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgvxn/qwen_25_omni_7b_is_out/"&gt; &lt;img alt="Qwen 2.5 Omni 7B is out" src="https://external-preview.redd.it/8SmAxGhIQPYbKQ360sskPqKhJl5vPSWEfB2CyOiyRq8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be40495e2b1d57173ebf46c043544693d2bbcf52" title="Qwen 2.5 Omni 7B is out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/huvknotdh2re1.png?width=1182&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=866c6cca9237016d2756c3b36b573cb2e3a92172"&gt;https://preview.redd.it/huvknotdh2re1.png?width=1182&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=866c6cca9237016d2756c3b36b573cb2e3a92172&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF link: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B"&gt;https://huggingface.co/Qwen/Qwen2.5-Omni-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Tweet seems to have been deleted so attached image&lt;br /&gt; Edit #2: Reposted tweet: &lt;a href="https://x.com/Alibaba_Qwen/status/1904944923159445914"&gt;https://x.com/Alibaba_Qwen/status/1904944923159445914&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lowkey_LokiSN"&gt; /u/Lowkey_LokiSN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgvxn/qwen_25_omni_7b_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgvxn/qwen_25_omni_7b_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgvxn/qwen_25_omni_7b_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T17:08:12+00:00</published>
  </entry>
</feed>
