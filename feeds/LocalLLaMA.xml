<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-13T15:35:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jy8zya</id>
    <title>Using AI help to write book</title>
    <updated>2025-04-13T14:32:49+00:00</updated>
    <author>
      <name>/u/Hjemmelegen</name>
      <uri>https://old.reddit.com/user/Hjemmelegen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im working on a book, and considering using AI to help with expanding it some. Anybody experience with it? Is for example Claude and Gemini 2.5 good enough to actually help expand chapters in a science fiction books? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hjemmelegen"&gt; /u/Hjemmelegen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8zya/using_ai_help_to_write_book/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8zya/using_ai_help_to_write_book/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8zya/using_ai_help_to_write_book/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T14:32:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jxo7lb</id>
    <title>llama.cpp got 2 fixes for Llama 4 (RoPE &amp; wrong norms)</title>
    <updated>2025-04-12T18:40:48+00:00</updated>
    <author>
      <name>/u/jubilantcoffin</name>
      <uri>https://old.reddit.com/user/jubilantcoffin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12889"&gt;https://github.com/ggml-org/llama.cpp/pull/12889&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12882"&gt;https://github.com/ggml-org/llama.cpp/pull/12882&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No idea what this does to performance. If I understand correctly, the RoPE fix is in the GGUF conversion so all models will have to be redownloaded.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jubilantcoffin"&gt; /u/jubilantcoffin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxo7lb/llamacpp_got_2_fixes_for_llama_4_rope_wrong_norms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxo7lb/llamacpp_got_2_fixes_for_llama_4_rope_wrong_norms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jxo7lb/llamacpp_got_2_fixes_for_llama_4_rope_wrong_norms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-12T18:40:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jxohy4</id>
    <title>Intel A.I. ask me anything (AMA)</title>
    <updated>2025-04-12T18:53:53+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I asked if we can get a 64 GB GPU card:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/user/IntelBusiness/comments/1juqi3c/comment/mmndtk8/?context=3"&gt;https://www.reddit.com/user/IntelBusiness/comments/1juqi3c/comment/mmndtk8/?context=3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;AMA title:&lt;/p&gt; &lt;p&gt;Hi Reddit, I'm Melissa Evers (VP Office of the CTO) at Intel. Ask me anything about AI including building, innovating, the role of an open source ecosystem and more on 4/16 at 10a PDT.&lt;/p&gt; &lt;p&gt;Update: This is an advert for an AMA on Tuesday.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxohy4/intel_ai_ask_me_anything_ama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxohy4/intel_ai_ask_me_anything_ama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jxohy4/intel_ai_ask_me_anything_ama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-12T18:53:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy33fn</id>
    <title>What's the cheapest way to host a model on a server?</title>
    <updated>2025-04-13T08:32:45+00:00</updated>
    <author>
      <name>/u/ThaisaGuilford</name>
      <uri>https://old.reddit.com/user/ThaisaGuilford</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For context: currently I'm using huggingface API to access Qwen 2.5 Model for a customized customer chat experience. It works fine for me as we don't have many visitors chatting at the same time.&lt;/p&gt; &lt;p&gt;I can do it practically free of charge.&lt;/p&gt; &lt;p&gt;I was wondering if this is the best I can do.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThaisaGuilford"&gt; /u/ThaisaGuilford &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy33fn/whats_the_cheapest_way_to_host_a_model_on_a_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy33fn/whats_the_cheapest_way_to_host_a_model_on_a_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy33fn/whats_the_cheapest_way_to_host_a_model_on_a_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T08:32:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy4xks</id>
    <title>Query on distributed speculative decoding using llama.cpp.</title>
    <updated>2025-04-13T10:47:38+00:00</updated>
    <author>
      <name>/u/ekaknr</name>
      <uri>https://old.reddit.com/user/ekaknr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've asked this &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/12928"&gt;question on llama.cpp discussions forum&lt;/a&gt; on Github. A related discussion, which I couldn't quite understand &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/6853"&gt;happened earlier&lt;/a&gt;. Hoping to find an answer soon, so am posting the same question here:&lt;br /&gt; I've got two mac mins - one with 16GB RAM (M2 Pro), and the other with 8GB RAM (M2). Now, I was wondering if I can leverage the power of speculative decoding to speed up inference performance of a main model (like a Qwen2.5-Coder-14B 4bits quantized GGUF) on the M2 Pro mac, while having the draft model (like a Qwen2.5-Coder-0.5B 8bits quantized GGUF) running via the M2 mac. Is this feasible, perhaps using &lt;code&gt;rpc-server&lt;/code&gt;? Can someone who's done something like this help me out please? Also, if this is possible, is it scalable even further (I have an old desktop with an RTX 2060).&lt;/p&gt; &lt;p&gt;I'm open to any suggestions on achieveing this using MLX or similar frameworks. Exo or rpc-server's distributed capabilities are not what I'm looking for here (those run the models quite slow anyway, and I'm looking for speed).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ekaknr"&gt; /u/ekaknr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy4xks/query_on_distributed_speculative_decoding_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy4xks/query_on_distributed_speculative_decoding_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy4xks/query_on_distributed_speculative_decoding_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T10:47:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy926u</id>
    <title>Local AI Scheduling Assistant/Project management</title>
    <updated>2025-04-13T14:35:40+00:00</updated>
    <author>
      <name>/u/wallstreetiscasino</name>
      <uri>https://old.reddit.com/user/wallstreetiscasino</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all,&lt;/p&gt; &lt;p&gt;I am currently looking for a tool to help me organize my company and help me to schedule tasks. I have a small team that I need to delegate tasks to, as well as scheduling calls and meetings for myself. Looking into apps like Monday, Motion, Reclaim and scheduler AI, however, if I can do it locally for free that would be Ideal. I do have a machine running oLlama, but I have very basic knowledge on it thus far. Does anyone out there currently use something like this?&lt;/p&gt; &lt;p&gt;Thanks in advance for your input! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wallstreetiscasino"&gt; /u/wallstreetiscasino &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy926u/local_ai_scheduling_assistantproject_management/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy926u/local_ai_scheduling_assistantproject_management/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy926u/local_ai_scheduling_assistantproject_management/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T14:35:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jxqwnh</id>
    <title>Dot - Draft Of Thought workflow for local LLMs</title>
    <updated>2025-04-12T20:42:58+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxqwnh/dot_draft_of_thought_workflow_for_local_llms/"&gt; &lt;img alt="Dot - Draft Of Thought workflow for local LLMs" src="https://external-preview.redd.it/NGZjdngwNG50Z3VlMQspLSWSm-3hpJbmhl6PTxl5UJ1U1d3Jla2YPW084JJG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2bc1e7b7ff0c69a99a97ef2c9a6121cdbbc36f10" title="Dot - Draft Of Thought workflow for local LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What is this?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A workflow inspired by the &lt;a href="https://arxiv.org/abs/2502.18600"&gt;Chain of Draft&lt;/a&gt; paper. Here, LLM produces a high level skeleton for reasoning first and then fills it step-by-step while referring to the previous step outputs. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6rh5363ntgue1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxqwnh/dot_draft_of_thought_workflow_for_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jxqwnh/dot_draft_of_thought_workflow_for_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-12T20:42:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jxiia5</id>
    <title>Next on your rig: Google Gemini PRO 2.5 as Google Open to let entreprises self host models</title>
    <updated>2025-04-12T14:26:30+00:00</updated>
    <author>
      <name>/u/coding_workflow</name>
      <uri>https://old.reddit.com/user/coding_workflow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From a major player, this sounds like a big shift and would mostly offer enterprises an interesting perspective on data privacy. Mistral is already doing this a lot while OpenAI and Anthropic maintain more closed offerings or through partners.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.cnbc.com/2025/04/09/google-will-let-companies-run-gemini-models-in-their-own-data-centers.html"&gt;https://www.cnbc.com/2025/04/09/google-will-let-companies-run-gemini-models-in-their-own-data-centers.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: fix typo&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coding_workflow"&gt; /u/coding_workflow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxiia5/next_on_your_rig_google_gemini_pro_25_as_google/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxiia5/next_on_your_rig_google_gemini_pro_25_as_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jxiia5/next_on_your_rig_google_gemini_pro_25_as_google/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-12T14:26:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jxwk05</id>
    <title>Intel 6944P the most cost effective CPU solution for llm</title>
    <updated>2025-04-13T01:29:18+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;at $13k for 330t/s prompt processing and 17.46t/s inference.&lt;/p&gt; &lt;p&gt;ktransformer says for Intel CPUs with AMX instructions (2x6454S) can get 195.62t/s prompt processing and 8.73t/s inference for DeepSeek R1.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md"&gt;https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;2x6454S = 2*32*2.2GHz = 70.4GHz. 6944P = 72*1.8GHz = 129.6GHz. That means 6944P can get to 330t/s prompt processing.&lt;/p&gt; &lt;p&gt;1x6454S supports 8xDDR5-4800 =&amp;gt; 307.2GB/s. 1x6944P supports 12xDDR5-6400 =&amp;gt; 614.4GB/s. So inference is expected to double at 17.46t/s&lt;/p&gt; &lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Granite_Rapids"&gt;https://en.wikipedia.org/wiki/Granite_Rapids&lt;/a&gt;&lt;/p&gt; &lt;p&gt;6944P CPU is $6850. 12xMicron DDR5-6400 64GB is $4620. So a full system should be around $13k.&lt;/p&gt; &lt;p&gt;Prompt processing of 330t/s is quite close to the 2x3090's 393t/s for llama 70b Q4_K_M and triple the performance of M2 Ultra.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/XiongjieDai/GPU-Benchmarks-on-LLM-Inference"&gt;https://github.com/XiongjieDai/GPU-Benchmarks-on-LLM-Inference&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxwk05/intel_6944p_the_most_cost_effective_cpu_solution/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxwk05/intel_6944p_the_most_cost_effective_cpu_solution/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jxwk05/intel_6944p_the_most_cost_effective_cpu_solution/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T01:29:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jxe6al</id>
    <title>Droidrun: Enable Ai Agents to control Android</title>
    <updated>2025-04-12T10:21:58+00:00</updated>
    <author>
      <name>/u/Sleyn7</name>
      <uri>https://old.reddit.com/user/Sleyn7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxe6al/droidrun_enable_ai_agents_to_control_android/"&gt; &lt;img alt="Droidrun: Enable Ai Agents to control Android" src="https://external-preview.redd.it/dHg4MzZlNmNyZHVlMUHUE0oWEIK87Cyw4Z52vKlJ71Jnb-eBsoVMPDCEfoF3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=feabb957432f963c9ee0084379bb67872517a6c7" title="Droidrun: Enable Ai Agents to control Android" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been working on a project called DroidRun, which gives your AI agent the ability to control your phone, just like a human would. Think of it as giving your LLM-powered assistant real hands-on access to your Android device. You can connect any LLM to it.&lt;/p&gt; &lt;p&gt;I just made a video that shows how it works. It‚Äôs still early, but the results are super promising.&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts, feedback, or ideas on what you'd want to automate!&lt;/p&gt; &lt;p&gt;&lt;a href="http://www.droidrun.ai"&gt;www.droidrun.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sleyn7"&gt; /u/Sleyn7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/61xh0p4crdue1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxe6al/droidrun_enable_ai_agents_to_control_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jxe6al/droidrun_enable_ai_agents_to_control_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-12T10:21:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy3qh8</id>
    <title>Research tip</title>
    <updated>2025-04-13T09:20:40+00:00</updated>
    <author>
      <name>/u/danja</name>
      <uri>https://old.reddit.com/user/danja</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy3qh8/research_tip/"&gt; &lt;img alt="Research tip" src="https://preview.redd.it/95wwmofalkue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eac3dd47e72cb031c969b40fa441e7da880cd0f1" title="Research tip" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...for the s/lazy/time-constrained.&lt;/p&gt; &lt;p&gt;Yesterday I wanted to catch up on recent work in a particular niche. It was also time to take Claudio for his walk. I hit upon this easy procedure :&lt;/p&gt; &lt;ol&gt; &lt;li&gt;ask Perplexity [1], set on &amp;quot;Deep Research&amp;quot;, to look into what I wanted&lt;/li&gt; &lt;li&gt;export its response as markdown&lt;/li&gt; &lt;li&gt;lightly skim the text, find the most relevant papers linked, download these&lt;/li&gt; &lt;li&gt;create a new project on Notebook LM [2], upload those papers, give it any extra prompting required, plus the full markdown text&lt;/li&gt; &lt;li&gt;in the Studio tab, ask it to render a Chat (it's worth setting the style prompt there, eg. tell it the listener knows the basics, otherwise you get a lot of inconsequential, typical podcast, fluff)&lt;/li&gt; &lt;li&gt;take Mr. Dog out&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You get 3 free goes daily with Perplexity set to max. I haven't hit any paywalls on Notebook LM yet.&lt;/p&gt; &lt;p&gt;btw, if you have any multi-agent workflows like this, I'd love to hear them. My own mini-framework is now at the stage where I need to consider such scenarios/use cases. It's not yet ready to implement them in a useful fashion, but it's getting there, piano piano...&lt;/p&gt; &lt;p&gt;[1] &lt;a href="https://www.perplexity.ai/"&gt;https://www.perplexity.ai/&lt;/a&gt; [2] &lt;a href="https://notebooklm.google.com/"&gt;https://notebooklm.google.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danja"&gt; /u/danja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/95wwmofalkue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy3qh8/research_tip/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy3qh8/research_tip/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T09:20:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jxn8x7</id>
    <title>What if you could run 50+ LLMs per GPU ‚Äî without keeping them in memory?</title>
    <updated>2025-04-12T17:58:12+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôve been experimenting with an AI-native runtime that snapshot-loads LLMs (13B‚Äì65B) in 2‚Äì5 seconds and dynamically runs 50+ models per GPU without keeping them always resident in memory.&lt;/p&gt; &lt;p&gt;Instead of preloading models (like in vLLM or Triton), we serialize GPU execution state + memory buffers, and restore models on demand even in shared GPU environments where full device access isn‚Äôt available.&lt;/p&gt; &lt;p&gt;This seems to unlock: ‚Ä¢Real serverless LLM behavior (no idle GPU cost)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢Multi-model orchestration at low latency ‚Ä¢Better GPU utilization for agentic or dynamic workflows &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Curious if others here are exploring similar ideas especially with: ‚Ä¢Multi-model/agent stacks&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢Dynamic GPU memory management (MIG, KAI Scheduler, etc.) ‚Ä¢Cuda-checkpoint / partial device access challenges &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Happy to share more technical details if helpful. Would love to exchange notes or hear what pain points you‚Äôre seeing with current model serving infra!&lt;/p&gt; &lt;p&gt;P.S. Sharing more on X: @InferXai . follow if you‚Äôre into local inference, GPU orchestration, and memory tricks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxn8x7/what_if_you_could_run_50_llms_per_gpu_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxn8x7/what_if_you_could_run_50_llms_per_gpu_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jxn8x7/what_if_you_could_run_50_llms_per_gpu_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-12T17:58:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy41fx</id>
    <title>Fast, Zero-Bloat LLM CLI with Streaming, History, and Template Support ‚Äî Written in Perl</title>
    <updated>2025-04-13T09:43:23+00:00</updated>
    <author>
      <name>/u/jaggzh</name>
      <uri>https://old.reddit.com/user/jaggzh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy41fx/fast_zerobloat_llm_cli_with_streaming_history_and/"&gt; &lt;img alt="Fast, Zero-Bloat LLM CLI with Streaming, History, and Template Support ‚Äî Written in Perl" src="https://external-preview.redd.it/8J_6sy0IBFKR1WJCX4P9PWZrs5zCr49p_lmJXb6Ggkc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a416bd94be3aa8839c93426a682d5f978204cd4b" title="Fast, Zero-Bloat LLM CLI with Streaming, History, and Template Support ‚Äî Written in Perl" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/jaggzh/z"&gt;https://github.com/jaggzh/z&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been working on this, and using it, for over a year..&lt;/p&gt; &lt;p&gt;A local LLM CLI interface that‚Äôs super fast, and is usable for ultra-convenient command-line use, OR incorporating into pipe workflows or scripts.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qo3m0q9r2mue1.jpg?width=1142&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7b6e8ffd4d6982c08d616f6faa2bc0337b33f87b"&gt;https://preview.redd.it/qo3m0q9r2mue1.jpg?width=1142&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7b6e8ffd4d6982c08d616f6faa2bc0337b33f87b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's super-minimal, while providing tons of [optional] power.&lt;/p&gt; &lt;p&gt;My tests show python calls have way too much overhead, dependency issues, etc. Perl is blazingly-fast (see my benchmarks) -- many times faster than python.&lt;/p&gt; &lt;p&gt;I currently have only used it with its API calls to llama.cpp's llama-server.&lt;/p&gt; &lt;p&gt;‚úÖ Bash-style &amp;quot;REPL&amp;quot; usability (ChatGPT suggested I say this)&lt;/p&gt; &lt;p&gt;‚úÖ Configurable prompt templates&lt;/p&gt; &lt;p&gt;‚úÖ Auto history, context, and system prompts&lt;/p&gt; &lt;p&gt;‚úÖ Great for scripting or just chatting&lt;/p&gt; &lt;p&gt;‚úÖ Streaming &amp;amp; chain-of-thought toggling (--think)&lt;/p&gt; &lt;p&gt;Perl's dependencies are also very stable, and small, and fast.&lt;/p&gt; &lt;p&gt;It makes your llm use &amp;quot;close&amp;quot;, &amp;quot;native&amp;quot;, and convenient.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jaggzh/z"&gt;https://github.com/jaggzh/z&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jaggzh"&gt; /u/jaggzh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy41fx/fast_zerobloat_llm_cli_with_streaming_history_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy41fx/fast_zerobloat_llm_cli_with_streaming_history_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy41fx/fast_zerobloat_llm_cli_with_streaming_history_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T09:43:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy9g9y</id>
    <title>Waifu GPU for AI GF?</title>
    <updated>2025-04-13T14:53:23+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy9g9y/waifu_gpu_for_ai_gf/"&gt; &lt;img alt="Waifu GPU for AI GF?" src="https://external-preview.redd.it/BnNrd2081VNoP-o3smbLrBMM4J_6XEXvavemnoJv_qM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea747f555a0c92c80b42a36a37e0db735083b1b2" title="Waifu GPU for AI GF?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/lpqhvyq68mue1.png?width=1142&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad31ac4af8529144b8d1be6323d09048cbf4d8b4"&gt;https://videocardz.com/newz/asus-officially-reveals-first-geforce-rtx-5060-ti-ahead-of-launch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I dont know these characters, but is this the future of mankind?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy9g9y/waifu_gpu_for_ai_gf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy9g9y/waifu_gpu_for_ai_gf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy9g9y/waifu_gpu_for_ai_gf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T14:53:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy6627</id>
    <title>I benchmarked the top models used for translation on openrouter V2!</title>
    <updated>2025-04-13T12:07:56+00:00</updated>
    <author>
      <name>/u/AdventurousFly4909</name>
      <uri>https://old.reddit.com/user/AdventurousFly4909</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6627/i_benchmarked_the_top_models_used_for_translation/"&gt; &lt;img alt="I benchmarked the top models used for translation on openrouter V2!" src="https://preview.redd.it/87czci55flue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=20e1c9e41710c1bf16869ecf9ef366993c47d550" title="I benchmarked the top models used for translation on openrouter V2!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I benchmarked the top models listed on openrouter(that are used for translation) on 1000 Chinese-English pairs. I asked each model to translate a Chinese passage to English. I then ranked the translation with &lt;a href="https://github.com/Unbabel/COMET"&gt;comet&lt;/a&gt;. The origin of the test data are Chinese web novels translated into english you can find the test data in the repo. The results are really similar to the results of my last post(The standings of a model compared to others rather than the precise score). This suggest that the ranking is pretty trustworthy especially after a increase of 5x of the test data.&lt;/p&gt; &lt;p&gt;A lot of people had concerns about the scores being too similar I think this is partly because of human nature of how it perceives 0.7815 and 78.15 differently while they are essentially the same. And secondly of really close &lt;strong&gt;some&lt;/strong&gt; of these results are to each other but fret not because can still make trustworthy judgements based on the results.&lt;/p&gt; &lt;p&gt;How to comprehend these results: If the first decimal place differs then the quality difference will be very noticeable. If the second decimal place differs it means that there is a noticeable quality difference. If the third decimal place differs then there will be a minimal quality difference noticeable. If only the fourth place differs then the models can be considered the same&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ProgrammedInsanity/llm_eval_on_test_data"&gt;Repo with all the code and data&lt;/a&gt;. Btw the comet score is from 0 to 1. You could also scale the score with 100 to get for example for deepseek-v3 a score of 78.15.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdventurousFly4909"&gt; /u/AdventurousFly4909 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/87czci55flue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6627/i_benchmarked_the_top_models_used_for_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6627/i_benchmarked_the_top_models_used_for_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T12:07:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy5p12</id>
    <title>Another budget build. 160gb of VRAM for $1000, maybe?</title>
    <updated>2025-04-13T11:38:27+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/"&gt; &lt;img alt="Another budget build. 160gb of VRAM for $1000, maybe?" src="https://b.thumbs.redditmedia.com/hXo0FDMBRfQWGUqiymAq8zmPyIP8zsjaaUWaeo6d10Q.jpg" title="Another budget build. 160gb of VRAM for $1000, maybe?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just grabbed 10 AMD MI50 gpus from eBay, $90 each. $900. I bought an Octominer Ultra x12 case (CPU, MB, 12 pcie slots, fan, ram, ethernet all included) for $100. Ideally, I should be able to just wire them up with no extra expense. Unfortunately the Octominer I got has weak PSU, 3 750w for a total of 2250W. The MI50 consumes 300w. For a peak total of 3000W, the rest of the system itself perhaps bout 350w. I'm team llama.cpp so it won't put much load, and only the active GPU will be used, so it might be possible to stuff 10 GPUs in there (with power limited and using an 8pin to dual 8pin splitter, I won't recommend) I plan on doing 6 first and seeing how it performs. Then either I put the rest in the same case or I split it 5/5 for now across another Octominer case. Specs wise, the MI50 looks about the same as the P40s, it's no longer unofficial supported by AMD, but who cares? :-)&lt;/p&gt; &lt;p&gt;If you plan to do a GPU only build, get this case. The octominer system is a weak system, it's designed for crypto mining, so weak celeron CPUs, weak memory. Don't try to offload, they usually come with about 4-8gb of ram. Mine came with 4gb. Will have hiveOS installed, you can install Ubuntu in it. No NVME, it's a few years ago, but it does take SSDs, it has 4 USB ports, it has a built in ethernet that's suppose to be a gigabit port, but mine is only 100M, I probably have a much older model. It has inbuilt VGA &amp;amp; HDMI port. So no need to be 100% headless. It has 140x38 fans that can uses static pressure to move air through the case. Sounds like a jet, however, you can control it. beats my fan rig for the P40s. My guess is the PCIe slot is x1 electrical lanes. So don't get this if you plan on doing training, unless if you are training a smol model maybe.&lt;/p&gt; &lt;p&gt;Putting a motherboard, CPU, ram, fan, PSU, risers, case/air frame, etc adds up. You will not match this system for $200. Yet you can pick up one with for $200.&lt;/p&gt; &lt;p&gt;There, go get you an Octominer case if you're team GPU.&lt;/p&gt; &lt;p&gt;With that said, I can't say much on the MI50s yet. I'm currently hiking the AMD/Vulkan path of hell, Linux already has vulkan by default. I built llama.cpp, but inference output is garbage, still trying to sort it out. I did a partial RPC offload to one of the cards and output was reasonable so cards are not garbage. With the 100Mbps network traffic, file transfer is slow, so in a few hours, I'm going to go to the store and pick up a 1Gbps network card or ethernet USB stick. More updates to come.&lt;/p&gt; &lt;p&gt;The goal is to add this to my build so I can run even better quant of DeepSeek R1/V3. Unsloth team cooked the hell out of their UD quants.&lt;/p&gt; &lt;p&gt;If you have experience with these AMD instinct MI cards, please let me know how the heck to get them to behave with llama.cpp if you have the experience.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9oq5fzei9lue1.jpg?width=3072&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e138eed8ee146e284d2b109c84ea8af5b3259b03"&gt;https://preview.redd.it/9oq5fzei9lue1.jpg?width=3072&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e138eed8ee146e284d2b109c84ea8af5b3259b03&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Go ye forth my friends and be resourceful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T11:38:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy0zjw</id>
    <title>Gave Maverick another shot (much better!)</title>
    <updated>2025-04-13T06:02:12+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For some reason Maverick was hit particularly hard on my multiple choice cyber security benchmark by the llama.cpp inference bug. &lt;/p&gt; &lt;p&gt;Went from one of the worst models to one of the best.&lt;/p&gt; &lt;p&gt;1st - GPT-4.5 - 95.01% - $3.87&lt;br /&gt; &lt;strong&gt;2nd - Llama-4-Maverick-UD-Q4-GGUF-latest-Llama.cpp 94.06%&lt;/strong&gt;&lt;br /&gt; 3rd - Claude-3.7 - 92.87% - $0.30&lt;br /&gt; 3rd - Claude-3.5-October - 92.87%&lt;br /&gt; &lt;strong&gt;5th - Meta-Llama3.1-405b-FP8 - 92.64%&lt;/strong&gt;&lt;br /&gt; 6th - GPT-4o - 92.40%&lt;br /&gt; 6th - Mistral-Large-123b-2411-FP16 92.40%&lt;br /&gt; 8th - Deepseek-v3-api - 91.92% - $0.03&lt;br /&gt; 9th - GPT-4o-mini - 91.75%&lt;br /&gt; 10th - DeepSeek-v2.5-1210-BF16 - 90.50%&lt;br /&gt; 11th - Meta-LLama3.3-70b-FP8 - 90.26%&lt;br /&gt; 12th - Qwen-2.5-72b-FP8 - 90.09%&lt;br /&gt; 13th - Meta-Llama3.1-70b-FP8 - 89.15%&lt;br /&gt; 14th - Llama-4-scout-Lambda-Last-Week - 88.6%&lt;br /&gt; 14th - Phi-4-GGUF-Fixed-Q4 - 88.6%&lt;br /&gt; 16th - Hunyuan-Large-389b-FP8 - 88.60%&lt;br /&gt; 17th - Qwen-2.5-14b-awq - 85.75%&lt;br /&gt; 18th - Qwen2.5-7B-FP16 - 83.73%&lt;br /&gt; 19th - IBM-Granite-3.1-8b-FP16 - 82.19%&lt;br /&gt; 20th - Meta-Llama3.1-8b-FP16 - 81.37%&lt;br /&gt; &lt;strong&gt;*** - Llama-4-Maverick-UD-Q4-GGUF-Old-Llama.cpp 77.44%&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;*** - Llama-4-Maverick-FP8-Lambda-Last-Week- 77.2%&lt;/strong&gt;&lt;br /&gt; 21st - IBM-Granite-3.0-8b-FP16 - 73.82% &lt;/p&gt; &lt;p&gt;Not sure how much faith I put in the bouncing balls test, but it does still struggle with that one.&lt;br /&gt; So guessing this is still not going to be a go-to for coding.&lt;br /&gt; Still this at least gives me a lot more hope for the L4 reasoner. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy0zjw/gave_maverick_another_shot_much_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy0zjw/gave_maverick_another_shot_much_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy0zjw/gave_maverick_another_shot_much_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T06:02:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy1x1b</id>
    <title>Vocalis: Local Conversational AI Assistant (Speech ‚ÜîÔ∏è Speech in Real Time with Vision Capabilities)</title>
    <updated>2025-04-13T07:07:25+00:00</updated>
    <author>
      <name>/u/townofsalemfangay</name>
      <uri>https://old.reddit.com/user/townofsalemfangay</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy1x1b/vocalis_local_conversational_ai_assistant_speech/"&gt; &lt;img alt="Vocalis: Local Conversational AI Assistant (Speech ‚ÜîÔ∏è Speech in Real Time with Vision Capabilities)" src="https://external-preview.redd.it/mDmX7u3VLHRX-kGFVwmgM9o7wWa38WRF25RrGfaO_ys.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd2b04faff1651442b1a681ea604e94423b30bd0" title="Vocalis: Local Conversational AI Assistant (Speech ‚ÜîÔ∏è Speech in Real Time with Vision Capabilities)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã&lt;/p&gt; &lt;p&gt;Been a long project, but I have Just released &lt;strong&gt;Vocalis&lt;/strong&gt;, a real-time local assistant that goes full speech-to-speech‚ÄîCustom VAD, Faster Whisper ASR, LLM in the middle, TTS out. Built for speed, fluidity, and actual usability in voice-first workflows. Latency will depend on your setup, ASR preference and LLM/TTS model size (all configurable via the .env in backend).&lt;/p&gt; &lt;p&gt;üí¨ &lt;strong&gt;Talk to it like a person&lt;/strong&gt;.&lt;br /&gt; üéß &lt;strong&gt;Interrupt mid-response&lt;/strong&gt; (barge-in).&lt;br /&gt; üß† &lt;strong&gt;Silence detection for follow-ups&lt;/strong&gt; (the assistant will speak without you following up based on the context of the conversation).&lt;br /&gt; üñºÔ∏è &lt;strong&gt;Image analysis support to provide multi-modal context to non-vision capable endpoints&lt;/strong&gt; (&lt;a href="https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct"&gt;SmolVLM-256M&lt;/a&gt;).&lt;br /&gt; üßæ &lt;strong&gt;Session save/load support&lt;/strong&gt; with full context.&lt;/p&gt; &lt;p&gt;It uses your local LLM via OpenAI-style endpoint (LM Studio, llama.cpp, GPUStack, etc), and any TTS server (like my &lt;a href="https://github.com/Lex-au/Orpheus-FastAPI"&gt;Orpheus-FastAPI&lt;/a&gt; or for super low latency, &lt;a href="https://github.com/remsky/Kokoro-FastAPI"&gt;Kokoro-FastAPI&lt;/a&gt;). Frontend is React, backend is FastAPI‚ÄîWebSocket-native with real-time audio streaming and UI states like &lt;em&gt;Listening&lt;/em&gt;, &lt;em&gt;Processing&lt;/em&gt;, and &lt;em&gt;Speaking&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Speech Recognition Performance (using Vocalis-Q4_K_M + Koroko-FASTAPI TTS)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The system uses Faster-Whisper with the &lt;code&gt;base.en&lt;/code&gt; model and a beam size of 2, striking an optimal balance between accuracy and speed. This configuration achieves:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;ASR Processing&lt;/strong&gt;: ~0.43 seconds for typical utterances&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Response Generation&lt;/strong&gt;: ~0.18 seconds&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Total Round-Trip Latency&lt;/strong&gt;: ~0.61 seconds&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Real-world example from system logs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;INFO:faster_whisper:Processing audio with duration 00:02.229 INFO:backend.services.transcription:Transcription completed in 0.51s: Hi, how are you doing today?... INFO:backend.services.tts:Sending TTS request with 147 characters of text INFO:backend.services.tts:Received TTS response after 0.16s, size: 390102 bytes &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There's a full breakdown of the architecture and latency information on my readme.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Lex-au/VocalisConversational"&gt;https://github.com/Lex-au/VocalisConversational&lt;/a&gt;&lt;br /&gt; model (optional): &lt;a href="https://huggingface.co/lex-au/Vocalis-Q4_K_M.gguf"&gt;https://huggingface.co/lex-au/Vocalis-Q4_K_M.gguf&lt;/a&gt;&lt;br /&gt; Some demo videos during project progress here: &lt;a href="https://www.youtube.com/@AJ-sj5ik"&gt;https://www.youtube.com/@AJ-sj5ik&lt;/a&gt;&lt;br /&gt; License: Apache 2.0&lt;/p&gt; &lt;p&gt;Let me know what you think or if you have questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/townofsalemfangay"&gt; /u/townofsalemfangay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Lex-au/Vocalis"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy1x1b/vocalis_local_conversational_ai_assistant_speech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy1x1b/vocalis_local_conversational_ai_assistant_speech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T07:07:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jxv644</id>
    <title>I chopped the screen off my MacBook Air to be a full time LLM server</title>
    <updated>2025-04-13T00:12:36+00:00</updated>
    <author>
      <name>/u/mark-lord</name>
      <uri>https://old.reddit.com/user/mark-lord</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxv644/i_chopped_the_screen_off_my_macbook_air_to_be_a/"&gt; &lt;img alt="I chopped the screen off my MacBook Air to be a full time LLM server" src="https://preview.redd.it/qrnzf9tguhue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16bb48263ccc8f44559ef992fd4e2e9901fdac0f" title="I chopped the screen off my MacBook Air to be a full time LLM server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got the thing for ¬£250 used with a broken screen; finally just got around to removing it permanently lol&lt;/p&gt; &lt;p&gt;Runs Qwen-7b at 14 tokens-per-second, which isn‚Äôt amazing, but honestly is actually a lot better than I expected for an M1 8gb chip!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mark-lord"&gt; /u/mark-lord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qrnzf9tguhue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxv644/i_chopped_the_screen_off_my_macbook_air_to_be_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jxv644/i_chopped_the_screen_off_my_macbook_air_to_be_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T00:12:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jxu0f7</id>
    <title>We should have a monthly ‚Äúwhich models are you using‚Äù discussion</title>
    <updated>2025-04-12T23:12:01+00:00</updated>
    <author>
      <name>/u/Arkhos-Winter</name>
      <uri>https://old.reddit.com/user/Arkhos-Winter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since a lot of people keep coming on here and asking which models they should use (either through API or on their GPU), I propose that we have a formalized discussion on what we think are the best models (both proprietary and open-weights) for different purposes (coding, writing, etc.) on the 1st of every month.&lt;/p&gt; &lt;p&gt;It‚Äôll go something like this: ‚ÄúI‚Äôm currently using Deepseek v3.1, 4o (March 2025 version), and Gemini 2.5 Pro for writing, and I‚Äôm using R1, Qwen 2.5 Max, and Sonnet 3.7 (thinking) for coding.‚Äù&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arkhos-Winter"&gt; /u/Arkhos-Winter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxu0f7/we_should_have_a_monthly_which_models_are_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxu0f7/we_should_have_a_monthly_which_models_are_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jxu0f7/we_should_have_a_monthly_which_models_are_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-12T23:12:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy8h2i</id>
    <title>Skywork-OR1: new SOTA 32B thinking model with open weight, training code, and training data</title>
    <updated>2025-04-13T14:08:06+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8h2i/skyworkor1_new_sota_32b_thinking_model_with_open/"&gt; &lt;img alt="Skywork-OR1: new SOTA 32B thinking model with open weight, training code, and training data" src="https://b.thumbs.redditmedia.com/Xv5xeh-T-_FI_nUFqM0bvDA9nB-t2tXuTjAVsmlXjdE.jpg" title="Skywork-OR1: new SOTA 32B thinking model with open weight, training code, and training data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;github repo: &lt;a href="https://github.com/SkyworkAI/Skywork-OR1"&gt;https://github.com/SkyworkAI/Skywork-OR1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;blog: &lt;a href="https://capricious-hydrogen-41c.notion.site/Skywork-Open-Reasoner-Series-1d0bc9ae823a80459b46c149e4f51680"&gt;https://capricious-hydrogen-41c.notion.site/Skywork-Open-Reasoner-Series-1d0bc9ae823a80459b46c149e4f51680&lt;/a&gt;&lt;/p&gt; &lt;p&gt;huggingface: &lt;a href="https://huggingface.co/collections/Skywork/skywork-or1-67fa1bcb41b436ef2def76b9"&gt;https://huggingface.co/collections/Skywork/skywork-or1-67fa1bcb41b436ef2def76b9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uuodxdre0mue1.png?width=1532&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0567ead14bd49bdc33066bf3bca19e1ad566676e"&gt;https://preview.redd.it/uuodxdre0mue1.png?width=1532&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0567ead14bd49bdc33066bf3bca19e1ad566676e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8h2i/skyworkor1_new_sota_32b_thinking_model_with_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8h2i/skyworkor1_new_sota_32b_thinking_model_with_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8h2i/skyworkor1_new_sota_32b_thinking_model_with_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T14:08:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy16yi</id>
    <title>LMArena ruined language models</title>
    <updated>2025-04-13T06:16:18+00:00</updated>
    <author>
      <name>/u/Dogeboja</name>
      <uri>https://old.reddit.com/user/Dogeboja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LMArena is way too easy to game, you just optimize for whatever their front-end is capable of rendering and especially focus on bulleted lists since those seem to get the most clicks. Maybe sprinkle in some emojis and that's it, no need to actually produce excellent answers.&lt;/p&gt; &lt;p&gt;Markdown especially is starting to become very tightly ingrained into all model answers, it's not like it's the be-all and end-all of human communication. You can somewhat combat this with system instructions but I am worried it could cause unexpected performance degradation.&lt;/p&gt; &lt;p&gt;The recent LLaMA 4 fiasco and the fact that Claude Sonnet 3.7 is at rank 22 below models like Gemma 3 27B tells the whole story.&lt;/p&gt; &lt;p&gt;How could this be fixed at this point? My solution would be to simply disable Markdown in the front-end, I really think language generation and formatting should be separate capabilities.&lt;/p&gt; &lt;p&gt;By the way, if you are struggling with this, try this system prompt: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Prefer natural language, avoid formulaic responses.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This works quite well most of the time but it can sometimes lead to worse answers if the formulaic answer was truly the best style for that prompt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dogeboja"&gt; /u/Dogeboja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy16yi/lmarena_ruined_language_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy16yi/lmarena_ruined_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy16yi/lmarena_ruined_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T06:16:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy813d</id>
    <title>From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models</title>
    <updated>2025-04-13T13:47:07+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2504.06214"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy813d/from_128k_to_4m_efficient_training_of_ultralong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy813d/from_128k_to_4m_efficient_training_of_ultralong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T13:47:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jxy26m</id>
    <title>Sam Altman: "We're going to do a very powerful open source model... better than any current open source model out there."</title>
    <updated>2025-04-13T02:55:45+00:00</updated>
    <author>
      <name>/u/mw11n19</name>
      <uri>https://old.reddit.com/user/mw11n19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxy26m/sam_altman_were_going_to_do_a_very_powerful_open/"&gt; &lt;img alt="Sam Altman: &amp;quot;We're going to do a very powerful open source model... better than any current open source model out there.&amp;quot;" src="https://external-preview.redd.it/eDJobnVwZ3luaXVlMdXj0QNvtvvTvdLhyylbR9Y6PzQjPjUyfN1eoWAw2jEe.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a5f48835aebe28a468ef3c09a1d306d926d0876" title="Sam Altman: &amp;quot;We're going to do a very powerful open source model... better than any current open source model out there.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mw11n19"&gt; /u/mw11n19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wzjs6qgyniue1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxy26m/sam_altman_were_going_to_do_a_very_powerful_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jxy26m/sam_altman_were_going_to_do_a_very_powerful_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T02:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy6ns6</id>
    <title>Coming soon‚Ä¶..</title>
    <updated>2025-04-13T12:36:19+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6ns6/coming_soon/"&gt; &lt;img alt="Coming soon‚Ä¶.." src="https://preview.redd.it/1cwv3wz7klue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abbae222e535c2c110583987226650f6391ac918" title="Coming soon‚Ä¶.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1cwv3wz7klue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6ns6/coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6ns6/coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T12:36:19+00:00</published>
  </entry>
</feed>
