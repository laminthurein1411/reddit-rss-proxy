<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-08T13:05:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j6c556</id>
    <title>How to Summarize Long Documents on Mobile Devices with Hardware Constraints?</title>
    <updated>2025-03-08T07:38:53+00:00</updated>
    <author>
      <name>/u/Timely-Jackfruit8885</name>
      <uri>https://old.reddit.com/user/Timely-Jackfruit8885</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm developing an AI-powered mobile app (&lt;a href="https://play.google.com/store/apps/details?id=com.DAI.DAIapp"&gt;https://play.google.com/store/apps/details?id=com.DAI.DAIapp&lt;/a&gt;)that needs to summarize long documents efficiently. The challenge is that I want to keep everything running locally, so I have to deal with hardware limitations (RAM, CPU, and storage constraints).&lt;/p&gt; &lt;p&gt;I’m currently using llama.cpp to run LLMs on-device and have integrated embeddings for semantic search. However, summarizing long documents is tricky due to context length limits and performance bottlenecks on mobile.&lt;/p&gt; &lt;p&gt;Has anyone tackled this problem before? Are there any optimized techniques, libraries, or models that work well on mobile hardware?&lt;/p&gt; &lt;p&gt;Any insights or recommendations would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Timely-Jackfruit8885"&gt; /u/Timely-Jackfruit8885 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6c556/how_to_summarize_long_documents_on_mobile_devices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6c556/how_to_summarize_long_documents_on_mobile_devices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6c556/how_to_summarize_long_documents_on_mobile_devices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T07:38:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5l66b</id>
    <title>Flappy Bird game by QwQ 32B IQ4_XS GGUF</title>
    <updated>2025-03-07T11:14:49+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l66b/flappy_bird_game_by_qwq_32b_iq4_xs_gguf/"&gt; &lt;img alt="Flappy Bird game by QwQ 32B IQ4_XS GGUF" src="https://external-preview.redd.it/MDVpZGpwYnUzOW5lMU90rGzJ1hZd2Ko9NJiQB4OtIZYL8dNtOZuKS2VIGG38.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78b96d60c5871140edd7d5640114998de50d9192" title="Flappy Bird game by QwQ 32B IQ4_XS GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6usunobu39ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l66b/flappy_bird_game_by_qwq_32b_iq4_xs_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l66b/flappy_bird_game_by_qwq_32b_iq4_xs_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T11:14:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j69zl7</id>
    <title>Looking for beta testers for a local LLM, kokoro TTS supported, chat &amp; news app.</title>
    <updated>2025-03-08T05:15:45+00:00</updated>
    <author>
      <name>/u/clockentyne</name>
      <uri>https://old.reddit.com/user/clockentyne</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j69zl7/looking_for_beta_testers_for_a_local_llm_kokoro/"&gt; &lt;img alt="Looking for beta testers for a local LLM, kokoro TTS supported, chat &amp;amp; news app." src="https://external-preview.redd.it/bzZmbWc3ZW9nZW5lMXX6-fvAK-sQelTG_mLXF8zDOxdsMWF5oRG9uoSpFfgL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c3b18632d71a4317c761154b009ac0dbb3f2da7" title="Looking for beta testers for a local LLM, kokoro TTS supported, chat &amp;amp; news app." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clockentyne"&gt; /u/clockentyne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0wclz6eogene1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j69zl7/looking_for_beta_testers_for_a_local_llm_kokoro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j69zl7/looking_for_beta_testers_for_a_local_llm_kokoro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T05:15:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j66oe7</id>
    <title>RLAMA: A Simple RAG Interface to Chat with Your Documents via Ollama</title>
    <updated>2025-03-08T02:08:16+00:00</updated>
    <author>
      <name>/u/DonTizi</name>
      <uri>https://old.reddit.com/user/DonTizi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;/p&gt; &lt;p&gt;I developed RLAMA to solve a straightforward but frustrating problem: how to easily query my own documents with a local LLM without using cloud services.&lt;/p&gt; &lt;h1&gt;What it actually is&lt;/h1&gt; &lt;p&gt;RLAMA is a command-line tool that bridges your local documents and Ollama models. It implements RAG (Retrieval-Augmented Generation) in a minimalist way:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Index a folder of documents rlama rag llama3 project-docs ./documentation # Start an interactive session rlama run project-docs &amp;gt; How does the authentication module work? &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;You point the tool to a folder containing your files (.txt, .md, .pdf, source code, etc.)&lt;/li&gt; &lt;li&gt;RLAMA extracts text from the documents and generates embeddings via Ollama&lt;/li&gt; &lt;li&gt;When you ask a question, it retrieves relevant passages and sends them to the model&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The tool handles many formats automatically. For PDFs, it first tries pdftotext, then tesseract if necessary. For binary files, it has several fallback methods to extract what it can.&lt;/p&gt; &lt;h1&gt;Problems it solves&lt;/h1&gt; &lt;p&gt;I use it daily for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Finding information in old technical documents without having to reread everything&lt;/li&gt; &lt;li&gt;Exploring code I'm not familiar with (e.g., &amp;quot;explain how part X works&amp;quot;)&lt;/li&gt; &lt;li&gt;Creating summaries of long documents&lt;/li&gt; &lt;li&gt;Querying my research or meeting notes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The real time-saver comes from being able to ask questions instead of searching for keywords. For example, I can ask &amp;quot;What are the possible errors in the authentication API?&amp;quot; and get consolidated answers from multiple files.&lt;/p&gt; &lt;h1&gt;Why use it?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;It's simple&lt;/strong&gt;: four commands are enough (rag, run, list, delete)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It's local&lt;/strong&gt;: no data is sent over the internet&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It's lightweight&lt;/strong&gt;: no need for Docker or a complete stack&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It's flexible&lt;/strong&gt;: compatible with all Ollama models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I created it because other solutions were either too complex to configure or required sending my documents to external services.&lt;/p&gt; &lt;p&gt;If you already have Ollama installed and are looking for a simple way to query your documents, this might be useful for you.&lt;/p&gt; &lt;h1&gt;In conclusion&lt;/h1&gt; &lt;p&gt;I've found that in discussions on &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; point to several pressing needs for local RAG without cloud dependencies: we need to simplify the ingestion of data (PDFs, web pages, videos...) via tools that can automatically transform them into usable text, reduce hardware requirements or better leverage common hardware (model quantization, multi-GPU support) to improve performance, and integrate advanced retrieval methods (hybrid search, rerankers, etc.) to increase answer reliability.&lt;/p&gt; &lt;p&gt;The emergence of integrated solutions (OpenWebUI, LangChain/Langroid, RAGStack, etc.) moves in this direction: the ultimate goal is a tool where users only need to provide their local files to benefit from an AI assistant trained on their own knowledge, while remaining 100% private and local so I wanted to develop something easy to use!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/dontizi/rlama"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DonTizi"&gt; /u/DonTizi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j66oe7/rlama_a_simple_rag_interface_to_chat_with_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j66oe7/rlama_a_simple_rag_interface_to_chat_with_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j66oe7/rlama_a_simple_rag_interface_to_chat_with_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T02:08:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5i8di</id>
    <title>QwQ Bouncing ball (it took 15 minutes of yapping)</title>
    <updated>2025-03-07T07:42:23+00:00</updated>
    <author>
      <name>/u/philschmid</name>
      <uri>https://old.reddit.com/user/philschmid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5i8di/qwq_bouncing_ball_it_took_15_minutes_of_yapping/"&gt; &lt;img alt="QwQ Bouncing ball (it took 15 minutes of yapping)" src="https://external-preview.redd.it/c3MxaHh0bHoxOG5lMVsIOv4dsf9lRkZrSYg6c4izCiXravlzRnamhYWv4oaG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51e2eb5a7001f62ab3db3f78e329b604eb60560a" title="QwQ Bouncing ball (it took 15 minutes of yapping)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philschmid"&gt; /u/philschmid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c2tvpslz18ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5i8di/qwq_bouncing_ball_it_took_15_minutes_of_yapping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5i8di/qwq_bouncing_ball_it_took_15_minutes_of_yapping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T07:42:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j61cbx</id>
    <title>I just tried out a model and it blew me away: llama3.2 1b</title>
    <updated>2025-03-07T21:51:53+00:00</updated>
    <author>
      <name>/u/Firm_Newspaper3370</name>
      <uri>https://old.reddit.com/user/Firm_Newspaper3370</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been thoroughly enjoying both llama3.3 70b and qwq 32b recently. Both are very impressive for their size, and these are the first models that a poor boy like me can run locally that I feel can give Chat got and Claude a run for their money.&lt;/p&gt; &lt;p&gt;But about an hour ago I had a thought.&lt;/p&gt; &lt;p&gt;If really good models are getting down to medium model sizes, how good are super tiny models getting?&lt;/p&gt; &lt;p&gt;So I downloaded llama3.3 1b.&lt;/p&gt; &lt;p&gt;Having run it through my normal phthon writing prompts, I am truly blown away. It's obviously not a 70b replacement, nor an 8b replacement. But I can hardly fathom that you can fit so much intelligence into 1.3gb.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Firm_Newspaper3370"&gt; /u/Firm_Newspaper3370 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j61cbx/i_just_tried_out_a_model_and_it_blew_me_away/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j61cbx/i_just_tried_out_a_model_and_it_blew_me_away/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j61cbx/i_just_tried_out_a_model_and_it_blew_me_away/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T21:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6ez58</id>
    <title>M3 Ultra 512GB - Could I run 4 70B LLMs at the same time?</title>
    <updated>2025-03-08T11:10:55+00:00</updated>
    <author>
      <name>/u/TechNerd10191</name>
      <uri>https://old.reddit.com/user/TechNerd10191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since agentic workflows become more and more common, if I wanted to try a project where 4 70B LLMs (e.g. Llama 3.3 70B at Q4 and 72k context) work in parallel, would I be able to do this on a 512GB memory Studio? I know it's a bit early to ask this - as no Mac Studios are available yet - but it's an interesting though. What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechNerd10191"&gt; /u/TechNerd10191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ez58/m3_ultra_512gb_could_i_run_4_70b_llms_at_the_same/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ez58/m3_ultra_512gb_could_i_run_4_70b_llms_at_the_same/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ez58/m3_ultra_512gb_could_i_run_4_70b_llms_at_the_same/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T11:10:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5vjtr</id>
    <title>Cydonia 24B v2.1 - Bolder, better, brighter</title>
    <updated>2025-03-07T18:10:20+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5vjtr/cydonia_24b_v21_bolder_better_brighter/"&gt; &lt;img alt="Cydonia 24B v2.1 - Bolder, better, brighter" src="https://external-preview.redd.it/Y53sGNZqFT7O2LPxR0ifOBO74G3g4p3oD2d89H5ZiiM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f99bb247d7af48f7df7184bd4ce33a8e1090f74f" title="Cydonia 24B v2.1 - Bolder, better, brighter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v2.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5vjtr/cydonia_24b_v21_bolder_better_brighter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5vjtr/cydonia_24b_v21_bolder_better_brighter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T18:10:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5s629</id>
    <title>The Genius of DeepSeek’s 57X Efficiency Boost [MLA]</title>
    <updated>2025-03-07T16:12:48+00:00</updated>
    <author>
      <name>/u/Different-Olive-8745</name>
      <uri>https://old.reddit.com/user/Different-Olive-8745</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5s629/the_genius_of_deepseeks_57x_efficiency_boost_mla/"&gt; &lt;img alt="The Genius of DeepSeek’s 57X Efficiency Boost [MLA]" src="https://external-preview.redd.it/cNA68CfuuchA-pKjC19aWS3fLxcXM9n1EmEaCjksBPI.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e6d61fc0d91e37ef5748cd5797b5f122d3ab207" title="The Genius of DeepSeek’s 57X Efficiency Boost [MLA]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Olive-8745"&gt; /u/Different-Olive-8745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/0VLAoVGf_74?si=OSwMMKsz9EpLOISJ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5s629/the_genius_of_deepseeks_57x_efficiency_boost_mla/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5s629/the_genius_of_deepseeks_57x_efficiency_boost_mla/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T16:12:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5h7k8</id>
    <title>QwQ, one token after giving the most incredible R1-destroying correct answer in its think tags</title>
    <updated>2025-03-07T06:28:57+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5h7k8/qwq_one_token_after_giving_the_most_incredible/"&gt; &lt;img alt="QwQ, one token after giving the most incredible R1-destroying correct answer in its think tags" src="https://preview.redd.it/efyqdgtwo7ne1.jpeg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c2f05b3315ef35bc7ca516d97097a37aff4994d" title="QwQ, one token after giving the most incredible R1-destroying correct answer in its think tags" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/efyqdgtwo7ne1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5h7k8/qwq_one_token_after_giving_the_most_incredible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5h7k8/qwq_one_token_after_giving_the_most_incredible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T06:28:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6e4j7</id>
    <title>DNA-R1 (REASONING MODEL)</title>
    <updated>2025-03-08T10:07:28+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6e4j7/dnar1_reasoning_model/"&gt; &lt;img alt="DNA-R1 (REASONING MODEL)" src="https://external-preview.redd.it/eCZla5i8qBbR-EcKbP_YjinL9_9ebCep5Gy1Q0SHWrA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae9ff94ebc41043fb178b0e03601e7c624ce8570" title="DNA-R1 (REASONING MODEL)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We introduce DNA-R1, a specialized reasoning model optimized for Korean language based on Microsoft's Phi-4. By applying large-scale reinforcement learning (RL) using the same methodology as DeepSeek-R1, we have significantly enhanced the model's Korean reasoning capabilities. This model demonstrates deep understanding of Korean text and exhibits exceptional reasoning abilities across mathematics, coding, and general reasoning tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/krkq6jlpwfne1.png?width=697&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94af500a4c9592dafc9daa7646789e3ce0071c17"&gt;https://preview.redd.it/krkq6jlpwfne1.png?width=697&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94af500a4c9592dafc9daa7646789e3ce0071c17&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7onilg2rwfne1.png?width=795&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78d40f1ae4658605f5cba2f6eacc27eba16e2f09"&gt;https://preview.redd.it/7onilg2rwfne1.png?width=795&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78d40f1ae4658605f5cba2f6eacc27eba16e2f09&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/degflmwrwfne1.png?width=187&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbd79a3183e7a8688690cc100200f0c0172becb2"&gt;https://preview.redd.it/degflmwrwfne1.png?width=187&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbd79a3183e7a8688690cc100200f0c0172becb2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/dnotitia/DNA-R1"&gt;https://huggingface.co/dnotitia/DNA-R1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6e4j7/dnar1_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6e4j7/dnar1_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6e4j7/dnar1_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T10:07:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6e2yz</id>
    <title>Open-source framework achieves 58.18 average on GAIA benchmark</title>
    <updated>2025-03-08T10:04:21+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6e2yz/opensource_framework_achieves_5818_average_on/"&gt; &lt;img alt="Open-source framework achieves 58.18 average on GAIA benchmark" src="https://b.thumbs.redditmedia.com/PL6cz7XPh_f6opT3d6mcZEbNqZyfKjzqNCNfXJLvOHw.jpg" title="Open-source framework achieves 58.18 average on GAIA benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/7rivbps3wfne1.png?width=1732&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f1f1527e2a1db60106357ed84f5e60896936da8"&gt;https://preview.redd.it/7rivbps3wfne1.png?width=1732&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f1f1527e2a1db60106357ed84f5e60896936da8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6e2yz/opensource_framework_achieves_5818_average_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6e2yz/opensource_framework_achieves_5818_average_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6e2yz/opensource_framework_achieves_5818_average_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T10:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j60bc4</id>
    <title>AMD &amp; tinygrad cooperation happening</title>
    <updated>2025-03-07T21:08:39+00:00</updated>
    <author>
      <name>/u/Danmoreng</name>
      <uri>https://old.reddit.com/user/Danmoreng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j60bc4/amd_tinygrad_cooperation_happening/"&gt; &lt;img alt="AMD &amp;amp; tinygrad cooperation happening" src="https://external-preview.redd.it/BbTfUmQNA0TFAIFilHM3Y4In9mUXZYXfhSxTgqgwTN8.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b210445be3d20e6a3900593f94809ea783979b0" title="AMD &amp;amp; tinygrad cooperation happening" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Danmoreng"&gt; /u/Danmoreng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/AnushElangovan/status/1898101178728431637"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j60bc4/amd_tinygrad_cooperation_happening/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j60bc4/amd_tinygrad_cooperation_happening/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T21:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5qo7q</id>
    <title>QwQ-32B infinite generations fixes + best practices, bug fixes</title>
    <updated>2025-03-07T15:20:03+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"&gt; &lt;img alt="QwQ-32B infinite generations fixes + best practices, bug fixes" src="https://external-preview.redd.it/C8aU2vS5rsrlIktUq8a_5r42ZGVY34rKstBbebj3EEA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09742bfb9b718b50a05ce6019bcbb8a232d8e890" title="QwQ-32B infinite generations fixes + best practices, bug fixes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! If you're having &lt;strong&gt;infinite repetitions with QwQ-32B&lt;/strong&gt;, you're not alone! I made a guide to help debug stuff! I also uploaded dynamic 4bit quants &amp;amp; other GGUFs! Link to guide: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;When using &lt;strong&gt;repetition penalties&lt;/strong&gt; to counteract looping, it rather causes looping!&lt;/li&gt; &lt;li&gt;The Qwen team confirmed for long context (128K), you should use YaRN.&lt;/li&gt; &lt;li&gt;When using repetition penalties, add &lt;code&gt;--samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot;&lt;/code&gt; to stop infinite generations.&lt;/li&gt; &lt;li&gt;Using &lt;code&gt;min_p = 0.1&lt;/code&gt; helps remove low probability tokens.&lt;/li&gt; &lt;li&gt;Try using &lt;code&gt;--repeat-penalty 1.1 --dry-multiplier 0.5&lt;/code&gt; to reduce repetitions.&lt;/li&gt; &lt;li&gt;Please use &lt;code&gt;--temp 0.6 --top-k 40 --top-p 0.95&lt;/code&gt; as suggested by the Qwen team.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For example my settings in llama.cpp which work great - uses the DeepSeek R1 1.58bit Flappy Bird test I introduced back here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama.cpp/llama-cli \ --model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \ --threads 32 \ --ctx-size 16384 \ --n-gpu-layers 99 \ --seed 3407 \ --prio 2 \ --temp 0.6 \ --repeat-penalty 1.1 \ --dry-multiplier 0.5 \ --min-p 0.1 \ --top-k 40 \ --top-p 0.95 \ -no-cnv \ --samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot; \ --prompt &amp;quot;&amp;lt;|im_start|&amp;gt;user\nCreate a Flappy Bird game in Python. You must include these things:\n1. You must use pygame.\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\n3. Pressing SPACE multiple times will accelerate the bird.\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&amp;lt;think&amp;gt;\n&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I also uploaded dynamic 4bit quants for QwQ to &lt;a href="https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit"&gt;https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit&lt;/a&gt; which are directly vLLM compatible since 0.7.3&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w65lgkmh5ane1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f77f68e9639bbd8dccdb51c1314d084802b7b213"&gt;Quantization errors for QwQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Links to models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B-GGUF"&gt;QwQ-32B GGUFs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit"&gt;QwQ-32B dynamic 4bit&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B-bnb-4bit"&gt;QwQ-32B bitsandbytes 4bit&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B"&gt;QwQ-32B 16bit&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wrote more details on my findings, and made a guide here: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T15:20:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5wzea</id>
    <title>New AMD Driver Yields Up To 11% Performance Increase In koboldcpp</title>
    <updated>2025-03-07T19:02:32+00:00</updated>
    <author>
      <name>/u/WokeCapitalist</name>
      <uri>https://old.reddit.com/user/WokeCapitalist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-7000-series/amd-radeon-rx-7900-xt.html"&gt;AMD's Adrenalin 25.3.1 driver&lt;/a&gt; release mentioned &lt;strong&gt;&lt;em&gt;&amp;quot;AI Performance Improvements on AMD Radeon™ RX 7000 Series&amp;quot;&lt;/em&gt;&lt;/strong&gt; in the release notes along with some large percentage increases for applications like Adobe Lightroom Denoise or DaVinci Resolve. As I had their previous WHQL recommended driver already installed, I decided to test it out in koboldcpp. It turns out there was a nice performance bump there, too. Worth a download if you haven't done so already!&lt;/p&gt; &lt;h1&gt;Hardware Test Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OS:&lt;/strong&gt; Microsoft Windows 11 Professional (x64) Build 26100.3194 (24H2)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel Core Ultra 7 265K&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; AMD Radeon RX 7900 XT (20GB)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; ASRock Z890I Nova WiFi (BIOS 2.22)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Disk:&lt;/strong&gt; Lexar SSD NM800PRO 2TB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory:&lt;/strong&gt; 64GB (2×32GB DDR5 6400 CL32)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Secure Boot, BitLocker &amp;amp; HVCI Enabled&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Software Test Setup:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; koboldcpp 1.83.1&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; phi-4-q4,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Size&lt;/strong&gt;: 16384&lt;/li&gt; &lt;li&gt;&lt;strong&gt;BLAS Batch Size&lt;/strong&gt;: 512&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Threads&lt;/strong&gt;: 8&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU Layers&lt;/strong&gt;: 43/43&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Backend: Vulkan&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 24.12.1&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 25.3.1&lt;/th&gt; &lt;th align="left"&gt;% Change&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Time&lt;/td&gt; &lt;td align="left"&gt;30.312 s&lt;/td&gt; &lt;td align="left"&gt;27.607 s&lt;/td&gt; &lt;td align="left"&gt;8.95% faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Speed&lt;/td&gt; &lt;td align="left"&gt;537.21 T/s&lt;/td&gt; &lt;td align="left"&gt;589.85 T/s&lt;/td&gt; &lt;td align="left"&gt;9.84% higher&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Time&lt;/td&gt; &lt;td align="left"&gt;5.203 s&lt;/td&gt; &lt;td align="left"&gt;5.301 s&lt;/td&gt; &lt;td align="left"&gt;1.87% slower&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Speed&lt;/td&gt; &lt;td align="left"&gt;19.22 T/s&lt;/td&gt; &lt;td align="left"&gt;18.86 T/s&lt;/td&gt; &lt;td align="left"&gt;1.88% lower&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Total Time&lt;/td&gt; &lt;td align="left"&gt;35.515 s&lt;/td&gt; &lt;td align="left"&gt;32.908 s&lt;/td&gt; &lt;td align="left"&gt;7.35% faster&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Backend: ROCm&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 24.12.1&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 25.3.1&lt;/th&gt; &lt;th align="left"&gt;% Change&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Time&lt;/td&gt; &lt;td align="left"&gt;24.861 s&lt;/td&gt; &lt;td align="left"&gt;22.370 s&lt;/td&gt; &lt;td align="left"&gt;10.06% faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Speed&lt;/td&gt; &lt;td align="left"&gt;655.00 T/s&lt;/td&gt; &lt;td align="left"&gt;727.94 T/s&lt;/td&gt; &lt;td align="left"&gt;11.15% higher&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Time&lt;/td&gt; &lt;td align="left"&gt;5.831 s&lt;/td&gt; &lt;td align="left"&gt;5.586 s&lt;/td&gt; &lt;td align="left"&gt;4.20% faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Speed&lt;/td&gt; &lt;td align="left"&gt;17.15 T/s&lt;/td&gt; &lt;td align="left"&gt;17.90 T/s&lt;/td&gt; &lt;td align="left"&gt;4.32% higher&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Total Time&lt;/td&gt; &lt;td align="left"&gt;30.692 s&lt;/td&gt; &lt;td align="left"&gt;27.956 s&lt;/td&gt; &lt;td align="left"&gt;8.97% faster&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WokeCapitalist"&gt; /u/WokeCapitalist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5wzea/new_amd_driver_yields_up_to_11_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5wzea/new_amd_driver_yields_up_to_11_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5wzea/new_amd_driver_yields_up_to_11_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T19:02:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j60wt1</id>
    <title>NVIDIA RTX "PRO" 6000 X Blackwell GPU Spotted In Shipping Log: GB202 Die, 96 GB VRAM, TBP of 600W</title>
    <updated>2025-03-07T21:34:40+00:00</updated>
    <author>
      <name>/u/newdoria88</name>
      <uri>https://old.reddit.com/user/newdoria88</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j60wt1/nvidia_rtx_pro_6000_x_blackwell_gpu_spotted_in/"&gt; &lt;img alt="NVIDIA RTX &amp;quot;PRO&amp;quot; 6000 X Blackwell GPU Spotted In Shipping Log: GB202 Die, 96 GB VRAM, TBP of 600W" src="https://external-preview.redd.it/8nLxMIJQrz_2tTIhvuryMxrtnbjPwWSOP7OO-C_HgM0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8c481144f93be83af79ca767c97e43a6750c4ac" title="NVIDIA RTX &amp;quot;PRO&amp;quot; 6000 X Blackwell GPU Spotted In Shipping Log: GB202 Die, 96 GB VRAM, TBP of 600W" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newdoria88"&gt; /u/newdoria88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-rtx-pro-6000-x-blackwell-leak-96-gb-gddr7-600w/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j60wt1/nvidia_rtx_pro_6000_x_blackwell_gpu_spotted_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j60wt1/nvidia_rtx_pro_6000_x_blackwell_gpu_spotted_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T21:34:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1j66mpo</id>
    <title>Qwen QwQ slots between Claude 3.7 Sonnet Thinking and o1-mini on the Extended NYT Connections benchmark</title>
    <updated>2025-03-08T02:05:50+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j66mpo/qwen_qwq_slots_between_claude_37_sonnet_thinking/"&gt; &lt;img alt="Qwen QwQ slots between Claude 3.7 Sonnet Thinking and o1-mini on the Extended NYT Connections benchmark" src="https://preview.redd.it/ig84dy8oidne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1ee2b26596ddc8e881d50f0eb5e76449003b478" title="Qwen QwQ slots between Claude 3.7 Sonnet Thinking and o1-mini on the Extended NYT Connections benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ig84dy8oidne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j66mpo/qwen_qwq_slots_between_claude_37_sonnet_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j66mpo/qwen_qwq_slots_between_claude_37_sonnet_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T02:05:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j681n3</id>
    <title>Qwen QwQ-32B ranks right among GPT-4o models on Confabulations/Hallucinations benchmark</title>
    <updated>2025-03-08T03:22:26+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j681n3/qwen_qwq32b_ranks_right_among_gpt4o_models_on/"&gt; &lt;img alt="Qwen QwQ-32B ranks right among GPT-4o models on Confabulations/Hallucinations benchmark" src="https://preview.redd.it/izw2ej1cwdne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73dec38ebaddd2720f9cf241a4ae7cf9de6d8481" title="Qwen QwQ-32B ranks right among GPT-4o models on Confabulations/Hallucinations benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/izw2ej1cwdne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j681n3/qwen_qwq32b_ranks_right_among_gpt4o_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j681n3/qwen_qwq32b_ranks_right_among_gpt4o_models_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T03:22:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5zzue</id>
    <title>QwQ on LiveBench - is better than Sonnet 3.7 (non thinking)!</title>
    <updated>2025-03-07T20:48:08+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5zzue/qwq_on_livebench_is_better_than_sonnet_37_non/"&gt; &lt;img alt="QwQ on LiveBench - is better than Sonnet 3.7 (non thinking)!" src="https://preview.redd.it/gc42vz36ybne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a74298e2e3d4a3128892ea9834b44f8efd5e1a9" title="QwQ on LiveBench - is better than Sonnet 3.7 (non thinking)!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gc42vz36ybne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5zzue/qwq_on_livebench_is_better_than_sonnet_37_non/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5zzue/qwq_on_livebench_is_better_than_sonnet_37_non/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T20:48:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6f61q</id>
    <title>QwQ-32B takes second place in EQ-Bench creative writing, above GPT 4.5 and Claude 3.7</title>
    <updated>2025-03-08T11:24:32+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6f61q/qwq32b_takes_second_place_in_eqbench_creative/"&gt; &lt;img alt="QwQ-32B takes second place in EQ-Bench creative writing, above GPT 4.5 and Claude 3.7" src="https://preview.redd.it/opow8do3agne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=00cbc87c29904f9341ccf656e804f32edd07064a" title="QwQ-32B takes second place in EQ-Bench creative writing, above GPT 4.5 and Claude 3.7" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/opow8do3agne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6f61q/qwq32b_takes_second_place_in_eqbench_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6f61q/qwq32b_takes_second_place_in_eqbench_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T11:24:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j68wr1</id>
    <title>Qwen team seems sure that their model is better than LiveBench ranks it and demand a rerun with more optimal settings, which is crazy because it already performed really great</title>
    <updated>2025-03-08T04:11:36+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"&gt; &lt;img alt="Qwen team seems sure that their model is better than LiveBench ranks it and demand a rerun with more optimal settings, which is crazy because it already performed really great" src="https://a.thumbs.redditmedia.com/H0M55_ytNjyQLjluGxIhsq01_P1u9IVqCdRWbacevz8.jpg" title="Qwen team seems sure that their model is better than LiveBench ranks it and demand a rerun with more optimal settings, which is crazy because it already performed really great" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/dtej53d65ene1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dd9f1517a1661a1a7533874cd88daca591e7690"&gt;https://preview.redd.it/dtej53d65ene1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dd9f1517a1661a1a7533874cd88daca591e7690&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In case you're wondering right now it scores about a 66 global average but Qwen advertised it scores around 73 so maybe with more optimal settings it will get closer to that range&lt;/p&gt; &lt;p&gt;This rerun with be posted on Monday&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T04:11:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6dryj</id>
    <title>Mistral Small 24B did in 51 seconds what QwQ couldn't in 40 minutes</title>
    <updated>2025-03-08T09:41:19+00:00</updated>
    <author>
      <name>/u/2TierKeir</name>
      <uri>https://old.reddit.com/user/2TierKeir</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dryj/mistral_small_24b_did_in_51_seconds_what_qwq/"&gt; &lt;img alt="Mistral Small 24B did in 51 seconds what QwQ couldn't in 40 minutes" src="https://external-preview.redd.it/aGlvZGVrdDZzZm5lMc_Az5p3qLdEN__5qSL7XTQoE-2LI7eWZo3yGOsqXnkB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7745bb1240348e2c2b8426f85b17a2fe6e2edeed" title="Mistral Small 24B did in 51 seconds what QwQ couldn't in 40 minutes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/2TierKeir"&gt; /u/2TierKeir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9xkdwav2sfne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dryj/mistral_small_24b_did_in_51_seconds_what_qwq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dryj/mistral_small_24b_did_in_51_seconds_what_qwq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T09:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6a0s2</id>
    <title>Pov: when you overthink too much</title>
    <updated>2025-03-08T05:17:47+00:00</updated>
    <author>
      <name>/u/kernel348</name>
      <uri>https://old.reddit.com/user/kernel348</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"&gt; &lt;img alt="Pov: when you overthink too much" src="https://preview.redd.it/m9paekz5hene1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b886e3b4eb343a109cd3fef74702179d30c3c20d" title="Pov: when you overthink too much" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kernel348"&gt; /u/kernel348 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9paekz5hene1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T05:17:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6dzai</id>
    <title>Real-time token graph in Open WebUI</title>
    <updated>2025-03-08T09:56:58+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dzai/realtime_token_graph_in_open_webui/"&gt; &lt;img alt="Real-time token graph in Open WebUI" src="https://external-preview.redd.it/dm1rY2E3dWl1Zm5lMeNo1g2VbIy6NNGx_1T_ctYYVLkaFt3bwpFyaChfDLc3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c66a72211f8ad427c81d09e427101d7638dfd38" title="Real-time token graph in Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zscr76uiufne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dzai/realtime_token_graph_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dzai/realtime_token_graph_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T09:56:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j67bxt</id>
    <title>16x 3090s - It's alive!</title>
    <updated>2025-03-08T02:43:38+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"&gt; &lt;img alt="16x 3090s - It's alive!" src="https://b.thumbs.redditmedia.com/VvyYO_xrL0vczMCglIvOXlchOAjzJG3mEsXsV_k93PQ.jpg" title="16x 3090s - It's alive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j67bxt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T02:43:38+00:00</published>
  </entry>
</feed>
