<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-26T14:49:29+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kuzk3t</id>
    <title>Online inference is a privacy nightmare</title>
    <updated>2025-05-25T10:39:04+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I dont understand how big tech just convinced people to hand over so much stuff to be processed in plain text. Cloud storage at least can be all encrypted. But people have got comfortable sending emails, drafts, their deepest secrets, all in the open on some servers somewhere. Am I crazy? People were worried about posts and likes on social media for privacy but this is magnitudes larger in scope. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuzk3t/online_inference_is_a_privacy_nightmare/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuzk3t/online_inference_is_a_privacy_nightmare/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuzk3t/online_inference_is_a_privacy_nightmare/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T10:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvd0jr</id>
    <title>M3 Ultra Mac Studio Benchmarks (96gb VRAM, 60 GPU cores)</title>
    <updated>2025-05-25T21:03:03+00:00</updated>
    <author>
      <name>/u/procraftermc</name>
      <uri>https://old.reddit.com/user/procraftermc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I recently got the M3 Ultra Mac Studio (96 GB RAM, 60 core GPU). Here's its performance.&lt;/p&gt; &lt;p&gt;I loaded each model freshly in LMStudio, and input 30-40k tokens of Lorem Ipsum text (the text itself shouldn't matter, all that matters is token counts)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarking Results&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name &amp;amp; Size&lt;/th&gt; &lt;th align="left"&gt;Time to First Token (s)&lt;/th&gt; &lt;th align="left"&gt;Tokens / Second&lt;/th&gt; &lt;th align="left"&gt;Input Context Size (tokens)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 0.6b (bf16)&lt;/td&gt; &lt;td align="left"&gt;18.21&lt;/td&gt; &lt;td align="left"&gt;78.61&lt;/td&gt; &lt;td align="left"&gt;40240&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30b-a3b (8-bit)&lt;/td&gt; &lt;td align="left"&gt;67.74&lt;/td&gt; &lt;td align="left"&gt;34.62&lt;/td&gt; &lt;td align="left"&gt;40240&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma 3 27B (4-bit)&lt;/td&gt; &lt;td align="left"&gt;108.15&lt;/td&gt; &lt;td align="left"&gt;29.55&lt;/td&gt; &lt;td align="left"&gt;30869&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LLaMA4 Scout 17B-16E (4-bit)&lt;/td&gt; &lt;td align="left"&gt;111.33&lt;/td&gt; &lt;td align="left"&gt;33.85&lt;/td&gt; &lt;td align="left"&gt;32705&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral Large 123B (4-bit)&lt;/td&gt; &lt;td align="left"&gt;900.61&lt;/td&gt; &lt;td align="left"&gt;7.75&lt;/td&gt; &lt;td align="left"&gt;32705&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Additional Information&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Input was 30,000 - 40,000 tokens of Lorem Ipsum text&lt;/li&gt; &lt;li&gt;Model was reloaded with no prior caching&lt;/li&gt; &lt;li&gt;After caching, prompt processing (time to first token) dropped to almost zero&lt;/li&gt; &lt;li&gt;Prompt processing times on input &amp;lt;10,000 tokens was also workably low&lt;/li&gt; &lt;li&gt;Interface used was LM Studio&lt;/li&gt; &lt;li&gt;All models were 4-bit &amp;amp; MLX except Qwen3 0.6b and Qwen3 30b-a3b (they were bf16 and 8bit, respectively)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Token speeds were generally good, especially for MoE's like Qen 30b and Llama4. Of course, time-to-first-token was quite high as expected.&lt;/p&gt; &lt;p&gt;Loading models was way more efficient than I thought, I could load Mistral Large (4-bit) with 32k context using only ~70GB VRAM.&lt;/p&gt; &lt;p&gt;Feel free to request benchmarks for any model, I'll see if I can download and benchmark it :).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/procraftermc"&gt; /u/procraftermc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvd0jr/m3_ultra_mac_studio_benchmarks_96gb_vram_60_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvd0jr/m3_ultra_mac_studio_benchmarks_96gb_vram_60_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvd0jr/m3_ultra_mac_studio_benchmarks_96gb_vram_60_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T21:03:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvp0g1</id>
    <title>What's the latest in conversational voice-to-voice models that is self-hostable?</title>
    <updated>2025-05-26T08:07:18+00:00</updated>
    <author>
      <name>/u/surveypoodle</name>
      <uri>https://old.reddit.com/user/surveypoodle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been a bit out-of-touch for a while. Are self-hostable voice-to-voice models with a reasonably low latency still a farfetched pipedream or is there anything out there that works reasonably well without a robotic voice?&lt;/p&gt; &lt;p&gt;I don't mind buying an RTX4090 if that works but even okay with an RTX Pro 6000 if there is a good model out there.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/surveypoodle"&gt; /u/surveypoodle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvp0g1/whats_the_latest_in_conversational_voicetovoice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvp0g1/whats_the_latest_in_conversational_voicetovoice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvp0g1/whats_the_latest_in_conversational_voicetovoice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T08:07:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvsnj4</id>
    <title>UI + RAG solution for 5000 documents possible?</title>
    <updated>2025-05-26T12:04:18+00:00</updated>
    <author>
      <name>/u/Small_Caterpillar_50</name>
      <uri>https://old.reddit.com/user/Small_Caterpillar_50</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am investigating how to leverage my 5000 documents of strategy documents (market reports, strategy sessions, etc.). Files are PDFs, PPTX, and DOCS, with charts, pictures, tables, and texts.&lt;br /&gt; My use case is that when I receive a new market report, I want to query my knowledge base of the 5000 documents and ask: &amp;quot;Is there a new market player or new trends compared to current knowledge&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CURRENT UNDERSTANDING AFTER RESEARCH:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;My current research has shown that Openweb UI's built in knowledge base does not ingest the complex PDF and PPTX, then it works well with DOCX files.&lt;/li&gt; &lt;li&gt;Uploading the documents to google drive and use Gemini doest not seem to work neither, as there is a limit of Gemini in terms of how many documents it can manage within a context window. Same issue with Onedrive and Copilot.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;POPSSIBLE SOLUTIONS:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Local solution built with python: Building my own rag with &lt;a href="http://Unstructured.io"&gt;Unstructured.io&lt;/a&gt; to Document Loading &amp;amp; Parsing, Chunking, Colpali for Embedding Generation, Qdrant for vector database indexing, Colpali for Query Embedding, Qdrant Search for Vector Search (Retrieval), Ollama &amp;amp; OpenwebUI for Local LLMs Response Generation.&lt;/li&gt; &lt;li&gt;local n8n solution: Build something similar but with N8N for all the above.&lt;/li&gt; &lt;li&gt;Cloud solution: using Google's AI Cloud and Document AI suite to do all of the above.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;MY QUESTION:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I dont mind to spend the next month building and coding, as a learning journey, but for the use case above, would you mind guiding me which is the most appropriate solution as a relatively new to coding?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Small_Caterpillar_50"&gt; /u/Small_Caterpillar_50 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvsnj4/ui_rag_solution_for_5000_documents_possible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvsnj4/ui_rag_solution_for_5000_documents_possible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvsnj4/ui_rag_solution_for_5000_documents_possible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T12:04:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvn51x</id>
    <title>Vector Space - Llama running locally on Apple Neural Engine</title>
    <updated>2025-05-26T06:04:10+00:00</updated>
    <author>
      <name>/u/Glad-Speaker3006</name>
      <uri>https://old.reddit.com/user/Glad-Speaker3006</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvn51x/vector_space_llama_running_locally_on_apple/"&gt; &lt;img alt="Vector Space - Llama running locally on Apple Neural Engine" src="https://external-preview.redd.it/chjzdPRsgKclskhkJtHyR9G4ascbrd4AkBc1D_gB_VY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3aac9a23637bf6c47f86301fe243a6a9117af54b" title="Vector Space - Llama running locally on Apple Neural Engine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/6yhsn8x0g23f1.png?width=1368&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=42f7f189fdda7cabc2dd3055a55917468a9beba9"&gt;Llama 3.2 1B Full Precision (float16) running on iPhone 14 Pro Max&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Core ML is Apple‚Äôs official way to run Machine Learning models on device, and also appears to be the only way to engage the Neural Engine, which is a powerful NPU installed on every iPhone/iPad that is capable of performing tens of billions of computations per second.&lt;/p&gt; &lt;p&gt;In recent years, Apple has improved support for Large Language Models (and other transformer-based models) to run on device by introducing Stateful models, quantizations, etc. Despite these improvements, developers still face hurdles and a steep learning curve if they try to incorporate a large language model on-device. This leads to an (often paid) network API call for even the most basic AI-functions. For this reason, an Agentic AI often has to charge tens of dollars per month while still limiting usage for the user.&lt;/p&gt; &lt;p&gt;I have founded the Vector Space project to conquer the above issues. My Goal is two folds:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Enable users to use AI (marginally) freely and smoothly&lt;/li&gt; &lt;li&gt;Enable small developers o build agentic apps without cost, without having to understand how AI works under the hood, and without having to worry about API key safety.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1kvn51x/video/kagsls50h23f1/player"&gt;Llama 3.2 1B Full Precision (float16) on the Vector Space App&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To achieve the above goals, Vector Space will provide&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Architecture and tools that can convert models to Core ML format that can be run on Apple Neural Engine.&lt;/li&gt; &lt;li&gt;Swift Package that can run performant model inference.&lt;/li&gt; &lt;li&gt;App for users to directly download and manage model on Device, and for developers and enthusiasts to try out different models directly on iPhone. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My goal is NOT to:&lt;/p&gt; &lt;p&gt;Completely replace server-based AI, where models with hundreds of billions of parameters can be hosted, with context length of hundreds of k. Online models will still excel at complex tasks. However, it is also important to note that not every user is asking AI to do programing and math challenges. &lt;/p&gt; &lt;p&gt;Current Progress:&lt;/p&gt; &lt;p&gt;I have already preliminarily supported Llama 3.2 1B in full precision. The Model runs on ANE and supports MLState.&lt;/p&gt; &lt;p&gt;I am pleased to release the TestFlight Beta of the App mentioned in goal #3 above so you can try it out directly on your iPhone. &lt;/p&gt; &lt;p&gt;&lt;a href="https://testflight.apple.com/join/HXyt2bjU"&gt;https://testflight.apple.com/join/HXyt2bjU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you decide to try out the TestFlight version, please note the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;We do NOT collect any information about your chat messages. It remains completely on device and/or in your iCloud.&lt;/li&gt; &lt;li&gt;The first model load into memory (after downloading) will take about 1-2 minutes. Subsequent load will only take a couple seconds.&lt;/li&gt; &lt;li&gt;Chat history would not persist across app launches.&lt;/li&gt; &lt;li&gt;I cannot guarantee the downloaded app will continue work when I release the next update. You might need to delete and redownload the app when an update is released in the future.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Next Step:&lt;/p&gt; &lt;p&gt;I will be working on a quantized version of Llama 3.2 1B that is expected to have significant inference speed improvement. I will then provide a much wider selection of models available for download.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glad-Speaker3006"&gt; /u/Glad-Speaker3006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvn51x/vector_space_llama_running_locally_on_apple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvn51x/vector_space_llama_running_locally_on_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvn51x/vector_space_llama_running_locally_on_apple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T06:04:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvsy37</id>
    <title>Has anyone come across a good (open source) "AI native" document editor?</title>
    <updated>2025-05-26T12:19:46+00:00</updated>
    <author>
      <name>/u/sammcj</name>
      <uri>https://old.reddit.com/user/sammcj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm interested to know if anyone has found a slick open source document editor (&amp;quot;word processor&amp;quot;) that has features we've come to expect in the likes of our IDEs and conversational interfaces.&lt;/p&gt; &lt;p&gt;I'd love if there was an app (ideally native, not web based) that gave a Word / Pages / iA Writer like experience with good, in context tab-complete, section rewriting, idea branching etc...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sammcj"&gt; /u/sammcj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvsy37/has_anyone_come_across_a_good_open_source_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvsy37/has_anyone_come_across_a_good_open_source_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvsy37/has_anyone_come_across_a_good_open_source_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T12:19:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvvwqu</id>
    <title>I created a purely client-side, browser-based PDF to Markdown library with local AI rewrites</title>
    <updated>2025-05-26T14:34:47+00:00</updated>
    <author>
      <name>/u/Designer_Athlete7286</name>
      <uri>https://old.reddit.com/user/Designer_Athlete7286</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h3&gt;I created a purely client-side, browser-based PDF to Markdown library with local AI rewrites&lt;/h3&gt; &lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm excited to share a project I've been working on: &lt;strong&gt;Extract2MD&lt;/strong&gt;. It's a client-side JavaScript library that converts PDFs into Markdown, but with a few powerful twists. The biggest feature is that it can use a local large language model (LLM) running entirely in the browser to enhance and reformat the output, so no data ever leaves your machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.google.com/search?q=https://github.com/hashangit/Extract2MD"&gt;Link to GitHub Repo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes it different?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of a one-size-fits-all approach, I've designed it around 5 specific &amp;quot;scenarios&amp;quot; depending on your needs:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;strong&gt;Quick Convert Only&lt;/strong&gt;: This is for speed. It uses PDF.js to pull out selectable text and quickly convert it to Markdown. Best for simple, text-based PDFs.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;High Accuracy Convert Only&lt;/strong&gt;: For the tough stuff like scanned documents or PDFs with lots of images. This uses Tesseract.js for Optical Character Recognition (OCR) to extract text.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Quick Convert + LLM&lt;/strong&gt;: This takes the fast extraction from scenario 1 and pipes it through a local AI (using WebLLM) to clean up the formatting, fix structural issues, and make the output much cleaner.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;High Accuracy + LLM&lt;/strong&gt;: Same as above, but for OCR output. It uses the AI to enhance the text extracted by Tesseract.js.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Combined + LLM (Recommended)&lt;/strong&gt;: This is the most comprehensive option. It uses &lt;em&gt;both&lt;/em&gt; PDF.js and Tesseract.js, then feeds both results to the LLM with a special prompt that tells it how to best combine them. This generally produces the best possible result by leveraging the strengths of both extraction methods.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here‚Äôs a quick look at how simple it is to use:&lt;/p&gt; &lt;p&gt;```javascript import Extract2MDConverter from 'extract2md';&lt;/p&gt; &lt;p&gt;// For the most comprehensive conversion const markdown = await Extract2MDConverter.combinedConvertWithLLM(pdfFile);&lt;/p&gt; &lt;p&gt;// Or if you just need fast, simple conversion const quickMarkdown = await Extract2MDConverter.quickConvertOnly(pdfFile); ```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;PDF.js&lt;/strong&gt; for standard text extraction.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tesseract.js&lt;/strong&gt; for OCR on images and scanned docs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;WebLLM&lt;/strong&gt; for the client-side AI enhancements, running models like Qwen entirely in the browser.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's also highly configurable. You can set custom prompts for the LLM, adjust OCR settings, and even bring your own custom models. It also has full TypeScript support and a detailed progress callback system for UI integration.&lt;/p&gt; &lt;p&gt;For anyone using an older version, I've kept the legacy API available but wrapped it so migration is smooth.&lt;/p&gt; &lt;p&gt;The project is open-source under the &lt;strong&gt;MIT License&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I'd love for you all to check it out, give me some feedback, or even contribute! You can find any issues on the &lt;a href="https://www.google.com/search?q=https://github.com/hashangit/Extract2MD/issues"&gt;GitHub Issues page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Designer_Athlete7286"&gt; /u/Designer_Athlete7286 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvvwqu/i_created_a_purely_clientside_browserbased_pdf_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvvwqu/i_created_a_purely_clientside_browserbased_pdf_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvvwqu/i_created_a_purely_clientside_browserbased_pdf_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T14:34:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvv026</id>
    <title>Should I resize the image before sending it to Qwen VL 7B? Would it give better results?</title>
    <updated>2025-05-26T13:57:13+00:00</updated>
    <author>
      <name>/u/Zealousideal-Feed383</name>
      <uri>https://old.reddit.com/user/Zealousideal-Feed383</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using Qwen model to get transactional data from bank pdfs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Feed383"&gt; /u/Zealousideal-Feed383 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvv026/should_i_resize_the_image_before_sending_it_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvv026/should_i_resize_the_image_before_sending_it_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvv026/should_i_resize_the_image_before_sending_it_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T13:57:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvnt5u</id>
    <title>Best Uncensored model for 42GB of VRAM</title>
    <updated>2025-05-26T06:47:23+00:00</updated>
    <author>
      <name>/u/KeinNiemand</name>
      <uri>https://old.reddit.com/user/KeinNiemand</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the current best uncensored model for &amp;quot;Roleplay&amp;quot;.&lt;br /&gt; Well Not really roleplay in the sense that I'm roleplaying with an AI character with a character card and all that. Usually I'm more doing like some sort of choose your own adventure or text adventure thing where I give the AI some basic prompt about the world, let it generate and then I tell it what I want my character to do, there's some roleplay involved but it's not the typical me downloading or making a character card and then roleplaying with a singular AI character.&lt;br /&gt; I care more about how well the AI (in terms of creativity) does with short, relatively basic prompts then how well it performs when all my prompts are long, elaborate and well written. &lt;/p&gt; &lt;p&gt;I've got 42GB of VRAM (1 5090 + 1 3080 10GB), so it should probably a 70B model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KeinNiemand"&gt; /u/KeinNiemand &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnt5u/best_uncensored_model_for_42gb_of_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnt5u/best_uncensored_model_for_42gb_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnt5u/best_uncensored_model_for_42gb_of_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T06:47:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kv6jjk</id>
    <title>Fine-tuning HuggingFace SmolVLM (256M) to control the robot</title>
    <updated>2025-05-25T16:24:19+00:00</updated>
    <author>
      <name>/u/Complex-Indication</name>
      <uri>https://old.reddit.com/user/Complex-Indication</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv6jjk/finetuning_huggingface_smolvlm_256m_to_control/"&gt; &lt;img alt="Fine-tuning HuggingFace SmolVLM (256M) to control the robot" src="https://external-preview.redd.it/b29vMmxwbTNmeTJmMSsIw5Jo6JnOhGNxnxMg56RLkadqgoRaNULw7zamXe9N.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d86f3549101d3f35fb75d562c3bd121473237386" title="Fine-tuning HuggingFace SmolVLM (256M) to control the robot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with tiny LLMs and VLMs for a while now, perhaps some of your saw my earlier post here about running &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1g9seqf/a_tiny_language_model_260k_params_is_running/"&gt;LLM on ESP32 for Dalek&lt;/a&gt; Halloween prop. This time I decided to use HuggingFace really tiny (256M parameters!) SmolVLM to control robot just from camera frames. The input is a prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Based on the image choose one action: forward, left, right, back. If there is an obstacle blocking the view, choose back. If there is an obstacle on the left, choose right. If there is an obstacle on the right, choose left. If there are no obstacles, choose forward. Based on the image choose one action: forward, left, right, back. If there is an obstacle blocking the view, choose back. If there is an obstacle on the left, choose right. If there is an obstacle on the right, choose left. If there are no obstacles, choose forward.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;and an image from Raspberry Pi Camera Module 2. The output is text.&lt;/p&gt; &lt;p&gt;The base model didn't work at all, but after collecting some data (200 images) and fine-tuning with LORA, it actually (to my surprise) started working!&lt;/p&gt; &lt;p&gt;Currently the model runs on local PC and the data is exchanged between Raspberry Pi Zero 2 and the PC over local network. I know for a fact I can run SmolVLM fast enough on Raspberry Pi 5, but I was not able to do it due to power issues (Pi 5 is very power hungry), so I decided to leave it for the next video.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Complex-Indication"&gt; /u/Complex-Indication &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9s2q9nm3fy2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv6jjk/finetuning_huggingface_smolvlm_256m_to_control/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kv6jjk/finetuning_huggingface_smolvlm_256m_to_control/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T16:24:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvrgjv</id>
    <title>Consensus on best local STT?</title>
    <updated>2025-05-26T10:54:23+00:00</updated>
    <author>
      <name>/u/That_Em</name>
      <uri>https://old.reddit.com/user/That_Em</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, I‚Äôm currently devving a tool that needs STT. I‚Äôm currently using Whispercpp/whisper for transcription (large v3), whisperx for alignment/diarization/prosodic analysis, and embeddings and llms for the rest.&lt;/p&gt; &lt;p&gt;I find Whisper does a good job at transcription - however speaker identification/diarization with whisperx kinda sucks. Used pyannote before but was heaps slower and still not ideal. Is there some good model to do this kind of analysis or is this what I‚Äôm stuck with?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/That_Em"&gt; /u/That_Em &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvrgjv/consensus_on_best_local_stt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvrgjv/consensus_on_best_local_stt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvrgjv/consensus_on_best_local_stt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T10:54:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvc9w6</id>
    <title>Cheapest Ryzen AI Max+ 128GB yet at $1699. Ships June 10th.</title>
    <updated>2025-05-25T20:30:17+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bosgamepc.com/products/bosgame-m5-ai-mini-desktop-ryzen-ai-max-395"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvc9w6/cheapest_ryzen_ai_max_128gb_yet_at_1699_ships/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvc9w6/cheapest_ryzen_ai_max_128gb_yet_at_1699_ships/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T20:30:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvmrgu</id>
    <title>nvidia/AceReason-Nemotron-7B ¬∑ Hugging Face</title>
    <updated>2025-05-26T05:40:40+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvmrgu/nvidiaacereasonnemotron7b_hugging_face/"&gt; &lt;img alt="nvidia/AceReason-Nemotron-7B ¬∑ Hugging Face" src="https://external-preview.redd.it/E-gtAbU28GDV0NLa926rL1ecWFn9v0jJKe5iqmUNFzo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52002294beca04a31b018ffdca2c01eba72b139a" title="nvidia/AceReason-Nemotron-7B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/AceReason-Nemotron-7B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvmrgu/nvidiaacereasonnemotron7b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvmrgu/nvidiaacereasonnemotron7b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T05:40:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvnf46</id>
    <title>QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning</title>
    <updated>2025-05-26T06:22:26+00:00</updated>
    <author>
      <name>/u/Fancy_Fanqi77</name>
      <uri>https://old.reddit.com/user/Fancy_Fanqi77</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnf46/qwenlongl1_towards_longcontext_large_reasoning/"&gt; &lt;img alt="QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning" src="https://external-preview.redd.it/4fgEuTMOx_oXXqA0kQVQE7o892NJc01radrTqR_KFkw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=084020eb1aeecf203d2fdf8aa8277a361260d5b2" title="QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/pekhw95jl23f1.png?width=1480&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ce794b4e65f4ab48d59fe760345c3dce91d329b"&gt;https://preview.redd.it/pekhw95jl23f1.png?width=1480&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ce794b4e65f4ab48d59fe760345c3dce91d329b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1-32B"&gt;ü§ó QwenLong-L1-32B&lt;/a&gt; is the first long-context Large Reasoning Model (LRM) trained with reinforcement learning for long-context document reasoning tasks. Experiments on seven long-context DocQA benchmarks demonstrate that &lt;strong&gt;QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini and Qwen3-235B-A22B, achieving performance on par with Claude-3.7-Sonnet-Thinking&lt;/strong&gt;, demonstrating leading performance among state-of-the-art LRMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fancy_Fanqi77"&gt; /u/Fancy_Fanqi77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnf46/qwenlongl1_towards_longcontext_large_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnf46/qwenlongl1_towards_longcontext_large_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnf46/qwenlongl1_towards_longcontext_large_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T06:22:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvf8d2</id>
    <title>Nvidia RTX PRO 6000 Workstation 96GB - Benchmarks</title>
    <updated>2025-05-25T22:46:05+00:00</updated>
    <author>
      <name>/u/fuutott</name>
      <uri>https://old.reddit.com/user/fuutott</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Posting here as it's something I would like to know before I acquired it. No regrets.&lt;/p&gt; &lt;p&gt;RTX 6000 PRO 96GB @ 600W - Platform w5-3435X rubber dinghy rapids&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;zero context input - &amp;quot;who was copernicus?&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;40K token input 40000 tokens of lorem ipsum - &lt;a href="https://pastebin.com/yAJQkMzT"&gt;https://pastebin.com/yAJQkMzT&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;model settings : flash attention enabled - &lt;strong&gt;128K context&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;LM Studio 0.3.16 beta - cuda 12 runtime 1.33.0&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Zero Context (tok/sec)&lt;/th&gt; &lt;th&gt;First Token (s)&lt;/th&gt; &lt;th&gt;40K Context (tok/sec)&lt;/th&gt; &lt;th&gt;First Token 40K (s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;llama-3.3-70b-instruct@q8_0 64000 context Q8 KV cache (81GB VRAM)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;9.72&lt;/td&gt; &lt;td&gt;0.45&lt;/td&gt; &lt;td&gt;3.61&lt;/td&gt; &lt;td&gt;66.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;gigaberg-mistral-large-123b@Q4_K_S 64000 context Q8 KV cache (90.8GB VRAM)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;18.61&lt;/td&gt; &lt;td&gt;0.14&lt;/td&gt; &lt;td&gt;11.01&lt;/td&gt; &lt;td&gt;71.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;meta/llama-3.3-70b@q4_k_m (84.1GB VRAM)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;28.56&lt;/td&gt; &lt;td&gt;0.11&lt;/td&gt; &lt;td&gt;18.14&lt;/td&gt; &lt;td&gt;33.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-32b@&lt;strong&gt;BF16&lt;/strong&gt; 40960 context&lt;/td&gt; &lt;td&gt;21.55&lt;/td&gt; &lt;td&gt;0.26&lt;/td&gt; &lt;td&gt;16.24&lt;/td&gt; &lt;td&gt;19.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;qwen3-32b-128k@q8_k_xl&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;33.01&lt;/td&gt; &lt;td&gt;0.17&lt;/td&gt; &lt;td&gt;21.73&lt;/td&gt; &lt;td&gt;20.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;gemma-3-27b-instruct-qat@Q4_0&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;45.25&lt;/td&gt; &lt;td&gt;0.08&lt;/td&gt; &lt;td&gt;&lt;strong&gt;45.44&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;15.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;qwq-32b@q4_k_m&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;53.18&lt;/td&gt; &lt;td&gt;0.07&lt;/td&gt; &lt;td&gt;33.81&lt;/td&gt; &lt;td&gt;18.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;deepseek-r1-distill-qwen-32b@q4_k_m&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;53.91&lt;/td&gt; &lt;td&gt;0.07&lt;/td&gt; &lt;td&gt;33.48&lt;/td&gt; &lt;td&gt;18.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Llama-4-Scout-17B-16E-Instruct@Q4_K_M (Q8 KV cache)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;68.22&lt;/td&gt; &lt;td&gt;0.08&lt;/td&gt; &lt;td&gt;46.26&lt;/td&gt; &lt;td&gt;30.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;google_gemma-3-12b-it-Q8_0&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;68.47&lt;/td&gt; &lt;td&gt;0.06&lt;/td&gt; &lt;td&gt;53.34&lt;/td&gt; &lt;td&gt;11.53&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;mistral-small-3.1-24b-instruct-2503@q4_k_m ‚Äì my beloved&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;79.00&lt;/td&gt; &lt;td&gt;0.03&lt;/td&gt; &lt;td&gt;51.71&lt;/td&gt; &lt;td&gt;11.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mistral-small-3.1-24b-instruct-2503@q4_k_m ‚Äì &lt;strong&gt;400W CAP&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;78.02&lt;/td&gt; &lt;td&gt;0.11&lt;/td&gt; &lt;td&gt;49.78&lt;/td&gt; &lt;td&gt;14.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mistral-small-3.1-24b-instruct-2503@q4_k_m ‚Äì &lt;strong&gt;300W CAP&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;69.02&lt;/td&gt; &lt;td&gt;0.12&lt;/td&gt; &lt;td&gt;39.78&lt;/td&gt; &lt;td&gt;18.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;qwen3-14b-128k@q4_k_m&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;107.51&lt;/td&gt; &lt;td&gt;0.22&lt;/td&gt; &lt;td&gt;61.57&lt;/td&gt; &lt;td&gt;10.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;qwen3-30b-a3b-128k@q8_k_xl&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;122.95&lt;/td&gt; &lt;td&gt;0.25&lt;/td&gt; &lt;td&gt;64.93&lt;/td&gt; &lt;td&gt;7.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;qwen3-8b-128k@q4_k_m&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;153.63&lt;/td&gt; &lt;td&gt;0.06&lt;/td&gt; &lt;td&gt;79.31&lt;/td&gt; &lt;td&gt;8.42&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fuutott"&gt; /u/fuutott &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvf8d2/nvidia_rtx_pro_6000_workstation_96gb_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvf8d2/nvidia_rtx_pro_6000_workstation_96gb_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvf8d2/nvidia_rtx_pro_6000_workstation_96gb_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T22:46:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvj0nt</id>
    <title>New LocalLLM Hardware complete</title>
    <updated>2025-05-26T02:04:11+00:00</updated>
    <author>
      <name>/u/ubrtnk</name>
      <uri>https://old.reddit.com/user/ubrtnk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvj0nt/new_localllm_hardware_complete/"&gt; &lt;img alt="New LocalLLM Hardware complete" src="https://b.thumbs.redditmedia.com/ZBd82qXPCpl5Y406kiGaXRlPIUh1aiKasaz1i4H363A.jpg" title="New LocalLLM Hardware complete" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I spent this last week at Red Hats conference with this hardware sitting at home waiting for me. Finally got it put together. The conference changed my thought on what I was going to deploy but interest in everyone's thoughts.&lt;/p&gt; &lt;p&gt;The hardware is an AMD Ryzen 7 5800x with 64GB of ram, 2x 3909Ti that my best friend gave me (2x 4.0x8) with a 500gb boot and 4TB nvme. &lt;/p&gt; &lt;p&gt;The rest of the lab isal also available for ancillary things.&lt;/p&gt; &lt;p&gt;At the conference, I shifted my session from Ansible and Openshift to as much vLLM as I could and it's gotten me excited for IT Work for the first time in a while. &lt;/p&gt; &lt;p&gt;Currently still setting thingd up - got the Qdrant DB installed on the proxmox cluster in the rack. Plan to use vLLM/ HF with Open-WebUI for a GPT front end for the rest of the family with RAG, TTS/STT and maybe even Home Assistant voice.&lt;/p&gt; &lt;p&gt;Any recommendations? Ivr got nvidia-smi working g and both gpus are detected. Got them power limited ton300w each with the persistence configured (I have a 1500w psu but no need to blow a breaker lol). Im coming from my M3 Ultra Mac Studio running Ollama, that's really for my music studio - wanted to separate out the functions.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ubrtnk"&gt; /u/ubrtnk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kvj0nt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvj0nt/new_localllm_hardware_complete/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvj0nt/new_localllm_hardware_complete/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T02:04:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvskpq</id>
    <title>Leveling Up: From RAG to an AI Agent</title>
    <updated>2025-05-26T12:00:27+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvskpq/leveling_up_from_rag_to_an_ai_agent/"&gt; &lt;img alt="Leveling Up: From RAG to an AI Agent" src="https://preview.redd.it/qourugv0943f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9dd5856da1c363f33ad9545c0f33914cbc5403a" title="Leveling Up: From RAG to an AI Agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I've been exploring more advanced ways to use AI, and recently I made a big jump - moving from the usual RAG (Retrieval-Augmented Generation) approach to something more powerful: an &lt;strong&gt;AI Agent that uses a real web browser to search the internet and get stuff done on its own&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;In my last guide (&lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-LightRAG.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-LightRAG.md&lt;/a&gt;), I showed how we could manually gather info online and feed it into a RAG pipeline. It worked well, but it still needed a human in the loop.&lt;/p&gt; &lt;p&gt;This time, the AI Agent does &lt;em&gt;everything&lt;/em&gt; by itself.&lt;/p&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;p&gt;I asked it the same question - &lt;em&gt;‚ÄúHow much tax was collected in the US in 2024?‚Äù&lt;/em&gt;&lt;/p&gt; &lt;p&gt;The Agent opened a browser, went to Google, searched the query, clicked through results, read the content, and gave me a clean, accurate answer.&lt;/p&gt; &lt;p&gt;I didn‚Äôt touch the keyboard after asking the question.&lt;/p&gt; &lt;p&gt;I put together a guide so you can run this setup on your own bare metal server with an Nvidia GPU. It takes just a few minutes:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-AI-AGENT.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-AI-AGENT.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üõ†Ô∏è What you'll spin up:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A server running &lt;strong&gt;Sbnb Linux&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;A VM with &lt;strong&gt;Ubuntu 24.04&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Ollama with default model &lt;code&gt;qwen2.5:7b&lt;/code&gt; for local GPU-accelerated inference (no cloud, no API calls)&lt;/li&gt; &lt;li&gt;The open-source &lt;strong&gt;Browser Use AI Agent&lt;/strong&gt; &lt;a href="https://github.com/browser-use/web-ui"&gt;https://github.com/browser-use/web-ui&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Give it a shot and let me know how it goes! Curious to hear what use cases you come up with (for more ideas and examples of AI Agents, be sure to follow the amazing Browser Use project!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qourugv0943f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvskpq/leveling_up_from_rag_to_an_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvskpq/leveling_up_from_rag_to_an_ai_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T12:00:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvtfco</id>
    <title>lmarena.ai responded to Cohere's paper a couple of weeks ago.</title>
    <updated>2025-05-26T12:44:15+00:00</updated>
    <author>
      <name>/u/JustTellingUWatHapnd</name>
      <uri>https://old.reddit.com/user/JustTellingUWatHapnd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://blog.lmarena.ai/blog/2025/our-response"&gt;I think we all missed it.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In unrelated news, they just secured &lt;a href="https://techcrunch.com/2025/05/21/lm-arena-the-organization-behind-popular-ai-leaderboards-lands-100m"&gt;$100M in funding at $600M valuation&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustTellingUWatHapnd"&gt; /u/JustTellingUWatHapnd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvtfco/lmarenaai_responded_to_coheres_paper_a_couple_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvtfco/lmarenaai_responded_to_coheres_paper_a_couple_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvtfco/lmarenaai_responded_to_coheres_paper_a_couple_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T12:44:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvulw7</id>
    <title>Teortaxes gets a direct denial</title>
    <updated>2025-05-26T13:39:42+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvulw7/teortaxes_gets_a_direct_denial/"&gt; &lt;img alt="Teortaxes gets a direct denial" src="https://external-preview.redd.it/tt677u92R9MEYtrBNDcks9ssLBDK7B5sGTxkGz8ubRE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0290749a4078a3e7cda4e7fec66b86eaf8dcbc9e" title="Teortaxes gets a direct denial" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/teortaxesTex/status/1926994950278807565"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvulw7/teortaxes_gets_a_direct_denial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvulw7/teortaxes_gets_a_direct_denial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T13:39:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvqgpv</id>
    <title>Deepseek R2 might be coming soon, unsloth released an article about deepseek v3 -05-26</title>
    <updated>2025-05-26T09:48:05+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It should be coming soon! &lt;a href="https://docs.unsloth.ai/basics/deepseek-v3-0526-how-to-run-locally"&gt;https://docs.unsloth.ai/basics/deepseek-v3-0526-how-to-run-locally&lt;/a&gt;&lt;br /&gt; opus 4 level? I think v3 0526 should be out this week, actually i think it is probable that it will be like qwen, reasoning and nonthinking will be together‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvqgpv/deepseek_r2_might_be_coming_soon_unsloth_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvqgpv/deepseek_r2_might_be_coming_soon_unsloth_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvqgpv/deepseek_r2_might_be_coming_soon_unsloth_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T09:48:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvknlo</id>
    <title>Speechless: Speech Instruction Training Without Speech for Low Resource Languages</title>
    <updated>2025-05-26T03:36:39+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvknlo/speechless_speech_instruction_training_without/"&gt; &lt;img alt="Speechless: Speech Instruction Training Without Speech for Low Resource Languages" src="https://preview.redd.it/ju7kqbqjq13f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24c19692c9e98b21bf15c0e3f564d6800ac0fa76" title="Speechless: Speech Instruction Training Without Speech for Low Resource Languages" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, it‚Äôs me from &lt;strong&gt;Menlo Research&lt;/strong&gt; again üëã. Today I want to share some news + a new model!&lt;/p&gt; &lt;p&gt;Exciting news - our paper &lt;em&gt;‚ÄúSpeechLess‚Äù&lt;/em&gt; just got accepted to &lt;strong&gt;Interspeech 2025&lt;/strong&gt;, and we‚Äôve finished the camera-ready version! üéâ&lt;/p&gt; &lt;p&gt;The idea came out of a challenge we faced while building a speech instruction model - we didn‚Äôt have enough speech instruction data for our use case. That got us thinking: Could we train the model entirely using synthetic data?&lt;/p&gt; &lt;p&gt;That‚Äôs how &lt;strong&gt;SpeechLess&lt;/strong&gt; was born.&lt;br /&gt; &lt;strong&gt;Method Overview (with diagrams in the paper):&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Convert real speech ‚Üí discrete tokens (train a quantizer)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Convert text ‚Üí discrete tokens (train SpeechLess to simulate speech tokens from text)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Use this pipeline (text ‚Üí synthetic speech tokens) to train a LLM on speech instructions- just like training any other language model.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Training on fully synthetic speech tokens is surprisingly effective - performance holds up, and it opens up new possibilities for building speech systems in &lt;strong&gt;low-resource settings&lt;/strong&gt; where collecting audio data is difficult or expensive.&lt;/p&gt; &lt;p&gt;We hope this helps other teams in similar situations and inspires more exploration of synthetic data in speech applications.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;br /&gt; - Paper: &lt;a href="https://arxiv.org/abs/2502.14669"&gt;https://arxiv.org/abs/2502.14669&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Speechless Model: &lt;a href="https://huggingface.co/Menlo/Speechless-llama3.2-v0.1"&gt;https://huggingface.co/Menlo/Speechless-llama3.2-v0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Dataset: &lt;a href="https://huggingface.co/datasets/Menlo/Ichigo-pretrain-tokenized-v0.1"&gt;https://huggingface.co/datasets/Menlo/Ichigo-pretrain-tokenized-v0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- LLM: &lt;a href="https://huggingface.co/Menlo/Ichigo-llama3.1-8B-v0.5"&gt;https://huggingface.co/Menlo/Ichigo-llama3.1-8B-v0.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Github: &lt;a href="https://github.com/menloresearch/ichigo"&gt;https://github.com/menloresearch/ichigo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ju7kqbqjq13f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvknlo/speechless_speech_instruction_training_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvknlo/speechless_speech_instruction_training_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T03:36:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvoobg</id>
    <title>If only its true...</title>
    <updated>2025-05-26T07:44:42+00:00</updated>
    <author>
      <name>/u/Famous-Associate-436</name>
      <uri>https://old.reddit.com/user/Famous-Associate-436</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/YouJiacheng/status/1926885863952159102"&gt;https://x.com/YouJiacheng/status/1926885863952159102&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Deepseek-v3-0526, some guy saw this on changelog&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Famous-Associate-436"&gt; /u/Famous-Associate-436 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvoobg/if_only_its_true/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvoobg/if_only_its_true/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvoobg/if_only_its_true/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T07:44:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvqrzl</id>
    <title>AI Baby Monitor ‚Äì fully local Video-LLM nanny (beeps when safety rules are violated)</title>
    <updated>2025-05-26T10:09:06+00:00</updated>
    <author>
      <name>/u/CheeringCheshireCat</name>
      <uri>https://old.reddit.com/user/CheeringCheshireCat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvqrzl/ai_baby_monitor_fully_local_videollm_nanny_beeps/"&gt; &lt;img alt="AI Baby Monitor ‚Äì fully local Video-LLM nanny (beeps when safety rules are violated)" src="https://external-preview.redd.it/dXQydzR0cjNwMzNmMVMRslQYMYRN8ZJ1qBgR4-LlFEA6jckhHIJ4it6HP21k.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07fe467bd7f592919e65184006ed23558a3fe52e" title="AI Baby Monitor ‚Äì fully local Video-LLM nanny (beeps when safety rules are violated)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks!&lt;/p&gt; &lt;p&gt;I‚Äôve hacked together a VLM video nanny, that watches a video stream(s) and predefined set of safety instructions, and makes a beep sound if the instructions are violated.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/zeenolife/ai-baby-monitor"&gt;https://github.com/zeenolife/ai-baby-monitor&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I built it?&lt;/strong&gt;&lt;br /&gt; First day we assembled the crib, my daughter tried to climb over the rail. I got a bit paranoid about constantly watching her. So I thought of an additional eye that would actively watch her, while parent is semi-actively alert.&lt;br /&gt; It's not meant to be a replacement for an adult supervision, more of a supplement, thus just a &amp;quot;beep&amp;quot; sound, so that you could quickly turn back attention to the baby when you got a bit distracted.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works?&lt;/strong&gt;&lt;br /&gt; I'm using Qwen 2.5VL(empirically it works better) and vLLM. Redis is used to orchestrate video and llm log streams. Streamlit for UI.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Funny bit&lt;/strong&gt;&lt;br /&gt; I've also used it to monitor my smartphone usage. When you subconsciously check on your phone, it beeps :)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Further plans&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Add support for other backends apart from vLLM&lt;/li&gt; &lt;li&gt;Gemma 3n looks rather promising&lt;/li&gt; &lt;li&gt;Add support for image based &amp;quot;no-go-zones&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feedback is welcome :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CheeringCheshireCat"&gt; /u/CheeringCheshireCat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gzn6itr3p33f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvqrzl/ai_baby_monitor_fully_local_videollm_nanny_beeps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvqrzl/ai_baby_monitor_fully_local_videollm_nanny_beeps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T10:09:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvnti4</id>
    <title>Open-source project that use LLM as deception system</title>
    <updated>2025-05-26T06:48:01+00:00</updated>
    <author>
      <name>/u/mario_candela</name>
      <uri>https://old.reddit.com/user/mario_candela</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone üëã&lt;/p&gt; &lt;p&gt;I wanted to share a project I've been working on that I think you'll find really interesting. It's called Beelzebub, an open-source honeypot framework that uses LLMs to create incredibly realistic and dynamic deception environments.&lt;/p&gt; &lt;p&gt;By integrating LLMs, it can mimic entire operating systems and interact with attackers in a super convincing way. Imagine an SSH honeypot where the LLM provides plausible responses to commands, even though nothing is actually executed on a real system.&lt;/p&gt; &lt;p&gt;The goal is to keep attackers engaged for as long as possible, diverting them from your real systems and collecting valuable, real-world data on their tactics, techniques, and procedures. We've even had success capturing real threat actors with it!&lt;/p&gt; &lt;p&gt;I'd love for you to try it out, give it a star on GitHub, and maybe even contribute! Your feedback,&lt;br /&gt; especially from an LLM-centric perspective, would be incredibly valuable as we continue to develop it.&lt;/p&gt; &lt;p&gt;You can find the project here:&lt;/p&gt; &lt;p&gt;üëâ GitHub:&lt;a href="https://github.com/mariocandela/beelzebub"&gt;https://github.com/mariocandela/beelzebub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think in the comments! Do you have ideas for new LLM-powered honeypot features?&lt;/p&gt; &lt;p&gt;Thanks for your time! üòä&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mario_candela"&gt; /u/mario_candela &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnti4/opensource_project_that_use_llm_as_deception/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnti4/opensource_project_that_use_llm_as_deception/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnti4/opensource_project_that_use_llm_as_deception/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T06:48:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvpwq3</id>
    <title>Deepseek v3 0526?</title>
    <updated>2025-05-26T09:09:20+00:00</updated>
    <author>
      <name>/u/Stock_Swimming_6015</name>
      <uri>https://old.reddit.com/user/Stock_Swimming_6015</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvpwq3/deepseek_v3_0526/"&gt; &lt;img alt="Deepseek v3 0526?" src="https://external-preview.redd.it/fxYCW6fqdbJ5RWjh_x1fsIyj0ZtZFx8MOAvXVxIw2PE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e74df95b54af72feafa558281ef5e11bc4e8a7c" title="Deepseek v3 0526?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stock_Swimming_6015"&gt; /u/Stock_Swimming_6015 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://docs.unsloth.ai/basics/deepseek-v3-0526-how-to-run-locally"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvpwq3/deepseek_v3_0526/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvpwq3/deepseek_v3_0526/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T09:09:20+00:00</published>
  </entry>
</feed>
