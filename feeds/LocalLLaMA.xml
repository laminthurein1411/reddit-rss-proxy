<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-12T19:48:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j8u90g</id>
    <title>New Gemma models on 12th of March</title>
    <updated>2025-03-11T16:03:39+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8u90g/new_gemma_models_on_12th_of_march/"&gt; &lt;img alt="New Gemma models on 12th of March" src="https://preview.redd.it/8qfnwj7433oe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed4ac1bb57e9292b5685c7637a5bd9e4ac889d7c" title="New Gemma models on 12th of March" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;X pos&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qfnwj7433oe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8u90g/new_gemma_models_on_12th_of_march/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8u90g/new_gemma_models_on_12th_of_march/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T16:03:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9iq4w</id>
    <title>Gemma3-12b-Q4 seems a lot slower on Ollama than Deepseek-R1-14b-q8? Did I mess something up?</title>
    <updated>2025-03-12T12:19:07+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9iq4w/gemma312bq4_seems_a_lot_slower_on_ollama_than/"&gt; &lt;img alt="Gemma3-12b-Q4 seems a lot slower on Ollama than Deepseek-R1-14b-q8? Did I mess something up?" src="https://b.thumbs.redditmedia.com/4ay_KKrgtBYQvDUAcyo7ae-1n_dMK9Xm5bS78dAQ9jY.jpg" title="Gemma3-12b-Q4 seems a lot slower on Ollama than Deepseek-R1-14b-q8? Did I mess something up?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j9iq4w"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9iq4w/gemma312bq4_seems_a_lot_slower_on_ollama_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9iq4w/gemma312bq4_seems_a_lot_slower_on_ollama_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T12:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9qvem</id>
    <title>Gemma3 makes too many mistakes to be usable</title>
    <updated>2025-03-12T18:21:14+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tested it today on many tasks, including coding, and I don't think it's better than phi4 14b. First, I thought ollama had got the wrong parameters, so I tested it on aistudio with their default params but got the same results.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Visual understanding is sometimes pretty good, but sometimes unusable (particularly ocr)&lt;/li&gt; &lt;li&gt;It breaks often after a couple of prompts by repeating a sentence forever.&lt;/li&gt; &lt;li&gt;Coding is worse than phi4, especially when fixing the code after I tell it what is wrong.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Am I doing something wrong? How is your experience so far?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9qvem/gemma3_makes_too_many_mistakes_to_be_usable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9qvem/gemma3_makes_too_many_mistakes_to_be_usable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9qvem/gemma3_makes_too_many_mistakes_to_be_usable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T18:21:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9poij</id>
    <title>üöÄ VPTQ Now Supports Deepseek R1 (671B) Inference on 4√óA100 GPUs!</title>
    <updated>2025-03-12T17:33:01+00:00</updated>
    <author>
      <name>/u/YangWang92</name>
      <uri>https://old.reddit.com/user/YangWang92</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9poij/vptq_now_supports_deepseek_r1_671b_inference_on/"&gt; &lt;img alt="üöÄ VPTQ Now Supports Deepseek R1 (671B) Inference on 4√óA100 GPUs!" src="https://external-preview.redd.it/DePwF61MnQEcaYohkDVKnoydKXGu_TLt8lRG2GnBLfc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63cbb2d0debe1e781a96326374d21a47f859fc70" title="üöÄ VPTQ Now Supports Deepseek R1 (671B) Inference on 4√óA100 GPUs!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;VPTQ now provides preliminary support for inference with Deepseek R1! With our quantized models, you can efficiently run Deepseek R1 on A100 GPUs, which only support BF16/FP16 formats.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1j9poij/video/vqq6pszlnaoe1/player"&gt;https://reddit.com/link/1j9poij/video/vqq6pszlnaoe1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to share us more feedback!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/microsoft/VPTQ/blob/main/documents/deepseek.md"&gt;https://github.com/microsoft/VPTQ/blob/main/documents/deepseek.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YangWang92"&gt; /u/YangWang92 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9poij/vptq_now_supports_deepseek_r1_671b_inference_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9poij/vptq_now_supports_deepseek_r1_671b_inference_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9poij/vptq_now_supports_deepseek_r1_671b_inference_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T17:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9rc6r</id>
    <title>JSON makes llms dumber?</title>
    <updated>2025-03-12T18:40:21+00:00</updated>
    <author>
      <name>/u/raul3820</name>
      <uri>https://old.reddit.com/user/raul3820</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9rc6r/json_makes_llms_dumber/"&gt; &lt;img alt="JSON makes llms dumber?" src="https://preview.redd.it/lq1hr2400boe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7189befe9903f32560815b7f970fc4316a624e5e" title="JSON makes llms dumber?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source:&lt;/p&gt; &lt;p&gt;&lt;a href="https://blog.kuzudb.com/post/kuzu-wasm-rag/"&gt;https://blog.kuzudb.com/post/kuzu-wasm-rag/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/raul3820"&gt; /u/raul3820 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lq1hr2400boe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9rc6r/json_makes_llms_dumber/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9rc6r/json_makes_llms_dumber/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T18:40:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9gz1i</id>
    <title>I call it Daddy LLM</title>
    <updated>2025-03-12T10:29:03+00:00</updated>
    <author>
      <name>/u/dazzou5ouh</name>
      <uri>https://old.reddit.com/user/dazzou5ouh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gz1i/i_call_it_daddy_llm/"&gt; &lt;img alt="I call it Daddy LLM" src="https://preview.redd.it/0kbolzlck8oe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0acfd5a5ea43b09dd3077e24c0ebd0fa9b2f4238" title="I call it Daddy LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;4x 3090 on an Asus rampage V extreme motherboard. Using LM studio it can do 15 tokens/s on 70b models, but I think 2 3090 are enough for that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dazzou5ouh"&gt; /u/dazzou5ouh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0kbolzlck8oe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gz1i/i_call_it_daddy_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gz1i/i_call_it_daddy_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T10:29:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9ih6e</id>
    <title>English K_Quantization of LLMs Does Not Disproportionately Diminish Multilingual Performance</title>
    <updated>2025-03-12T12:04:59+00:00</updated>
    <author>
      <name>/u/FrostAutomaton</name>
      <uri>https://old.reddit.com/user/FrostAutomaton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9ih6e/english_k_quantization_of_llms_does_not/"&gt; &lt;img alt="English K_Quantization of LLMs Does Not Disproportionately Diminish Multilingual Performance" src="https://b.thumbs.redditmedia.com/8rQh_ppCr3h41WHkafDa1NsZ4BhdFCIkML4NhFbcs2w.jpg" title="English K_Quantization of LLMs Does Not Disproportionately Diminish Multilingual Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I should be more open to making negative (positive?) results publicly available so here they are.&lt;/p&gt; &lt;p&gt;TLDR: Quantization on the .gguf format is generally done with an importance matrix which calculates how important each weight is to an LLM. I had a thought that quantizing a model based on different language importance matrices (unsurprisingly, the quants we find online are practically always made with an English importance matrix) might be less destructive to multi-lingual performance, but the results do not back this up. In fact, quanting based on these alternate importance matrices might &lt;em&gt;slightly&lt;/em&gt; harm it, though these results are not statistically significant.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6hm3cfw728oe1.png?width=4764&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=20e10986170889985682e41d01851e7acaee1e27"&gt;Results on MixEval multiple choice questions&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4325127c28oe1.png?width=4764&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2dc1301bbfa782ec473d162c65af547d27516182"&gt;Results on MixEval Free-form questions&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Experiments were performed by quanting Llama 3.3 70B based on English, Norwegian, and Malayalam importance matrices and evaluating them on MixEval in English and translated to Norwegian. I've published a write-up on Arxiv here: &lt;a href="https://arxiv.org/abs/2503.03592"&gt;https://arxiv.org/abs/2503.03592&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I want to improve my paper-writing skills, so critiques and suggestions for it are appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrostAutomaton"&gt; /u/FrostAutomaton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9ih6e/english_k_quantization_of_llms_does_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9ih6e/english_k_quantization_of_llms_does_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9ih6e/english_k_quantization_of_llms_does_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T12:04:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9drfk</id>
    <title>Gemma 3: Technical Report</title>
    <updated>2025-03-12T06:49:36+00:00</updated>
    <author>
      <name>/u/David-Kunz</name>
      <uri>https://old.reddit.com/user/David-Kunz</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/David-Kunz"&gt; /u/David-Kunz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9drfk/gemma_3_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9drfk/gemma_3_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j90u4u</id>
    <title>What happened to the promised open source o3-mini ?</title>
    <updated>2025-03-11T20:32:37+00:00</updated>
    <author>
      <name>/u/i-have-the-stash</name>
      <uri>https://old.reddit.com/user/i-have-the-stash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does everybody forget that this was once promised ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i-have-the-stash"&gt; /u/i-have-the-stash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j90u4u/what_happened_to_the_promised_open_source_o3mini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j90u4u/what_happened_to_the_promised_open_source_o3mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j90u4u/what_happened_to_the_promised_open_source_o3mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T20:32:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9qxmw</id>
    <title>Anyone using a rack mount case for &gt;2 GPU's</title>
    <updated>2025-03-12T18:23:48+00:00</updated>
    <author>
      <name>/u/barnett9</name>
      <uri>https://old.reddit.com/user/barnett9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9qxmw/anyone_using_a_rack_mount_case_for_2_gpus/"&gt; &lt;img alt="Anyone using a rack mount case for &amp;gt;2 GPU's" src="https://preview.redd.it/bltyuht1xaoe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87137e106b3180b57389fec5bbc012983a9136cf" title="Anyone using a rack mount case for &amp;gt;2 GPU's" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If so, what case are you using?&lt;/p&gt; &lt;p&gt;My current setup has enough pcie slots for up to 4 more gpu's, but as you can see I've already had to cut off half of the cpu cooler to fit the first two lol. I can use pcie extenders, but I don't see many cases that are designed to fit such monstrous cards. &lt;/p&gt; &lt;p&gt;Any ideas or pics of your rack mount cases for inspiration would be greatly appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/barnett9"&gt; /u/barnett9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bltyuht1xaoe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9qxmw/anyone_using_a_rack_mount_case_for_2_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9qxmw/anyone_using_a_rack_mount_case_for_2_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T18:23:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9dt8l</id>
    <title>Gemma 3 on Huggingface</title>
    <updated>2025-03-12T06:52:16+00:00</updated>
    <author>
      <name>/u/DataCraftsman</name>
      <uri>https://old.reddit.com/user/DataCraftsman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google Gemma 3! Comes in 1B, 4B, 12B, 27B:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-1b-it"&gt;https://huggingface.co/google/gemma-3-1b-it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-4b-it"&gt;https://huggingface.co/google/gemma-3-4b-it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-12b-it"&gt;https://huggingface.co/google/gemma-3-12b-it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-27b-it"&gt;https://huggingface.co/google/gemma-3-27b-it&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Inputs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text string, such as a question, a prompt, or a document to be summarized&lt;/li&gt; &lt;li&gt;Images, normalized to 896 x 896 resolution and encoded to 256 tokens each&lt;/li&gt; &lt;li&gt;Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and 32K tokens for the 1B size&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Outputs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Context of 8192 tokens&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Update: They have added it to Ollama already!&lt;/p&gt; &lt;p&gt;Ollama: &lt;a href="https://ollama.com/library/gemma3"&gt;https://ollama.com/library/gemma3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Apparently it has an ELO of 1338 on Chatbot Arena, better than DeepSeek V3 671B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataCraftsman"&gt; /u/DataCraftsman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dt8l/gemma_3_on_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dt8l/gemma_3_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dt8l/gemma_3_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:52:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9reim</id>
    <title>LM Studio updated with Gemma 3 GGUF support!</title>
    <updated>2025-03-12T18:42:52+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Update to the latest available runtime (v1.19.0) and you'll be able to run Gemma 3 GGUFs with vision!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9reim/lm_studio_updated_with_gemma_3_gguf_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9reim/lm_studio_updated_with_gemma_3_gguf_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9reim/lm_studio_updated_with_gemma_3_gguf_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T18:42:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j981ci</id>
    <title>This is the first response from an LLM that has made me cry laughing</title>
    <updated>2025-03-12T01:50:28+00:00</updated>
    <author>
      <name>/u/Ninjinka</name>
      <uri>https://old.reddit.com/user/Ninjinka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j981ci/this_is_the_first_response_from_an_llm_that_has/"&gt; &lt;img alt="This is the first response from an LLM that has made me cry laughing" src="https://preview.redd.it/kw96telpz5oe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7223e09ee41672180f06db34a031ef87fae195a" title="This is the first response from an LLM that has made me cry laughing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ninjinka"&gt; /u/Ninjinka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kw96telpz5oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j981ci/this_is_the_first_response_from_an_llm_that_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j981ci/this_is_the_first_response_from_an_llm_that_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T01:50:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j96j3g</id>
    <title>I hacked Unsloth's GRPO code to support agentic tool use. In 1 hour of training on my RTX 4090, Llama-8B taught itself to take baby steps towards deep research! (23%‚Üí53% accuracy)</title>
    <updated>2025-03-12T00:40:21+00:00</updated>
    <author>
      <name>/u/diegocaples</name>
      <uri>https://old.reddit.com/user/diegocaples</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! I've been experimenting with getting &lt;a href="https://github.com/dCaples/AutoDidact/"&gt;Llama-8B to bootstrap its own research skills through self-play.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I modified Unsloth's GRPO implementation (‚ù§Ô∏è Unsloth!) to support function calling and agentic feedback loops.&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Llama generates its own questions about documents (you can have it learn from any documents, but I chose the Apollo 13 mission report)&lt;/li&gt; &lt;li&gt;It learns to search for answers in the corpus using a search tool&lt;/li&gt; &lt;li&gt;It evaluates its own success/failure using llama-as-a-judge&lt;/li&gt; &lt;li&gt;Finally, it trains itself through RL to get better at research&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The model starts out hallucinating and making all kinds of mistakes, but after an hour of training on my 4090, it quickly improves. It goes from getting 23% of answers correct to 53%!&lt;/p&gt; &lt;p&gt;Here is the full &lt;a href="https://github.com/dCaples/AutoDidact/"&gt;code and instructions&lt;/a&gt;!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/diegocaples"&gt; /u/diegocaples &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j96j3g/i_hacked_unsloths_grpo_code_to_support_agentic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j96j3g/i_hacked_unsloths_grpo_code_to_support_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j96j3g/i_hacked_unsloths_grpo_code_to_support_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T00:40:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9nioo</id>
    <title>Let‚Äôs make Gemma 3 think! Here's a notebook to do GRPO on Gemma3 to make it reason.</title>
    <updated>2025-03-12T16:05:14+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here‚Äôs a notebook to make Gemma reason with GRPO &amp;amp; TRL. I made this whilst prepping the next unit of the reasoning course:&lt;/p&gt; &lt;p&gt;In this notebooks I combine together google‚Äôs model with some community tooling&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First, I load the model from the Hugging Face hub with transformers‚Äôs latest release for Gemma 3&lt;/li&gt; &lt;li&gt;I use PEFT and bitsandbytes to get it running on Colab&lt;/li&gt; &lt;li&gt;Then, I took Will Browns processing and reward functions to make reasoning chains from GSM8k&lt;/li&gt; &lt;li&gt;Finally, I used TRL‚Äôs GRPOTrainer to train the model&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Next step is to bring Unsloth AI in, then ship it in the reasoning course. Links to notebook below.&lt;/p&gt; &lt;p&gt;&lt;a href="https://colab.research.google.com/drive/1Vkl69ytCS3bvOtV9_stRETMthlQXR4wX?usp=sharing"&gt;https://colab.research.google.com/drive/1Vkl69ytCS3bvOtV9_stRETMthlQXR4wX?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9nioo/lets_make_gemma_3_think_heres_a_notebook_to_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9nioo/lets_make_gemma_3_think_heres_a_notebook_to_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9nioo/lets_make_gemma_3_think_heres_a_notebook_to_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T16:05:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9dmny</id>
    <title>Gemma 3 27B</title>
    <updated>2025-03-12T06:42:38+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dmny/gemma_3_27b/"&gt; &lt;img alt="Gemma 3 27B" src="https://preview.redd.it/foonq7ewf7oe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d30b8fa38fe5d18d26cf0aca72f47b346cd9ad56" title="Gemma 3 27B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/foonq7ewf7oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dmny/gemma_3_27b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dmny/gemma_3_27b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:42:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9bvll</id>
    <title>Gemma 3 27b now available on Google AI Studio</title>
    <updated>2025-03-12T05:13:10+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9bvll/gemma_3_27b_now_available_on_google_ai_studio/"&gt; &lt;img alt="Gemma 3 27b now available on Google AI Studio" src="https://external-preview.redd.it/4sjcMoBy8c8hywZZD7DFEQHtY85E3eDlhYRBqIdn2eQ.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f55bb78cef85467f757df883df24bca99ee8925" title="Gemma 3 27b now available on Google AI Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://aistudio.google.com/"&gt;https://aistudio.google.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context length 128k&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Output length 8k&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://imgur.com/a/2WvMTPS"&gt;&lt;strong&gt;https://imgur.com/a/2WvMTPS&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1pbvvqtwz6oe1.png?width=1259&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0da97a547c24c616b8c3c1cc1ccd43e659245dd"&gt;https://preview.redd.it/1pbvvqtwz6oe1.png?width=1259&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0da97a547c24c616b8c3c1cc1ccd43e659245dd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9bvll/gemma_3_27b_now_available_on_google_ai_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9bvll/gemma_3_27b_now_available_on_google_ai_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9bvll/gemma_3_27b_now_available_on_google_ai_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T05:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9iazd</id>
    <title>Gemma3 technical report detailed analysis üíé</title>
    <updated>2025-03-12T11:54:57+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9iazd/gemma3_technical_report_detailed_analysis/"&gt; &lt;img alt="Gemma3 technical report detailed analysis üíé" src="https://preview.redd.it/a5jgz1vlz8oe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd67d4f1cca341b88b273756d22a450cb91848ec" title="Gemma3 technical report detailed analysis üíé" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a5jgz1vlz8oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9iazd/gemma3_technical_report_detailed_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9iazd/gemma3_technical_report_detailed_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T11:54:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9gafp</id>
    <title>EXO Labs ran full 8-bit DeepSeek R1 distributed across 2 M3 Ultra 512GB Mac Studios - 11 t/s</title>
    <updated>2025-03-12T09:40:39+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gafp/exo_labs_ran_full_8bit_deepseek_r1_distributed/"&gt; &lt;img alt="EXO Labs ran full 8-bit DeepSeek R1 distributed across 2 M3 Ultra 512GB Mac Studios - 11 t/s" src="https://external-preview.redd.it/TzhJISgYeqmJ66gJa8ISrsZbvEzELCLxSu1XvFxOBXk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c3d163b23ac0c0c9e4c6af40dd5ca4af8494305" title="EXO Labs ran full 8-bit DeepSeek R1 distributed across 2 M3 Ultra 512GB Mac Studios - 11 t/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/alexocheema/status/1899735281781411907"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gafp/exo_labs_ran_full_8bit_deepseek_r1_distributed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gafp/exo_labs_ran_full_8bit_deepseek_r1_distributed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T09:40:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9hsfc</id>
    <title>Gemma 3 - GGUFs + recommended settings</title>
    <updated>2025-03-12T11:22:36+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We uploaded GGUFs and 16-bit versions of Gemma 3 to Hugging Face! Gemma 3 is Google's new multimodal models that come in 1B, 4B, 12B and 27B sizes. We also made a step-by-step guide on How to run Gemma 3 correctly: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Training Gemma 3 with Unsloth does work (yet), but there's currently bugs with training in 4-bit QLoRA (not on Unsloth's side) so 4-bit dynamic and QLoRA training with our notebooks will be released tomorrow!&lt;/p&gt; &lt;p&gt;Gemma 3 GGUF uploads:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-1b-it-GGUF"&gt;1B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-4b-it-GGUF"&gt;4B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-12b-it-GGUF"&gt;12B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-27b-it-GGUF"&gt;27B&lt;/a&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Gemma 3 Instruct 16-bit uploads:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-1b-it"&gt;1B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-4b-it"&gt;4B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-12b-it"&gt;12B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-27b-it"&gt;27B&lt;/a&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;See the rest of our models &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;in our docs&lt;/a&gt;. Remember to pull the &lt;strong&gt;LATEST llama.cpp&lt;/strong&gt; for stuff to work!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update: Confirmed with the Gemma + Hugging Face team&lt;/strong&gt;, that the recommended settings for inference are (I auto made a params file for example in &lt;a href="https://huggingface.co/unsloth/gemma-3-27b-it-GGUF/blob/main/params"&gt;https://huggingface.co/unsloth/gemma-3-27b-it-GGUF/blob/main/params&lt;/a&gt; which can help if you use Ollama ie like &lt;code&gt;ollama run&lt;/code&gt; &lt;a href="http://hf.co/unsloth/gemma-3-27b-it-GGUF:Q4_K_M"&gt;&lt;code&gt;hf.co/unsloth/gemma-3-27b-it-GGUF:Q4_K_M&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;temperature = 1.0 top_k = 64 top_p = 0.95 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And the chat template is:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;bos&amp;gt;&amp;lt;start_of_turn&amp;gt;user\nHello!&amp;lt;end_of_turn&amp;gt;\n&amp;lt;start_of_turn&amp;gt;model\nHey there!&amp;lt;end_of_turn&amp;gt;\n&amp;lt;start_of_turn&amp;gt;user\nWhat is 1+1?&amp;lt;end_of_turn&amp;gt;\n&amp;lt;start_of_turn&amp;gt;model\n &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;WARNING: Do not add a &amp;lt;bos&amp;gt; to llama.cpp&lt;/strong&gt; or other inference engines, or else you will get &lt;strong&gt;DOUBLE &amp;lt;BOS&amp;gt; tokens&lt;/strong&gt;! llama.cpp auto adds the token for you!&lt;/p&gt; &lt;p&gt;More spaced out chat template (newlines rendered):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;bos&amp;gt;&amp;lt;start_of_turn&amp;gt;user Hello!&amp;lt;end_of_turn&amp;gt; &amp;lt;start_of_turn&amp;gt;model Hey there!&amp;lt;end_of_turn&amp;gt; &amp;lt;start_of_turn&amp;gt;user What is 1+1?&amp;lt;end_of_turn&amp;gt; &amp;lt;start_of_turn&amp;gt;model\n &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Read more in our docs on how to run Gemma 3 effectively: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9hsfc/gemma_3_ggufs_recommended_settings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9hsfc/gemma_3_ggufs_recommended_settings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9hsfc/gemma_3_ggufs_recommended_settings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T11:22:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9relp</id>
    <title>So Gemma 4b on cell phone!</title>
    <updated>2025-03-12T18:42:58+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9relp/so_gemma_4b_on_cell_phone/"&gt; &lt;img alt="So Gemma 4b on cell phone!" src="https://external-preview.redd.it/MXZmcHZzOWcwYm9lMbq2BLsFwXRwHAZDEkm4Sv4WVOKYAc4eNAszXI2Ja4-v.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7573b6dd8e6be666b739de408843f1ca19c69e5" title="So Gemma 4b on cell phone!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i0bqxnig0boe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9relp/so_gemma_4b_on_cell_phone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9relp/so_gemma_4b_on_cell_phone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T18:42:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9lwlw</id>
    <title>QwQ on high thinking effort setup one-shotting the bouncing balls example</title>
    <updated>2025-03-12T14:56:52+00:00</updated>
    <author>
      <name>/u/ASL_Dev</name>
      <uri>https://old.reddit.com/user/ASL_Dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9lwlw/qwq_on_high_thinking_effort_setup_oneshotting_the/"&gt; &lt;img alt="QwQ on high thinking effort setup one-shotting the bouncing balls example" src="https://external-preview.redd.it/YXRidHp4czB3OW9lMV0giusrq7hVuZKSGoynYltxxXlZH0h5sQXtJgMJk00r.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=015cf534b9b44ba5711148e6aabed4d1a9d18009" title="QwQ on high thinking effort setup one-shotting the bouncing balls example" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ASL_Dev"&gt; /u/ASL_Dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nrf0zws0w9oe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9lwlw/qwq_on_high_thinking_effort_setup_oneshotting_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9lwlw/qwq_on_high_thinking_effort_setup_oneshotting_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T14:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9kxqq</id>
    <title>Gemma 3 - Open source efforts - llama.cpp - MLX community</title>
    <updated>2025-03-12T14:09:13+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9kxqq/gemma_3_open_source_efforts_llamacpp_mlx_community/"&gt; &lt;img alt="Gemma 3 - Open source efforts - llama.cpp - MLX community" src="https://preview.redd.it/x3jb302hn9oe1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57903a9d653b1c8233ccf0bdddef2bb91bebc72a" title="Gemma 3 - Open source efforts - llama.cpp - MLX community" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x3jb302hn9oe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9kxqq/gemma_3_open_source_efforts_llamacpp_mlx_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9kxqq/gemma_3_open_source_efforts_llamacpp_mlx_community/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T14:09:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9dkvh</id>
    <title>Gemma 3 Release - a google Collection</title>
    <updated>2025-03-12T06:39:59+00:00</updated>
    <author>
      <name>/u/ayyndrew</name>
      <uri>https://old.reddit.com/user/ayyndrew</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dkvh/gemma_3_release_a_google_collection/"&gt; &lt;img alt="Gemma 3 Release - a google Collection" src="https://external-preview.redd.it/XbF6RBBvzvCU6XDYyRoYk_HGSNjj77rcnuXfCRK9sgQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d72653de324cc030e9dad7f7ea4df6ef94e0688" title="Gemma 3 Release - a google Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayyndrew"&gt; /u/ayyndrew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dkvh/gemma_3_release_a_google_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dkvh/gemma_3_release_a_google_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:39:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9jfbt</id>
    <title>M3 Ultra Runs DeepSeek R1 With 671 Billion Parameters Using 448GB Of Unified Memory, Delivering High Bandwidth Performance At Under 200W Power Consumption, With No Need For A Multi-GPU Setup</title>
    <updated>2025-03-12T12:56:28+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9jfbt/m3_ultra_runs_deepseek_r1_with_671_billion/"&gt; &lt;img alt="M3 Ultra Runs DeepSeek R1 With 671 Billion Parameters Using 448GB Of Unified Memory, Delivering High Bandwidth Performance At Under 200W Power Consumption, With No Need For A Multi-GPU Setup" src="https://external-preview.redd.it/H9R-bqnloX40RFggVkXhbjJRVXE72K4CKNfmbfAALSA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e94f5a141e87c847ee3d0f8fbf75c728e3ce893" title="M3 Ultra Runs DeepSeek R1 With 671 Billion Parameters Using 448GB Of Unified Memory, Delivering High Bandwidth Performance At Under 200W Power Consumption, With No Need For A Multi-GPU Setup" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/m3-ultra-chip-handles-deepseek-r1-model-with-671-billion-parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9jfbt/m3_ultra_runs_deepseek_r1_with_671_billion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9jfbt/m3_ultra_runs_deepseek_r1_with_671_billion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T12:56:28+00:00</published>
  </entry>
</feed>
