<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-13T15:24:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j9reim</id>
    <title>LM Studio updated with Gemma 3 GGUF support!</title>
    <updated>2025-03-12T18:42:52+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Update to the latest available runtime (v1.19.0) and you'll be able to run Gemma 3 GGUFs with vision!&lt;/p&gt; &lt;p&gt;Edit to add two things:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;They just pushed another update enabling GPU usage for vision, so grab that if you want to offload for faster processing!&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It seems a lot of the quants out there are lacking the mmproj file, while still being tagged as Image-Text-to-Text, which will make it misbehave in LM Studio, be sure to grab either from lmstudio-community, or my own (bartowski) if you want to use vision&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lmstudio-community?search_models=Gemma-3"&gt;https://huggingface.co/lmstudio-community?search_models=Gemma-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski?search_models=Google_gemma-3"&gt;https://huggingface.co/bartowski?search_models=Google_gemma-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From a quick search it looks like the following users also properly uploades with vision: second-state, gaianet, and DevQuasar&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9reim/lm_studio_updated_with_gemma_3_gguf_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9reim/lm_studio_updated_with_gemma_3_gguf_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9reim/lm_studio_updated_with_gemma_3_gguf_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T18:42:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9lwlw</id>
    <title>QwQ on high thinking effort setup one-shotting the bouncing balls example</title>
    <updated>2025-03-12T14:56:52+00:00</updated>
    <author>
      <name>/u/ASL_Dev</name>
      <uri>https://old.reddit.com/user/ASL_Dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9lwlw/qwq_on_high_thinking_effort_setup_oneshotting_the/"&gt; &lt;img alt="QwQ on high thinking effort setup one-shotting the bouncing balls example" src="https://external-preview.redd.it/YXRidHp4czB3OW9lMV0giusrq7hVuZKSGoynYltxxXlZH0h5sQXtJgMJk00r.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=015cf534b9b44ba5711148e6aabed4d1a9d18009" title="QwQ on high thinking effort setup one-shotting the bouncing balls example" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ASL_Dev"&gt; /u/ASL_Dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nrf0zws0w9oe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9lwlw/qwq_on_high_thinking_effort_setup_oneshotting_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9lwlw/qwq_on_high_thinking_effort_setup_oneshotting_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T14:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9dkvh</id>
    <title>Gemma 3 Release - a google Collection</title>
    <updated>2025-03-12T06:39:59+00:00</updated>
    <author>
      <name>/u/ayyndrew</name>
      <uri>https://old.reddit.com/user/ayyndrew</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dkvh/gemma_3_release_a_google_collection/"&gt; &lt;img alt="Gemma 3 Release - a google Collection" src="https://external-preview.redd.it/XbF6RBBvzvCU6XDYyRoYk_HGSNjj77rcnuXfCRK9sgQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d72653de324cc030e9dad7f7ea4df6ef94e0688" title="Gemma 3 Release - a google Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayyndrew"&gt; /u/ayyndrew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dkvh/gemma_3_release_a_google_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dkvh/gemma_3_release_a_google_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:39:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9x4qj</id>
    <title>I'm just going to say it: When are we going to get uncensored Gemma 3?</title>
    <updated>2025-03-12T22:46:54+00:00</updated>
    <author>
      <name>/u/CreepyMan121</name>
      <uri>https://old.reddit.com/user/CreepyMan121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When do you guys think an uncensored version of Gemma 3 will release? I'm quite eager to know bc I really want to do ERP already and I hate having an AI model that refuses to answer even the most slightest controversial question, its like talking with a local version of Goody2 lol.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CreepyMan121"&gt; /u/CreepyMan121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9x4qj/im_just_going_to_say_it_when_are_we_going_to_get/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9x4qj/im_just_going_to_say_it_when_are_we_going_to_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9x4qj/im_just_going_to_say_it_when_are_we_going_to_get/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T22:46:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9kxqq</id>
    <title>Gemma 3 - Open source efforts - llama.cpp - MLX community</title>
    <updated>2025-03-12T14:09:13+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9kxqq/gemma_3_open_source_efforts_llamacpp_mlx_community/"&gt; &lt;img alt="Gemma 3 - Open source efforts - llama.cpp - MLX community" src="https://preview.redd.it/x3jb302hn9oe1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57903a9d653b1c8233ccf0bdddef2bb91bebc72a" title="Gemma 3 - Open source efforts - llama.cpp - MLX community" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x3jb302hn9oe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9kxqq/gemma_3_open_source_efforts_llamacpp_mlx_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9kxqq/gemma_3_open_source_efforts_llamacpp_mlx_community/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T14:09:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ja9ro5</id>
    <title>What‚Äôs the open source equivalent of ChatGPT voice mode?</title>
    <updated>2025-03-13T11:28:09+00:00</updated>
    <author>
      <name>/u/javatextbook</name>
      <uri>https://old.reddit.com/user/javatextbook</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for a way to have a voice conversation with an open model using OSS which performs as close to ChatGPT voice mode as possible. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/javatextbook"&gt; /u/javatextbook &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja9ro5/whats_the_open_source_equivalent_of_chatgpt_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja9ro5/whats_the_open_source_equivalent_of_chatgpt_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ja9ro5/whats_the_open_source_equivalent_of_chatgpt_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T11:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ja81np</id>
    <title>GitHub - jonasfrey/gpu-monitor-browser-gui: a browser gui for nvidia smi</title>
    <updated>2025-03-13T09:29:31+00:00</updated>
    <author>
      <name>/u/maifee</name>
      <uri>https://old.reddit.com/user/maifee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja81np/github_jonasfreygpumonitorbrowsergui_a_browser/"&gt; &lt;img alt="GitHub - jonasfrey/gpu-monitor-browser-gui: a browser gui for nvidia smi" src="https://external-preview.redd.it/jrJX5o4U_bmbYCRMldgsvagnOuxS95mMDYP1a2MX9CM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e8fbadc1ff6a6fae472303a740147275fefe1997" title="GitHub - jonasfrey/gpu-monitor-browser-gui: a browser gui for nvidia smi" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maifee"&gt; /u/maifee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jonasfrey/gpu-monitor-browser-gui"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja81np/github_jonasfreygpumonitorbrowsergui_a_browser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ja81np/github_jonasfreygpumonitorbrowsergui_a_browser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T09:29:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ja7oa6</id>
    <title>Best Approach for Summarizing 100 PDFs</title>
    <updated>2025-03-13T09:00:13+00:00</updated>
    <author>
      <name>/u/Proof-Exercise2695</name>
      <uri>https://old.reddit.com/user/Proof-Exercise2695</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I have about 100 PDFs, and I need a way to generate answers based on their content‚Äînot using similarity search, but rather by analyzing the files in-depth. For now, I created different indexes: one for similarity-based retrieval and another for summarization.&lt;/p&gt; &lt;p&gt;I'm looking for advice on the best approach to summarizing these documents. I‚Äôve experimented with various models and parsing methods, but I feel that the generated summaries don't fully capture the key points. Here‚Äôs what I‚Äôve tried:&lt;/p&gt; &lt;h1&gt;&amp;quot;Models&amp;quot; (Brand) used:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Mistral&lt;/li&gt; &lt;li&gt;OpenAI&lt;/li&gt; &lt;li&gt;LLaMA 3.2&lt;/li&gt; &lt;li&gt;DeepSeek-r1:7b&lt;/li&gt; &lt;li&gt;DeepScaler&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Parsing methods:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Docling&lt;/li&gt; &lt;li&gt;Unstructured&lt;/li&gt; &lt;li&gt;PyMuPDF4LLM&lt;/li&gt; &lt;li&gt;LLMWhisperer&lt;/li&gt; &lt;li&gt;LlamaParse&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Current Approaches:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;LangChain:&lt;/strong&gt; Concatenating summaries of each file and then re-summarizing using &lt;code&gt;load_summarize_chain(llm, chain_type=&amp;quot;map_reduce&amp;quot;)&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LlamaIndex:&lt;/strong&gt; Using &lt;code&gt;SummaryIndex&lt;/code&gt; or &lt;code&gt;DocumentSummaryIndex.from_documents(all my docs)&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI Cookbook Summary:&lt;/strong&gt; Following the example from &lt;a href="https://github.com/openai/openai-cookbook/blob/main/examples/Summarizing_long_documents.ipynb"&gt;this notebook&lt;/a&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Despite these efforts, I feel that the summaries lack depth and don‚Äôt extract the most critical information effectively. Do you have a better approach? If possible, could you share a GitHub repository or some code that could help?&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proof-Exercise2695"&gt; /u/Proof-Exercise2695 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja7oa6/best_approach_for_summarizing_100_pdfs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja7oa6/best_approach_for_summarizing_100_pdfs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ja7oa6/best_approach_for_summarizing_100_pdfs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T09:00:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9zqdr</id>
    <title>Why Deepseek R1 is still a reference while Qwen QwQ 32B has similar performance for a much more reasonable size?</title>
    <updated>2025-03-13T00:45:12+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9zqdr/why_deepseek_r1_is_still_a_reference_while_qwen/"&gt; &lt;img alt="Why Deepseek R1 is still a reference while Qwen QwQ 32B has similar performance for a much more reasonable size?" src="https://b.thumbs.redditmedia.com/yoLzmTyPDsKluGUxlqS5WMO-QORC4i1xdJ6jWNYry7A.jpg" title="Why Deepseek R1 is still a reference while Qwen QwQ 32B has similar performance for a much more reasonable size?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If the performances are similar, why bother to load a gargantuan model of 671B parameters? Why QwQ does not become the king of open weight LLMs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j9zqdr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9zqdr/why_deepseek_r1_is_still_a_reference_while_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9zqdr/why_deepseek_r1_is_still_a_reference_while_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T00:45:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9relp</id>
    <title>So Gemma 4b on cell phone!</title>
    <updated>2025-03-12T18:42:58+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9relp/so_gemma_4b_on_cell_phone/"&gt; &lt;img alt="So Gemma 4b on cell phone!" src="https://external-preview.redd.it/MXZmcHZzOWcwYm9lMbq2BLsFwXRwHAZDEkm4Sv4WVOKYAc4eNAszXI2Ja4-v.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7573b6dd8e6be666b739de408843f1ca19c69e5" title="So Gemma 4b on cell phone!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i0bqxnig0boe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9relp/so_gemma_4b_on_cell_phone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9relp/so_gemma_4b_on_cell_phone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T18:42:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9wkc2</id>
    <title>Slim attention: cut your context memory in half without loss of accuracy</title>
    <updated>2025-03-12T22:21:51+00:00</updated>
    <author>
      <name>/u/Ok-Commercial-2205</name>
      <uri>https://old.reddit.com/user/Ok-Commercial-2205</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2503.05840"&gt;https://arxiv.org/pdf/2503.05840&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Slim attention shrinks the context memory size by 2x for transformer models with MHA (multi-head attention), which can speed up inference by up to 2x for large context windows. Slim attention is an exact, mathematically identical implementation of the standard attention mechanism and therefore doesn‚Äôt compromise model accuracy. In other words, slim attention losslessly compresses the context memory by a factor of 2. For encoder-decoder transformers, the context memory size can be reduced even further: For the Whisper models for example, slim attention reduces the context memory by 8x, which can speed up token generation by 5x for batch size 64 for example. And for rare cases where the MHA projection dimension is larger than dmodel, the memory can be reduced by a factor of 32 for the T5-11B model for example&lt;/p&gt; &lt;p&gt;For questions/comments: [&lt;a href="mailto:info@openmachine.ai"&gt;info@openmachine.ai&lt;/a&gt;](mailto:&lt;a href="mailto:info@openmachine.ai"&gt;info@openmachine.ai&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/OpenMachine-ai/transformer-tricks"&gt;https://github.com/OpenMachine-ai/transformer-tricks&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Commercial-2205"&gt; /u/Ok-Commercial-2205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9wkc2/slim_attention_cut_your_context_memory_in_half/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9wkc2/slim_attention_cut_your_context_memory_in_half/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9wkc2/slim_attention_cut_your_context_memory_in_half/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T22:21:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9jfbt</id>
    <title>M3 Ultra Runs DeepSeek R1 With 671 Billion Parameters Using 448GB Of Unified Memory, Delivering High Bandwidth Performance At Under 200W Power Consumption, With No Need For A Multi-GPU Setup</title>
    <updated>2025-03-12T12:56:28+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9jfbt/m3_ultra_runs_deepseek_r1_with_671_billion/"&gt; &lt;img alt="M3 Ultra Runs DeepSeek R1 With 671 Billion Parameters Using 448GB Of Unified Memory, Delivering High Bandwidth Performance At Under 200W Power Consumption, With No Need For A Multi-GPU Setup" src="https://external-preview.redd.it/H9R-bqnloX40RFggVkXhbjJRVXE72K4CKNfmbfAALSA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e94f5a141e87c847ee3d0f8fbf75c728e3ce893" title="M3 Ultra Runs DeepSeek R1 With 671 Billion Parameters Using 448GB Of Unified Memory, Delivering High Bandwidth Performance At Under 200W Power Consumption, With No Need For A Multi-GPU Setup" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/m3-ultra-chip-handles-deepseek-r1-model-with-671-billion-parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9jfbt/m3_ultra_runs_deepseek_r1_with_671_billion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9jfbt/m3_ultra_runs_deepseek_r1_with_671_billion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T12:56:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ja90bw</id>
    <title>What some people think "vibe coding" looks like</title>
    <updated>2025-03-13T10:38:52+00:00</updated>
    <author>
      <name>/u/shokuninstudio</name>
      <uri>https://old.reddit.com/user/shokuninstudio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja90bw/what_some_people_think_vibe_coding_looks_like/"&gt; &lt;img alt="What some people think &amp;quot;vibe coding&amp;quot; looks like" src="https://external-preview.redd.it/GZR187Zl_2NpqsrKaFlSi_u2epjjTSVuy_SHWC8_Kzs.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0aeac553e1e96402b7cbcfd7a9f559f03fed6439" title="What some people think &amp;quot;vibe coding&amp;quot; looks like" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shokuninstudio"&gt; /u/shokuninstudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=u1Ds9CeG-VY"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja90bw/what_some_people_think_vibe_coding_looks_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ja90bw/what_some_people_think_vibe_coding_looks_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T10:38:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ja5pf9</id>
    <title>Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models</title>
    <updated>2025-03-13T06:24:51+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja5pf9/block_diffusion_interpolating_between/"&gt; &lt;img alt="Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models" src="https://b.thumbs.redditmedia.com/n4PintnGt1LQCTPvMCJUBJ0-rX4Su2iBf9xyMBgN8pc.jpg" title="Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2503.09573"&gt;https://arxiv.org/abs/2503.09573&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/kuleshov-group/BD3-LMs"&gt;https://github.com/kuleshov-group/BD3-LMs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/collections/kuleshov-group/BD3-LMs-67be95f81b96b15fec50d53f"&gt;https://huggingface.co/collections/kuleshov-group/BD3-LMs-67be95f81b96b15fec50d53f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Project Page: &lt;a href="https://m-arriola.com/bd3lms/"&gt;https://m-arriola.com/bd3lms/&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Abstract&lt;/h1&gt; &lt;blockquote&gt; &lt;p&gt;Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Autoregression&lt;/strong&gt;: ‚úÖ High quality ‚úÖ Arbitrary-length ‚úÖ KV caching ‚ùå Not parallelizable&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Diffusion&lt;/strong&gt;: ‚ùå Lower quality ‚ùå Fixed-length ‚ùå No KV caching ‚úÖ Parallelizable&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Block Diffusion&lt;/strong&gt;: ‚úÖ High quality ‚úÖ Arbitrary-length ‚úÖ KV caching ‚úÖ Parallelizable&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja5pf9/block_diffusion_interpolating_between/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja5pf9/block_diffusion_interpolating_between/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ja5pf9/block_diffusion_interpolating_between/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T06:24:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ja4qz1</id>
    <title>Gemma 3 Deep Dive: Is Google Cranking Up the Compute Budget?</title>
    <updated>2025-03-13T05:17:17+00:00</updated>
    <author>
      <name>/u/mimirium_</name>
      <uri>https://old.reddit.com/user/mimirium_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been digging into the tech report details emerging on Gemma 3 and wanted to share some interesting observations and spark a discussion. Google seems to be making some deliberate design choices with this generation.&lt;/p&gt; &lt;p&gt;Key Takeaways (from my analysis of publicly available information):&lt;/p&gt; &lt;p&gt;FFN Size Explosion: The feedforward network (FFN) sizes for the 12B and 27B Gemma 3 models are significantly larger than their Qwen2.5 counterparts. We're talking a massive increase. This probably suggests a shift towards leveraging more compute within each layer.&lt;/p&gt; &lt;p&gt;Compensating with Hidden Size: To balance the FFN bloat, it looks like they're deliberately lowering the hidden size (d_model) for the Gemma 3 models compared to Qwen. This could be a clever way to maintain memory efficiency while maximizing the impact of the larger FFN.&lt;/p&gt; &lt;p&gt;Head Count Differences: Interesting trend here ‚Äì much fewer heads generally, but it seems the 4B model has more kv_heads than the rest. Makes you wonder if Google are playing with their version of MQA or GQA&lt;/p&gt; &lt;p&gt;Training Budgets: The jump in training tokens is substantial:&lt;/p&gt; &lt;p&gt;1B -&amp;gt; 2T (same as Gemma 2-2B) 2B -&amp;gt; 4T 12B -&amp;gt; 12T 27B -&amp;gt; 14T&lt;/p&gt; &lt;p&gt;Context Length Performance:&lt;/p&gt; &lt;p&gt;Pretrained on 32k which is not common, No 128k on the 1B + confirmation that larger model are easier to do context extension Only increase the rope (10k-&amp;gt;1M) on the global attention layer. 1 shot 32k -&amp;gt; 128k ?&lt;/p&gt; &lt;p&gt;Architectural changes:&lt;/p&gt; &lt;p&gt;No softcaping but QK-Norm Pre AND Post norm&lt;/p&gt; &lt;p&gt;Possible Implications &amp;amp; Discussion Points:&lt;/p&gt; &lt;p&gt;Compute-Bound? The FFN size suggests Google is throwing more raw compute at the problem, possibly indicating that they've optimized other aspects of the architecture and are now pushing the limits of their hardware.&lt;/p&gt; &lt;p&gt;KV Cache Optimizations: They seem to be prioritizing KV cache optimizations Scaling Laws Still Hold? Are the gains from a larger FFN linear, or are we seeing diminishing returns? How does this affect the scaling laws we've come to expect?&lt;/p&gt; &lt;p&gt;The &amp;quot;4B Anomaly&amp;quot;: What's with the relatively higher KV head count on the 4B model? Is this a specific optimization for that size, or an experimental deviation?&lt;/p&gt; &lt;p&gt;Distillation Strategies? Early analysis suggests they used small vs large teacher distillation methods&lt;/p&gt; &lt;p&gt;Local-Global Ratio: They tested Local:Global ratio on the perplexity and found the impact minimal What do you all think? Is Google betting on brute force with Gemma 3? Are these architectural changes going to lead to significant performance improvements, or are they more about squeezing out marginal gains? Let's discuss!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mimirium_"&gt; /u/mimirium_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja4qz1/gemma_3_deep_dive_is_google_cranking_up_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja4qz1/gemma_3_deep_dive_is_google_cranking_up_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ja4qz1/gemma_3_deep_dive_is_google_cranking_up_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T05:17:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9v3lf</id>
    <title>Gemma 3 - Insanely good</title>
    <updated>2025-03-12T21:18:47+00:00</updated>
    <author>
      <name>/u/kaizoku156</name>
      <uri>https://old.reddit.com/user/kaizoku156</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm just shocked by how good gemma 3 is, even the 1b model is so good, a good chunk of world knowledge jammed into such a small parameter size, I'm finding that i'm liking the answers of gemma 3 27b on ai studio more than gemini 2.0 flash for some Q&amp;amp;A type questions something like &amp;quot;how does back propogation work in llm training ?&amp;quot;. It's kinda crazy that this level of knowledge is available and can be run on something like a gt 710&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaizoku156"&gt; /u/kaizoku156 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9v3lf/gemma_3_insanely_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9v3lf/gemma_3_insanely_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9v3lf/gemma_3_insanely_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T21:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9vjf1</id>
    <title>üî• DeepSeek R1 671B Q4 - M3 Ultra 512GB with MLXüî•</title>
    <updated>2025-03-12T21:36:54+00:00</updated>
    <author>
      <name>/u/ifioravanti</name>
      <uri>https://old.reddit.com/user/ifioravanti</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes it works! First test, and I'm blown away!&lt;/p&gt; &lt;p&gt;Prompt: &amp;quot;Create an amazing animation using p5js&amp;quot;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;18.43 tokens/sec&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Generates a p5js zero-shot, tested at video's end&lt;/li&gt; &lt;li&gt;Video in real-time, no acceleration!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1j9vjf1/video/nmcm91wpvboe1/player"&gt;https://reddit.com/link/1j9vjf1/video/nmcm91wpvboe1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ifioravanti"&gt; /u/ifioravanti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9vjf1/deepseek_r1_671b_q4_m3_ultra_512gb_with_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9vjf1/deepseek_r1_671b_q4_m3_ultra_512gb_with_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9vjf1/deepseek_r1_671b_q4_m3_ultra_512gb_with_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T21:36:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jabin6</id>
    <title>C4AI Command A 111B</title>
    <updated>2025-03-13T13:06:41+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://cohere.com/blog/command-a"&gt;https://cohere.com/blog/command-a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/CohereForAI/c4ai-command-a-03-2025?ref=cohere-ai.ghost.io"&gt;https://huggingface.co/CohereForAI/c4ai-command-a-03-2025?ref=cohere-ai.ghost.io&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabin6/c4ai_command_a_111b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabin6/c4ai_command_a_111b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jabin6/c4ai_command_a_111b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T13:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jacyqt</id>
    <title>Check out the new theme of my open sourced desktop app, you can run LLMs locally with built-in RAG knowledge base and note-taking capabilities.</title>
    <updated>2025-03-13T14:15:51+00:00</updated>
    <author>
      <name>/u/w-zhong</name>
      <uri>https://old.reddit.com/user/w-zhong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jacyqt/check_out_the_new_theme_of_my_open_sourced/"&gt; &lt;img alt="Check out the new theme of my open sourced desktop app, you can run LLMs locally with built-in RAG knowledge base and note-taking capabilities." src="https://preview.redd.it/kzrghtnotgoe1.gif?width=640&amp;amp;crop=smart&amp;amp;s=464ed1f234a1c459c1c1cc69de42594f8ce42dd7" title="Check out the new theme of my open sourced desktop app, you can run LLMs locally with built-in RAG knowledge base and note-taking capabilities." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w-zhong"&gt; /u/w-zhong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kzrghtnotgoe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jacyqt/check_out_the_new_theme_of_my_open_sourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jacyqt/check_out_the_new_theme_of_my_open_sourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T14:15:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ja5m3w</id>
    <title>Open SORA 2.0 ! They are trolling openai again</title>
    <updated>2025-03-13T06:18:06+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://twitter.com/YangYou1991/status/1899973689460044010"&gt;https://twitter.com/YangYou1991/status/1899973689460044010&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo : &lt;a href="https://github.com/hpcaitech/Open-Sora"&gt;https://github.com/hpcaitech/Open-Sora&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja5m3w/open_sora_20_they_are_trolling_openai_again/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja5m3w/open_sora_20_they_are_trolling_openai_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ja5m3w/open_sora_20_they_are_trolling_openai_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T06:18:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ja0xnh</id>
    <title>Does Google not understand that DeepSeek R1 was trained in FP8?</title>
    <updated>2025-03-13T01:44:39+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja0xnh/does_google_not_understand_that_deepseek_r1_was/"&gt; &lt;img alt="Does Google not understand that DeepSeek R1 was trained in FP8?" src="https://preview.redd.it/5kbayoq13doe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=477d6f469b86c99e53a84538ec86ec48a60f156a" title="Does Google not understand that DeepSeek R1 was trained in FP8?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5kbayoq13doe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja0xnh/does_google_not_understand_that_deepseek_r1_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ja0xnh/does_google_not_understand_that_deepseek_r1_was/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T01:44:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ja2ers</id>
    <title>The duality of man</title>
    <updated>2025-03-13T03:00:08+00:00</updated>
    <author>
      <name>/u/jhanjeek</name>
      <uri>https://old.reddit.com/user/jhanjeek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja2ers/the_duality_of_man/"&gt; &lt;img alt="The duality of man" src="https://preview.redd.it/1ukvrj06hdoe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=392789bc2b1887610312054c20af823db4ab6078" title="The duality of man" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jhanjeek"&gt; /u/jhanjeek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1ukvrj06hdoe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja2ers/the_duality_of_man/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ja2ers/the_duality_of_man/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T03:00:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jabj70</id>
    <title>New model from Cohere: Command A!</title>
    <updated>2025-03-13T13:07:26+00:00</updated>
    <author>
      <name>/u/slimyXD</name>
      <uri>https://old.reddit.com/user/slimyXD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Command A is our new state-of-the-art addition to Command family optimized for demanding enterprises that require fast, secure, and high-quality models. &lt;/p&gt; &lt;p&gt;It offers maximum performance with minimal hardware costs when compared to leading proprietary and open-weights models, such as GPT-4o and DeepSeek-V3.&lt;/p&gt; &lt;p&gt;It features 111b, a 256k context window, with: * inference at a rate of up to 156 tokens/sec which is 1.75x higher than GPT-4o and 2.4x higher than DeepSeek-V3 * excelling performance on business-critical agentic and multilingual tasks * minimal hardware needs - its deployable on just two GPUs, compared to other models that typically require as many as 32&lt;/p&gt; &lt;p&gt;Check out our full report: &lt;a href="https://cohere.com/blog/command-a"&gt;https://cohere.com/blog/command-a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And the model card: &lt;a href="https://huggingface.co/CohereForAI/c4ai-command-a-03-2025"&gt;https://huggingface.co/CohereForAI/c4ai-command-a-03-2025&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's available to everyone now via Cohere API as &lt;code&gt;command-a-03-2025&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slimyXD"&gt; /u/slimyXD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabj70/new_model_from_cohere_command_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabj70/new_model_from_cohere_command_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jabj70/new_model_from_cohere_command_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T13:07:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jabh4m</id>
    <title>CohereForAI/c4ai-command-a-03-2025 ¬∑ Hugging Face</title>
    <updated>2025-03-13T13:04:32+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabh4m/cohereforaic4aicommanda032025_hugging_face/"&gt; &lt;img alt="CohereForAI/c4ai-command-a-03-2025 ¬∑ Hugging Face" src="https://external-preview.redd.it/8uC-fRvLBGmT6kbHk8wyBrnHknVe8WnRlviRost1iDM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=316e90209b2b29e953d13bdd363e048d518bc1d0" title="CohereForAI/c4ai-command-a-03-2025 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/CohereForAI/c4ai-command-a-03-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabh4m/cohereforaic4aicommanda032025_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jabh4m/cohereforaic4aicommanda032025_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T13:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jabmwz</id>
    <title>AMA with the Gemma Team</title>
    <updated>2025-03-13T13:12:40+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLlama! During the next day, the Gemma research and product team from DeepMind will be around to answer with your questions! Looking forward to them!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Technical Report: &lt;a href="https://goo.gle/Gemma3Report"&gt;https://goo.gle/Gemma3Report&lt;/a&gt;&lt;/li&gt; &lt;li&gt;AI Studio: &lt;a href="https://aistudio.google.com/prompts/new_chat?model=gemma-3-27b-it"&gt;https://aistudio.google.com/prompts/new_chat?model=gemma-3-27b-it&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Technical blog post &lt;a href="https://developers.googleblog.com/en/introducing-gemma3/"&gt;https://developers.googleblog.com/en/introducing-gemma3/&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Kaggle &lt;a href="https://www.kaggle.com/models/google/gemma-3"&gt;https://www.kaggle.com/models/google/gemma-3&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Hugging Face &lt;a href="https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d"&gt;https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ollama &lt;a href="https://ollama.com/library/gemma3"&gt;https://ollama.com/library/gemma3&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabmwz/ama_with_the_gemma_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabmwz/ama_with_the_gemma_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jabmwz/ama_with_the_gemma_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T13:12:40+00:00</published>
  </entry>
</feed>
