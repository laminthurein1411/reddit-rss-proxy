<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-28T12:09:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1izbmbb</id>
    <title>Perplexity R1 1776 performs worse than DeepSeek R1 for complex problems.</title>
    <updated>2025-02-27T09:06:25+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perplexity claims the reasoning abilities of R1 1776 are not affected by the decensoring process, but after testing it in &lt;a href="https://github.com/fairydreaming/lineage-bench/"&gt;lineage-bench&lt;/a&gt; I found that for very complex problems there are significant differences in the model performance.&lt;/p&gt; &lt;p&gt;Below you can see benchmark results for different problem sizes:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;lineage-8&lt;/th&gt; &lt;th align="left"&gt;lineage-16&lt;/th&gt; &lt;th align="left"&gt;lineage-32&lt;/th&gt; &lt;th align="left"&gt;lineage-64&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek R1&lt;/td&gt; &lt;td align="left"&gt;0.965&lt;/td&gt; &lt;td align="left"&gt;0.980&lt;/td&gt; &lt;td align="left"&gt;0.945&lt;/td&gt; &lt;td align="left"&gt;0.780&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;R1 1776&lt;/td&gt; &lt;td align="left"&gt;0.980&lt;/td&gt; &lt;td align="left"&gt;0.975&lt;/td&gt; &lt;td align="left"&gt;0.675&lt;/td&gt; &lt;td align="left"&gt;0.205&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;While for lineage-8 and lineage-16 problem sizes the model performance matches or even exceeds the original DeepSeek R1, for lineage-32 we can already observe difference in scores, while for lineage-64 R1 1776 score reached random guessing level.&lt;/p&gt; &lt;p&gt;So it looks like Perplexity claims about reasoning abilities not being affected by the decensoring process are not true.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We also ensured that the model‚Äôs math and reasoning abilities remained intact after the decensoring process. Evaluations on multiple benchmarks showed that our post-trained model performed on par with the base R1 model, indicating that the decensoring had no impact on its core reasoning capabilities.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Edit: here's one example prompt for lineage-64 and the model output generated in Perplexity Labs playground in case anyone is interested: &lt;a href="https://pastebin.com/EPy06bqp"&gt;https://pastebin.com/EPy06bqp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also Perplexity staff noticed my findings and are looking into the problem.&lt;/p&gt; &lt;p&gt;Update: Apparently it's a problem with the model serving stack and not with the model itself (it scored similar to DeepSeek R1 on lineage-64 in Perplexity internal test). Still waiting for the solution.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izbmbb/perplexity_r1_1776_performs_worse_than_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izbmbb/perplexity_r1_1776_performs_worse_than_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izbmbb/perplexity_r1_1776_performs_worse_than_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T09:06:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1izsmu7</id>
    <title>Anyone tried Granite3.2 yet?</title>
    <updated>2025-02-27T22:39:18+00:00</updated>
    <author>
      <name>/u/Hujkis9</name>
      <uri>https://old.reddit.com/user/Hujkis9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izsmu7/anyone_tried_granite32_yet/"&gt; &lt;img alt="Anyone tried Granite3.2 yet?" src="https://external-preview.redd.it/0c14HAxPkb4aDj5vcS7W2_hQsWUHo_3ZSJZ9EO23nF8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb6c6699299da5a1633be975fbfd08d1875472c9" title="Anyone tried Granite3.2 yet?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hujkis9"&gt; /u/Hujkis9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://research.ibm.com/blog/inference-scaling-reasoning-ai-model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izsmu7/anyone_tried_granite32_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izsmu7/anyone_tried_granite32_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T22:39:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1izy2l1</id>
    <title>Web Search using Local LLMs/We have Perplexity at home.</title>
    <updated>2025-02-28T03:09:45+00:00</updated>
    <author>
      <name>/u/Tokamakium</name>
      <uri>https://old.reddit.com/user/Tokamakium</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use the Page Assist browser plugin as frontend, it has Web Search built-in.&lt;/li&gt; &lt;li&gt;Any model good at following instructions will be good at web search.&lt;/li&gt; &lt;li&gt;The number of pages and the search engine used will be more important. For my testing, I searched 10 pages and used Google. You can change those in the Page Assist settings.&lt;/li&gt; &lt;li&gt;Keep it brief. Ask only one question. Be as specific as possible.&lt;/li&gt; &lt;li&gt;Hallucinations/Incomplete information is to be expected.&lt;/li&gt; &lt;li&gt;Always start a new chat for a new question.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Uses:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;When you want to know about something new but don't have the time to dig in.&lt;/li&gt; &lt;li&gt;Quickly checking the news.&lt;/li&gt; &lt;li&gt;That's pretty much it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Testing Parameters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;4k context length. Rest of the Ollama settings at default.&lt;/li&gt; &lt;li&gt;Models: Llama 3.1 8b q6_k, Gemma 9b, Phi 4 14b, Qwen 2.5-Coder 14b, DeepSeek r1 14b. Default quantizations available on Ollama, except for the Llama model.&lt;/li&gt; &lt;li&gt;3060 12GB with 16 GB RAM. Naturally, Llama 3.1 is the quickest and I can use up to 16k context length without using the CPU.&lt;/li&gt; &lt;li&gt;Tested with 2 pages/DDG and then 10 pages/Google. Made the largest difference.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Questions Asked:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What are the latest gameplay changes and events in Helldivers 2?&lt;/li&gt; &lt;li&gt;Summarize the latest Rust in Linux drama.&lt;/li&gt; &lt;li&gt;What is the best LLM I can run on a 3060 12GB?&lt;/li&gt; &lt;li&gt;What is the new Minion protocol for LLMs?&lt;/li&gt; &lt;li&gt;Give me a detailed summary of the latest Framework Company launch, including their specs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Summary of the replies:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama 3.1 8b is the quickest and performs almost at par with the other top models, so this will be my go-to.&lt;/li&gt; &lt;li&gt;Other models that performed well were DeepSeek and Qwen. After that was Phi and lastly Gemma.&lt;/li&gt; &lt;li&gt;No model recommended a specific model to run on my GPU.&lt;/li&gt; &lt;li&gt;The Framework question was the trickiest. Unless I mentioned that Framework is a company, models didn't know what to do with the question. Almost no model mentioned the new desktop launch, so I had to edit the question to get the answer I was seeking.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tokamakium"&gt; /u/Tokamakium &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izy2l1/web_search_using_local_llmswe_have_perplexity_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izy2l1/web_search_using_local_llmswe_have_perplexity_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izy2l1/web_search_using_local_llmswe_have_perplexity_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T03:09:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1izdrsd</id>
    <title>vLLM just landed FlashMLA (DeepSeek - day 1) in vLLM and it is already boosting output throughput 2-16% - expect more improvements in the coming days</title>
    <updated>2025-02-27T11:38:48+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izdrsd/vllm_just_landed_flashmla_deepseek_day_1_in_vllm/"&gt; &lt;img alt="vLLM just landed FlashMLA (DeepSeek - day 1) in vLLM and it is already boosting output throughput 2-16% - expect more improvements in the coming days" src="https://preview.redd.it/wnphfz5s4ole1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d23c35465c203ce3194ca69901bcbe56c0961102" title="vLLM just landed FlashMLA (DeepSeek - day 1) in vLLM and it is already boosting output throughput 2-16% - expect more improvements in the coming days" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wnphfz5s4ole1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izdrsd/vllm_just_landed_flashmla_deepseek_day_1_in_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izdrsd/vllm_just_landed_flashmla_deepseek_day_1_in_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T11:38:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j037ib</id>
    <title>Overview of best LLMs for each use-case</title>
    <updated>2025-02-28T08:38:45+00:00</updated>
    <author>
      <name>/u/Bitter-College8786</name>
      <uri>https://old.reddit.com/user/Bitter-College8786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I often read posts about people asking &amp;quot;what is the current best model for XY?&amp;quot; which is a fair question since there are new models every week. Maybe to make life easier, is there an overview site containing the best models for various categories sorted by size (best 3B for roleplay, best 7B for roleplay etc.)? which is curated regularly? &lt;/p&gt; &lt;p&gt;I was about to ask which LLM fits 6GB VRAM is good for an agent that can summarize E-mails and call functions. And then I thought maybe it can be generalized.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-College8786"&gt; /u/Bitter-College8786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j037ib/overview_of_best_llms_for_each_usecase/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j037ib/overview_of_best_llms_for_each_usecase/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j037ib/overview_of_best_llms_for_each_usecase/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T08:38:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1izm9pu</id>
    <title>I created this tool I named Reddit Thread Analyzer ‚Äì just paste a link, tweak a few settings, and get a detailed thread analysis. It's open-source and freely hosted.</title>
    <updated>2025-02-27T18:09:20+00:00</updated>
    <author>
      <name>/u/kyazoglu</name>
      <uri>https://old.reddit.com/user/kyazoglu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izm9pu/i_created_this_tool_i_named_reddit_thread/"&gt; &lt;img alt="I created this tool I named Reddit Thread Analyzer ‚Äì just paste a link, tweak a few settings, and get a detailed thread analysis. It's open-source and freely hosted." src="https://external-preview.redd.it/bWhwMHFib2gycWxlMYR5n1H2-KAhKVY699t1y87JffT7MDLWeztGuBbhiJoR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ce3d64ee8a302c6a140ce04850b2b4f5ed3d45c" title="I created this tool I named Reddit Thread Analyzer ‚Äì just paste a link, tweak a few settings, and get a detailed thread analysis. It's open-source and freely hosted." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyazoglu"&gt; /u/kyazoglu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rs0obaoh2qle1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izm9pu/i_created_this_tool_i_named_reddit_thread/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izm9pu/i_created_this_tool_i_named_reddit_thread/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T18:09:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1izztgk</id>
    <title>Ollama-VIC-20: A private Javascript based Ollama frontend weighing less than 20 kilobytes in size</title>
    <updated>2025-02-28T04:48:14+00:00</updated>
    <author>
      <name>/u/shokuninstudio</name>
      <uri>https://old.reddit.com/user/shokuninstudio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izztgk/ollamavic20_a_private_javascript_based_ollama/"&gt; &lt;img alt="Ollama-VIC-20: A private Javascript based Ollama frontend weighing less than 20 kilobytes in size" src="https://external-preview.redd.it/j90xDhGvNkgaqpvkfk9Dhqw9ktcgRVk8BghPgcLVYs8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ac9ae0469889f323659ca164a11205c27a03038" title="Ollama-VIC-20: A private Javascript based Ollama frontend weighing less than 20 kilobytes in size" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shokuninstudio"&gt; /u/shokuninstudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/shokuninstudio/Ollama-VIC-20"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izztgk/ollamavic20_a_private_javascript_based_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izztgk/ollamavic20_a_private_javascript_based_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T04:48:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j00nez</id>
    <title>El Salvador Passes Landmark AI Legislation</title>
    <updated>2025-02-28T05:38:48+00:00</updated>
    <author>
      <name>/u/Kurcide</name>
      <uri>https://old.reddit.com/user/Kurcide</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j00nez/el_salvador_passes_landmark_ai_legislation/"&gt; &lt;img alt="El Salvador Passes Landmark AI Legislation" src="https://external-preview.redd.it/ocfVLC1Uh_PhI6aexePFfI5dqkDdBhhX70fUuKjGFNE.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7189cab36fb3a0d65f6a8e9f6565e6d9e21e3d43" title="El Salvador Passes Landmark AI Legislation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As one of the main drafters for the law, I wanted to share this information here with a community I hold close. &lt;/p&gt; &lt;p&gt;Here is a brief with pieces paraphrased from a post by Mario Nawfal:&lt;/p&gt; &lt;p&gt;El Salvador today has passed pioneering AI legislation with support from President Bukele and the El Salvador Assembly.&lt;/p&gt; &lt;p&gt;The law provides regulatory clarity while protecting both proprietary and open-source AI models‚Äîparticularly safeguarding open-source development.&lt;/p&gt; &lt;p&gt;The legislation establishes legal protections for developers, including sandbox environments and shields against third-party misuse.&lt;/p&gt; &lt;p&gt;Additionally, the law will kick off the formation of a new AI Agency called ‚ÄúANIA‚Äù which will govern over the regulations put in place while also focusing on the adoption, implementation and support of AI technologies within the nation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kurcide"&gt; /u/Kurcide &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/kurcide/status/1895285130694140220?s=46&amp;amp;t=VR99OurDGxwYbO4oMH65gQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j00nez/el_salvador_passes_landmark_ai_legislation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j00nez/el_salvador_passes_landmark_ai_legislation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T05:38:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1izfy2d</id>
    <title>LLaDA - Large Language Diffusion Model (weights + demo)</title>
    <updated>2025-02-27T13:36:28+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;HF Demo:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/spaces/multimodalart/LLaDA"&gt;https://huggingface.co/spaces/multimodalart/LLaDA&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct"&gt;https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/GSAI-ML/LLaDA-8B-Base"&gt;https://huggingface.co/GSAI-ML/LLaDA-8B-Base&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2502.09992"&gt;https://arxiv.org/abs/2502.09992&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Diffusion LLMs are looking promising for alternative architecture. Some lab also recently announced a proprietary one (inception) which you could test, it can generate code quite well. &lt;/p&gt; &lt;p&gt;This stuff comes with the promise of parallelized token generation.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;LLaDA predicts all masked tokens simultaneously during each step of the reverse process.&amp;quot; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So we wouldn't need super high bandwidth for fast t/s anymore. It's not memory bandwidth bottlenecked, it has a compute bottleneck. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izfy2d/llada_large_language_diffusion_model_weights_demo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izfy2d/llada_large_language_diffusion_model_weights_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izfy2d/llada_large_language_diffusion_model_weights_demo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T13:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1izm42j</id>
    <title>What is Aider?</title>
    <updated>2025-02-27T18:03:05+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izm42j/what_is_aider/"&gt; &lt;img alt="What is Aider?" src="https://preview.redd.it/6kgjr75i1qle1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a96cf91fad0adc3fea1292d175b92cbc64800ec" title="What is Aider?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seriously, what is Aider? Is it a model? Or a benchmark? Or a cli? Or a browser extension? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6kgjr75i1qle1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izm42j/what_is_aider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izm42j/what_is_aider/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T18:03:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ize4n0</id>
    <title>Dual 5090FE</title>
    <updated>2025-02-27T12:01:15+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ize4n0/dual_5090fe/"&gt; &lt;img alt="Dual 5090FE" src="https://preview.redd.it/defh49ux8ole1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=843767edca5506f54e0bdb3a8b57d7e022c97a89" title="Dual 5090FE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/defh49ux8ole1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ize4n0/dual_5090fe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ize4n0/dual_5090fe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T12:01:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1iztn9l</id>
    <title>New Karpathy's video: How I use LLMs</title>
    <updated>2025-02-27T23:25:34+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iztn9l/new_karpathys_video_how_i_use_llms/"&gt; &lt;img alt="New Karpathy's video: How I use LLMs" src="https://external-preview.redd.it/eYKZhXcOfzaddtzPaAUR7x_cqHQEGFWwSvoLcPORhV0.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f77784dc12aee9b44f5def7b81176eaff04e2195" title="New Karpathy's video: How I use LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not as techical as his past videos, but still lots of nice insights.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/EWvNQjAaOHw?si=lixNIZJRppLshiw9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iztn9l/new_karpathys_video_how_i_use_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iztn9l/new_karpathys_video_how_i_use_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T23:25:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1j00wiz</id>
    <title>LongRoPE2: Near-Lossless LLM Context Window Scaling</title>
    <updated>2025-02-28T05:55:04+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.20082"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j00wiz/longrope2_nearlossless_llm_context_window_scaling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j00wiz/longrope2_nearlossless_llm_context_window_scaling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T05:55:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1izqhfw</id>
    <title>Any theories on what's going on here for this coding benchmark?</title>
    <updated>2025-02-27T21:05:52+00:00</updated>
    <author>
      <name>/u/__eita__</name>
      <uri>https://old.reddit.com/user/__eita__</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izqhfw/any_theories_on_whats_going_on_here_for_this/"&gt; &lt;img alt="Any theories on what's going on here for this coding benchmark?" src="https://preview.redd.it/uc4k9x64yqle1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e3fc22fdd5fdbf678a6a10b6d9392048ae32f70" title="Any theories on what's going on here for this coding benchmark?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why a reasoning model would perform way better for swe-bench verified while performing poorly for swe-lancer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__eita__"&gt; /u/__eita__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uc4k9x64yqle1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izqhfw/any_theories_on_whats_going_on_here_for_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izqhfw/any_theories_on_whats_going_on_here_for_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T21:05:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1izqu52</id>
    <title>Its ARC-AGI | DeepSeek R1 is better than GPT 4.5</title>
    <updated>2025-02-27T21:20:59+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izqu52/its_arcagi_deepseek_r1_is_better_than_gpt_45/"&gt; &lt;img alt="Its ARC-AGI | DeepSeek R1 is better than GPT 4.5" src="https://preview.redd.it/f2320u7s0rle1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7d73253b157077c71953201a581eb62570026ee" title="Its ARC-AGI | DeepSeek R1 is better than GPT 4.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f2320u7s0rle1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izqu52/its_arcagi_deepseek_r1_is_better_than_gpt_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izqu52/its_arcagi_deepseek_r1_is_better_than_gpt_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T21:20:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j00pjp</id>
    <title>Is it not possible for NVIDIA to make VRAM extensions for other PCIE slots? Or other dedicated AI hardware?</title>
    <updated>2025-02-28T05:42:32+00:00</updated>
    <author>
      <name>/u/Business_Respect_910</name>
      <uri>https://old.reddit.com/user/Business_Respect_910</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it not possible for NVIDIA to make a new (or old idk) kind of hardware to just expand your vram?&lt;/p&gt; &lt;p&gt;I'm assuming the PCIE slots carry the same data speeds but if this is not possible at all, i will ask could NVIDIA then make a dedicated AI module rather than a graphics card?&lt;/p&gt; &lt;p&gt;Seems like the market for such a thing might not be huge but couldn't they do a decent markup and make them in smaller batches?&lt;/p&gt; &lt;p&gt;Just seems like 32gb vram is pretty small for the storage options we have today? But idk maybe the speeds they operate at are much more expensive to make?&lt;/p&gt; &lt;p&gt;Very curious to see in the future if we get actual AI hardware or we just keep working off what we have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Business_Respect_910"&gt; /u/Business_Respect_910 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j00pjp/is_it_not_possible_for_nvidia_to_make_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j00pjp/is_it_not_possible_for_nvidia_to_make_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j00pjp/is_it_not_possible_for_nvidia_to_make_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T05:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1izmsrl</id>
    <title>Building a robot that can see, hear, talk, and dance. Powered by on-device AI!</title>
    <updated>2025-02-27T18:31:16+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izmsrl/building_a_robot_that_can_see_hear_talk_and_dance/"&gt; &lt;img alt="Building a robot that can see, hear, talk, and dance. Powered by on-device AI!" src="https://external-preview.redd.it/Y3JjNzRwc2k2cWxlMe__omCO_n66cYU7Fe7wXFz05iYznG-U5sQ5kSodSfXF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c3b1aca9ec3aa181f1b6214eda1a89059c04a5ab" title="Building a robot that can see, hear, talk, and dance. Powered by on-device AI!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h13dgnsi6qle1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izmsrl/building_a_robot_that_can_see_hear_talk_and_dance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izmsrl/building_a_robot_that_can_see_hear_talk_and_dance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T18:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1izf4zf</id>
    <title>Pythagoras : i should've guessed first hand üò© !</title>
    <updated>2025-02-27T12:59:00+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izf4zf/pythagoras_i_shouldve_guessed_first_hand/"&gt; &lt;img alt="Pythagoras : i should've guessed first hand üò© !" src="https://preview.redd.it/m3vrfaz8jole1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e5f34f13dda8627c1b31cbceebdf6cfb503c19e" title="Pythagoras : i should've guessed first hand üò© !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m3vrfaz8jole1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izf4zf/pythagoras_i_shouldve_guessed_first_hand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izf4zf/pythagoras_i_shouldve_guessed_first_hand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T12:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1izwh49</id>
    <title>DeepSeek OpenSourceWeek Day 5</title>
    <updated>2025-02-28T01:45:14+00:00</updated>
    <author>
      <name>/u/EssayHealthy5075</name>
      <uri>https://old.reddit.com/user/EssayHealthy5075</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fire-Flyer File System (3FS)&lt;/p&gt; &lt;p&gt;Fire-Flyer File System (3FS) - a parallel file system that utilizes the full bandwidth of modern SSDs and RDMA networks.&lt;/p&gt; &lt;p&gt;‚ö° 6.6 TiB/s aggregate read throughput in a 180-node cluster.&lt;/p&gt; &lt;p&gt;‚ö° 3.66 TiB/min throughput on GraySort benchmark in a 25-node cluster.&lt;/p&gt; &lt;p&gt;‚ö° 40+ GiB/s peak throughput per client node for KVCache lookup.&lt;/p&gt; &lt;p&gt;üß¨ Disaggregated architecture with strong consistency semantics.&lt;/p&gt; &lt;p&gt;‚úÖ Training data preprocessing, dataset loading, checkpoint saving/reloading, embedding vector search &amp;amp; KVCache lookups for inference in V3/R1.&lt;/p&gt; &lt;p&gt;üîó 3FS ‚Üí &lt;a href="https://github.com/deepseek-ai/3FS"&gt;https://github.com/deepseek-ai/3FS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Smallpond - data processing framework on 3FS ‚Üí &lt;a href="https://github.com/deepseek-ai/smallpond"&gt;https://github.com/deepseek-ai/smallpond&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EssayHealthy5075"&gt; /u/EssayHealthy5075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izwh49/deepseek_opensourceweek_day_5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izwh49/deepseek_opensourceweek_day_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izwh49/deepseek_opensourceweek_day_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T01:45:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1izy4by</id>
    <title>2 diffusion LLMs in one day -&gt; don't undermine the underdog</title>
    <updated>2025-02-28T03:12:18+00:00</updated>
    <author>
      <name>/u/dp3471</name>
      <uri>https://old.reddit.com/user/dp3471</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First, its awesome that we're getting frequent and amazing model releases - seemingly by the day right now. &lt;/p&gt; &lt;p&gt;Inception labs released&lt;a href="https://www.inceptionlabs.ai/news"&gt; mercury coder&lt;/a&gt;, a (by my testing) somewhat competent model which can code on a 1 to 2 year old SOTA (as good as the best models 1-2 years ago), having the benefit of being really cool to see the diffusion process. Really scratches an itch (perhaps one of some interpretability?). &lt;strong&gt;Promises 700-1000 t/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Reason why I say time period instead of model - it suffers from many of the same issues I remember GPT 4 (and turbo) suffering from. You should check it out anyways.&lt;/p&gt; &lt;p&gt;And, for some reason on the same day (at least model weights uploaded, preprint earlier), we get &lt;a href="https://arxiv.org/pdf/2502.09992"&gt;LLaDA&lt;/a&gt;, an open-source diffusion model which seems to be somewhat of a contender for llama 3 8b with benchmarks, and gives some degree of freedom in terms of guiding (not forcing, sometimes doesn't work) the nth word to be a specified one. I found the quality in the &lt;a href="https://huggingface.co/spaces/multimodalart/LLaDA"&gt;demo &lt;/a&gt;to be much worse than any recent models, but I also noticed it improved a TON as I played around and adjusted certain prompting (and word targets, really cool). Check this out too - its different from mercury. &lt;/p&gt; &lt;p&gt;TLDR; 2 cool new diffusion-based LLMs, a closed-source one comparable to GPT-4 (based on my vibe checking) promising 700-1000 t/s (technically 2 different models by size), and an open-source one reported to be LLaMa3.1-8b-like, but testing (again, mine only) shows more testing is needed lol.&lt;/p&gt; &lt;p&gt;Don't let the open source model be overshadowed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dp3471"&gt; /u/dp3471 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izy4by/2_diffusion_llms_in_one_day_dont_undermine_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izy4by/2_diffusion_llms_in_one_day_dont_undermine_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izy4by/2_diffusion_llms_in_one_day_dont_undermine_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T03:12:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1izt03h</id>
    <title>I have to share this with you - Free-Form Chat for writing, 100% local</title>
    <updated>2025-02-27T22:55:55+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izt03h/i_have_to_share_this_with_you_freeform_chat_for/"&gt; &lt;img alt="I have to share this with you - Free-Form Chat for writing, 100% local" src="https://preview.redd.it/7781ripihrle1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=503141823b634142caf9d0103c96974965baa8ef" title="I have to share this with you - Free-Form Chat for writing, 100% local" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7781ripihrle1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izt03h/i_have_to_share_this_with_you_freeform_chat_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izt03h/i_have_to_share_this_with_you_freeform_chat_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T22:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1izoyxk</id>
    <title>A diffusion based 'small' coding LLM that is 10x faster in token generation than transformer based LLMs (apparently 1000 tok/s on H100)</title>
    <updated>2025-02-27T20:01:26+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Karpathy post: &lt;a href="https://xcancel.com/karpathy/status/1894923254864978091"&gt;https://xcancel.com/karpathy/status/1894923254864978091&lt;/a&gt; (covers some interesting nuance about transformer vs diffusion for image/video vs text)&lt;/p&gt; &lt;p&gt;Artificial analysis comparison: &lt;a href="https://pbs.twimg.com/media/GkvZinZbAAABLVq.jpg?name=orig"&gt;https://pbs.twimg.com/media/GkvZinZbAAABLVq.jpg?name=orig&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo video: &lt;a href="https://xcancel.com/InceptionAILabs/status/1894847919624462794"&gt;https://xcancel.com/InceptionAILabs/status/1894847919624462794&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The chat link (down rn, probably over capacity) &lt;a href="https://chat.inceptionlabs.ai/"&gt;https://chat.inceptionlabs.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What's interesting here is that this thing generates all tokens at once and then goes through refinements as opposed to transformer based one token at a time. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izoyxk/a_diffusion_based_small_coding_llm_that_is_10x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izoyxk/a_diffusion_based_small_coding_llm_that_is_10x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izoyxk/a_diffusion_based_small_coding_llm_that_is_10x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T20:01:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1j00v4y</id>
    <title>"Crossing the uncanny valley of conversational voice" post by Sesame - realtime conversation audio model rivalling OpenAI</title>
    <updated>2025-02-28T05:52:31+00:00</updated>
    <author>
      <name>/u/iGermanProd</name>
      <uri>https://old.reddit.com/user/iGermanProd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So this is one of the craziest voice demos I've heard so far, and they apparently want to release their models under an Apache-2.0 license in the future: I've never heard of Sesame, they seem to be very new.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Our models will be available under an Apache 2.0 license&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Your thoughts? Check the demo first: &lt;a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo"&gt;https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;No public weights yet, we can only dream and hope, but this easily matches or beats OpenAI's Advanced Voice Mode. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iGermanProd"&gt; /u/iGermanProd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j00v4y/crossing_the_uncanny_valley_of_conversational/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j00v4y/crossing_the_uncanny_valley_of_conversational/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j00v4y/crossing_the_uncanny_valley_of_conversational/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T05:52:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1izvwck</id>
    <title>DeepSeek Realse 5th Bomb! Cluster Bomb Again! 3FS (distributed file system) &amp; smallpond (A lightweight data processing framework)</title>
    <updated>2025-02-28T01:15:12+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izvwck/deepseek_realse_5th_bomb_cluster_bomb_again_3fs/"&gt; &lt;img alt="DeepSeek Realse 5th Bomb! Cluster Bomb Again! 3FS (distributed file system) &amp;amp; smallpond (A lightweight data processing framework)" src="https://external-preview.redd.it/HvC95tBfvHDGJxAbUH6W9PmwC54Tm2U3z7QQDPE9EaM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f064ebf3053086f57b27efe553869e937081d60d" title="DeepSeek Realse 5th Bomb! Cluster Bomb Again! 3FS (distributed file system) &amp;amp; smallpond (A lightweight data processing framework)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can't believe DeepSeek has even revolutionized storage architecture... The last time I was amazed by a network file system was with HDFS and CEPH. But those are disk-oriented distributed file systems. Now, a truly modern SSD and RDMA network-oriented file system has been born!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3FS&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications&lt;/p&gt; &lt;p&gt;link: &lt;a href="https://github.com/deepseek-ai/3FS"&gt;https://github.com/deepseek-ai/3FS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;smallpond&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A lightweight data processing framework built on &lt;a href="https://duckdb.org/"&gt;DuckDB&lt;/a&gt; and &lt;a href="https://github.com/deepseek-ai/3FS"&gt;3FS&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;link: &lt;a href="https://github.com/deepseek-ai/smallpond"&gt;https://github.com/deepseek-ai/smallpond&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/inqemmkh6sle1.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f451f20a22278229810505083e59b914b64fd82"&gt;https://preview.redd.it/inqemmkh6sle1.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f451f20a22278229810505083e59b914b64fd82&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izvwck/deepseek_realse_5th_bomb_cluster_bomb_again_3fs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izvwck/deepseek_realse_5th_bomb_cluster_bomb_again_3fs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izvwck/deepseek_realse_5th_bomb_cluster_bomb_again_3fs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T01:15:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1j045xn</id>
    <title>I trained a reasoning model that speaks French‚Äîfor just $20! ü§Øüá´üá∑</title>
    <updated>2025-02-28T09:51:24+00:00</updated>
    <author>
      <name>/u/TheREXincoming</name>
      <uri>https://old.reddit.com/user/TheREXincoming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1j045xn/video/mvudzukrpule1/player"&gt;https://reddit.com/link/1j045xn/video/mvudzukrpule1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheREXincoming"&gt; /u/TheREXincoming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j045xn/i_trained_a_reasoning_model_that_speaks_frenchfor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j045xn/i_trained_a_reasoning_model_that_speaks_frenchfor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j045xn/i_trained_a_reasoning_model_that_speaks_frenchfor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T09:51:24+00:00</published>
  </entry>
</feed>
