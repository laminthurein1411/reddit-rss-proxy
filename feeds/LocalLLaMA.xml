<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-18T09:48:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i421qo</id>
    <title>huggingface alernatives</title>
    <updated>2025-01-18T07:05:15+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;out of curiosity what are some good huggingface alternatives? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i421qo/huggingface_alernatives/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i421qo/huggingface_alernatives/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i421qo/huggingface_alernatives/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T07:05:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3ilu3</id>
    <title>[REPOST]Linux 6.14 will have amdxdna! The Ryzen AI NPU driver</title>
    <updated>2025-01-17T15:17:23+00:00</updated>
    <author>
      <name>/u/KillerX629</name>
      <uri>https://old.reddit.com/user/KillerX629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What will this mean for amd cards and AI inference?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KillerX629"&gt; /u/KillerX629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3ilu3/repostlinux_614_will_have_amdxdna_the_ryzen_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3ilu3/repostlinux_614_will_have_amdxdna_the_ryzen_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3ilu3/repostlinux_614_will_have_amdxdna_the_ryzen_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T15:17:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3r6iu</id>
    <title>Function calling in llama.cpp?</title>
    <updated>2025-01-17T21:25:35+00:00</updated>
    <author>
      <name>/u/Few_Acanthisitta_858</name>
      <uri>https://old.reddit.com/user/Few_Acanthisitta_858</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How are you using function calling in llama.cpp? I tried few things but it doesn't really seem to work üòï &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Acanthisitta_858"&gt; /u/Few_Acanthisitta_858 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3r6iu/function_calling_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3r6iu/function_calling_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3r6iu/function_calling_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T21:25:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i40bx0</id>
    <title>What do I need to use to lip sync with audio just a few seconds / segment of a video?</title>
    <updated>2025-01-18T05:10:49+00:00</updated>
    <author>
      <name>/u/thescientificindian</name>
      <uri>https://old.reddit.com/user/thescientificindian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a project, I'm looking to record an actor, and swap just a few words from the video with their voice customized to the user's preference. For example: If in the video, the actor says: I know David. If you're wondering how he makes great videos, checkout this page.&lt;/p&gt; &lt;p&gt;Here I want to configure it this way: I know $name. If you're wondering how $genderpronoun makes great videos, checkout this page.&lt;/p&gt; &lt;p&gt;So, on an input box of my website, if they input their name to Steve, and select the gender as Male, it needs to lip sync the audio and video to that name and pronoun and provide the updated video with the same voice and lip sync output video. &lt;/p&gt; &lt;p&gt;Any ideas on how to make this happen? I've looked into HeyGen, Wave2Lip and others, but they're mostly for making new videos from scratch with completely new scripts or training them. I'm looking for it to generate within a few seconds to a minute by sticking to the original video and script but only changing 2 words. Any local implementation or free or paid APIs would be much helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thescientificindian"&gt; /u/thescientificindian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i40bx0/what_do_i_need_to_use_to_lip_sync_with_audio_just/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i40bx0/what_do_i_need_to_use_to_lip_sync_with_audio_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i40bx0/what_do_i_need_to_use_to_lip_sync_with_audio_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T05:10:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3fli7</id>
    <title>Laptop LLM performance - beware of the power settings!</title>
    <updated>2025-01-17T12:48:10+00:00</updated>
    <author>
      <name>/u/YordanTU</name>
      <uri>https://old.reddit.com/user/YordanTU</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's pity that I did such a lame negligence, but want to share with you, in case someone struggles with the same issue.&lt;/p&gt; &lt;p&gt;Both me and the wife have Lenovo gaming laptops:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Rizen 5, 16GB DDR5 RAM, 3050ti 4GB&lt;/li&gt; &lt;li&gt;i5, 16GB DDR5 RAM, 4060 8GB&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Logically, if a model fits entirely in the VRAM, the machine 2 runs it noticeble faster. BUT, everything beyond 7B which is partially offloaded in VRAM, (like Qwen 2.5 14B, 26/49 layers offloaded to GPU) practically goes with less than 0.2T/s and takes 2-3 minutes to output the first token on the machine 2! While machine 1 runs the same Qwen 2.5 (14B, 9/49 layers offloaded to GPU) quite acceptable with around 2T/s.&lt;/p&gt; &lt;p&gt;I was changing nVidia/CUDA drivers, settings of llama.cpp - nothing helped. Till I checked the &amp;quot;power settings&amp;quot; of Windows and changed the presets from &amp;quot;balanced&amp;quot; to &amp;quot;performance&amp;quot;. It was the CPU/RAM of the machine which killed all the fun. Now I get 5-10 T/s with 14B model and 26/49 layers to GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YordanTU"&gt; /u/YordanTU &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3fli7/laptop_llm_performance_beware_of_the_power/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3fli7/laptop_llm_performance_beware_of_the_power/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3fli7/laptop_llm_performance_beware_of_the_power/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T12:48:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3h7hs</id>
    <title>Attend - Proof of Concept</title>
    <updated>2025-01-17T14:12:36+00:00</updated>
    <author>
      <name>/u/Pedalnomica</name>
      <uri>https://old.reddit.com/user/Pedalnomica</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've gotten fed up with hoping on the computer to do one thing, and doing other stuff instead.&lt;/p&gt; &lt;p&gt;I'm building Attend so that our devices can help us dedicate our time and attention on what matters to us, instead of what some algorithm was optimized for.&lt;/p&gt; &lt;p&gt;Right now, it is a voice assistant that uses a vision LLM to &amp;quot;watch&amp;quot; your screen and help you get back on track if what you're doing isn't aligned with what you said you wanted to do.&lt;/p&gt; &lt;p&gt;I've got some work to do on the workflows and prompts to reduce false positives, but it &amp;quot;works&amp;quot; and I'm very excited about it!&lt;/p&gt; &lt;p&gt;I'd like to get this down to a single 3090, but two seems pretty feasible. Part of the problem is most open weight vision language models are garbage with 4K images/screenshots. Qwen2-VL seems to be an exception, but it (especially the 7B) is garbage when it comes to driving the workflows behind Attend. So, I've just been using Qwen2-VL-7B-Instruct and Llama-3.3 at 8-bit as I get it working. I'd love to hear suggestions for minimizing the VRAM required (Intern2_5-VL also seems to handle 4K alright, but I haven't tested it enough on the workflows).&lt;/p&gt; &lt;p&gt;Attend interfaces with all models using OpenAI compatable API calls. So, you should be able to use the cloud, if you're into that kinda thing... You could also take a hybrid approach. I think you could get the STT and vision LLM into 16GB VRAM and run that locally. Piper TTS runs well on CPU. You could then use a cloud model just for the text LLM and STT and keep the most sensitive stuff (screenshots!) local.&lt;/p&gt; &lt;p&gt;Check out the open-source code &lt;a href="https://github.com/hyperfocAIs/Attend/"&gt;https://github.com/hyperfocAIs/Attend/&lt;/a&gt; and a proof of concept video &lt;a href="https://youtu.be/PETrY540zMM"&gt;https://youtu.be/PETrY540zMM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Typos, clarified that this project is open source.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pedalnomica"&gt; /u/Pedalnomica &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3h7hs/attend_proof_of_concept/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3h7hs/attend_proof_of_concept/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3h7hs/attend_proof_of_concept/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T14:12:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3nbb7</id>
    <title>Any "mainstream" apps with genuinely useful local AI features?</title>
    <updated>2025-01-17T18:36:57+00:00</updated>
    <author>
      <name>/u/intofuture</name>
      <uri>https://old.reddit.com/user/intofuture</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious if any of you actually regularly use features in apps with local AI processing?&lt;/p&gt; &lt;p&gt;When I say &amp;quot;mainstream app&amp;quot;, I mean more like PyCharm from JetBrains (i.e. making lots of money, large teams behind them, etc.) than an open-source/indie dev app.&lt;/p&gt; &lt;p&gt;And I'm more talking about a feature in an app (which does a bunch of things other than that AI feature), as opposed to an app that's entirely about using AI locally, like Ollama, LMStudio, etc.&lt;/p&gt; &lt;p&gt;I'm also not talking about OS features, e.g. auto-complete on iPhones. More interested in apps that you've downloaded.&lt;/p&gt; &lt;p&gt;Currently, the only thing I can think of in my day-to-day is &lt;a href="https://blog.jetbrains.com/ai/2024/11/jetbrains-ai-assistant-2024-3/"&gt;code completion in PyCharm&lt;/a&gt;, but even that is now some kind of hybrid local/cloud thing.&lt;/p&gt; &lt;p&gt;EDIT: Not necessarily just talking about LLM stuff. Realized that I also use some photo editing apps every now and then with local ML models (but that's all pretty old tech, e.g. interactive background removal/segmentation)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/intofuture"&gt; /u/intofuture &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3nbb7/any_mainstream_apps_with_genuinely_useful_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3nbb7/any_mainstream_apps_with_genuinely_useful_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3nbb7/any_mainstream_apps_with_genuinely_useful_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T18:36:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3as1m</id>
    <title>OpenWebUI Canvas Implementation -- Coming Soon! (Better Artifacts)</title>
    <updated>2025-01-17T07:02:43+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3as1m/openwebui_canvas_implementation_coming_soon/"&gt; &lt;img alt="OpenWebUI Canvas Implementation -- Coming Soon! (Better Artifacts)" src="https://a.thumbs.redditmedia.com/11a6AQbm8PHTqUzymosrsOz6WrQ1h5fnohaqQF7icF0.jpg" title="OpenWebUI Canvas Implementation -- Coming Soon! (Better Artifacts)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ytezb1q05ide1.png?width=1862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=93364222443da5f695a745265842c91ee604d9e5"&gt;C# and XML View&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1ttzjm4s5ide1.png?width=1862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd00eb16ef20e090d9f5ebee0d69f48c4f3b8bf0"&gt;Design View&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7tj92xav5ide1.png?width=1749&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=81d8f9dec9bd3575fb4fc4ea8d399627b2eacd4a"&gt;Code View&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi all! I'm implementing Canvas (beefing up Artifacts) on OpenWebUI.&lt;/p&gt; &lt;p&gt;This was my only issue ever with OpenWebUI, just the very limited canvas feature (only restricted to HTML, CSS, JavaScript and SVG).&lt;/p&gt; &lt;p&gt;I've expanded support for the following languages:&lt;/p&gt; &lt;p&gt;C#, Python, Java, PHP, Ruby, Bash, Shell, AppleScript, SQL, JSON, XML, YAML, Markdown, HTML&lt;/p&gt; &lt;p&gt;If I'm missing one feel free to comment it! It's super easy to add at this point.&lt;/p&gt; &lt;p&gt;Another notable feature I'm adding is to switch between Design view and Code view for web design.&lt;/p&gt; &lt;p&gt;I'm super close to finishing! I just need to clean it up and visualize/track changes between revisions. Expect my pull request it in the next couple of weeks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3as1m/openwebui_canvas_implementation_coming_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3as1m/openwebui_canvas_implementation_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3as1m/openwebui_canvas_implementation_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T07:02:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i43sdo</id>
    <title>Object and shape detection</title>
    <updated>2025-01-18T09:14:19+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, are there some model that can be trained to detect shape objects I drew?&lt;/p&gt; &lt;p&gt;Do you have some ressource to help for that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i43sdo/object_and_shape_detection/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i43sdo/object_and_shape_detection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i43sdo/object_and_shape_detection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T09:14:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3qfgy</id>
    <title>AI Research</title>
    <updated>2025-01-17T20:52:03+00:00</updated>
    <author>
      <name>/u/ASI-Enjoyer</name>
      <uri>https://old.reddit.com/user/ASI-Enjoyer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do we still need AI research, or is ASI just a matter of scaling? I'm 17 years old and I want to become an AI researcher. I want to know your opinion/get advice&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ASI-Enjoyer"&gt; /u/ASI-Enjoyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3qfgy/ai_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3qfgy/ai_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3qfgy/ai_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T20:52:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3rpsh</id>
    <title>The ‚Äúapple‚Äù test - Why aren‚Äôt newer reasoning models doing better on this basic benchmark? (and yes, I know token prediction mechanics play a role)</title>
    <updated>2025-01-17T21:49:22+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most of you are probably familiar with the infamous LLM ‚Äúapple test‚Äù benchmark.&lt;/p&gt; &lt;p&gt;If you‚Äôre not, here it is, you give an LLM the following seemingly simple instruction prompt:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Write 10 sentences that end in the word ‚Äúapple‚Äù.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sadly, most open source (and even a lot of frontier models fail miserably at this task. I‚Äôve read that it has a lot to do with the way token prediction works, but some models can actually pass this test easily.&lt;/p&gt; &lt;p&gt;Models that I‚Äôve tested that pass or fail on this test:&lt;/p&gt; &lt;p&gt;LLMs that PASS the apple test:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama 3.3:70b (Q4KM)&lt;/li&gt; &lt;li&gt;Athene-V2 (Q4KM)&lt;/li&gt; &lt;li&gt;Nemotron (Q4KM)&lt;/li&gt; &lt;li&gt;Qwen 2.5:72b (Q4KM)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;LLMs that FAIL the apple test (most are newer models) &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Phi-4 14b (FP16)&lt;/li&gt; &lt;li&gt;InternLM3 (FP16)&lt;/li&gt; &lt;li&gt;Falcon 3 10b (FP16)&lt;/li&gt; &lt;li&gt;Granite 3 Dense (FP16)&lt;/li&gt; &lt;li&gt;QwQ 32b (Q_8)&lt;/li&gt; &lt;li&gt;GLM-4 8b (FP16)&lt;/li&gt; &lt;li&gt;Command-R (Q4KM)&lt;/li&gt; &lt;li&gt;MiniCPM 8b v2.6 (FP16)&lt;/li&gt; &lt;li&gt;Mistral Small 22b (Q4KM)&lt;/li&gt; &lt;li&gt;Nemotron Mini 4b (FP16)&lt;/li&gt; &lt;li&gt;Qwen 2.5 7b (FP16) &lt;/li&gt; &lt;li&gt;WizardLM2 7b (FP16)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;FAILED but with an honorable mention:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Olmo2 14b (FP16) - this model is lightning fast and got 8 of 10 consistently correct and was able to fix its mistake after a second shot at it (most models won‚Äôt do better with more chances). &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This task seems to be challenging for models under 70b to complete. Even the newer reasoning models with higher test time compute capabilities don‚Äôt seem to do well at all. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Why haven‚Äôt newer models gotten better at this task over time? &lt;/li&gt; &lt;li&gt;Is the underlying mechanism of token prediction still preventing success? &lt;/li&gt; &lt;li&gt;Are the models that this works with just cheating by training to pass the specific benchmark? &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone found an open source model under 70b that can pass the apple test consistently? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3rpsh/the_apple_test_why_arent_newer_reasoning_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3rpsh/the_apple_test_why_arent_newer_reasoning_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3rpsh/the_apple_test_why_arent_newer_reasoning_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T21:49:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1i42sxd</id>
    <title>Self hosted avatar generation?</title>
    <updated>2025-01-18T08:00:46+00:00</updated>
    <author>
      <name>/u/x0rchid</name>
      <uri>https://old.reddit.com/user/x0rchid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a model/platform/framework for generating personal avatars (i.e., avatar replica from images/videos, own voice, etc)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/x0rchid"&gt; /u/x0rchid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i42sxd/self_hosted_avatar_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i42sxd/self_hosted_avatar_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i42sxd/self_hosted_avatar_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T08:00:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3kv1n</id>
    <title>[Magnum/SE] LLama 3.3 70b</title>
    <updated>2025-01-17T16:54:07+00:00</updated>
    <author>
      <name>/u/lucyknada</name>
      <uri>https://old.reddit.com/user/lucyknada</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello again, folks!&lt;/p&gt; &lt;p&gt;We've got something a little different to share this time. It's not a full release or a new series as of yet, but more like an epilogue to the v4 series we released a few months back. DoctorShotgun wasn't entirely satisfied with how the large models in the series turned out, so he spent some more time in the lab - this time on the newer llama 3.3 model for a change:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Doctor-Shotgun/L3.3-70B-Magnum-v4-SE"&gt;https://huggingface.co/Doctor-Shotgun/L3.3-70B-Magnum-v4-SE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This time, the model was trained as an rslora with recommendations from Gryphe of Mythomax fame, and it comes with the full set of adapter checkpoints for mergers and other experimenters to play around with (&lt;a href="https://huggingface.co/Doctor-Shotgun/Magnum-v4-SE-70B-LoRA"&gt;available here&lt;/a&gt;). Preliminary testing suggests that rslora adequately style-transfers the classic Claude-y flavor of magnum to the llama 3.3 model.&lt;/p&gt; &lt;p&gt;In terms of changes to the data, the model doesn't deviate too far from the v4 series. The dataset includes some further cleaning of the RP log dataset used in v4, as well as the re-introduction of a subset of the data used in the v2 and earlier models. As per usual, the training config is linked from the model card in the spirit of open source.&lt;/p&gt; &lt;p&gt;No first-party quants are available at this time, but links to those created by well-known quanters are linked in the model description.&lt;/p&gt; &lt;p&gt;Hope you enjoy this belated New Years present, and stay tuned for what's to come!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lucyknada"&gt; /u/lucyknada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3kv1n/magnumse_llama_33_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3kv1n/magnumse_llama_33_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3kv1n/magnumse_llama_33_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T16:54:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3ztzu</id>
    <title>The best embedding model so far iamgroot42/rover_nexus</title>
    <updated>2025-01-18T04:40:54+00:00</updated>
    <author>
      <name>/u/AlgorithmicKing</name>
      <uri>https://old.reddit.com/user/AlgorithmicKing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No need for reranker just use it and its also top in &lt;a href="https://huggingface.co/spaces/mteb/leaderboard"&gt;MTEB Leader Board&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I tested it in OpenWebUI and it's the best I've ever tested and its fast AF.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/iamgroot42/rover_nexus"&gt;https://huggingface.co/iamgroot42/rover_nexus&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlgorithmicKing"&gt; /u/AlgorithmicKing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3ztzu/the_best_embedding_model_so_far_iamgroot42rover/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3ztzu/the_best_embedding_model_so_far_iamgroot42rover/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3ztzu/the_best_embedding_model_so_far_iamgroot42rover/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T04:40:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3z6cb</id>
    <title>Grokking at the Edge of Numerical Stability</title>
    <updated>2025-01-18T04:01:09+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2501.04697"&gt;https://arxiv.org/abs/2501.04697&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Grokking, the sudden generalization that occurs after prolonged overfitting, is a surprising phenomenon challenging our understanding of deep learning. Although significant progress has been made in understanding grokking, the reasons behind the delayed generalization and its dependence on regularization remain unclear. In this work, we argue that without regularization, grokking tasks push models to the edge of numerical stability, introducing floating point errors in the Softmax function, which we refer to as Softmax Collapse (SC). We demonstrate that SC prevents grokking and that mitigating SC enables grokking without regularization. Investigating the root cause of SC, we find that beyond the point of overfitting, the gradients strongly align with what we call the na√Øve loss minimization (NLM) direction. This component of the gradient does not alter the model's predictions but decreases the loss by scaling the logits, typically by scaling the weights along their current direction. We show that this scaling of the logits explains the delay in generalization characteristic of grokking and eventually leads to SC, halting further learning. To validate our hypotheses, we introduce two key contributions that address the challenges in grokking tasks: StableMax, a new activation function that prevents SC and enables grokking without regularization, and ‚ä•Grad, a training algorithm that promotes quick generalization in grokking tasks by preventing NLM altogether. These contributions provide new insights into grokking, elucidating its delayed generalization, reliance on regularization, and the effectiveness of existing grokking-inducing methods. Code for this paper is available at this https URL.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3z6cb/grokking_at_the_edge_of_numerical_stability/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3z6cb/grokking_at_the_edge_of_numerical_stability/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3z6cb/grokking_at_the_edge_of_numerical_stability/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T04:01:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i401lt</id>
    <title>Whisper turbo fine tuning guidance</title>
    <updated>2025-01-18T04:53:51+00:00</updated>
    <author>
      <name>/u/fgoricha</name>
      <uri>https://old.reddit.com/user/fgoricha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking to try fine tuning whisper large v3 turbo on runpod. I have a 3090 which I could use locally, but why not play with a cloud gpu so I can use my gpu for other stuff. Does anyone have any guides I can follow to help with the fine tuning process? I asked ChatGPT and it almost seems too easy. I already have my audio files in .wav format and their correctly transcribed text files. &lt;/p&gt; &lt;p&gt;Thanks for any help or advice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fgoricha"&gt; /u/fgoricha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i401lt/whisper_turbo_fine_tuning_guidance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i401lt/whisper_turbo_fine_tuning_guidance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i401lt/whisper_turbo_fine_tuning_guidance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T04:53:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3qzom</id>
    <title>5090 OpenCL &amp; Vulkan leaks</title>
    <updated>2025-01-17T21:17:14+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ack, not crushing 4090.&lt;br /&gt; &lt;a href="https://videocardz.com/newz/nvidia-geforce-rtx-5090-appears-in-first-geekbench-opencl-vulkan-leaks"&gt;https://videocardz.com/newz/nvidia-geforce-rtx-5090-appears-in-first-geekbench-opencl-vulkan-leaks&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3qzom/5090_opencl_vulkan_leaks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3qzom/5090_opencl_vulkan_leaks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3qzom/5090_opencl_vulkan_leaks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T21:17:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3xoyd</id>
    <title>What's the cheapest way to run Llama 3.x 8B class models with realtime-like (chatgpt speed) tokens per second?</title>
    <updated>2025-01-18T02:38:29+00:00</updated>
    <author>
      <name>/u/synexo</name>
      <uri>https://old.reddit.com/user/synexo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;fireworks.ai? spin up on runpod? build a home server?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/synexo"&gt; /u/synexo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3xoyd/whats_the_cheapest_way_to_run_llama_3x_8b_class/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3xoyd/whats_the_cheapest_way_to_run_llama_3x_8b_class/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3xoyd/whats_the_cheapest_way_to_run_llama_3x_8b_class/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T02:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3w7ao</id>
    <title>[2403.09919] Recurrent Drafter for Fast Speculative Decoding in Large Language Models</title>
    <updated>2025-01-18T01:20:34+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2403.09919"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3w7ao/240309919_recurrent_drafter_for_fast_speculative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3w7ao/240309919_recurrent_drafter_for_fast_speculative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T01:20:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3pup0</id>
    <title>Beating cuBLAS in SGEMM from Scratch</title>
    <updated>2025-01-17T20:26:10+00:00</updated>
    <author>
      <name>/u/salykova</name>
      <uri>https://old.reddit.com/user/salykova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pup0/beating_cublas_in_sgemm_from_scratch/"&gt; &lt;img alt="Beating cuBLAS in SGEMM from Scratch" src="https://a.thumbs.redditmedia.com/5VtAEp46vu6qAMyyshFSAQ0PS4VyO1ibLsIEkWU_HY0.jpg" title="Beating cuBLAS in SGEMM from Scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A while ago, I shared my article here about optimizing matrix multiplication on CPUs - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1dt3rqc/beating_numpys_matrix_multiplication_in_150_lines/"&gt;Beating NumPy's matrix multiplication in 150 lines of C code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I received positive feedback from you, and today I'm excited to share my second blog post. This one focuses on an SGEMM (Single-precision GEneral Matrix Multiply) that outperforms NVIDIA's implementation from cuBLAS library with its (modified?) CUTLASS kernel across a wide range of matrix sizes. This project primarily targets &lt;strong&gt;CUDA-learners&lt;/strong&gt; and aims to bridge the gap between the SGEMM implementations explained in books/blogs and those used in NVIDIA‚Äôs BLAS libraries. The blog delves into benchmarking code on CUDA devices and explains the algorithm's design along with optimization techniques. These include inlined PTX, asynchronous memory copies, double-buffering, avoiding shared memory bank conflicts, and efficient coalesced storage through shared memory.&lt;/p&gt; &lt;p&gt;The code is super easy to tweak, so you can customize it for your projects with kernel fusion or just drop it into your libraries as-is. Below, I've included performance comparisons against cuBLAS and Simon Boehm‚Äôs highly cited work, which is now integrated into llamafile aka tinyBLAS.&lt;/p&gt; &lt;p&gt;P.S. The next blog post will cover implementing HGEMM (FP16 GEMM) and HGEMV (FP16 Matrix-Vector Multiplication) on Tensor Cores achieving performance comparable to cuBLAS (or maybe even faster? let's see). If you enjoy educational content like this and would like to see more, please share the article. If you have any questions, feel free to comment or send me a direct message - I'd love to hear your feedback and answer any questions you may have!&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://salykova.github.io/sgemm-gpu"&gt;https://salykova.github.io/sgemm-gpu&lt;/a&gt;&lt;br /&gt; Code: &lt;a href="https://github.com/salykova/sgemm.cu"&gt;https://github.com/salykova/sgemm.cu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uq14ysfvamde1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d3998a50e61643c76e82ff048d1dd20703e3a65f"&gt;https://preview.redd.it/uq14ysfvamde1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d3998a50e61643c76e82ff048d1dd20703e3a65f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/salykova"&gt; /u/salykova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pup0/beating_cublas_in_sgemm_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pup0/beating_cublas_in_sgemm_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pup0/beating_cublas_in_sgemm_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T20:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3mybo</id>
    <title>LCLV: Real-time video analysis with Moondream 2B &amp; OLLama (open source, local). Anyone want a set up guide?</title>
    <updated>2025-01-17T18:21:33+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3mybo/lclv_realtime_video_analysis_with_moondream_2b/"&gt; &lt;img alt="LCLV: Real-time video analysis with Moondream 2B &amp;amp; OLLama (open source, local). Anyone want a set up guide?" src="https://external-preview.redd.it/MXZ5aHh4bWZpbGRlMSTqk2DOPEdgmnDyQ8guvDBrE8AyiWMeqDE4BRKGe_SG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ecd38999e371e083e545019f1eaf8d324a146b50" title="LCLV: Real-time video analysis with Moondream 2B &amp;amp; OLLama (open source, local). Anyone want a set up guide?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c3kcfymfilde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3mybo/lclv_realtime_video_analysis_with_moondream_2b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3mybo/lclv_realtime_video_analysis_with_moondream_2b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T18:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3nsbx</id>
    <title>Realtime speaker diarization</title>
    <updated>2025-01-17T18:57:16+00:00</updated>
    <author>
      <name>/u/Lonligrin</name>
      <uri>https://old.reddit.com/user/Lonligrin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3nsbx/realtime_speaker_diarization/"&gt; &lt;img alt="Realtime speaker diarization" src="https://external-preview.redd.it/JO_yTxc06ktYf5LFR-Rn-h9sKgRJ8XcsPo1m_3iqmLE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a91a5195fd0960c0d708c2f400bd55c115bba5a" title="Realtime speaker diarization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lonligrin"&gt; /u/Lonligrin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=-zpyi1KHOUk&amp;amp;si=qzksOIhsLjo9J8Zp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3nsbx/realtime_speaker_diarization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3nsbx/realtime_speaker_diarization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T18:57:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3o7a8</id>
    <title>I am open sourcing a smart text editor that runs completely in-browser using WebLLM + LLAMA (requires Chrome + WebGPU)</title>
    <updated>2025-01-17T19:14:56+00:00</updated>
    <author>
      <name>/u/yyjhao</name>
      <uri>https://old.reddit.com/user/yyjhao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3o7a8/i_am_open_sourcing_a_smart_text_editor_that_runs/"&gt; &lt;img alt="I am open sourcing a smart text editor that runs completely in-browser using WebLLM + LLAMA (requires Chrome + WebGPU)" src="https://external-preview.redd.it/MGt0ZzN4Y3NzbGRlMeSgvI1GdDqWZSs569grdhgwadhN-F5M6UL9TiNWoaqW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7c56ce3e01a5f41dcffc115930e49f7b1fee821" title="I am open sourcing a smart text editor that runs completely in-browser using WebLLM + LLAMA (requires Chrome + WebGPU)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yyjhao"&gt; /u/yyjhao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n3fmqwcsslde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3o7a8/i_am_open_sourcing_a_smart_text_editor_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3o7a8/i_am_open_sourcing_a_smart_text_editor_that_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T19:14:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3pexj</id>
    <title>DeepSeek-R1 (Preview) Benchmarked on LiveCodeBench</title>
    <updated>2025-01-17T20:06:47+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pexj/deepseekr1_preview_benchmarked_on_livecodebench/"&gt; &lt;img alt="DeepSeek-R1 (Preview) Benchmarked on LiveCodeBench" src="https://external-preview.redd.it/RiXxcULN7VvmAA8zRKm9Hg6sMZIuDEZ9SdZM3h7z4e0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c43191d847a8866681673c575cc88d8e702dd05" title="DeepSeek-R1 (Preview) Benchmarked on LiveCodeBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgur.com/a/WdpIkiy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pexj/deepseekr1_preview_benchmarked_on_livecodebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pexj/deepseekr1_preview_benchmarked_on_livecodebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T20:06:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i435so</id>
    <title>KoboldCpp 1.82 - Now supports OuteTTS v0.2+0.3 with speaker voice synthesis and XTTS/OpenAI speech API, TAESD for Flux &amp; SD3, multilingual whisper (plus RAG and WebSearch from v1.81)</title>
    <updated>2025-01-18T08:27:13+00:00</updated>
    <author>
      <name>/u/HadesThrowaway</name>
      <uri>https://old.reddit.com/user/HadesThrowaway</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey it's me Concedo, here again playing how-many-more-API-endpoints-can-koboldcpp-serve. &lt;/p&gt; &lt;p&gt;Today's release brings long awaited TTS support, which works on all versions of OuteTTS GGUFs including the newly released &lt;strong&gt;v0.3 500M and 1B&lt;/strong&gt; models. It also provides XTTS and OpenAI Speech compatible APIs, so it can work as a direct TTS drop-in for existing frontends that use those features. &lt;/p&gt; &lt;p&gt;There are also some pretty cool improvements, as well as many other features, so do check out the release notes if you haven't yet. Last release, we also added WebSearch and a simple browser based RAG, so check that out if you missed it. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/LostRuins/koboldcpp/releases"&gt;https://github.com/LostRuins/koboldcpp/releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HadesThrowaway"&gt; /u/HadesThrowaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i435so/koboldcpp_182_now_supports_outetts_v0203_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i435so/koboldcpp_182_now_supports_outetts_v0203_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i435so/koboldcpp_182_now_supports_outetts_v0203_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T08:27:13+00:00</published>
  </entry>
</feed>
