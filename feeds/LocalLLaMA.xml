<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-30T08:24:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jm9l6q</id>
    <title>New release of EQ-Bench creative writing leaderboard w/ new prompts, more headroom, &amp; cozy sample reader</title>
    <updated>2025-03-29T00:05:35+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm9l6q/new_release_of_eqbench_creative_writing/"&gt; &lt;img alt="New release of EQ-Bench creative writing leaderboard w/ new prompts, more headroom, &amp;amp; cozy sample reader" src="https://b.thumbs.redditmedia.com/1wR0A4z7POWQrz_UIslETNDq8PPe15red_LwAnuSiTU.jpg" title="New release of EQ-Bench creative writing leaderboard w/ new prompts, more headroom, &amp;amp; cozy sample reader" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Find the leaderboard here: &lt;a href="https://eqbench.com/creative_writing.html"&gt;https://eqbench.com/creative_writing.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A nice long writeup: &lt;a href="https://eqbench.com/about.html#creative-writing-v3"&gt;https://eqbench.com/about.html#creative-writing-v3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source code: &lt;a href="https://github.com/EQ-bench/creative-writing-bench"&gt;https://github.com/EQ-bench/creative-writing-bench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jm9l6q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm9l6q/new_release_of_eqbench_creative_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jm9l6q/new_release_of_eqbench_creative_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T00:05:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn1njb</id>
    <title>Which LLM's are the best and opensource for code generation.</title>
    <updated>2025-03-30T01:29:37+00:00</updated>
    <author>
      <name>/u/According_Fig_4784</name>
      <uri>https://old.reddit.com/user/According_Fig_4784</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am planning to build an Agent for code generation and with all the new models coming up I am confused with which model to use, I trying feasibility on Llama 3.3 70B , Qwen 2.5 Coder 32B, Mistral Chat, which was available for free use in their respective website and spaces. &lt;/p&gt; &lt;p&gt;What I found was that, as long as the Code remained simple with less complexities in the given prompt llama did better, but as we increased the complexity Mistral did better than other models mentioned. But grok gave very convincing answers with fewer rewrite, now how to go about building the system, which model to use?&lt;/p&gt; &lt;p&gt;It would be great if you could tell me a model with an API to use (like gradio).&lt;/p&gt; &lt;p&gt;Also I am planning to use an interpreter tool in the chain to interpret the code geneated and send it back if any issue found and planning to use Riza or bearly, any suggestions on this would be great. &lt;/p&gt; &lt;p&gt;TLDR; which code LLM to use with an open API access, if present, and which interpreter tool to use for python in langchain? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/According_Fig_4784"&gt; /u/According_Fig_4784 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn1njb/which_llms_are_the_best_and_opensource_for_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn1njb/which_llms_are_the_best_and_opensource_for_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn1njb/which_llms_are_the_best_and_opensource_for_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T01:29:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn5ngq</id>
    <title>RAG Observations</title>
    <updated>2025-03-30T05:28:21+00:00</updated>
    <author>
      <name>/u/v1sual3rr0r</name>
      <uri>https://old.reddit.com/user/v1sual3rr0r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been into computers for a long time. I started out programming in BASIC years ago, and while I‚Äôm not a developer AT ALL, I‚Äôve always enjoyed messing with tech. I have been exploring AI, especially local LLMs and I am interested how RAG systems can help. &lt;/p&gt; &lt;p&gt;Right now I‚Äôm trying to build (with AI &amp;quot;help&amp;quot;) a lightweight AI Help Desk that uses a small language model with a highly optimized RAG backend. The goal is to see how much performance I can get out of a low-resource setup by focusing on smart retrieval. I‚Äôm aiming to use components like &lt;strong&gt;e5-small-v2&lt;/strong&gt; for dense embeddings, &lt;strong&gt;BM25&lt;/strong&gt; for sparse keyword matching, and &lt;strong&gt;UPR&lt;/strong&gt; for unsupervised re-ranking to tighten up the results. This is taking a while. UGH!&lt;/p&gt; &lt;p&gt;While working on this project I‚Äôve also been converting raw data into semantically meaningful chunks optimized for retrieval in a RAG setup. So I wanted to see how this would perform in a test. So I tried a couple easy to use systems...&lt;/p&gt; &lt;p&gt;While testing platforms like AnythingLLM and LM Studio, even with larger models like Gemma 3 12B, I noticed a surprising amount of hallucination, even when feeding in a small, well-structured sample database. It raised some questions for me:&lt;/p&gt; &lt;p&gt;Are these tools doing shallow or naive retrieval that undermines the results&lt;/p&gt; &lt;p&gt;Is the model ignoring the retrieved context, or is the chunking strategy too weak?&lt;/p&gt; &lt;p&gt;With the right retrieval pipeline, could a smaller model actually perform more reliably?&lt;/p&gt; &lt;p&gt;What am I doing wrong?&lt;/p&gt; &lt;p&gt;I understand those platforms are meant to be user-friendly and generalized, but I‚Äôm aiming for something a bit more deliberate and fine-tuned. Just curious if others have run into similar issues or have insights into where things tend to fall apart in these implementations.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/v1sual3rr0r"&gt; /u/v1sual3rr0r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5ngq/rag_observations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5ngq/rag_observations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5ngq/rag_observations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T05:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm4agx</id>
    <title>Qwen-2.5-72b is now the best open source OCR model</title>
    <updated>2025-03-28T20:07:08+00:00</updated>
    <author>
      <name>/u/Tylernator</name>
      <uri>https://old.reddit.com/user/Tylernator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This has been a big week for open source LLMs. In the last few days we got:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen 2.5 VL (72b and 32b)&lt;/li&gt; &lt;li&gt;Gemma-3 (27b)&lt;/li&gt; &lt;li&gt;DeepSeek-v3-0324&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And a couple weeks ago we got the new mistral-ocr model. We updated our OCR benchmark to include the new models.&lt;/p&gt; &lt;p&gt;We evaluated 1,000 documents for JSON extraction accuracy. Major takeaways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen 2.5 VL (72b and 32b) are by far the most impressive. Both landed right around 75% accuracy (equivalent to GPT-4o‚Äôs performance). Qwen 72b was only 0.4% above 32b. Within the margin of error.&lt;/li&gt; &lt;li&gt;Both Qwen models passed mistral-ocr (72.2%), which is specifically trained for OCR.&lt;/li&gt; &lt;li&gt;Gemma-3 (27B) only scored 42.9%. Particularly surprising given that it's architecture is based on Gemini 2.0 which still tops the accuracy chart.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The data set and benchmark runner is fully open source. You can check out the code and reproduction steps here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/getomni-ai/benchmark"&gt;https://github.com/getomni-ai/benchmark&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/getomni-ai/ocr-benchmark"&gt;https://huggingface.co/datasets/getomni-ai/ocr-benchmark&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tylernator"&gt; /u/Tylernator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://getomni.ai/blog/benchmarking-open-source-models-for-ocr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm4agx/qwen2572b_is_now_the_best_open_source_ocr_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jm4agx/qwen2572b_is_now_the_best_open_source_ocr_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T20:07:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn6w95</id>
    <title>CUDA GPUs vs Price Tradeoff (Local CSM/Sesame on RX GPU)</title>
    <updated>2025-03-30T07:00:17+00:00</updated>
    <author>
      <name>/u/elchurnerista</name>
      <uri>https://old.reddit.com/user/elchurnerista</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it possible to run a LLama 1B locally alongside another &lt;a href="https://huggingface.co/sesame/csm-1b"&gt;model&lt;/a&gt; that explicitly mentions the need to have CUDA-compatible hardware (CUDA 12.4 or 12.6) on a RX GPU with a CUDA adapter (ZLUDA or another variety) with 16-20GB VRAM and get similar native-CUDA performance?&lt;/p&gt; &lt;p&gt;Now, is the potential better performance by running in a NVidia GPU worth ~800$? I'm not technically in a budget, but I'd prefer not to burn all my cash given the GPU market.&lt;/p&gt; &lt;p&gt;I'm trying to get ~20 T/s on 1B LLama, &lt;strong&gt;&lt;em&gt;at least&lt;/em&gt;&lt;/strong&gt;. Running it on the cloud it's not an option.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elchurnerista"&gt; /u/elchurnerista &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn6w95/cuda_gpus_vs_price_tradeoff_local_csmsesame_on_rx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn6w95/cuda_gpus_vs_price_tradeoff_local_csmsesame_on_rx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn6w95/cuda_gpus_vs_price_tradeoff_local_csmsesame_on_rx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T07:00:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn3miw</id>
    <title>finetune llm to make comfyui workflow</title>
    <updated>2025-03-30T03:20:34+00:00</updated>
    <author>
      <name>/u/kigy_x</name>
      <uri>https://old.reddit.com/user/kigy_x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm new to the field of LLM training. I'm thinking of finetuning a small, open-source model as an initial step towards creating and editing images through prompt only, where it will be trained on ComfyUI JSON text files. What are good, lightweight, open-source models suitable for this task? I believe there are many datasets available, but if there are any additional tips, I'd be happy to discuss them&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kigy_x"&gt; /u/kigy_x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn3miw/finetune_llm_to_make_comfyui_workflow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn3miw/finetune_llm_to_make_comfyui_workflow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn3miw/finetune_llm_to_make_comfyui_workflow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T03:20:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn7kmq</id>
    <title>Exo and Thunderbolt 5 link aggregation on Mac studio ?</title>
    <updated>2025-03-30T07:51:48+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Suppose I have two Mac studio each having four Thunderbolt 5 ports. I was wondering if it‚Äôs possible to cluster them together. Say by connecting them using two or more thunderbolt 5 ports to increase bandwidth for Exo. Is it possible now or ever?. I think the hardware allows it but I don‚Äôt know about Mac OS or Exo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn7kmq/exo_and_thunderbolt_5_link_aggregation_on_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn7kmq/exo_and_thunderbolt_5_link_aggregation_on_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn7kmq/exo_and_thunderbolt_5_link_aggregation_on_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T07:51:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmvwnx</id>
    <title>Video with some of the tasks in ARC-AGI-2, contains spoilers</title>
    <updated>2025-03-29T20:50:12+00:00</updated>
    <author>
      <name>/u/neoneye2</name>
      <uri>https://old.reddit.com/user/neoneye2</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neoneye2"&gt; /u/neoneye2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=3ki7oWI18I4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmvwnx/video_with_some_of_the_tasks_in_arcagi2_contains/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmvwnx/video_with_some_of_the_tasks_in_arcagi2_contains/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T20:50:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmlopu</id>
    <title>Why is Falcon3-7b so rarely used (or cited) as a model?</title>
    <updated>2025-03-29T12:56:38+00:00</updated>
    <author>
      <name>/u/Prudence-0</name>
      <uri>https://old.reddit.com/user/Prudence-0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a model that adheres well to prompting, its knowledge and responses are relevant, and it supports system/user/assistant prompts very well.&lt;/p&gt; &lt;p&gt;As a &amp;quot;small&amp;quot; model, I use it professionally in conjunction with the RAG system for chat.&lt;/p&gt; &lt;p&gt;I'd like your opinion on this model as well as the alternatives you use (&amp;lt;8b), Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prudence-0"&gt; /u/Prudence-0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmlopu/why_is_falcon37b_so_rarely_used_or_cited_as_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmlopu/why_is_falcon37b_so_rarely_used_or_cited_as_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmlopu/why_is_falcon37b_so_rarely_used_or_cited_as_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T12:56:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmruim</id>
    <title>[Build] A Beautiful Contradiction</title>
    <updated>2025-03-29T17:46:01+00:00</updated>
    <author>
      <name>/u/taylorwilsdon</name>
      <uri>https://old.reddit.com/user/taylorwilsdon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmruim/build_a_beautiful_contradiction/"&gt; &lt;img alt="[Build] A Beautiful Contradiction" src="https://external-preview.redd.it/cjdycmFoa3Mxb3JlMXAQIb2sMeAVglK21tTmFYitWWLXfsVRBH8Hkw8Jz_5k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e9b78cea2f802b090ea4497620a669bf9afa261" title="[Build] A Beautiful Contradiction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing my absolute contradiction of a local LLM rig - I found a 2019 Mac Pro outer shell for sale on eBay for $250 and wanted room to upsize my ITX build so I said fuck it and thus, a monstrosity was born. &lt;/p&gt; &lt;p&gt;Specs in the comments, hate welcomed üôè &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taylorwilsdon"&gt; /u/taylorwilsdon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/irhtg3os1ore1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmruim/build_a_beautiful_contradiction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmruim/build_a_beautiful_contradiction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T17:46:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn766v</id>
    <title>Agentic coding with LLMs</title>
    <updated>2025-03-30T07:21:05+00:00</updated>
    <author>
      <name>/u/mobileappz</name>
      <uri>https://old.reddit.com/user/mobileappz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone successfully using agents to create code with local LLMs? Where the files are written for you with tooling. Rather than just copy and pasting the code in to files you have created yourself.&lt;/p&gt; &lt;p&gt;If so which models and parameter count / quantization and IDE are you using? Does it produce effective code?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mobileappz"&gt; /u/mobileappz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn766v/agentic_coding_with_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn766v/agentic_coding_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn766v/agentic_coding_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T07:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmqlii</id>
    <title>I Made a simple online tokenizer for any Hugging Face model</title>
    <updated>2025-03-29T16:50:57+00:00</updated>
    <author>
      <name>/u/Tweed_Beetle</name>
      <uri>https://old.reddit.com/user/Tweed_Beetle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;When I'm experimenting with different open models from Hugging Face, I often want to know how many tokens my prompts or texts actually are &lt;em&gt;for that specific model's tokenizer&lt;/em&gt;. It felt clunky to do this locally every time, and online tools seemed non-existent apart from OpenAI's tokenizer.&lt;/p&gt; &lt;p&gt;So I built a little web tool to help with this: &lt;strong&gt;Tokiwi&lt;/strong&gt; -&amp;gt; &lt;a href="https://tokiwi.dev"&gt;https://tokiwi.dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You just paste text and give it any HF repo ID (like &lt;code&gt;google/gemma-3-27b-it&lt;/code&gt;, &lt;code&gt;deepseek-ai/DeepSeek-V3-0324&lt;/code&gt;, your own fine-tune if it's public, etc.) and it shows the token count and the tokens themselves. It can also handle gated models if you give it an HF access token.&lt;/p&gt; &lt;p&gt;Wondering if this might be useful to others here. Let me know what you think! Any feedback is appreciated.&lt;/p&gt; &lt;p&gt;Thank you for your time!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tweed_Beetle"&gt; /u/Tweed_Beetle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqlii/i_made_a_simple_online_tokenizer_for_any_hugging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqlii/i_made_a_simple_online_tokenizer_for_any_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqlii/i_made_a_simple_online_tokenizer_for_any_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T16:50:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn10lx</id>
    <title>Gemini 2.5 Pro unusable for coding?</title>
    <updated>2025-03-30T00:56:48+00:00</updated>
    <author>
      <name>/u/hyperknot</name>
      <uri>https://old.reddit.com/user/hyperknot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Something really strange is going on with Gemini 2.5 Pro.&lt;/p&gt; &lt;p&gt;On one hand, it's supposedly the smartest coding model ever made. But on the other hand, I ask it to add one single parameter, and instead of a simple 2-line diff, it generates a 35-line one where it randomly changes logic, removes a time.sleep() from an API call pagination loop, and is generally just totally &amp;quot;drunk&amp;quot; about what I asked it to do. It's somehow both pedantic and drunk at the same time.&lt;/p&gt; &lt;p&gt;Every other model, even much smaller ones, can easily make the 2-line change and leave everything else alone.&lt;/p&gt; &lt;p&gt;I'm wondering how this thing beat the Aider leaderboard. Did something change since the launch?&lt;/p&gt; &lt;p&gt;Setting temp to 0.0 doesn't help either.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hyperknot"&gt; /u/hyperknot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn10lx/gemini_25_pro_unusable_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn10lx/gemini_25_pro_unusable_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn10lx/gemini_25_pro_unusable_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T00:56:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmvwi5</id>
    <title>GMKtec announces imminent availability of Strix Halo EVO-X2 mini PC</title>
    <updated>2025-03-29T20:50:00+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.notebookcheck.net/GMKtec-announces-imminent-availability-of-Strix-Halo-EVO-X2-mini-PC.989734.0.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmvwi5/gmktec_announces_imminent_availability_of_strix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmvwi5/gmktec_announces_imminent_availability_of_strix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T20:50:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jml2w8</id>
    <title>Nemotron-49B uses 70% less KV cache compare to source Llama-70B</title>
    <updated>2025-03-29T12:21:40+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While studying how much KV cache major models uses using formula and empirically running it with llama.cpp if possible, I found that the Nemotron models are not only 30% smaller in model size, KV cache is also 70% less. Overall, it is 38% VRAM saving if you run at 128k context.&lt;/p&gt; &lt;p&gt;This is because the non-self attention layers doesn't have any KV cache at all. For Nemotron-49B, 31 out of 80 layers are non-self attention. For 51B, 26 out of 80 layers.&lt;/p&gt; &lt;p&gt;So if you are into 128k context and have 48GB VRAM, Nemotron can run at Q5_K_M at 128k with unquantized KV cache. On the other hand, QwQ can only run at IQ3_M due to 32GB KV cache.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jl33br/qwq32b_has_the_highest_kv_cachemodel_size_ratio/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jl33br/qwq32b_has_the_highest_kv_cachemodel_size_ratio/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Other things I learned:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;gemma-3 is pretty bad at KV cache while running with llama.cpp but this is because llama.cpp doesn't implement interleaved sliding window attention that can reduce KV cache to one sixth. (probably HF's transformers is the only one that support iSWA?)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Deepseek should make smaller MLA models that fit in 24GB or 48GB VRAM. This will blow the competition out of the water for local long context use.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jml2w8/nemotron49b_uses_70_less_kv_cache_compare_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jml2w8/nemotron49b_uses_70_less_kv_cache_compare_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jml2w8/nemotron49b_uses_70_less_kv_cache_compare_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T12:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmxdgg</id>
    <title>SplitQuantV2: Enhancing Low-Bit Quantization of LLMs Without GPUs</title>
    <updated>2025-03-29T21:58:54+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.07657"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmxdgg/splitquantv2_enhancing_lowbit_quantization_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmxdgg/splitquantv2_enhancing_lowbit_quantization_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T21:58:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmpjeu</id>
    <title>SOTA 3d?</title>
    <updated>2025-03-29T16:03:22+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmpjeu/sota_3d/"&gt; &lt;img alt="SOTA 3d?" src="https://external-preview.redd.it/ErYaOL2J__P1a1nSZoN5VkFh-_pWwoLL-ogamC2v0BM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16974cbb9664a3f501eea6ca32995ed70308e190" title="SOTA 3d?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/VAST-AI/TripoSG"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmpjeu/sota_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmpjeu/sota_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T16:03:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmvsm3</id>
    <title>Local, GPU-Accelerated AI Characters with C#, ONNX &amp; Your LLM (Speech-to-Speech)</title>
    <updated>2025-03-29T20:44:56+00:00</updated>
    <author>
      <name>/u/fagenorn</name>
      <uri>https://old.reddit.com/user/fagenorn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing &lt;strong&gt;Persona Engine&lt;/strong&gt;, an open-source project I built for creating interactive AI characters. Think VTuber tech meets your local AI stack.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Voice Input:&lt;/strong&gt; Listens via mic (Whisper.net ASR).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Your LLM:&lt;/strong&gt; Connects to any &lt;strong&gt;OpenAI-compatible API&lt;/strong&gt; (perfect for Ollama, LM Studio, etc., via LiteLLM perhaps). Personality defined in personality.txt.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Voice Output:&lt;/strong&gt; Advanced TTS pipeline + optional &lt;strong&gt;Real-time Voice Cloning (RVC)&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live2D Avatar:&lt;/strong&gt; Animates your character.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Spout Output:&lt;/strong&gt; Direct feed to OBS/streaming software.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Tech Deep Dive:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Everything Runs Locally:&lt;/strong&gt; The ASR, TTS, RVC, and rendering are all done on your machine. Point it at your local LLM, and the whole loop stays offline.&lt;/li&gt; &lt;li&gt;C# &lt;strong&gt;Powered:&lt;/strong&gt; The entire engine is built in &lt;strong&gt;C# on .NET 9&lt;/strong&gt;. This involved rewriting a lot of common Python AI tooling/pipelines, but gives us great performance and lovely async/await patterns for managing all the concurrent tasks (listening, thinking, speaking, rendering).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ONNX Runtime Under the Hood:&lt;/strong&gt; I leverage ONNX for the AI models (Whisper, TTS components, RVC). &lt;strong&gt;Theoretically,&lt;/strong&gt; this means it could target different execution providers (DirectML for AMD/Intel, CoreML, CPU). &lt;strong&gt;However,&lt;/strong&gt; the current build and included dependencies are optimized and primarily tested for &lt;strong&gt;NVIDIA CUDA/cuDNN&lt;/strong&gt; for maximum performance, especially with RVC. Getting other backends working would require compiling/sourcing the appropriate ONNX Runtime builds and potentially some code adjustments.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross-Platform Potential:&lt;/strong&gt; Being C#/.NET means it could run on Linux/macOS, but you'd need to handle platform-specific native dependencies (like PortAudio, Spout alternatives e.g., Syphon) and compile things yourself. Windows is the main supported platform right now via the releases.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo (Code &amp;amp; Releases):&lt;/strong&gt; &lt;a href="https://github.com/fagenorn/handcrafted-persona-engine"&gt;https://github.com/fagenorn/handcrafted-persona-engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Short Demo Video:&lt;/strong&gt; &lt;a href="https://www.youtube.com/watch?v=4V2DgI7OtHE"&gt;https://www.youtube.com/watch?v=4V2DgI7OtHE&lt;/a&gt; (forgive the cheesiness, I was having a bit of fun with capcut)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick Heads-up:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For the pre-built releases: &lt;strong&gt;Requires NVIDIA GPU + correctly installed CUDA/cuDNN&lt;/strong&gt; for good performance. The README has a detailed guide for this.&lt;/li&gt; &lt;li&gt;Configure appsettings.json with your LLM endpoint/model.&lt;/li&gt; &lt;li&gt;Using standard LLMs? Grab personality_example.txt from the repo root as a starting point for personality.txt (requires prompt tuning!).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Excited to share this with a community that appreciates running things locally and diving into the tech! Let me know what you think or if you give it a spin. üòä&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fagenorn"&gt; /u/fagenorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmvsm3/local_gpuaccelerated_ai_characters_with_c_onnx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmvsm3/local_gpuaccelerated_ai_characters_with_c_onnx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmvsm3/local_gpuaccelerated_ai_characters_with_c_onnx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T20:44:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmqqxz</id>
    <title>First time testing: Qwen2.5:72b -&gt; Ollama Mac + open-webUI -&gt; M3 Ultra 512 gb</title>
    <updated>2025-03-29T16:57:54+00:00</updated>
    <author>
      <name>/u/Turbulent_Pin7635</name>
      <uri>https://old.reddit.com/user/Turbulent_Pin7635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqqxz/first_time_testing_qwen2572b_ollama_mac_openwebui/"&gt; &lt;img alt="First time testing: Qwen2.5:72b -&amp;gt; Ollama Mac + open-webUI -&amp;gt; M3 Ultra 512 gb" src="https://b.thumbs.redditmedia.com/GHJGnHixtYfi5hcwQIzYQveJXry9-u0b_5OgRRmDegc.jpg" title="First time testing: Qwen2.5:72b -&amp;gt; Ollama Mac + open-webUI -&amp;gt; M3 Ultra 512 gb" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First time using it. Tested with the qwen2.5:72b, I add in the gallery the results of the first run. I would appreciate any comment that could help me to improve it. I also, want to thanks the community for the patience answering some doubts I had before buying this machine. I'm just beginning. &lt;/p&gt; &lt;p&gt;Doggo is just a plus!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent_Pin7635"&gt; /u/Turbulent_Pin7635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jmqqxz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqqxz/first_time_testing_qwen2572b_ollama_mac_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqqxz/first_time_testing_qwen2572b_ollama_mac_openwebui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T16:57:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmjq5h</id>
    <title>Finally someone's making a GPU with expandable memory!</title>
    <updated>2025-03-29T10:54:13+00:00</updated>
    <author>
      <name>/u/Normal-Ad-7114</name>
      <uri>https://old.reddit.com/user/Normal-Ad-7114</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a RISC-V gpu with SO-DIMM slots, so don't get your hopes up just yet, but it's &lt;em&gt;something&lt;/em&gt;!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.servethehome.com/bolt-graphics-zeus-the-new-gpu-architecture-with-up-to-2-25tb-of-memory-and-800gbe/2/"&gt;https://www.servethehome.com/bolt-graphics-zeus-the-new-gpu-architecture-with-up-to-2-25tb-of-memory-and-800gbe/2/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://bolt.graphics/"&gt;https://bolt.graphics/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Normal-Ad-7114"&gt; /u/Normal-Ad-7114 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmjq5h/finally_someones_making_a_gpu_with_expandable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmjq5h/finally_someones_making_a_gpu_with_expandable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmjq5h/finally_someones_making_a_gpu_with_expandable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T10:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmttah</id>
    <title>Seen a lot of setups but I had to laugh at this one. Price isn't terrible but with how it looks to be maintained I'd be worried about springing a leak.</title>
    <updated>2025-03-29T19:13:59+00:00</updated>
    <author>
      <name>/u/sleepy_roger</name>
      <uri>https://old.reddit.com/user/sleepy_roger</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sleepy_roger"&gt; /u/sleepy_roger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rvhj7wnchore1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmttah/seen_a_lot_of_setups_but_i_had_to_laugh_at_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmttah/seen_a_lot_of_setups_but_i_had_to_laugh_at_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T19:13:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmx0ih</id>
    <title>Someone created a highly optimized RDNA3 kernel that outperforms RocBlas by 60% on 7900XTX. How can I implement this and would it significantly benefit LLM inference?</title>
    <updated>2025-03-29T21:41:45+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmx0ih/someone_created_a_highly_optimized_rdna3_kernel/"&gt; &lt;img alt="Someone created a highly optimized RDNA3 kernel that outperforms RocBlas by 60% on 7900XTX. How can I implement this and would it significantly benefit LLM inference?" src="https://external-preview.redd.it/1DvBQgPBbFWMlcok52huGfBv7vgJ1oQojIIBOC8IpDA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d50ef4e8c8eec3ec397e0751d55c871986cab02e" title="Someone created a highly optimized RDNA3 kernel that outperforms RocBlas by 60% on 7900XTX. How can I implement this and would it significantly benefit LLM inference?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://seb-v.github.io/optimization/update/2025/01/20/Fast-GPU-Matrix-multiplication.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmx0ih/someone_created_a_highly_optimized_rdna3_kernel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmx0ih/someone_created_a_highly_optimized_rdna3_kernel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T21:41:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmyvpd</id>
    <title>Moondream 2025-03-27 Release</title>
    <updated>2025-03-29T23:10:50+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmyvpd/moondream_20250327_release/"&gt; &lt;img alt="Moondream 2025-03-27 Release" src="https://external-preview.redd.it/GtrXq5esaL1vBtb6j5XRN12_1xTaHr3DjPq8-x_uFDM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=149650fb9eeb4a3684aaac7092e35ed112f39db9" title="Moondream 2025-03-27 Release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://moondream.ai/blog/moondream-2025-03-27-release"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmyvpd/moondream_20250327_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmyvpd/moondream_20250327_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T23:10:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmtkgo</id>
    <title>4x3090</title>
    <updated>2025-03-29T19:02:48+00:00</updated>
    <author>
      <name>/u/zetan2600</name>
      <uri>https://old.reddit.com/user/zetan2600</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmtkgo/4x3090/"&gt; &lt;img alt="4x3090" src="https://preview.redd.it/zi8ghi2ifore1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1eaa2ef7723a30f4134fa44b42f76a17aa5ba357" title="4x3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is the only benefit of multiple GPUs concurrency of requests? I have 4x3090 but still seem limited to small models because it needs to fit in 24G vram. &lt;/p&gt; &lt;p&gt;AMD threadripper pro 5965wx 128 PCIe lanes ASUS ws pro wrx80 256G ddr4 3200 8 channels Primary PSU Corsair i1600 watt Secondary PSU 750watt 4 gigabyte 3090 turbos Phanteks Enthoo Pro II case Noctua industrial fans Artic cpu cooler&lt;/p&gt; &lt;p&gt;I am using vllm with tensor parallism of 4. I see all 4 cards loaded up and utilized evenly but doesn't seem any faster than 2 GPUs. &lt;/p&gt; &lt;p&gt;Currently using Qwen/Qwen2.5-14B-Instruct-AWQ with good success paired with Cline. &lt;/p&gt; &lt;p&gt;Will a nvlink bridge help? How can I run larger models? &lt;/p&gt; &lt;p&gt;14b seems really dumb compared to Anthropic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zetan2600"&gt; /u/zetan2600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zi8ghi2ifore1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmtkgo/4x3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmtkgo/4x3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T19:02:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn5uto</id>
    <title>MacBook M4 Max isn't great for LLMs</title>
    <updated>2025-03-30T05:42:51+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had M1 Max and recently upgraded to M4 Max - inferance speed difference is huge improvement (~3x) but it's still much slower than 5 years old RTX 3090 you can get for 700$ USD. &lt;/p&gt; &lt;p&gt;While it's nice to be able to load large models, they're just not gonna be very usable on that machine. An example - pretty small 14b distilled Qwen 4bit quant runs pretty slow for coding (40tps, with diff frequently failing so needs to redo whole file), and quality is very low. 32 bit is pretty unusable via Roo Code and Cline because of low speed.&lt;/p&gt; &lt;p&gt;And this is the best a money can buy you as Apple laptop.&lt;/p&gt; &lt;p&gt;Those are very pricey machines and I don't see any mentions that they aren't practical for local AI. You likely better off getting 1-2 generations old Nvidia rig if really need it, or renting, or just paying for API, as quality/speed will be day and night without upfront cost. &lt;/p&gt; &lt;p&gt;If you're getting MBP - save yourselves thousands $ and just get minimal ram you need with a bit extra SSD, and use more specialized hardware for local AI. &lt;/p&gt; &lt;p&gt;It's an awesome machine, all I'm saying - it prob won't deliver if you have high AI expectations for it. &lt;/p&gt; &lt;p&gt;PS: to me, this is not about getting or not getting a MacBook. I've been getting them for 15 years now and think they are awesome. The top models might not be quite the AI beast you were hoping for dropping these kinda $$$$, this is all I'm saying. I've had M1 Max with 64GB for years, and after the initial euphoria of holy smokes I can run large stuff there - never did it again for the reasons mentioned above. M4 is much faster but does feel similar in that sense. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T05:42:51+00:00</published>
  </entry>
</feed>
