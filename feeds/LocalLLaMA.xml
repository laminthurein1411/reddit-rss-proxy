<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-16T23:48:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lcq4gt</id>
    <title>FuturixAI - Cost-Effective Online RFT with Plug-and-Play LoRA Judge</title>
    <updated>2025-06-16T11:15:54+00:00</updated>
    <author>
      <name>/u/Aquaaa3539</name>
      <uri>https://old.reddit.com/user/Aquaaa3539</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A tiny LoRA adapter and a simple JSON prompt turn a 7B LLM into a powerful reward model that beats much larger ones - saving massive compute. It even helps a 7B model outperform top 70B baselines on GSM-8K using online RLHF&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aquaaa3539"&gt; /u/Aquaaa3539 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.futurixai.com/publications"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcq4gt/futurixai_costeffective_online_rft_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcq4gt/futurixai_costeffective_online_rft_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T11:15:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld08xa</id>
    <title>Real Time Speech to Text</title>
    <updated>2025-06-16T18:13:14+00:00</updated>
    <author>
      <name>/u/ThomasSparrow0511</name>
      <uri>https://old.reddit.com/user/ThomasSparrow0511</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As an intern in a finance related company, I need to know about realtime speech to text solutions for our product. I don't have advance knowledge in STT. 1) Any resources to know more about real time STT 2) Best existing products for real time audio (like phone calls) to text for our MLOps pipeline &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasSparrow0511"&gt; /u/ThomasSparrow0511 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld08xa/real_time_speech_to_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld08xa/real_time_speech_to_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld08xa/real_time_speech_to_text/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T18:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lchamn</id>
    <title>Whatâ€™s your current tech stack</title>
    <updated>2025-06-16T02:08:55+00:00</updated>
    <author>
      <name>/u/hokies314</name>
      <uri>https://old.reddit.com/user/hokies314</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m using Ollama for local models (but Iâ€™ve been following the threads that talk about ditching it) and LiteLLM as a proxy layer so I can connect to OpenAI and Anthropic models too. I have a Postgres database for LiteLLM to use. All but Ollama is orchestrated through a docker compose and Portainer for docker management.&lt;/p&gt; &lt;p&gt;The I have OpenWebUI as the frontend and it connects to LiteLLM or Iâ€™m using Langgraph for my agents. &lt;/p&gt; &lt;p&gt;Iâ€™m kinda exploring my options and want to hear what everyone is using. (And I ditched Docker desktop for Rancher but Iâ€™m exploring other options there too)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hokies314"&gt; /u/hokies314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lchamn/whats_your_current_tech_stack/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lchamn/whats_your_current_tech_stack/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lchamn/whats_your_current_tech_stack/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T02:08:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lco9ik</id>
    <title>Recommendations for Local LLMs (Under 70B) with Cline/Roo Code</title>
    <updated>2025-06-16T09:20:49+00:00</updated>
    <author>
      <name>/u/AMOVCS</name>
      <uri>https://old.reddit.com/user/AMOVCS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd like to know what, if any, are some good local models under 70b that can handle tasks well when using Cline/Roo Code. Iâ€™ve tried a &lt;em&gt;lot&lt;/em&gt; to use Cline or Roo Code for various things, and most of the time it's simple tasks, but the agents often get stuck in loops or make things worse. It feels like the size of the instructions is too much for these smaller LLMs to handle well â€“ many times I see the task using 15k+ tokens just to edit a couple lines of code. Maybe Iâ€™m doing something very wrong, maybe it's a configuration issue with the agents? Anyway, I was hoping you guys could recommend some models (could also be configurations, advice, anything) that work well with Cline/Roo Code.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some information for context:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I always use at least Q5 or better (sometimes I use Q4_UD from Unsloth).&lt;/li&gt; &lt;li&gt;Most of the time I give 20k+ context window to the agents.&lt;/li&gt; &lt;li&gt;My projects are a reasonable size, between 2k and 10k lines, but I only open the files needed when asking the agents to code.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Models I've Tried:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Devistral - Bad in general; I was on high expectations for this one but it didnâ€™t work.&lt;/li&gt; &lt;li&gt;Magistral - Even worse.&lt;/li&gt; &lt;li&gt;Qwen 3 series (and R1 distilled versions) - Not that bad, but just works when the project is very, very small.&lt;/li&gt; &lt;li&gt;GLM4 - Very good at coding on its own, not so good when using it with agents.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;So, are there any recommendations for models to use with Cline/Roo Code that actually work well?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AMOVCS"&gt; /u/AMOVCS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lco9ik/recommendations_for_local_llms_under_70b_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lco9ik/recommendations_for_local_llms_under_70b_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lco9ik/recommendations_for_local_llms_under_70b_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T09:20:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lc6tii</id>
    <title>I wrapped Appleâ€™s new on-device models in an OpenAI-compatible API</title>
    <updated>2025-06-15T18:06:56+00:00</updated>
    <author>
      <name>/u/FixedPt</name>
      <uri>https://old.reddit.com/user/FixedPt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent the weekend vibe-coding in Cursor and ended up with a small Swift app that turns the new macOS 26 on-device Apple Intelligence models into a local server you can hit with standard OpenAI &lt;code&gt;/v1/chat/completions&lt;/code&gt; calls. Point any client you like at &lt;code&gt;http://127.0.0.1:11535&lt;/code&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nothing leaves your Mac&lt;/li&gt; &lt;li&gt;Works with any OpenAI-compatible client&lt;/li&gt; &lt;li&gt;Open source, MIT-licensed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repoâ€™s here â†’ &lt;a href="https://github.com/gety-ai/apple-on-device-openai"&gt;&lt;strong&gt;https://github.com/gety-ai/apple-on-device-openai&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It was a fun hackâ€”let me know if you try it out or run into any weirdness. Cheers! ðŸš€&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FixedPt"&gt; /u/FixedPt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc6tii/i_wrapped_apples_new_ondevice_models_in_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc6tii/i_wrapped_apples_new_ondevice_models_in_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lc6tii/i_wrapped_apples_new_ondevice_models_in_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T18:06:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcbs7z</id>
    <title>FULL LEAKED v0 System Prompts and Tools [UPDATED]</title>
    <updated>2025-06-15T21:37:55+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Latest system prompt: 15/06/2025)&lt;/p&gt; &lt;p&gt;I managed to get FULL updated v0 system prompt and internal tools info. Over 900 lines&lt;/p&gt; &lt;p&gt;You can it out at: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcbs7z/full_leaked_v0_system_prompts_and_tools_updated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcbs7z/full_leaked_v0_system_prompts_and_tools_updated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcbs7z/full_leaked_v0_system_prompts_and_tools_updated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T21:37:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld6x18</id>
    <title>What is DeepSeek-R1-0528's knowledge cutoff?</title>
    <updated>2025-06-16T22:35:33+00:00</updated>
    <author>
      <name>/u/sixft2</name>
      <uri>https://old.reddit.com/user/sixft2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's super hard to find online!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sixft2"&gt; /u/sixft2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld6x18/what_is_deepseekr10528s_knowledge_cutoff/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld6x18/what_is_deepseekr10528s_knowledge_cutoff/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld6x18/what_is_deepseekr10528s_knowledge_cutoff/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T22:35:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld3nb3</id>
    <title>Are there any local llm options for android that have image recognition?</title>
    <updated>2025-06-16T20:23:54+00:00</updated>
    <author>
      <name>/u/diggels</name>
      <uri>https://old.reddit.com/user/diggels</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Found a few localllm apps - but theyâ€™re just text only which is useless. &lt;/p&gt; &lt;p&gt;Iâ€™ve heard some people use termux and either ollama or kobold?&lt;/p&gt; &lt;p&gt;Do these options allow for image recognition&lt;/p&gt; &lt;p&gt;Is there a certain gguf type that does image recognition? &lt;/p&gt; &lt;p&gt;Would that work as an option ðŸ¤”&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/diggels"&gt; /u/diggels &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld3nb3/are_there_any_local_llm_options_for_android_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld3nb3/are_there_any_local_llm_options_for_android_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld3nb3/are_there_any_local_llm_options_for_android_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T20:23:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld8gs4</id>
    <title>Fine-tuning may be underestimated</title>
    <updated>2025-06-16T23:44:37+00:00</updated>
    <author>
      <name>/u/AgreeableCaptain1372</name>
      <uri>https://old.reddit.com/user/AgreeableCaptain1372</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I often see comments and posts online dismissing fine-tuning and saying that RAG is the way to go. While RAG is very powerful, what if i want to save both on tokens and compute? Fine tuning allows you to achieve the same results as RAG with smaller LLMs and fewer tokens. LORA wonâ€™t always be enough but you can get a model to memorize much of what a RAG knowledge base contains with a full fine tune. And the best part is you donâ€™t need a huge model, the model can suck at everything else as long as it excels at your very specialized task. Even if you struggle to make the model memorize enough from your knowledge base and still need RAG, you will still save on compute by being able to rely on a smaller-sized LLM.&lt;/p&gt; &lt;p&gt;Now I think a big reason for this dismissal is many people seem to equate fine tuning to LORA and don't consider full tuning. Granted, full fine tuning is more expensive in the short run but it pays off in the long run.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AgreeableCaptain1372"&gt; /u/AgreeableCaptain1372 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld8gs4/finetuning_may_be_underestimated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld8gs4/finetuning_may_be_underestimated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld8gs4/finetuning_may_be_underestimated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T23:44:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcziww</id>
    <title>Recommending Practical Experiments from Research Papers</title>
    <updated>2025-06-16T17:47:17+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcziww/recommending_practical_experiments_from_research/"&gt; &lt;img alt="Recommending Practical Experiments from Research Papers" src="https://preview.redd.it/y35s13wkrb7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86e22fa7387d9780a5686b29439d2c933cb0510a" title="Recommending Practical Experiments from Research Papers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lately, I've been using LLMs to rank new arXiv papers based on the context of my own work.&lt;/p&gt; &lt;p&gt;This has helped me find relevant results hours after they've been posted, regardless of the virality.&lt;/p&gt; &lt;p&gt;Historically, I've been finetuning VLMs with LoRA, so &lt;a href="https://hsi-che-lin.github.io/EMLoC/"&gt;EMLoC&lt;/a&gt; recently came recommended.&lt;/p&gt; &lt;p&gt;Ultimately, I want to go beyond supporting my own intellectual curiosity to make suggestions rooted in my application context: constraints, hardware, prior experiments, and what has worked in the past.&lt;/p&gt; &lt;p&gt;I'm building toward a workflow where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Past experiment logs feed into paper recommendations&lt;/li&gt; &lt;li&gt;AI proposes lightweight trials using existing code, models, datasets&lt;/li&gt; &lt;li&gt;I can test methods fast and learn what transfers to my use case&lt;/li&gt; &lt;li&gt;Feed the results back into the loop&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Think of it as a &lt;strong&gt;knowledge flywheel&lt;/strong&gt; assisted with an experiment copilot to help you &lt;strong&gt;decide what to try next&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;How are you discovering your next great idea? &lt;/p&gt; &lt;p&gt;Looking to make research more reproducible and relevant, let's chat!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y35s13wkrb7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcziww/recommending_practical_experiments_from_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcziww/recommending_practical_experiments_from_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T17:47:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld4rei</id>
    <title>What's new in vLLM and llm-d</title>
    <updated>2025-06-16T21:07:09+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld4rei/whats_new_in_vllm_and_llmd/"&gt; &lt;img alt="What's new in vLLM and llm-d" src="https://external-preview.redd.it/_GTeYJTqgCY78BPqBcLVZkHTyQQTs_Fy5gkJz9OR8A0.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5802705364be59c611ba7a77034aecf7e02357b6" title="What's new in vLLM and llm-d" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hot off the press:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;In this session, we explored the latest updates in the vLLM v0.9.1 release, including the new Magistral model, FlexAttention support, multi-node serving optimization, and more.&lt;/p&gt; &lt;p&gt;We also did a deep dive into llm-d, the new Kubernetes-native high-performance distributed LLM inference framework co-designed with Inference Gateway (IGW). You'll learn what llm-d is, how it works, and see a live demo of it in action.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=pYujrc3rGjk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld4rei/whats_new_in_vllm_and_llmd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld4rei/whats_new_in_vllm_and_llmd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T21:07:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcya8p</id>
    <title>Local Image gen dead?</title>
    <updated>2025-06-16T17:01:52+00:00</updated>
    <author>
      <name>/u/maglat</name>
      <uri>https://old.reddit.com/user/maglat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it me or is the progress on local image generation entirely stagnated? No big release since ages. Latest Flux release is a paid cloud service.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maglat"&gt; /u/maglat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcya8p/local_image_gen_dead/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcya8p/local_image_gen_dead/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcya8p/local_image_gen_dead/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T17:01:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcksww</id>
    <title>Do AI wrapper startups have a real future?</title>
    <updated>2025-06-16T05:26:41+00:00</updated>
    <author>
      <name>/u/Samonji</name>
      <uri>https://old.reddit.com/user/Samonji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve been thinking about how many startups right now are essentially just wrappers around GPT or Claude, where they take the base model, add a nice UI or some prompt chains, and maybe tailor it to a niche, all while calling it a product.&lt;/p&gt; &lt;p&gt;Some of them are even making money, but I keep wonderingâ€¦ how long can that really last?&lt;/p&gt; &lt;p&gt;Like, once OpenAI or whoever bakes those same features into their platform, whatâ€™s stopping these wrapper apps from becoming irrelevant overnight? Can any of them actually build a moat?&lt;/p&gt; &lt;p&gt;Or is the only real path to focus super hard on a specific vertical (like legal or finance), gather your own data, and basically evolve beyond being just a wrapper?&lt;/p&gt; &lt;p&gt;Curious what you all think. Are these wrapper apps legit businesses, or just temporary hacks riding the hype wave?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Samonji"&gt; /u/Samonji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcksww/do_ai_wrapper_startups_have_a_real_future/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcksww/do_ai_wrapper_startups_have_a_real_future/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcksww/do_ai_wrapper_startups_have_a_real_future/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T05:26:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld3ivo</id>
    <title>Mixed Ram+Vram strategies for large MoE models - is it viable on consumer hardware?</title>
    <updated>2025-06-16T20:19:11+00:00</updated>
    <author>
      <name>/u/LagOps91</name>
      <uri>https://old.reddit.com/user/LagOps91</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am currently running a system with 24gb vram and 32gb ram and am thinking of getting an upgrade to 128gb (and later possibly 256 gb) ram to enable inference for large MoE models, such as dots.llm, Qwen 3 and possibly V3 if i was to go to 256gb ram.&lt;/p&gt; &lt;p&gt;The question is, what can you actually expect on such a system? I would have 2-channel ddr5 6400MT/s rams (either 2x or 4x 64gb) and a PCIe 4.0 Ã—16 connection to my gpu.&lt;/p&gt; &lt;p&gt;I have heard that using the gpu to hold the kv cache and having enough space to hold the active weights can help speed up inference for MoE models signifficantly, even if most of the weights are held in ram.&lt;/p&gt; &lt;p&gt;Before making any purchase however, I would want to get a rough idea about the t/s for prompt processing and inference i can expect for those different models at 32k context.&lt;/p&gt; &lt;p&gt;In addition, I am not sure how to set up the offloading strategy to make the most out of my gpu in this scenario. As I understand it, I'm not just offloading layers and do something else instead?&lt;/p&gt; &lt;p&gt;It would be a huge help if someone with a roughly comparable system could provide benchmark numbers and/or I could get some helpful explaination about how such a setup works. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LagOps91"&gt; /u/LagOps91 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld3ivo/mixed_ramvram_strategies_for_large_moe_models_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld3ivo/mixed_ramvram_strategies_for_large_moe_models_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld3ivo/mixed_ramvram_strategies_for_large_moe_models_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T20:19:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld4okl</id>
    <title>How are you using your local LLM to code and why?</title>
    <updated>2025-06-16T21:04:05+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;chat (cut &amp;amp; paste)&lt;/p&gt; &lt;p&gt;editor plugin- copilot, vscode, zed, &lt;a href="http://continue.dev"&gt;continue.dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;cli - aider&lt;/p&gt; &lt;p&gt;agentic editor - roo/cline/windsurf&lt;/p&gt; &lt;p&gt;agent - something like claude code&lt;/p&gt; &lt;p&gt;I still prefer chat cut &amp;amp; paste. I can control the input, prompt and get faster response and I can steer towards my idea faster. It does require a lot of work, but I make it up in speed vs the other means.&lt;/p&gt; &lt;p&gt;I use to use aider, and thinking of going back to it, but the best model then was qwen2.5-coder, with much improved models, it seems it's worth getting back in. &lt;/p&gt; &lt;p&gt;How are you coding and why are you using your approach?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld4okl/how_are_you_using_your_local_llm_to_code_and_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld4okl/how_are_you_using_your_local_llm_to_code_and_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld4okl/how_are_you_using_your_local_llm_to_code_and_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T21:04:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcxcuv</id>
    <title>Which vectorDB do you use? and why?</title>
    <updated>2025-06-16T16:26:48+00:00</updated>
    <author>
      <name>/u/Expert-Address-2918</name>
      <uri>https://old.reddit.com/user/Expert-Address-2918</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hate pinecone, why do you hate it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expert-Address-2918"&gt; /u/Expert-Address-2918 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcxcuv/which_vectordb_do_you_use_and_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcxcuv/which_vectordb_do_you_use_and_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcxcuv/which_vectordb_do_you_use_and_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T16:26:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcy6fc</id>
    <title>DeepSeek R1 0528 Ties Claude Opus 4 for #1 in WebDev Arena â€” [Ranks #6 Overall, #2 in Coding, #4 in Hard Prompts, &amp; #5 in Math]</title>
    <updated>2025-06-16T16:58:09+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcy6fc/deepseek_r1_0528_ties_claude_opus_4_for_1_in/"&gt; &lt;img alt="DeepSeek R1 0528 Ties Claude Opus 4 for #1 in WebDev Arena â€” [Ranks #6 Overall, #2 in Coding, #4 in Hard Prompts, &amp;amp; #5 in Math]" src="https://b.thumbs.redditmedia.com/w2fxA8vcEvjyTTb7SE1rMd45lIMFNP_po--PCvpiWNc.jpg" title="DeepSeek R1 0528 Ties Claude Opus 4 for #1 in WebDev Arena â€” [Ranks #6 Overall, #2 in Coding, #4 in Hard Prompts, &amp;amp; #5 in Math]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/pqp7qmk8kb7f1.png?width=1970&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7357f25c7c29c823ca444d21cd450535b0473912"&gt;https://preview.redd.it/pqp7qmk8kb7f1.png?width=1970&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7357f25c7c29c823ca444d21cd450535b0473912&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hser7yj9kb7f1.png?width=1970&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75e29998321e3d94009940d1fe181606352977cc"&gt;https://preview.redd.it/hser7yj9kb7f1.png?width=1970&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75e29998321e3d94009940d1fe181606352977cc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qjy2shnakb7f1.png?width=1970&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4febbdb1c60a985f5ab48b312a063fe2d5f45984"&gt;https://preview.redd.it/qjy2shnakb7f1.png?width=1970&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4febbdb1c60a985f5ab48b312a063fe2d5f45984&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/leaderboard"&gt;&lt;em&gt;https://lmarena.ai/leaderboard&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcy6fc/deepseek_r1_0528_ties_claude_opus_4_for_1_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcy6fc/deepseek_r1_0528_ties_claude_opus_4_for_1_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcy6fc/deepseek_r1_0528_ties_claude_opus_4_for_1_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T16:58:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcud8j</id>
    <title>Local Open Source VScode Copilot model with MCP</title>
    <updated>2025-06-16T14:32:16+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You don't need remote APIs for a coding copliot, or the MCP Course! Set up a fully local IDE with MCP integration using Continue. In this tutorial Continue guides you through setting it up.&lt;/p&gt; &lt;p&gt;This is what you need to do to take control of your copilot:&lt;br /&gt; &lt;strong&gt;-&lt;/strong&gt; Get the Continue extension from the &lt;a href="https://marketplace.visualstudio.com/items?itemName=Continue.continue"&gt;VS Code marketplace&lt;/a&gt; to serve as the AI coding assistant.&lt;br /&gt; - Serve the model with an OpenAI compatible server in Llama.cpp / LmStudio/ etc.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server -hf unsloth/Devstral-Small-2505-GGUF:Q4_K_M &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;-&lt;/strong&gt; Create a &lt;code&gt;.continue/models/llama-max.yaml&lt;/code&gt; file in your project to tell Continue how to use the local Ollama model.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;name: Llama.cpp model version: 0.0.1 schema: v1 models: - provider: llama.cpp model: unsloth/Devstral-Small-2505-GGUF apiBase: http://localhost:8080 defaultCompletionOptions: contextLength: 8192 # Adjust based on the model name: Llama.cpp Devstral-Small roles: - chat - edit &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;-&lt;/strong&gt; Create a &lt;code&gt;.continue/mcpServers/playwright-mcp.yaml&lt;/code&gt; file to integrate a tool, like the Playwright browser automation tool, with your assistant.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;name: Playwright mcpServer version: 0.0.1 schema: v1 mcpServers: - name: Browser search command: npx args: - &amp;quot;@playwright/mcp@latest&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Check out the full tutorial here: &lt;a href="https://huggingface.co/learn/mcp-course/unit2/continue-client"&gt;https://huggingface.co/learn/mcp-course/unit2/continue-client&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcud8j/local_open_source_vscode_copilot_model_with_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcud8j/local_open_source_vscode_copilot_model_with_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcud8j/local_open_source_vscode_copilot_model_with_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T14:32:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld11x4</id>
    <title>Humanity's last library, which locally ran LLM would be best?</title>
    <updated>2025-06-16T18:43:42+00:00</updated>
    <author>
      <name>/u/TheCuriousBread</name>
      <uri>https://old.reddit.com/user/TheCuriousBread</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An apocalypse has come upon us. The internet is no more. Libraries are no more. The only things left are local networks and people with the electricity to run them. &lt;/p&gt; &lt;p&gt;If you were to create humanity's last library, a distilled LLM with the entirety of human knowledge. What would be a good model for that? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheCuriousBread"&gt; /u/TheCuriousBread &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld11x4/humanitys_last_library_which_locally_ran_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld11x4/humanitys_last_library_which_locally_ran_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld11x4/humanitys_last_library_which_locally_ran_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T18:43:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcuglb</id>
    <title>MiniMax-M1 - a MiniMaxAI Collection</title>
    <updated>2025-06-16T14:35:55+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcuglb/minimaxm1_a_minimaxai_collection/"&gt; &lt;img alt="MiniMax-M1 - a MiniMaxAI Collection" src="https://external-preview.redd.it/KeaWNzZG0TAkUEwWyVGmsizl5dXuAOVMgFreGf02gFI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2286c64db955bf2850b44ae4b5c870213ee65afe" title="MiniMax-M1 - a MiniMaxAI Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/MiniMaxAI/minimax-m1-68502ad9634ec0eeac8cf094"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcuglb/minimaxm1_a_minimaxai_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcuglb/minimaxm1_a_minimaxai_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T14:35:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcrt1k</id>
    <title>Just finished recording 29 videos on "How to Build DeepSeek from Scratch"</title>
    <updated>2025-06-16T12:43:06+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Playlist link: &lt;a href="https://www.youtube.com/playlist?list=PLPTV0NXA_ZSiOpKKlHCyOq9lnp-dLvlms"&gt;https://www.youtube.com/playlist?list=PLPTV0NXA_ZSiOpKKlHCyOq9lnp-dLvlms&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are the 29 videos and their title:&lt;/p&gt; &lt;p&gt;(1) DeepSeek series introduction&lt;/p&gt; &lt;p&gt;(2) DeepSeek basics&lt;/p&gt; &lt;p&gt;(3) Journey of a token into the LLM architecture&lt;/p&gt; &lt;p&gt;(4) Attention mechanism explained in 1 hour&lt;/p&gt; &lt;p&gt;(5) Self Attention Mechanism - Handwritten from scratch&lt;/p&gt; &lt;p&gt;(6) Causal Attention Explained: Don't Peek into the Future&lt;/p&gt; &lt;p&gt;(7) Multi-Head Attention Visually Explained&lt;/p&gt; &lt;p&gt;(8) Multi-Head Attention Handwritten from Scratch&lt;/p&gt; &lt;p&gt;(9) Key Value Cache from Scratch&lt;/p&gt; &lt;p&gt;(10) Multi-Query Attention Explained&lt;/p&gt; &lt;p&gt;(11) Understand Grouped Query Attention (GQA)&lt;/p&gt; &lt;p&gt;(12) Multi-Head Latent Attention From Scratch&lt;/p&gt; &lt;p&gt;(13) Multi-Head Latent Attention Coded from Scratch in Python&lt;/p&gt; &lt;p&gt;(14) Integer and Binary Positional Encodings&lt;/p&gt; &lt;p&gt;(15) All about Sinusoidal Positional Encodings&lt;/p&gt; &lt;p&gt;(16) Rotary Positional Encodings&lt;/p&gt; &lt;p&gt;(17) How DeepSeek exactly implemented Latent Attention | MLA + RoPE&lt;/p&gt; &lt;p&gt;(18) Mixture of Experts (MoE) Introduction&lt;/p&gt; &lt;p&gt;(19) Mixture of Experts Hands on Demonstration&lt;/p&gt; &lt;p&gt;(20) Mixture of Experts Balancing Techniques&lt;/p&gt; &lt;p&gt;(21) How DeepSeek rewrote Mixture of Experts (MoE)?&lt;/p&gt; &lt;p&gt;(22) Code Mixture of Experts (MoE) from Scratch in Python&lt;/p&gt; &lt;p&gt;(23) Multi-Token Prediction Introduction&lt;/p&gt; &lt;p&gt;(24) How DeepSeek rewrote Multi-Token Prediction&lt;/p&gt; &lt;p&gt;(25) Multi-Token Prediction coded from scratch&lt;/p&gt; &lt;p&gt;(26) Introduction to LLM Quantization&lt;/p&gt; &lt;p&gt;(27) How DeepSeek rewrote Quantization Part 1&lt;/p&gt; &lt;p&gt;(28) How DeepSeek rewrote Quantization Part 2&lt;/p&gt; &lt;p&gt;(29) Build DeepSeek from Scratch 20 minute summary&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcrt1k/just_finished_recording_29_videos_on_how_to_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcrt1k/just_finished_recording_29_videos_on_how_to_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcrt1k/just_finished_recording_29_videos_on_how_to_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T12:43:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcn0vz</id>
    <title>Qwen releases official MLX quants for Qwen3 models in 4 quantization levels: 4bit, 6bit, 8bit, and BF16</title>
    <updated>2025-06-16T07:54:58+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcn0vz/qwen_releases_official_mlx_quants_for_qwen3/"&gt; &lt;img alt="Qwen releases official MLX quants for Qwen3 models in 4 quantization levels: 4bit, 6bit, 8bit, and BF16" src="https://preview.redd.it/5jpskt9dw87f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3979f7c8b5f11e8d9ae4fd59f4defeeebd8adae2" title="Qwen releases official MLX quants for Qwen3 models in 4 quantization levels: 4bit, 6bit, 8bit, and BF16" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ðŸš€ Excited to launch Qwen3 models in MLX format today!&lt;/p&gt; &lt;p&gt;Now available in 4 quantization levels: 4bit, 6bit, 8bit, and BF16 â€” Optimized for MLX framework. &lt;/p&gt; &lt;p&gt;ðŸ‘‰ Try it now!&lt;/p&gt; &lt;p&gt;X post: &lt;a href="https://x.com/alibaba_qwen/status/1934517774635991412?s=46"&gt;https://x.com/alibaba_qwen/status/1934517774635991412?s=46&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f"&gt;https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5jpskt9dw87f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcn0vz/qwen_releases_official_mlx_quants_for_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcn0vz/qwen_releases_official_mlx_quants_for_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T07:54:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld66t0</id>
    <title>Fortune 500s Are Burning Millions on LLM APIs. Why Not Build Their Own?</title>
    <updated>2025-06-16T22:04:50+00:00</updated>
    <author>
      <name>/u/Neat-Knowledge5642</name>
      <uri>https://old.reddit.com/user/Neat-Knowledge5642</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Youâ€™re at a Fortune 500 company, spending millions annually on LLM APIs (OpenAI, Google, etc). Yet youâ€™re limited by IP concerns, data control, and vendor constraints.&lt;/p&gt; &lt;p&gt;At what point does it make sense to build your own LLM in-house?&lt;/p&gt; &lt;p&gt;I work at a company behind one of the major LLMs, and the amount enterprises pay us is wild. Why arenâ€™t more of them building their own models? Is it talent? Infra complexity? Risk aversion?&lt;/p&gt; &lt;p&gt;Curious where this logic breaks.&lt;/p&gt; &lt;p&gt;Edit: What about an acquisition? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neat-Knowledge5642"&gt; /u/Neat-Knowledge5642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld66t0/fortune_500s_are_burning_millions_on_llm_apis_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld66t0/fortune_500s_are_burning_millions_on_llm_apis_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld66t0/fortune_500s_are_burning_millions_on_llm_apis_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T22:04:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcw50r</id>
    <title>Kimi-Dev-72B</title>
    <updated>2025-06-16T15:40:31+00:00</updated>
    <author>
      <name>/u/realJoeTrump</name>
      <uri>https://old.reddit.com/user/realJoeTrump</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcw50r/kimidev72b/"&gt; &lt;img alt="Kimi-Dev-72B" src="https://external-preview.redd.it/1kvJDTWOvntivVoW834gDLI4V0P6WaqmrGfz5xyEWNU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b09e977edec166ad9c212551ee72f79018be5fa2" title="Kimi-Dev-72B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realJoeTrump"&gt; /u/realJoeTrump &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Dev-72B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcw50r/kimidev72b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcw50r/kimidev72b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T15:40:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld116d</id>
    <title>MiniMax latest open-sourcing LLM, MiniMax-M1 â€” setting new standards in long-context reasoning,m</title>
    <updated>2025-06-16T18:42:52+00:00</updated>
    <author>
      <name>/u/srtng</name>
      <uri>https://old.reddit.com/user/srtng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld116d/minimax_latest_opensourcing_llm_minimaxm1_setting/"&gt; &lt;img alt="MiniMax latest open-sourcing LLM, MiniMax-M1 â€” setting new standards in long-context reasoning,m" src="https://external-preview.redd.it/NmY1emg2N3kzYzdmMYrLLSKpxq16_nlRw_xdAcAPTlqNhk8r4UDdsUawD6kP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3275f690016b299979a56d72371c6133b5aa21d3" title="MiniMax latest open-sourcing LLM, MiniMax-M1 â€” setting new standards in long-context reasoning,m" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The coding demo in video is so amazing!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Worldâ€™s longest context window: 1M-token input, 80k-token output&lt;/li&gt; &lt;li&gt;State-of-the-art agentic use among open-source models&lt;/li&gt; &lt;li&gt;&lt;p&gt;RL at unmatched efficiency: trained with just $534,700&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;40k: â€‹&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-40k"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M1-40k&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;80k: â€‹&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-80k"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M1-80k&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Space: â€‹&lt;a href="https://huggingface.co/spaces/MiniMaxAI/MiniMax-M1"&gt;https://huggingface.co/spaces/MiniMaxAI/MiniMax-M1&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/MiniMax-AI/MiniMax-M1"&gt;https://github.com/MiniMax-AI/MiniMax-M1&lt;/a&gt; &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Tech Report: &lt;a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/MiniMax_M1_tech_report.pdf"&gt;https://github.com/MiniMax-AI/MiniMax-M1/blob/main/MiniMax_M1_tech_report.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Apache 2.0 license&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/srtng"&gt; /u/srtng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t859utey3c7f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld116d/minimax_latest_opensourcing_llm_minimaxm1_setting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld116d/minimax_latest_opensourcing_llm_minimaxm1_setting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T18:42:52+00:00</published>
  </entry>
</feed>
