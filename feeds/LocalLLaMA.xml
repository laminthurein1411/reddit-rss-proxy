<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-22T12:49:25+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k4oufe</id>
    <title>Orpheus-TTS local speech synthesizer in C#</title>
    <updated>2025-04-21T21:13:48+00:00</updated>
    <author>
      <name>/u/ajpy</name>
      <uri>https://old.reddit.com/user/ajpy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/TheAjaykrishnanR/TaraSharp"&gt;Repo&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No python dependencies&lt;/li&gt; &lt;li&gt;No LM Studio&lt;/li&gt; &lt;li&gt;Should work out of the box &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Uses LlamaSharp (llama.cpp) backend for inference and TorchSharp for decoding. Requires .NET 9 and Cuda 12.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajpy"&gt; /u/ajpy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4oufe/orpheustts_local_speech_synthesizer_in_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4oufe/orpheustts_local_speech_synthesizer_in_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4oufe/orpheustts_local_speech_synthesizer_in_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T21:13:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1k55eeo</id>
    <title>Gemma3:12b hallucinating when reading images, anyone else?</title>
    <updated>2025-04-22T12:45:48+00:00</updated>
    <author>
      <name>/u/just-crawling</name>
      <uri>https://old.reddit.com/user/just-crawling</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k55eeo/gemma312b_hallucinating_when_reading_images/"&gt; &lt;img alt="Gemma3:12b hallucinating when reading images, anyone else?" src="https://b.thumbs.redditmedia.com/J2hoDxMmhrWQFydb3uGTflkCCVt567t2-rt0y0ZTLNk.jpg" title="Gemma3:12b hallucinating when reading images, anyone else?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running the gemma3:12b model (tried the base model, and also the qat model) on ollama (with OpenWeb UI).&lt;/p&gt; &lt;p&gt;And it looks like it massively hallucinates, it even does the math wrong and occasionally (actually quite often) attempts to add in random PC parts to the list.&lt;/p&gt; &lt;p&gt;I see many people claiming that it is a breakthrough for OCR, but I feel like it is unreliable. Is it just my setup?&lt;/p&gt; &lt;p&gt;Rig: 5070TI with 16GB Vram&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/just-crawling"&gt; /u/just-crawling &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k55eeo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k55eeo/gemma312b_hallucinating_when_reading_images/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k55eeo/gemma312b_hallucinating_when_reading_images/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T12:45:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4wt5n</id>
    <title>Does anyone know of a repository of high quality sample voices with descriptions?</title>
    <updated>2025-04-22T03:34:00+00:00</updated>
    <author>
      <name>/u/Erdeem</name>
      <uri>https://old.reddit.com/user/Erdeem</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for as professional sample voices (not celebrities) that come with descriptions, attributes or labels, similar too Elevenlabs. I'd like to be able to use it in Orpheus. &lt;/p&gt; &lt;p&gt;Ex:: Oracle X- An experienced British female voice narrator with a smooth, warm, engaging tone. Attributes- Professional Voice Clone HQ&lt;/p&gt; &lt;p&gt;Labels- Calm Middle-Aged Female English (British) Narrative &amp;amp; Story&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Erdeem"&gt; /u/Erdeem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4wt5n/does_anyone_know_of_a_repository_of_high_quality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4wt5n/does_anyone_know_of_a_repository_of_high_quality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4wt5n/does_anyone_know_of_a_repository_of_high_quality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T03:34:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1k49h0n</id>
    <title>24GB Arc GPU might still be on the way - less expensive alternative for a 3090/4090/7900XTX to run LLMs?</title>
    <updated>2025-04-21T09:35:34+00:00</updated>
    <author>
      <name>/u/PhantomWolf83</name>
      <uri>https://old.reddit.com/user/PhantomWolf83</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k49h0n/24gb_arc_gpu_might_still_be_on_the_way_less/"&gt; &lt;img alt="24GB Arc GPU might still be on the way - less expensive alternative for a 3090/4090/7900XTX to run LLMs?" src="https://external-preview.redd.it/WdUGpP3unGKFZihZrELR3GH6ZUOa768rHIdn2YSXrsA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=461a9ba85d2877f5c00bb8c11f93f1ceac11d893" title="24GB Arc GPU might still be on the way - less expensive alternative for a 3090/4090/7900XTX to run LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhantomWolf83"&gt; /u/PhantomWolf83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/sparkle-confirms-arc-battlemage-gpu-with-24gb-memory-slated-for-may-june"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k49h0n/24gb_arc_gpu_might_still_be_on_the_way_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k49h0n/24gb_arc_gpu_might_still_be_on_the_way_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T09:35:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4txxw</id>
    <title>Using a Thunderbolt eGPU Enclosure to Increase VRAM Availability on my Desktop - My Experience</title>
    <updated>2025-04-22T01:05:57+00:00</updated>
    <author>
      <name>/u/Anarchaotic</name>
      <uri>https://old.reddit.com/user/Anarchaotic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;This was a fun experiment and a pretty niche use-case, but I basically had everything sitting around anyway. &lt;/p&gt; &lt;p&gt;My desktop is running an RTX 5080, 32GB of RAM, and a 14700k. It was never built to be an LLM machine, but I figured I'd start experimenting with some smaller models that fit within the VRAM.&lt;/p&gt; &lt;p&gt;I also had an old Razer Core X eGPU enclosure sitting around - and put my 3070 in it.&lt;/p&gt; &lt;p&gt;My current PSU wouldn't have been able to handle both cards plugged directly into the MOBO, and I wasn't about to buy a new PSU just to try this out.&lt;/p&gt; &lt;p&gt;I already had a Thunderbolt 4 (GC Maple Ridge) card in my desktop, so I just needed to hook them all up.&lt;/p&gt; &lt;p&gt;Well I was surprised to see how easy it was for Ollama to just start utilizing all of the GPUs. I changed the OLLAMA_VISIBLE_DEVICES environment variable to &amp;quot;0,1&amp;quot; and OLLAMA_SCHED_SPREAD to &amp;quot;1&amp;quot;, and that was about it.&lt;/p&gt; &lt;p&gt;I can go in-depth into findings, but here's generally what I've seen:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Models that previously fit in VRAM ran 30-40% slower. That's pretty expected, the bottleneck of TB4 shows a 141GB/s throughput for the 3070, which is much lower than its 481GB/s BUS speed that it can hypothetically hit. So I was bottlenecked immediately. However I'm okay with that because it allows to me to significantly increase the context size for models I was running before, at rates I'm still perfectly happy with (30&amp;gt; tk/s). &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Models that fit within 24GB of VRAM ran 5-6x better overall. Also expected - even with the TB4 bottleneck, being able to run the entire model in-memory was a massive improvement. As an example, qwq 32b Q4 runs at 13.1tk/s on average with both cards, but gets crushed down to 2.5tk/s on just the 5080. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If I had a 1250W PSU I would love to try hooking it up the 3070 to a motherboard to get a much better idea the TB4 bottleneck. A hypothetical Oculink-supported enclosure + interface would also double my speeds, but that's way more effort to try and lock down.&lt;/p&gt; &lt;p&gt;This makes me curious enough to keep an eye out for 16gb 4060tis, as it would give me 32GB of usable VRAM, which opens up options for much stronger models than the 8b/12b ones I've been running before.&lt;/p&gt; &lt;p&gt;tl;dr - Using an eGPU enclosure with another Nvidia card works on a desktop - assuming you have a thunderbolt connector installed. This makes models that fit in the pooled VRAM space run significantly better than offloading to CPU/RAM, but by default will hinder performance of models that fit in a single card due to TB4 bottlenecks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anarchaotic"&gt; /u/Anarchaotic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4txxw/using_a_thunderbolt_egpu_enclosure_to_increase/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4txxw/using_a_thunderbolt_egpu_enclosure_to_increase/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4txxw/using_a_thunderbolt_egpu_enclosure_to_increase/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T01:05:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4gqje</id>
    <title>[llama.cpp git] mtmd: merge llava, gemma3 and minicpmv CLI into single llama-mtmd-cli</title>
    <updated>2025-04-21T15:46:41+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4gqje/llamacpp_git_mtmd_merge_llava_gemma3_and_minicpmv/"&gt; &lt;img alt="[llama.cpp git] mtmd: merge llava, gemma3 and minicpmv CLI into single llama-mtmd-cli" src="https://external-preview.redd.it/chyB3Fwy2UcKLBJMyzabSe7PfMaM2G1ZJw5k660LQOY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c97a934134489f4b1d7e573c0218731d1a8a5d5b" title="[llama.cpp git] mtmd: merge llava, gemma3 and minicpmv CLI into single llama-mtmd-cli" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/commit/84a9bf2fc2875205f0806fbbfbb66dc67204094c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4gqje/llamacpp_git_mtmd_merge_llava_gemma3_and_minicpmv/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4gqje/llamacpp_git_mtmd_merge_llava_gemma3_and_minicpmv/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T15:46:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4p8a1</id>
    <title>HyperAgent: open-source Browser Automation with LLMs</title>
    <updated>2025-04-21T21:29:40+00:00</updated>
    <author>
      <name>/u/LawfulnessFlat9560</name>
      <uri>https://old.reddit.com/user/LawfulnessFlat9560</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4p8a1/hyperagent_opensource_browser_automation_with_llms/"&gt; &lt;img alt="HyperAgent: open-source Browser Automation with LLMs" src="https://external-preview.redd.it/pG2fXTePiHCTWQNdBrjC6ZrKGhk2WsgN52b6QusON9Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3492e14aad472fd9e73368bb9ecf897800ec09b0" title="HyperAgent: open-source Browser Automation with LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to show you &lt;strong&gt;HyperAgent&lt;/strong&gt;, a wrapper around Playwright that lets you control pages with LLMs.&lt;/p&gt; &lt;p&gt;With HyperAgent, you can run functions like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;await page.ai(&amp;quot;search for noise-cancelling headphones under $100 and click the best option&amp;quot;); or const data = await page.ai( &amp;quot;Give me the director, release year, and rating for 'The Matrix'&amp;quot;, { outputSchema: z.object({ director: z.string().describe(&amp;quot;The name of the movie director&amp;quot;), releaseYear: z.number().describe(&amp;quot;The year the movie was released&amp;quot;), rating: z.string().describe(&amp;quot;The IMDb rating of the movie&amp;quot;), }), } ); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We built this because automation is still too brittle and manual. HTML keeps changing and selectors break constantly, Writing full automation scripts is overkill for quick one-offs. Also, and possibly most importantly, AI Agents need some way to interact with the web with natural language.&lt;/p&gt; &lt;p&gt;Excited to see what you all think! We are rapidly adding new features so would love any ideas for how we can make this better :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LawfulnessFlat9560"&gt; /u/LawfulnessFlat9560 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/hyperbrowserai/HyperAgent"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4p8a1/hyperagent_opensource_browser_automation_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4p8a1/hyperagent_opensource_browser_automation_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T21:29:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4avlq</id>
    <title>What's the best models available today to run on systems with 8 GB / 16 GB / 24 GB / 48 GB / 72 GB / 96 GB of VRAM today?</title>
    <updated>2025-04-21T11:08:51+00:00</updated>
    <author>
      <name>/u/Severin_Suveren</name>
      <uri>https://old.reddit.com/user/Severin_Suveren</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says, since many aren't that experienced with running local LLMs and the choice of models, what are the best models available today for the different ranges of VRAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severin_Suveren"&gt; /u/Severin_Suveren &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4avlq/whats_the_best_models_available_today_to_run_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4avlq/whats_the_best_models_available_today_to_run_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4avlq/whats_the_best_models_available_today_to_run_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T11:08:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4sxh7</id>
    <title>So, is it reasonable to expect the next generation of local oriented models to be QAT out of the oven?</title>
    <updated>2025-04-22T00:16:02+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With Gemma3 news and posts all around‚Ä¶ would next Gen of model‚Äôs, Either Dense or MoE, go from 32b to 128b, ‚ÄúQAT‚Äôed‚Äù since training, aiming to be deployed in common VRAM sizes of 8-16-24/32 in the end anyway?&lt;/p&gt; &lt;p&gt;Is QAT less resource intense during training, or is the same?&lt;/p&gt; &lt;p&gt;Just elaborating here‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4sxh7/so_is_it_reasonable_to_expect_the_next_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4sxh7/so_is_it_reasonable_to_expect_the_next_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4sxh7/so_is_it_reasonable_to_expect_the_next_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T00:16:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4m3az</id>
    <title>Here is the HUGE Ollama main dev contribution to llamacpp :)</title>
    <updated>2025-04-21T19:21:36+00:00</updated>
    <author>
      <name>/u/Nexter92</name>
      <uri>https://old.reddit.com/user/Nexter92</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4m3az/here_is_the_huge_ollama_main_dev_contribution_to/"&gt; &lt;img alt="Here is the HUGE Ollama main dev contribution to llamacpp :)" src="https://b.thumbs.redditmedia.com/70s_LNOtMI87Du6TdCS17PIa9bztkX6iXhhbwCa8IeY.jpg" title="Here is the HUGE Ollama main dev contribution to llamacpp :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Less than 100 lines of code ü§°&lt;/p&gt; &lt;p&gt;If you truly want to support open source LLM space, use anything else than ollama specily if you have an AMD GPU, you loose way to much performance in text generation using ROCm with ollama.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6979nmxwm8we1.png?width=2020&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=91e49f15bee12d308716de607ce6763b8e1870b3"&gt;https://preview.redd.it/6979nmxwm8we1.png?width=2020&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=91e49f15bee12d308716de607ce6763b8e1870b3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nexter92"&gt; /u/Nexter92 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4m3az/here_is_the_huge_ollama_main_dev_contribution_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4m3az/here_is_the_huge_ollama_main_dev_contribution_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4m3az/here_is_the_huge_ollama_main_dev_contribution_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T19:21:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4zs4i</id>
    <title>An Easy-to-use Knowledge Editing Framework for LLMs.</title>
    <updated>2025-04-22T06:38:28+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4zs4i/an_easytouse_knowledge_editing_framework_for_llms/"&gt; &lt;img alt="An Easy-to-use Knowledge Editing Framework for LLMs." src="https://external-preview.redd.it/z5X2wY3bbph0pn54BXd1X3klLMzXze22WcY9yNGnCV4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c3795dcfdd6bdfc7f874246e3aaf38faeb4539d8" title="An Easy-to-use Knowledge Editing Framework for LLMs." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/zjunlp/EasyEdit"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4zs4i/an_easytouse_knowledge_editing_framework_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4zs4i/an_easytouse_knowledge_editing_framework_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T06:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1k50u8i</id>
    <title>Sleep-time Compute: Beyond Inference Scaling at Test-time</title>
    <updated>2025-04-22T07:54:24+00:00</updated>
    <author>
      <name>/u/newdoria88</name>
      <uri>https://old.reddit.com/user/newdoria88</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newdoria88"&gt; /u/newdoria88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2504.13171"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k50u8i/sleeptime_compute_beyond_inference_scaling_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k50u8i/sleeptime_compute_beyond_inference_scaling_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T07:54:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1k547al</id>
    <title>New Lib to process PDFs</title>
    <updated>2025-04-22T11:43:07+00:00</updated>
    <author>
      <name>/u/Electronic-Lab-7343</name>
      <uri>https://old.reddit.com/user/Electronic-Lab-7343</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I built a library over the holiday that converts PDF documents to Markdown. It segments by page, extracts relevant elements like titles, images, and tables, and even counts tokens per page. (&lt;a href="https://github.com/matthsena/AlcheMark"&gt;AlcheMark&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Some advantages compared to competitors (Docling):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: In my test with a 500-page file, this library parsed it in 45 seconds. Docling around 3 minutes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;References&lt;/strong&gt;: Docling convert the entire file into a single large Markdown block without page segmentation, making it harder for LLMs to reference which page the information came from. This library returns a vector of objects‚Äîone for each page.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Token estimation&lt;/strong&gt;: The library shows the token count for each page, allowing better cost estimation before sending a prompt.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For this project, I make a ensemble of several existing libraries with a different approach to data handling.&lt;/p&gt; &lt;p&gt;If you'd like to contribute or support the project, feel free to leave a star on GitHub:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/matthsena/AlcheMark"&gt;https://github.com/matthsena/AlcheMark&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic-Lab-7343"&gt; /u/Electronic-Lab-7343 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k547al/new_lib_to_process_pdfs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k547al/new_lib_to_process_pdfs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k547al/new_lib_to_process_pdfs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T11:43:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4ov9e</id>
    <title>Meta Perception Language Model: Enhancing Understanding of Visual Perception Tasks</title>
    <updated>2025-04-21T21:14:47+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4ov9e/meta_perception_language_model_enhancing/"&gt; &lt;img alt="Meta Perception Language Model: Enhancing Understanding of Visual Perception Tasks" src="https://external-preview.redd.it/dXlleHRwcW03OXdlMTJfwlZ1QfIuL9mmXOeUB99y5PuEqD7QQlGvCc8SfvTb.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ed0a94e182a642f2f9da5dfdbf4855a0e1e9902" title="Meta Perception Language Model: Enhancing Understanding of Visual Perception Tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Continuing their work on perception, Meta is releasing the Perception Language Model (PLM), an open and reproducible vision-language model designed to tackle challenging visual recognition tasks.&lt;/p&gt; &lt;p&gt;Meta trained PLM using synthetic data generated at scale and open vision-language understanding datasets, without any distillation from external models. They then identified key gaps in existing data for video understanding and collected 2.5 million new, human-labeled fine-grained video QA and spatio-temporal caption samples to fill these gaps, forming the largest dataset of its kind to date.&lt;/p&gt; &lt;p&gt;PLM is trained on this massive dataset, using a combination of human-labeled and synthetic data to create a robust, accurate, and fully reproducible model. PLM offers variants with 1, 3, and 8 billion parameters, making it well suited for fully transparent academic research.&lt;/p&gt; &lt;p&gt;Meta is also sharing a new benchmark, PLM-VideoBench, which focuses on tasks that existing benchmarks miss: fine-grained activity understanding and spatiotemporally grounded reasoning. It is hoped that their open and large-scale dataset, challenging benchmark, and strong models together enable the open source community to build more capable computer vision systems.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/facebook/perception-lm-67f9783f171948c383ee7498"&gt;Download the model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/facebookresearch/perception_models"&gt;Download the code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ai.meta.com/datasets/plm-data/"&gt;Download the dataset&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ai.meta.com/research/publications/perceptionlm-open-access-data-and-models-for-detailed-visual-understanding/"&gt;Read the paper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5n4izmqm79we1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4ov9e/meta_perception_language_model_enhancing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4ov9e/meta_perception_language_model_enhancing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T21:14:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4oqpi</id>
    <title>Skywork releases SkyReels-V2 - unlimited duration video generation model</title>
    <updated>2025-04-21T21:09:27+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4oqpi/skywork_releases_skyreelsv2_unlimited_duration/"&gt; &lt;img alt="Skywork releases SkyReels-V2 - unlimited duration video generation model" src="https://b.thumbs.redditmedia.com/b-ouJGfwVRVGxRTcTn9sJ47FxSTE7u69a29L1PRi4yg.jpg" title="Skywork releases SkyReels-V2 - unlimited duration video generation model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Available in 1.3B and 14B, these models allow us to generate Infinite-Length videos. &lt;/p&gt; &lt;p&gt;They support both text-to-video (T2V) and image-to-video (I2V)tasks.&lt;/p&gt; &lt;p&gt;According to the benchmarks shared in model‚Äôs card, SkyReels-V2 outperforms all compared models including HunyuanVideo-13B and Wan2.1-14B.&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://huggingface.co/papers/2504.13074"&gt;https://huggingface.co/papers/2504.13074&lt;/a&gt; Models: &lt;a href="https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9"&gt;https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All-in-one creator toolkit and guide: &lt;a href="https://x.com/ai_for_success/status/1914159352812036463?s=46"&gt;https://x.com/ai_for_success/status/1914159352812036463?s=46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k4oqpi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4oqpi/skywork_releases_skyreelsv2_unlimited_duration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4oqpi/skywork_releases_skyreelsv2_unlimited_duration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T21:09:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1k50wo0</id>
    <title>Veiled Rose 22B : Bigger, Smarter and Noicer</title>
    <updated>2025-04-22T07:59:30+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k50wo0/veiled_rose_22b_bigger_smarter_and_noicer/"&gt; &lt;img alt="Veiled Rose 22B : Bigger, Smarter and Noicer" src="https://preview.redd.it/99f47p7cecwe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f01384216591c1b49b1cd83d95948d1d577c9571" title="Veiled Rose 22B : Bigger, Smarter and Noicer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If youve tried my &lt;a href="https://huggingface.co/soob3123/Veiled-Calla-12B"&gt;Veiled Calla 12B&lt;/a&gt; you know how it goes. but since it was a 12B model, there were some pretty obvious short comings. &lt;/p&gt; &lt;p&gt;Here is the Mistral Based 22B model, with better cognition and reasoning. Test it out and let me your feedback!&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/soob3123/Veiled-Rose-22B"&gt;soob3123/Veiled-Rose-22B ¬∑ Hugging Face &lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/soob3123/Veiled-Rose-22B-gguf"&gt;soob3123/Veiled-Rose-22B-gguf ¬∑ Hugging Face&lt;/a&gt;&lt;a href="https://huggingface.co/soob3123/Veiled-Rose-22B"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/99f47p7cecwe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k50wo0/veiled_rose_22b_bigger_smarter_and_noicer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k50wo0/veiled_rose_22b_bigger_smarter_and_noicer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T07:59:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4juhd</id>
    <title>Don‚Äôt Trust This Woman ‚Äî She Keeps Lying</title>
    <updated>2025-04-21T17:53:19+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4juhd/dont_trust_this_woman_she_keeps_lying/"&gt; &lt;img alt="Don‚Äôt Trust This Woman ‚Äî She Keeps Lying" src="https://b.thumbs.redditmedia.com/Ynyt95TWgEz727VUzv3tN5Ii66-Y9GWTf5C3vgNu0QA.jpg" title="Don‚Äôt Trust This Woman ‚Äî She Keeps Lying" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/j64rtjys78we1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3dad28af7dddad7a111f8585f646fa0fb66940fc"&gt;Qwen Official Denial&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9e6t4n4x78we1.png?width=1192&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31e0c2ca15b925b8d06dfede2548d4c375505196"&gt;New Deepseek Rumor&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4juhd/dont_trust_this_woman_she_keeps_lying/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4juhd/dont_trust_this_woman_she_keeps_lying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4juhd/dont_trust_this_woman_she_keeps_lying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T17:53:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1k54mdz</id>
    <title>Stanford CS 25 Transformers Course (OPEN TO EVERYBODY)</title>
    <updated>2025-04-22T12:05:15+00:00</updated>
    <author>
      <name>/u/MLPhDStudent</name>
      <uri>https://old.reddit.com/user/MLPhDStudent</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Tl;dr: One of Stanford's hottest seminar courses. We open the course through Zoom to the public. Lectures on Tuesdays, 3-4:20pm PDT (Zoom link on course website). Talks will be recorded and released ~3 weeks after each lecture. Course website:&lt;/strong&gt; &lt;a href="https://web.stanford.edu/class/cs25/"&gt;&lt;strong&gt;https://web.stanford.edu/class/cs25/&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our lecture later &lt;strong&gt;today at 3pm PDT&lt;/strong&gt; is &lt;strong&gt;Eric Zelikman from xAI&lt;/strong&gt;, discussing ‚ÄúWe're All in this Together: Human Agency in an Era of Artificial Agents‚Äù. &lt;strong&gt;This talk will NOT be recorded!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Each week, we invite folks at the forefront of Transformers research to discuss the latest breakthroughs, from LLM architectures like GPT and Gemini to creative use cases in generating art (e.g. DALL-E and Sora), biology and neuroscience applications, robotics, and so forth!&lt;/p&gt; &lt;p&gt;We invite the coolest speakers such as &lt;strong&gt;Andrej Karpathy, Geoffrey Hinton, Jim Fan, Ashish Vaswani&lt;/strong&gt;, and folks from &lt;strong&gt;OpenAI, Google, NVIDIA&lt;/strong&gt;, etc.&lt;/p&gt; &lt;p&gt;The &lt;strong&gt;recording of the first lecture&lt;/strong&gt; is released! &lt;strong&gt;Check it out&lt;/strong&gt; &lt;a href="https://www.youtube.com/watch?v=JKbtWimlzAE"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; We gave a brief overview of Transformers, discussed pretraining (focusing on data strategies [&lt;a href="https://arxiv.org/abs/2408.03617"&gt;1&lt;/a&gt;,&lt;a href="https://arxiv.org/abs/2412.15285"&gt;2&lt;/a&gt;]) and post-training, and highlighted recent trends, applications, and remaining challenges/weaknesses of Transformers. Slides are &lt;a href="https://docs.google.com/presentation/d/16tMMBUjPnqw-PvxF8xzu2m1Epdo1fH7nXWlt3mt2q5w/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Check out our course website for more!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLPhDStudent"&gt; /u/MLPhDStudent &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://web.stanford.edu/class/cs25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k54mdz/stanford_cs_25_transformers_course_open_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k54mdz/stanford_cs_25_transformers_course_open_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T12:05:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4god7</id>
    <title>GLM-4 32B is mind blowing</title>
    <updated>2025-04-21T15:41:35+00:00</updated>
    <author>
      <name>/u/Timely_Second_6414</name>
      <uri>https://old.reddit.com/user/Timely_Second_6414</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4god7/glm4_32b_is_mind_blowing/"&gt; &lt;img alt="GLM-4 32B is mind blowing" src="https://external-preview.redd.it/KGA1Keg1D7oCkdV6UW_ifq_mQe-5jNP1DvhwwJ2Stbs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=128cb2de26af5c23c04d8bec8b39d61ce2d36274" title="GLM-4 32B is mind blowing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1k4god7/video/815w430kg7we1/player"&gt;GLM-4 32B pygame earth simulation, I tried this with gemini 2.5 flash which gave an error as output.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Title says it all. I tested out GLM-4 32B Q8 locally using PiDack's llama.cpp pr (&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12957/"&gt;https://github.com/ggml-org/llama.cpp/pull/12957/&lt;/a&gt;) as ggufs are currently broken.&lt;/p&gt; &lt;p&gt;I am absolutely amazed by this model. It outperforms every single other ~32B local model and even outperforms 72B models. It's literally Gemini 2.5 flash (non reasoning) at home, but better. It's also fantastic with tool calling and works well with cline/aider.&lt;/p&gt; &lt;p&gt;But the thing I like the most is that this model is not afraid to output a lot of code. It does not truncate anything or leave out implementation details. Below I will provide an example where it 0-shot produced 630 lines of code (I had to ask it to continue because the response got cut off at line 550). I have no idea how they trained this, but I am really hoping qwen 3 does something similar. &lt;/p&gt; &lt;p&gt;Below are some examples of 0 shot requests comparing GLM 4 versus gemini 2.5 flash (non-reasoning). GLM is run locally with temp 0.6 and top_p 0.95 at Q8. Output speed is 22t/s for me on 3x 3090.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Solar system&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;prompt: Create a realistic rendition of our solar system using html, css and js. Make it stunning! reply with one file.&lt;/p&gt; &lt;p&gt;Gemini response:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k4god7/video/vhn6r9kmi7we1/player"&gt;Gemini 2.5 flash: nothing is interactible, planets dont move at all&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM response:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k4god7/video/ylcl9s4ri7we1/player"&gt;GLM-4-32B response. Sun label and orbit rings are off, but it looks way better and theres way more detail.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Neural network visualization&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;prompt: code me a beautiful animation/visualization in html, css, js of how neural networks learn. Make it stunningly beautiful, yet intuitive to understand. Respond with all the code in 1 file. You can use threejs&lt;/p&gt; &lt;p&gt;Gemini:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k4god7/video/nkgj1wc1j7we1/player"&gt;Gemini response: network looks good, but again nothing moves, no interactions.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM 4:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k4god7/video/equidag5j7we1/player"&gt;GLM 4 response (one shot 630 lines of code): It tried to plot data that will be fit on the axes. Although you dont see the fitting process you can see the neurons firing and changing in size based on their weight. Theres also sliders to adjust lr and hidden size. Not perfect, but still better.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also did a few other prompts and GLM generally outperformed gemini on most tests. Note that this is only Q8, I imaging full precision might be even a little better. &lt;/p&gt; &lt;p&gt;Please share your experiences or examples if you have tried the model. I havent tested the reasoning variant yet, but I imagine its also very good.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Timely_Second_6414"&gt; /u/Timely_Second_6414 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4god7/glm4_32b_is_mind_blowing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4god7/glm4_32b_is_mind_blowing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4god7/glm4_32b_is_mind_blowing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T15:41:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4w9p2</id>
    <title>I uploaded GLM-4-32B-0414 &amp; GLM-Z1-32B-0414 Q4_K_M to ollama</title>
    <updated>2025-04-22T03:05:20+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4w9p2/i_uploaded_glm432b0414_glmz132b0414_q4_k_m_to/"&gt; &lt;img alt="I uploaded GLM-4-32B-0414 &amp;amp; GLM-Z1-32B-0414 Q4_K_M to ollama" src="https://external-preview.redd.it/nNe4kZO07RNpumX0f4yeWEEM85RJxuepCtUYC_PxayA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46802610d70a0e31fc2090661cc81ad54474c358" title="I uploaded GLM-4-32B-0414 &amp;amp; GLM-Z1-32B-0414 Q4_K_M to ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;This model requires &lt;a href="https://github.com/ollama/ollama/releases"&gt;Ollama v0.6.6 or later&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;instruct: &lt;code&gt;ollama run JollyLlama/GLM-4-32B-0414-Q4_K_M&lt;/code&gt;&lt;/p&gt; &lt;p&gt;reasoning: &lt;code&gt;ollama run JollyLlama/GLM-Z1-32B-0414-Q4_K_M&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.ollama.com/JollyLlama/GLM-4-32B-0414-Q4_K_M"&gt;https://www.ollama.com/JollyLlama/GLM-4-32B-0414-Q4_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.ollama.com/JollyLlama/GLM-Z1-32B-0414-Q4_K_M"&gt;https://www.ollama.com/JollyLlama/GLM-Z1-32B-0414-Q4_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks to matteo for uploading the fixed gguf to HF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/matteogeniaccio"&gt;https://huggingface.co/matteogeniaccio&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qno0sx5iyawe1.png?width=995&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9f21e057c723a0351238aff1e49f1d365368954d"&gt;https://preview.redd.it/qno0sx5iyawe1.png?width=995&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9f21e057c723a0351238aff1e49f1d365368954d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4w9p2/i_uploaded_glm432b0414_glmz132b0414_q4_k_m_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4w9p2/i_uploaded_glm432b0414_glmz132b0414_q4_k_m_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4w9p2/i_uploaded_glm432b0414_glmz132b0414_q4_k_m_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T03:05:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1k52h4n</id>
    <title>Why is MythoMax13B still in high demand?</title>
    <updated>2025-04-22T09:54:54+00:00</updated>
    <author>
      <name>/u/Consistent_Winner596</name>
      <uri>https://old.reddit.com/user/Consistent_Winner596</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently noticed, that MythoMax13B is really high ranked on openrouter in the RPG section and has high demand. That makes no sense to me, as it is a still a Llama2 era model. Is that model so good or is it promoted in the openrouter chat rooms or on other platforms actively, but even if that is the reason it makes no sense. Why didn't they then use modern RP models and stick to that one, can someone who played with that model answer it? Is it just that good or brings still using a L2 other benefits I don't see at the moment? Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Consistent_Winner596"&gt; /u/Consistent_Winner596 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k52h4n/why_is_mythomax13b_still_in_high_demand/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k52h4n/why_is_mythomax13b_still_in_high_demand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k52h4n/why_is_mythomax13b_still_in_high_demand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T09:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1k54foj</id>
    <title>Let us build DeepSeek from Scratch | No fluff | 13 lectures uploaded</title>
    <updated>2025-04-22T11:55:45+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k54foj/let_us_build_deepseek_from_scratch_no_fluff_13/"&gt; &lt;img alt="Let us build DeepSeek from Scratch | No fluff | 13 lectures uploaded" src="https://external-preview.redd.it/pAa68GpmjnpZeahm_YMGQkYTs9KtW9HemhGbAYHU02s.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=555355166a247eb92939344c89b96ed48dd7655a" title="Let us build DeepSeek from Scratch | No fluff | 13 lectures uploaded" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/5w0lu5m2ldwe1.gif"&gt;A few notes I made as part of this playlist&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚ÄúCan I build the DeepSeek architecture and model myself, from scratch?‚Äù&lt;/p&gt; &lt;p&gt;You can. You need to know the nuts and bolts. &lt;/p&gt; &lt;p&gt;4 weeks back, we launched our playlist: ‚ÄúBuild DeepSeek from Scratch‚Äù &lt;/p&gt; &lt;p&gt;Until now, we have uploaded 13 lectures in this playlist: &lt;/p&gt; &lt;p&gt;(1) DeepSeek series introduction: &lt;a href="https://youtu.be/QWNxQIq0hMo"&gt;https://youtu.be/QWNxQIq0hMo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(2) DeepSeek basics: &lt;a href="https://youtu.be/WjhDDeZ7DvM"&gt;https://youtu.be/WjhDDeZ7DvM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(3) Journey of a token into the LLM architecture: &lt;a href="https://youtu.be/rkEYwH4UGa4"&gt;https://youtu.be/rkEYwH4UGa4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(4) Attention mechanism explained in 1 hour: &lt;a href="https://youtu.be/K45ze9Yd5UE"&gt;https://youtu.be/K45ze9Yd5UE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(5) Self Attention Mechanism - Handwritten from scratch: &lt;a href="https://youtu.be/s8mskq-nzec"&gt;https://youtu.be/s8mskq-nzec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(6) Causal Attention Explained: Don't Peek into the Future: &lt;a href="https://youtu.be/c6Kkj6iLeBg"&gt;https://youtu.be/c6Kkj6iLeBg&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(7) Multi-Head Attention Visually Explained: &lt;a href="https://youtu.be/qbN4ulK-bZA"&gt;https://youtu.be/qbN4ulK-bZA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(8) Multi-Head Attention Handwritten from Scratch: &lt;a href="https://youtu.be/rvsEW-EsD-Y"&gt;https://youtu.be/rvsEW-EsD-Y&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(9) Key Value Cache from Scratch: &lt;a href="https://youtu.be/IDwTiS4_bKo"&gt;https://youtu.be/IDwTiS4_bKo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(10) Multi-Query Attention Explained: &lt;a href="https://youtu.be/Z6B51Odtn-Y"&gt;https://youtu.be/Z6B51Odtn-Y&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(11) Understand Grouped Query Attention (GQA): &lt;a href="https://youtu.be/kx3rETIxo4Q"&gt;https://youtu.be/kx3rETIxo4Q&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(12) Multi-Head Latent Attention From Scratch: &lt;a href="https://youtu.be/NlDQUj1olXM"&gt;https://youtu.be/NlDQUj1olXM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(13) Multi-Head Latent Attention Coded from Scratch in Python: &lt;a href="https://youtu.be/mIaWmJVrMpc"&gt;https://youtu.be/mIaWmJVrMpc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Next to come: &lt;/p&gt; &lt;p&gt;- Rotary Positional Encoding (RoPE)&lt;/p&gt; &lt;p&gt;- DeepSeek MLA + RoPE&lt;/p&gt; &lt;p&gt;- DeepSeek Mixture of Experts (MoE)&lt;/p&gt; &lt;p&gt;- Multi-token Prediction (MTP)&lt;/p&gt; &lt;p&gt;- Supervised Fine-Tuning (SFT)&lt;/p&gt; &lt;p&gt;- Group Relative Policy Optimisation (GRPO)&lt;/p&gt; &lt;p&gt;- DeepSeek PTX innovation&lt;/p&gt; &lt;p&gt;This playlist won‚Äôt be a 1 hour or 2 hour video. This will be a mega playlist of 35-40 videos with a duration of 40+ hours.&lt;/p&gt; &lt;p&gt;I have made this with a lot of passion. &lt;/p&gt; &lt;p&gt;Would look forward to support and your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k54foj/let_us_build_deepseek_from_scratch_no_fluff_13/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k54foj/let_us_build_deepseek_from_scratch_no_fluff_13/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k54foj/let_us_build_deepseek_from_scratch_no_fluff_13/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T11:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4lmil</id>
    <title>A new TTS model capable of generating ultra-realistic dialogue</title>
    <updated>2025-04-21T19:02:56+00:00</updated>
    <author>
      <name>/u/aadoop6</name>
      <uri>https://old.reddit.com/user/aadoop6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4lmil/a_new_tts_model_capable_of_generating/"&gt; &lt;img alt="A new TTS model capable of generating ultra-realistic dialogue" src="https://external-preview.redd.it/wNcjaJIfjy5w8Wuatdn7tqqANxkzwO5-UB9WQmMCT3w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f1d15a76610dd0dbe8a436684ca2985b2cc492b" title="A new TTS model capable of generating ultra-realistic dialogue" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aadoop6"&gt; /u/aadoop6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/nari-labs/dia"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4lmil/a_new_tts_model_capable_of_generating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4lmil/a_new_tts_model_capable_of_generating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T19:02:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1k546sq</id>
    <title>THUDM/SWE-Dev-9B ¬∑ Hugging Face</title>
    <updated>2025-04-22T11:42:23+00:00</updated>
    <author>
      <name>/u/bobby-chan</name>
      <uri>https://old.reddit.com/user/bobby-chan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k546sq/thudmswedev9b_hugging_face/"&gt; &lt;img alt="THUDM/SWE-Dev-9B ¬∑ Hugging Face" src="https://external-preview.redd.it/H-WqhcnMRaUejHpQLqeLIABkVOOs-PNNxG0QszsYGF4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=33500f8d40e8a012a0d58c9cc5020fee1f8beadd" title="THUDM/SWE-Dev-9B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The creators of the GLM-4 models released a collection of coder models&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SWE-Dev-7B (Qwen-2.5-7B-Instruct): &lt;a href="https://huggingface.co/THUDM/SWE-Dev-7B/"&gt;https://huggingface.co/THUDM/SWE-Dev-7B/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;SWE-Dev-9B (GLM-4-9B-Chat): &lt;a href="https://huggingface.co/THUDM/SWE-Dev-9B/"&gt;https://huggingface.co/THUDM/SWE-Dev-9B/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;SWE-Dev-32B (Qwen-2.5-32B-Instruct): &lt;a href="https://huggingface.co/THUDM/SWE-Dev-32B/"&gt;https://huggingface.co/THUDM/SWE-Dev-32B/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobby-chan"&gt; /u/bobby-chan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/THUDM/SWE-Dev-9B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k546sq/thudmswedev9b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k546sq/thudmswedev9b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T11:42:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4v5fm</id>
    <title>Dia 1.6B is one of the funnest models I've ever come across.</title>
    <updated>2025-04-22T02:07:19+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/w2jq98c7oawe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4v5fm/dia_16b_is_one_of_the_funnest_models_ive_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4v5fm/dia_16b_is_one_of_the_funnest_models_ive_ever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T02:07:19+00:00</published>
  </entry>
</feed>
