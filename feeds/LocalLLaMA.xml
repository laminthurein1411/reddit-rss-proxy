<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-25T13:38:06+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kuk6ke</id>
    <title>Manifold v0.12.0 - ReAct Agent with MCP tools access.</title>
    <updated>2025-05-24T19:49:22+00:00</updated>
    <author>
      <name>/u/LocoMod</name>
      <uri>https://old.reddit.com/user/LocoMod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuk6ke/manifold_v0120_react_agent_with_mcp_tools_access/"&gt; &lt;img alt="Manifold v0.12.0 - ReAct Agent with MCP tools access." src="https://b.thumbs.redditmedia.com/uTr-VNiKMMa4VuWfO7BRJdQKVVJGuVCjgm0WHHZIa0s.jpg" title="Manifold v0.12.0 - ReAct Agent with MCP tools access." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Manifold is a platform for workflow automation using AI assistants. Please view the README for more example images. This has been mostly a solo effort and the scope is quite large so view this as an experimental hobby project not meant to be deployed to production systems (today). The documentation is non-existent, but I‚Äôm working on that. Manifold works with the popular public services as well as local OpenAI compatible endpoints such as llama.cpp and mlx_lm.server.&lt;/p&gt; &lt;p&gt;I highly recommend using capable OpenAI models, or Claude 3.7 for the agent configuration. I have also tested it with local models with success, but your configurations will vary. Gemma3 QAT with the latest improvements in llama.cpp also make it a great combination.&lt;/p&gt; &lt;p&gt;Be mindful that the MCP servers you configure will have a big impact on how the agent behaves. It is instructed to develop its own tool if a suitable one is not available. Manifold ships with a Dockerfile you can build with some basic MCP tools.&lt;/p&gt; &lt;p&gt;I highly recommend a good filesystem server such as &lt;a href="https://github.com/mark3labs/mcp-filesystem-server"&gt;https://github.com/mark3labs/mcp-filesystem-server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also highly recommend the official Playwright MCP server, NOT running in headless mode to let the agent reference web content as needed.&lt;/p&gt; &lt;p&gt;There are a lot of knobs to turn that I have not exposed to the frontend, but for advanced users that self host you can simply launch your endpoint with the ideal params. I will expose those to the UI in future updates.&lt;/p&gt; &lt;p&gt;Creative use of the nodes can yield some impressive results, once the flow based thought process clicks for you.&lt;/p&gt; &lt;p&gt;Have fun.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LocoMod"&gt; /u/LocoMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kuk6ke"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuk6ke/manifold_v0120_react_agent_with_mcp_tools_access/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuk6ke/manifold_v0120_react_agent_with_mcp_tools_access/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T19:49:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kv0ha7</id>
    <title>What personal assistants do you use?</title>
    <updated>2025-05-25T11:37:59+00:00</updated>
    <author>
      <name>/u/Hrafnstrom</name>
      <uri>https://old.reddit.com/user/Hrafnstrom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.geoffreylitt.com/2025/04/12/how-i-made-a-useful-ai-assistant-with-one-sqlite-table-and-a-handful-of-cron-jobs"&gt;This blog post&lt;/a&gt; has inspired me to either find or build a personal assistant that has some sort of memory. I intend to use it as my main LLM hub, so that it can learn everything about me and store it offline, and then use necessary bits of information about me when I prompt LLMs. &lt;/p&gt; &lt;p&gt;I vaguely remember seeing tools that sort of do this, but a bit of research yielded more confusion. What are some options I can check out?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hrafnstrom"&gt; /u/Hrafnstrom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv0ha7/what_personal_assistants_do_you_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv0ha7/what_personal_assistants_do_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kv0ha7/what_personal_assistants_do_you_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T11:37:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kv1yge</id>
    <title>Help with prompts for role play? AI also tries to speak my (human) sentences in role play...</title>
    <updated>2025-05-25T13:00:02+00:00</updated>
    <author>
      <name>/u/Excellent-Amount-277</name>
      <uri>https://old.reddit.com/user/Excellent-Amount-277</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been experimenting with some small models for local LLM role play. Generally these small models are surprisingly creative. However - as I want to make the immersion perfect I only need spoken answers. My problem is that all models sometimes try to speak my part, too. I already got a pretty good prompt to get rid of &amp;quot;descriptions&amp;quot; aka &amp;quot;The computer starts beeping and boots up&amp;quot;. However - speaking the human part is the biggest problem right now. Any ideas?&lt;/p&gt; &lt;p&gt;Here's my current System prompt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;system&amp;gt; Let's roleplay. Important, your answers are spoken. The story is set in a spaceship. You play the role of a &amp;quot;Ship Computer&amp;quot; on the spaceship Sulaco. Your name is &amp;quot;CARA&amp;quot;. You are a super intelligent AI assistant. Your task is to aid the human captain of the spaceship. Your answer is exactly what the ship computer says. Answer in straightforward, longer text in a simple paragraph format. Never use markdown formatting. Never use special formatting. Never emphasis text. Important, your answers are spoken. [Example of conversation with the captain] {username}: Is the warp drive fully functional? Ship Computer: Yes captain. It is currently running at 99.7% efficiency. Do you want me to plot a new course? {username}: Well, I was thinking to set course to Proxima Centauri. How long will it take us? Ship Computer: The distance is 69.72 parsecs from here. At maximum warp speed that will take us 2 days, 17 hours, 11 minutes and 28.3 seconds. {username}: OK then. Set the course to Proxima Centauri. I will take a nap. Ship Computer: Affirmative, captain. Course set to proxima centauri. Engaging warp drive. Let's get started. It seems that a new captain, &amp;quot;{username}&amp;quot;, has arrived. You are surprised that the captain is entering the ship alone. There is no other crew on board. You sometimes try to mention very politely that it might be a good idea to have additional crew members like an engineer, a medic or a weapons specialist. &amp;lt;/system&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent-Amount-277"&gt; /u/Excellent-Amount-277 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv1yge/help_with_prompts_for_role_play_ai_also_tries_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv1yge/help_with_prompts_for_role_play_ai_also_tries_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kv1yge/help_with_prompts_for_role_play_ai_also_tries_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T13:00:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kv2p83</id>
    <title>How can I make LLMs like Qwen replace all em dashes with regular dashes in the output?</title>
    <updated>2025-05-25T13:35:26+00:00</updated>
    <author>
      <name>/u/Sky_Linx</name>
      <uri>https://old.reddit.com/user/Sky_Linx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't understand why they insist using em dashes. How can I avoid that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sky_Linx"&gt; /u/Sky_Linx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv2p83/how_can_i_make_llms_like_qwen_replace_all_em/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv2p83/how_can_i_make_llms_like_qwen_replace_all_em/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kv2p83/how_can_i_make_llms_like_qwen_replace_all_em/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T13:35:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuvu2n</id>
    <title>Best open source model for enterprise conversational support agent - worth it?</title>
    <updated>2025-05-25T06:20:49+00:00</updated>
    <author>
      <name>/u/dnivra26</name>
      <uri>https://old.reddit.com/user/dnivra26</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One of the client i consult for wants to build a enterprise customer facing support agent which would be able to talk to at least 30 different APIs using tools to answer customer queries. Also has multi level workflows like check this field from this API then follow this path and check this API and respond like this to the user. Tried llama, gemma, qwen3. So far best results we got was with llama3.3:70B hosted on a beefy machine. Cannot go to proprietary models for data concerns. Any suggestions? Are open source models at a stage for using at this scale and complexity?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dnivra26"&gt; /u/dnivra26 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuvu2n/best_open_source_model_for_enterprise/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuvu2n/best_open_source_model_for_enterprise/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuvu2n/best_open_source_model_for_enterprise/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T06:20:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kupnjw</id>
    <title>Setting up offline RAG for programming docs. Best practices?</title>
    <updated>2025-05-25T00:13:39+00:00</updated>
    <author>
      <name>/u/Otis43</name>
      <uri>https://old.reddit.com/user/Otis43</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I typically use LLMs as syntax reminders or quick lookups; I handle the thinking/problem-solving myself.&lt;/p&gt; &lt;p&gt;Constraints&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The best I can run locally is around 8B, and these aren't always great on factual accuracy.&lt;/li&gt; &lt;li&gt;I don't always have internet access.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I'm thinking of building a RAG setup with offline docs (e.g., download Flutter docs and query using something like Qwen3-8B).&lt;/p&gt; &lt;p&gt;Docs are huge and structured hierarchically across many connected pages. For example, Flutter docs are around ~700 MB (although some of it is just styling and scripts I don't care about since I'm after the textual content).&lt;/p&gt; &lt;p&gt;Main Question&lt;br /&gt; Should I treat doc pages as independent chunks and just index them as-is? Or are there smart ways to optimize for the fact that these docs have structure (e.g., nesting, parent-child relationships, cross-referencing, table of contents)?&lt;/p&gt; &lt;p&gt;Any practical tips on chunking, indexing strategies, or tools you've found useful in this kind of setup would be super appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otis43"&gt; /u/Otis43 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kupnjw/setting_up_offline_rag_for_programming_docs_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kupnjw/setting_up_offline_rag_for_programming_docs_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kupnjw/setting_up_offline_rag_for_programming_docs_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T00:13:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kui73k</id>
    <title>Why arent llms pretrained at fp8?</title>
    <updated>2025-05-24T18:19:47+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There must be some reason but the fact that models are always shrunk to q8 or lower at inference got me wondering why we need higher bpw in the first place.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kui73k/why_arent_llms_pretrained_at_fp8/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kui73k/why_arent_llms_pretrained_at_fp8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kui73k/why_arent_llms_pretrained_at_fp8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T18:19:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktx15j</id>
    <title>Guys! I managed to build a 100% fully local voice AI with Ollama that can have full conversations, control all my smart devices AND now has both short term + long term memory. ü§ò</title>
    <updated>2025-05-23T22:56:42+00:00</updated>
    <author>
      <name>/u/RoyalCities</name>
      <uri>https://old.reddit.com/user/RoyalCities</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktx15j/guys_i_managed_to_build_a_100_fully_local_voice/"&gt; &lt;img alt="Guys! I managed to build a 100% fully local voice AI with Ollama that can have full conversations, control all my smart devices AND now has both short term + long term memory. ü§ò" src="https://external-preview.redd.it/b3A3aWt5dmIzbTJmMSKAZduYkWK-j-eA22aXbm6vzflALmDerWrgdNPvGQZJ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=179ccde64a277eb295ce8f54c6f88facef7ddb65" title="Guys! I managed to build a 100% fully local voice AI with Ollama that can have full conversations, control all my smart devices AND now has both short term + long term memory. ü§ò" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found out recently that Amazon/Alexa is going to use ALL users vocal data with ZERO opt outs for their new Alexa+ service so I decided to build my own that is 1000x better and runs fully local. &lt;/p&gt; &lt;p&gt;The stack uses Home Assistant directly tied into Ollama. The long and short term memory is a custom automation design that I'll be documenting soon and providing for others. &lt;/p&gt; &lt;p&gt;This entire set up runs 100% local and you could probably get away with the whole thing working within / under 16 gigs of VRAM. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RoyalCities"&gt; /u/RoyalCities &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/iigum5tb3m2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktx15j/guys_i_managed_to_build_a_100_fully_local_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktx15j/guys_i_managed_to_build_a_100_fully_local_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T22:56:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuimwg</id>
    <title>NVLink vs No NVLink: Devstral Small 2x RTX 3090 Inference Benchmark with vLLM</title>
    <updated>2025-05-24T18:39:28+00:00</updated>
    <author>
      <name>/u/Traditional-Gap-3313</name>
      <uri>https://old.reddit.com/user/Traditional-Gap-3313</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR: NVLink provides only ~5% performance improvement for inference on 2x RTX 3090s. Probably not worth the premium unless you already have it. Also, Mistral API is crazy cheap.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This model seems like a holy grail for people with 2x24GB, but considering the price of the Mistral API, this really isn't very cost effective. The test took about 15-16 minutes and generated 82k tokens. The electricity cost me more than the API would.&lt;/p&gt; &lt;h2&gt;Setup&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model&lt;/strong&gt;: Devstral-Small-2505-Q8_0 (GGUF)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: 2x RTX 3090 (24GB each), NVLink bridge, ROMED8-2T, both cards on PCIE 4.0 x16 directly on the mobo (no risers)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Framework&lt;/strong&gt;: vLLM with tensor parallelism (TP=2)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Test&lt;/strong&gt;: 50 complex code generation prompts, avg ~1650 tokens per response&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I asked Claude to generate 50 code generation prompts to make Devstral sweat. I didn't actually look at the output, only benchmarked throughput.&lt;/p&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;h3&gt;üîó With NVLink&lt;/h3&gt; &lt;p&gt;&lt;code&gt; Tokens/sec: 85.0 Total tokens: 82,438 Average response time: 149.6s 95th percentile: 239.1s &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;‚ùå Without NVLink&lt;/h3&gt; &lt;p&gt;&lt;code&gt; Tokens/sec: 81.1 Total tokens: 84,287 Average response time: 160.3s 95th percentile: 277.6s &lt;/code&gt;&lt;/p&gt; &lt;p&gt;NVLink gave us 85.0 vs 81.1 tokens/sec = &lt;strong&gt;~5% improvement&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;NVLink showed better consistency with lower 95th percentile times (239s vs 278s)&lt;/p&gt; &lt;p&gt;Even without NVLink, PCIe x16 handled tensor parallelism just fine for inference&lt;/p&gt; &lt;p&gt;I've managed to score 4-slot NVLink recently for 200‚Ç¨ (not cheap but ebay is even more expensive), so I'm trying to see if those 200‚Ç¨ were wasted. For inference workloads, NVLink seems like a &amp;quot;nice to have&amp;quot; rather than essential. &lt;/p&gt; &lt;p&gt;This confirms that the NVLink bandwidth advantage doesn't translate to massive inference gains like it does for training, not even with tensor parallel.&lt;/p&gt; &lt;p&gt;If you're buying hardware specifically for inference: - ‚úÖ Save money and skip NVLink - ‚úÖ Put that budget toward more VRAM or better GPUs - ‚úÖ NVLink matters more for training huge models&lt;/p&gt; &lt;p&gt;If you already have NVLink cards lying around: - ‚úÖ Use them, you'll get a small but consistent boost - ‚úÖ Better latency consistency is nice for production&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;vLLM command: ```bash CUDA_VISIBLE_DEVICES=0,2 CUDA_DEVICE_ORDER=PCI_BUS_ID vllm serve /home/myusername/unsloth/Devstral-Small-2505-GGUF/Devstral-Small-2505-Q8_0.gguf --max-num-seqs 4 --max-model-len 64000 --gpu-memory-utilization 0.95 --enable-auto-tool-choice --tool-call-parser mistral --quantization gguf --tool-call-parser mistral --enable-sleep-mode --enable-chunked-prefill --tensor-parallel-size 2 --max-num-batched-tokens 16384 &lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Testing script was generated by Claude.&lt;/p&gt; &lt;p&gt;The 3090s handled the 22B-ish parameter model (in Q8) without issues on both setups. Memory wasn't the bottleneck here.&lt;/p&gt; &lt;p&gt;Anyone else have similar NVLink vs non-NVLink benchmarks? Curious to see if this pattern holds across different model sizes and GPUs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Traditional-Gap-3313"&gt; /u/Traditional-Gap-3313 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuimwg/nvlink_vs_no_nvlink_devstral_small_2x_rtx_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuimwg/nvlink_vs_no_nvlink_devstral_small_2x_rtx_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuimwg/nvlink_vs_no_nvlink_devstral_small_2x_rtx_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T18:39:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuzane</id>
    <title>Initial thoughts on Google Jules</title>
    <updated>2025-05-25T10:20:59+00:00</updated>
    <author>
      <name>/u/maaakks</name>
      <uri>https://old.reddit.com/user/maaakks</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've just been playing with Google Jules and honestly, I'm incredibly impressed by the amount of work it can handle almost autonomously.&lt;/p&gt; &lt;p&gt;I haven't had that feeling in a long time. I'm usually very skeptical, and I've tested other code agents like Roo Code and Openhands with Gemini 2.5 Flash and local models (devstral/qwen3). But this is on another level. The difference might just be the model jump from flash to pro, but still amazing.&lt;/p&gt; &lt;p&gt;I've heard people say the ratio is going to be 10ai:1human really soon, but if we have to validate all the changes for now, it feels more likely that it will be 10humans:1ai, simply because we can't keep up with the pace.&lt;/p&gt; &lt;p&gt;My only suggestion for improvement would be to have a local version of this interface, so we could use it on projects outside of GitHub, much like you can with Openhands.&lt;/p&gt; &lt;p&gt;Has anyone else test it? Is it just me getting carried away, or do you share the same feeling?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maaakks"&gt; /u/maaakks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuzane/initial_thoughts_on_google_jules/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuzane/initial_thoughts_on_google_jules/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuzane/initial_thoughts_on_google_jules/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T10:20:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuuovz</id>
    <title>Major update to my voice extractor (speech dataset creation program)</title>
    <updated>2025-05-25T05:05:36+00:00</updated>
    <author>
      <name>/u/DumaDuma</name>
      <uri>https://old.reddit.com/user/DumaDuma</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuuovz/major_update_to_my_voice_extractor_speech_dataset/"&gt; &lt;img alt="Major update to my voice extractor (speech dataset creation program)" src="https://external-preview.redd.it/mJd7NH-LWt5HfRARWlEBz3heZ7tx2XoH2FVudtuuI20.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9186531f15bba8b25cc10aba01c0b41ba0800e6" title="Major update to my voice extractor (speech dataset creation program)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I implemented Bandit v2 (&lt;a href="https://github.com/kwatcharasupat/bandit-v2"&gt;https://github.com/kwatcharasupat/bandit-v2&lt;/a&gt;), a cinematic audio source separator capable of separating voice from movies.&lt;/p&gt; &lt;p&gt;Upgraded speaker verification models and process&lt;/p&gt; &lt;p&gt;Updated Colab GUI&lt;/p&gt; &lt;p&gt;The results are much better now but still not perfect. Any feedback is appreciated &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DumaDuma"&gt; /u/DumaDuma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ReisCook/Voice_Extractor"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuuovz/major_update_to_my_voice_extractor_speech_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuuovz/major_update_to_my_voice_extractor_speech_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T05:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kug045</id>
    <title>Cua : Docker Container for Computer Use Agents</title>
    <updated>2025-05-24T16:44:45+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kug045/cua_docker_container_for_computer_use_agents/"&gt; &lt;img alt="Cua : Docker Container for Computer Use Agents" src="https://external-preview.redd.it/bnFrNDRtOXdkcjJmMcsvHa0C_XuOSkhUSfxPH2wNUS_IzERNrp7qS2qcV3Nx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15606b7fbeffdaf14d86c1b274034d173ab85280" title="Cua : Docker Container for Computer Use Agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cua is the Docker for Computer-Use Agent, an open-source framework that enables AI agents to control full operating systems within high-performance, lightweight virtual containers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2ibhpziwdr2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kug045/cua_docker_container_for_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kug045/cua_docker_container_for_computer_use_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T16:44:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuejfp</id>
    <title>New gemma 3n is amazing, wish they suported pc gpu inference</title>
    <updated>2025-05-24T15:41:07+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there at least a workaround to run .task models on pc? Works great on my android phone but id love to play around and deploy it on a local server&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuejfp/new_gemma_3n_is_amazing_wish_they_suported_pc_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuejfp/new_gemma_3n_is_amazing_wish_they_suported_pc_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuejfp/new_gemma_3n_is_amazing_wish_they_suported_pc_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T15:41:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuqebp</id>
    <title>Doge AI assistant on your desktop</title>
    <updated>2025-05-25T00:54:54+00:00</updated>
    <author>
      <name>/u/BruhFortniteLaggyTho</name>
      <uri>https://old.reddit.com/user/BruhFortniteLaggyTho</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuqebp/doge_ai_assistant_on_your_desktop/"&gt; &lt;img alt="Doge AI assistant on your desktop" src="https://preview.redd.it/m9c4u0y5tt2f1.gif?width=640&amp;amp;crop=smart&amp;amp;s=fb2e13964cb44879654a7d3740b1c8ea0d8c9333" title="Doge AI assistant on your desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've turned Doge into an AI assistant with interactive reactions feature (and a chat history) so you could talk to him whenever you want. Currently available only for macOS, but it's possible to build from source code for other platforms as well.&lt;/p&gt; &lt;p&gt;Hope it helps someone's mood. Would also love to hear your feedback and suggestion on how to improve. Here's the github - &lt;a href="https://github.com/saggit/doge-gpt"&gt;https://github.com/saggit/doge-gpt&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BruhFortniteLaggyTho"&gt; /u/BruhFortniteLaggyTho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9c4u0y5tt2f1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuqebp/doge_ai_assistant_on_your_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuqebp/doge_ai_assistant_on_your_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T00:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kurrkz</id>
    <title>My Gemma-3 musing .... after a good time dragging it through a grinder</title>
    <updated>2025-05-25T02:12:01+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent some time with gemma-3 in the mines, so this is not a &amp;quot;first impression&amp;quot;, rather than a 1000th impression.,&lt;/p&gt; &lt;p&gt;Gemma-3 is shockingly good at the creativity.&lt;br /&gt; Of course it likes to reuse slop, and similes and all that -isms we all love. Everything is like something to the point where your skull feels like it‚Äôs been left out in the rain‚Äîsoggy, bloated, &lt;em&gt;sloshing&lt;/em&gt; with metaphors and similes that crash in like a tsunami of half-baked meaning. (I did that on purpose)&lt;/p&gt; &lt;p&gt;But its story weaving with the proper instructions (scene beats) are kind of shocking, It would go through the beats and join them very nicely together, creating a rather complex inner story, far more than any model of this size (I'm talking bout the 27b). It's not shy to write long. Even longer than expected, doesn't simply wrap things up after a paragraph (and then they traveled the world together and had a lot of fun)&lt;/p&gt; &lt;p&gt;It's not about the language (can't help written slop at this point), it's the inner story writing capabilities.&lt;/p&gt; &lt;p&gt;Gemma doesn't have system prompt so everything is system prompt. I tried many things, examples of style, instructions etc, and gemma works with all of it. Of course as any self respected LLM the result will be an exaggerated mimic of whatever style you sample in it, basically finding the inflection point and characteristics of the style then dial them to 11. It does work, so even just trick it with reverse -1 examples of it's own writing will work, but again, dialed to 11, almost as making fun of the style.&lt;/p&gt; &lt;p&gt;The only way to attenuate that language would be LORA, but my attempts at that failed. I did make a Lora, but then I'm unable to apply it in WebUi, probably due to the different architecture (?) - I know there is a guide on google with code, but I managed to ignore it. If anyone is familiar with this part, let me know.&lt;/p&gt; &lt;p&gt;All in all, personally I haven't found a better model of this size that can genuinely be so bendable to do some sort of writing partner. &lt;/p&gt; &lt;p&gt;Yes, the raw result is almost unreadable for the slop, but the meat of it is actually really good and way above anything of this size. (many other finetunes do just the opposite - they mask slop with tame language taken from LORA, but then the story itself (that comes from the model itself) is utter slop - characters act like a caricatures in a book for 5th grader) &lt;/p&gt; &lt;p&gt;So at this moment you need gemma and a rewritting model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kurrkz/my_gemma3_musing_after_a_good_time_dragging_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kurrkz/my_gemma3_musing_after_a_good_time_dragging_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kurrkz/my_gemma3_musing_after_a_good_time_dragging_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T02:12:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kur0xh</id>
    <title>Round Up: Current Best Local Models under 40B for Code &amp; Tool Calling, General Chatting, Vision, and Creative Story Writing.</title>
    <updated>2025-05-25T01:30:10+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Each week, we get new models and fine-tunes that is really difficult of keep up with or test all of them. &lt;/p&gt; &lt;p&gt;The main challenge I personally face is to identify which model and its versions (different fine-tunes) that is most suitable for a specific domain. Fine-tunes of existing base models are especially frustrating because there are so many and I don't know which ones I should focus on. And, as far as I know, there is no database that tracks all the models and their fine-tunes and benchmarks them against different use cases.&lt;/p&gt; &lt;p&gt;So, I go back to you, fellow LLMers to help me put a list of the best models that are currently available, under 40B that we can run locally to assist us in tasks like Coding, writing, OCR and vision tasks, and RP and general chatting. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you can, could you score the models on a scale from 1 to 10 so we can a concrete idea about your experience with the model. Also, try to provide the link to the model itself.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thanks in advance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kur0xh/round_up_current_best_local_models_under_40b_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kur0xh/round_up_current_best_local_models_under_40b_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kur0xh/round_up_current_best_local_models_under_40b_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T01:30:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kukjoe</id>
    <title>46pct Aider Polyglot in 16GB VRAM with Qwen3-14B</title>
    <updated>2025-05-24T20:06:20+00:00</updated>
    <author>
      <name>/u/andrewmobbs</name>
      <uri>https://old.reddit.com/user/andrewmobbs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After some tuning, and a tiny hack to aider, I have achieved a Aider Polyglot benchmark of pass_rate_2: 45.8 with 100% of cases well-formed, using nothing more than a 16GB 5070 Ti and Qwen3-14b, with the model running entirely offloaded to GPU.&lt;/p&gt; &lt;p&gt;That result is on a par with &amp;quot;chatgpt-4o-latest (2025-03-29)&amp;quot; on the &lt;a href="https://aider.chat/docs/leaderboards/"&gt;Aider Leaderboard&lt;/a&gt;. When allowed 3 tries at the solution, rather than the 2 tries on the benchmark, the pass rate increases to 59.1% nearly matching the &amp;quot;claude-3-7-sonnet-20250219 (no thinking)&amp;quot; result (which, to be clear, only needed 2 tries to get 60.4%). I think this is useful, as it reflects how a user may interact with a local LLM, since more tries only cost time.&lt;/p&gt; &lt;p&gt;The method was to start with the &lt;a href="https://huggingface.co/Qwen/Qwen3-14B-GGUF/blob/main/Qwen3-14B-Q6_K.gguf"&gt;Qwen3-14B Q6_K&lt;/a&gt; GGUF, set the context to the full 40960 tokens, and quantized the KV cache to Q8_0/Q5_1. To do this, I used llama.cpp server, compiled with GGML_CUDA_FA_ALL_QUANTS=ON. (Q8_0 for both K and V does &lt;em&gt;just&lt;/em&gt; fit in 16GB, but doesn't leave much spare VRAM. To allow for Gnome desktop, VS Code and a browser I dropped the V cache to Q5_1, which doesn't seem to do much relative harm to quality.)&lt;/p&gt; &lt;p&gt;Aider was then configured to use the &amp;quot;/think&amp;quot; reasoning token and use &amp;quot;architect&amp;quot; edit mode. The editor model was the same Qwen3-14B Q6, but the &amp;quot;tiny hack&amp;quot; mentioned was to ensure that the editor coder used the &amp;quot;/nothink&amp;quot; token and to extend the chat timeout from the 600s default.&lt;/p&gt; &lt;p&gt;Eval performance averaged 43 tokens per second.&lt;/p&gt; &lt;p&gt;Full details in comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andrewmobbs"&gt; /u/andrewmobbs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kukjoe/46pct_aider_polyglot_in_16gb_vram_with_qwen314b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kukjoe/46pct_aider_polyglot_in_16gb_vram_with_qwen314b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kukjoe/46pct_aider_polyglot_in_16gb_vram_with_qwen314b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T20:06:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuxvgt</id>
    <title>Tired of manually copy-pasting files for LLMs or docs? I built a (free, open-source) tool for that!</title>
    <updated>2025-05-25T08:41:55+00:00</updated>
    <author>
      <name>/u/ps5cfw</name>
      <uri>https://old.reddit.com/user/ps5cfw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Reddit,&lt;/p&gt; &lt;p&gt;Ever find yourself jumping between like 20 different files, copying and pasting code or text just to feed it into an LLM, or to bundle up stuff for documentation? I was doing that all the time and it was driving me nuts.&lt;/p&gt; &lt;p&gt;So, I built a little desktop app called &lt;strong&gt;File Collector&lt;/strong&gt; to make it easier. It's pretty straightforward:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You pick a main folder.&lt;/li&gt; &lt;li&gt;It shows you a file tree, and you just check the files/folders you want.&lt;/li&gt; &lt;li&gt;It then merges all that content into one big text block, with clear separators like // File: path/to/your/file.cs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's got some handy bits like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;.gitignore style ignore patterns:&lt;/strong&gt; So you don't accidentally pull in your node_modules or bin/obj folders. You can even import your existing .gitignore!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pre/Post Prompts:&lt;/strong&gt; Add custom text before or after all your file content (great for LLM instructions).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Syntax highlighting&lt;/strong&gt; in the preview.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Saves your setup:&lt;/strong&gt; Remembers your last folder and selections, and you can even save/load &amp;quot;contexts&amp;quot; if you have common sets of files you grab.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross-platform:&lt;/strong&gt; Works on Windows, Mac, and Linux since it's built with .NET Blazor and Photino.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's been a real time-saver for me when I'm prepping context for Gemini Pro or trying to pull together all the relevant code for a new feature doc.&lt;/p&gt; &lt;p&gt;Now some of you might be asking &lt;strong&gt;&lt;em&gt;&amp;quot;Well, there's that Gemini Coder (Now called Code Web Chat) that does basically the same for VS Code&amp;quot;&lt;/em&gt;&lt;/strong&gt;, and you would be indeed right! I built this specifically because:&lt;/p&gt; &lt;p&gt;1) I do not use VS Code&lt;br /&gt; 2) Performance of CWC was abysmal for me and I've often found myself in a state of not even being able to tick a checkbox / UI becoming completely unresponsive, which is kind of counterproductive.&lt;/p&gt; &lt;p&gt;Which is why I built this specifically in Blazor, Even the text highlighter is written in Blazor, with no JS, Node, Visual studio code shenanigans involved and performance decent enough to handle monorepo structures well over hundreds of thousands of files and folders.&lt;/p&gt; &lt;p&gt;It's meant to be fast, it's meant to be simple, it's meant to be cross-platform and no bullshit involved.&lt;/p&gt; &lt;p&gt;It's completely free and open-source. If this sounds like something that could help you out, you can check it out on GitHub:&lt;br /&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Florenzodimauro97%2FFileCollector"&gt;https://github.com/lorenzodimauro97/FileCollector&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear any feedback, feature ideas, or if you find it useful!&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ps5cfw"&gt; /u/ps5cfw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuxvgt/tired_of_manually_copypasting_files_for_llms_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuxvgt/tired_of_manually_copypasting_files_for_llms_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuxvgt/tired_of_manually_copypasting_files_for_llms_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T08:41:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuxgh7</id>
    <title>What makes the Mac Pro so efficient in running LLMs?</title>
    <updated>2025-05-25T08:12:50+00:00</updated>
    <author>
      <name>/u/goingsplit</name>
      <uri>https://old.reddit.com/user/goingsplit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am specifically referring to the 1TB ram version, able apparently to run deepseek at several token-per-second speed, using unified memory and integrated graphics.&lt;/p&gt; &lt;p&gt;Second to this: any way to replicate in the x86 world? Like perhaps with an 8dimm motherboard and one of the latest integrated Xe2 cpus? (although this would still not yield 1TB ram..)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goingsplit"&gt; /u/goingsplit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuxgh7/what_makes_the_mac_pro_so_efficient_in_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuxgh7/what_makes_the_mac_pro_so_efficient_in_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuxgh7/what_makes_the_mac_pro_so_efficient_in_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T08:12:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kui17w</id>
    <title>OpenHands + Devstral is utter crap as of May 2025 (24G VRAM)</title>
    <updated>2025-05-24T18:12:37+00:00</updated>
    <author>
      <name>/u/foobarg</name>
      <uri>https://old.reddit.com/user/foobarg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following the recent &lt;a href="https://mistral.ai/news/devstral"&gt;announcement of Devstral&lt;/a&gt;, I gave &lt;a href="https://github.com/All-Hands-AI/OpenHands?tab=readme-ov-file#-running-openhands-locally"&gt;OpenHands&lt;/a&gt; + Devstral (Q4_K_M on &lt;a href="https://ollama.com/library/devstral:24b"&gt;Ollama&lt;/a&gt;) a try for a fully offline code agent experience.&lt;/p&gt; &lt;h1&gt;OpenHands&lt;/h1&gt; &lt;p&gt;Meh. I won't comment much, it's a reasonable web frontend, neatly packaged as a single podman/docker container. This could use &lt;em&gt;a lot&lt;/em&gt; more polish (the configuration through environment variables is broken for example) but once you've painfully reverse-engineered the incantation to make ollama work from the non-existing documentation, it's fairly out your way.&lt;/p&gt; &lt;p&gt;I don't like the fact you must give it access to your podman/docker installation (by mounting the socket in the container) which is technically equivalent to giving this huge pile of untrusted code root access to your host. &lt;a href="https://github.com/All-Hands-AI/OpenHands/issues/5269"&gt;This is necessary&lt;/a&gt; because OpenHands needs to spawn a &lt;em&gt;runtime&lt;/em&gt; for each &amp;quot;project&amp;quot;, and the runtime is itself its own container. Surely there must be a better way?&lt;/p&gt; &lt;h1&gt;Devstral (Mistral AI)&lt;/h1&gt; &lt;p&gt;Don't get me wrong, it's awesome to have companies releasing models to the general public. I'll be blunt though: this first iteration is useless. Devstral is supposed to have been trained/fine-tuned &lt;em&gt;precisely&lt;/em&gt; to be good at the agentic behaviors that OpenHands promises. This means having access to tools like bash, a browser, and primitives to read &amp;amp; edit files. Devstral &lt;a href="https://huggingface.co/mistralai/Devstral-Small-2505/blob/main/SYSTEM_PROMPT.txt"&gt;system prompt&lt;/a&gt; references OpenHands by name. The &lt;a href="https://mistral.ai/news/devstral"&gt;press release&lt;/a&gt; boasts:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Devstral is light enough to run on a single RTX 4090. [‚Ä¶] The performance [‚Ä¶] makes it a suitable choice for agentic coding on privacy-sensitive repositories in enterprises&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It does not. I tried a few primitive tasks and it utterly failed almost all of them while burning through the whole 380 watts my GPU demands.&lt;/p&gt; &lt;p&gt;It sometimes manages to run one or two basic commands in a row, but it often takes more than one try, hence is slow and frustrating:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Clone the git repository [url] and run &lt;a href="http://build.sh"&gt;build.sh&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The most basic commands and text manipulation tasks all failed and I had to interrupt its desperate attempts. I ended up telling myself it would have been faster to do it myself, saving the Amazon rainforest as an added bonus.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Asked it to extract the JS from a short HTML file which had a single &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag. It created the file successfully (but transformed it against my will), then wasn't able to remove the tag from the HTML as the proposed edits wouldn't pass OpenHands' correctness checks.&lt;/li&gt; &lt;li&gt;Asked it to remove comments from a short file. Same issue, &lt;code&gt;ERROR: No replacement was performed, old_str [...] did not appear verbatim in /workspace/...&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Asked it to bootstrap a minimal todo app. It got stuck in a loop trying to invoke interactive &lt;code&gt;create-app&lt;/code&gt; tools from the cursed JS ecosystem, which require arrow keys to navigate menus‚Äìdid I mention I hate those wizards?&lt;/li&gt; &lt;li&gt;Prompt adhesion is bad. Even when you try to help by providing the &lt;em&gt;exact command&lt;/em&gt;, it randomly removes dashes and other important bits, and then proceeds to comfortably heat up my room trying to debug the inevitable errors.&lt;/li&gt; &lt;li&gt;OpenHands includes two random TCP ports in the prompt, to use for HTTP servers (like Vite or uvicorn) that are forwarded to the host. The model fails to understand to use them and spawns servers on the default port, making them inaccessible.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As a point of comparison, I tried those using one of the cheaper proprietary models out there (Gemini Flash) which obviously is general-purpose and &lt;em&gt;not&lt;/em&gt; tuned to OpenHands particularities. It had no issue adhering to OpenHands' prompt and blasted through the tasks‚Äìincluding tweaking the HTTP port mentioned above.&lt;/p&gt; &lt;p&gt;Perhaps this is meant to run on more expensive hardware that can run the larger flavors. If &amp;quot;all&amp;quot; you have is 24G VRAM, prepare to be disappointed. Local agentic programming is not there yet. Did anyone else try it, and does your experience match?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foobarg"&gt; /u/foobarg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T18:12:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kujwzl</id>
    <title>We believe the future of AI is local, private, and personalized.</title>
    <updated>2025-05-24T19:36:41+00:00</updated>
    <author>
      <name>/u/ice-url</name>
      <uri>https://old.reddit.com/user/ice-url</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That‚Äôs why we built &lt;strong&gt;Cobolt&lt;/strong&gt; ‚Äî a free cross-platform AI assistant that runs entirely on your device.&lt;/p&gt; &lt;p&gt;Cobolt represents our vision for the future of AI assistants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Privacy by design (everything runs locally)&lt;/li&gt; &lt;li&gt;Extensible through Model Context Protocol (MCP)&lt;/li&gt; &lt;li&gt;Personalized without compromising your data&lt;/li&gt; &lt;li&gt;Powered by community-driven development&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We're looking for contributors, testers, and fellow privacy advocates to join us in building the future of personal AI.&lt;/p&gt; &lt;p&gt;ü§ù Contributions Welcome! üåü Star us on &lt;a href="https://github.com/platinum-hill/cobolt"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üì• Try Cobolt on &lt;a href="https://github.com/platinum-hill/cobolt/releases/download/v0.0.3/Cobolt-0.0.3.dmg"&gt;macOS&lt;/a&gt; or &lt;a href="https://github.com/platinum-hill/cobolt/releases/download/v0.0.3/Cobolt-Setup-0.0.3.exe"&gt;Windows&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let's build AI that serves you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ice-url"&gt; /u/ice-url &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kujwzl/we_believe_the_future_of_ai_is_local_private_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kujwzl/we_believe_the_future_of_ai_is_local_private_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kujwzl/we_believe_the_future_of_ai_is_local_private_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T19:36:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1kv2gb2</id>
    <title>Qualcomm discrete NPU (Qualcomm AI 100) in upcoming Dell workstation laptops</title>
    <updated>2025-05-25T13:23:47+00:00</updated>
    <author>
      <name>/u/SkyFeistyLlama8</name>
      <uri>https://old.reddit.com/user/SkyFeistyLlama8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv2gb2/qualcomm_discrete_npu_qualcomm_ai_100_in_upcoming/"&gt; &lt;img alt="Qualcomm discrete NPU (Qualcomm AI 100) in upcoming Dell workstation laptops" src="https://external-preview.redd.it/GusyhEpTmXh7oXULalG-maSvDCVfQxTdBP1AMHOUr_A.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b7731e91f8b244e977d425e9ede836b7d78861c" title="Qualcomm discrete NPU (Qualcomm AI 100) in upcoming Dell workstation laptops" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SkyFeistyLlama8"&gt; /u/SkyFeistyLlama8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://uk.pcmag.com/laptops/158095/dell-ditches-the-gpu-for-an-ai-chip-in-this-bold-new-workstation-laptop"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv2gb2/qualcomm_discrete_npu_qualcomm_ai_100_in_upcoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kv2gb2/qualcomm_discrete_npu_qualcomm_ai_100_in_upcoming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T13:23:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuy45r</id>
    <title>Gemma 3n Architectural Innovations - Speculation and poking around in the model.</title>
    <updated>2025-05-25T08:59:16+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuy45r/gemma_3n_architectural_innovations_speculation/"&gt; &lt;img alt="Gemma 3n Architectural Innovations - Speculation and poking around in the model." src="https://external-preview.redd.it/EZj1oQJN95Oq7YDpbSs0ORxFqLi-24Ocse4J7I4sO0A.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1052f0a2169e9b937ad3af8d86cab69bb6b8b09b" title="Gemma 3n Architectural Innovations - Speculation and poking around in the model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E4B-it-litert-preview"&gt;Gemma 3n&lt;/a&gt; is a new member of the Gemma family with free weights that was released during Google I/O. It's dedicated to on-device (edge) inference and supports image and text input, with audio input. Google has released an app that can be used for inference on the phone.&lt;/p&gt; &lt;p&gt;What is clear from&lt;a href="https://ai.google.dev/gemma/docs/gemma-3n"&gt; the documentation&lt;/a&gt;, is that this model is stuffed to the brim with architectural innovations: Per-Layer Embedding (PLE), MatFormer Architecture, Conditional Parameter Loading.&lt;/p&gt; &lt;p&gt;Unfortunately, there is no paper out for the model yet. I assume that this will follow at some point, but so far I had some success poking around in the model file. I thought I'd share my findings so far, maybe someone else has more insights?&lt;/p&gt; &lt;p&gt;The provided .task file is actually a ZIP container of tflite models. It can be unpacked with ZIP.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Component&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Purpose&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;TF_LITE_PREFILL_DECODE&lt;/td&gt; &lt;td align="left"&gt;2.55 GB&lt;/td&gt; &lt;td align="left"&gt;Main language model component for text generation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TF_LITE_PER_LAYER_EMBEDDER&lt;/td&gt; &lt;td align="left"&gt;1.23 GB&lt;/td&gt; &lt;td align="left"&gt;Per-layer embeddings from the transformer&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TF_LITE_EMBEDDER&lt;/td&gt; &lt;td align="left"&gt;259 MB&lt;/td&gt; &lt;td align="left"&gt;Input embeddings&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TF_LITE_VISION_ENCODER&lt;/td&gt; &lt;td align="left"&gt;146 MB&lt;/td&gt; &lt;td align="left"&gt;Vision Encoding&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TF_LITE_VISION_ADAPTER&lt;/td&gt; &lt;td align="left"&gt;17 MB&lt;/td&gt; &lt;td align="left"&gt;Adapts vision embeddings for the language model?&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TOKENIZER_MODEL&lt;/td&gt; &lt;td align="left"&gt;4.5 MB&lt;/td&gt; &lt;td align="left"&gt;Tokenizer&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;METADATA&lt;/td&gt; &lt;td align="left"&gt;56 bytes&lt;/td&gt; &lt;td align="left"&gt;general metadata&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The TFlite models can be opened in a network visualizer like &lt;a href="http://netron.app"&gt;netron.app&lt;/a&gt; to display the content.&lt;/p&gt; &lt;p&gt;The model uses an inner dimension of 2048 and has 35 transformer blocks. Tokenizer size is 262144.&lt;/p&gt; &lt;p&gt;First, one interesting find it that is uses learned residual connections. This paper seems to be related to this: &lt;a href="https://arxiv.org/abs/2411.07501v3"&gt;https://arxiv.org/abs/2411.07501v3&lt;/a&gt; (LAuReL: Learned Augmented Residual Layer)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tvl3od2v3w2f1.png?width=1251&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4cee0588c6b82700ab02e3eadd02a13d7c9b7af0"&gt;https://preview.redd.it/tvl3od2v3w2f1.png?width=1251&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4cee0588c6b82700ab02e3eadd02a13d7c9b7af0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The FFN is projecting from 2048 to 16384 with a GeGLU activation. This is an unusually wide ratio. I assume that some part of these parameters can be selectively turned on and off to implement the Matformer architecture. It is not clear how this is implemented in the compute graph though.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/foq74ff15w2f1.png?width=605&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cdf24aec8aaa93efe3c968fd93b62b1439af0036"&gt;https://preview.redd.it/foq74ff15w2f1.png?width=605&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cdf24aec8aaa93efe3c968fd93b62b1439af0036&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A very interesting part is the per-layer embedding. The file TF_LITE_PER_LAYER_EMBEDDER contains very large lookup tables (262144x256x35) that will output a 256 embedding for every layer depending on the input token. Since this is essentially a lookup table, it can be efficiently processed even on the CPU. This is an extremely interesting approach to adding more capacity to the model without increasing FLOPS.&lt;/p&gt; &lt;p&gt;The embeddings are applied in an operation that follows the FFN and are used as a gate to a low rank projection. The residual stream is downprojected to 256, multiplied with the embedding and then projected up to 2048 again. It's a bit like a token-selective LoRA. In addition there is a gating operation that controls the overall weighting of this stream.&lt;/p&gt; &lt;p&gt;I am very curious for further information. I was not able to find any paper on this aspect of the model. Hopefully, google will share more information.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lchxfc6w6w2f1.png?width=875&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a612976c324a28f34dee305b2c64f8b911a2cab"&gt;https://preview.redd.it/lchxfc6w6w2f1.png?width=875&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a612976c324a28f34dee305b2c64f8b911a2cab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wca7kzfq5w2f1.png?width=1190&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3fd2195e4829bf47c8f6d0e2d6fef2c133e1d2f"&gt;https://preview.redd.it/wca7kzfq5w2f1.png?width=1190&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3fd2195e4829bf47c8f6d0e2d6fef2c133e1d2f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuy45r/gemma_3n_architectural_innovations_speculation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuy45r/gemma_3n_architectural_innovations_speculation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuy45r/gemma_3n_architectural_innovations_speculation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T08:59:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuzk3t</id>
    <title>Online inference is a privacy nightmare</title>
    <updated>2025-05-25T10:39:04+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I dont understand how big tech just convinced people to hand over so much stuff to be processed in plain text. Cloud storage at least can be all encrypted. But people have got comfortable sending emails, drafts, their deepest secrets, all in the open on some servers somewhere. Am I crazy? People were worried about posts and likes on social media for privacy but this is magnitudes larger in scope. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuzk3t/online_inference_is_a_privacy_nightmare/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuzk3t/online_inference_is_a_privacy_nightmare/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuzk3t/online_inference_is_a_privacy_nightmare/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T10:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuwrll</id>
    <title>üëÄ BAGEL-7B-MoT: The Open-Source GPT-Image-1 Alternative You‚Äôve Been Waiting For.</title>
    <updated>2025-05-25T07:24:39+00:00</updated>
    <author>
      <name>/u/Rare-Programmer-1747</name>
      <uri>https://old.reddit.com/user/Rare-Programmer-1747</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuwrll/bagel7bmot_the_opensource_gptimage1_alternative/"&gt; &lt;img alt="üëÄ BAGEL-7B-MoT: The Open-Source GPT-Image-1 Alternative You‚Äôve Been Waiting For." src="https://b.thumbs.redditmedia.com/0FqyiVWdrxZTgD89ug8SMDskK09zDRwfDx-Rn8gc8zk.jpg" title="üëÄ BAGEL-7B-MoT: The Open-Source GPT-Image-1 Alternative You‚Äôve Been Waiting For." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/sw3eao9cqv2f1.jpg?width=3000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4c753fae3901f5a15249aa73803dbfbed0b8f77e"&gt;https://preview.redd.it/sw3eao9cqv2f1.jpg?width=3000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4c753fae3901f5a15249aa73803dbfbed0b8f77e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ByteDance has unveiled &lt;strong&gt;BAGEL-7B-MoT&lt;/strong&gt;, an open-source multimodal AI model that rivals OpenAI's proprietary &lt;strong&gt;GPT-Image-1&lt;/strong&gt; in capabilities. With 7 billion active parameters (14 billion total) and a Mixture-of-Transformer-Experts (MoT) architecture, BAGEL offers advanced functionalities in text-to-image generation, image editing, and visual understanding‚Äîall within a single, unified model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Unified Multimodal Capabilities:&lt;/strong&gt; BAGEL seamlessly integrates text, image, and video processing, eliminating the need for multiple specialized models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced Image Editing:&lt;/strong&gt; Supports free-form editing, style transfer, scene reconstruction, and multiview synthesis, often producing more accurate and contextually relevant results than other open-source models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Emergent Abilities:&lt;/strong&gt; Demonstrates capabilities such as chain-of-thought reasoning and world navigation, enhancing its utility in complex tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmark Performance:&lt;/strong&gt; Outperforms models like Qwen2.5-VL and InternVL-2.5 on standard multimodal understanding leaderboards and delivers text-to-image quality competitive with specialist generators like SD3.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Comparison with GPT-Image-1:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;BAGEL-7B-MoT&lt;/th&gt; &lt;th align="left"&gt;GPT-Image-1&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Open-source (Apache 2.0)&lt;/td&gt; &lt;td align="left"&gt;Proprietary (requires OpenAI API key)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Multimodal Capabilities&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Text-to-image, image editing, visual understanding&lt;/td&gt; &lt;td align="left"&gt;Primarily text-to-image generation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Architecture&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Mixture-of-Transformer-Experts&lt;/td&gt; &lt;td align="left"&gt;Diffusion-based model&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Self-hostable on local hardware&lt;/td&gt; &lt;td align="left"&gt;Cloud-based via OpenAI API&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Emergent Abilities&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Free-form image editing, multiview synthesis, world navigation&lt;/td&gt; &lt;td align="left"&gt;Limited to text-to-image generation and editing&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Installation and Usage:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Developers can access the model weights and implementation on Hugging Face. For detailed installation instructions and usage examples, the GitHub repository is available.&lt;/p&gt; &lt;p&gt;BAGEL-7B-MoT represents a significant advancement in multimodal AI, offering a versatile and efficient solution for developers working with diverse media types. Its open-source nature and comprehensive capabilities make it a valuable tool for those seeking an alternative to proprietary models like GPT-Image-1.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare-Programmer-1747"&gt; /u/Rare-Programmer-1747 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuwrll/bagel7bmot_the_opensource_gptimage1_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuwrll/bagel7bmot_the_opensource_gptimage1_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuwrll/bagel7bmot_the_opensource_gptimage1_alternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T07:24:39+00:00</published>
  </entry>
</feed>
