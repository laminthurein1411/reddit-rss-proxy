<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-14T22:35:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i1ibte</id>
    <title>My First Small AI Project for my company</title>
    <updated>2025-01-14T22:26:34+00:00</updated>
    <author>
      <name>/u/cri10095</name>
      <uri>https://old.reddit.com/user/cri10095</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I just wrapped up my first little project at the company I work for: a simple RAG chatbot able to help my colleagues in the assistance department based on internal reports on common issues, manuals, standard procedures and website pages for general knowledge on the company / product links.&lt;/p&gt; &lt;p&gt;I built it using LangChain for vector DB search and Flutter for the UI, locally hosted on a RPi.&lt;/p&gt; &lt;p&gt;I had fun trying to squeeze as much performance as possible from old office hardware. I experimented with small and quantized models (mostly from Bartosky [thanks for those!]). Unfortunately and as supposed, not even a LLaMA 3.2 1B Q4 couldn't hit decent speeds (&amp;gt; 1 token/s). So, while waiting for GPUs, I'm testing Mistral, groq (really fast inference!!) and other few providers through their APIs.&lt;/p&gt; &lt;p&gt;AI development has become a real hobby for me, even though my background is in a different type of engineering. I spend my &amp;quot;free&amp;quot; time at work (simple but time-consuming tasks) listening model-testing, try to learn how neural networks work, or with hands on video like Google Colab tutorials. I know I won't become a researcher publishing papers or a top developer in the field, but I‚Äôd love to get better.&lt;/p&gt; &lt;p&gt;What would you recommend I focus on or study to improve as an AI developer?&lt;/p&gt; &lt;p&gt;Thanks in advance for any advice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cri10095"&gt; /u/cri10095 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ibte/my_first_small_ai_project_for_my_company/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ibte/my_first_small_ai_project_for_my_company/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ibte/my_first_small_ai_project_for_my_company/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T22:26:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i16s5q</id>
    <title>Llama 3 8b or Mistral Nemo 12b for 12gb Vram?</title>
    <updated>2025-01-14T14:05:31+00:00</updated>
    <author>
      <name>/u/NaviGray</name>
      <uri>https://old.reddit.com/user/NaviGray</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a ryzen 5 5500 and an rtx 3060 12gb. I'm new to LLM stuff but I want to start learning to fine-tune one. Which one should I use. I found online that both are fantastic but Llama might be too much with 12gb?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NaviGray"&gt; /u/NaviGray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i16s5q/llama_3_8b_or_mistral_nemo_12b_for_12gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i16s5q/llama_3_8b_or_mistral_nemo_12b_for_12gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i16s5q/llama_3_8b_or_mistral_nemo_12b_for_12gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T14:05:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1hmd7</id>
    <title>What do you use your local LLM on your phone to do?</title>
    <updated>2025-01-14T21:56:18+00:00</updated>
    <author>
      <name>/u/t0f0b0</name>
      <uri>https://old.reddit.com/user/t0f0b0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Those of you who have set up a local LLM on your phone: What do you use it for? Have you found any interesting things you can do with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/t0f0b0"&gt; /u/t0f0b0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1hmd7/what_do_you_use_your_local_llm_on_your_phone_to_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1hmd7/what_do_you_use_your_local_llm_on_your_phone_to_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1hmd7/what_do_you_use_your_local_llm_on_your_phone_to_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T21:56:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i12g9x</id>
    <title>Android voice input method based on Whisper</title>
    <updated>2025-01-14T09:35:42+00:00</updated>
    <author>
      <name>/u/DocWolle</name>
      <uri>https://old.reddit.com/user/DocWolle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://f-droid.org/de/packages/org.woheller69.whisper/"&gt;https://f-droid.org/de/packages/org.woheller69.whisper/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocWolle"&gt; /u/DocWolle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i12g9x/android_voice_input_method_based_on_whisper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i12g9x/android_voice_input_method_based_on_whisper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i12g9x/android_voice_input_method_based_on_whisper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T09:35:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1i19ysx</id>
    <title>Deepseek v3 Experiences</title>
    <updated>2025-01-14T16:30:17+00:00</updated>
    <author>
      <name>/u/easyrider99</name>
      <uri>https://old.reddit.com/user/easyrider99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;I would like to probe the community to find out your experiences with running Deepseek v3 locally. I have been building a local inference machine and managed to get enough ram to be able to run the Q4_K_M.&lt;/p&gt; &lt;p&gt;Build:&lt;br /&gt; Xeon w7-3455&lt;br /&gt; Asus W790 Sage&lt;br /&gt; 432gb DDR5 @ 4800 ( 4x32, 3x96, 16 ) &lt;/p&gt; &lt;p&gt;3 x RTX 3090&lt;/p&gt; &lt;p&gt;llama command:&lt;/p&gt; &lt;p&gt;./build/bin/llama-server --model ~/llm/models/unsloth_DeepSeek-V3-GGUF_f_Q4_K_M/DeepSeek-V3-Q4_K_M/DeepSeek-V3-Q4_K_M-00001-of-00009.gguf --cache-type-k q5_0 --threads 22 --host &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; --no-context-shift --port 9999 --ctx-size 8240 --gpu-layers 6&lt;/p&gt; &lt;p&gt;Results with small context: (What is deepseek?) about 7&lt;/p&gt; &lt;p&gt;prompt eval time = 1317.45 ms / 7 tokens ( 188.21 ms per token, 5.31 tokens per second)&lt;/p&gt; &lt;p&gt;eval time = 81081.39 ms / 269 tokens ( 301.42 ms per token, 3.32 tokens per second)&lt;/p&gt; &lt;p&gt;total time = 82398.83 ms / 276 tokens&lt;/p&gt; &lt;p&gt;Results with large context: ( Shopify theme file + prompt )&lt;br /&gt; prompt eval time = 368904.48 ms / 3099 tokens ( 119.04 ms per token, 8.40 tokens per second)&lt;/p&gt; &lt;p&gt;eval time = 372849.73 ms / 779 tokens ( 478.63 ms per token, 2.09 tokens per second)&lt;/p&gt; &lt;p&gt;total time = 741754.21 ms / 3878 tokens&lt;/p&gt; &lt;p&gt;It doesn't seem like running this model locally makes any sense until the ktransformers team can integrate it. What do you guys think? Is there something I am missing to get the performance higher?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/easyrider99"&gt; /u/easyrider99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i19ysx/deepseek_v3_experiences/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i19ysx/deepseek_v3_experiences/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i19ysx/deepseek_v3_experiences/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T16:30:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0vrm5</id>
    <title>Here is our new reranker model, which we trained on over 95 languages and it achieves better performance than comparable rerankers on our eval benchmarks. Weights, data, and training code are all open source.</title>
    <updated>2025-01-14T02:30:00+00:00</updated>
    <author>
      <name>/u/Peter_Lightblue</name>
      <uri>https://old.reddit.com/user/Peter_Lightblue</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0vrm5/here_is_our_new_reranker_model_which_we_trained/"&gt; &lt;img alt="Here is our new reranker model, which we trained on over 95 languages and it achieves better performance than comparable rerankers on our eval benchmarks. Weights, data, and training code are all open source." src="https://external-preview.redd.it/IqG-6k7nq3XlqnkLxN4BETD1asSgHQ6DDeSuHbGLcoE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ead37f624cbf68ece5069b4337ea6165f2619b57" title="Here is our new reranker model, which we trained on over 95 languages and it achieves better performance than comparable rerankers on our eval benchmarks. Weights, data, and training code are all open source." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Peter_Lightblue"&gt; /u/Peter_Lightblue &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lightblue/lb-reranker-0.5B-v1.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0vrm5/here_is_our_new_reranker_model_which_we_trained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0vrm5/here_is_our_new_reranker_model_which_we_trained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T02:30:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1bi3b</id>
    <title>Running a 2B LLM on an iphone with swift-mlx</title>
    <updated>2025-01-14T17:34:11+00:00</updated>
    <author>
      <name>/u/l-m-z</name>
      <uri>https://old.reddit.com/user/l-m-z</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all üëã!&lt;/p&gt; &lt;p&gt;A bit of self promotion in this post but hopefully that's fine :) I work at Kyutai and we released yesterday a new multilingual 2B LLM aimed at on device inference, Helium 2B. Just wanted to share a video with the model running locally on an iPhone 16 Pro at ~28 tok/s (seems to reach ~35 tok/s when plugged in) üöÄ All that uses mlx-swift with q4 quantization - not much optimizations at this stage so just relying on mlx to do all the hard work for us!&lt;/p&gt; &lt;p&gt;It's just a proof of concept at this stage as you cannot even enter a prompt and we don't have an instruct variant of the model anyway. We're certainly looking forward to some feedback on the model itself, we plan on supporting more languages in the near future as well as releasing the whole training pipeline. And we also plan to release more models that run on device too!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1i1bi3b/video/gswzis8ewzce1/player"&gt;https://reddit.com/link/1i1bi3b/video/gswzis8ewzce1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/l-m-z"&gt; /u/l-m-z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1bi3b/running_a_2b_llm_on_an_iphone_with_swiftmlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1bi3b/running_a_2b_llm_on_an_iphone_with_swiftmlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1bi3b/running_a_2b_llm_on_an_iphone_with_swiftmlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T17:34:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1de3o</id>
    <title>AI Search Assistant with Local model and Knowledge Base Support</title>
    <updated>2025-01-14T18:54:16+00:00</updated>
    <author>
      <name>/u/LeetTools</name>
      <uri>https://old.reddit.com/user/LeetTools</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1de3o/ai_search_assistant_with_local_model_and/"&gt; &lt;img alt="AI Search Assistant with Local model and Knowledge Base Support" src="https://external-preview.redd.it/X7lWSwknYi-M9GhyRLKkteIF2TostzEhXH1nEzNr3rE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1235ae89fa654008dcc53cafea8e75a3a216cb4" title="AI Search Assistant with Local model and Knowledge Base Support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, just want to share with you an open source search assistant with local model and knowledge base support called LeetTools (&lt;a href="https://github.com/leettools-dev/leettools"&gt;https://github.com/leettools-dev/leettools&lt;/a&gt;). You can run highly customizable AI search workflows (like Perplexity, Google Deep Research) locally on your command line with a full automated document pipeline. The search results and generated outputs are saved to local knowledge bases, which can add your own data and be queried together.&lt;/p&gt; &lt;p&gt;Here is an example of an article about ‚ÄúHow does Ollama work‚Äù, generated with the digest flow that is similar to Google deep research:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/leettools-dev/leettools/blob/main/docs/examples/ollama.md"&gt;https://github.com/leettools-dev/leettools/blob/main/docs/examples/ollama.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The digest flow works as follows:&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/n8ar4jaca0de1.gif"&gt;https://i.redd.it/n8ar4jaca0de1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With a DuckDB-backend and configurable LLM settings, LeetTools can run with minimal resource requirements on the command line and can be easily integrated with other applications needing AI search and knowledge base support. You can use any LLM service by switch simple configuration: we have examples for both Ollama and the new DeepSeek V3 API.&lt;/p&gt; &lt;p&gt;The tool is totally free with Apache license. Feedbacks and suggestions would be highly appreciated. Thanks and enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeetTools"&gt; /u/LeetTools &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1de3o/ai_search_assistant_with_local_model_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1de3o/ai_search_assistant_with_local_model_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1de3o/ai_search_assistant_with_local_model_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T18:54:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1ax9u</id>
    <title>What is your efficient go-to model for TTS?</title>
    <updated>2025-01-14T17:09:57+00:00</updated>
    <author>
      <name>/u/requizm</name>
      <uri>https://old.reddit.com/user/requizm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do I want?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU inference&lt;/li&gt; &lt;li&gt;Multilanguage. Not just the top 7 languages.&lt;/li&gt; &lt;li&gt;Voice cloning. I prefer voice cloning over fine-tuning for most cases.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I checked recent posts about TTS models and the leaderboard. Tried 3 of them:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/rhasspy/piper"&gt;Piper&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This is the fastest model in my experience. It even works instantly on my crappy server.&lt;/li&gt; &lt;li&gt;Multilanguage.&lt;/li&gt; &lt;li&gt;It doesn't have voice cloning but fine-tuning is not hard.&lt;/li&gt; &lt;li&gt;One thing I don't like, it is not maintained anymore. I wish they could update pytorch version to 2.0, so I can easily fine-tune on GPU rented servers(48GB+ GPU). Currently, I couldn't even fine-tune on RTX 4090.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/SWivid/F5-TTS/"&gt;F5TTS&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multilanguage and voice cloning.&lt;/li&gt; &lt;li&gt;Inference speed is bad compared to Piper.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/idiap/coqui-ai-TTS"&gt;XTTS (coqui-ai-fork)&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multilanguage.&lt;/li&gt; &lt;li&gt;Don't have voice cloning.&lt;/li&gt; &lt;li&gt;Inference speed is bad compared to Piper.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;Kokoro-TTS&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It is #1 on the leaderboard, I didn't even try because &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M/discussions/30"&gt;language support&lt;/a&gt; is not enough for me.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/requizm"&gt; /u/requizm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ax9u/what_is_your_efficient_goto_model_for_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ax9u/what_is_your_efficient_goto_model_for_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ax9u/what_is_your_efficient_goto_model_for_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T17:09:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i16lvy</id>
    <title>openbmb/MiniCPM-o-2_6 ¬∑ Hugging Face</title>
    <updated>2025-01-14T13:57:25+00:00</updated>
    <author>
      <name>/u/Durian881</name>
      <uri>https://old.reddit.com/user/Durian881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i16lvy/openbmbminicpmo2_6_hugging_face/"&gt; &lt;img alt="openbmb/MiniCPM-o-2_6 ¬∑ Hugging Face" src="https://external-preview.redd.it/HfgLF9MP4qFrrT-J0Ft0hKLbfWPg5ZEDF194P91AP-U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf06c624cc0dcbf599f5edea7b4be7e420f634b7" title="openbmb/MiniCPM-o-2_6 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The model is built in an end-to-end fashion based on SigLip-400M, Whisper-medium-300M, ChatTTS-200M, and Qwen2.5-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-V 2.6, and introduces new features for realtime speech conversation and multimodal live streaming.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Durian881"&gt; /u/Durian881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i16lvy/openbmbminicpmo2_6_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i16lvy/openbmbminicpmo2_6_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T13:57:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0ysa7</id>
    <title>Qwen released a 72B and a 7B process reward models (PRM) on their recent math models</title>
    <updated>2025-01-14T05:12:52+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0ysa7/qwen_released_a_72b_and_a_7b_process_reward/"&gt; &lt;img alt="Qwen released a 72B and a 7B process reward models (PRM) on their recent math models" src="https://external-preview.redd.it/VHyrFnoulzKOpumKfTYgd3z5gbzActL7OlrPPgPm9KM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c713b68d12275996f8c476345dd6d21eae4d0b75" title="Qwen released a 72B and a 7B process reward models (PRM) on their recent math models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-Math-PRM-72B"&gt;https://huggingface.co/Qwen/Qwen2.5-Math-PRM-72B&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Math-PRM-7B"&gt;https://huggingface.co/Qwen/Qwen2.5-Math-PRM-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;In addition to the mathematical Outcome Reward Model (ORM) Qwen2.5-Math-RM-72B, we release the Process Reward Model (PRM), namely Qwen2.5-Math-PRM-7B and Qwen2.5-Math-PRM-72B. PRMs emerge as a promising approach for process supervision in mathematical reasoning of Large Language Models (LLMs), aiming to identify and mitigate intermediate errors in the reasoning processes. Our trained PRMs exhibit both impressive performance in the Best-of-N (BoN) evaluation and stronger error identification performance in ProcessBench.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ekfsi99i7wce1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=412d38180444dc55ea80687762ea2ed8d0cb3e8f"&gt;https://preview.redd.it/ekfsi99i7wce1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=412d38180444dc55ea80687762ea2ed8d0cb3e8f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The paper: The Lessons of Developing Process Reward Models in Mathematical Reasoning&lt;br /&gt; arXiv:2501.07301 [cs.CL]: &lt;a href="https://arxiv.org/abs/2501.07301"&gt;https://arxiv.org/abs/2501.07301&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0ysa7/qwen_released_a_72b_and_a_7b_process_reward/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0ysa7/qwen_released_a_72b_and_a_7b_process_reward/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0ysa7/qwen_released_a_72b_and_a_7b_process_reward/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T05:12:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1bf5j</id>
    <title>New Thematic Generalization Benchmark: measures how effectively LLMs infer a specific "theme" from a small set of examples and anti-examples</title>
    <updated>2025-01-14T17:30:37+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1bf5j/new_thematic_generalization_benchmark_measures/"&gt; &lt;img alt="New Thematic Generalization Benchmark: measures how effectively LLMs infer a specific &amp;quot;theme&amp;quot; from a small set of examples and anti-examples" src="https://external-preview.redd.it/wF_HKR0BhWrREPFLC9pRJkrUmcKOwJnL6VjPCFunPqU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=713f1dd254baaea660314069551e52bcb224426f" title="New Thematic Generalization Benchmark: measures how effectively LLMs infer a specific &amp;quot;theme&amp;quot; from a small set of examples and anti-examples" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/lechmazur/generalization"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1bf5j/new_thematic_generalization_benchmark_measures/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1bf5j/new_thematic_generalization_benchmark_measures/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T17:30:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1b0bo</id>
    <title>An LLM serving framework that can fast run o1-like SmallThinker on smartphones</title>
    <updated>2025-01-14T17:13:24+00:00</updated>
    <author>
      <name>/u/Zealousideal_Bad_52</name>
      <uri>https://old.reddit.com/user/Zealousideal_Bad_52</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1b0bo/an_llm_serving_framework_that_can_fast_run_o1like/"&gt; &lt;img alt="An LLM serving framework that can fast run o1-like SmallThinker on smartphones" src="https://external-preview.redd.it/qjrUvoYZJnvSkWJhxCodjQr_xKXfBbf0Wm7xUIvDP4Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48af74da7e0e05784fc236d508cf81bf9193d9e4" title="An LLM serving framework that can fast run o1-like SmallThinker on smartphones" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, we're excited to announce the release of &lt;strong&gt;PowerServe&lt;/strong&gt;, a highly optimized serving framework specifically designed for smartphone.&lt;br /&gt; &lt;a href="https://github.com/powerserve-project/PowerServe"&gt;Github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1i1b0bo/video/uf85o248szce1/player"&gt;Running on Qualcomm 8 Gen4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Key Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;One-click&lt;/strong&gt; deployment&lt;/li&gt; &lt;li&gt;NPU &lt;strong&gt;speculative&lt;/strong&gt; inference support&lt;/li&gt; &lt;li&gt;Achieves &lt;strong&gt;40&lt;/strong&gt; tokens/s running o1-like reasoning model Smallthinker on mobile devices&lt;/li&gt; &lt;li&gt;Support &lt;strong&gt;Android&lt;/strong&gt;, &lt;strong&gt;Harmony Next&lt;/strong&gt; SmartPhone&lt;/li&gt; &lt;li&gt;Support &lt;strong&gt;Qwen2/Qwen2.5&lt;/strong&gt;, &lt;strong&gt;Llama3&lt;/strong&gt; series and &lt;strong&gt;SmallThinker-3B-Preview&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the future, we will integrate more acceleration methods, including PowerInfer, PowerInfer-2, and more speculative inference algorithms.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal_Bad_52"&gt; /u/Zealousideal_Bad_52 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1b0bo/an_llm_serving_framework_that_can_fast_run_o1like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1b0bo/an_llm_serving_framework_that_can_fast_run_o1like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1b0bo/an_llm_serving_framework_that_can_fast_run_o1like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T17:13:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1eyl5</id>
    <title>2025 and the future of Local AI</title>
    <updated>2025-01-14T19:59:17+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2024 was an amazing year for Local AI. We had great free models Llama 3.x, Qwen2.5 Deepseek v3 and much more.&lt;/p&gt; &lt;p&gt;However, we also see some counter-trends such as Mistral previously released very liberal licenses, but started moving towards Research licenses. We see some AI shops closing down.&lt;/p&gt; &lt;p&gt;I wonder if we are getting close to Peak 'free' AI as competition heats up and competitors drop out leaving remaining competitors forced to monetize.&lt;/p&gt; &lt;p&gt;We still have LLama, Qwen and Deepseek providing open models - but even here, there are questions on whether we can really deploy these easily (esp. with monstrous 405B Llama and DS v3).&lt;/p&gt; &lt;p&gt;Let's also think about economics. Imagine a world where OpenAI does make a leap ahead. They release an AI which they sell to corporations for $1,000 a month subject to a limited duty cycle. Let's say this is powerful enough and priced right to wipe out 30% of office jobs. What will this do to society and the economy? What happens when this 30% ticks upwards to 50%, 70%?&lt;/p&gt; &lt;p&gt;Currently, we have software companies like Google which have huge scale, servicing the world with a relatively small team. What if most companies are like this? A core team of execs with the work done mainly through AI systems. What happens when this comes to manual jobs through AI robots?&lt;/p&gt; &lt;p&gt;What would the average person do? How can such an economy function?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1eyl5/2025_and_the_future_of_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1eyl5/2025_and_the_future_of_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1eyl5/2025_and_the_future_of_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T19:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1i10okg</id>
    <title>The more you buy...</title>
    <updated>2025-01-14T07:21:39+00:00</updated>
    <author>
      <name>/u/Wrong_User_Logged</name>
      <uri>https://old.reddit.com/user/Wrong_User_Logged</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i10okg/the_more_you_buy/"&gt; &lt;img alt="The more you buy..." src="https://preview.redd.it/eprbpw3puwce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee912b8940316504b27f1914cf643a624a21541a" title="The more you buy..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong_User_Logged"&gt; /u/Wrong_User_Logged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eprbpw3puwce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i10okg/the_more_you_buy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i10okg/the_more_you_buy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T07:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i11961</id>
    <title>MiniCPM-o 2.6: An 8B size, GPT-4o level Omni Model runs on device</title>
    <updated>2025-01-14T08:03:44+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i11961/minicpmo_26_an_8b_size_gpt4o_level_omni_model/"&gt; &lt;img alt="MiniCPM-o 2.6: An 8B size, GPT-4o level Omni Model runs on device " src="https://external-preview.redd.it/kijGNUkOqzC0sy8QxKC6uApBn38O_fAQyil0gLR68s8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2741dd1ca03bff7cebb63910f7d365982fb4506e" title="MiniCPM-o 2.6: An 8B size, GPT-4o level Omni Model runs on device " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/OpenBMB/status/1879074895113621907"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i11961/minicpmo_26_an_8b_size_gpt4o_level_omni_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i11961/minicpmo_26_an_8b_size_gpt4o_level_omni_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T08:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i142iy</id>
    <title>What % of these do you think will be here by 2026?</title>
    <updated>2025-01-14T11:31:28+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i142iy/what_of_these_do_you_think_will_be_here_by_2026/"&gt; &lt;img alt="What % of these do you think will be here by 2026?" src="https://preview.redd.it/bk6yk62f3yce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e98a42c3a350d3d500729e61e160e27f838a59bd" title="What % of these do you think will be here by 2026?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bk6yk62f3yce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i142iy/what_of_these_do_you_think_will_be_here_by_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i142iy/what_of_these_do_you_think_will_be_here_by_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T11:31:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i192xf</id>
    <title>DDR6 RAM and a reasonable GPU should be able to run 70b models with good speed</title>
    <updated>2025-01-14T15:51:58+00:00</updated>
    <author>
      <name>/u/itsnottme</name>
      <uri>https://old.reddit.com/user/itsnottme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Right now low VRAM GPUs are the bottleneck in running bigger models, but DDR6 ram should somewhat fix this issue. The ram can supplement GPUs to run LLMs at pretty good speed. &lt;/p&gt; &lt;p&gt;Running bigger models on CPU alone is not ideal, a reasonable speed GPU will still be needed to calculate the context. Let's use a RTX 4080 for example but a slower one is fine as well.&lt;/p&gt; &lt;p&gt;A 70b Q4 KM model is ~40 GB&lt;/p&gt; &lt;p&gt;8192 context is around 3.55 GB&lt;/p&gt; &lt;p&gt;RTX 4080 can hold around 12 GB of the model + 3.55 GB context + leaving 0.45 GB for system memory.&lt;/p&gt; &lt;p&gt;RTX 4080 Memory Bandwidth is 716.8 GB/s x 0.7 for efficiency = ~502 GB/s&lt;/p&gt; &lt;p&gt;For DDR6 ram, it's hard to say for sure but should be around twice the speed of DDR5 and supports Quad Channel so should be close to 360 GB/s * 0.7 = 252 GB/s&lt;/p&gt; &lt;p&gt;(0.3√ó502) + (0.7√ó252) = 327 GB/s&lt;/p&gt; &lt;p&gt;So the model should run at around 8.2 tokens/s&lt;/p&gt; &lt;p&gt;It should be a pretty reasonable speed for the average user. Even a slower GPU should be fine as well.&lt;/p&gt; &lt;p&gt;If I made a mistake in the calculation, feel free to let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/itsnottme"&gt; /u/itsnottme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i192xf/ddr6_ram_and_a_reasonable_gpu_should_be_able_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i192xf/ddr6_ram_and_a_reasonable_gpu_should_be_able_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i192xf/ddr6_ram_and_a_reasonable_gpu_should_be_able_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T15:51:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1b2xq</id>
    <title>Transformer^2: Self-adaptive LLMs</title>
    <updated>2025-01-14T17:16:19+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2501.06252"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1b2xq/transformer2_selfadaptive_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1b2xq/transformer2_selfadaptive_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T17:16:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1i19e8u</id>
    <title>Agentic setups beat vanilla LLMs by a huge margin üìà</title>
    <updated>2025-01-14T16:05:40+00:00</updated>
    <author>
      <name>/u/unofficialmerve</name>
      <uri>https://old.reddit.com/user/unofficialmerve</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i19e8u/agentic_setups_beat_vanilla_llms_by_a_huge_margin/"&gt; &lt;img alt="Agentic setups beat vanilla LLMs by a huge margin üìà" src="https://b.thumbs.redditmedia.com/VaHY2thgF4XeViAsdm5g4KaSQqjvdrVHVe17FR8kgTs.jpg" title="Agentic setups beat vanilla LLMs by a huge margin üìà" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello folks üëãüèª I'm Merve, I work on Hugging Face's new agents library smolagents. &lt;/p&gt; &lt;p&gt;We recently observed that many people are sceptic of agentic systems, so we benchmarked our CodeAgents (agents that write their actions/tool calls in python blobs) against vanilla LLM calls.&lt;/p&gt; &lt;p&gt;Plot twist: agentic setups easily bring 40 percentage point improvements compared to vanilla LLMs This crazy score increase makes sense, let's take this SimpleQA question:&lt;br /&gt; &amp;quot;Which Dutch player scored an open-play goal in the 2022 Netherlands vs Argentina game in the men‚Äôs FIFA World Cup?&amp;quot;&lt;/p&gt; &lt;p&gt;If I had to answer that myself, I certainly would do better with access to a web search tool than with my vanilla knowledge. (argument put forward by Andrew Ng in a great talk at Sequoia)&lt;br /&gt; Here each benchmark is a subsample of ~50 questions from the original benchmarks. Find the whole benchmark here: &lt;a href="https://github.com/huggingface/smolagents/blob/main/examples/benchmark.ipynb"&gt;https://github.com/huggingface/smolagents/blob/main/examples/benchmark.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7p6lbz7fgzce1.png?width=1467&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30d91e22b32e572e8824b08b4d95a52aeb82c5d5"&gt;https://preview.redd.it/7p6lbz7fgzce1.png?width=1467&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30d91e22b32e572e8824b08b4d95a52aeb82c5d5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialmerve"&gt; /u/unofficialmerve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i19e8u/agentic_setups_beat_vanilla_llms_by_a_huge_margin/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i19e8u/agentic_setups_beat_vanilla_llms_by_a_huge_margin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i19e8u/agentic_setups_beat_vanilla_llms_by_a_huge_margin/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T16:05:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i148es</id>
    <title>Today I start my very own org 100% devoted to open-source - and it's all thanks to LLMs</title>
    <updated>2025-01-14T11:43:10+00:00</updated>
    <author>
      <name>/u/mark-lord</name>
      <uri>https://old.reddit.com/user/mark-lord</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; Big thank you to every single one of you here!! My background is in biology - not software dev. This huge milestone in my life could never have happened if it wasn't for LLMs, the fantastic open source ecosystem around them, and of course all the awesome folks here in r /LocalLlama!&lt;/p&gt; &lt;p&gt;Also this post was originally a lot longer but I keep getting autofiltered lol - will put the rest in comments üòÑ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mark-lord"&gt; /u/mark-lord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i148es/today_i_start_my_very_own_org_100_devoted_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i148es/today_i_start_my_very_own_org_100_devoted_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i148es/today_i_start_my_very_own_org_100_devoted_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T11:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i11hre</id>
    <title>Why are they releasing open source models for free?</title>
    <updated>2025-01-14T08:21:50+00:00</updated>
    <author>
      <name>/u/wochiramen</name>
      <uri>https://old.reddit.com/user/wochiramen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are getting several quite good AI models. It takes money to train them, yet they are being released for free.&lt;/p&gt; &lt;p&gt;Why? What‚Äôs the incentive to release a model for free?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wochiramen"&gt; /u/wochiramen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i11hre/why_are_they_releasing_open_source_models_for_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i11hre/why_are_they_releasing_open_source_models_for_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i11hre/why_are_they_releasing_open_source_models_for_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T08:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1a88y</id>
    <title>MiniMax-Text-01 - A powerful new MoE language model with 456B total parameters (45.9 billion activated)</title>
    <updated>2025-01-14T16:41:20+00:00</updated>
    <author>
      <name>/u/Many_SuchCases</name>
      <uri>https://old.reddit.com/user/Many_SuchCases</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1a88y/minimaxtext01_a_powerful_new_moe_language_model/"&gt; &lt;img alt="MiniMax-Text-01 - A powerful new MoE language model with 456B total parameters (45.9 billion activated)" src="https://external-preview.redd.it/HbANHzNsjzvIfVaIHJm0DQnyVjkhdwH7FoXz3GLoR3k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ceab60c72e05525604b9367fa7915922146839a5" title="MiniMax-Text-01 - A powerful new MoE language model with 456B total parameters (45.9 billion activated)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-Text-01"&gt;https://huggingface.co/MiniMaxAI/MiniMax-Text-01&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8os84sl2mzce1.png?width=3320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4f6f93b8a0965d65139ba727de29c55880f1b91"&gt;https://preview.redd.it/8os84sl2mzce1.png?width=3320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4f6f93b8a0965d65139ba727de29c55880f1b91&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Description:&lt;/strong&gt; MiniMax-Text-01 is a powerful language model with 456 billion total parameters, of which 45.9 billion are activated per token. To better unlock the long context capabilities of the model, MiniMax-Text-01 adopts a hybrid architecture that combines Lightning Attention, Softmax Attention and Mixture-of-Experts (MoE). Leveraging advanced parallel strategies and innovative compute-communication overlap methods‚Äîsuch as Linear Attention Sequence Parallelism Plus (LASP+), varlen ring attention, Expert Tensor Parallel (ETP), etc., MiniMax-Text-01's training context length is extended to 1 million tokens, and it can handle a context of up to 4 million tokens during the inference. On various academic benchmarks, MiniMax-Text-01 also demonstrates the performance of a top-tier model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Architecture:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Total Parameters: 456B&lt;/li&gt; &lt;li&gt;Activated Parameters per Token: 45.9B&lt;/li&gt; &lt;li&gt;Number Layers: 80&lt;/li&gt; &lt;li&gt;Hybrid Attention: a softmax attention is positioned after every 7 lightning attention. &lt;ul&gt; &lt;li&gt;Number of attention heads: 64&lt;/li&gt; &lt;li&gt;Attention head dimension: 128&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Mixture of Experts: &lt;ul&gt; &lt;li&gt;Number of experts: 32&lt;/li&gt; &lt;li&gt;Expert hidden dimension: 9216&lt;/li&gt; &lt;li&gt;Top-2 routing strategy&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Positional Encoding: Rotary Position Embedding (RoPE) applied to half of the attention head dimension with a base frequency of 10,000,000&lt;/li&gt; &lt;li&gt;Hidden Size: 6144&lt;/li&gt; &lt;li&gt;Vocab Size: 200,064&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Blog post:&lt;/strong&gt; &lt;a href="https://www.minimaxi.com/en/news/minimax-01-series-2"&gt;https://www.minimaxi.com/en/news/minimax-01-series-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HuggingFace:&lt;/strong&gt; &lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-Text-01"&gt;https://huggingface.co/MiniMaxAI/MiniMax-Text-01&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try online:&lt;/strong&gt; &lt;a href="https://www.hailuo.ai/"&gt;https://www.hailuo.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href="https://github.com/MiniMax-AI/MiniMax-01"&gt;https://github.com/MiniMax-AI/MiniMax-01&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Homepage:&lt;/strong&gt; &lt;a href="https://www.minimaxi.com/en"&gt;https://www.minimaxi.com/en&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PDF paper:&lt;/strong&gt; &lt;a href="https://filecdn.minimax.chat/_Arxiv_MiniMax_01_Report.pdf"&gt;https://filecdn.minimax.chat/_Arxiv_MiniMax_01_Report.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: I am not affiliated&lt;/p&gt; &lt;p&gt;GGUF quants might take a while because the architecture is new (MiniMaxText01ForCausalLM)&lt;/p&gt; &lt;p&gt;A Vision model was also released: &lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-VL-01"&gt;https://huggingface.co/MiniMaxAI/MiniMax-VL-01&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Many_SuchCases"&gt; /u/Many_SuchCases &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1a88y/minimaxtext01_a_powerful_new_moe_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1a88y/minimaxtext01_a_powerful_new_moe_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1a88y/minimaxtext01_a_powerful_new_moe_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T16:41:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1ffid</id>
    <title>I accidentally built an open alternative to Google AI Studio</title>
    <updated>2025-01-14T20:18:39+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday, I had a mini heart attack when I discovered Google AI Studio, a product that looked (at first glance) just like the tool I've been building for 5 months. However, I dove in and was super relieved once I got into the details. There were a bunch of differences, which I've detailed below.&lt;/p&gt; &lt;p&gt;I thought I‚Äôd share what I have, in case anyone has been using G AI Sudio, and might want to check out my &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;rapid prototyping tool on Github, called Kiln&lt;/a&gt;. There are some similarities, but there are also some big differences when it comes to privacy, collaboration, model support, fine-tuning, and ML techniques. I built Kiln because I've been building AI products for ~10 years (most recently at Apple, and my own startup &amp;amp; MSFT before that), and I wanted to build an easy to use, privacy focused, open source AI tooling.&lt;/p&gt; &lt;p&gt;Differences:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model Support: Kiln allows any LLM (including Gemini/Gemma) through a ton of hosts: Ollama, OpenRouter, OpenAI, etc. Google supports only Gemini &amp;amp; Gemma via Google Cloud.&lt;/li&gt; &lt;li&gt;Fine Tuning: Google lets you fine tune only Gemini, with at most 500 samples. Kiln has no limits on data size, 9 models you can tune in a few clicks (no code), and support for tuning any open model via Unsloth.&lt;/li&gt; &lt;li&gt;Data Privacy: Kiln can't access your data (it runs locally, data stays local); Google stores everything. Kiln can run/train local models (Ollama/Unsloth/LiteLLM); Google always uses their cloud.&lt;/li&gt; &lt;li&gt;Collaboration: Google is single user, while Kiln allows unlimited users/collaboration.&lt;/li&gt; &lt;li&gt;ML Techniques: Google has standard prompting. Kiln has standard prompts, chain-of-thought/reasoning, and auto-prompts (using your dataset for multi-shot).&lt;/li&gt; &lt;li&gt;Dataset management: Google has a table with max 500 rows. Kiln has powerful dataset management for teams with Git sync, tags, unlimited rows, human ratings, and more.&lt;/li&gt; &lt;li&gt;Python Library: Google is UI only. Kiln has a python library for extending it for when you need more than the UI can offer.&lt;/li&gt; &lt;li&gt;Open Source: Google‚Äôs is completely proprietary and private source. Kiln‚Äôs library is MIT open source; the UI isn‚Äôt MIT, but it is 100% source-available, on Github, and free.&lt;/li&gt; &lt;li&gt;Similarities: Both handle structured data well, both have a prompt library, both have similar ‚ÄúRun‚Äù UX, both had user friendly UIs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If anyone wants to check Kiln out, &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;here's the GitHub repository&lt;/a&gt; and &lt;a href="https://docs.getkiln.ai"&gt;docs are here&lt;/a&gt;. Getting started is super easy - it's a one-click install to get setup and running.&lt;/p&gt; &lt;p&gt;I‚Äôm very interested in any feedback or feature requests (model requests, integrations with other tools, etc.) I'm currently working on comprehensive evals, so feedback on what you'd like to see in that area would be super helpful. My hope is to make something as easy to use as G AI Studio, as powerful as Vertex AI, all while open and private.&lt;/p&gt; &lt;p&gt;Thanks in advance! I‚Äôm happy to answer any questions.&lt;/p&gt; &lt;p&gt;Side note: I‚Äôm usually pretty good at competitive research before starting a project. I had looked up Google's &amp;quot;AI Studio&amp;quot; before I started. However, I found and looked at &amp;quot;Vertex AI Studio&amp;quot;, which is a completely different type of product. How one company can have 2 products with almost identical names is beyond me...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ffid/i_accidentally_built_an_open_alternative_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ffid/i_accidentally_built_an_open_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ffid/i_accidentally_built_an_open_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T20:18:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i17k5e</id>
    <title>OASIS: Open social media stimulator that uses up to 1 million agents.</title>
    <updated>2025-01-14T14:43:05+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i17k5e/oasis_open_social_media_stimulator_that_uses_up/"&gt; &lt;img alt="OASIS: Open social media stimulator that uses up to 1 million agents." src="https://preview.redd.it/rgfjjzbf1zce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad7c4c95213e6848f1fad91fc11eacb2cb18e3b8" title="OASIS: Open social media stimulator that uses up to 1 million agents." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rgfjjzbf1zce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i17k5e/oasis_open_social_media_stimulator_that_uses_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i17k5e/oasis_open_social_media_stimulator_that_uses_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T14:43:05+00:00</published>
  </entry>
</feed>
