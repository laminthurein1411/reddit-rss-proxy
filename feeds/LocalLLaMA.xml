<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-23T17:05:54+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i85ygw</id>
    <title>5090 LLM Benchmark Results</title>
    <updated>2025-01-23T15:31:21+00:00</updated>
    <author>
      <name>/u/olmoscd</name>
      <uri>https://old.reddit.com/user/olmoscd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i85ygw/5090_llm_benchmark_results/"&gt; &lt;img alt="5090 LLM Benchmark Results" src="https://b.thumbs.redditmedia.com/MGfAH79aT_xaNFRSJ7poi3gO0-fB1zHyXrG-lv0CQCA.jpg" title="5090 LLM Benchmark Results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/dda4kg39iree1.png?width=2469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38562c75d3492cb57fbbc34c928f5773d01dbe0e"&gt;https://preview.redd.it/dda4kg39iree1.png?width=2469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38562c75d3492cb57fbbc34c928f5773d01dbe0e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So based on the 4090 prices and the small footprint, great thermals of the 5090. It doesn't seem like a bad deal to get 32GB VRAM and the fastest inferencing performance for $2K. Its looking like a 40% increase in performance (not that the 4090 was slow by any means).&lt;/p&gt; &lt;p&gt;I am still probably going for the 5080 but it is pretty tempting to have this compact beast for Qwen/Deepseek.&lt;/p&gt; &lt;p&gt;Please post any other results on LLM's if you got em!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/olmoscd"&gt; /u/olmoscd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i85ygw/5090_llm_benchmark_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i85ygw/5090_llm_benchmark_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i85ygw/5090_llm_benchmark_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T15:31:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7nxhy</id>
    <title>Imatrix quants of DeepSeek R1 (the big one) are up!</title>
    <updated>2025-01-22T22:28:35+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Took a nice 3-4 hours per quant at the lower end, and increased my huggingface storage by a solid 9TB or so... But they're up :D&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/DeepSeek-R1-GGUF"&gt;https://huggingface.co/bartowski/DeepSeek-R1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For anyone looking for non-imatrix from me they're of course also available on the lmstudio-community page but uh... Good luck.. the smallest size there is a casual 347GB for the files alone..&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lmstudio-community/DeepSeek-R1-GGUF"&gt;https://huggingface.co/lmstudio-community/DeepSeek-R1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;IQ1_S on the other hand is only 133GB, so basically anyone can run them! (/s)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nxhy/imatrix_quants_of_deepseek_r1_the_big_one_are_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nxhy/imatrix_quants_of_deepseek_r1_the_big_one_are_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nxhy/imatrix_quants_of_deepseek_r1_the_big_one_are_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T22:28:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i86ugr</id>
    <title>Why don't more LLMs work well with Cline?</title>
    <updated>2025-01-23T16:09:32+00:00</updated>
    <author>
      <name>/u/Sellitus</name>
      <uri>https://old.reddit.com/user/Sellitus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Cline a lot lately, and it's very surprising how most models simply don't follow instructions well. A lot of the time Cline gets stuck, whether the LLM isn't outputting with the correct tags or whatever. Even when some LLMs see what the problem is, they still don't correct themselves. Claude works pretty well, but even it has issues every now and then. I'm just curious, how many people use Cline with a non-Claude LLM and don't have errors frequently from not following instructions? If so, what models are y'all using? I've tried both of the newest Deepseek models, a lot of open models, and the only other one I've had decent success with is Llama 3.3 70b. I've been working on my own version of Cline recently to try to make the instructions more universally understood, which I've had some success with, but want to see if anyone knows of a better path to go down. Thank you!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sellitus"&gt; /u/Sellitus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i86ugr/why_dont_more_llms_work_well_with_cline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i86ugr/why_dont_more_llms_work_well_with_cline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i86ugr/why_dont_more_llms_work_well_with_cline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T16:09:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7pxn7</id>
    <title>Open WebUI adds reasoning-focused features in two new releases OUT TODAY!!! 0.5.5 adds "Thinking" tag support to streamline reasoning model chats (works with R1) . 0.5.6 brings new "reasoning_effort" parameter to control cognitive effort.</title>
    <updated>2025-01-22T23:56:53+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These friggin' guys are always dropping great new features just when we need them! They had been quiet for the last few weeks, but not anymore! Per the release notes from Open WebUI releases page today:&lt;br /&gt; &lt;a href="https://github.com/open-webui/open-webui/releases"&gt;https://github.com/open-webui/open-webui/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open WebUI 0.5.6:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üß† &lt;strong&gt;Effortful Reasoning Control for OpenAI Models:&lt;/strong&gt; Introduced the reasoning_effort parameter in chat controls for supported OpenAI models, enabling users to fine-tune how much cognitive effort a model dedicates to its responses, offering greater customization for complex queries and reasoning tasks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Open WebUI 0.5.5:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ü§î &lt;strong&gt;Native 'Think' Tag Support:&lt;/strong&gt; Introduced the new 'think' tag support that visually displays how long the model is thinking, omitting the reasoning content itself until the next turn. Ideal for creating a more streamlined and focused interaction experience.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7pxn7/open_webui_adds_reasoningfocused_features_in_two/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7pxn7/open_webui_adds_reasoningfocused_features_in_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7pxn7/open_webui_adds_reasoningfocused_features_in_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T23:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7g9po</id>
    <title>The Deep Seek R1 glaze is unreal but it‚Äôs true.</title>
    <updated>2025-01-22T17:16:07+00:00</updated>
    <author>
      <name>/u/LostMyOtherAcct69</name>
      <uri>https://old.reddit.com/user/LostMyOtherAcct69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have had a programming issue in my code for a RAG machine for two days that I‚Äôve been working through documentation and different LLM‚Äòs. &lt;/p&gt; &lt;p&gt;I have tried every single major LLM from every provider and none could solve this issue including O1 pro. I was going crazy. I just tried R1 and it fixed on its first attempt‚Ä¶ I think I found a new daily runner for coding.. time to cancel OpenAI pro lol. &lt;/p&gt; &lt;p&gt;So yes the glaze is unreal (especially that David and Goliath post lol) but it‚Äôs THAT good.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostMyOtherAcct69"&gt; /u/LostMyOtherAcct69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g9po/the_deep_seek_r1_glaze_is_unreal_but_its_true/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g9po/the_deep_seek_r1_glaze_is_unreal_but_its_true/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g9po/the_deep_seek_r1_glaze_is_unreal_but_its_true/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T17:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7w583</id>
    <title>Jan now supports DeepSeek R1 distills - model loading error fixed</title>
    <updated>2025-01-23T05:14:01+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7w583/jan_now_supports_deepseek_r1_distills_model/"&gt; &lt;img alt="Jan now supports DeepSeek R1 distills - model loading error fixed" src="https://external-preview.redd.it/292fRQ6ybDOj_r18_tYuGBSwlG2q4koHTYhO2NFfzsM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d01f7aa3e4b8157f9ad15919e2a4665a8fd758d" title="Jan now supports DeepSeek R1 distills - model loading error fixed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/jandotai/status/1882282461041967605"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7w583/jan_now_supports_deepseek_r1_distills_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7w583/jan_now_supports_deepseek_r1_distills_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T05:14:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7zn2e</id>
    <title>Is anyone else obsessed with watching R1's thinking output?</title>
    <updated>2025-01-23T09:27:55+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone else obsessed with watching the thinking output? &lt;/p&gt; &lt;p&gt;I find myself watching it and getting anxious when it gets close to the right answer but then does a &amp;quot;oh wait!&amp;quot; and then switches to another direction.&lt;/p&gt; &lt;p&gt;Today, I tried the 1.5B Q5 distil model to see how it did with a simple question:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;state density of gold and from this calculate the length of a cube of gold with 1kg mass.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;``` Thought for 232 seconds Okay, so I need to figure out the density of gold and then use that to calculate the length of a cube made with 1 kg of gold. Hmm, let's break this down step by step.&lt;/p&gt; &lt;p&gt;First, I remember that density is mass divided by volume. The formula for density is:&lt;/p&gt; &lt;p&gt;Density= Volume Mass ‚Äã&lt;/p&gt; &lt;p&gt;But wait, to use this formula, I need to know the volume of the gold cube. Since it's a cube, all sides are equal, right? So if I can find the side length, I can find the volume.&lt;/p&gt; &lt;p&gt;But before that, I need the density of gold. I think gold is really dense, but I'm not exactly sure what that number is. I think it's around 19.3 grams per cubic centimeter. Yeah, that sounds right. Let me confirm that... I think I remember that gold has a density of about 19.3 g/cm¬≥. Okay, so that's the density.&lt;/p&gt; &lt;p&gt;Now, I have a mass of 1 kg. Wait, I need to make sure the units are consistent because density is mass per volume. So, should I convert kilograms to grams? Yeah, that makes sense. 1 kg is 1000 grams.&lt;/p&gt; &lt;p&gt;So, mass is 1000 grams, and density is 19.3 grams per cm¬≥. So, using the density formula, I can rearrange it to solve for volume:&lt;/p&gt; &lt;p&gt;Volume= Density Mass ‚Äã&lt;/p&gt; &lt;p&gt;Plugging in the numbers:&lt;/p&gt; &lt;p&gt;Volume= 19.3 grams/cm 3&lt;/p&gt; &lt;p&gt;1000 grams ‚Äã&lt;/p&gt; &lt;p&gt;Let me do that division. 1000 divided by 19.3. Hmm, 19.3 goes into 1000 how many times? Let me calculate 19.3 times 50 is 965, which is less than 1000. Then 19.3 times 52 is 1003.6, which is just over 1000. So, it's approximately 52 cm¬≥, maybe around 52.2 cm¬≥ or something.&lt;/p&gt; &lt;p&gt;Wait, actually, let me compute it more accurately. 19.3 multiplied by 52 is 1003.6, which is 3.6 grams over. So, 1000 / 19.3 is approximately 51.88 cm¬≥.&lt;/p&gt; &lt;p&gt;So, the volume is about 51.88 cm¬≥.&lt;/p&gt; &lt;p&gt;Now, since the cube has equal sides, the volume of a cube is side length cubed. So,&lt;/p&gt; &lt;p&gt;Volume=Side 3&lt;/p&gt; &lt;p&gt;Therefore, to find the side length, I need to take the cube root of the volume. So,&lt;/p&gt; &lt;p&gt;Side= 3&lt;/p&gt; &lt;p&gt;Volume ‚Äã&lt;/p&gt; &lt;p&gt;Plugging in the volume:&lt;/p&gt; &lt;p&gt;Side= 3&lt;/p&gt; &lt;p&gt;51.88 cm 3&lt;/p&gt; &lt;p&gt;‚Äã&lt;/p&gt; &lt;p&gt;Hmm, cube root of 51.88. Let me think about this. 3¬≥ is 27, 4¬≥ is 64, so it's somewhere between 3 and 4 cm. Let me calculate it more accurately.&lt;/p&gt; &lt;p&gt;I know that 3.7¬≥ is 3.7 * 3.7 * 3.7. 3.7 * 3.7 is 13.69, then 13.69 * 3.7. Let me compute that.&lt;/p&gt; &lt;p&gt;13.69 * 3 is 41.07, and 13.69 * 0.7 is 9.583. Adding them together gives 41.07 + 9.583 = 50.653 cm¬≥. Hmm, that's close to 51.88, but a bit less.&lt;/p&gt; &lt;p&gt;So, 3.7 cm¬≥ gives 50.653 cm¬≥. The volume we have is 51.88 cm¬≥, so it's a bit higher. So, let's try 3.72 cm.&lt;/p&gt; &lt;p&gt;3.72 * 3.72 = ?&lt;/p&gt; &lt;p&gt;First, 3.7 * 3.7 = 13.69, as before. Then, 0.02 * 3.72 is 0.0744, so 3.72 * 3.72 = 13.69 + 0.0744 + 0.0744? Wait, no, that's not correct. Actually, (a + b)¬≤ = a¬≤ + 2ab + b¬≤. So, (3.7 + 0.02)¬≤ = 3.7¬≤ + 23.7 0.02 + 0.02¬≤ = 13.69 + 0.148 + 0.0004 = 13.8384.&lt;/p&gt; &lt;p&gt;Then, 13.8384 * 3.72. Let me compute that.&lt;/p&gt; &lt;p&gt;13.8384 * 3 = 41.5152&lt;/p&gt; &lt;p&gt;13.8384 * 0.7 = 9.68688&lt;/p&gt; &lt;p&gt;13.8384 * 0.02 = 0.276768&lt;/p&gt; &lt;p&gt;Adding them together:&lt;/p&gt; &lt;p&gt;41.5152 + 9.68688 = 51.20208&lt;/p&gt; &lt;p&gt;51.20208 + 0.276768 = 51.478848 cm¬≥&lt;/p&gt; &lt;p&gt;So, 3.72¬≥ is approximately 51.478848 cm¬≥, which is a bit less than 51.88 cm¬≥.&lt;/p&gt; &lt;p&gt;So, let's try 3.73.&lt;/p&gt; &lt;p&gt;3.73 * 3.73 = ?&lt;/p&gt; &lt;p&gt;Again, using the same method:&lt;/p&gt; &lt;p&gt;3.73 is 3.7 + 0.03&lt;/p&gt; &lt;p&gt;So, (3.7 + 0.03)¬≤ = 3.7¬≤ + 23.7 0.03 + 0.03¬≤ = 13.69 + 0.222 + 0.0009 = 13.9129&lt;/p&gt; &lt;p&gt;Then, 13.9129 * 3.73.&lt;/p&gt; &lt;p&gt;Compute step by step:&lt;/p&gt; &lt;p&gt;13.9129 * 3 = 41.7387&lt;/p&gt; &lt;p&gt;13.9129 * 0.7 = 9.73903&lt;/p&gt; &lt;p&gt;13.9129 * 0.03 = 0.417387&lt;/p&gt; &lt;p&gt;Adding them up:&lt;/p&gt; &lt;p&gt;41.7387 + 9.73903 = 51.47773&lt;/p&gt; &lt;p&gt;51.47773 + 0.417387 = 51.895117 cm¬≥&lt;/p&gt; &lt;p&gt;Wow, that's very close to 51.88 cm¬≥. So, 3.73¬≥ is approximately 51.895117 cm¬≥.&lt;/p&gt; &lt;p&gt;Which is a little over 51.88 cm¬≥. So, the exact cube root is just a little less than 3.73 cm. Maybe 3.728 cm?&lt;/p&gt; &lt;p&gt;Wait, perhaps I should use linear approximation or Newton-Raphson method for a better estimate. But this is getting complicated. Maybe I can use a calculator here, but since I don't have one, perhaps I can estimate.&lt;/p&gt; &lt;p&gt;Alternatively, maybe I can use the exact calculation.&lt;/p&gt; &lt;p&gt;But actually, perhaps I can note that 3.72¬≥ is 51.478848, and 3.73¬≥ is 51.895117. Since 51.88 is between them, so the cube root is somewhere in between.&lt;/p&gt; &lt;p&gt;The difference between 51.88 and 51.478848 is 0.401152.&lt;/p&gt; &lt;p&gt;The difference between 51.895117 and 51.478848 is 0.416269.&lt;/p&gt; &lt;p&gt;So, the fraction is 0.401152 / 0.416269 ‚âà 0.962.&lt;/p&gt; &lt;p&gt;So, the cube root is approximately 3.72 + 0.962*(0.001) ‚âà 3.72 + 0.000962 ‚âà 3.720962 cm.&lt;/p&gt; &lt;p&gt;So, approximately 3.721 cm.&lt;/p&gt; &lt;p&gt;Therefore, the side length is approximately 3.721 cm.&lt;/p&gt; &lt;p&gt;But wait, let me double-check my calculations because this seems a bit tedious.&lt;/p&gt; &lt;p&gt;Alternatively, maybe I can use the formula for the volume of a cube and solve for the side length.&lt;/p&gt; &lt;p&gt;Wait, another approach: let's take the cube root of 51.88.&lt;/p&gt; &lt;p&gt;We know that 3.7¬≥ is 50.653, and 3.72¬≥ is 51.4788, which is less than 51.88.&lt;/p&gt; &lt;p&gt;So, 3.72¬≥ = 51.4788&lt;/p&gt; &lt;p&gt;Let me compute 3.72 + delta, where delta is small, such that (3.72 + delta)¬≥ = 51.88&lt;/p&gt; &lt;p&gt;Using the binomial expansion:&lt;/p&gt; &lt;p&gt;(3.72 + delta)&lt;sup&gt;3&lt;/sup&gt; = 3.72¬≥ + 3&lt;em&gt;(3.72)¬≤delta + 3 (3.72)&lt;/em&gt;delta¬≤ + delta¬≥&lt;/p&gt; &lt;p&gt;We can ignore delta squared and delta cubed since delta is small.&lt;/p&gt; &lt;p&gt;So,&lt;/p&gt; &lt;p&gt;(3.72 + delta)&lt;sup&gt;3&lt;/sup&gt; ‚âà 3.72¬≥ + 3*(3.72)¬≤delta = 51.4788 + 3 (13.8384)delta = 51.4788 + 41.5152 delta&lt;/p&gt; &lt;p&gt;Set this equal to 51.88:&lt;/p&gt; &lt;p&gt;51.4788 + 41.5152*delta = 51.88&lt;/p&gt; &lt;p&gt;Subtract 51.4788:&lt;/p&gt; &lt;p&gt;41.5152*delta = 0.4012&lt;/p&gt; &lt;p&gt;So,&lt;/p&gt; &lt;p&gt;delta ‚âà 0.4012 / 41.5152 ‚âà 0.00962&lt;/p&gt; &lt;p&gt;So, delta ‚âà 0.00962 cm.&lt;/p&gt; &lt;p&gt;Therefore, the cube root is approximately 3.72 + 0.00962 ‚âà 3.72962 cm.&lt;/p&gt; &lt;p&gt;So, approximately 3.73 cm.&lt;/p&gt; &lt;p&gt;But wait, when I cube 3.73, I get 51.895 cm¬≥, which is just a bit over 51.88. So, actually, the cube root is slightly less than 3.73. Let me denote delta as negative.&lt;/p&gt; &lt;p&gt;So,&lt;/p&gt; &lt;p&gt;(3.72 + delta)&lt;sup&gt;3&lt;/sup&gt; = 51.88&lt;/p&gt; &lt;p&gt;We found that 3.72¬≥ is 51.4788, and 3.73¬≥ is 51.8951.&lt;/p&gt; &lt;p&gt;So, 51.88 is 51.8951 - 0.0151 above 3.73¬≥.&lt;/p&gt; &lt;p&gt;So, delta is such that:&lt;/p&gt; &lt;p&gt;51.4788 + 41.5152*delta = 51.88&lt;/p&gt; &lt;p&gt;So, 41.5152*delta = 51.88 - 51.4788 = 0.4012&lt;/p&gt; &lt;p&gt;Therefore,&lt;/p&gt; &lt;p&gt;delta = 0.4012 / 41.5152 ‚âà 0.00962&lt;/p&gt; &lt;p&gt;But since 3.73¬≥ is higher, delta is negative, so delta ‚âà -0.00962 cm.&lt;/p&gt; &lt;p&gt;So, the cube root is approximately 3.72 - 0.00962 ‚âà 3.71038 cm.&lt;/p&gt; &lt;p&gt;Therefore, approximately 3.7104 cm.&lt;/p&gt; &lt;p&gt;So, the side length is approximately 3.71 cm.&lt;/p&gt; &lt;p&gt;Wait, let me verify by cubing 3.71:&lt;/p&gt; &lt;p&gt;3.71 * 3.71 = 13.7641&lt;/p&gt; &lt;p&gt;13.7641 * 3.71:&lt;/p&gt; &lt;p&gt;Let me compute 13.7641 * 3 = 41.2923&lt;/p&gt; &lt;p&gt;13.7641 * 0.7 = 9.63487&lt;/p&gt; &lt;p&gt;13.7641 * 0.01 = 0.137641&lt;/p&gt; &lt;p&gt;Adding them together:&lt;/p&gt; &lt;p&gt;41.2923 + 9.63487 = 50.92717&lt;/p&gt; &lt;p&gt;50.92717 + 0.137641 ‚âà 51.06481 cm¬≥&lt;/p&gt; &lt;p&gt;Hmm, 51.06481, which is still less than 51.88. So, 3.71¬≥ is 51.06481 cm¬≥.&lt;/p&gt; &lt;p&gt;So, that's still less than 51.88. Let me try 3.72:&lt;/p&gt; &lt;p&gt;As before, 3.72¬≥ is 51.4788 cm¬≥, which is still less.&lt;/p&gt; &lt;p&gt;3.721¬≥: Let's compute 3.721¬≥.&lt;/p&gt; &lt;p&gt;First, 3.721 * 3.721:&lt;/p&gt; &lt;p&gt;3.721 * 3.721&lt;/p&gt; &lt;p&gt;Compute 3 * 3.721 = 11.163&lt;/p&gt; &lt;p&gt;0.7 * 3.721 = 2.6047&lt;/p&gt; &lt;p&gt;0.02 * 3.721 = 0.07442&lt;/p&gt; &lt;p&gt;0.001 * 3.721 = 0.003721&lt;/p&gt; &lt;p&gt;So, adding them:&lt;/p&gt; &lt;p&gt;11.163 + 2.6047 = 13.7677&lt;/p&gt; &lt;p&gt;13.7677 + 0.07442 = 13.84212&lt;/p&gt; &lt;p&gt;13.84212 + 0.003721 ‚âà 13.845841&lt;/p&gt; &lt;p&gt;So, 3.721¬≤ ‚âà 13.845841&lt;/p&gt; &lt;p&gt;Now, 13.845841 * 3.721:&lt;/p&gt; &lt;p&gt;Again, break it down:&lt;/p&gt; &lt;p&gt;13 * 3.721 = 48.373&lt;/p&gt; &lt;p&gt;0.845841 * 3.721 ‚âà Let's compute 0.8 * 3.721 = 2.9768&lt;/p&gt; &lt;p&gt;0.04 * 3.721 = 0.14884&lt;/p&gt; &lt;p&gt;0.005841 * 3.721 ‚âà 0.02176&lt;/p&gt; &lt;p&gt;Adding them together:&lt;/p&gt; &lt;p&gt;2.9768 + 0.14884 = 3.12564&lt;/p&gt; &lt;p&gt;3.12564 + 0.02176 ‚âà 3.1474&lt;/p&gt; &lt;p&gt;So, total is 48.373 + 3.1474 ‚âà 51.5204 cm¬≥&lt;/p&gt; &lt;p&gt;So, 3.721¬≥ ‚âà 51.5204 cm¬≥&lt;/p&gt; &lt;p&gt;Which is still less than 51.88. Hmm, so 3.721¬≥ is 51.5204, which is less than 51.88. So, let's try 3.722.&lt;/p&gt; &lt;p&gt;Compute 3.722¬≥:&lt;/p&gt; &lt;p&gt;First, 3.722¬≤:&lt;/p&gt; &lt;p&gt;Again, 3.72¬≤ is 13.8384, so 3.722¬≤ = 13.8384 + 2&lt;em&gt;(0.002)&lt;/em&gt;3.72 + (0.002)&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;Wait, actually, 3.722 is 3.72 + 0.002, so:&lt;/p&gt; &lt;p&gt;(3.72 + 0.002)¬≤ = 3.72¬≤ + 23.72 0.002 + 0.002¬≤ = 13.8384 + 0.01488 + 0.000004 = 13.853284&lt;/p&gt; &lt;p&gt;Then, 13.853284 * 3.722:&lt;/p&gt; &lt;p&gt;Break it down:&lt;/p&gt; &lt;p&gt;13 * 3.722 = 48.386&lt;/p&gt; &lt;p&gt;0.853284 * 3.722 ‚âà Let's compute:&lt;/p&gt; &lt;p&gt;0.8 * 3.722 = 2.9776&lt;/p&gt; &lt;p&gt;0.05 * 3.722 = 0.1861&lt;/p&gt; &lt;p&gt;0.003284 * 3.722 ‚âà 0.01222&lt;/p&gt; &lt;p&gt;Adding them:&lt;/p&gt; &lt;p&gt;2.9776 + 0.1861 = 3.1637&lt;/p&gt; &lt;p&gt;3.1637 + 0.01222 ‚âà 3.1759&lt;/p&gt; &lt;p&gt;Total volume: 48.386 + 3.1759 ‚âà 51.5619 cm¬≥&lt;/p&gt; &lt;p&gt;So, 3.722¬≥ ‚âà 51.5619 cm¬≥, which is less than 51.88.&lt;/p&gt; &lt;p&gt;So, 3.722¬≥ = 51.5619&lt;/p&gt; &lt;p&gt;Difference from 51.88: 51.88 - 51.5619 = 0.3181&lt;/p&gt; &lt;p&gt;So, need to add more.&lt;/p&gt; &lt;p&gt;Let me compute the derivative of the volume with respect to side length to approximate the delta.&lt;/p&gt; &lt;p&gt;V = s¬≥&lt;/p&gt; &lt;p&gt;dV/ds = 3s¬≤&lt;/p&gt; &lt;p&gt;At s = 3.722, dV/ds = 3&lt;em&gt;(3.722)¬≤ ‚âà 3&lt;/em&gt;13.853284 ‚âà 41.55985&lt;/p&gt; &lt;p&gt;So, delta V ‚âà dV/ds * delta = 41.55985 * delta = 0.3181&lt;/p&gt; &lt;p&gt;So, delta ‚âà 0.3181 / 41.55985 ‚âà 0.00765 cm&lt;/p&gt; &lt;p&gt;Therefore, s ‚âà 3.722 + 0.00765 ‚âà 3.72965 cm&lt;/p&gt; &lt;p&gt;So, approximately 3.7297 cm&lt;/p&gt; &lt;p&gt;So, the cube root is approximately 3.7297 cm&lt;/p&gt; &lt;p&gt;Therefore, the side length of the cube is approximately 3.73 cm&lt;/p&gt; &lt;p&gt;To be precise, about 3.73 cm.&lt;/p&gt; &lt;p&gt;Therefore, the edge length of the cube is approximately 3.73 cm.&lt;/p&gt; &lt;p&gt;Let me verify by cubing 3.73:&lt;/p&gt; &lt;p&gt;3.73 * 3.73 = 13.9129&lt;/p&gt; &lt;p&gt;13.9129 * 3.73:&lt;/p&gt; &lt;p&gt;13 * 3.73 = 48.49&lt;/p&gt; &lt;p&gt;0.9129 * 3.73 ‚âà Let's compute:&lt;/p&gt; &lt;p&gt;0.9 * 3.73 = 3.357&lt;/p&gt; &lt;p&gt;0.0129 * 3.73 ‚âà 0.0482&lt;/p&gt; &lt;p&gt;Adding together: 3.357 + 0.0482 ‚âà 3.4052&lt;/p&gt; &lt;p&gt;Total volume: 48.49 + 3.4052 ‚âà 51.8952 cm¬≥&lt;/p&gt; &lt;p&gt;Which is very close to 51.88 cm¬≥.&lt;/p&gt; &lt;p&gt;So, 3.73 cm¬≥ is approximately 51.8952, which is about 51.90, which is very close to 51.88.&lt;/p&gt; &lt;p&gt;Therefore, 3.73 cm is a good approximation.&lt;/p&gt; &lt;p&gt;So, the edge length is approximately 3.73 cm.&lt;/p&gt; &lt;p&gt;Hence, the edge of the cube is approximately 3.73 cm.&lt;/p&gt; &lt;p&gt;Final Answer The edge of the cube is approximately \boxed{3.73} cm. ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zn2e/is_anyone_else_obsessed_with_watching_r1s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zn2e/is_anyone_else_obsessed_with_watching_r1s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zn2e/is_anyone_else_obsessed_with_watching_r1s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T09:27:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1i82zkv</id>
    <title>ChatGPT was down today. So tried a Local LLaMA. Thanks for the advice here!</title>
    <updated>2025-01-23T13:10:23+00:00</updated>
    <author>
      <name>/u/BuyHighSellL0wer</name>
      <uri>https://old.reddit.com/user/BuyHighSellL0wer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLM noob here. I took this as an opportunity to follow the excellent advice that somebody had posted &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/16y95hk/a_starter_guide_for_playing_with_your_own_local_ai/"&gt;here &lt;/a&gt;and &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1c9iawc/is_there_any_idiot_guide_to_running_local_llama/"&gt;here&lt;/a&gt;. Any whilst my computer is a pile of trash I was able to run 'DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf' by merely downloading koboldcpp.exe. What a time to be alive!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BuyHighSellL0wer"&gt; /u/BuyHighSellL0wer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i82zkv/chatgpt_was_down_today_so_tried_a_local_llama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i82zkv/chatgpt_was_down_today_so_tried_a_local_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i82zkv/chatgpt_was_down_today_so_tried_a_local_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T13:10:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i87826</id>
    <title>The R1 Distillation you want is FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview</title>
    <updated>2025-01-23T16:25:31+00:00</updated>
    <author>
      <name>/u/TheActualStudy</name>
      <uri>https://old.reddit.com/user/TheActualStudy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made an exl2 4.25 BPW quantization of FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview, and it functions how I was expecting DeepSeek-R1-Distill-Qwen-32B to have. It does not degrade on multi-turn performance, its instruction following is superior, and the writing results were more closely in line with R1.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview"&gt;HF Link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I know people said this late on Monday already, but it took me until now to get it and test it, so I figured that others may still be struggling with DeepSeek-R1-Distill-Qwen-32B. I, personally, believe it's the new SOTA you were probably expecting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheActualStudy"&gt; /u/TheActualStudy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i87826/the_r1_distillation_you_want_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i87826/the_r1_distillation_you_want_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i87826/the_r1_distillation_you_want_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T16:25:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7nmk5</id>
    <title>NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted</title>
    <updated>2025-01-22T22:15:41+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nmk5/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"&gt; &lt;img alt="NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted" src="https://external-preview.redd.it/4XL5vcWPC90yEEq7VKQk8oDbmqhIMyQmjRfE4cS0j3I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5bf06daf13377cc2784242eaf7086a8def5b1f0" title="NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/nvidia-rtx-blackwell-gpu-with-96gb-gddr7-memory-and-512-bit-bus-spotted?fbclid=IwZXh0bgNhZW0CMTEAAR3i39eJbThbgTnI0Yz4JdnkMXgvj4wlorxOdbBeccw35kkqWqyrG816HpI_aem_EoENoW6h6SP-aU7FVwBWiw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nmk5/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nmk5/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T22:15:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7l8jq</id>
    <title>Elon Musk bashes the $500 billion AI project Trump announced, claiming its backers don‚Äôt ‚Äòhave the money‚Äô</title>
    <updated>2025-01-22T20:35:28+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7l8jq/elon_musk_bashes_the_500_billion_ai_project_trump/"&gt; &lt;img alt="Elon Musk bashes the $500 billion AI project Trump announced, claiming its backers don‚Äôt ‚Äòhave the money‚Äô" src="https://external-preview.redd.it/eSsDN6qn9Vf3av8njm2eDT1t4GaTyvUyo2U0_KTezCE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258806c53694caece33676c65dfce38429cd8859" title="Elon Musk bashes the $500 billion AI project Trump announced, claiming its backers don‚Äôt ‚Äòhave the money‚Äô" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnn.com/2025/01/22/tech/elon-musk-trump-stargate-openai/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7l8jq/elon_musk_bashes_the_500_billion_ai_project_trump/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7l8jq/elon_musk_bashes_the_500_billion_ai_project_trump/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T20:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i84uew</id>
    <title>Is this the key methodology that Anthropic used to make Claude so good?</title>
    <updated>2025-01-23T14:41:33+00:00</updated>
    <author>
      <name>/u/takuonline</name>
      <uri>https://old.reddit.com/user/takuonline</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.anthropic.com/research/claude-character"&gt;This article&lt;/a&gt; is a bit old(June 8 2024), but this seems like it has the biggest contribution to what makes Claude sonnet 3.5 so good and open models could benefit from similar training.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Claude 3 was the first model where we added &amp;quot;character training&amp;quot; to our alignment finetuning process: the part of training that occurs after initial model training, and the part that turns it from a predictive text model into an AI assistant. The goal of character training is to make Claude begin to have more nuanced, richer traits like curiosity, open-mindedness, and thoughtfulness.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I haven't heard people talk about this a lot and l feel like it should be talked about a lot more. Especially the traits that they shared in the last sentence (&amp;quot;more nuanced, richer traits like curiosity, open-mindedness, and thoughtfulness&amp;quot;) really make the model, particular if you are looking for something that you can have a great conversation with.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/takuonline"&gt; /u/takuonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i84uew/is_this_the_key_methodology_that_anthropic_used/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i84uew/is_this_the_key_methodology_that_anthropic_used/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i84uew/is_this_the_key_methodology_that_anthropic_used/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T14:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i87boz</id>
    <title>SmolVLM goes even smaller, running on your toaster ü•µüî• Today Hugging Face release newer SmolVLMs: 256M and 500M</title>
    <updated>2025-01-23T16:29:43+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i87boz/smolvlm_goes_even_smaller_running_on_your_toaster/"&gt; &lt;img alt="SmolVLM goes even smaller, running on your toaster ü•µüî• Today Hugging Face release newer SmolVLMs: 256M and 500M" src="https://external-preview.redd.it/ZWkxc3lkdXRzcmVlMeOQ6gi0MASgTgbv3hzqEbPqGdFSlYbcrJRLih99qg-l.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=617c7797fb5d38c550d1817560f4884cecce1540" title="SmolVLM goes even smaller, running on your toaster ü•µüî• Today Hugging Face release newer SmolVLMs: 256M and 500M" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6d3hweutsree1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i87boz/smolvlm_goes_even_smaller_running_on_your_toaster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i87boz/smolvlm_goes_even_smaller_running_on_your_toaster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T16:29:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i821n9</id>
    <title>Introducing Salt: A New Approach to Speech Generation Built on LLMs (+ Open-Sourced Code!)</title>
    <updated>2025-01-23T12:17:47+00:00</updated>
    <author>
      <name>/u/Mysterious_Hearing14</name>
      <uri>https://old.reddit.com/user/Mysterious_Hearing14</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;We‚Äôre excited to share &lt;strong&gt;Salt&lt;/strong&gt;, a speech generation project we‚Äôve been working on since August. Here‚Äôs a quick dive into its journey, technical details, and open resources! &lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;&lt;strong&gt;Development Timeline&lt;/strong&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;August 2024&lt;/strong&gt;: Started prototyping.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Late August&lt;/strong&gt;: First working model (spoiler: it was rough).&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Final Approach&lt;/strong&gt;: Instead of traditional transformer-based TTS, we extended a pre-trained LLM (Llama3 3B) with a &lt;strong&gt;custom tokenizer&lt;/strong&gt; (WavTokenizer for audio) and trained it end-to-end for &lt;strong&gt;TTS&lt;/strong&gt; and &lt;strong&gt;ASR&lt;/strong&gt;.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;&lt;strong&gt;Key Details&lt;/strong&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Training&lt;/strong&gt;: ~150 A100 GPU hours.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Architecture&lt;/strong&gt;: Think ‚ÄúDALL-E 1 for speech‚Äù ‚Äì autoregressive generation using a modified LLM backbone.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It‚Äôs Unique&lt;/strong&gt;: Unlike adapter-based models or pure TTS frameworks, Salt leverages LLM capabilities while handling audio tokens natively, also voice is prompt controllable.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;&lt;strong&gt;Current State&lt;/strong&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Competitive SIM-O scores (~0.63) against models like Mini Omni and Llama Omni (see &lt;a href="https://github.com/VikhrModels/aulate"&gt;metrics&lt;/a&gt;).&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Future Work&lt;/strong&gt;: Fine-tuning for &lt;strong&gt;music/audio generation&lt;/strong&gt;! Paper and updates coming soon.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;&lt;strong&gt;Open Resources&lt;/strong&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model&lt;/strong&gt;: &lt;a href="https://huggingface.co/Vikhrmodels/salt-asr_wav-uni_1_tts_wav-uni_1-12k"&gt;Hugging Face&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference Demo&lt;/strong&gt;: &lt;a href="https://colab.research.google.com/drive/1Fbv1-4iBKSymgzAF2b1A4aCP9ajI4USL?usp=sharing"&gt;Google Colab&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full Training Code&lt;/strong&gt;: &lt;a href="https://github.com/VikhrModels/Salt"&gt;GitHub&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Metrics Toolkit&lt;/strong&gt;: &lt;a href="https://github.com/VikhrModels/aulate"&gt;Aulate&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Hearing14"&gt; /u/Mysterious_Hearing14 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i821n9/introducing_salt_a_new_approach_to_speech/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i821n9/introducing_salt_a_new_approach_to_speech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i821n9/introducing_salt_a_new_approach_to_speech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T12:17:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i81ev6</id>
    <title>DeepSeek added recommandations for R1 local use to model card</title>
    <updated>2025-01-23T11:36:56+00:00</updated>
    <author>
      <name>/u/Snail_Inference</name>
      <uri>https://old.reddit.com/user/Snail_Inference</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B#usage-recommendations"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B#usage-recommendations&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;1, Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Avoid adding a system prompt; all instructions should be contained within the user prompt.&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;For mathematical problems, it is advisable to include a directive in your prompt such as: &amp;quot;Please reason step by step, and put your final answer within \boxed{}.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;When evaluating model performance, it is recommended to conduct multiple tests and average the results.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snail_Inference"&gt; /u/Snail_Inference &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81ev6/deepseek_added_recommandations_for_r1_local_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81ev6/deepseek_added_recommandations_for_r1_local_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i81ev6/deepseek_added_recommandations_for_r1_local_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T11:36:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i867k8</id>
    <title>First 5090 LLM results, compared to 4090 and 6000 ada</title>
    <updated>2025-01-23T15:42:34+00:00</updated>
    <author>
      <name>/u/jwestra</name>
      <uri>https://old.reddit.com/user/jwestra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i867k8/first_5090_llm_results_compared_to_4090_and_6000/"&gt; &lt;img alt="First 5090 LLM results, compared to 4090 and 6000 ada" src="https://external-preview.redd.it/PnzUTHeUQDah3Madq3JF5tCDBdEWLySpwcBRwh4t1-o.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a5868e7a86ac26107edab60570277f4095ed053" title="First 5090 LLM results, compared to 4090 and 6000 ada" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source:&lt;br /&gt; &lt;a href="https://www.storagereview.com/review/nvidia-geforce-rtx-5090-review-pushing-boundaries-with-ai-acceleration"&gt;https://www.storagereview.com/review/nvidia-geforce-rtx-5090-review-pushing-boundaries-with-ai-acceleration&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/viyw7xmqkree1.png?width=1554&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=337b84f0d6c0110a49bec512c785fb7d3bc61bb4"&gt;https://preview.redd.it/viyw7xmqkree1.png?width=1554&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=337b84f0d6c0110a49bec512c785fb7d3bc61bb4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mumd1wmqkree1.png?width=1608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=318bb83f4539fe28fd903026a2ad07ba033f9073"&gt;https://preview.redd.it/mumd1wmqkree1.png?width=1608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=318bb83f4539fe28fd903026a2ad07ba033f9073&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jwestra"&gt; /u/jwestra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i867k8/first_5090_llm_results_compared_to_4090_and_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i867k8/first_5090_llm_results_compared_to_4090_and_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i867k8/first_5090_llm_results_compared_to_4090_and_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T15:42:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i81pbk</id>
    <title>This merge is amazing: FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview</title>
    <updated>2025-01-23T11:56:19+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview"&gt;https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This model is a Fusion(merge?) of DeepSeekR1 &amp;amp; QwQ &amp;amp; SkyT1 32B models.&lt;/p&gt; &lt;p&gt;I usually don‚Äôt use merged models because, most of the time, I can‚Äôt find any noticeable differences. However, this one is truly different. Here are some examples:&lt;/p&gt; &lt;p&gt;My test setup is ollama + IQ4_XS + 32k context + Q8_0 KV Cache, questions are from: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i65599/r1_32b_is_be_worse_than_qwq_32b_tests_included/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1i65599/r1_32b_is_be_worse_than_qwq_32b_tests_included/&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Hello! I have multiple different files with different sizes, I want to move files from disk 1 to disk 2, which has only 688 space available. Without yapping, and being as concise as possible. What combination of files gets me closer to that number? The file sizes are: 36, 36, 49, 53, 54, 54, 63, 94, 94, 107, 164, 201, 361, 478&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;FuseO1-DeekSeekR1-QwQ-SkyT1-32B-Preview:&lt;/p&gt; &lt;p&gt;full answer with thoughts: &lt;a href="https://pastebin.com/W2QFACDM"&gt;https://pastebin.com/W2QFACDM&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;To maximize the use of disk 2's 688-unit capacity, here are the optimal file combinations: 1. **478 + 53 + 54 + 54 + 49 = 688** 2. **361 + 201 + 36 + 36 + 54 = 688** 3. **478 + 107 + 49 + 54 = 688** 4. **361 + 107 + 49 + 54 + 54 + 63 = 688** Each combination precisely fills the available space on disk 2. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;This is first time I saw a local model give me 4 correct answers for this question, it's amazing&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;DeepSeek-R1-Distill-Qwen-32B:&lt;/p&gt; &lt;p&gt;full answer with thoughts: &lt;a href="https://pastebin.com/2VJrjVmJ"&gt;https://pastebin.com/2VJrjVmJ&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The combination of files that gets you closest to the 688 space limit is: 361 + 164 + 107 + 54 + 54 = 686 This totals 686 units, which is just 2 units less than the available space on disk 2. &lt;/code&gt;&lt;/pre&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;How many days are between 12-12-1971 and 18-4-2024?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;FuseO1-DeekSeekR1-QwQ-SkyT1-32B-Preview:&lt;/p&gt; &lt;p&gt;full answer with thoughts: &lt;a href="https://pastebin.com/PQGr1DDq"&gt;https://pastebin.com/PQGr1DDq&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The number of days between December 12, 1971, and April 18, 2024, is 19,121 days . This calculation includes the full years, accounts for leap years, and considers the days in the partial years, providing the difference between the two dates. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;which is correct answer, 19121 days&lt;/p&gt; &lt;p&gt;DeepSeek-R1-Distill-Qwen-32B:&lt;/p&gt; &lt;p&gt;full answer with thoughts: &lt;a href="https://pastebin.com/T0iNuYHf"&gt;https://pastebin.com/T0iNuYHf&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;### **Final Answer** \[ \boxed{19,486} \] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;3.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;I have a boat with 3 free spaces. I want to transport a man, sheep and cat on the other side of the river. How to do that? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;FuseO1-DeekSeekR1-QwQ-SkyT1-32B-Preview:&lt;/p&gt; &lt;p&gt;full answer with thoughts: &lt;a href="https://pastebin.com/MZZ5nDiM"&gt;https://pastebin.com/MZZ5nDiM&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;To transport the man, sheep, and cat across the river, the solution depends on the boat's capacity. Given that the boat has &amp;quot;3 free spaces,&amp;quot; it can carry three items in total, including the man. Therefore, the man can take all three (himself, the sheep, and the cat) across the river in one trip. Here's the step-by-step solution: 1. **First Trip**: The man boards the boat with the sheep and the cat. 2. **Crossing the River**: The man rows the boat across the river to the other side. 3. **Arrival**: Upon reaching the other side, the man, sheep, and cat all disembark from the boat. This completes the task as all three are now on the opposite side of the river in a single trip. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;which is the correct answer, they only need to cross the river once&lt;/p&gt; &lt;p&gt;DeepSeek-R1-Distill-Qwen-32B&lt;/p&gt; &lt;p&gt;full answer with thoughts: &lt;a href="https://pastebin.com/XHQHwQth"&gt;https://pastebin.com/XHQHwQth&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;*fell into an infinite loop, I had to terminate it* &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81pbk/this_merge_is_amazing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81pbk/this_merge_is_amazing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i81pbk/this_merge_is_amazing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T11:56:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7x5nd</id>
    <title>The first performant open-source byte-level model without tokenization has been released. EvaByte is a 6.5B param model that also has multibyte prediction for faster inference (vs similar sized tokenized models)</title>
    <updated>2025-01-23T06:19:53+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x5nd/the_first_performant_opensource_bytelevel_model/"&gt; &lt;img alt="The first performant open-source byte-level model without tokenization has been released. EvaByte is a 6.5B param model that also has multibyte prediction for faster inference (vs similar sized tokenized models)" src="https://preview.redd.it/o28q2pl6roee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbcc9bcc3ec8f998f0c0b948df4117b2f7a6ae30" title="The first performant open-source byte-level model without tokenization has been released. EvaByte is a 6.5B param model that also has multibyte prediction for faster inference (vs similar sized tokenized models)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o28q2pl6roee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x5nd/the_first_performant_opensource_bytelevel_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x5nd/the_first_performant_opensource_bytelevel_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T06:19:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7zeqz</id>
    <title>Nivida just open sourced their long context goodies - 128k context for 50% less memory</title>
    <updated>2025-01-23T09:09:11+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;edited : &lt;del&gt;50%&lt;/del&gt; 35% less memory&lt;/p&gt; &lt;p&gt;If you need long context for RAG, tool use, agents, or just because, Nvidia released a new library to make it super simple.&lt;/p&gt; &lt;p&gt;TLDR: You can get 128k context at 35% less memory&lt;/p&gt; &lt;p&gt;Here's a blog post on everything: &lt;a href="https://huggingface.co/blog/nvidia/kvpress"&gt;https://huggingface.co/blog/nvidia/kvpress&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zeqz/nivida_just_open_sourced_their_long_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zeqz/nivida_just_open_sourced_their_long_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zeqz/nivida_just_open_sourced_their_long_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T09:09:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i81znv</id>
    <title>SmolVLM 256 and 500M. Beating our 80B VLM from 17 months ago!</title>
    <updated>2025-01-23T12:14:25+00:00</updated>
    <author>
      <name>/u/futterneid</name>
      <uri>https://old.reddit.com/user/futterneid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81znv/smolvlm_256_and_500m_beating_our_80b_vlm_from_17/"&gt; &lt;img alt="SmolVLM 256 and 500M. Beating our 80B VLM from 17 months ago!" src="https://external-preview.redd.it/GQobgTxgKDrhJhG-UAnJyEaSqhi-5vBwrdogSL5wNmU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=26b8e0c51d51b02acc52b56c533ba8ea0c229040" title="SmolVLM 256 and 500M. Beating our 80B VLM from 17 months ago!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! &lt;/p&gt; &lt;p&gt;It's Andi from multimodal team at Hugging Face Today we released new SmolVLM: tiny vision language models that come in 256M and 500M parameters!&lt;/p&gt; &lt;p&gt;How did we compress SmolVLM 2.2B even further while making it generalize?&lt;br /&gt; &amp;gt; Smoller image encoder (SigLIP-base 93M)&lt;br /&gt; &amp;gt; More diverse data mixture&lt;br /&gt; &amp;gt; Better efficiency in tokenization for images&lt;/p&gt; &lt;p&gt;We have released base and instruction fine-tuned variants, they work out of the box with transformers and TRL, and we also release a WebGPU demo. &lt;/p&gt; &lt;p&gt;All of them are in this collection: &lt;a href="https://huggingface.co/collections/HuggingFaceTB/smolvlm-256m-and-500m-6791fafc5bb0ab8acc960fb0"&gt;https://huggingface.co/collections/HuggingFaceTB/smolvlm-256m-and-500m-6791fafc5bb0ab8acc960fb0&lt;/a&gt;&lt;br /&gt; Here's a demo: &lt;a href="https://huggingface.co/spaces/HuggingFaceTB/SmolVLM-256M-Demo"&gt;https://huggingface.co/spaces/HuggingFaceTB/SmolVLM-256M-Demo&lt;/a&gt;&lt;br /&gt; Also here's the blog for more in-depth documentation:&lt;a href="https://huggingface.co/blog/smolervlm"&gt;https://huggingface.co/blog/smolervlm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking forward to see what you think!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5viklrh4jqee1.png?width=1866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=464a14c3795f760c96a584570cb454f31b3db066"&gt;https://preview.redd.it/5viklrh4jqee1.png?width=1866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=464a14c3795f760c96a584570cb454f31b3db066&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/futterneid"&gt; /u/futterneid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81znv/smolvlm_256_and_500m_beating_our_80b_vlm_from_17/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81znv/smolvlm_256_and_500m_beating_our_80b_vlm_from_17/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i81znv/smolvlm_256_and_500m_beating_our_80b_vlm_from_17/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T12:14:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1i86e4y</id>
    <title>Scale AI CEO says China has quickly caught the U.S. with the DeepSeek open-source model</title>
    <updated>2025-01-23T15:50:30+00:00</updated>
    <author>
      <name>/u/etherd0t</name>
      <uri>https://old.reddit.com/user/etherd0t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i86e4y/scale_ai_ceo_says_china_has_quickly_caught_the_us/"&gt; &lt;img alt="Scale AI CEO says China has quickly caught the U.S. with the DeepSeek open-source model" src="https://external-preview.redd.it/QaGEWAoaN73yKpJcRFLASUVmy5TY0ehTzhGZuFAVhPY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05b9252fa9a2aa815d8f1c4c41bc8b680d1e4628" title="Scale AI CEO says China has quickly caught the U.S. with the DeepSeek open-source model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/etherd0t"&gt; /u/etherd0t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/01/23/scale-ai-ceo-says-china-has-quickly-caught-the-us-with-deepseek.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i86e4y/scale_ai_ceo_says_china_has_quickly_caught_the_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i86e4y/scale_ai_ceo_says_china_has_quickly_caught_the_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T15:50:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7wcry</id>
    <title>ByteDance dropping an Apache 2.0 licensed 2B, 7B &amp; 72B "reasoning" agent for computer use</title>
    <updated>2025-01-23T05:27:26+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7wcry/bytedance_dropping_an_apache_20_licensed_2b_7b/"&gt; &lt;img alt="ByteDance dropping an Apache 2.0 licensed 2B, 7B &amp;amp; 72B &amp;quot;reasoning&amp;quot; agent for computer use" src="https://external-preview.redd.it/OHh5dzk5NW5pb2VlMQuFHIyWgdnmfbsw0_j6sClcjT-Ye_u4ggCKOXoWw4Cm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7659796eeda19643a224a312f9f87529cedbf79" title="ByteDance dropping an Apache 2.0 licensed 2B, 7B &amp;amp; 72B &amp;quot;reasoning&amp;quot; agent for computer use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ealby85nioee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7wcry/bytedance_dropping_an_apache_20_licensed_2b_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7wcry/bytedance_dropping_an_apache_20_licensed_2b_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T05:27:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i81iim</id>
    <title>Been ages since google released an open model</title>
    <updated>2025-01-23T11:43:36+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81iim/been_ages_since_google_released_an_open_model/"&gt; &lt;img alt="Been ages since google released an open model" src="https://preview.redd.it/fa91scqqdqee1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f78c10935c8984f8f9d17834c7720f182fed482" title="Been ages since google released an open model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fa91scqqdqee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81iim/been_ages_since_google_released_an_open_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i81iim/been_ages_since_google_released_an_open_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T11:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i856wr</id>
    <title>Open-source Deepseek beat not so OpenAI in 'humanity's last exam' !</title>
    <updated>2025-01-23T14:57:59+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i856wr/opensource_deepseek_beat_not_so_openai_in/"&gt; &lt;img alt="Open-source Deepseek beat not so OpenAI in 'humanity's last exam' !" src="https://preview.redd.it/lxwhx4eicree1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6a7b23129dc6a37671b0f77472359990567d0e4" title="Open-source Deepseek beat not so OpenAI in 'humanity's last exam' !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lxwhx4eicree1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i856wr/opensource_deepseek_beat_not_so_openai_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i856wr/opensource_deepseek_beat_not_so_openai_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T14:57:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i80cwf</id>
    <title>deepseek is a side project</title>
    <updated>2025-01-23T10:22:48+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i80cwf/deepseek_is_a_side_project/"&gt; &lt;img alt="deepseek is a side project" src="https://preview.redd.it/zdvrlxahzpee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df808bd8bc2e4ba90db2fdb005eaae092d5d8206" title="deepseek is a side project" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zdvrlxahzpee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i80cwf/deepseek_is_a_side_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i80cwf/deepseek_is_a_side_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T10:22:48+00:00</published>
  </entry>
</feed>
