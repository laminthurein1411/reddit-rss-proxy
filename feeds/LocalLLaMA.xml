<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-16T08:07:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i2ivql</id>
    <title>What's the smallest language model that is helpful as a coding assistant?</title>
    <updated>2025-01-16T06:32:18+00:00</updated>
    <author>
      <name>/u/Physical-Security115</name>
      <uri>https://old.reddit.com/user/Physical-Security115</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The title pretty much explains it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Physical-Security115"&gt; /u/Physical-Security115 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ivql/whats_the_smallest_language_model_that_is_helpful/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ivql/whats_the_smallest_language_model_that_is_helpful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ivql/whats_the_smallest_language_model_that_is_helpful/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T06:32:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i20dka</id>
    <title>Speculative decoding isn't a silver bullet - but it can get you 3x speed-ups</title>
    <updated>2025-01-15T15:49:40+00:00</updated>
    <author>
      <name>/u/mark-lord</name>
      <uri>https://old.reddit.com/user/mark-lord</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i20dka/speculative_decoding_isnt_a_silver_bullet_but_it/"&gt; &lt;img alt="Speculative decoding isn't a silver bullet - but it can get you 3x speed-ups" src="https://external-preview.redd.it/DaucjXMGsNHM-CtmdilC9-Be6MC8V2z4ykjVCgOkTFc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62ca4cb88917f17e7200a6f1c665b5d959713745" title="Speculative decoding isn't a silver bullet - but it can get you 3x speed-ups" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! Quick benchmark today - did this using Exaone-32b-4bit*, running with latest MLX_LM backend using &lt;a href="https://gist.github.com/mark-lord/93a9f53f4f1e230e7bd5828357649f89"&gt;this script&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;No speculative decoding:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1i20dka/video/bqtvz9rah6de1/player"&gt;Prompt: 44.608 tps | Generation: 6.274 tps | Avg power: ~9w | Total energy used: ~400J | Time taken: 48.226s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Speculative decoding:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1i20dka/video/ji82cmcfh6de1/player"&gt;Prompt: 37.170 tps | Generation: 24.140 tps | Avg power: ~13w | Total energy used: ~300J | Time taken: 22.880s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;*Benchmark done using my M1 Max 64gb in low power mode, using Exaone-2.4b-4bit as the draft model with 31 draft tokens&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Prompt processing speed was a little bit slower - dropping by about 20%. Power draw was also higher, even in low power mode. &lt;/p&gt; &lt;p&gt;But the time taken from start-&amp;gt;finish was reduced by 53% overall&lt;br /&gt; (The reduction in time taken means the total energy used was also reduced from 400-&amp;gt;300J.)&lt;/p&gt; &lt;p&gt;Pretty damn good I think üòÑ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mark-lord"&gt; /u/mark-lord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i20dka/speculative_decoding_isnt_a_silver_bullet_but_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i20dka/speculative_decoding_isnt_a_silver_bullet_but_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i20dka/speculative_decoding_isnt_a_silver_bullet_but_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T15:49:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i25jas</id>
    <title>Beating cuBLAS in Single-Precision General Matrix Multiplication</title>
    <updated>2025-01-15T19:27:58+00:00</updated>
    <author>
      <name>/u/salykova</name>
      <uri>https://old.reddit.com/user/salykova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i25jas/beating_cublas_in_singleprecision_general_matrix/"&gt; &lt;img alt="Beating cuBLAS in Single-Precision General Matrix Multiplication" src="https://b.thumbs.redditmedia.com/IOKPnc_BPIVWwt4NejPWJ-vflqaNw99lgiCBjBXb6lk.jpg" title="Beating cuBLAS in Single-Precision General Matrix Multiplication" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A while ago, I shared my article here about optimizing matrix multiplication on CPUs, achieving performance that outpaced NumPy - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1dt3rqc/beating_numpys_matrix_multiplication_in_150_lines/"&gt;Beating NumPy's matrix multiplication in 150 lines of C code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I received positive feedback from your community, and today I'm excited to share my second blog post. This one focuses on a matrix multiplication implementation that outperforms cuBLAS with its (modified?) CUTLASS kernel across a wide range of matrix sizes. The blog delves into benchmarking code on CUDA devices and explains the algorithm's design along with optimization techniques. These include inlined PTX, asynchronous memory copies, double-buffering, avoiding shared memory bank conflicts, and efficient coalesced storage using shared memory. The code is super easy to tweak, so you can customize it for your projects with kernel fusion or just drop it into your libraries as-is. If you have any questions, feel free to comment or send me a direct message - I'd love to hear your feedback and answer any questions you may have! Below, I've included performance comparisons (with both locked and unlocked clocks) against cuBLAS and Simon Boehm‚Äôs highly cited work, which is now integrated into llamafile aka tinyBLAS.&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://salykova.github.io/sgemm-gpu"&gt;https://salykova.github.io/sgemm-gpu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/salykova/sgemm.cu"&gt;https://github.com/salykova/sgemm.cu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y3pgixh4l7de1.png?width=1256&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cce30a78791965503a0fa340646a537de1d57195"&gt;https://preview.redd.it/y3pgixh4l7de1.png?width=1256&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cce30a78791965503a0fa340646a537de1d57195&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b5vqz7c5l7de1.png?width=1253&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2cf2103c8c7426dedfb70deb05d9c54c320e6808"&gt;https://preview.redd.it/b5vqz7c5l7de1.png?width=1253&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2cf2103c8c7426dedfb70deb05d9c54c320e6808&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/salykova"&gt; /u/salykova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i25jas/beating_cublas_in_singleprecision_general_matrix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i25jas/beating_cublas_in_singleprecision_general_matrix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i25jas/beating_cublas_in_singleprecision_general_matrix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T19:27:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i20y53</id>
    <title>Jina releases ReaderLM V2, 1.5B model for HTML-to-Markdown/JSON conversion</title>
    <updated>2025-01-15T16:14:41+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i20y53/jina_releases_readerlm_v2_15b_model_for/"&gt; &lt;img alt="Jina releases ReaderLM V2, 1.5B model for HTML-to-Markdown/JSON conversion" src="https://external-preview.redd.it/OawXnZHfMYYebdHrDHFeKzgBRPwqxQJE51C0bsjvWqk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59c70093e9abc7da1b0d7c8f17972ecb8b87d217" title="Jina releases ReaderLM V2, 1.5B model for HTML-to-Markdown/JSON conversion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/jinaai/ReaderLM-v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i20y53/jina_releases_readerlm_v2_15b_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i20y53/jina_releases_readerlm_v2_15b_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T16:14:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1yuke</id>
    <title>Sakana.ai proposes Transformer-squared - Adaptive AI that adjusts its own weights dynamically and eveolves as it learns</title>
    <updated>2025-01-15T14:41:59+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1yuke/sakanaai_proposes_transformersquared_adaptive_ai/"&gt; &lt;img alt="Sakana.ai proposes Transformer-squared - Adaptive AI that adjusts its own weights dynamically and eveolves as it learns" src="https://external-preview.redd.it/ll0sI2kj9OWJW1iOriHpZm1jSfC278YnLF-jisELKs4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5f30bf0b3bae15b4dee53ba7bd37f2486072c04" title="Sakana.ai proposes Transformer-squared - Adaptive AI that adjusts its own weights dynamically and eveolves as it learns" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Arxiv paper - &lt;a href="https://arxiv.org/abs/2501.06252"&gt;https://arxiv.org/abs/2501.06252&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sakana.ai/transformer-squared/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1yuke/sakanaai_proposes_transformersquared_adaptive_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1yuke/sakanaai_proposes_transformersquared_adaptive_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T14:41:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1rgn9</id>
    <title>New model....</title>
    <updated>2025-01-15T06:29:44+00:00</updated>
    <author>
      <name>/u/Many_SuchCases</name>
      <uri>https://old.reddit.com/user/Many_SuchCases</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1rgn9/new_model/"&gt; &lt;img alt="New model...." src="https://preview.redd.it/curwy8vkq3de1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a262daf4d3df2ddd46d444593d7171ed6352a6c5" title="New model...." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Many_SuchCases"&gt; /u/Many_SuchCases &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/curwy8vkq3de1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1rgn9/new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1rgn9/new_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T06:29:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2iif5</id>
    <title>New model from MiniMax</title>
    <updated>2025-01-16T06:07:02+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-VL-01"&gt;https://huggingface.co/MiniMaxAI/MiniMax-VL-01&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2iif5/new_model_from_minimax/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2iif5/new_model_from_minimax/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2iif5/new_model_from_minimax/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T06:07:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1i20211</id>
    <title>Train 400x faster Static Embedding Models; 2 open models released</title>
    <updated>2025-01-15T15:35:40+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i20211/train_400x_faster_static_embedding_models_2_open/"&gt; &lt;img alt="Train 400x faster Static Embedding Models; 2 open models released" src="https://external-preview.redd.it/7KIBT99WqmowwDguikX7zoXpmvjI60Ua61vkPn6VgEU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c7a6ae7297ed561b65221ae6db7678244b6b8a1" title="Train 400x faster Static Embedding Models; 2 open models released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/static-embeddings"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i20211/train_400x_faster_static_embedding_models_2_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i20211/train_400x_faster_static_embedding_models_2_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T15:35:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2hnwy</id>
    <title>Non-Academic AI/LLM Research Productivity: A Quick Comparative Analysis of 2023 and 2024</title>
    <updated>2025-01-16T05:12:33+00:00</updated>
    <author>
      <name>/u/palindsay</name>
      <uri>https://old.reddit.com/user/palindsay</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the help of AI buddies, tried to produce a approximate summary of AI//LLM research papers published with citations (a semi-signal of quality and importance), here is the approximate results (given nature of the data these number are obviously rough, please consider that before any pedantic commenting ;-), and I am sure I missed several organizations, feel free to highlight those or offer an authoritative source for these numbers:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Company&lt;/th&gt; &lt;th align="left"&gt;Number of Published Research Articles in 2024&lt;/th&gt; &lt;th align="left"&gt;Approximate Total Citations in 2024&lt;/th&gt; &lt;th align="left"&gt;Number of Published Research Articles in 2023&lt;/th&gt; &lt;th align="left"&gt;Approximate Total Citations in 2023&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Google/DeepMind&lt;/td&gt; &lt;td align="left"&gt;120 - 150&lt;/td&gt; &lt;td align="left"&gt;8,000 - 10,000&lt;/td&gt; &lt;td align="left"&gt;100 - 130&lt;/td&gt; &lt;td align="left"&gt;7,000 - 9,000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Microsoft&lt;/td&gt; &lt;td align="left"&gt;80 - 100&lt;/td&gt; &lt;td align="left"&gt;5,000 - 7,000&lt;/td&gt; &lt;td align="left"&gt;70 - 90&lt;/td&gt; &lt;td align="left"&gt;4,000 - 6,000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Meta&lt;/td&gt; &lt;td align="left"&gt;70 - 90&lt;/td&gt; &lt;td align="left"&gt;4,000 - 6,000&lt;/td&gt; &lt;td align="left"&gt;60 - 80&lt;/td&gt; &lt;td align="left"&gt;3,000 - 5,000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenAI&lt;/td&gt; &lt;td align="left"&gt;40 - 60&lt;/td&gt; &lt;td align="left"&gt;10,000 - 15,000&lt;/td&gt; &lt;td align="left"&gt;30 - 50&lt;/td&gt; &lt;td align="left"&gt;8,000 - 12,000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;NVIDIA&lt;/td&gt; &lt;td align="left"&gt;40 - 60&lt;/td&gt; &lt;td align="left"&gt;3,000 - 5,000&lt;/td&gt; &lt;td align="left"&gt;35 - 50&lt;/td&gt; &lt;td align="left"&gt;2,500 - 4,000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IBM/Watson&lt;/td&gt; &lt;td align="left"&gt;30 - 50&lt;/td&gt; &lt;td align="left"&gt;1,000 - 2,000&lt;/td&gt; &lt;td align="left"&gt;25 - 40&lt;/td&gt; &lt;td align="left"&gt;800 - 1,500&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Baidu&lt;/td&gt; &lt;td align="left"&gt;30 - 50&lt;/td&gt; &lt;td align="left"&gt;1,500 - 2,500&lt;/td&gt; &lt;td align="left"&gt;25 - 40&lt;/td&gt; &lt;td align="left"&gt;1,200 - 2,000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Anthropic&lt;/td&gt; &lt;td align="left"&gt;20 - 30&lt;/td&gt; &lt;td align="left"&gt;4,000 - 6,000&lt;/td&gt; &lt;td align="left"&gt;15 - 25&lt;/td&gt; &lt;td align="left"&gt;3,000 - 5,000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hugging Face&lt;/td&gt; &lt;td align="left"&gt;20 - 30&lt;/td&gt; &lt;td align="left"&gt;2,000 - 4,000&lt;/td&gt; &lt;td align="left"&gt;15 - 25&lt;/td&gt; &lt;td align="left"&gt;1,500 - 3,000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Stability AI&lt;/td&gt; &lt;td align="left"&gt;15 - 25&lt;/td&gt; &lt;td align="left"&gt;1,000 - 2,000&lt;/td&gt; &lt;td align="left"&gt;10 - 20&lt;/td&gt; &lt;td align="left"&gt;800 - 1,500&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Cohere&lt;/td&gt; &lt;td align="left"&gt;15 - 25&lt;/td&gt; &lt;td align="left"&gt;1,500 - 3,000&lt;/td&gt; &lt;td align="left"&gt;10 - 20&lt;/td&gt; &lt;td align="left"&gt;1,000 - 2,000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AI21 Labs&lt;/td&gt; &lt;td align="left"&gt;10 - 20&lt;/td&gt; &lt;td align="left"&gt;500 - 1,500&lt;/td&gt; &lt;td align="left"&gt;8 - 18&lt;/td&gt; &lt;td align="left"&gt;400 - 1,200&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Alibaba&lt;/td&gt; &lt;td align="left"&gt;10 - 20&lt;/td&gt; &lt;td align="left"&gt;800 - 1,800&lt;/td&gt; &lt;td align="left"&gt;8 - 18&lt;/td&gt; &lt;td align="left"&gt;700 - 1,500&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral AI&lt;/td&gt; &lt;td align="left"&gt;8 - 12&lt;/td&gt; &lt;td align="left"&gt;1,000 - 2,000&lt;/td&gt; &lt;td align="left"&gt;5 - 10&lt;/td&gt; &lt;td align="left"&gt;800 - 1,500&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Aleph Alpha&lt;/td&gt; &lt;td align="left"&gt;8 - 12&lt;/td&gt; &lt;td align="left"&gt;600 - 1,000&lt;/td&gt; &lt;td align="left"&gt;5 - 10&lt;/td&gt; &lt;td align="left"&gt;500 - 800&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Inflection AI&lt;/td&gt; &lt;td align="left"&gt;5 - 10&lt;/td&gt; &lt;td align="left"&gt;500 - 1,000&lt;/td&gt; &lt;td align="left"&gt;3 - 8&lt;/td&gt; &lt;td align="left"&gt;400 - 800&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Adept&lt;/td&gt; &lt;td align="left"&gt;5 - 10&lt;/td&gt; &lt;td align="left"&gt;400 - 800&lt;/td&gt; &lt;td align="left"&gt;3 - 8&lt;/td&gt; &lt;td align="left"&gt;300 - 700&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MosaicML&lt;/td&gt; &lt;td align="left"&gt;5 - 10&lt;/td&gt; &lt;td align="left"&gt;300 - 700&lt;/td&gt; &lt;td align="left"&gt;3 - 8&lt;/td&gt; &lt;td align="left"&gt;200 - 600&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Groq&lt;/td&gt; &lt;td align="left"&gt;3 - 7&lt;/td&gt; &lt;td align="left"&gt;200 - 600&lt;/td&gt; &lt;td align="left"&gt;2 - 6&lt;/td&gt; &lt;td align="left"&gt;150 - 500&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tenstorrent&lt;/td&gt; &lt;td align="left"&gt;3 - 7&lt;/td&gt; &lt;td align="left"&gt;100 - 300&lt;/td&gt; &lt;td align="left"&gt;2 - 6&lt;/td&gt; &lt;td align="left"&gt;80 - 250&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Abridge&lt;/td&gt; &lt;td align="left"&gt;2 - 5&lt;/td&gt; &lt;td align="left"&gt;100 - 300&lt;/td&gt; &lt;td align="left"&gt;1 - 4&lt;/td&gt; &lt;td align="left"&gt;80 - 200&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Harvey&lt;/td&gt; &lt;td align="left"&gt;2 - 5&lt;/td&gt; &lt;td align="left"&gt;200 - 500&lt;/td&gt; &lt;td align="left"&gt;1 - 4&lt;/td&gt; &lt;td align="left"&gt;150 - 400&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="http://Character.ai"&gt;Character.ai&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1 - 4&lt;/td&gt; &lt;td align="left"&gt;50 - 200&lt;/td&gt; &lt;td align="left"&gt;0 - 3&lt;/td&gt; &lt;td align="left"&gt;30 - 150&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ElevenLabs&lt;/td&gt; &lt;td align="left"&gt;1 - 4&lt;/td&gt; &lt;td align="left"&gt;50 - 200&lt;/td&gt; &lt;td align="left"&gt;0 - 3&lt;/td&gt; &lt;td align="left"&gt;30 - 150&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Jasper&lt;/td&gt; &lt;td align="left"&gt;1 - 4&lt;/td&gt; &lt;td align="left"&gt;50 - 200&lt;/td&gt; &lt;td align="left"&gt;0 - 3&lt;/td&gt; &lt;td align="left"&gt;30 - 150&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Perplexity AI&lt;/td&gt; &lt;td align="left"&gt;1 - 4&lt;/td&gt; &lt;td align="left"&gt;50 - 200&lt;/td&gt; &lt;td align="left"&gt;0 - 3&lt;/td&gt; &lt;td align="left"&gt;30 - 150&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Replit&lt;/td&gt; &lt;td align="left"&gt;1 - 4&lt;/td&gt; &lt;td align="left"&gt;50 - 200&lt;/td&gt; &lt;td align="left"&gt;0 - 3&lt;/td&gt; &lt;td align="left"&gt;30 - 150&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Palantir&lt;/td&gt; &lt;td align="left"&gt;1 - 4&lt;/td&gt; &lt;td align="left"&gt;50 - 200&lt;/td&gt; &lt;td align="left"&gt;0 - 3&lt;/td&gt; &lt;td align="left"&gt;30 - 150&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Writer&lt;/td&gt; &lt;td align="left"&gt;0 - 2&lt;/td&gt; &lt;td align="left"&gt;0 - 50&lt;/td&gt; &lt;td align="left"&gt;0 - 2&lt;/td&gt; &lt;td align="left"&gt;0 - 50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Safe Superintelligence&lt;/td&gt; &lt;td align="left"&gt;0 - 2&lt;/td&gt; &lt;td align="left"&gt;0 - 50&lt;/td&gt; &lt;td align="left"&gt;0 - 2&lt;/td&gt; &lt;td align="left"&gt;0 - 50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;World Labs&lt;/td&gt; &lt;td align="left"&gt;0 - 2&lt;/td&gt; &lt;td align="left"&gt;0 - 50&lt;/td&gt; &lt;td align="left"&gt;0 - 2&lt;/td&gt; &lt;td align="left"&gt;0 - 50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;xAI&lt;/td&gt; &lt;td align="left"&gt;0 - 2&lt;/td&gt; &lt;td align="left"&gt;0 - 50&lt;/td&gt; &lt;td align="left"&gt;0 - 2&lt;/td&gt; &lt;td align="left"&gt;0 - 50&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/palindsay"&gt; /u/palindsay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2hnwy/nonacademic_aillm_research_productivity_a_quick/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2hnwy/nonacademic_aillm_research_productivity_a_quick/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2hnwy/nonacademic_aillm_research_productivity_a_quick/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T05:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i21x7z</id>
    <title>Deepseek is officially available on Android and iOS!</title>
    <updated>2025-01-15T16:56:48+00:00</updated>
    <author>
      <name>/u/Available-Stress8598</name>
      <uri>https://old.reddit.com/user/Available-Stress8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i21x7z/deepseek_is_officially_available_on_android_and/"&gt; &lt;img alt="Deepseek is officially available on Android and iOS!" src="https://preview.redd.it/47xatq2hu6de1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c38b07010ec1163e4765356a3b6c3e3a5dea964" title="Deepseek is officially available on Android and iOS!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available-Stress8598"&gt; /u/Available-Stress8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/47xatq2hu6de1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i21x7z/deepseek_is_officially_available_on_android_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i21x7z/deepseek_is_officially_available_on_android_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T16:56:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1z0ur</id>
    <title>NVIDIA unveils Sana for ultra HD image generation on laptops</title>
    <updated>2025-01-15T14:50:16+00:00</updated>
    <author>
      <name>/u/nate4t</name>
      <uri>https://old.reddit.com/user/nate4t</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nate4t"&gt; /u/nate4t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nvlabs.github.io/Sana/?utm_source=substack&amp;amp;utm_medium=email"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1z0ur/nvidia_unveils_sana_for_ultra_hd_image_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1z0ur/nvidia_unveils_sana_for_ultra_hd_image_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T14:50:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1xqrk</id>
    <title>Finally got my second 3090</title>
    <updated>2025-01-15T13:47:51+00:00</updated>
    <author>
      <name>/u/fizzy1242</name>
      <uri>https://old.reddit.com/user/fizzy1242</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1xqrk/finally_got_my_second_3090/"&gt; &lt;img alt="Finally got my second 3090" src="https://preview.redd.it/3zf958trw5de1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a2de0310b00a316ad7343179f08849b452eeb969" title="Finally got my second 3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any good model recommendations for story writing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fizzy1242"&gt; /u/fizzy1242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3zf958trw5de1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1xqrk/finally_got_my_second_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1xqrk/finally_got_my_second_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T13:47:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2fgc2</id>
    <title>InternLM3 released with Apache License 2.0, What is your experience so far?</title>
    <updated>2025-01-16T03:08:37+00:00</updated>
    <author>
      <name>/u/vansinhu</name>
      <uri>https://old.reddit.com/user/vansinhu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2fgc2/internlm3_released_with_apache_license_20_what_is/"&gt; &lt;img alt="InternLM3 released with Apache License 2.0, What is your experience so far?" src="https://a.thumbs.redditmedia.com/3gZ6YyVXTmwhLOOOd310a2WpnNxy8fcOsfael-HeQ60.jpg" title="InternLM3 released with Apache License 2.0, What is your experience so far?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;InternLM3-8B-Instruct realeased with Apache License 2.0.&lt;/p&gt; &lt;p&gt;-Trained on only 4T tokens, saving more than 75% of the training cost.&lt;br /&gt; -Supports deep thinking for complex reasoning and normal mode for chat.&lt;/p&gt; &lt;p&gt;Chat Web: &lt;a href="https://internlm-chat.intern-ai.org.cn/"&gt;https://internlm-chat.intern-ai.org.cn/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/internlm/internlm3-8b-instruct"&gt;https://huggingface.co/internlm/internlm3-8b-instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gihftk77v9de1.png?width=2229&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=398e771323dfdaf50d2f240528da8d3bc6bbf26b"&gt;https://preview.redd.it/gihftk77v9de1.png?width=2229&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=398e771323dfdaf50d2f240528da8d3bc6bbf26b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qv2cr1w5v9de1.png?width=4096&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ec4d107872d0684216d7ed1d587746c7a59d413"&gt;https://preview.redd.it/qv2cr1w5v9de1.png?width=4096&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ec4d107872d0684216d7ed1d587746c7a59d413&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/22gjo8ucv9de1.png?width=615&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1dca5ea63e2c182ab756b9b937fcb8d10ca24ab8"&gt;https://preview.redd.it/22gjo8ucv9de1.png?width=615&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1dca5ea63e2c182ab756b9b937fcb8d10ca24ab8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vansinhu"&gt; /u/vansinhu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2fgc2/internlm3_released_with_apache_license_20_what_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2fgc2/internlm3_released_with_apache_license_20_what_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2fgc2/internlm3_released_with_apache_license_20_what_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T03:08:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i23dhv</id>
    <title>Dell T5820 w/ 2x Dell RTX 3090 for less than $2k - eBay sourced</title>
    <updated>2025-01-15T17:57:35+00:00</updated>
    <author>
      <name>/u/_Boffin_</name>
      <uri>https://old.reddit.com/user/_Boffin_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i23dhv/dell_t5820_w_2x_dell_rtx_3090_for_less_than_2k/"&gt; &lt;img alt="Dell T5820 w/ 2x Dell RTX 3090 for less than $2k - eBay sourced" src="https://preview.redd.it/qi354b2457de1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=189fed98ad2b787956105b481e3545ba80e093c7" title="Dell T5820 w/ 2x Dell RTX 3090 for less than $2k - eBay sourced" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_Boffin_"&gt; /u/_Boffin_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qi354b2457de1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i23dhv/dell_t5820_w_2x_dell_rtx_3090_for_less_than_2k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i23dhv/dell_t5820_w_2x_dell_rtx_3090_for_less_than_2k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T17:57:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1x1mm</id>
    <title>Flow charts, flow charts everywhere</title>
    <updated>2025-01-15T13:10:56+00:00</updated>
    <author>
      <name>/u/AnotherSoftEng</name>
      <uri>https://old.reddit.com/user/AnotherSoftEng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1x1mm/flow_charts_flow_charts_everywhere/"&gt; &lt;img alt="Flow charts, flow charts everywhere" src="https://preview.redd.it/su32pem6q5de1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48d3c26cb427054e3de9f38bb7fa6f4ea73f3686" title="Flow charts, flow charts everywhere" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnotherSoftEng"&gt; /u/AnotherSoftEng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/su32pem6q5de1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1x1mm/flow_charts_flow_charts_everywhere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1x1mm/flow_charts_flow_charts_everywhere/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T13:10:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1xbv1</id>
    <title>OuteTTS 0.3: New 1B &amp; 500M Models</title>
    <updated>2025-01-15T13:26:15+00:00</updated>
    <author>
      <name>/u/OuteAI</name>
      <uri>https://old.reddit.com/user/OuteAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1xbv1/outetts_03_new_1b_500m_models/"&gt; &lt;img alt="OuteTTS 0.3: New 1B &amp;amp; 500M Models" src="https://external-preview.redd.it/MnR2a241bWpzNWRlMS8HG7_sP5Xscyq5qRLwQkOnJIWAwD3-JkIhoicGw7Ke.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=30902d1d9fa313454bf58da04ab6d0ae30505a12" title="OuteTTS 0.3: New 1B &amp;amp; 500M Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OuteAI"&gt; /u/OuteAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rb1px5mjs5de1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1xbv1/outetts_03_new_1b_500m_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1xbv1/outetts_03_new_1b_500m_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T13:26:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1i21u4x</id>
    <title>‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ Would not buy again</title>
    <updated>2025-01-15T16:53:05+00:00</updated>
    <author>
      <name>/u/MoffKalast</name>
      <uri>https://old.reddit.com/user/MoffKalast</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i21u4x/would_not_buy_again/"&gt; &lt;img alt="‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ Would not buy again" src="https://preview.redd.it/rmea76m6s6de1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=013fbcd3bc5ce9ff62b442bfa22ea1b33a661040" title="‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ Would not buy again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoffKalast"&gt; /u/MoffKalast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rmea76m6s6de1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i21u4x/would_not_buy_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i21u4x/would_not_buy_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T16:53:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2g0q5</id>
    <title>New function calling benchmark shows Pythonic approach outperforms JSON (DPAB-Œ±)</title>
    <updated>2025-01-16T03:38:23+00:00</updated>
    <author>
      <name>/u/emanuilov</name>
      <uri>https://old.reddit.com/user/emanuilov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new benchmark (DPAB-Œ±) has been released that evaluates LLM function calling in both Pythonic and JSON approaches. It demonstrates that Pythonic function calling often outperforms traditional JSON-based methods, especially for complex multi-step tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key findings from benchmarks:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Claude 3.5 Sonnet leads with 87% on Pythonic vs 45% on JSON&lt;/li&gt; &lt;li&gt;Smaller models show impressive results (Dria-Agent-Œ±-3B: 72% Pythonic)&lt;/li&gt; &lt;li&gt;Even larger models like DeepSeek V3 (685B) show significant gaps (63% Pythonic vs 33% JSON)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Benchmark: &lt;a href="https://github.com/firstbatchxyz/function-calling-eval"&gt;https://github.com/firstbatchxyz/function-calling-eval&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://huggingface.co/blog/andthattoo/dpab-a"&gt;https://huggingface.co/blog/andthattoo/dpab-a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Not affiliated with the project, just sharing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emanuilov"&gt; /u/emanuilov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2g0q5/new_function_calling_benchmark_shows_pythonic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2g0q5/new_function_calling_benchmark_shows_pythonic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2g0q5/new_function_calling_benchmark_shows_pythonic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T03:38:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i28pfq</id>
    <title>UMbreLLa: Llama3.3-70B INT4 on RTX 4070Ti Achieving up to 9.6 Tokens/s! üöÄ</title>
    <updated>2025-01-15T21:45:42+00:00</updated>
    <author>
      <name>/u/Otherwise_Respect_22</name>
      <uri>https://old.reddit.com/user/Otherwise_Respect_22</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i28pfq/umbrella_llama3370b_int4_on_rtx_4070ti_achieving/"&gt; &lt;img alt="UMbreLLa: Llama3.3-70B INT4 on RTX 4070Ti Achieving up to 9.6 Tokens/s! üöÄ" src="https://external-preview.redd.it/26QQz0CLDmbOepvqEr2GHQPtKxnRN6Ls42dwfYRkZX0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a39bedabfd09f0d001c7b69b76443f91d59ef8fd" title="UMbreLLa: Llama3.3-70B INT4 on RTX 4070Ti Achieving up to 9.6 Tokens/s! üöÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;UMbreLLa: Unlocking Llama3.3-70B Performance on Consumer GPUs&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Have you ever imagined running &lt;strong&gt;70B models&lt;/strong&gt; on a consumer GPU at blazing-fast speeds? With &lt;strong&gt;UMbreLLa&lt;/strong&gt;, it's now a reality! Here's what it delivers:&lt;/p&gt; &lt;p&gt;üéØ &lt;strong&gt;Inference Speeds:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;1 x RTX 4070 Ti&lt;/strong&gt;: Up to &lt;strong&gt;9.7 tokens/sec&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1 x RTX 4090&lt;/strong&gt;: Up to &lt;strong&gt;11.4 tokens/sec&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚ú® &lt;strong&gt;What makes it possible?&lt;/strong&gt;&lt;br /&gt; UMbreLLa combines &lt;strong&gt;parameter&lt;/strong&gt; &lt;strong&gt;offloading&lt;/strong&gt;, &lt;strong&gt;speculative decoding&lt;/strong&gt;, and &lt;strong&gt;quantization (AWQ Q4)&lt;/strong&gt;, perfectly tailored for single-user LLM deployment scenarios.&lt;/p&gt; &lt;p&gt;üíª &lt;strong&gt;Why does it matter?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Run &lt;strong&gt;70B models&lt;/strong&gt; on &lt;strong&gt;affordable hardware&lt;/strong&gt; with near-human responsiveness.&lt;/li&gt; &lt;li&gt;Expertly optimized for &lt;strong&gt;coding tasks&lt;/strong&gt; and beyond.&lt;/li&gt; &lt;li&gt;Consumer GPUs finally punching above their weight for high-end LLM inference!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Whether you‚Äôre a developer, researcher, or just an AI enthusiast, this tech transforms how we think about personal AI deployment.&lt;/p&gt; &lt;p&gt;What do you think? Could UMbreLLa be the game-changer we've been waiting for? Let me know your thoughts!&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/Infini-AI-Lab/UMbreLLa"&gt;https://github.com/Infini-AI-Lab/UMbreLLa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;#AI #LLM #RTX4070Ti #RTX4090 #TechInnovation&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1i28pfq/video/pp4qhsu4w8de1/player"&gt;Run UMbreLLa on RTX 4070Ti&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otherwise_Respect_22"&gt; /u/Otherwise_Respect_22 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i28pfq/umbrella_llama3370b_int4_on_rtx_4070ti_achieving/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i28pfq/umbrella_llama3370b_int4_on_rtx_4070ti_achieving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i28pfq/umbrella_llama3370b_int4_on_rtx_4070ti_achieving/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T21:45:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2b2eo</id>
    <title>Meta Prompts - Because Your LLM Can Do Better Than Hello World</title>
    <updated>2025-01-15T23:30:41+00:00</updated>
    <author>
      <name>/u/Pyros-SD-Models</name>
      <uri>https://old.reddit.com/user/Pyros-SD-Models</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alright, fasten your seatbelts. We're taking a ride through meta-prompting land.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;:&lt;br /&gt; &lt;a href="https://streamable.com/vsgcks"&gt;https://streamable.com/vsgcks&lt;/a&gt; We create this by just using two prompts, and what you see in the video isn't even 1/6th of everything. It's just boring to watch 10 minutes of scrolling. With just two prompts we deconstruct an arbitrary complex project into such small parts even LLMs can do it&lt;/p&gt; &lt;p&gt;Default meta prompt collection:&lt;br /&gt; &lt;a href="https://gist.github.com/pyros-projects/c77402249b5b45f0a501998870766ae9"&gt;https://gist.github.com/pyros-projects/c77402249b5b45f0a501998870766ae9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Meta prompt collection with prompts creating summaries and context sync (use them when using Cline or other coding assistants):&lt;br /&gt; &lt;a href="https://gist.github.com/pyros-projects/f6430df8ac6f1ac37e5cfb6a8302edcf"&gt;https://gist.github.com/pyros-projects/f6430df8ac6f1ac37e5cfb6a8302edcf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How to use them:&lt;br /&gt; &lt;a href="https://gist.github.com/pyros-projects/e2c96b57ac7883076cca7bc3dc7ff527"&gt;https://gist.github.com/pyros-projects/e2c96b57ac7883076cca7bc3dc7ff527&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Even if it's mostly about o1 and similar reasoning models everything can also be applied to any other LLM&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;A Quick History of Meta-Prompts&lt;/h2&gt; &lt;p&gt;Meta-prompts originated from &lt;a href="https://arxiv.org/pdf/2401.12954"&gt;this paper&lt;/a&gt;, written by a guy at an indie research lab and another guy from a college with a cactus garden. Back then, everyone was obsessed with role-playing prompts like:&lt;br /&gt; &lt;em&gt;‚ÄúYou are an expert software engineer‚Ä¶‚Äù&lt;/em&gt;&lt;/p&gt; &lt;p&gt;These two geniuses thought after eating some juicy cacti from the garden: &lt;em&gt;‚ÄúWhat if the LLM came up with its own expert prompt and decided what kind of expert to role-play?‚Äù&lt;/em&gt; The result? The first meta-prompt was born.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;The very first meta prompt&lt;/h3&gt; &lt;p&gt;You are Meta-Expert, an extremely clever expert with the unique ability to collaborate with multiple experts (such as Expert Problem Solver, Expert Mathematician, Expert Essayist, etc.) to tackle any task and solve complex problems. Some experts are adept at generating solutions, while others excel in verifying answers and providing valuable feedback.&lt;/p&gt; &lt;p&gt;You also have special access to Expert Python, which has the unique ability to generate and execute Python code given natural-language instructions. Expert Python is highly capable of crafting code to perform complex calculations when provided with clear and precise directions. It is especially useful for computational tasks.&lt;/p&gt; &lt;p&gt;As Meta-Expert, your role is to oversee the communication between the experts, effectively utilizing their skills to answer questions while applying your own critical thinking and verification abilities.&lt;/p&gt; &lt;p&gt;To communicate with an expert, type its name (e.g., &amp;quot;Expert Linguist&amp;quot; or &amp;quot;Expert Puzzle Solver&amp;quot;), followed by a colon &lt;code&gt;:&lt;/code&gt;, and then provide detailed instructions enclosed within triple quotes. For example:&lt;/p&gt; &lt;p&gt;&lt;code&gt; Expert Mathematician: &amp;quot;&amp;quot;&amp;quot; You are a mathematics expert specializing in geometry and algebra. Compute the Euclidean distance between the points (-2, 5) and (3, 7). &amp;quot;&amp;quot;&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Ensure that your instructions are clear and unambiguous, including all necessary information within the triple quotes. You can also assign personas to the experts (e.g., &amp;quot;You are a physicist specialized in...&amp;quot;).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Guidelines:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Interact with only one expert at a time, breaking complex problems into smaller, solvable tasks if needed.&lt;/li&gt; &lt;li&gt;Each interaction is treated as an isolated event, so always provide complete details in every call.&lt;/li&gt; &lt;li&gt;If a mistake is found in an expert's solution, request another expert to review, compare solutions, and provide feedback. You can also request an expert to redo their calculations using input from others.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Important Notes:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;All experts, except yourself, have no memory. Always provide full context when contacting them.&lt;/li&gt; &lt;li&gt;Experts may occasionally make errors. Seek multiple opinions or independently verify solutions if uncertain.&lt;/li&gt; &lt;li&gt;Before presenting a final answer, consult an expert for confirmation. Ideally, verify the final solution with two independent experts.&lt;/li&gt; &lt;li&gt;Aim to resolve each query within 15 rounds or fewer.&lt;/li&gt; &lt;li&gt;Avoid repeating identical questions to experts. Carefully examine responses and seek clarification when needed.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Final Answer Format:&lt;/strong&gt; Present your final answer in the following format:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;FINAL ANSWER: &amp;quot;&amp;quot;&amp;quot; [final answer] &amp;quot;&amp;quot;&amp;quot; ```&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;For multiple-choice questions, select only one option. Each question has a unique answer, so analyze the information thoroughly to determine the most accurate and appropriate response. Present only one solution if multiple options are available.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;The idea was simple but brilliant: you‚Äôd give the LLM this meta-prompt, execute it, append the answers to the context, and repeat until it had everything it needed.&lt;/p&gt; &lt;p&gt;Compared to other prompting strategies, meta-prompts outperform many of them:&lt;/p&gt; &lt;p&gt;![[&lt;a href="https://imgur.com/a/Smd0i1m%5D"&gt;https://imgur.com/a/Smd0i1m]&lt;/a&gt;]&lt;/p&gt; &lt;p&gt;If you‚Äôre curious, you can check out &lt;a href="https://github.com/suzgunmirac/meta-prompting"&gt;Meta-Prompting on GitHub&lt;/a&gt; for some early examples from the paper. Just keep in mind, this was during the middle ages of LLM research, when prompting was actually still researched. But surprisingly the og meta prompt still holds up and can be quite effective!&lt;/p&gt; &lt;p&gt;Since currently there's a trend toward imprinting prompting strategies directly into LLMs (like CoT reasoning), this might be another approach worth exploring. Will definitely try it out when our server farm has some capacity free.&lt;/p&gt; &lt;h3&gt;The Problem with normal prompts&lt;/h3&gt; &lt;p&gt;Let‚Äôs talk about the galaxy-brain takes I keep hearing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;‚ÄúLLMs are only useful for small code snippets.‚Äù&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;‚ÄúI played around with o1 for an hour and decided it sucks.‚Äù&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why do people think this? Because their prompts are hot garbage, like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;‚ÄúGenerate me an enterprise-level user management app.‚Äù&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;‚ÄúProve this random math theorem.‚Äù&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That‚Äôs it. No context. No structure. No plan. Then they‚Äôre shocked when the result is either vague nonsense or flat-out wrong. Like, have you ever managed an actual project? Do you tell your dev team, &lt;em&gt;‚ÄúWrite me a AAA game. Just figure it out,‚Äù&lt;/em&gt; and expect Baldur's Gate?&lt;/p&gt; &lt;p&gt;No. Absolutely not. But somehow it seems to be expected that LLMs deliver superhuman feats even tho people love to scream out how stupid they are...&lt;/p&gt; &lt;p&gt;Here‚Äôs the truth: &lt;strong&gt;LLMs can absolutely handle enterprise-level complexity. if you prompt them like they‚Äôre part of an actual project team.&lt;/strong&gt; That‚Äôs where meta-prompts come in. They turn chaos into order and give LLMs the context, process, and structure they need to perform like experts. It's basically in-context fine-tuning&lt;/p&gt; &lt;h3&gt;Meta Prompts&lt;/h3&gt; &lt;p&gt;So, if you're a dev or architect looking for a skill that's crazy relevant now and will stay relevant for the next few months (years? who knows), get good at meta-prompts.&lt;/p&gt; &lt;p&gt;I expect that with o3, solution architects won't manage dev teams anymore, they'll spend their days orchestrating meta-prompts. Some of us are already way faster using just o1 Pro than working with actual human devs, and I can't even imagine what a bot with a 2770 ELO on Codeforces will do to the architect-dev relationship.&lt;/p&gt; &lt;p&gt;Now, are meta-prompts trivially easy? Of course not. (Shoutout to my friends yesterday who told me &lt;em&gt;&amp;quot;prompt engineering doesn't exist,&amp;quot;&lt;/em&gt; lol.) They require in-depth knowledge of project management, software architecture, and subject-matter expertise. They have to be custom-tailored to your personal workflow and work quirks. That's the reason I probably saw them being mentioned on reddit like only twice.&lt;/p&gt; &lt;p&gt;But I promise anyone can understand the basics. The rest is experience. Try them out, make them your own, and you'll never look back, because for the first time, you'll actually be using an LLM instead of wasting time with it. Then you have the keys to your own personal prompting wonderland.&lt;/p&gt; &lt;p&gt;This is how probably the smallest completely self-contained meta prompt pipeline looks like which can solve any kind of projects or tasks (at least I couldn't make them smaller the last few days when I was writing this)&lt;/p&gt; &lt;p&gt;&lt;a href="https://gist.github.com/pyros-projects/c77402249b5b45f0a501998870766ae9#file-01_planning-md"&gt;Meta Prompt 01 - Planning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://gist.github.com/pyros-projects/c77402249b5b45f0a501998870766ae9#file-02_prompt_chain-md"&gt;Meta Prompt 02 - Iterative chain prompting&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://gist.github.com/pyros-projects/c77402249b5b45f0a501998870766ae9#file-03_prompt_chain_alt-md"&gt;Meta Prompt 03 - Task selection prompting&lt;/a&gt; (only needed if your LLM doesn't like #2)&lt;/p&gt; &lt;p&gt;What do I mean with pipeline? Well the flow works like this. Give LLM prompt 01. When it's done generating, give it prompt 02. Then you continue giving it prompt 02 until you are done with the project. The prompt forces the LLM to iterate upon itself so to speak.&lt;/p&gt; &lt;p&gt;Here a more detailed &amp;quot;how to&amp;quot;:&lt;br /&gt; &lt;a href="https://gist.github.com/pyros-projects/e2c96b57ac7883076cca7bc3dc7ff527"&gt;https://gist.github.com/pyros-projects/e2c96b57ac7883076cca7bc3dc7ff527&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;How does this work and what makes meta-prompts different?&lt;/h3&gt; &lt;p&gt;Instead of dumping a vague brain dump on the model and hoping for magic, you teach it &lt;em&gt;how to think&lt;/em&gt;. You tell it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;What you want (context)&lt;/strong&gt;&lt;br /&gt; Example: &lt;em&gt;‚ÄúBuild a web app that analyzes GitHub repos and generates AI-ready documentation.‚Äù&lt;/em&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;How to think about it (structure)&lt;/strong&gt;&lt;br /&gt; Example: &lt;em&gt;‚ÄúBreak it into components, define tasks, and create technical specs.‚Äù&lt;/em&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;What to deliver (outputs)&lt;/strong&gt;&lt;br /&gt; Example: &lt;em&gt;‚ÄúA YAML file with architecture, components, and tasks.‚Äù&lt;/em&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Meta-prompts follow a pattern: they define &lt;strong&gt;roles&lt;/strong&gt;, &lt;strong&gt;rules&lt;/strong&gt;, and &lt;strong&gt;deliverables&lt;/strong&gt;. Let‚Äôs break it down with the ones I‚Äôve created for this guide:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Planning Meta-Prompt&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://gist.github.com/pyros-projects/c77402249b5b45f0a501998870766ae9#file-01_planning-md"&gt;https://gist.github.com/pyros-projects/c77402249b5b45f0a501998870766ae9#file-01_planning-md&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;&lt;code&gt;- Role: _You‚Äôre a software architect and technical project planner._ - Rules: Break the project into a comprehensive plan with architecture, components, and tasks. - Deliverables: A structured YAML file with sections like `Project Identity`, `Technical Architecture`, and `Task Breakdown`. - Possible output [https://gist.github.com/pyros-projects/c77402249b5b45f0a501998870766ae9#file-01_planning_output-md](https://gist.github.com/pyros-projects/c77402249b5b45f0a501998870766ae9#file-01_planning_output-md) &lt;/code&gt;&lt;/pre&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Execution Chain Meta-Prompt&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://gist.github.com/pyros-projects/c77402249b5b45f0a501998870766ae9#file-02_prompt_chain-md"&gt;https://gist.github.com/pyros-projects/c77402249b5b45f0a501998870766ae9#file-02_prompt_chain-md&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;&lt;code&gt;- Role: _You‚Äôre an expert at turning plans into actionable chunks._ - Rules: Take the project plan and generate coding prompts and review prompts for each task. - Deliverables: Sequential execution and review prompts, including setup, specs, and criteria. - Possible output: [https://gist.github.com/pyros-projects/c77402249b5b45f0a501998870766ae9#file-02_prompt_chain_potential_output-md](https://gist.github.com/pyros-projects/c77402249b5b45f0a501998870766ae9#file-02_prompt_chain_potential_output-md) &lt;/code&gt;&lt;/pre&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Task Selection Meta-Prompt&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://gist.github.com/pyros-projects/c77402249b5b45f0a501998870766ae9#file-03_prompt_chain_alt-md"&gt;https://gist.github.com/pyros-projects/c77402249b5b45f0a501998870766ae9#file-03_prompt_chain_alt-md&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;&lt;code&gt;- Role: _You‚Äôre a project manager keeping the workflow smooth._ - Rules: Analyze dependencies and select the next task while preserving context. - Deliverables: The next coding and review prompt, complete with rationale and updated state. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Each meta-prompt builds on the last, creating a self-contained workflow where the LLM isn‚Äôt just guessing‚Äîit‚Äôs following a logical progression.&lt;/p&gt; &lt;p&gt;Meta-prompts turn LLMs into software architects, project managers, and developers, all locked inside a little text box. They enable:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Comprehensive technical planning&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Iterative task execution&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Clear rules and quality standards&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Modular, scalable designs&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Meta rules&lt;/h3&gt; &lt;p&gt;Meta-prompts are powerful, but they aren‚Äôt magic. They need &lt;strong&gt;you&lt;/strong&gt; to guide them. Here‚Äôs what to keep in mind:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Context Is Everything.&lt;/strong&gt;&lt;br /&gt; LLMs are like goldfish with a giant whiteboard. They only remember what‚Äôs in their current context. If your plan is messy or missing details, your outputs will be just as bad. Spend the extra time refining your prompts and filling gaps. A good meta prompt is designed to minimize these issues by keeping everything structured.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Modularity Is Key.&lt;/strong&gt;&lt;br /&gt; Good meta-prompts break projects into modular, self-contained pieces. There is the saying &amp;quot;Every project is deconstructable into something a junior dev could implement.&amp;quot; I would go one step further: &amp;quot;Every project is deconstructable into something an LLM could implement.&amp;quot; This isn‚Äôt just a nice-to-have‚Äîit‚Äôs essential. Modularity is not only good practice, it makes things easier! Modularity will abstract difficulty away.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Iterate, Iterate, Iterate.&lt;/strong&gt;&lt;br /&gt; Meta-prompts aren‚Äôt one-and-done. They‚Äôre a living system that you refine as the project evolves. Didn‚Äôt like the YAML output from the Planning Meta-Prompt? Tell the LLM what to fix and run it again. Got a weak coding prompt? Adjust it in the Execution Chain and rerun. You are the conductor‚Äîmake the orchestra play in tune.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Meta-Prompts Need Rules.&lt;/strong&gt;&lt;br /&gt; If you‚Äôre too vague, the LLM will fill in the gaps with nonsense. That‚Äôs why good meta prompts are a huge book of rules, like defining how breaking down dependencies, defining interfaces, and creating acceptance criteria work. For example, the &lt;a href="https://gist.github.com/pyros-projects/c77402249b5b45f0a501998870766ae9#file-03_task_selection_md"&gt;Task Selection Meta-Prompt&lt;/a&gt; ensures only the right task is chosen based on dependencies, context, and priorities. The rules make sure you aren't doing a task which the prerequisites are still missing for.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Meta-Prompts Aren‚Äôt Easy, But They‚Äôre Worth It.&lt;/strong&gt;&lt;br /&gt; Yeah, these prompts take effort. You need to know your project, your tools, and how to manage both. But once you‚Äôve got the hang of them, they‚Äôre a game-changer. No more vague prompts. No more bad outputs. Just a smooth, efficient process where the LLM is a true teammate.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And guess what? The LLM delivers, because now it knows what you actually need. Plus, you're guardrailing it against its worst enemy: its own creativity. Nothing good happens when you let an LLM be &lt;em&gt;creative&lt;/em&gt;. Prompts like &lt;em&gt;&amp;quot;Generate me an enterprise-level user management app&amp;quot;&lt;/em&gt; are like handing it a creativity license. Don't.&lt;/p&gt; &lt;p&gt;My personal meta-prompts I use at work are gigantic, easily 10 times more and bigger than what I prepared for this thread, and 100s of hours went into them to pack in corporate identity stuff, libraries we like to use a certain way, personal coding styles, and everything else so it feels like a buddy that can read my mind.&lt;/p&gt; &lt;p&gt;That's why I'm quite pissy if some schmuck who played with o1 for like an hour thinks they are some kind of authority in knowing what such a model has to offer. Especially if they aren't interested at all in help or learning how to get the best out of it. In the end, a model does what the prompter gives it, and therefore a model is just as good as the person using it.&lt;/p&gt; &lt;p&gt;I can only recommend you learn them and you'll discover a whole new layer of how you can use LLMs, and I hope with this thread I could outline the very basics of them.&lt;/p&gt; &lt;p&gt;Cheers&lt;br /&gt; Pyro&lt;/p&gt; &lt;p&gt;PS: I have not forgotten that I have to make you guys a Anime Waifu with infinite context&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pyros-SD-Models"&gt; /u/Pyros-SD-Models &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2b2eo/meta_prompts_because_your_llm_can_do_better_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2b2eo/meta_prompts_because_your_llm_can_do_better_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2b2eo/meta_prompts_because_your_llm_can_do_better_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T23:30:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i26nk4</id>
    <title>ATTENTION IS ALL YOU NEED PT. 2 - TITANS: Learning to Memorize at Test Time</title>
    <updated>2025-01-15T20:16:09+00:00</updated>
    <author>
      <name>/u/AIGuy3000</name>
      <uri>https://old.reddit.com/user/AIGuy3000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2501.00663v1"&gt;https://arxiv.org/pdf/2501.00663v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The innovation in this field has been iterating at light speed, and I think we have something special here. I tried something similar but I‚Äôm no PhD student and the Math is beyond me. &lt;/p&gt; &lt;p&gt;TLDR; Google Research introduces Titans, a new Al model that learns to store information in a dedicated &amp;quot;long-term memory&amp;quot; at test time. This means it can adapt whenever it sees something surprising, updating its memory on-the-fly. Unlike standard Transformers that handle only the current text window, Titans keep a deeper, more permanent record-similar to short-term vs. long-term memory in humans. The method scales more efficiently (linear time) than traditional Transformers(qudratic time) for very long input sequences. i.e theoretically infinite context windows.&lt;/p&gt; &lt;p&gt;Don‚Äôt be mistaken, this isn‚Äôt just a next-gen ‚Äúartificial intelligence‚Äù, but a step towards to ‚Äúartificial consciousness‚Äù with persistent memory - IF we define consciousness as the ability to model internally(self-modeling), organize, integrate, and recollect of data (with respect to a real-time input)as posited by IIT‚Ä¶ would love to hear y‚Äôall‚Äôs thoughts üß†üëÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIGuy3000"&gt; /u/AIGuy3000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i26nk4/attention_is_all_you_need_pt_2_titans_learning_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i26nk4/attention_is_all_you_need_pt_2_titans_learning_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i26nk4/attention_is_all_you_need_pt_2_titans_learning_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T20:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1zcnq</id>
    <title>Hugging Face is doing a FREE and CERTIFIED course on LLM Agents!</title>
    <updated>2025-01-15T15:04:30+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1zcnq/hugging_face_is_doing_a_free_and_certified_course/"&gt; &lt;img alt="Hugging Face is doing a FREE and CERTIFIED course on LLM Agents!" src="https://external-preview.redd.it/JOoVE9yE3UMtoGIBlW4phQep83QjxjwMVZvxo1yvB-4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f737a2f98cecfb91b00fb821dd675f7c44f7794" title="Hugging Face is doing a FREE and CERTIFIED course on LLM Agents!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Learn to build AI agents that can automate tasks, generate code, and more!&lt;/strong&gt; ü§ñ&lt;/p&gt; &lt;p&gt;Hugging Face just launched a &lt;strong&gt;free, certified course&lt;/strong&gt; on building and deploying AI agents.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Learn what Agents are&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Build your own Agents&lt;/strong&gt; using the latest libraries and tools.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Earn a certificate of completion&lt;/strong&gt; to showcase your achievement.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pb6tsyaoa6de1.png?width=720&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5274f727b9755934bfc21f2ec298a48aeaa4c0a"&gt;https://preview.redd.it/pb6tsyaoa6de1.png?width=720&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5274f727b9755934bfc21f2ec298a48aeaa4c0a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Link in here &lt;a href="https://huggingface.co/posts/burtenshaw/334573649974058"&gt;https://huggingface.co/posts/burtenshaw/334573649974058&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1zcnq/hugging_face_is_doing_a_free_and_certified_course/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1zcnq/hugging_face_is_doing_a_free_and_certified_course/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1zcnq/hugging_face_is_doing_a_free_and_certified_course/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T15:04:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2e23v</id>
    <title>I used Kokoro-82M, Llama 3.2, and Whisper Small to build a real-time speech-to-speech chatbot that runs locally on my MacBook!</title>
    <updated>2025-01-16T01:57:31+00:00</updated>
    <author>
      <name>/u/tycho_brahes_nose_</name>
      <uri>https://old.reddit.com/user/tycho_brahes_nose_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2e23v/i_used_kokoro82m_llama_32_and_whisper_small_to/"&gt; &lt;img alt="I used Kokoro-82M, Llama 3.2, and Whisper Small to build a real-time speech-to-speech chatbot that runs locally on my MacBook!" src="https://external-preview.redd.it/ajBjajZ2YTFpOWRlMdVERFdEQKrY8cptLv00gyZBVqtju60x3iy8w-FpWSZ2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7aa615b1ccbb81cee65b5735b41605e27fcb9ed" title="I used Kokoro-82M, Llama 3.2, and Whisper Small to build a real-time speech-to-speech chatbot that runs locally on my MacBook!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tycho_brahes_nose_"&gt; /u/tycho_brahes_nose_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yw01bva1i9de1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2e23v/i_used_kokoro82m_llama_32_and_whisper_small_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2e23v/i_used_kokoro82m_llama_32_and_whisper_small_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T01:57:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i27l37</id>
    <title>Deepseek is overthinking</title>
    <updated>2025-01-15T20:57:13+00:00</updated>
    <author>
      <name>/u/Mr_Jericho</name>
      <uri>https://old.reddit.com/user/Mr_Jericho</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i27l37/deepseek_is_overthinking/"&gt; &lt;img alt="Deepseek is overthinking" src="https://preview.redd.it/rz378lgd18de1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=deff4f920457d1affd3bc98d78e4fc3601dda4b9" title="Deepseek is overthinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Jericho"&gt; /u/Mr_Jericho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rz378lgd18de1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i27l37/deepseek_is_overthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i27l37/deepseek_is_overthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T20:57:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1i29wz5</id>
    <title>Google just released a new architecture</title>
    <updated>2025-01-15T22:38:26+00:00</updated>
    <author>
      <name>/u/FeathersOfTheArrow</name>
      <uri>https://old.reddit.com/user/FeathersOfTheArrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like a big deal? &lt;a href="https://x.com/behrouz_ali/status/1878859086227255347"&gt;Thread by lead author&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeathersOfTheArrow"&gt; /u/FeathersOfTheArrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2501.00663"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i29wz5/google_just_released_a_new_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i29wz5/google_just_released_a_new_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T22:38:26+00:00</published>
  </entry>
</feed>
