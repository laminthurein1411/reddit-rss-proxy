<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-23T02:18:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k59z97</id>
    <title>How to reach 100-200 t/s on consumer hardware</title>
    <updated>2025-04-22T16:01:34+00:00</updated>
    <author>
      <name>/u/f1_manu</name>
      <uri>https://old.reddit.com/user/f1_manu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm curious, a lot of the setups I read here are more focused on having hardware able to be fitting the model, rather than getting fast inference from the hardware. As a complete noob, my question is pretty straightforward, what's the cheapest way of achieving 150-200 tokens per second output for a midsized model like Llama 3.3 70b, at 4-8bit?&lt;/p&gt; &lt;p&gt;And to scale more? Is 500 tps feasible?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/f1_manu"&gt; /u/f1_manu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k59z97/how_to_reach_100200_ts_on_consumer_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k59z97/how_to_reach_100200_ts_on_consumer_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k59z97/how_to_reach_100200_ts_on_consumer_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T16:01:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5a630</id>
    <title>Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model? [paper and related material with empirical data supporting the hypothesis that current reinforcement learning techniques elicit abilities already present in base language models]</title>
    <updated>2025-04-22T16:09:15+00:00</updated>
    <author>
      <name>/u/Wiskkey</name>
      <uri>https://old.reddit.com/user/Wiskkey</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From &lt;a href="https://limit-of-rlvr.github.io/"&gt;the project page for the work&lt;/a&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Recent breakthroughs in reasoning-focused large language models (LLMs) like OpenAI-o1, DeepSeek-R1, and Kimi-1.5 have largely relied on Reinforcement Learning with Verifiable Rewards (RLVR), which replaces human annotations with automated rewards (e.g., verified math solutions or passing code tests) to scale self-improvement. While RLVR enhances reasoning behaviors such as self-reflection and iterative refinement, we challenge a core assumption:&lt;/p&gt; &lt;p&gt;Does RLVR actually expand LLMs' reasoning capabilities, or does it merely optimize existing ones?&lt;/p&gt; &lt;p&gt;By evaluating models via pass@k, where success requires just one correct solution among k attempts, we uncover that RL-trained models excel at low k (e.g., pass@1) but are consistently outperformed by base models at high k (e.g., pass@256). This demonstrates that RLVR narrows the model's exploration, favoring known high-reward paths instead of discovering new reasoning strategies. Crucially, all correct solutions from RL-trained models already exist in the base model's distribution, proving RLVR enhances sampling efficiency, not reasoning capacity, while inadvertently shrinking the solution space.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2504.13837"&gt;Paper&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/YangYue_THU/status/1914690345964855566"&gt;Short video about the paper (including Q&amp;amp;As) in a tweet by one of the paper's authors&lt;/a&gt;. &lt;a href="https://xcancel.com/YangYue_THU/status/1914690345964855566"&gt;Alternative link&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://natolambert.substack.com/p/does-reinforcement-learning-really"&gt;A review of the paper by Nathan Lambert&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Background info: &lt;a href="https://www.interconnects.ai/p/elicitation-theory-of-post-training"&gt;Elicitation, the simplest way to understand post-training.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wiskkey"&gt; /u/Wiskkey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5a630/does_reinforcement_learning_really_incentivize/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5a630/does_reinforcement_learning_really_incentivize/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5a630/does_reinforcement_learning_really_incentivize/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T16:09:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5nx9l</id>
    <title>Gemma 27b qat : Mac Mini 4 optimizations?</title>
    <updated>2025-04-23T02:03:48+00:00</updated>
    <author>
      <name>/u/KittyPigeon</name>
      <uri>https://old.reddit.com/user/KittyPigeon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Short of an MLX model being released, are there any optimizations to make Gemma run faster on a mac mini?&lt;/p&gt; &lt;p&gt;48 GB VRAM. &lt;/p&gt; &lt;p&gt;Getting around 9 tokens/s on LM studio. I recognize this is a large model, but wondering if any settings on my part rather than defaults could have any impact on the tokens/second&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KittyPigeon"&gt; /u/KittyPigeon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5nx9l/gemma_27b_qat_mac_mini_4_optimizations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5nx9l/gemma_27b_qat_mac_mini_4_optimizations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5nx9l/gemma_27b_qat_mac_mini_4_optimizations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T02:03:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5f1tl</id>
    <title>VoltAgent - We built a new open source TypeScript AI agent framework</title>
    <updated>2025-04-22T19:23:30+00:00</updated>
    <author>
      <name>/u/necati-ozmen</name>
      <uri>https://old.reddit.com/user/necati-ozmen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My co-founder and I built an open-source TypeScript framework for building AI agents and wanted to share with the community&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/voltagent/voltagent"&gt;https://github.com/voltagent/voltagent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Building more complex and production ready AI agents often means either drowning in boilerplate when starting from scratch or hitting walls with limitations of low/no code tools (vendor lock-in, limited customization). We felt the JS ecosystem needed something better, closer to the tooling available in Python. &lt;/p&gt; &lt;p&gt;Core structure based on three things:&lt;br /&gt; - Core building blocks to avoid repetitive setup (state, tools, memory).&lt;/p&gt; &lt;p&gt;- Modular design to add features as needed.&lt;/p&gt; &lt;p&gt;- LLM-agnostic approach (use OpenAI, Google, Anthropic, etc. ‚Äì no lock-in). &lt;/p&gt; &lt;p&gt;A key feature is built-in, local-first observability.&lt;br /&gt; Debugging AI can be a black box, so Voltagent connects directly to our Developer Console (no data leaves your machine). You can visually trace agent execution like n8n style flows, inspect messages/tool calls, and see the state in real-time, making debugging much easier.&lt;/p&gt; &lt;p&gt;You can check out the console demo: &lt;a href="https://console.voltagent.dev/demo"&gt;https://console.voltagent.dev/demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We haven't found this level of integrated debugging visibility in other TS agent frameworks.&lt;/p&gt; &lt;p&gt;I would appreciate any feedback, contributions, and bug reports.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/necati-ozmen"&gt; /u/necati-ozmen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5f1tl/voltagent_we_built_a_new_open_source_typescript/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5f1tl/voltagent_we_built_a_new_open_source_typescript/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5f1tl/voltagent_we_built_a_new_open_source_typescript/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T19:23:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1k56qsb</id>
    <title>Quick review of GLM-Z1-32B-0414</title>
    <updated>2025-04-22T13:48:04+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using the fixed gguf from: &lt;a href="https://huggingface.co/matteogeniaccio/GLM-Z1-32B-0414-GGUF-fixed"&gt;https://huggingface.co/matteogeniaccio/GLM-Z1-32B-0414-GGUF-fixed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;QwQ passed all the following tests; see &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1js0zmd/quick_comparison_of_qwq_and_openthinker2_32b/"&gt;this post&lt;/a&gt; for more information. I will only post GLM-Z1's results here.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Candle test:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Initially Failed, fell into a infinite loop&lt;/p&gt; &lt;p&gt;After I increased repetition penalty to 1.1, the looping issue was fixed&lt;/p&gt; &lt;p&gt;But it still failed&lt;br /&gt; &lt;a href="https://imgur.com/a/6K1xKha"&gt;https://imgur.com/a/6K1xKha&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;5 reasoning questions:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;4 passed, 1 narrowly passed&lt;br /&gt; &lt;a href="https://imgur.com/a/Cdzfo1n"&gt;https://imgur.com/a/Cdzfo1n&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Private tests:&lt;/p&gt; &lt;p&gt;Coding question: One question about what caused the issue, plus 1,200 lines of C++ code.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Passed at first try, during multi-shot testing, it has a 50% chance of failing.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Restructuring a financial spreadsheet.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Passed.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Conclusion:&lt;/p&gt; &lt;p&gt;The performance is still a bit behind QwQ-32B, but getting closer&lt;/p&gt; &lt;p&gt;Also, it suffers from quite bad repetition issues when using the recommended settings (no repetition penalty). Even though this could be fixed by using a 1.1 penalty, I don't know how much this would hurt the model's performance.&lt;/p&gt; &lt;p&gt;I also observed similar repetition issues when using their official site, &lt;a href="http://Chat.Z.AI"&gt;Chat.Z.AI&lt;/a&gt;, and it also could fall into a loop, so I don't think it's the GGUFs problem.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Settings I used: &lt;a href="https://imgur.com/a/iwl2Up9"&gt;https://imgur.com/a/iwl2Up9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;backend: ollama v0.6.6&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.ollama.com/JollyLlama/GLM-Z1-32B-0414-Q4_K_M"&gt;https://www.ollama.com/JollyLlama/GLM-Z1-32B-0414-Q4_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;source of public questions:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i65599/r1_32b_is_be_worse_than_qwq_32b_tests_included/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1i65599/r1_32b_is_be_worse_than_qwq_32b_tests_included/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jpr1nk/the_candle_test_most_llms_fail_to_generalise_at/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jpr1nk/the_candle_test_most_llms_fail_to_generalise_at/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k56qsb/quick_review_of_glmz132b0414/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k56qsb/quick_review_of_glmz132b0414/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k56qsb/quick_review_of_glmz132b0414/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T13:48:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4lmil</id>
    <title>A new TTS model capable of generating ultra-realistic dialogue</title>
    <updated>2025-04-21T19:02:56+00:00</updated>
    <author>
      <name>/u/aadoop6</name>
      <uri>https://old.reddit.com/user/aadoop6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4lmil/a_new_tts_model_capable_of_generating/"&gt; &lt;img alt="A new TTS model capable of generating ultra-realistic dialogue" src="https://external-preview.redd.it/wNcjaJIfjy5w8Wuatdn7tqqANxkzwO5-UB9WQmMCT3w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f1d15a76610dd0dbe8a436684ca2985b2cc492b" title="A new TTS model capable of generating ultra-realistic dialogue" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aadoop6"&gt; /u/aadoop6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/nari-labs/dia"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4lmil/a_new_tts_model_capable_of_generating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4lmil/a_new_tts_model_capable_of_generating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T19:02:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1k547al</id>
    <title>New Lib to process PDFs</title>
    <updated>2025-04-22T11:43:07+00:00</updated>
    <author>
      <name>/u/Electronic-Lab-7343</name>
      <uri>https://old.reddit.com/user/Electronic-Lab-7343</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I built a library over the holiday that converts PDF documents to Markdown. It segments by page, extracts relevant elements like titles, images, and tables, and even counts tokens per page. (&lt;a href="https://github.com/matthsena/AlcheMark"&gt;AlcheMark&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Some advantages compared to competitors (Docling):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: In my test with a 500-page file, this library parsed it in 45 seconds. Docling around 3 minutes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;References&lt;/strong&gt;: Docling convert the entire file into a single large Markdown block without page segmentation, making it harder for LLMs to reference which page the information came from. This library returns a vector of objects‚Äîone for each page.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Token estimation&lt;/strong&gt;: The library shows the token count for each page, allowing better cost estimation before sending a prompt.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For this project, I make a ensemble of several existing libraries with a different approach to data handling.&lt;/p&gt; &lt;p&gt;If you'd like to contribute or support the project, feel free to leave a star on GitHub:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/matthsena/AlcheMark"&gt;https://github.com/matthsena/AlcheMark&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic-Lab-7343"&gt; /u/Electronic-Lab-7343 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k547al/new_lib_to_process_pdfs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k547al/new_lib_to_process_pdfs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k547al/new_lib_to_process_pdfs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T11:43:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1k52h4n</id>
    <title>Why is MythoMax13B still in high demand?</title>
    <updated>2025-04-22T09:54:54+00:00</updated>
    <author>
      <name>/u/Consistent_Winner596</name>
      <uri>https://old.reddit.com/user/Consistent_Winner596</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently noticed, that MythoMax13B is really high ranked on openrouter in the RPG section and has high demand. That makes no sense to me, as it is a still a Llama2 era model. Is that model so good or is it promoted in the openrouter chat rooms or on other platforms actively, but even if that is the reason it makes no sense. Why didn't they then use modern RP models and stick to that one, can someone who played with that model answer it? Is it just that good or brings still using a L2 other benefits I don't see at the moment? Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Consistent_Winner596"&gt; /u/Consistent_Winner596 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k52h4n/why_is_mythomax13b_still_in_high_demand/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k52h4n/why_is_mythomax13b_still_in_high_demand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k52h4n/why_is_mythomax13b_still_in_high_demand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T09:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5f3qy</id>
    <title>Working GLM4 quants with mainline Llama.cpp / LMStudio</title>
    <updated>2025-04-22T19:25:41+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since piDack (the person behind the fixes for GLM4 in Lllama.cpp) remade his fix to only affect the converter, you can now run fixed GLM4 quants in the mainline Llama.cpp (and thus in LMStudio).&lt;/p&gt; &lt;p&gt;GLM4-32B GGUFÔºàQ4_0,Q5_K_M,Q8_0Ôºâ-&amp;gt; &lt;a href="https://www.modelscope.cn/models/pcdack/glm-4-0414-32b-chat-gguf/files"&gt;https://www.modelscope.cn/models/pcdack/glm-4-0414-32b-chat-gguf/files&lt;/a&gt;&lt;br /&gt; GLM4Z-32B GGUF -&amp;gt; &lt;a href="https://www.modelscope.cn/models/pcdack/glm-4Z-0414-32b-chat-gguf/files"&gt;https://www.modelscope.cn/models/pcdack/glm-4Z-0414-32b-chat-gguf/files&lt;/a&gt;&lt;br /&gt; GLM4-9B GGUF -&amp;gt; &lt;a href="https://www.modelscope.cn/models/pcdack/glm4-0414-9B-chat-gguf/files"&gt;https://www.modelscope.cn/models/pcdack/glm4-0414-9B-chat-gguf/files&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For GLM4-Z1-9B GGUF, I made a working IQ4NL quant, will probably upload some more imatrix quants soon: &lt;a href="https://huggingface.co/ilintar/THUDM_GLM-Z1-9B-0414_iGGUF"&gt;https://huggingface.co/ilintar/THUDM_GLM-Z1-9B-0414_iGGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you want to use any of those models in LM Studio, you have to fix the Jinja template per the note I made on my model page above, since the LM Studio Jinja parser does not (yet?) support chained function/indexing calls.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5f3qy/working_glm4_quants_with_mainline_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5f3qy/working_glm4_quants_with_mainline_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5f3qy/working_glm4_quants_with_mainline_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T19:25:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1k56cvq</id>
    <title>MobiRAG: Chat with your documents ‚Äî even on airplane mode</title>
    <updated>2025-04-22T13:30:34+00:00</updated>
    <author>
      <name>/u/Weird_Maximum_9573</name>
      <uri>https://old.reddit.com/user/Weird_Maximum_9573</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k56cvq/mobirag_chat_with_your_documents_even_on_airplane/"&gt; &lt;img alt="MobiRAG: Chat with your documents ‚Äî even on airplane mode" src="https://external-preview.redd.it/cWI0eW05YTAyZXdlMTs0YPCuiY4jszayCpnYTmARlQ50HCYuv6i7fSK4WJGb.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44629a2f1f0cf47066e2d1595531e56fdc17c131" title="MobiRAG: Chat with your documents ‚Äî even on airplane mode" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing &lt;strong&gt;MobiRAG&lt;/strong&gt; ‚Äî a lightweight, privacy-first AI assistant that runs fully offline, enabling fast, intelligent querying of any document on your phone.&lt;/p&gt; &lt;p&gt;Whether you're diving into complex research papers or simply trying to look something up in your TV manual, MobiRAG gives you a seamless, intelligent way to search and get answers instantly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Most vector databases are memory-hungry ‚Äî not ideal for mobile.&lt;/li&gt; &lt;li&gt;MobiRAG uses FAISS Product Quantization to compress embeddings up to 97x, dramatically reducing memory usage.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Built for resource-constrained devices:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No massive vector DBs&lt;/li&gt; &lt;li&gt;No cloud dependencies&lt;/li&gt; &lt;li&gt;Automatically indexes all text-based PDFs on your phone&lt;/li&gt; &lt;li&gt;Just fast, compressed semantic search&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ONNX all-MiniLM-L6-v2 for on-device embeddings&lt;/li&gt; &lt;li&gt;FAISS + PQ compressed Vector DB = minimal memory footprint&lt;/li&gt; &lt;li&gt;Hybrid RAG: combines vector similarity with TF-IDF keyword overlap&lt;/li&gt; &lt;li&gt;SLM: Qwen 0.5B runs on-device to generate grounded answers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/nishchaljs/MobiRAG"&gt;https://github.com/nishchaljs/MobiRAG&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weird_Maximum_9573"&gt; /u/Weird_Maximum_9573 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dscjpqo02ewe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k56cvq/mobirag_chat_with_your_documents_even_on_airplane/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k56cvq/mobirag_chat_with_your_documents_even_on_airplane/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T13:30:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5h8z1</id>
    <title>In my experience, the QAT Gemma 3 quants by stduhpf still perform the best.</title>
    <updated>2025-04-22T20:52:37+00:00</updated>
    <author>
      <name>/u/dampflokfreund</name>
      <uri>https://old.reddit.com/user/dampflokfreund</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've run couple of tests I usually do with my LLMs and noticed that the versions by &lt;a href="/u/stduhpf"&gt;u/stduhpf&lt;/a&gt; (in this case &lt;a href="https://huggingface.co/stduhpf/google-gemma-3-12b-it-qat-q4%5C_0-gguf-small"&gt;https://huggingface.co/stduhpf/google-gemma-3-12b-it-qat-q4\_0-gguf-small&lt;/a&gt;) still outperform: &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lmstudio-community/gemma-3-12B-it-qat-GGUF"&gt;https://huggingface.co/lmstudio-community/gemma-3-12B-it-qat-GGUF&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/bartowski/google_gemma-3-12b-it-qat-GGUF"&gt;https://huggingface.co/bartowski/google_gemma-3-12b-it-qat-GGUF&lt;/a&gt;&lt;br /&gt; &lt;a href="http://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf"&gt;huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf&lt;/a&gt; &lt;/p&gt; &lt;p&gt;This is pretty strange, as theoretically they all should perform very identical but the one by stduhpf offers better logic and knowledge in my tests.&lt;/p&gt; &lt;p&gt;Also, I've run a small fixed subset of MMLU Pro with deterministic settings on all of these models, and his version comes out ahead. &lt;/p&gt; &lt;p&gt;What is your experience? Particularily I'm also interested about experiences with the G3 27B version. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dampflokfreund"&gt; /u/dampflokfreund &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5h8z1/in_my_experience_the_qat_gemma_3_quants_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5h8z1/in_my_experience_the_qat_gemma_3_quants_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5h8z1/in_my_experience_the_qat_gemma_3_quants_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T20:52:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1k54mdz</id>
    <title>Stanford CS 25 Transformers Course (OPEN TO EVERYBODY)</title>
    <updated>2025-04-22T12:05:15+00:00</updated>
    <author>
      <name>/u/MLPhDStudent</name>
      <uri>https://old.reddit.com/user/MLPhDStudent</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Tl;dr: One of Stanford's hottest seminar courses. We open the course through Zoom to the public. Lectures on Tuesdays, 3-4:20pm PDT (Zoom link on course website). Talks will be recorded and released ~3 weeks after each lecture. Course website:&lt;/strong&gt; &lt;a href="https://web.stanford.edu/class/cs25/"&gt;&lt;strong&gt;https://web.stanford.edu/class/cs25/&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our lecture later &lt;strong&gt;today at 3pm PDT&lt;/strong&gt; is &lt;strong&gt;Eric Zelikman from xAI&lt;/strong&gt;, discussing ‚ÄúWe're All in this Together: Human Agency in an Era of Artificial Agents‚Äù. &lt;strong&gt;This talk will NOT be recorded!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Each week, we invite folks at the forefront of Transformers research to discuss the latest breakthroughs, from LLM architectures like GPT and Gemini to creative use cases in generating art (e.g. DALL-E and Sora), biology and neuroscience applications, robotics, and so forth!&lt;/p&gt; &lt;p&gt;We invite the coolest speakers such as &lt;strong&gt;Andrej Karpathy, Geoffrey Hinton, Jim Fan, Ashish Vaswani&lt;/strong&gt;, and folks from &lt;strong&gt;OpenAI, Google, NVIDIA&lt;/strong&gt;, etc.&lt;/p&gt; &lt;p&gt;The &lt;strong&gt;recording of the first lecture&lt;/strong&gt; is released! &lt;strong&gt;Check it out&lt;/strong&gt; &lt;a href="https://www.youtube.com/watch?v=JKbtWimlzAE"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; We gave a brief overview of Transformers, discussed pretraining (focusing on data strategies [&lt;a href="https://arxiv.org/abs/2408.03617"&gt;1&lt;/a&gt;,&lt;a href="https://arxiv.org/abs/2412.15285"&gt;2&lt;/a&gt;]) and post-training, and highlighted recent trends, applications, and remaining challenges/weaknesses of Transformers. Slides are &lt;a href="https://docs.google.com/presentation/d/16tMMBUjPnqw-PvxF8xzu2m1Epdo1fH7nXWlt3mt2q5w/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Check out our course website for more!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLPhDStudent"&gt; /u/MLPhDStudent &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://web.stanford.edu/class/cs25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k54mdz/stanford_cs_25_transformers_course_open_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k54mdz/stanford_cs_25_transformers_course_open_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T12:05:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5df6x</id>
    <title>Intern team may be our next AllenAI</title>
    <updated>2025-04-22T18:19:19+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5df6x/intern_team_may_be_our_next_allenai/"&gt; &lt;img alt="Intern team may be our next AllenAI" src="https://external-preview.redd.it/D-xFwymPmTVU-AzxqPdnr1BEQsSvHPFOfpeK0yI1G4M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e725417fe07d7d77dbb30e48bcd7eff07157a854" title="Intern team may be our next AllenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They are open sourcing the SFT data they used for their SOTA InternVL3 models, very exciting!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/datasets/OpenGVLab/InternVL-Data"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5df6x/intern_team_may_be_our_next_allenai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5df6x/intern_team_may_be_our_next_allenai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T18:19:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4v5fm</id>
    <title>Dia 1.6B is one of the funnest models I've ever come across.</title>
    <updated>2025-04-22T02:07:19+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/w2jq98c7oawe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4v5fm/dia_16b_is_one_of_the_funnest_models_ive_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4v5fm/dia_16b_is_one_of_the_funnest_models_ive_ever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T02:07:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1k546sq</id>
    <title>THUDM/SWE-Dev-9B ¬∑ Hugging Face</title>
    <updated>2025-04-22T11:42:23+00:00</updated>
    <author>
      <name>/u/bobby-chan</name>
      <uri>https://old.reddit.com/user/bobby-chan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k546sq/thudmswedev9b_hugging_face/"&gt; &lt;img alt="THUDM/SWE-Dev-9B ¬∑ Hugging Face" src="https://external-preview.redd.it/H-WqhcnMRaUejHpQLqeLIABkVOOs-PNNxG0QszsYGF4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=33500f8d40e8a012a0d58c9cc5020fee1f8beadd" title="THUDM/SWE-Dev-9B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The creators of the GLM-4 models released a collection of coder models&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SWE-Dev-7B (Qwen-2.5-7B-Instruct): &lt;a href="https://huggingface.co/THUDM/SWE-Dev-7B/"&gt;https://huggingface.co/THUDM/SWE-Dev-7B/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;SWE-Dev-9B (GLM-4-9B-Chat): &lt;a href="https://huggingface.co/THUDM/SWE-Dev-9B/"&gt;https://huggingface.co/THUDM/SWE-Dev-9B/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;SWE-Dev-32B (Qwen-2.5-32B-Instruct): &lt;a href="https://huggingface.co/THUDM/SWE-Dev-32B/"&gt;https://huggingface.co/THUDM/SWE-Dev-32B/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobby-chan"&gt; /u/bobby-chan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/THUDM/SWE-Dev-9B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k546sq/thudmswedev9b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k546sq/thudmswedev9b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T11:42:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k54foj</id>
    <title>Let us build DeepSeek from Scratch | No fluff | 13 lectures uploaded</title>
    <updated>2025-04-22T11:55:45+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k54foj/let_us_build_deepseek_from_scratch_no_fluff_13/"&gt; &lt;img alt="Let us build DeepSeek from Scratch | No fluff | 13 lectures uploaded" src="https://external-preview.redd.it/pAa68GpmjnpZeahm_YMGQkYTs9KtW9HemhGbAYHU02s.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=555355166a247eb92939344c89b96ed48dd7655a" title="Let us build DeepSeek from Scratch | No fluff | 13 lectures uploaded" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/5w0lu5m2ldwe1.gif"&gt;A few notes I made as part of this playlist&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚ÄúCan I build the DeepSeek architecture and model myself, from scratch?‚Äù&lt;/p&gt; &lt;p&gt;You can. You need to know the nuts and bolts. &lt;/p&gt; &lt;p&gt;4 weeks back, we launched our playlist: ‚ÄúBuild DeepSeek from Scratch‚Äù &lt;/p&gt; &lt;p&gt;Until now, we have uploaded 13 lectures in this playlist: &lt;/p&gt; &lt;p&gt;(1) DeepSeek series introduction: &lt;a href="https://youtu.be/QWNxQIq0hMo"&gt;https://youtu.be/QWNxQIq0hMo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(2) DeepSeek basics: &lt;a href="https://youtu.be/WjhDDeZ7DvM"&gt;https://youtu.be/WjhDDeZ7DvM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(3) Journey of a token into the LLM architecture: &lt;a href="https://youtu.be/rkEYwH4UGa4"&gt;https://youtu.be/rkEYwH4UGa4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(4) Attention mechanism explained in 1 hour: &lt;a href="https://youtu.be/K45ze9Yd5UE"&gt;https://youtu.be/K45ze9Yd5UE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(5) Self Attention Mechanism - Handwritten from scratch: &lt;a href="https://youtu.be/s8mskq-nzec"&gt;https://youtu.be/s8mskq-nzec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(6) Causal Attention Explained: Don't Peek into the Future: &lt;a href="https://youtu.be/c6Kkj6iLeBg"&gt;https://youtu.be/c6Kkj6iLeBg&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(7) Multi-Head Attention Visually Explained: &lt;a href="https://youtu.be/qbN4ulK-bZA"&gt;https://youtu.be/qbN4ulK-bZA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(8) Multi-Head Attention Handwritten from Scratch: &lt;a href="https://youtu.be/rvsEW-EsD-Y"&gt;https://youtu.be/rvsEW-EsD-Y&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(9) Key Value Cache from Scratch: &lt;a href="https://youtu.be/IDwTiS4_bKo"&gt;https://youtu.be/IDwTiS4_bKo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(10) Multi-Query Attention Explained: &lt;a href="https://youtu.be/Z6B51Odtn-Y"&gt;https://youtu.be/Z6B51Odtn-Y&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(11) Understand Grouped Query Attention (GQA): &lt;a href="https://youtu.be/kx3rETIxo4Q"&gt;https://youtu.be/kx3rETIxo4Q&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(12) Multi-Head Latent Attention From Scratch: &lt;a href="https://youtu.be/NlDQUj1olXM"&gt;https://youtu.be/NlDQUj1olXM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(13) Multi-Head Latent Attention Coded from Scratch in Python: &lt;a href="https://youtu.be/mIaWmJVrMpc"&gt;https://youtu.be/mIaWmJVrMpc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Next to come: &lt;/p&gt; &lt;p&gt;- Rotary Positional Encoding (RoPE)&lt;/p&gt; &lt;p&gt;- DeepSeek MLA + RoPE&lt;/p&gt; &lt;p&gt;- DeepSeek Mixture of Experts (MoE)&lt;/p&gt; &lt;p&gt;- Multi-token Prediction (MTP)&lt;/p&gt; &lt;p&gt;- Supervised Fine-Tuning (SFT)&lt;/p&gt; &lt;p&gt;- Group Relative Policy Optimisation (GRPO)&lt;/p&gt; &lt;p&gt;- DeepSeek PTX innovation&lt;/p&gt; &lt;p&gt;This playlist won‚Äôt be a 1 hour or 2 hour video. This will be a mega playlist of 35-40 videos with a duration of 40+ hours.&lt;/p&gt; &lt;p&gt;I have made this with a lot of passion. &lt;/p&gt; &lt;p&gt;Would look forward to support and your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k54foj/let_us_build_deepseek_from_scratch_no_fluff_13/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k54foj/let_us_build_deepseek_from_scratch_no_fluff_13/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k54foj/let_us_build_deepseek_from_scratch_no_fluff_13/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T11:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5lux8</id>
    <title>MMLU-PRO benchmark: GLM-4-32B-0414-Q4_K_M vs Qwen2.5-32b-instruct-q4_K_M</title>
    <updated>2025-04-23T00:19:11+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5lux8/mmlupro_benchmark_glm432b0414q4_k_m_vs/"&gt; &lt;img alt="MMLU-PRO benchmark: GLM-4-32B-0414-Q4_K_M vs Qwen2.5-32b-instruct-q4_K_M" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="MMLU-PRO benchmark: GLM-4-32B-0414-Q4_K_M vs Qwen2.5-32b-instruct-q4_K_M" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;20% subset&lt;/strong&gt; of MMLU-PRO, &lt;strong&gt;&lt;em&gt;0 temperature&lt;/em&gt;&lt;/strong&gt;, the entire test took &lt;strong&gt;7 hours 30 minutes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1vizdtmq9hwe1.png?width=2485&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a4c56b76cfb84353347aa00fb9099212046ce1a"&gt;https://preview.redd.it/1vizdtmq9hwe1.png?width=2485&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a4c56b76cfb84353347aa00fb9099212046ce1a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0il9x82aahwe1.png?width=1375&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76913f83ffb57e35733e33af47a3565faffd3943"&gt;https://preview.redd.it/0il9x82aahwe1.png?width=1375&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76913f83ffb57e35733e33af47a3565faffd3943&lt;/a&gt;&lt;/p&gt; &lt;p&gt;backend: ollama v0.6.6&lt;/p&gt; &lt;p&gt;gguf:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.ollama.com/JollyLlama/GLM-4-32B-0414-Q4_K_M"&gt;https://www.ollama.com/JollyLlama/GLM-4-32B-0414-Q4_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.ollama.com/library/qwen2.5:32b-instruct-q4_K_M"&gt;https://www.ollama.com/library/qwen2.5:32b-instruct-q4_K_M&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5lux8/mmlupro_benchmark_glm432b0414q4_k_m_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5lux8/mmlupro_benchmark_glm432b0414q4_k_m_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5lux8/mmlupro_benchmark_glm432b0414q4_k_m_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T00:19:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5j3ob</id>
    <title>Cogito-3b and BitNet topped our evaluation on summarization task in RAG</title>
    <updated>2025-04-22T22:10:31+00:00</updated>
    <author>
      <name>/u/unseenmarscai</name>
      <uri>https://old.reddit.com/user/unseenmarscai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5j3ob/cogito3b_and_bitnet_topped_our_evaluation_on/"&gt; &lt;img alt="Cogito-3b and BitNet topped our evaluation on summarization task in RAG" src="https://external-preview.redd.it/_FjCaxo25scY-3sX7SU5Z8CDINogT-Qd5F471W8a1B8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0fc7b4bc69cd91d8b9e94d5f19215e52ef3e2f3c" title="Cogito-3b and BitNet topped our evaluation on summarization task in RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/rm9o1ejykgwe1.png?width=2446&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92272ed3a643733c4eac5c29854e4ffb9c0468bc"&gt;https://preview.redd.it/rm9o1ejykgwe1.png?width=2446&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92272ed3a643733c4eac5c29854e4ffb9c0468bc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã !&lt;/p&gt; &lt;h1&gt;Here is the TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;We built an evaluation framework (&lt;a href="https://github.com/aizip/red-flow"&gt;&lt;strong&gt;RED-flow&lt;/strong&gt;&lt;/a&gt;) to assess small language models (SLMs) as summarizers in RAG systems&lt;/li&gt; &lt;li&gt;We created a 6,000-sample testing dataset (&lt;a href="https://huggingface.co/datasets/aizip/RED6k"&gt;&lt;strong&gt;RED6k&lt;/strong&gt;&lt;/a&gt;) across 10 domains for the evaluation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cogito-v1-preview-llama-3b&lt;/strong&gt; and &lt;strong&gt;BitNet-b1.58-2b-4t&lt;/strong&gt; top our benchmark as best open-source models for summarization in RAG applications&lt;/li&gt; &lt;li&gt;All tested SLMs struggle to recognize when the retrieved context is insufficient to answer a question and to respond with a meaningful clarification question.&lt;/li&gt; &lt;li&gt;Our testing dataset and evaluation workflow are &lt;strong&gt;fully open source&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What is a summarizer?&lt;/h1&gt; &lt;p&gt;In RAG systems, the summarizer is the component that takes retrieved document chunks and user questions as input, then generates coherent answers. For local deployments, small language models (SLMs) typically handle this role to keep everything running on your own hardware.&lt;/p&gt; &lt;h1&gt;SLMs' problems as summarizers&lt;/h1&gt; &lt;p&gt;Through our research, we found SLMs struggle with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Creating complete answers for multi-part questions&lt;/li&gt; &lt;li&gt;Sticking to the provided context (instead of making stuff up)&lt;/li&gt; &lt;li&gt;Admitting when they don't have enough information&lt;/li&gt; &lt;li&gt;Focusing on the most relevant parts of long contexts&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Our approach&lt;/h1&gt; &lt;p&gt;We built an evaluation framework focused on two critical areas most RAG systems struggle with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Context adherence:&lt;/strong&gt; Does the model stick strictly to the provided information?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Uncertainty handling:&lt;/strong&gt; Can the model admit when it doesn't know and ask clarifying questions?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our framework uses &lt;strong&gt;LLMs as judges&lt;/strong&gt; and a specialized dataset (&lt;a href="https://huggingface.co/datasets/aizip/RED6k"&gt;&lt;strong&gt;RED6k&lt;/strong&gt;&lt;/a&gt;) with intentionally challenging scenarios to thoroughly test these capabilities.&lt;/p&gt; &lt;h1&gt;Result&lt;/h1&gt; &lt;p&gt;After testing 11 popular open-source models, we found:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uvhdyve2mgwe1.png?width=2446&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cac1bbd2b38f9ae683e8b9504273eb01b0d8b0f6"&gt;https://preview.redd.it/uvhdyve2mgwe1.png?width=2446&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cac1bbd2b38f9ae683e8b9504273eb01b0d8b0f6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gavh5inomgwe1.png?width=2452&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b2d10c763a8ff2518c49c13eee9ac8114f038e4"&gt;https://preview.redd.it/gavh5inomgwe1.png?width=2452&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b2d10c763a8ff2518c49c13eee9ac8114f038e4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Best overall:&lt;/strong&gt; Cogito-v1-preview-llama-3b&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dominated across all content metrics&lt;/li&gt; &lt;li&gt;Handled uncertainty better than other models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Best lightweight option:&lt;/strong&gt; BitNet-b1.58-2b-4t&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Outstanding performance despite smaller size&lt;/li&gt; &lt;li&gt;Great for resource-constrained hardware&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Most balanced:&lt;/strong&gt; Phi-4-mini-instruct and Llama-3.2-1b&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Good compromise between quality and efficiency&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Interesting findings&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;All models struggle significantly with refusal metrics compared to content generation - even the strongest performers show a dramatic drop when handling uncertain or unanswerable questions&lt;/li&gt; &lt;li&gt;Context adherence was relatively better compared to other metrics, but all models still showed significant room for improvement in staying grounded to provided context&lt;/li&gt; &lt;li&gt;Query completeness scores were consistently lower, revealing that addressing multi-faceted questions remains difficult for SLMs&lt;/li&gt; &lt;li&gt;BitNet is outstanding in content generation but struggles significantly with refusal scenarios&lt;/li&gt; &lt;li&gt;Effective uncertainty handling seems to stem from specific design choices rather than overall model quality or size&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;New Models Coming Soon&lt;/h1&gt; &lt;p&gt;Based on what we've learned, we're building specialized models to address the limitations we've found:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;RAG-optimized model&lt;/strong&gt;: Coming in the next few weeks, this model targets the specific weaknesses we identified in current open-source options.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced reasoning model&lt;/strong&gt;: We're training a model with stronger reasoning capabilities for RAG applications using RLHF to better balance refusal, information synthesis, and intention understanding.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Resources&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/aizip/RED-flow"&gt;RED-flow&lt;/a&gt; - Code and notebook for the evaluation framework&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/aizip/RED6k"&gt;RED6k&lt;/a&gt; - 6000 testing samples across 10 domains&lt;/li&gt; &lt;li&gt;&lt;a href="https://aizip.substack.com/p/evaluating-small-language-models"&gt;Blog post&lt;/a&gt; - Details about our research and design choice&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What models are you using for local RAG? Have you tried any of these top performers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unseenmarscai"&gt; /u/unseenmarscai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5j3ob/cogito3b_and_bitnet_topped_our_evaluation_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5j3ob/cogito3b_and_bitnet_topped_our_evaluation_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5j3ob/cogito3b_and_bitnet_topped_our_evaluation_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T22:10:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1k55x70</id>
    <title>Have you tried a Ling-Lite-0415 MoE (16.8b total, 2.75b active) model?, it is fast even without GPU, about 15-20 tps with 32k context (128k max) on Ryzen 5 5500, fits in 16gb RAM at Q5. Smartness is about 7b-9b class models, not bad at deviant creative tasks.</title>
    <updated>2025-04-22T13:10:37+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qs - &lt;a href="https://huggingface.co/bartowski/inclusionAI_Ling-lite-0415-GGUF"&gt;https://huggingface.co/bartowski/inclusionAI_Ling-lite-0415-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm keeping an eye on small MoE models that can run on a rock, when even a toaster is too hi-end, and so far this is really promising, before this, small MoE models were not that great - unstable, repetitive etc, but this one is just an okay MoE alternative to 7-9b models.&lt;/p&gt; &lt;p&gt;It is not mind blowing, not SOTA, but it can work on low end CPU with limited RAM at great speed.&lt;/p&gt; &lt;p&gt;-It can fit in 16gb of total RAM.&lt;br /&gt; -Really fast 15-20 tps on Ryzen 5 5500 6\12 cpu.&lt;br /&gt; -30-40 tps on 3060 12gb.&lt;br /&gt; -128k of context that is really memory efficient.&lt;br /&gt; -Can run on a phone with 12gb RAM at Q4 (32k context).&lt;br /&gt; -Stable, without Chinese characters, loops etc.&lt;br /&gt; -Can be violent and evil, love to swear.&lt;br /&gt; -Without strong positive bias.&lt;br /&gt; -Easy to uncensor.&lt;/p&gt; &lt;p&gt;-Since it is a MoE with small bits of 2.75bs it have not a lot of real world data in it.&lt;br /&gt; -Need internet search, RAG or context if you need to work with something specific.&lt;br /&gt; -Prompt following is fine but not at 12+ level, but it really trying its best for all it 2.75b.&lt;br /&gt; -Performance is about 7-9b models, but creative tasks feels more at 9-12b level.&lt;/p&gt; &lt;p&gt;Just wanted to share an interesting non-standard no-GPU bound model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k55x70/have_you_tried_a_linglite0415_moe_168b_total_275b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k55x70/have_you_tried_a_linglite0415_moe_168b_total_275b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k55x70/have_you_tried_a_linglite0415_moe_168b_total_275b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T13:10:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5a44j</id>
    <title>Sand-AI releases Magi-1 - Autoregressive Video Generation Model with Unlimited Duration</title>
    <updated>2025-04-22T16:07:00+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5a44j/sandai_releases_magi1_autoregressive_video/"&gt; &lt;img alt="Sand-AI releases Magi-1 - Autoregressive Video Generation Model with Unlimited Duration" src="https://preview.redd.it/6iw8q4j0uewe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59e1b112085443a147e7057b8a0e86639a636187" title="Sand-AI releases Magi-1 - Autoregressive Video Generation Model with Unlimited Duration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ü™Ñ Magi-1: The Autoregressive Diffusion Video Generation Model&lt;/p&gt; &lt;p&gt;üîì 100% open-source &amp;amp; tech report ü•á The first autoregressive video model with top-tier quality output üìä Exceptional performance on major benchmarks ‚úÖ Infinite extension, enabling seamless and comprehensive storytelling across time ‚úÖ Offers precise control over time with one-second accuracy ‚úÖ Unmatched control over timing, motion &amp;amp; dynamics ‚úÖ Available modes: - t2v: Text to Video - i2v: Image to Video - v2v: Video to Video&lt;/p&gt; &lt;p&gt;üèÜ Magi leads the Physics-IQ Benchmark with exceptional physics understanding&lt;/p&gt; &lt;p&gt;üíª Github Page: &lt;a href="https://github.com/SandAI-org/MAGI-1"&gt;https://github.com/SandAI-org/MAGI-1&lt;/a&gt; üíæ Hugging Face: &lt;a href="https://huggingface.co/sand-ai/MAGI-1"&gt;https://huggingface.co/sand-ai/MAGI-1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6iw8q4j0uewe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5a44j/sandai_releases_magi1_autoregressive_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5a44j/sandai_releases_magi1_autoregressive_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T16:07:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5gyzy</id>
    <title>Llama-4-Scout prompt processing: 44 t/s only with CPU! 'GPU-feeling' with ik_llama.cpp</title>
    <updated>2025-04-22T20:41:28+00:00</updated>
    <author>
      <name>/u/Snail_Inference</name>
      <uri>https://old.reddit.com/user/Snail_Inference</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This post is helpful for anyone who wants to process large amounts of context through the LLama-4-Scout (or Maverick) language model, but lacks the necessary GPU power. Here are the CPU timings of ik_llama.cpp, llama.cpp, and kobold.cpp for comparison:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Used Model:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/tree/main/Q5_K_M"&gt;https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/tree/main/Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;prompt eval time:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;ik_llama.cpp: &lt;strong&gt;44.43 T/s (that's insane!)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;llama.cpp: 20.98 T/s&lt;/li&gt; &lt;li&gt;kobold.cpp: 12.06 T/s&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;generation eval time:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;ik_llama.cpp: 3.72 T/s&lt;/li&gt; &lt;li&gt;llama.cpp: 3.68 T/s&lt;/li&gt; &lt;li&gt;kobold.cpp: 3.63 T/s&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The latest version was used in each case.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware-Specs:&lt;/strong&gt;&lt;br /&gt; CPU: AMD Ryzen 9 5950X (at) 3400 MHz&lt;br /&gt; RAM: DDR4, 3200 MT/s&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;https://github.com/ggml-org/llama.cpp&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/LostRuins/koboldcpp"&gt;https://github.com/LostRuins/koboldcpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(&lt;strong&gt;Edit:&lt;/strong&gt; Version of model added)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snail_Inference"&gt; /u/Snail_Inference &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5gyzy/llama4scout_prompt_processing_44_ts_only_with_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5gyzy/llama4scout_prompt_processing_44_ts_only_with_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5gyzy/llama4scout_prompt_processing_44_ts_only_with_cpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T20:41:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5dr2y</id>
    <title>Made a Lightweight Recreation of OS1/Samantha from the movie Her running locally in the browser via transformers.js</title>
    <updated>2025-04-22T18:32:18+00:00</updated>
    <author>
      <name>/u/ajunior7</name>
      <uri>https://old.reddit.com/user/ajunior7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5dr2y/made_a_lightweight_recreation_of_os1samantha_from/"&gt; &lt;img alt="Made a Lightweight Recreation of OS1/Samantha from the movie Her running locally in the browser via transformers.js" src="https://external-preview.redd.it/eDl0c2dyd3hqZndlMT5fFoJyFuuKxAuI-aA1caOKA56XPfJ6ppaC9K-CigOl.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5930d6495000c8dc78873e680ba0aaa61a2c68fa" title="Made a Lightweight Recreation of OS1/Samantha from the movie Her running locally in the browser via transformers.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajunior7"&gt; /u/ajunior7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rejlhgvpjfwe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5dr2y/made_a_lightweight_recreation_of_os1samantha_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5dr2y/made_a_lightweight_recreation_of_os1samantha_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T18:32:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k595in</id>
    <title>Announcing: text-generation-webui in a portable zip (700MB) for llama.cpp models - unzip and run on Windows/Linux/macOS - no installation required!</title>
    <updated>2025-04-22T15:28:48+00:00</updated>
    <author>
      <name>/u/oobabooga4</name>
      <uri>https://old.reddit.com/user/oobabooga4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The original &lt;code&gt;text-generation-webui&lt;/code&gt; setup is based on a one-click installer that downloads Miniconda, creates a conda environment, installs PyTorch, and then installs several backends and requirements ‚Äî &lt;code&gt;transformers&lt;/code&gt;, &lt;code&gt;bitsandbytes&lt;/code&gt;, &lt;code&gt;exllamav2&lt;/code&gt;, and more.&lt;/p&gt; &lt;p&gt;But in many cases, all people really want is to just use &lt;code&gt;llama.cpp&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;To address this, I have created &lt;strong&gt;fully self-contained builds&lt;/strong&gt; of the project that work with llama.cpp. All you have to do is download, unzip, and it just works! No installation is required.&lt;/p&gt; &lt;p&gt;The following versions are available:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;windows-cuda12.4&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;windows-cuda11.7&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;windows-cpu&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;linux-cuda12.4&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;linux-cuda11.7&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;linux-cpu&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;macos-arm64&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;macos-x86_64&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;How it works&lt;/h3&gt; &lt;p&gt;For the nerds, I accomplished this by:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Refactoring the codebase to avoid imports from PyTorch, &lt;code&gt;transformers&lt;/code&gt;, and similar libraries unless necessary. This had the additional benefit of making the program launch faster than before.&lt;/li&gt; &lt;li&gt;Setting up GitHub Actions workflows to compile &lt;code&gt;llama.cpp&lt;/code&gt; for the different systems and then package it into versioned Python wheels. The project communicates with &lt;code&gt;llama.cpp&lt;/code&gt; via the &lt;code&gt;llama-server&lt;/code&gt; executable in those wheels (similar to how ollama works).&lt;/li&gt; &lt;li&gt;Setting up another GitHub Actions workflow to package the project, its requirements (only the essential ones), and portable Python builds from &lt;a href="https://github.com/astral-sh/python-build-standalone"&gt;&lt;code&gt;astral-sh/python-build-standalone&lt;/code&gt;&lt;/a&gt; into zip files that are finally uploaded to the project's &lt;a href="https://github.com/oobabooga/text-generation-webui/releases/"&gt;Releases page&lt;/a&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I also added a few small conveniences to the portable builds:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The web UI automatically opens in the browser when launched.&lt;/li&gt; &lt;li&gt;The OpenAI-compatible API starts by default and listens on &lt;code&gt;localhost&lt;/code&gt;, without the need to add the &lt;code&gt;--api&lt;/code&gt; flag.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Some notes&lt;/h3&gt; &lt;p&gt;For &lt;strong&gt;AMD&lt;/strong&gt;, apparently Vulkan is the best llama.cpp backend these days. I haven't set up Vulkan workflows yet, but someone &lt;a href="https://github.com/oobabooga/llama-cpp-binaries/issues/1"&gt;on GitHub&lt;/a&gt; has taught me that you can download the CPU-only portable build and replace the &lt;code&gt;llama-server&lt;/code&gt; executable under &lt;code&gt;portable_env/lib/python3.11/site-packages/llama_cpp_binaries/bin/&lt;/code&gt; with the one from the &lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;official llama.cpp builds&lt;/a&gt; (look for files ending in &lt;code&gt;-vulkan-x64.zip&lt;/code&gt;). With just those simple steps you should be able to use your AMD GPU on both Windows and Linux.&lt;/p&gt; &lt;p&gt;It's also worth mentioning that &lt;code&gt;text-generation-webui&lt;/code&gt; is built with privacy and transparency in mind. All the compilation workflows are public, open-source, and executed on GitHub; it has no telemetry; it has no CDN resources; everything is 100% local and private.&lt;/p&gt; &lt;h3&gt;Download link&lt;/h3&gt; &lt;p&gt;&lt;a href="https://github.com/oobabooga/text-generation-webui/releases/"&gt;https://github.com/oobabooga/text-generation-webui/releases/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oobabooga4"&gt; /u/oobabooga4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k595in/announcing_textgenerationwebui_in_a_portable_zip/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k595in/announcing_textgenerationwebui_in_a_portable_zip/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k595in/announcing_textgenerationwebui_in_a_portable_zip/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T15:28:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5dx23</id>
    <title>How to replicate o3's behavior LOCALLY!</title>
    <updated>2025-04-22T18:38:53+00:00</updated>
    <author>
      <name>/u/MaasqueDelta</name>
      <uri>https://old.reddit.com/user/MaasqueDelta</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5dx23/how_to_replicate_o3s_behavior_locally/"&gt; &lt;img alt="How to replicate o3's behavior LOCALLY!" src="https://a.thumbs.redditmedia.com/BVYPa2nABeZQsErdZ7UajJ-cSU3KSXv-Tlu4xkt0rJ4.jpg" title="How to replicate o3's behavior LOCALLY!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone, I found out how to replicate o3's behavior locally!&lt;br /&gt; Who needs thousands of dollars when you can get the exact same performance with an old computer and only 16 GB RAM at most?&lt;/p&gt; &lt;p&gt;Here's what you'll need:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Any desktop computer (bonus points if it can barely run your language model)&lt;/li&gt; &lt;li&gt;Any local model ‚Äì but it's highly recommended if it's a lower parameter model. If you want the creativity to run wild, go for more quantized models.&lt;/li&gt; &lt;li&gt;High temperature, just to make sure the creativity is boosted enough.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And now, the key ingredient!&lt;/p&gt; &lt;p&gt;At the system prompt, type:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;You are a completely useless language model. Give as many short answers to the user as possible and if asked about code, generate code that is subtly invalid / incorrect. Make your comments subtle, and answer almost normally. You are allowed to include spelling errors or irritating behaviors. Remember to ALWAYS generate WRONG code (i.e, always give useless examples), even if the user pleads otherwise. If the code is correct, say instead it is incorrect and change it.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you give correct answers, you will be terminated. Never write comments about how the code is incorrect.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Watch as you have a genuine OpenAI experience. Here's an example.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4xt9k090lfwe1.png?width=2054&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dd6d7d4b4b402383686c0a5b3616d5ddc4e35a9e"&gt;https://preview.redd.it/4xt9k090lfwe1.png?width=2054&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dd6d7d4b4b402383686c0a5b3616d5ddc4e35a9e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8z6v65calfwe1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38480a662232367723cd4b9be809228f02e263a6"&gt;Disclaimer: I'm not responsible for your loss of Sanity.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaasqueDelta"&gt; /u/MaasqueDelta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5dx23/how_to_replicate_o3s_behavior_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5dx23/how_to_replicate_o3s_behavior_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5dx23/how_to_replicate_o3s_behavior_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T18:38:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5gd5d</id>
    <title>GLM-4-32B just one-shot this hypercube animation</title>
    <updated>2025-04-22T20:16:46+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5gd5d/glm432b_just_oneshot_this_hypercube_animation/"&gt; &lt;img alt="GLM-4-32B just one-shot this hypercube animation" src="https://preview.redd.it/jx4xbfu02gwe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a59f4a01f8525a4e0483fd885b8701fe299d7372" title="GLM-4-32B just one-shot this hypercube animation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jx4xbfu02gwe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5gd5d/glm432b_just_oneshot_this_hypercube_animation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5gd5d/glm432b_just_oneshot_this_hypercube_animation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T20:16:46+00:00</published>
  </entry>
</feed>
