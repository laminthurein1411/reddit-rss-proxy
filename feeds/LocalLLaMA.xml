<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-22T06:25:02+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jh0n3q</id>
    <title>PSA: Get Flash Attention v2 on AMD 7900 (gfx1100)</title>
    <updated>2025-03-22T04:37:53+00:00</updated>
    <author>
      <name>/u/canesin</name>
      <uri>https://old.reddit.com/user/canesin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Considering you have installed ROCm, PyTorch (official website worked) git and uv:&lt;/p&gt; &lt;p&gt;&lt;code&gt;uv pip install pip triton==3.2.0&lt;/code&gt;&lt;br /&gt; &lt;code&gt;git clone --single-branch --branch main_perf&lt;/code&gt; &lt;a href="https://github.com/ROCm/flash-attention.git"&gt;&lt;code&gt;https://github.com/ROCm/flash-attention.git&lt;/code&gt;&lt;/a&gt;&lt;br /&gt; &lt;code&gt;cd flash-attention/&lt;/code&gt;&lt;br /&gt; &lt;code&gt;export FLASH_ATTENTION_TRITON_AMD_ENABLE=&amp;quot;TRUE&amp;quot;&lt;/code&gt;&lt;br /&gt; &lt;code&gt;export GPU_ARCHS=&amp;quot;gfx1100&amp;quot;&lt;/code&gt;&lt;br /&gt; &lt;code&gt;python&lt;/code&gt; &lt;a href="http://setup.py"&gt;&lt;code&gt;setup.py&lt;/code&gt;&lt;/a&gt; &lt;code&gt;install&lt;/code&gt;&lt;/p&gt; &lt;p&gt;:-)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/canesin"&gt; /u/canesin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh0n3q/psa_get_flash_attention_v2_on_amd_7900_gfx1100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh0n3q/psa_get_flash_attention_v2_on_amd_7900_gfx1100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh0n3q/psa_get_flash_attention_v2_on_amd_7900_gfx1100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T04:37:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgdvr7</id>
    <title>GAIA: An Open-Source Project from AMD for Running Local LLMs on Ryzenâ„¢ AI</title>
    <updated>2025-03-21T10:41:39+00:00</updated>
    <author>
      <name>/u/blazerx</name>
      <uri>https://old.reddit.com/user/blazerx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgdvr7/gaia_an_opensource_project_from_amd_for_running/"&gt; &lt;img alt="GAIA: An Open-Source Project from AMD for Running Local LLMs on Ryzenâ„¢ AI" src="https://external-preview.redd.it/FfsH6_lD8ZDjWq58F-nI0b3jtl3po6p4fWNK_dOGUbY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=067fd5517027950d0c15b75fbdf4301c41337833" title="GAIA: An Open-Source Project from AMD for Running Local LLMs on Ryzenâ„¢ AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blazerx"&gt; /u/blazerx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.amd.com/en/developer/resources/technical-articles/gaia-an-open-source-project-from-amd-for-running-local-llms-on-ryzen-ai.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgdvr7/gaia_an_opensource_project_from_amd_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgdvr7/gaia_an_opensource_project_from_amd_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T10:41:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgvndh</id>
    <title>Open-Schizo-Leaderboard (The anti-leaderboard)</title>
    <updated>2025-03-22T00:11:39+00:00</updated>
    <author>
      <name>/u/Rombodawg</name>
      <uri>https://old.reddit.com/user/Rombodawg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its fun to see how bonkers model cards can be. Feel free to help me improve the code to better finetune the leaderboard filtering.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/rombodawg/Open-Schizo-Leaderboard"&gt;https://huggingface.co/spaces/rombodawg/Open-Schizo-Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rombodawg"&gt; /u/Rombodawg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgvndh/openschizoleaderboard_the_antileaderboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgvndh/openschizoleaderboard_the_antileaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgvndh/openschizoleaderboard_the_antileaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T00:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh1xpn</id>
    <title>Have you had a chance to try Trae, ByteDance's new AI-powered IDE built on VSCode? What are your initial thoughts or early impressions?</title>
    <updated>2025-03-22T06:02:46+00:00</updated>
    <author>
      <name>/u/valentino99</name>
      <uri>https://old.reddit.com/user/valentino99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ByteDance has introduced a new AI-powered editor named Trae, positioning itself as a competitor to established players like Cursor and Windsurf. Built on the foundation of VSCode, Trae boasts a sleek, modernized user interface that blends elements of JetBrains Fleet and VSCode, offering a fresh take on the traditional VSCode design.&lt;/p&gt; &lt;p&gt;One of Trae's standout features is its unlimited free access to advanced AI models, including GPT-4o and Claude-3.7-Sonnet, making it a powerful tool for developers.&lt;/p&gt; &lt;p&gt;It also supports VSCode configurations and allows users to import plugins seamlessly. Currently, Trae is available exclusively for macOS, with a Windows version in the works.&lt;/p&gt; &lt;p&gt;Trae is owned by ByteDance (tiktok), so it means Chinese Servers, and some people don't like that.&lt;/p&gt; &lt;p&gt;What are your thoughts?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.trae.ai/home"&gt;https://www.trae.ai/home&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valentino99"&gt; /u/valentino99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh1xpn/have_you_had_a_chance_to_try_trae_bytedances_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh1xpn/have_you_had_a_chance_to_try_trae_bytedances_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh1xpn/have_you_had_a_chance_to_try_trae_bytedances_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T06:02:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgau52</id>
    <title>Gemma 3 27b vs. Mistral 24b vs. QwQ 32b: I tested on personal benchmark, here's what I found out</title>
    <updated>2025-03-21T06:53:42+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was looking for LLMs to use locally; the requirements are good enough reasoning and understanding, coding, and some elementary-level mathematics. I was looking into QwQ 32b, which seemed very promising.&lt;br /&gt; Last week, Google and Mistral released Gemma 3 27b and Mistral small 3.1 24b; from the benchmarks, both seem capable models approximating Deepseek r1 in ELO rating, which is impressive.&lt;/p&gt; &lt;p&gt;But, tbh, I have stopped caring about benchmarks, especially Lmsys; idk. The rankings always seem off when you try the models IRL.&lt;/p&gt; &lt;p&gt;So, I ran a small test to vibe-check which models to pick. I also benchmarked answers with Deepseek r1, as I use it often to get a better picture. &lt;/p&gt; &lt;p&gt;Here's what I found out&lt;/p&gt; &lt;h1&gt;For Coding&lt;/h1&gt; &lt;p&gt;QwQ 32b is just miles ahead in coding among the three. It sometimes does better code than Deepseek r1. They weren't lying in the benchmarks. It feels good to talk to you as well. Gemma is 2nd and does the job for easy tasks. Mistral otoh was bad.&lt;/p&gt; &lt;h1&gt;For Reasoning&lt;/h1&gt; &lt;p&gt;Again, Qwen was better. Well, ofc it's a reasoning model, but Gemma was also excellent. They made a good base model. Mistral was there but not there.&lt;/p&gt; &lt;h1&gt;For Math&lt;/h1&gt; &lt;p&gt;Gemma and QwQ were good enough for simple math tasks. Gemma, being a base model, was faster. I might test more with these two. Mistral was decent but 3rd again.&lt;/p&gt; &lt;h1&gt;What to pick?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;QwQ 32b is no doubt the best available model in its class. Great at coding, reasoning, and math. It's been a long since I used a local model, the last one was Mixtral, a year ago, and I never expected them to be this good. QwQ is promising; I can't wait for their new max model.&lt;/li&gt; &lt;li&gt;Gemma 3 27b is a solid base model. Great vibes. And you wouldn't be missing a lot with this. But it comes with a Gemma-specific license, which is more restrictive than Apache 2.0.&lt;/li&gt; &lt;li&gt;Mistral small 3.1 24b didn't impress me much; perhaps it needs more rigorous testing. &lt;/li&gt; &lt;li&gt;Both Gemma and Mistral Small have image support, so consider that as well.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For the complete analysis, check out this blog post: &lt;a href="https://composio.dev/blog/qwq-32b-vs-gemma-3-mistral-small-vs-deepseek-r1/"&gt;Gemma 3 27b vs QwQ 32b vs Mistral 24b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would love to know which other model you're currently using and for what specific tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgau52/gemma_3_27b_vs_mistral_24b_vs_qwq_32b_i_tested_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgau52/gemma_3_27b_vs_mistral_24b_vs_qwq_32b_i_tested_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgau52/gemma_3_27b_vs_mistral_24b_vs_qwq_32b_i_tested_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T06:53:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgyj01</id>
    <title>RTX PRO 5000 Laptop 24GB GDDR7 10496 cores 175W</title>
    <updated>2025-03-22T02:38:52+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;256-bit 896GB/s bandwidth. 228TFLOPS Tensor Core F16 (60% faster than 3090).&lt;/p&gt; &lt;p&gt;Should have made a similar desktop card that would be a no-brainer upgrade for the 3090/4090 users.&lt;/p&gt; &lt;p&gt;&lt;a href="https://videocardz.com/newz/nvidia-announces-rtx-pro-blackwell-laptop-gpus-up-to-10496-cuda-cores-and-24gb-gddr7-memory"&gt;https://videocardz.com/newz/nvidia-announces-rtx-pro-blackwell-laptop-gpus-up-to-10496-cuda-cores-and-24gb-gddr7-memory&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgyj01/rtx_pro_5000_laptop_24gb_gddr7_10496_cores_175w/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgyj01/rtx_pro_5000_laptop_24gb_gddr7_10496_cores_175w/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgyj01/rtx_pro_5000_laptop_24gb_gddr7_10496_cores_175w/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T02:38:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jguyty</id>
    <title>I analyzed the word statistics in the reasoning traces of different llms - it seems many models are trained on R1 traces</title>
    <updated>2025-03-21T23:39:38+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jguyty/i_analyzed_the_word_statistics_in_the_reasoning/"&gt; &lt;img alt="I analyzed the word statistics in the reasoning traces of different llms - it seems many models are trained on R1 traces" src="https://b.thumbs.redditmedia.com/ahdGQ2lOgkPoEe4ifpzcMZTGeQzhCkDStw1-ajCYsdo.jpg" title="I analyzed the word statistics in the reasoning traces of different llms - it seems many models are trained on R1 traces" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I extracted thinking traces from different LLMs for the prompt below and analyzed the frequency of the first word in each line. The heatmap below shows the frequency of the most used words in each LLM.&lt;/p&gt; &lt;p&gt;The aim is to identify relationships between different thinking models. For example, it is know that certain words/tokens like &amp;quot;wait&amp;quot; indicate backtracking in the thinking process. These patterns emerge during the reinforcement learning process and can also be trained by finetuning the model on thinking traces.&lt;/p&gt; &lt;p&gt;We can see that a lot of models show a word statistic similar to R1. This may be random, but could also mean that the model has seen R1 thinking traces at some point in the process.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xp3v4grjp4qe1.png?width=2789&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9ecac49e05f3d6e483b341869d4c14d9f1f89bb"&gt;https://preview.redd.it/xp3v4grjp4qe1.png?width=2789&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9ecac49e05f3d6e483b341869d4c14d9f1f89bb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The prompt I used:&lt;br /&gt; &lt;em&gt;You have two ropes, each of which takes exactly 60 minutes to burn completely. However, the ropes burn unevenly, meaning some parts may burn faster or slower than others. You have no other timing device. How can you measure exactly 20 minutes using these two ropes and matches to light them?&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jguyty/i_analyzed_the_word_statistics_in_the_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jguyty/i_analyzed_the_word_statistics_in_the_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jguyty/i_analyzed_the_word_statistics_in_the_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T23:39:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgzpdb</id>
    <title>What are you using local LLMs for? How do they compare to the big tech offerings?</title>
    <updated>2025-03-22T03:43:40+00:00</updated>
    <author>
      <name>/u/TedHoliday</name>
      <uri>https://old.reddit.com/user/TedHoliday</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m just curious what all people are using local LLMs for. For me personally, I use Claude daily at work I like the idea of running an LLM locally, but I know it would be less accurate than my single PC with one single RTX 4090. &lt;/p&gt; &lt;p&gt;I like the idea of not being subject to the constantly changing pricing models and worrying about how many tokens Iâ€™ve used up, but I feel like even like 5% more accurate code is worth it due to the time it can save.&lt;/p&gt; &lt;p&gt;So Iâ€™m just curious what people are using them for, and how are they now compared to the big players (and with what hardware)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TedHoliday"&gt; /u/TedHoliday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgzpdb/what_are_you_using_local_llms_for_how_do_they/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgzpdb/what_are_you_using_local_llms_for_how_do_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgzpdb/what_are_you_using_local_llms_for_how_do_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T03:43:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgleax</id>
    <title>Hunyuan releases T1 reasoning model</title>
    <updated>2025-03-21T16:48:08+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgleax/hunyuan_releases_t1_reasoning_model/"&gt; &lt;img alt="Hunyuan releases T1 reasoning model" src="https://b.thumbs.redditmedia.com/HdMGpIqacQMghH2w635c1CNcfB8i3TAb_aRI9P856rE.jpg" title="Hunyuan releases T1 reasoning model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hunyuan announces T1 reasoning model&lt;/p&gt; &lt;p&gt;Meet Hunyuan-T1, the latest breakthrough in AI reasoning! Powered by Hunyuan TurboS, it's built for speed, accuracy, and efficiency. ðŸ”¥&lt;/p&gt; &lt;p&gt;âœ… Hybrid-Mamba-Transformer MoE Architecture â€“ The first of its kind for ultra-large-scale reasoning âœ… Strong Logic &amp;amp; Concise Writing â€“ Precise following of complex instructions âœ… Low Hallucination in Summaries â€“Trustworthy and reliable outputs âœ… Blazing Fast â€“First character in 1 sec, 60-80 tokens/sec generation speed âœ… Excellent Long-Text Processing â€“Handle complex contexts with ease&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://llm.hunyuan.tencent.com/#/blog/hy-t1?lang=en"&gt;https://llm.hunyuan.tencent.com/#/blog/hy-t1?lang=en&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://huggingface.co/spaces/tencent/Hunyuan-T1"&gt;https://huggingface.co/spaces/tencent/Hunyuan-T1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;** Model weights have not been released yet, but based on Hunyuanâ€™s promise to open source their models, I expect the weights to be released soon **&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jgleax"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgleax/hunyuan_releases_t1_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgleax/hunyuan_releases_t1_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T16:48:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgqx85</id>
    <title>AITER: AI Tensor Engine For ROCm</title>
    <updated>2025-03-21T20:38:55+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://rocm.blogs.amd.com/software-tools-optimization/aiter%3A-ai-tensor-engine-for-rocm%E2%84%A2/README.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqx85/aiter_ai_tensor_engine_for_rocm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqx85/aiter_ai_tensor_engine_for_rocm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T20:38:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh01fi</id>
    <title>Can someone ELI5 what makes NVIDIA a monopoly in AI race?</title>
    <updated>2025-03-22T04:02:06+00:00</updated>
    <author>
      <name>/u/Trysem</name>
      <uri>https://old.reddit.com/user/Trysem</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I heard somewhere it's cuda,then why some other companies like AMD is not making something like cuda of their own?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trysem"&gt; /u/Trysem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh01fi/can_someone_eli5_what_makes_nvidia_a_monopoly_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh01fi/can_someone_eli5_what_makes_nvidia_a_monopoly_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh01fi/can_someone_eli5_what_makes_nvidia_a_monopoly_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T04:02:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgmavv</id>
    <title>Llama 3.3 Nemotron 49B Super appears on LMSYS Arena</title>
    <updated>2025-03-21T17:25:23+00:00</updated>
    <author>
      <name>/u/jpydych</name>
      <uri>https://old.reddit.com/user/jpydych</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgmavv/llama_33_nemotron_49b_super_appears_on_lmsys_arena/"&gt; &lt;img alt="Llama 3.3 Nemotron 49B Super appears on LMSYS Arena" src="https://preview.redd.it/v330lkoru2qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e0578ce6d6bf5ebdcb8c012e8e2f02052435408" title="Llama 3.3 Nemotron 49B Super appears on LMSYS Arena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jpydych"&gt; /u/jpydych &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v330lkoru2qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgmavv/llama_33_nemotron_49b_super_appears_on_lmsys_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgmavv/llama_33_nemotron_49b_super_appears_on_lmsys_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T17:25:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh1m45</id>
    <title>Why Do I Feel Poor Each Time I Decide to Buy a New GPU Even Though I Make More Money?</title>
    <updated>2025-03-22T05:40:29+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean for God sake, this curse has been haunting me for decades now. The first time I bought a GPU with my own money, I had to dream for it for months, saving money every month for my scholarship. When I went to buy my dream GPU, prices increased and I ended up buying a mid-range NVIDIA card (I had to buy other PC component which were expensive). Then years later I got busy with work and had Playstation, so I didn't really need a good PC, couple with the fact that laptop prices were getting cheaper and performant, I just didn't need to build a new rig.&lt;/p&gt; &lt;p&gt;Fast forward a few year, and my old dream to create my own games came back strong, and I decided to learn (seriously this time) 3D modeling and rendering. There is just something satisfying fooling untrained (or trained) eyes looking at a CGI production and thinking it's real.&lt;br /&gt; That's when I decided to build a new PC. Alas, the new age of crypto reaches its peak and yeah.. shortage of GPUs. Then, I felt poor again even after my several years of work and money saving.&lt;/p&gt; &lt;p&gt;Then COVID hits, and an RTX3090 cost $4000, if you get your hand on one. I bought multiple parts from different countries just to minimize my spending, and I felt very poor.&lt;/p&gt; &lt;p&gt;Which brings me to today. I want to build a new rig from my new passion; tinkering with AI. Alas, I have the money to buy any GPU I want, but my damn rational brain isn't allowing me!!! It's too expensive.. Am I insane? An RTX5090 at a price equivalent to a second hand car is NOT A SMART PURCHASE. And, it only comes with 32GB of VRAM. I'd still run the same models my now old 3090 can run...&lt;/p&gt; &lt;p&gt;In short, no matter how much my income increases over the years, I will always feel poor when I want to buy an new GPU ðŸ˜­ðŸ˜­ðŸ˜­&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh1m45/why_do_i_feel_poor_each_time_i_decide_to_buy_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh1m45/why_do_i_feel_poor_each_time_i_decide_to_buy_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh1m45/why_do_i_feel_poor_each_time_i_decide_to_buy_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T05:40:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgkqio</id>
    <title>New BitNet Model from Deepgrove</title>
    <updated>2025-03-21T16:20:41+00:00</updated>
    <author>
      <name>/u/Jake-Boggs</name>
      <uri>https://old.reddit.com/user/Jake-Boggs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgkqio/new_bitnet_model_from_deepgrove/"&gt; &lt;img alt="New BitNet Model from Deepgrove" src="https://external-preview.redd.it/75Zv26Tb9ec8ndEGYBOYu42vtVHBipVRmB1cGkts4ZM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd92282c47c9015d2d9452a76f2cbf3d52257025" title="New BitNet Model from Deepgrove" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jake-Boggs"&gt; /u/Jake-Boggs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/deepgrove-ai/Bonsai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgkqio/new_bitnet_model_from_deepgrove/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgkqio/new_bitnet_model_from_deepgrove/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T16:20:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgft94</id>
    <title>ByteDance released on HuggingFace an open image model that generates Photo While Preserving Your Identity</title>
    <updated>2025-03-21T12:37:24+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgft94/bytedance_released_on_huggingface_an_open_image/"&gt; &lt;img alt="ByteDance released on HuggingFace an open image model that generates Photo While Preserving Your Identity" src="https://preview.redd.it/efejft8gf1qe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e80345d3319d8c121635235946e7b1d2c0eb17a6" title="ByteDance released on HuggingFace an open image model that generates Photo While Preserving Your Identity" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Flexible Photo Recrafting While Preserving Your Identity&lt;/p&gt; &lt;p&gt;Project page: &lt;a href="https://bytedance.github.io/InfiniteYou/"&gt;https://bytedance.github.io/InfiniteYou/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/bytedance/InfiniteYou"&gt;https://github.com/bytedance/InfiniteYou&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/ByteDance/InfiniteYou"&gt;https://huggingface.co/ByteDance/InfiniteYou&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/efejft8gf1qe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgft94/bytedance_released_on_huggingface_an_open_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgft94/bytedance_released_on_huggingface_an_open_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T12:37:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgnye9</id>
    <title>RTX Pro Blackwell Pricing Listed</title>
    <updated>2025-03-21T18:33:48+00:00</updated>
    <author>
      <name>/u/AlohaGrassDragon</name>
      <uri>https://old.reddit.com/user/AlohaGrassDragon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RTX Pro Blackwell pricing is up on &lt;a href="http://connection.com"&gt;connection.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;6000 (24064 cores, 96GB, 1.8 TB/s, 600W, 2-slot flow through) - $8565&lt;/p&gt; &lt;p&gt;6000 Max-Q (24064 cores, 96GB, 1.8 TB/s, 300W, 2-slot blower) - $8565&lt;/p&gt; &lt;p&gt;5000 (14080 cores, 48GB, 1.3 TB/s, 300W, 2-slot blower) - $4569&lt;/p&gt; &lt;p&gt;4500 (10496 cores, 32GB, 896 GB/s, 200W, 2-slot blower) - $2623&lt;/p&gt; &lt;p&gt;4000 (8960 cores, 24GB, 672 GB/s, 140W, 1-slot blower) - $1481&lt;/p&gt; &lt;p&gt;I'm not sure if this is real or final pricing, but I could see some of these models being compelling for local LLM. The 5000 is competitive with current A6000 used pricing, the 4500 is not too far away price-wise from a 5090 with better power/thermals, and the 4000 with 24 GB in a single slot for ~$1500 at 140W is very competitive with a used 3090. It costs more than a 3090, but comes with a warranty and you can fit many more in a system because of the size and power without having to implement an expensive watercooling or dual power supply setup.&lt;/p&gt; &lt;p&gt;All-in-all, if this is real pricing, it looks to me that they are marketing to us directly and they see their biggest competitor as used nVidia cards.&lt;/p&gt; &lt;p&gt;*Edited to add per-card specs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlohaGrassDragon"&gt; /u/AlohaGrassDragon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgnye9/rtx_pro_blackwell_pricing_listed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgnye9/rtx_pro_blackwell_pricing_listed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgnye9/rtx_pro_blackwell_pricing_listed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T18:33:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgsw1e</id>
    <title>We built an open source mock interviews platform empowered by ollama</title>
    <updated>2025-03-21T22:04:49+00:00</updated>
    <author>
      <name>/u/Boring_Rabbit2275</name>
      <uri>https://old.reddit.com/user/Boring_Rabbit2275</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgsw1e/we_built_an_open_source_mock_interviews_platform/"&gt; &lt;img alt="We built an open source mock interviews platform empowered by ollama" src="https://preview.redd.it/4k3ec6po84qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24694323661f97c1dfe5bb63a0f6f9d073c2e8ef" title="We built an open source mock interviews platform empowered by ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Come practice your interviews for free using our project on GitHub here: &lt;a href="https://github.com/Azzedde/aiva_mock_interviews"&gt;https://github.com/Azzedde/aiva_mock_interviews&lt;/a&gt; We are two junior AI engineers, and we would really appreciate feedback on our work. Please star it if you like it.&lt;/p&gt; &lt;p&gt;We find that the junior era is full of uncertainty, and we want to know if we are doing good work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Boring_Rabbit2275"&gt; /u/Boring_Rabbit2275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4k3ec6po84qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgsw1e/we_built_an_open_source_mock_interviews_platform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgsw1e/we_built_an_open_source_mock_interviews_platform/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T22:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgfmn8</id>
    <title>Docker's response to Ollama</title>
    <updated>2025-03-21T12:27:37+00:00</updated>
    <author>
      <name>/u/Barry_Jumps</name>
      <uri>https://old.reddit.com/user/Barry_Jumps</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Am I the only one excited about this?&lt;/p&gt; &lt;p&gt;Soon we can &lt;code&gt;docker run model mistral/mistral-small&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.docker.com/llm/"&gt;https://www.docker.com/llm/&lt;/a&gt;&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=mk_2MIWxLI0&amp;amp;t=1544s"&gt;https://www.youtube.com/watch?v=mk_2MIWxLI0&amp;amp;t=1544s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most exciting for me is that docker desktop will finally allow container to access my Mac's GPU&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Barry_Jumps"&gt; /u/Barry_Jumps &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgfmn8/dockers_response_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgfmn8/dockers_response_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgfmn8/dockers_response_to_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T12:27:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgopeg</id>
    <title>Orpheus-FastAPI: Local TTS with 8 Voices &amp; Emotion Tags (OpenAI Endpoint Compatible)</title>
    <updated>2025-03-21T19:04:54+00:00</updated>
    <author>
      <name>/u/townofsalemfangay</name>
      <uri>https://old.reddit.com/user/townofsalemfangay</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ðŸ‘‹&lt;/p&gt; &lt;p&gt;I just released Orpheus-FastAPI, a high-performance Text-to-Speech server that connects to your local LLM inference server using Orpheus's latest release. You can hook it up to OpenWebui, SillyTavern, or just use the web interface to generate audio natively.&lt;/p&gt; &lt;p&gt;I'd very much recommend if you want to get the most out of it in terms of suprasegmental features (the modalities of human voice, ums, arrs, pauses, like Sesame has) you use a System prompt to make the model respond as such (including the Syntax baked into the model). I included examples on my git so you can see how close this is to Sesame's CSM.&lt;/p&gt; &lt;p&gt;It uses a quantised version of the Orpheus 3B model (I've also included a direct link to my Q8 GGUF) that can run on consumer hardware, and works with GPUStack (my favourite), LM Studio, or llama.cpp.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Lex-au/Orpheus-FastAPI"&gt;https://github.com/Lex-au/Orpheus-FastAPI&lt;/a&gt;&lt;br /&gt; Model: &lt;a href="https://huggingface.co/lex-au/Orpheus-3b-FT-Q8_0.gguf"&gt;https://huggingface.co/lex-au/Orpheus-3b-FT-Q8_0.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think or if you have questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/townofsalemfangay"&gt; /u/townofsalemfangay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgopeg/orpheusfastapi_local_tts_with_8_voices_emotion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgopeg/orpheusfastapi_local_tts_with_8_voices_emotion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgopeg/orpheusfastapi_local_tts_with_8_voices_emotion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T19:04:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgap0q</id>
    <title>SpatialLM: A large language model designed for spatial understanding</title>
    <updated>2025-03-21T06:43:28+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgap0q/spatiallm_a_large_language_model_designed_for/"&gt; &lt;img alt="SpatialLM: A large language model designed for spatial understanding" src="https://external-preview.redd.it/Z2F4NmRpYWFvenBlMV9xklPr-alq2N0OOZexCtU6lC7spKP7fvQP_oR6XFl8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94d9dd664854a15924490e41428c31c299e3851e" title="SpatialLM: A large language model designed for spatial understanding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9hvol38aozpe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgap0q/spatiallm_a_large_language_model_designed_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgap0q/spatiallm_a_large_language_model_designed_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T06:43:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh0ovc</id>
    <title>MoshiVis by kyutai - first open-source real-time speech model that can talk about images</title>
    <updated>2025-03-22T04:40:57+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh0ovc/moshivis_by_kyutai_first_opensource_realtime/"&gt; &lt;img alt="MoshiVis by kyutai - first open-source real-time speech model that can talk about images" src="https://external-preview.redd.it/Y3ptd2t6NGE3NnFlMax15wl1W2mX3SQ6hWixr4c-XUrnbjt3Ig1vm4pgUatm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53210f67e2cb9507383ccec4a8ff094869da2fcb" title="MoshiVis by kyutai - first open-source real-time speech model that can talk about images" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/v86w8w4a76qe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh0ovc/moshivis_by_kyutai_first_opensource_realtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh0ovc/moshivis_by_kyutai_first_opensource_realtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T04:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgl41s</id>
    <title>Tencent introduces Hunyuan-T1, their large reasoning model. Competing with DeepSeek-R1!</title>
    <updated>2025-03-21T16:36:11+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgl41s/tencent_introduces_hunyuant1_their_large/"&gt; &lt;img alt="Tencent introduces Hunyuan-T1, their large reasoning model. Competing with DeepSeek-R1!" src="https://preview.redd.it/vcb57bt1m2qe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=374682829fe92002bc36926e45cf71896aada6ea" title="Tencent introduces Hunyuan-T1, their large reasoning model. Competing with DeepSeek-R1!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to their blog post here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vcb57bt1m2qe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgl41s/tencent_introduces_hunyuant1_their_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgl41s/tencent_introduces_hunyuant1_their_large/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T16:36:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgio2g</id>
    <title>Qwen 3 is coming soon!</title>
    <updated>2025-03-21T14:53:25+00:00</updated>
    <author>
      <name>/u/themrzmaster</name>
      <uri>https://old.reddit.com/user/themrzmaster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/36878"&gt;https://github.com/huggingface/transformers/pull/36878&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/themrzmaster"&gt; /u/themrzmaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgio2g/qwen_3_is_coming_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgio2g/qwen_3_is_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgio2g/qwen_3_is_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T14:53:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgp6sw</id>
    <title>China modified 4090s with 48gb sold cheaper than RTX 5090 - water cooled around 3400 usd</title>
    <updated>2025-03-21T19:25:39+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgp6sw/china_modified_4090s_with_48gb_sold_cheaper_than/"&gt; &lt;img alt="China modified 4090s with 48gb sold cheaper than RTX 5090 - water cooled around 3400 usd" src="https://b.thumbs.redditmedia.com/Uv9b5l37Z3HDbLOdsI_RvWZEDLPnFUNe8L0-bY4NmCE.jpg" title="China modified 4090s with 48gb sold cheaper than RTX 5090 - water cooled around 3400 usd" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jgp6sw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgp6sw/china_modified_4090s_with_48gb_sold_cheaper_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgp6sw/china_modified_4090s_with_48gb_sold_cheaper_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T19:25:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgqmlr</id>
    <title>"If we confuse users enough, they will overpay"</title>
    <updated>2025-03-21T20:26:10+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqmlr/if_we_confuse_users_enough_they_will_overpay/"&gt; &lt;img alt="&amp;quot;If we confuse users enough, they will overpay&amp;quot;" src="https://preview.redd.it/epfkc4xxq3qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f18b9505527bc8ed40557544a084be28952fd9b" title="&amp;quot;If we confuse users enough, they will overpay&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/epfkc4xxq3qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqmlr/if_we_confuse_users_enough_they_will_overpay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqmlr/if_we_confuse_users_enough_they_will_overpay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T20:26:10+00:00</published>
  </entry>
</feed>
