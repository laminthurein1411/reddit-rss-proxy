<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-04T06:25:41+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iggetv</id>
    <title>Make your Mistral Small 3 24B Think like R1-distilled models</title>
    <updated>2025-02-03T04:01:42+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"&gt; &lt;img alt="Make your Mistral Small 3 24B Think like R1-distilled models" src="https://external-preview.redd.it/P8lS0kk6BFe2IEo6TxCZd1LVwksc34IkzGTVx_SCc8w.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d74dbe4f1d67cc8b587db9aa01762f26e269bcf" title="Make your Mistral Small 3 24B Think like R1-distilled models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been seeing a lot of posts about the Mistral Small 3 24B model, and I remember having this CoT system prompt in my collection. I might as well try it out on this new model. I haven't used it for a long time since I switched to R1-distilled-32b. &lt;/p&gt; &lt;p&gt;I'm not the original writer of this prompt; I've rewritten some parts of it, and I can't remember where I got it from.&lt;/p&gt; &lt;p&gt;System prompt: &lt;a href="https://pastebin.com/sVMrgZBp"&gt;https://pastebin.com/sVMrgZBp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is just an experiment. I doubt it will actually make your model smarter in a noticeable way, this is not a replacement of Mistral's furture reasoning models&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/d1geatbckuge1.gif"&gt;https://i.redd.it/d1geatbckuge1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/hyrryecnkuge1.gif"&gt;https://i.redd.it/hyrryecnkuge1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T04:01:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihappk</id>
    <title>Looking for local RAG with API capability</title>
    <updated>2025-02-04T05:37:31+00:00</updated>
    <author>
      <name>/u/brandtiv</name>
      <uri>https://old.reddit.com/user/brandtiv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like the combination of LM Stuido and AnythingLLM, especially the RAG support, using just drag and drop. I have a need to query the LM Studio server using API. However, I can't get the data stored in the RAG because AnythingLLM currently doesn't have an API. I can only use its UI to get the RAG data.&lt;/p&gt; &lt;p&gt;I know I can probably roll my own using Python and a vector DB, but I would rather spend the time elsewhere ATM.&lt;/p&gt; &lt;p&gt;I am looking for a complete local solution that supports querying both LLM and RAG via API.&lt;/p&gt; &lt;p&gt;Let me know if you have suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brandtiv"&gt; /u/brandtiv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihappk/looking_for_local_rag_with_api_capability/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihappk/looking_for_local_rag_with_api_capability/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihappk/looking_for_local_rag_with_api_capability/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T05:37:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1igo6c9</id>
    <title>Don't forget to optimize your hardware! (Windows)</title>
    <updated>2025-02-03T12:41:48+00:00</updated>
    <author>
      <name>/u/rpwoerk</name>
      <uri>https://old.reddit.com/user/rpwoerk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igo6c9/dont_forget_to_optimize_your_hardware_windows/"&gt; &lt;img alt="Don't forget to optimize your hardware! (Windows)" src="https://b.thumbs.redditmedia.com/mq5j1Xjh-aMGbb1yZh8muOtGDY8qezCGISvJPZlb6kM.jpg" title="Don't forget to optimize your hardware! (Windows)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rpwoerk"&gt; /u/rpwoerk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1igo6c9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igo6c9/dont_forget_to_optimize_your_hardware_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igo6c9/dont_forget_to_optimize_your_hardware_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T12:41:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig6e6t</id>
    <title>DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt.</title>
    <updated>2025-02-02T20:12:17+00:00</updated>
    <author>
      <name>/u/Qaxar</name>
      <uri>https://old.reddit.com/user/Qaxar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"&gt; &lt;img alt="DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt." src="https://external-preview.redd.it/Er7i7V1ka8BO-MpGkuLs0Jmvu0-6GTVfn9JqY2PTKfM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd56ea2fa742541be1366b6615889d6a52f560b3" title="DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We knew R1 was good, but not that good. All the cries of CCP censorship are meaningless when it's trivial to bypass its guard rails.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qaxar"&gt; /u/Qaxar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/rohanpaul_ai/status/1886025249273339961?t=Wpp2kGJKVSZtSAOmTJjh0g&amp;amp;s=19"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T20:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih7abf</id>
    <title>PSA: MLX-GRPO trainer prototype (with QLoRA support) is functional</title>
    <updated>2025-02-04T02:29:14+00:00</updated>
    <author>
      <name>/u/mark-lord</name>
      <uri>https://old.reddit.com/user/mark-lord</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih7abf/psa_mlxgrpo_trainer_prototype_with_qlora_support/"&gt; &lt;img alt="PSA: MLX-GRPO trainer prototype (with QLoRA support) is functional" src="https://a.thumbs.redditmedia.com/wS0WohtGMXffXMZ-IMr2m0jPOJkWciq3zYhOwIXPv44.jpg" title="PSA: MLX-GRPO trainer prototype (with QLoRA support) is functional" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d30rs4vs91he1.png?width=1176&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6732047f90f640bc259720c9d0c6ee51a187a4c7"&gt;https://preview.redd.it/d30rs4vs91he1.png?width=1176&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6732047f90f640bc259720c9d0c6ee51a187a4c7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Big props to ActuallyIsaak for the trainer implementation draft - repo is here &lt;a href="https://github.com/ml-explore/mlx-examples/pull/1233"&gt;https://github.com/ml-explore/mlx-examples/pull/1233&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mark-lord"&gt; /u/mark-lord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih7abf/psa_mlxgrpo_trainer_prototype_with_qlora_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih7abf/psa_mlxgrpo_trainer_prototype_with_qlora_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ih7abf/psa_mlxgrpo_trainer_prototype_with_qlora_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T02:29:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih0c5x</id>
    <title>"Hyperfitting" a model to a small training set can postively impact human preference of model outputs</title>
    <updated>2025-02-03T21:19:00+00:00</updated>
    <author>
      <name>/u/LagOps91</name>
      <uri>https://old.reddit.com/user/LagOps91</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have stumbled accross a very strange result presented in a &lt;a href="https://www.youtube.com/watch?v=AAiMOFQJPx8"&gt;video&lt;/a&gt; that I think might be of interested to those of you who are into finetuning models, especially for creative writing or rp purposes.&lt;/p&gt; &lt;p&gt;According to the &lt;a href="https://www.youtube.com/redirect?event=video_description&amp;amp;redir_token=QUFFLUhqa0FRcHQtSmpVQkVmTlFXSC1zQ0NFQUJzd1otZ3xBQ3Jtc0tuV00yV3Y1eS1wb0JOQXhyMVR4aG14VW5RZlhVSUFuTl9LRVNKZW9VSF9PeEVZY0xNR09LLVA4bEFDT0VWc0VUQ20tZDd0MnhTVXFCWnhUeE9kZ1RwMG5hcXFNWHdWbVpTUUh2eExmM1VQeDNtZ3FnMA&amp;amp;q=https%3A%2F%2Farxiv.org%2Fabs%2F2412.04318&amp;amp;v=AAiMOFQJPx8"&gt;paper&lt;/a&gt; presented, overfitting a model to an extreme degree on a small training data set can help it stay coherent over long outputs and greatly reduce repetitions.&lt;/p&gt; &lt;p&gt;This results in a very sharpened output distribution, which has terrible perplexity (due to output distribution not matching natural language entropy), but interestingly enough, very high human preference values.&lt;/p&gt; &lt;p&gt;On one hand I can see this actually work, because the model learns how to keep the output coherent, as otherwise it can't match the training dataset to reduce loss.&lt;/p&gt; &lt;p&gt;On the other, intuitively, I would expect there to be more repetitions outside of the training data or copied over slop phrases, but according to the paper, that strangely enough doesn't happen.&lt;/p&gt; &lt;p&gt;Personally I have no experience in fine-tuning models and am not using NVIDIA hardware either, but perhaps someone could try this out by making an experimental finetune? Do you guys/girls think this has merit?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LagOps91"&gt; /u/LagOps91 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih0c5x/hyperfitting_a_model_to_a_small_training_set_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih0c5x/hyperfitting_a_model_to_a_small_training_set_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ih0c5x/hyperfitting_a_model_to_a_small_training_set_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T21:19:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih9tb6</id>
    <title>AllenAI Tulu 3 405b available for chat and download</title>
    <updated>2025-02-04T04:44:57+00:00</updated>
    <author>
      <name>/u/SuchSeries8760</name>
      <uri>https://old.reddit.com/user/SuchSeries8760</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure if this has been shared already, but AllenAI / Ai2 is a US-based nonprofit who are trying to build AIs as open-source and transparently as possible. &lt;/p&gt; &lt;p&gt;Their OLMO models have fully transparent training data. Their Tulu ones are as transparent as you can be building on top of Llama.&lt;br /&gt; For some positive news out of the US this week, they released their new 405B Parameter model for free online chat and download.&lt;/p&gt; &lt;p&gt;Chat: &lt;a href="https://playground.allenai.org/"&gt;https://playground.allenai.org/&lt;/a&gt;&lt;br /&gt; HuggingFace: &lt;a href="https://huggingface.co/allenai/Llama-3.1-Tulu-3-405B"&gt;https://huggingface.co/allenai/Llama-3.1-Tulu-3-405B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuchSeries8760"&gt; /u/SuchSeries8760 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih9tb6/allenai_tulu_3_405b_available_for_chat_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih9tb6/allenai_tulu_3_405b_available_for_chat_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ih9tb6/allenai_tulu_3_405b_available_for_chat_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T04:44:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1iggwff</id>
    <title>Mistral, Qwen, Deepseek</title>
    <updated>2025-02-03T04:28:53+00:00</updated>
    <author>
      <name>/u/Stargazer-8989</name>
      <uri>https://old.reddit.com/user/Stargazer-8989</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Aren't you noticing a pattern? Companies outside the USA are releasing models like Mistral AI, Qwen, and DeepSeek - reliable models that are made accessible, smaller and open-source, compared to most US-based companies &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stargazer-8989"&gt; /u/Stargazer-8989 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggwff/mistral_qwen_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggwff/mistral_qwen_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iggwff/mistral_qwen_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T04:28:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih6s1a</id>
    <title>I made Phi-14b into a (primitive) reasoner using a prototype MLX-GRPO trainer</title>
    <updated>2025-02-04T02:04:17+00:00</updated>
    <author>
      <name>/u/mark-lord</name>
      <uri>https://old.reddit.com/user/mark-lord</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih6s1a/i_made_phi14b_into_a_primitive_reasoner_using_a/"&gt; &lt;img alt="I made Phi-14b into a (primitive) reasoner using a prototype MLX-GRPO trainer" src="https://external-preview.redd.it/zVsUFCfc_S8cJO3WpDhkPDW9XBFjXKuImjmcfLcLj_M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=899c4ad7fc7f2c831a252b65e14de097d7892f71" title="I made Phi-14b into a (primitive) reasoner using a prototype MLX-GRPO trainer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4ldnwck141he1.png?width=962&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c64e19a26652495913675a71003e62b013c9526"&gt;https://preview.redd.it/4ldnwck141he1.png?width=962&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c64e19a26652495913675a71003e62b013c9526&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone! Quick post today, since it's 2am here and I really should be getting to bed ðŸ˜‚&lt;/p&gt; &lt;p&gt;So someone (@ActuallyIsaak) managed to get a prototype of the GRPO algo working in MLX, which you can check out the draft of &lt;a href="https://github.com/ml-explore/mlx-examples/pull/1233"&gt;here&lt;/a&gt;. I spent a little bit of time messing around with it a bit, and with only &lt;strong&gt;three hand-written samples,&lt;/strong&gt; I managed to get Phi-14b to implement this CoT! Not only that, but &lt;strong&gt;it managed to perfectly recall some factual information, and generalise from it&lt;/strong&gt;. Interestingly, I didn't get this generalisation behaviour with my first version, in which I didn't include a CoT (more often than not it started hallucinating some other Mark Lord - often, for some reason, a 60-year-old businessman lol)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mark-lord"&gt; /u/mark-lord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih6s1a/i_made_phi14b_into_a_primitive_reasoner_using_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih6s1a/i_made_phi14b_into_a_primitive_reasoner_using_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ih6s1a/i_made_phi14b_into_a_primitive_reasoner_using_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T02:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1igr55c</id>
    <title>Training deepseek r1 to trade stocks</title>
    <updated>2025-02-03T15:07:11+00:00</updated>
    <author>
      <name>/u/ExaminationNo8522</name>
      <uri>https://old.reddit.com/user/ExaminationNo8522</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like everyone else on the internet, I was really fascinated by deepseek's abilities, but the thing that got me the most was how they trained deepseek-r1-zero. Essentially, it just seemed to boil down to: &amp;quot;feed the machine an objective reward function, and train it a whole bunch, letting it think a variable amount&amp;quot;. So I thought: hey, you can use stock prices going up and down as an objective reward function kinda? &lt;/p&gt; &lt;p&gt;Anyways, so I used huggingface's open-r1 to write a version of deepseek that aims to maximize short-term stock prediction, by acting as a &amp;quot;stock analyst&amp;quot; of sort, offering buy and sell recommendations based on some signals I scraped for each company. All the code and colab and discussion is at &lt;a href="https://2084.substack.com/p/2084-deepstock-can-you-train-deepseek"&gt;2084: Deepstock - can you train deepseek to do stock trading?&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Training it rn over the next week, my goal is to get it to do better than random, altho getting it to that point is probably going to take a ton of compute. (Anyone got any spare?)&lt;/p&gt; &lt;p&gt;Thoughts on how I should expand this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExaminationNo8522"&gt; /u/ExaminationNo8522 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igr55c/training_deepseek_r1_to_trade_stocks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igr55c/training_deepseek_r1_to_trade_stocks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igr55c/training_deepseek_r1_to_trade_stocks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T15:07:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih8zba</id>
    <title>Whats better than Claude Sonnet 3.5?</title>
    <updated>2025-02-04T03:57:34+00:00</updated>
    <author>
      <name>/u/DynamicOnion_</name>
      <uri>https://old.reddit.com/user/DynamicOnion_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, any recommendations? Claude always limits me. Looking for an alternative for writing emails, business strategy and other work stuff. I like claude because it seems smart. My PC has 80gb unified RAM. I just cant find anything online to compare. Tia&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DynamicOnion_"&gt; /u/DynamicOnion_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih8zba/whats_better_than_claude_sonnet_35/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih8zba/whats_better_than_claude_sonnet_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ih8zba/whats_better_than_claude_sonnet_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T03:57:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ign0lz</id>
    <title>DeepSeek-R1 never ever relaxes...</title>
    <updated>2025-02-03T11:30:28+00:00</updated>
    <author>
      <name>/u/IrisColt</name>
      <uri>https://old.reddit.com/user/IrisColt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ign0lz/deepseekr1_never_ever_relaxes/"&gt; &lt;img alt="DeepSeek-R1 never ever relaxes..." src="https://external-preview.redd.it/P8lS0kk6BFe2IEo6TxCZd1LVwksc34IkzGTVx_SCc8w.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d74dbe4f1d67cc8b587db9aa01762f26e269bcf" title="DeepSeek-R1 never ever relaxes..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I was testing DeepSeek-R1 with a math problem I found in a textbook for 9-year-olds &lt;strong&gt;(yes, really)&lt;/strong&gt;, and the model managed to crack it.&lt;/p&gt; &lt;p&gt;The problem was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Find two 3-digit palindromic numbers that add up to a 4-digit palindromic number. Note: the first digit of any of these numbers can't be 0.&amp;quot;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ml5hnng3rwge1.jpg?width=1800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1456610eeff8d8b9a122d86fbb44967f84f682d9"&gt;R1 starts thinking...&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now, hereâ€™s where it gets interesting. R1 thought for a bit, found the correct answer in its &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; block, then went ahead to output itâ€”but made a mistake.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/77bke6q1swge1.jpg?width=1800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d6eac07677fe576be9e699776a2134cba1d15c62"&gt;R1 makes a mistake...&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Before even finishing its response, it caught its own error, backtracked, and corrected itself on the fly outside of the&lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; block.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yc3zjamsswge1.jpg?width=1800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=903d42998593e95a68ff32006b7bac6335df9f1e"&gt;R1 corrects itself...&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j8vgvxn3twge1.jpg?width=1800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b189fce4a099ed9182b315c2164a1071a4a32104"&gt;R1's final answer.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/0Ayv77LN"&gt;DeepSeek-R1 complete answer.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Regarding the problem, &lt;strong&gt;no other LLM solved it, except for&lt;/strong&gt; &lt;a href="https://pastebin.com/YCRR521W"&gt;&lt;strong&gt;OpenAI o1&lt;/strong&gt;&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;So now Iâ€™m wonderingâ€”&lt;strong&gt;what's holding them back?&lt;/strong&gt; Is it the tokenizer's weaknesses? The sampling parameters (even when all where at the recommended settings they failed)? Or maybe, just maybe, non-thinking LLMs are really that bad at math? &lt;/p&gt; &lt;p&gt;Would love to hear thoughts on this.&lt;/p&gt; &lt;p&gt;Unsuccessful attemps by other models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://pastebin.com/r8VKHrcA"&gt;chatgpt-4o-latest-20241120&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pastebin.com/tXc7wGVz"&gt;claude-3-5-sonnet-20241022&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pastebin.com/zGzQJ8B5"&gt;phi-4&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pastebin.com/vt54UFBe"&gt;amazon-nova-pro-v1.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pastebin.com/eSN4y6E0"&gt;gemini-exp-1206&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pastebin.com/jVj1KcMF"&gt;llama-3.1-405b-instruct-bf16&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pastebin.com/ZRLfhEfU"&gt;qwen-max-2025-01-25&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IrisColt"&gt; /u/IrisColt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ign0lz/deepseekr1_never_ever_relaxes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ign0lz/deepseekr1_never_ever_relaxes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ign0lz/deepseekr1_never_ever_relaxes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T11:30:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1igcvol</id>
    <title>I built a silent speech recognition tool that reads your lips in real-time and types whatever you mouth - runs 100% locally!</title>
    <updated>2025-02-03T01:00:09+00:00</updated>
    <author>
      <name>/u/tycho_brahes_nose_</name>
      <uri>https://old.reddit.com/user/tycho_brahes_nose_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcvol/i_built_a_silent_speech_recognition_tool_that/"&gt; &lt;img alt="I built a silent speech recognition tool that reads your lips in real-time and types whatever you mouth - runs 100% locally!" src="https://external-preview.redd.it/bnIwMGoyaXludGdlMVL1KlPwXSM4mwFtLRlx6KM67CArRsK705RfUy_x1msn.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b064a65e7251b4b07e096a39fc4d698d7f457b36" title="I built a silent speech recognition tool that reads your lips in real-time and types whatever you mouth - runs 100% locally!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tycho_brahes_nose_"&gt; /u/tycho_brahes_nose_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dh90m1iyntge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcvol/i_built_a_silent_speech_recognition_tool_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igcvol/i_built_a_silent_speech_recognition_tool_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T01:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1igpedw</id>
    <title>Mistral Small 3: Redefining Expectations â€“ Performance Beyond Its Size (Feels Like a 70B Model!)</title>
    <updated>2025-02-03T13:45:54+00:00</updated>
    <author>
      <name>/u/Vishnu_One</name>
      <uri>https://old.reddit.com/user/Vishnu_One</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igpedw/mistral_small_3_redefining_expectations/"&gt; &lt;img alt="Mistral Small 3: Redefining Expectations â€“ Performance Beyond Its Size (Feels Like a 70B Model!)" src="https://b.thumbs.redditmedia.com/65ozbJ1ALEQgiuu_PZ9iyOKM1ciEEq2a4swxnh3Ta_k.jpg" title="Mistral Small 3: Redefining Expectations â€“ Performance Beyond Its Size (Feels Like a 70B Model!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ðŸš€ Hold onto your hats, folks! Mistral Small 3 is here to blow your minds! This isn't just another small model â€“ it's a powerhouse that feels like you're wielding a 70B beast! I've thrown every complex question I could think of at it, and the results are mind-blowing. From coding conundrums to deep language understanding, this thing is breaking barriers left and right.&lt;/p&gt; &lt;p&gt;I dare you to try it out and share your experiences here. Let's see what crazy things we can make Mistral Small 3 do! Who else is ready to have their expectations redefined? ðŸ¤¯&lt;br /&gt; This is Q4_K_M just 14GB&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/fdqvgbm9gxge1.gif"&gt;https://i.redd.it/fdqvgbm9gxge1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Prompt&lt;/p&gt; &lt;p&gt;Create an interactive web page that animates the Sun and the planets in our Solar System. The animation should include the following features:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Sun&lt;/strong&gt; : A central, bright yellow circle representing the Sun.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Planets&lt;/strong&gt; : Eight planets (Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune) orbiting around the Sun with realistic relative sizes and distances.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Orbits&lt;/strong&gt; : Visible elliptical orbits for each planet to show their paths around the Sun.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Animation&lt;/strong&gt; : Smooth orbital motion for all planets, with varying speeds based on their actual orbital periods.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Labels&lt;/strong&gt; : Clickable labels for each planet that display additional information when hovered over or clicked (e.g., name, distance from the Sun, orbital period).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interactivity&lt;/strong&gt; : Users should be able to pause and resume the animation using buttons.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Ensure the design is visually appealing with a dark background to enhance the visibility of the planets and their orbits. Use CSS for styling and JavaScript for the animation logic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vishnu_One"&gt; /u/Vishnu_One &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igpedw/mistral_small_3_redefining_expectations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igpedw/mistral_small_3_redefining_expectations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igpedw/mistral_small_3_redefining_expectations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T13:45:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih0te1</id>
    <title>Why nobody talks about Stanford Co-Storm ? It writes advanced in depth reports</title>
    <updated>2025-02-03T21:38:31+00:00</updated>
    <author>
      <name>/u/sickleRunner</name>
      <uri>https://old.reddit.com/user/sickleRunner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih0te1/why_nobody_talks_about_stanford_costorm_it_writes/"&gt; &lt;img alt="Why nobody talks about Stanford Co-Storm ? It writes advanced in depth reports" src="https://b.thumbs.redditmedia.com/pZt9Jes8ZBCP_R2XLKjG-pRtdsk7r6xO_Op38zyGQRc.jpg" title="Why nobody talks about Stanford Co-Storm ? It writes advanced in depth reports" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/hudubs5stzge1.png?width=1432&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cdce1e4093acae16c81509c51bf887ceaa71ccf7"&gt;https://preview.redd.it/hudubs5stzge1.png?width=1432&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cdce1e4093acae16c81509c51bf887ceaa71ccf7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sickleRunner"&gt; /u/sickleRunner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih0te1/why_nobody_talks_about_stanford_costorm_it_writes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih0te1/why_nobody_talks_about_stanford_costorm_it_writes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ih0te1/why_nobody_talks_about_stanford_costorm_it_writes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T21:38:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1igpkc8</id>
    <title>I built a Linux Distro to run Nvidia GPUs for AI</title>
    <updated>2025-02-03T13:53:56+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igpkc8/i_built_a_linux_distro_to_run_nvidia_gpus_for_ai/"&gt; &lt;img alt="I built a Linux Distro to run Nvidia GPUs for AI" src="https://b.thumbs.redditmedia.com/pCPtLbPVUS7barizEgMp7xXMDTS6YQUYF470-tBLf_o.jpg" title="I built a Linux Distro to run Nvidia GPUs for AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I wanted to share a project Iâ€™ve been working on - I built a minimalist Linux distro called &lt;strong&gt;Sbnb Linux&lt;/strong&gt; thatâ€™s super easy to get up and running with Nvidia GPUs.&lt;/p&gt; &lt;p&gt;Hereâ€™s the cool part:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can boot straight from a USB flash drive, no installation needed.&lt;/li&gt; &lt;li&gt;Itâ€™s got all the tools to spin up a virtual machine and attach your Nvidia GPU using a low-overhead &lt;code&gt;vfio-pci&lt;/code&gt; setup.&lt;/li&gt; &lt;li&gt;Once thatâ€™s done, you can easily run AI like DeepSeek R1 in the VM using &lt;strong&gt;ollama&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But wait, thereâ€™s more! The bare metal server and VM are connected through &lt;strong&gt;Tailscale tunnels&lt;/strong&gt;, so you can SSH into them from anywhere using OAuth (Google, etc.).&lt;/p&gt; &lt;p&gt;If anyoneâ€™s curious to give it a shot, all you need is a USB flash drive and about 30 minutes to get up and running. The instructions are here: &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-NVIDIA.md"&gt;GitHub Link&lt;/a&gt;. If you run into any issues, drop a message below and Iâ€™ll be happy to help out!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As a fun weekend project, my kids and I built a beast of a home server powered by an AMD EPYC 7C13 (3rd gen). I posted all the nerdy details and costs over on &lt;a href="/r/homelab"&gt;r/homelab&lt;/a&gt; if you're into that kind of thing: &lt;a href="https://www.reddit.com/r/homelab/comments/1hmnnwg/built_a_powerful_and_silent_amd_epyc_home_server/"&gt;link here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1igpkc8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igpkc8/i_built_a_linux_distro_to_run_nvidia_gpus_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igpkc8/i_built_a_linux_distro_to_run_nvidia_gpus_for_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T13:53:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1igc6r0</id>
    <title>20 yrs in jail or $1 million for downloading Chinese models proposed at congress</title>
    <updated>2025-02-03T00:26:00+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.hawley.senate.gov/wp-content/uploads/2025/01/Hawley-Decoupling-Americas-Artificial-Intelligence-Capabilities-from-China-Act.pdf"&gt;https://www.hawley.senate.gov/wp-content/uploads/2025/01/Hawley-Decoupling-Americas-Artificial-Intelligence-Capabilities-from-China-Act.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Seriously stop giving your money to these anti open companies and encourage everyone and anyone you know to do the same, don't let your company use their products. Anthrophic and OpenAI are the worse. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igc6r0/20_yrs_in_jail_or_1_million_for_downloading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igc6r0/20_yrs_in_jail_or_1_million_for_downloading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igc6r0/20_yrs_in_jail_or_1_million_for_downloading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T00:26:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih0orp</id>
    <title>"Can I Run This LLM? â€“ A VRAM Estimator for Local AI Models"</title>
    <updated>2025-02-03T21:33:16+00:00</updated>
    <author>
      <name>/u/negative_entropie</name>
      <uri>https://old.reddit.com/user/negative_entropie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, Iâ€™m currently in my third semester of electrical engineering, and I built this project over the weekend.&lt;/p&gt; &lt;p&gt;&lt;a href="http://www.canirunthisllm.net/"&gt;http://www.canirunthisllm.net/&lt;/a&gt; helps users estimate the VRAM requirements for running local AI models. &lt;/p&gt; &lt;p&gt;This is my first web app project, and I would really appreciate any feedback!&lt;/p&gt; &lt;p&gt;Next, Iâ€™m working on a feature that lets users enter a Hugging Face model tag into a form, and the backend will automatically fetch the model parameters. After that, I plan to add support for multiple GPUs. Also planned for the future is an .exe that automatically detect PC specifications. I already wrote the script but dont know how to fetch it into Django.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/negative_entropie"&gt; /u/negative_entropie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih0orp/can_i_run_this_llm_a_vram_estimator_for_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih0orp/can_i_run_this_llm_a_vram_estimator_for_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ih0orp/can_i_run_this_llm_a_vram_estimator_for_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T21:33:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1igwkpw</id>
    <title>I trained a tinystories model from scratch for educational purposes, how cooked? (1M-parameters)</title>
    <updated>2025-02-03T18:48:54+00:00</updated>
    <author>
      <name>/u/THE--GRINCH</name>
      <uri>https://old.reddit.com/user/THE--GRINCH</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igwkpw/i_trained_a_tinystories_model_from_scratch_for/"&gt; &lt;img alt="I trained a tinystories model from scratch for educational purposes, how cooked? (1M-parameters)" src="https://preview.redd.it/mdy6shrszyge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cae5a6b70dd9f4b9a4cda0016978c57391368c6" title="I trained a tinystories model from scratch for educational purposes, how cooked? (1M-parameters)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/THE--GRINCH"&gt; /u/THE--GRINCH &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mdy6shrszyge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igwkpw/i_trained_a_tinystories_model_from_scratch_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igwkpw/i_trained_a_tinystories_model_from_scratch_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T18:48:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1igq9ud</id>
    <title>Jokes aside, which is your favorite local tts model and why?</title>
    <updated>2025-02-03T14:27:05+00:00</updated>
    <author>
      <name>/u/iaseth</name>
      <uri>https://old.reddit.com/user/iaseth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igq9ud/jokes_aside_which_is_your_favorite_local_tts/"&gt; &lt;img alt="Jokes aside, which is your favorite local tts model and why?" src="https://preview.redd.it/ao50vnrwoxge1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=298e2e3c1ed5241562b49a3f4402fcd6c67afae4" title="Jokes aside, which is your favorite local tts model and why?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iaseth"&gt; /u/iaseth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ao50vnrwoxge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igq9ud/jokes_aside_which_is_your_favorite_local_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igq9ud/jokes_aside_which_is_your_favorite_local_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T14:27:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1igyy0n</id>
    <title>Introducing Deeper Seeker - A simpler and OSS version of OpenAI's latest Deep Research feature.</title>
    <updated>2025-02-03T20:23:12+00:00</updated>
    <author>
      <name>/u/hjofficial</name>
      <uri>https://old.reddit.com/user/hjofficial</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igyy0n/introducing_deeper_seeker_a_simpler_and_oss/"&gt; &lt;img alt="Introducing Deeper Seeker - A simpler and OSS version of OpenAI's latest Deep Research feature." src="https://b.thumbs.redditmedia.com/0T4K52cWrmMADRDodzMWGaFaNxIvhWaLY2rbAQWA9ug.jpg" title="Introducing Deeper Seeker - A simpler and OSS version of OpenAI's latest Deep Research feature." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deeper Seeker is a simpler &lt;strong&gt;OSS version of OpenAI's latest Deep Research&lt;/strong&gt; feature in &lt;a href="http://ChatGPT.It"&gt;ChatGPT.It&lt;/a&gt; is an agentic research tool to reason, create multi-step tasks , synthesize data from multiple online resources and create neat reports.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github link :&lt;/strong&gt; &lt;a href="https://github.com/HarshJ23/Deeper-Seeker"&gt;&lt;strong&gt;Deeper Seeker&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I made it using Exa web search APIs. I didn't use langchain/langgraph or any agent orchestration framework.&lt;/p&gt; &lt;p&gt;Although it does not work well for complex queries, I welcome whoever is interested in contributing to the repo and improving it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open to hearing all the feedback from you all !!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/b859bc5egzge1.gif"&gt;demo &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hjofficial"&gt; /u/hjofficial &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igyy0n/introducing_deeper_seeker_a_simpler_and_oss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igyy0n/introducing_deeper_seeker_a_simpler_and_oss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igyy0n/introducing_deeper_seeker_a_simpler_and_oss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T20:23:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih9h11</id>
    <title>Ok, you LLaMA-fobics, Claude does have a moat, and impressive one</title>
    <updated>2025-02-04T04:24:57+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you know me, you might know I eat local LLMs for breakfast, ever since the first Llama with its &amp;quot;I have a borked tokenizer, but I love you&amp;quot; vibes came about. So this isn't some uneducated guess.&lt;/p&gt; &lt;p&gt;A few days ago, I was doing some C++ coding and tried Claude, which was working shockingly well, until it wanted MoooOOOoooney. So I gave in, mid-code, just to see how far this would go.&lt;/p&gt; &lt;p&gt;Darn. Triple darn. Quadruple darn.&lt;/p&gt; &lt;p&gt;Hereâ€™s the skinny: No other model understands code with the shocking capabilities of Sonet 3.5. You can fight me on this, and I'll fight back.&lt;/p&gt; &lt;p&gt;This thing is insane. And Iâ€™m not just making some simple &amp;quot;snake game&amp;quot; stuff. I have 25 years of C++ under my belt, so when I need something, I need something I &lt;em&gt;actually&lt;/em&gt; struggle with.&lt;/p&gt; &lt;p&gt;There were so many instances where I felt this was Coding AI (and Iâ€™m &lt;em&gt;very&lt;/em&gt; cautious about calling token predictors AI), but itâ€™s just &lt;em&gt;insane.&lt;/em&gt; In three days, I made a couple of classes that would have taken me months, and this thing chews through 10K-line classes like bubble gum.&lt;/p&gt; &lt;p&gt;Of course, I made it cry a few times when things didnâ€™t workâ€¦ and didnâ€™t workâ€¦ and &lt;em&gt;didnâ€™t work.&lt;/em&gt; Then Claude wrote an entirely new set of code just to test the old code, and at the end we sorted it out.&lt;/p&gt; &lt;p&gt;A lot of my code was for visual components, so Iâ€™d describe what I saw on the screen. It was like programming over the phone, yet it still got things right!&lt;/p&gt; &lt;p&gt;Told it, &amp;quot;Add multithreading&amp;quot; boom. Done. Unique mutexes. Clean as a whistle.&lt;/p&gt; &lt;p&gt;Told it: &amp;quot;Add multiple undo and redo to this class: The simplest 5 minutes in my programming carrier - and I've been adding and struggling with undo/redo in my stuff many times. &lt;/p&gt; &lt;p&gt;The code it writes is &lt;em&gt;incredibly&lt;/em&gt; well-structured. I feel like a messy duck playing in the mud by comparison.&lt;/p&gt; &lt;p&gt;I realized a few things:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It gives me the best solution when I &lt;em&gt;donâ€™t&lt;/em&gt; over-explain (codexplain) how I &lt;em&gt;think&lt;/em&gt; the structure or flow should be. Instead, if I just let it do its thing and pretend Iâ€™m stupid, it works better.&lt;/li&gt; &lt;li&gt;Many times, it automatically adds things I &lt;em&gt;didnâ€™t&lt;/em&gt; ask for, but would have ultimately needed, so itâ€™s not just predicting tokens, itâ€™s predicting my &lt;em&gt;next&lt;/em&gt; request.&lt;/li&gt; &lt;li&gt;More than once, it chose a future-proof, open-ended solution &lt;em&gt;as if&lt;/em&gt; it expected weâ€™d be building on it further and I was pretty surprised later when I wanted to add something how ready the code was&lt;/li&gt; &lt;li&gt;It comprehends alien code like nothing else Iâ€™ve seen. Just throw in my mess.&lt;/li&gt; &lt;li&gt;When I was wrong and it was right, it didn't took my wrong stance, but explained to me where I might got my idea wrong, even pointing on a part of the code I probably overlooked - which was the EXACT reason why I was wrong. When model can keep it's cool without trying to please me all the time, it is something!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My previous best model for coding was Google Gemini 2, but in comparison, it feels confused for serious code, creating complex confused structure that didn't work anyway. .&lt;/p&gt; &lt;p&gt;I got my moneyâ€™s worth in the first &lt;em&gt;ten minutes.&lt;/em&gt; The next 30.98 days? Just a bonus.&lt;/p&gt; &lt;p&gt;Iâ€™m saying this because while I &lt;em&gt;love&lt;/em&gt; Llama and Iâ€™m deep into the local LLM phase, this actually feels like magic. &lt;em&gt;So someone does thing s right, IMHO.&lt;/em&gt;&lt;br /&gt; Also, it is still next token predictor, that's even more impressive than if it actually reads the code.....&lt;/p&gt; &lt;p&gt;My biggest nightmare now: What if they take it away.... or &amp;quot;improve&amp;quot; it....&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih9h11/ok_you_llamafobics_claude_does_have_a_moat_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih9h11/ok_you_llamafobics_claude_does_have_a_moat_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ih9h11/ok_you_llamafobics_claude_does_have_a_moat_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T04:24:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1igpwzl</id>
    <title>Paradigm shift?</title>
    <updated>2025-02-03T14:10:33+00:00</updated>
    <author>
      <name>/u/RetiredApostle</name>
      <uri>https://old.reddit.com/user/RetiredApostle</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igpwzl/paradigm_shift/"&gt; &lt;img alt="Paradigm shift?" src="https://preview.redd.it/gre7z74ylxge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a0d8456581d7b4608dca287eb41e0f6185e191a" title="Paradigm shift?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RetiredApostle"&gt; /u/RetiredApostle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gre7z74ylxge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igpwzl/paradigm_shift/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igpwzl/paradigm_shift/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T14:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih5b3q</id>
    <title>Finally Found a Use Case for a Local LLM That Couldn't Be Done Any Other Way</title>
    <updated>2025-02-04T00:53:46+00:00</updated>
    <author>
      <name>/u/Captain_Coffee_III</name>
      <uri>https://old.reddit.com/user/Captain_Coffee_III</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ok, I now hate the title. But... &lt;/p&gt; &lt;p&gt;So this is a little bit of an edge case. I do old-school Industrial music as a hobby. Part of that is collecting sound samples from movies. That's part of the schtick from the '80s and '90s. Over the years, I've amassed a large amount of movies on DVD, which I've digitized. Thanks to the latest advancements that allow AI to strip out vocals, I can now capture just the spoken words from said movie.. which I then transcribed with OpenAI's Whisper. So I've been sitting here with a large database of sentences spoken in movies and not quite knowing what do do with it.&lt;/p&gt; &lt;p&gt;Enter one of the Llama 7B chat models. I thought that since the whole thing was based on the probability that tokens follow other tokens, I should be able to utilize that and find sentences that logically follow other sentences. When using the llama-cpp-python (cuda) module, you can tell it to track the probabilities of all the tokens so when I feed it two sentences, I can somewhat get an idea that they actually fit together. So phrases like &amp;quot;I ate the chicken.&amp;quot; and &amp;quot;That ain't my car.&amp;quot; have a lower probability matrix than if I ended it with &amp;quot;And it tasted good.&amp;quot; That was a no-go from the start though. I wanted to find sentences that logically fit together from random in 1500+ movies and each movie has about 1000 spoken lines. Nobody has time for that.&lt;/p&gt; &lt;p&gt;Round two. Prompt: &amp;quot;Given the theme '{Insert theme you want to classify by}', does the following phrase fit the theme? '{insert phrase here}', Answer yes or no. Answer:'&lt;/p&gt; &lt;p&gt;It's not super fast on my RTX2070, but I'm getting about one prompt every 0.8 seconds. But, it is totally digging through all the movies and finding individual lines that match up with a theme. The probability matrix actually works as well. I spent the morning throwing all kinds of crazy themes at it and it just nails them. I have over 15M lines of text to go through... and if I let it run continuously it would take 17 days to classify all lines to a single theme but having the Python script pick random movies then stopping when it finds the top 50 is totally good enough and can happen in hours.&lt;/p&gt; &lt;p&gt;There's no way I would pay for this volume of traffic on an paid API and even the 7B model can pull this off without a hitch. Precision isn't key here. And I can build a database of themes and have this churn away at night finding samples that match a theme. Absolutely loving this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Captain_Coffee_III"&gt; /u/Captain_Coffee_III &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih5b3q/finally_found_a_use_case_for_a_local_llm_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih5b3q/finally_found_a_use_case_for_a_local_llm_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ih5b3q/finally_found_a_use_case_for_a_local_llm_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T00:53:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih3nc6</id>
    <title>US Bill proposed to jail people who download Deepseek</title>
    <updated>2025-02-03T23:37:51+00:00</updated>
    <author>
      <name>/u/SuchSeries8760</name>
      <uri>https://old.reddit.com/user/SuchSeries8760</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih3nc6/us_bill_proposed_to_jail_people_who_download/"&gt; &lt;img alt="US Bill proposed to jail people who download Deepseek" src="https://external-preview.redd.it/aUM4Zo5M60iArpLso3HHGaMmvhrgIEshjneVeh2Hvq4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4794828de090d8686e777cd2679cfb73fcb8c4f" title="US Bill proposed to jail people who download Deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuchSeries8760"&gt; /u/SuchSeries8760 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.404media.co/senator-hawley-proposes-jail-time-for-people-who-download-deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih3nc6/us_bill_proposed_to_jail_people_who_download/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ih3nc6/us_bill_proposed_to_jail_people_who_download/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T23:37:51+00:00</published>
  </entry>
</feed>
