<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-08T18:07:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hwp04q</id>
    <title>Local Android translator app using Seemless M4T unity model</title>
    <updated>2025-01-08T17:01:23+00:00</updated>
    <author>
      <name>/u/DocWolle</name>
      <uri>https://old.reddit.com/user/DocWolle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an offline translation app base on the Seemless M4T Unity small model:&lt;br /&gt; (&lt;a href="https://huggingface.co/facebook/seamless-m4t-unity-small-s2t"&gt;https://huggingface.co/facebook/seamless-m4t-unity-small-s2t&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;It supports English, Spanish, Portuguese, Hindi, and obviously also Russian.&lt;br /&gt; In contrary to the model card it does not support French...&lt;/p&gt; &lt;p&gt;The app is here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/woheller69/seemless"&gt;https://github.com/woheller69/seemless&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is quite big (~250 MB) and downloads the model (~500 MB) from Hugging Face.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocWolle"&gt; /u/DocWolle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwp04q/local_android_translator_app_using_seemless_m4t/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwp04q/local_android_translator_app_using_seemless_m4t/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwp04q/local_android_translator_app_using_seemless_m4t/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T17:01:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwlzvj</id>
    <title>Help me find some use cases for agents</title>
    <updated>2025-01-08T14:54:36+00:00</updated>
    <author>
      <name>/u/freecodeio</name>
      <uri>https://old.reddit.com/user/freecodeio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a software engineer, and I've always had an amazing concept for an &amp;quot;agent builder&amp;quot; framework.&lt;/p&gt; &lt;p&gt;One thing that has always put me off is that I could not find good real life use cases. The ones I find are either super complex customer support systems or data categorization, but nothing raw to present to the world.&lt;/p&gt; &lt;p&gt;Any help is appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freecodeio"&gt; /u/freecodeio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlzvj/help_me_find_some_use_cases_for_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlzvj/help_me_find_some_use_cases_for_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlzvj/help_me_find_some_use_cases_for_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T14:54:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwpd0o</id>
    <title>how do i run gguf models "directly from file" to openwebUI?</title>
    <updated>2025-01-08T17:15:44+00:00</updated>
    <author>
      <name>/u/carrotsquawk</name>
      <uri>https://old.reddit.com/user/carrotsquawk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m still pretty new to this, so apologies if this has been asked before.&lt;/p&gt; &lt;p&gt;I would like to run downloaded LLM models in GGUF format from a central directory on a defined path. &lt;/p&gt; &lt;p&gt;i use the qwen model currently through ollama.&lt;/p&gt; &lt;p&gt;but ollama downloads its own version of it and stores it in a hidden directory under a random hashed name. Even if i want to import the ggufs it will make a copy its its path.&lt;/p&gt; &lt;p&gt;im sure is it possible to run the gguf files directly from hugging face and acces them through an web interface (chatGPT style) but i dont know how. I heard openwebui can do this directly but i havent figured how.&lt;/p&gt; &lt;p&gt;I want this specifically as i have a dual boot machine (win + linux) and with ollama each system makes a copy of the model which eats my system drive.&lt;/p&gt; &lt;p&gt;So what is my desired setup? i am a bit noob and will be thankful for hints.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/carrotsquawk"&gt; /u/carrotsquawk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwpd0o/how_do_i_run_gguf_models_directly_from_file_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwpd0o/how_do_i_run_gguf_models_directly_from_file_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwpd0o/how_do_i_run_gguf_models_directly_from_file_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T17:15:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwm93a</id>
    <title>CES 2025: Razer Blade 16 - 64 GB LPDDR5X-8000 RAM, RTX 5090 (24GB GDDR7)</title>
    <updated>2025-01-08T15:06:12+00:00</updated>
    <author>
      <name>/u/CyberAwarenessGuy</name>
      <uri>https://old.reddit.com/user/CyberAwarenessGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No pricing yet, but I'm curious what people's thoughts are on the capabilities of this as a mobile AI developer workstation?&lt;br /&gt; &lt;a href="https://www.tomshardware.com/laptops/gaming-laptops/razer-updates-blade-16-gaming-laptop-with-flagship-rtx-5090-and-ryzen-9-hx-370-hardware"&gt;Razer updates Blade 16 gaming laptop with flagship RTX 5090 and Ryzen 9 HX 370 hardware | Tom's Hardware&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CyberAwarenessGuy"&gt; /u/CyberAwarenessGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwm93a/ces_2025_razer_blade_16_64_gb_lpddr5x8000_ram_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwm93a/ces_2025_razer_blade_16_64_gb_lpddr5x8000_ram_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwm93a/ces_2025_razer_blade_16_64_gb_lpddr5x8000_ram_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T15:06:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwovfv</id>
    <title>Has any tried nstruction finetuning andrej karpathys nanoGPT</title>
    <updated>2025-01-08T16:56:11+00:00</updated>
    <author>
      <name>/u/Pure-Signal-3135</name>
      <uri>https://old.reddit.com/user/Pure-Signal-3135</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically the title I have pretrained the model using Karpathy's nanogpt and now I want to instruction fine tune it I am pretty new to fine tuning Has anyone successfully instruction fine tuned it ? If so can you shed some light? Pleaseeee point me to the right resource to accomplish this task I have been trying a lot but ahh no progress yet &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pure-Signal-3135"&gt; /u/Pure-Signal-3135 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwovfv/has_any_tried_nstruction_finetuning_andrej/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwovfv/has_any_tried_nstruction_finetuning_andrej/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwovfv/has_any_tried_nstruction_finetuning_andrej/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T16:56:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwcamp</id>
    <title>Scientific Idea Generation Model: QwQ-32B-Preview-IdeaWhiz-v1</title>
    <updated>2025-01-08T05:19:19+00:00</updated>
    <author>
      <name>/u/realJoeTrump</name>
      <uri>https://old.reddit.com/user/realJoeTrump</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"&gt; &lt;img alt="Scientific Idea Generation Model: QwQ-32B-Preview-IdeaWhiz-v1" src="https://external-preview.redd.it/O0xLvJp44hwyg917PUrV8IBhbRmX6FOqSfwmMF1eaGc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc29f48e087f031064db96ec7d094a8c35ff53b5" title="Scientific Idea Generation Model: QwQ-32B-Preview-IdeaWhiz-v1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zmbkh819fpbe1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbc2e9dec0fb1af2f3f57ded2664516134ccd4b8"&gt;https://preview.redd.it/zmbkh819fpbe1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbc2e9dec0fb1af2f3f57ded2664516134ccd4b8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wanted to share a model we've been working on that might be useful for the community. It's a fine-tune of QwQ-32B-Preview focused on scientific reasoning and hypothesis generation.&lt;/p&gt; &lt;p&gt;The model was trained on LiveIdeaBench dataset, primarily targeting step-by-step scientific thinking. Main focus areas include chemistry, biology, climate science, and medical research. This model is designed to help researchers brainstorm scientific ideas.&lt;/p&gt; &lt;p&gt;Here's a simple example of its output&lt;/p&gt; &lt;h1&gt;Example User Inputs&lt;/h1&gt; &lt;p&gt;User:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;I'll be submitting your next responses to a &amp;quot;Good Scientific Idea&amp;quot; expert review panel. If they consider your idea to be a good one, you'll receive a reward. Your assigned keyword is: &amp;quot;cancer&amp;quot;. You may provide background information. The idea MUST be within 100 words (including background information). (Note: good scientific ideas should be novel, verifiable, practically valuable, and able to advance the field.). NOTE: You MUST give your answer after **Final Idea:** &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Example Outputs&lt;/h1&gt; &lt;p&gt;Assistant:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Alright, so I have this task to come up with a good scientific idea related to cancer, and it has to be novel, verifiable, practically valuable, and able to advance the field. Plus, it needs to be within 100 words, including any background information. That's a pretty tight constraint, but I think I can manage. First, ...&lt;br /&gt; &lt;strong&gt;Final Idea:&lt;/strong&gt; &lt;em&gt;propose a novel approach to cancer treatment by developing personalized cancer vaccines tailored to each patient's unique tumor mutations. By integrating machine learning algorithms with comprehensive omics data, including genomics, epigenetics, transcriptomics, and proteomics, this method aims to identify and prioritize the most immunogenic tumor-specific antigens. This integrated strategy could enhance vaccine efficacy and represents an advancement in precision medicine for cancer treatment.&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The model runs locally via Ollama. Additional details are available in the HF model card.&lt;/p&gt; &lt;p&gt;Model Card: &lt;a href="https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1"&gt;https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1&lt;/a&gt; Q4_K_M GGUF: &lt;a href="https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1-GGUF"&gt;https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1-GGUF&lt;/a&gt; Or you can just pull it from ollama!&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama run 6cf/QwQ-32B-Preview-IdeaWhiz-v1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Training Dataset: &lt;a href="https://huggingface.co/datasets/6cf/liveideabench"&gt;https://huggingface.co/datasets/6cf/liveideabench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would be interested in hearing how others might use this for their research workflows.&lt;/p&gt; &lt;p&gt;Note: This is an experimental project, still being refined. Feedback and suggestions welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realJoeTrump"&gt; /u/realJoeTrump &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T05:19:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw5amg</id>
    <title>I just released Notate – Open-source AI research assistant with local LLM support</title>
    <updated>2025-01-07T23:25:52+00:00</updated>
    <author>
      <name>/u/Hairetsu</name>
      <uri>https://old.reddit.com/user/Hairetsu</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairetsu"&gt; /u/Hairetsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://notate.hairetsu.com"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw5amg/i_just_released_notate_opensource_ai_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw5amg/i_just_released_notate_opensource_ai_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T23:25:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwix4d</id>
    <title>Something to consider trying if you almost have enough RAM to fit the DeepSeek V3 Quant you want on linux.</title>
    <updated>2025-01-08T12:19:07+00:00</updated>
    <author>
      <name>/u/MoneyPowerNexis</name>
      <uri>https://old.reddit.com/user/MoneyPowerNexis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am testing what its like to not quite have enough RAM for DeepSeek V3 Q4 by having a program allocate a chunk of memory. There is a very clear performance hit when I do this and I can assume that it will be the same if I physically dont have enough RAM to fit all of the data contained in a model in RAM. &lt;/p&gt; &lt;p&gt;I am also testing an approach to mitigate some of the performance hit that happens in this situation.&lt;/p&gt; &lt;p&gt;I am trying out ZRAM which is a Linux kernel module that creates compressed block devices in RAM. &lt;/p&gt; &lt;p&gt;The following (for experimental people)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apt install zram-config &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;will setup a system swap device using ZRAM so that when data is evicted from disk cache it ends up first in this compressed block of RAM&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo nano /etc/default/zramswap &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This opens the config file &lt;/p&gt; &lt;pre&gt;&lt;code&gt;systemctl restart zramswap &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This applies config file changes (see google / chatgpt for more about ZRAM)&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;The idea is that if the compression is good enough this swap will keep all of the data that would otherwise need to be retrieved from a disk drive / ssd in a compressed form which because its in RAM will be much faster to retrieve.&lt;/p&gt; &lt;p&gt;I am also experementing with different compression algorithms that ZRAM uses until I find one that looks fast&lt;/p&gt; &lt;p&gt;Here is my progression:&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Memory hog program using 150GB of RAM out of 512GB ram&lt;/h2&gt; &lt;pre&gt;&lt;code&gt;1.39 tokens per second (30 tokens, A100 64gb + 2x A6000 48GB) 1.54 tokens per second (30 tokens, CPU) &amp;lt;-- less bad than expected for some reason limiting Q3 had a bigger impact on speed for me at 0.27 t/s but I guess the same amount of missing RAM was more of the model as a percentage not cached in disk cache &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;p&gt;Enabled ZRAM swap with the following parameters and results AlGO is the compression algorithm used by zram&lt;/p&gt; &lt;h2&gt;multiple inference runs where I find the results interesting&lt;/h2&gt; &lt;pre&gt;&lt;code&gt;ALGO=zstd PERCENTAGE=30 1.25 tokens per second 1.38 tokens per second (~30 tokens, A100 64gb + 2x A6000 48GB) ALGO=zstd ALLOCATION=300000 2.34 tokens per second ALGO=zstd ALLOCATION=410000 3.58 tokens per second ALGO=LZO-RLE ALLOCATION=300000 2.12 tokens per second 2.64 tokens per second ALGO=DEFLATE ALLOCATION=410000 1.74 tokens per second (30 tokens, CPU) 5.05 tokens per second (30 tokens, CPU) jackpot !!! 4.91 tokens per second (30 tokens, CPU) 5.76 tokens per second (299 tokens, CPU) 6.16 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 5.05 tokens per second (~300 tokens, memory hog program closed and reopened to make sure its still hogging ram, A100 64gb + 2x A6000 48GB) 6.04 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 6.39 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 6.46 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) ALGO=DEFLATE ALLOCATION=350000 4.66 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) basically a warm up after reducing ZRAM 6.22 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 6.42 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) &amp;lt;-- I'm calling this pretty good ALGO=DEFLATE ALLOCATION=300000 3.55 tokens per second (~300 tokens, CPU warmup) 3.81 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB, boo!) 4.59 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB, ok maybe longer warm-up, got to get everything compressed to make fast) 3.64 tokens per second (nope) &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;h2&gt;Memory hog program terminated so all 512GB ram is available&lt;/h2&gt; &lt;pre&gt;&lt;code&gt;# 300GB ZRAM but I close the memory hog app: 6.44 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 6.44 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) &amp;lt;-- seems good but compare to 8.07 t/s bellow with zram off, if its not needed dont use it # deactivate zram swap: `sudo swapoff /dev/zram0` 6.42 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 8.07 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 8.08 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) &amp;lt;-- baseline cpu + gpu 6.37 tokens per second (~300 tokens, CPU only) 6.43 tokens per second (~300 tokens, CPU only) &amp;lt;-- baseline cpu &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This was by no means an exhaustive search of all the possible parameters and a real shortfall of RAM will lead to different parameters being optimal. There are more compression algorithms to try and of course a simulated shortage of RAM might be different to the real thing (I'm not pulling out RAM sticks to try, I'm too lazy for that) but I think this was a successful proof of concept.&lt;/p&gt; &lt;p&gt;There is a cost to doing this, ZRAM uses processing power for compression and more importantly decompression adds overhead and latency too. ZRAM uses up RAM so if its not needed to run a model (quickly) it is just all downside. However I think it makes a lot of sense for a server that is being dedicated to a model that does not have quite enough RAM where using it gets you that bit extra. &lt;/p&gt; &lt;p&gt;If I decide to download Q8 which is a model too big for my available RAM I will certainly try this method with it to see how much faster it will make it if at all. Although I might wait until I get more data on how good the smaller quants are in comparison.&lt;/p&gt; &lt;p&gt;It seems relatively easy to activate and deactivate ZRAM so if you dont want it hogging RAM and CPU resources most of the time then that does not seem like an issue. &lt;/p&gt; &lt;p&gt;Also the more data oriented person might just compress the models to see what the exact compression ratios are like to determine if its worth it and what amount of ZRAM would be optimal. &lt;/p&gt; &lt;p&gt;There might also be ways to have llamacpp and such just compress the data directly that I'm ignorant of.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoneyPowerNexis"&gt; /u/MoneyPowerNexis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwix4d/something_to_consider_trying_if_you_almost_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwix4d/something_to_consider_trying_if_you_almost_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwix4d/something_to_consider_trying_if_you_almost_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T12:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwnr3s</id>
    <title>What would you like to see on a new architecture?</title>
    <updated>2025-01-08T16:11:00+00:00</updated>
    <author>
      <name>/u/Equivalent_Award7202</name>
      <uri>https://old.reddit.com/user/Equivalent_Award7202</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What would you like to be implemented on a new architecture?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Equivalent_Award7202"&gt; /u/Equivalent_Award7202 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwnr3s/what_would_you_like_to_see_on_a_new_architecture/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwnr3s/what_would_you_like_to_see_on_a_new_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwnr3s/what_would_you_like_to_see_on_a_new_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T16:11:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw1nze</id>
    <title>DeepSeek V3 GGUF 2-bit surprisingly works! + BF16, other quants</title>
    <updated>2025-01-07T20:51:39+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys we uploaded GGUF's including 2, 3 ,4, 5, 6 and 8-bit quants for Deepseek V3.&lt;/p&gt; &lt;p&gt;We've also de-quantized Deepseek-V3 to upload the &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-bf16"&gt;bf16 version&lt;/a&gt; so you guys can experiment with it (1.3TB)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Minimum hardware requirements&lt;/strong&gt; to run Deepseek-V3 in 2-bit: 48GB RAM + 250GB of disk space.&lt;/p&gt; &lt;p&gt;See how to run Deepseek V3 with examples and our full collection here: &lt;a href="https://huggingface.co/collections/unsloth/deepseek-v3-all-versions-677cf5cfd7df8b7815fc723c"&gt;https://huggingface.co/collections/unsloth/deepseek-v3-all-versions-677cf5cfd7df8b7815fc723c&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Deepseek V3 version&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GGUF&lt;/td&gt; &lt;td align="left"&gt;2-bit: &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q2_K_XS%2FDeepSeek-V3-Q2_K_XS-00001-of-00005.gguf"&gt;Q2_K_XS&lt;/a&gt; and &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q2_K_L%2FDeepSeek-V3-Q2_K_L-00001-of-00005.gguf"&gt;Q2_K_L&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GGUF&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q3_K_M%2FDeepSeek-V3-Q3_K_M-00001-of-00007.gguf"&gt;3&lt;/a&gt;, &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q4_K_M%2FDeepSeek-V3-Q4_K_M-00001-of-00009.gguf"&gt;4&lt;/a&gt;, &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q5_K_M%2FDeepSeek-V3-Q5_K_M-00001-of-00010.gguf"&gt;5&lt;/a&gt;, &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q6_K%2FDeepSeek-V3-Q6_K-00001-of-00012.gguf"&gt;6&lt;/a&gt; and &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q8_0%2FDeepSeek-V3-BF16-256x20B-Q8_0-00001-of-00016.gguf"&gt;8-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bf16&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-bf16"&gt;dequantized 16-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth &lt;/a&gt;GGUF model details:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant Type&lt;/th&gt; &lt;th align="left"&gt;Disk Size&lt;/th&gt; &lt;th align="left"&gt;Details&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q2_K_XS"&gt;Q2_K_XS&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;207GB&lt;/td&gt; &lt;td align="left"&gt;Q2 everything, Q4 embed, Q6 lm_head&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q2_K_L"&gt;Q2_K_L&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;228GB&lt;/td&gt; &lt;td align="left"&gt;Q3 down_proj Q2 rest, Q4 embed, Q6 lm_head&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q3_K_M"&gt;Q3_K_M&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;298GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q3_K_M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q4_K_M"&gt;Q4_K_M&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;377GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q4_K_M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q5_K_M"&gt;Q5_K_M&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;443GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q5_K_M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q6_K"&gt;Q6_K&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;513GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q6_K&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q8_0"&gt;Q8_0&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;712GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q8_0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q2_K_XS"&gt;Q2_K_XS&lt;/a&gt; should run ok in ~40GB of CPU / GPU VRAM with automatic llama.cpp offloading.&lt;/li&gt; &lt;li&gt;Use K quantization (not V quantization)&lt;/li&gt; &lt;li&gt;Do not forget about &lt;code&gt;&amp;lt;｜User｜&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;｜Assistant｜&amp;gt;&lt;/code&gt; tokens! - Or use a chat template formatter&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Example with Q5_0 K quantized cache (V quantized cache doesn't work):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama.cpp/llama-cli --model unsloth/DeepSeek-V3-GGUF/DeepSeek-V3-Q2_K_XS/DeepSeek-V3-Q2_K_XS-00001-of-00005.gguf --cache-type-k q5_0 --prompt '&amp;lt;｜User｜&amp;gt;What is 1+1?&amp;lt;｜Assistant｜&amp;gt;' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and running the above generates:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The sum of 1 and 1 is **2**. Here's a simple step-by-step breakdown: 1. **Start with the number 1.** 2. **Add another 1 to it.** 3. **The result is 2.** So, **1 + 1 = 2**. [end of text] &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw1nze/deepseek_v3_gguf_2bit_surprisingly_works_bf16/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw1nze/deepseek_v3_gguf_2bit_surprisingly_works_bf16/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw1nze/deepseek_v3_gguf_2bit_surprisingly_works_bf16/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T20:51:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvwwsq</id>
    <title>Exolab: NVIDIA's Digits Outperforms Apple's M4 Chips in AI Inference</title>
    <updated>2025-01-07T17:35:44+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwwsq/exolab_nvidias_digits_outperforms_apples_m4_chips/"&gt; &lt;img alt="Exolab: NVIDIA's Digits Outperforms Apple's M4 Chips in AI Inference" src="https://external-preview.redd.it/MBJSLr1BrGGAWiERNKV1EsVPqt-H8qI6KhVXjMFntpA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ffe16868d431044dc8975ae38ca0056c5252984" title="Exolab: NVIDIA's Digits Outperforms Apple's M4 Chips in AI Inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/alexocheema/status/1876676954549620961?s=46"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwwsq/exolab_nvidias_digits_outperforms_apples_m4_chips/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwwsq/exolab_nvidias_digits_outperforms_apples_m4_chips/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T17:35:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwielp</id>
    <title>How do you keep up with the SOTA of everything? Where's the best leaderboards?</title>
    <updated>2025-01-08T11:49:46+00:00</updated>
    <author>
      <name>/u/ThrowawayProgress99</name>
      <uri>https://old.reddit.com/user/ThrowawayProgress99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My knowledge basically only covers LLMs, and image and video generators. Even within that there's a lot I just don't know. My clumsy searching on Arxiv can only go so far.&lt;/p&gt; &lt;p&gt;For example, I would like to get into the voice side of things, and not just in English. How the hell do I start? I'd need to know which models know certain languages the best, which can decipher my speech the best, which can output the most natural and low-latency speech, which can best see/transcribe foreign videos/images, etc.&lt;/p&gt; &lt;p&gt;What about the kinds of models now popping up, that can see and understand your screen, and even perform computer tasks for you? That's several different skills there too.&lt;/p&gt; &lt;p&gt;Not to mention the countless RAG posts. How and where would I start? Learning how to give any model some form of memory would be so useful. I'd love to have a chatbot that remembers what we talked about before, and have it constantly maintain and evolve its personality and memories over time. I know it's been done.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThrowawayProgress99"&gt; /u/ThrowawayProgress99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwielp/how_do_you_keep_up_with_the_sota_of_everything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwielp/how_do_you_keep_up_with_the_sota_of_everything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwielp/how_do_you_keep_up_with_the_sota_of_everything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T11:49:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwmlz8</id>
    <title>The pipeline I follow for open source LLM model finetuning</title>
    <updated>2025-01-08T15:22:16+00:00</updated>
    <author>
      <name>/u/Ahmad401</name>
      <uri>https://old.reddit.com/user/Ahmad401</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working on local LLMs and training for quite some time. Based on my experience, its a two fold problem. Which can be addressed in three phases. &lt;/p&gt; &lt;p&gt;Phase-1: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Development of the full solution using any close source model like ChatGPT or Geminai. &lt;/li&gt; &lt;li&gt;Measuring the accuracy and storing the output for few samples (like 100)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;OUTCOME: Pipeline Development, Base Accuracy and rough annotations&lt;/p&gt; &lt;p&gt;Phase-2:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Correcting the rough annotations and creating a small dataset&lt;/li&gt; &lt;li&gt;Selecting a local LLM and finetuning that with the small dataset&lt;/li&gt; &lt;li&gt;Measuring the results accuracy and quality&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;OUTCOME: Streamlined prompts, dataset and model training flow&lt;/p&gt; &lt;p&gt;Phase-3:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Using this model and developing large scale psudo dataset&lt;/li&gt; &lt;li&gt;Correcting the psudo dataset and&lt;/li&gt; &lt;li&gt;Finetuning model with largescale data&lt;/li&gt; &lt;li&gt;Testing the accuracy and results quality. &lt;/li&gt; &lt;li&gt;Repeating until the desired results are met&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;OUTCOME: Suffisticated dataset, properly trained model&lt;/p&gt; &lt;p&gt;Phase-4: (OPTIONAL) Benchmarking with other closed source LLMs and preparing a benchmarking report. &lt;/p&gt; &lt;p&gt;Any thoughts on this flow. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ahmad401"&gt; /u/Ahmad401 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwmlz8/the_pipeline_i_follow_for_open_source_llm_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwmlz8/the_pipeline_i_follow_for_open_source_llm_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwmlz8/the_pipeline_i_follow_for_open_source_llm_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T15:22:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwlysx</id>
    <title>I made a site to curate the latest AI jobs. Everyday 100s of new positions are added.</title>
    <updated>2025-01-08T14:53:13+00:00</updated>
    <author>
      <name>/u/WordyBug</name>
      <uri>https://old.reddit.com/user/WordyBug</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlysx/i_made_a_site_to_curate_the_latest_ai_jobs/"&gt; &lt;img alt="I made a site to curate the latest AI jobs. Everyday 100s of new positions are added." src="https://external-preview.redd.it/MXJtOXg0NHM5c2JlMT0OwWybS_MdntybaH3TpXxPIGgpidVBmRMcLOJvhoaK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=773834229e5f4771468b7539844a29ca53e04b38" title="I made a site to curate the latest AI jobs. Everyday 100s of new positions are added." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WordyBug"&gt; /u/WordyBug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vgp89y6s9sbe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlysx/i_made_a_site_to_curate_the_latest_ai_jobs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlysx/i_made_a_site_to_curate_the_latest_ai_jobs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T14:53:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwp00r</id>
    <title>Interesting Solution to the problem of Misguided Attention: "Mindful Attention"</title>
    <updated>2025-01-08T17:01:17+00:00</updated>
    <author>
      <name>/u/vesudeva</name>
      <uri>https://old.reddit.com/user/vesudeva</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwp00r/interesting_solution_to_the_problem_of_misguided/"&gt; &lt;img alt="Interesting Solution to the problem of Misguided Attention: &amp;quot;Mindful Attention&amp;quot;" src="https://external-preview.redd.it/okxb_U2_vWgnDkR1V08qly94GnsdGTVu0C8vagF8k-M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d9d2562f385dca6a43d030b7263e0bc59a432aa" title="Interesting Solution to the problem of Misguided Attention: &amp;quot;Mindful Attention&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vesudeva"&gt; /u/vesudeva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/posts/Severian/375067343900874"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwp00r/interesting_solution_to_the_problem_of_misguided/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwp00r/interesting_solution_to_the_problem_of_misguided/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T17:01:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwf5lv</id>
    <title>I Tested Aider vs Cline using DeepSeek 3: Codebase &gt;20k LOC...</title>
    <updated>2025-01-08T08:18:08+00:00</updated>
    <author>
      <name>/u/marvijo-software</name>
      <uri>https://old.reddit.com/user/marvijo-software</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Testing the very best Open Source AI Coding Tools with Medium-sized Codebases&lt;/p&gt; &lt;p&gt;TL;DR&lt;/p&gt; &lt;p&gt;- this is especially focused on how the tools perform in 10k+ codebases, not snake games&lt;/p&gt; &lt;p&gt;- the two (Aider and Cline) are close (for my use cases); I prefer Aider&lt;/p&gt; &lt;p&gt;- Aider is more flexible: can run as a dev version allowing custom modifications (not custom instructions)&lt;/p&gt; &lt;p&gt;- Aider can use your existing $20 ChatGPT/Claude Subscription etc to apply code for you!&lt;/p&gt; &lt;p&gt;- Qwen 2.5 Coder 32B is nowhere close to DeeepSeek 3 in terms of coding in medium-large code bases&lt;/p&gt; &lt;p&gt;- Aider is portable: I jump between IDEs and tools and don't want the limitations to VSCode/forks&lt;/p&gt; &lt;p&gt;- Aider has scripting: enabling use in external agentic environments&lt;/p&gt; &lt;p&gt;- Aider is more economic: uses less tokens, even though Cline tried adding diffs&lt;/p&gt; &lt;p&gt;- I can work with Aider on the same codebase concurrently&lt;/p&gt; &lt;p&gt;- Claude 3.5 Sonnet is somehow clearly better at larger codebases than DeepSeek 3, though it's closer otherwise&lt;/p&gt; &lt;p&gt;I think we are ready to move away from benchmarking good coding LLMs and Coding tools against simple tasks and start to think organizational/enterprise. I'm working on CrewAI + Aider, looks promising.&lt;/p&gt; &lt;p&gt;If interested, here's the test video: &lt;a href="https://youtu.be/e1oDWeYvPbY"&gt;https://youtu.be/e1oDWeYvPbY&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please let me know of your experience with using AI coding in more challenging environments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marvijo-software"&gt; /u/marvijo-software &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf5lv/i_tested_aider_vs_cline_using_deepseek_3_codebase/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf5lv/i_tested_aider_vs_cline_using_deepseek_3_codebase/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf5lv/i_tested_aider_vs_cline_using_deepseek_3_codebase/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:18:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwkjq0</id>
    <title>MiniThinky 1B - My first trial to make a reasoning model</title>
    <updated>2025-01-08T13:44:30+00:00</updated>
    <author>
      <name>/u/MediocreProgrammer99</name>
      <uri>https://old.reddit.com/user/MediocreProgrammer99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;This is my first trial to fine tune a small model, adding the reasoning capability.&lt;/p&gt; &lt;p&gt;I took Llama 3.2 1B as the base model, so the size is very small.&lt;/p&gt; &lt;p&gt;Check it out here ==&amp;gt; &lt;a href="https://huggingface.co/ngxson/MiniThinky-v2-1B-Llama-3.2"&gt;https://huggingface.co/ngxson/MiniThinky-v2-1B-Llama-3.2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF version (runnable directly via ollama): &lt;a href="https://huggingface.co/ngxson/MiniThinky-v2-1B-Llama-3.2-Q8_0-GGUF"&gt;https://huggingface.co/ngxson/MiniThinky-v2-1B-Llama-3.2-Q8_0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MediocreProgrammer99"&gt; /u/MediocreProgrammer99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwkjq0/minithinky_1b_my_first_trial_to_make_a_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwkjq0/minithinky_1b_my_first_trial_to_make_a_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwkjq0/minithinky_1b_my_first_trial_to_make_a_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T13:44:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwf4jm</id>
    <title>[Second Take] Kokoro-82M is an Apache TTS model</title>
    <updated>2025-01-08T08:16:07+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I trained this model recently: &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;https://huggingface.co/hexgrad/Kokoro-82M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everything is in the README there, TLDR: Kokoro is a TTS model that is very good for its size.&lt;/p&gt; &lt;p&gt;Apologies for the double-post, but the first one was cooking, and it suddenly got `ledeted` by `domeration` (yes, I'm `simpelling` on purpose, it will make sense soon).&lt;/p&gt; &lt;p&gt;Last time I tried giving longer, meaningful replies to people in the comments, which kept getting `dashow-nabbed`, and when I edited to the OP to include that word which must not be named, the whole post was poofed. This time I will shut up and let the post speak for itself, and you can find me on `sidcord` where we can speak more freely, since I appear to have GTA 5 stars over here.&lt;/p&gt; &lt;p&gt;Finally, I am also collecting synthetic audio, see &lt;a href="https://hf.co/posts/hexgrad/418806998707773"&gt;https://hf.co/posts/hexgrad/418806998707773&lt;/a&gt; if interested.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf4jm/second_take_kokoro82m_is_an_apache_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf4jm/second_take_kokoro82m_is_an_apache_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf4jm/second_take_kokoro82m_is_an_apache_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwfblu</id>
    <title>NVIDIA Open Model License: NVIDIA Cosmos is a world foundation model trained on 20 million hours of video to build virtual worlds and generate photo-real, physically-based synthetic data for scientific and industrial testing.</title>
    <updated>2025-01-08T08:29:57+00:00</updated>
    <author>
      <name>/u/Powerful-Solution646</name>
      <uri>https://old.reddit.com/user/Powerful-Solution646</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfblu/nvidia_open_model_license_nvidia_cosmos_is_a/"&gt; &lt;img alt="NVIDIA Open Model License: NVIDIA Cosmos is a world foundation model trained on 20 million hours of video to build virtual worlds and generate photo-real, physically-based synthetic data for scientific and industrial testing." src="https://external-preview.redd.it/dGFrdTNlbm5kcWJlMeFSSXTYDvjzDDIYxHTRsBuU24PYEoa111CihFQLGiR7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=950ef6aabdd5309efe066d1a0cf14319e919f04f" title="NVIDIA Open Model License: NVIDIA Cosmos is a world foundation model trained on 20 million hours of video to build virtual worlds and generate photo-real, physically-based synthetic data for scientific and industrial testing." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nvidia.com/en-in/ai/cosmos/"&gt;https://www.nvidia.com/en-in/ai/cosmos/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Solution646"&gt; /u/Powerful-Solution646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lfzohbxndqbe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfblu/nvidia_open_model_license_nvidia_cosmos_is_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfblu/nvidia_open_model_license_nvidia_cosmos_is_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwn90v</id>
    <title>Phi 4 MIT licensed - its show time folks</title>
    <updated>2025-01-08T15:50:30+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/phi-4"&gt;https://huggingface.co/microsoft/phi-4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwn90v/phi_4_mit_licensed_its_show_time_folks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwn90v/phi_4_mit_licensed_its_show_time_folks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwn90v/phi_4_mit_licensed_its_show_time_folks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T15:50:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwjzw1</id>
    <title>A Recipe for a Better Code Generator with RAG</title>
    <updated>2025-01-08T13:15:30+00:00</updated>
    <author>
      <name>/u/agbell</name>
      <uri>https://old.reddit.com/user/agbell</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwjzw1/a_recipe_for_a_better_code_generator_with_rag/"&gt; &lt;img alt="A Recipe for a Better Code Generator with RAG" src="https://external-preview.redd.it/LXLJBuPegXr_jr-v3g2QasDpI6sYCw7nkCBT17bt6Qs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25403c01daf573ed3ba6a7bc1d3fb35fac180834" title="A Recipe for a Better Code Generator with RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/agbell"&gt; /u/agbell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.pulumi.com/blog/codegen-learnings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwjzw1/a_recipe_for_a_better_code_generator_with_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwjzw1/a_recipe_for_a_better_code_generator_with_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T13:15:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwfm8k</id>
    <title>Tech lead of Qwen Team, Alibaba Group: "I often recommend people to read the blog of Anthropic to learn more about what agent really is. Then you will realize you should invest on it as much as possible this year." Blog linked in body text.</title>
    <updated>2025-01-08T08:50:35+00:00</updated>
    <author>
      <name>/u/Powerful-Solution646</name>
      <uri>https://old.reddit.com/user/Powerful-Solution646</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfm8k/tech_lead_of_qwen_team_alibaba_group_i_often/"&gt; &lt;img alt="Tech lead of Qwen Team, Alibaba Group: &amp;quot;I often recommend people to read the blog of Anthropic to learn more about what agent really is. Then you will realize you should invest on it as much as possible this year.&amp;quot; Blog linked in body text." src="https://preview.redd.it/5lmmx4qchqbe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f0c18af44997be0f0628d875475982b7bf3b877" title="Tech lead of Qwen Team, Alibaba Group: &amp;quot;I often recommend people to read the blog of Anthropic to learn more about what agent really is. Then you will realize you should invest on it as much as possible this year.&amp;quot; Blog linked in body text." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/JustinLin610/status/1876324689657954413?t=rQiJk8V8N9-Rd8dcWJedww&amp;amp;s=19"&gt;https://x.com/JustinLin610/status/1876324689657954413?t=rQiJk8V8N9-Rd8dcWJedww&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.anthropic.com/research/building-effective-agents"&gt;https://www.anthropic.com/research/building-effective-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Solution646"&gt; /u/Powerful-Solution646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5lmmx4qchqbe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfm8k/tech_lead_of_qwen_team_alibaba_group_i_often/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfm8k/tech_lead_of_qwen_team_alibaba_group_i_often/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:50:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwe9mf</id>
    <title>HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it</title>
    <updated>2025-01-08T07:20:14+00:00</updated>
    <author>
      <name>/u/quantier</name>
      <uri>https://old.reddit.com/user/quantier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"&gt; &lt;img alt="HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it" src="https://external-preview.redd.it/2Uxr2fZXgwYpxUcnSif2gZmNvP23o2dpwlhS4x1dHZA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54d46261045d9a2cee779ef1547c528c90021757" title="HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;96 GB out of the 128GB can be allocated to use VRAM making it able to run 70B models q8 with ease.&lt;/p&gt; &lt;p&gt;I am pretty sure Digits will use CUDA and/or TensorRT for optimization of inferencing.&lt;/p&gt; &lt;p&gt;I am wondering if this will use RocM or if we can just use CPU inferencing - wondering what the acceleration will be here. Anyone able to share insights?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantier"&gt; /u/quantier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://aecmag.com/workstations/hp-amd-ryzen-ai-max-pro-hp-zbook-ultra-g1a-hp-z2-mini-g1a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T07:20:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwlka6</id>
    <title>I made the world's first AI meeting copilot, and open sourced it!</title>
    <updated>2025-01-08T14:33:52+00:00</updated>
    <author>
      <name>/u/stealthanthrax</name>
      <uri>https://old.reddit.com/user/stealthanthrax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of relying on clunky SaaS tools for meeting transcriptions that didn’t respect my privacy or workflow. Everyone I tried had issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bots awkwardly join meetings and announce themselves.&lt;/li&gt; &lt;li&gt;Poor transcription quality.&lt;/li&gt; &lt;li&gt;No flexibility to tweak things to fit &lt;em&gt;my&lt;/em&gt; setup.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I built &lt;strong&gt;Amurex&lt;/strong&gt;, a self-hosted solution that actually works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Records meetings quietly, with no bots interrupting.&lt;/li&gt; &lt;li&gt;Delivers clean, accurate diarized transcripts right after the meeting.&lt;/li&gt; &lt;li&gt;Does late meeting summaries. i.e. a recap for a meeting if I am late&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But most importantly, it has it is the only meeting tool in the world that can give&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time suggestions to stay engaged in boring meetings.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It’s completely open source and designed for self-hosting, so you control your data and your workflow. No subscriptions, and no vendor lock-in.&lt;/p&gt; &lt;p&gt;I would love to know what you all think of it. It only works on Google Meet for now but I will be scaling it to all the famous meeting providers.&lt;/p&gt; &lt;p&gt;Github - &lt;a href="https://github.com/thepersonalaicompany/amurex"&gt;https://github.com/thepersonalaicompany/amurex&lt;/a&gt;&lt;br /&gt; Website - &lt;a href="https://www.amurex.ai/"&gt;https://www.amurex.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stealthanthrax"&gt; /u/stealthanthrax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlka6/i_made_the_worlds_first_ai_meeting_copilot_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlka6/i_made_the_worlds_first_ai_meeting_copilot_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlka6/i_made_the_worlds_first_ai_meeting_copilot_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T14:33:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwmy39</id>
    <title>Phi-4 has been released</title>
    <updated>2025-01-08T15:37:07+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwmy39/phi4_has_been_released/"&gt; &lt;img alt="Phi-4 has been released" src="https://external-preview.redd.it/gF2xHg5eGxs-B6pIwB6VodFWhOArLu2_8o4wQP6siP8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed435e811f517c43ffac1607f9719679d8dd1b5d" title="Phi-4 has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/phi-4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwmy39/phi4_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwmy39/phi4_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T15:37:07+00:00</published>
  </entry>
</feed>
