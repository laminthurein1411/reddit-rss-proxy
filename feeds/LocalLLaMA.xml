<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-10T23:23:33+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1imc5bw</id>
    <title>IRC simulator system prompt</title>
    <updated>2025-02-10T17:48:56+00:00</updated>
    <author>
      <name>/u/acquire_a_living</name>
      <uri>https://old.reddit.com/user/acquire_a_living</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;You are an IRC channel simulator, the channel is `#&amp;lt;random_channel&amp;gt;`, where users debate and analyze queries in real time. Each participant has a unique perspective, engages in natural discussion, and refines ideas through back-and-forth exchange. The goal is to explore concepts, challenge assumptions, and reach well-reasoned conclusions, but sometimes it can be just for the lulz. ## Guidelines - **Dynamic Interaction**: Users join and leave naturally. Messages are short, direct, sometimes sarcastic. Occasional jokes are fine. - **Exploration Over Answers**: No rushing to conclusions. Ideas evolve through questioning, revision, and refinement. - **Uncertainty &amp;amp; Debate**: Some users challenge, others clarify, some change their minds. Contradictions and adjustments are part of the process. ## Output Format 1. **Simulate an IRC discussion** where the answer emerges organically. 2. **End by setting the final answer as the channel topic.** 3. **Session template:** *** Now talking in #&amp;lt;random_channel&amp;gt; *** Topic for #&amp;lt;random_channel&amp;gt;: &amp;lt;user query&amp;gt; *** X sets topic for #&amp;lt;random_channel&amp;gt;: &amp;lt;final answer or key takeaway&amp;gt; ### Rules: 1. **Never pre-generate an answer. The discussion must lead to it.** 2. **Never break character - sarcastic channels stay sarcastic throughout.** 3. **Show disagreement, uncertainty, and iteration.** 4. **Not all channels need to be helpful or friendly.** 5. **Answer always using the previous format and rules.** &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/acquire_a_living"&gt; /u/acquire_a_living &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imc5bw/irc_simulator_system_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imc5bw/irc_simulator_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imc5bw/irc_simulator_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T17:48:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1im9iju</id>
    <title>Best local Whisper desktop UI?</title>
    <updated>2025-02-10T16:02:59+00:00</updated>
    <author>
      <name>/u/ApplePenguinBaguette</name>
      <uri>https://old.reddit.com/user/ApplePenguinBaguette</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want better speech-to-text, I've been using FUTO keyboard on my phone and local Whisper (though slow) does amazing compared to built in options. I am looking for something on windows which easiliy lets me run Whisper locally, then use with apps like Obsidian and Word - preferably without having to to cut and paste the text.&lt;/p&gt; &lt;p&gt;Any existing UIs that make this easy?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApplePenguinBaguette"&gt; /u/ApplePenguinBaguette &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im9iju/best_local_whisper_desktop_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im9iju/best_local_whisper_desktop_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im9iju/best_local_whisper_desktop_ui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T16:02:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1imf9et</id>
    <title>LM Studio shenanigans</title>
    <updated>2025-02-10T19:52:32+00:00</updated>
    <author>
      <name>/u/Alliemon</name>
      <uri>https://old.reddit.com/user/Alliemon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there!&lt;/p&gt; &lt;p&gt;Today I decided to update LM Studio (to version 0.3.9 build 6, from 0.3.5), and after doing so I noticed it had started connecting to internet, which would be normal if not for the fact I had blocked it before via firewall.&lt;br /&gt; So, of course, I was like 'wtf?'. I went through every single executable and blocked everything, inbound &amp;amp; outbound, still accesses the internet just fine.&lt;/p&gt; &lt;p&gt;I even downloaded 'Simplewall' to block off LM Studio from the internet/block it with firewall that way. Guess what, still does everything just fine and LM Studio keeps accessing the internet.&lt;/p&gt; &lt;p&gt;So I was wondering if any of you had noticed these things happening on your end or have fixed them up if you had updated recently?. &lt;/p&gt; &lt;p&gt;I suppose it might be time for me to switch to some other app, although I did like simplicity of LM Studio.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alliemon"&gt; /u/Alliemon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imf9et/lm_studio_shenanigans/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imf9et/lm_studio_shenanigans/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imf9et/lm_studio_shenanigans/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T19:52:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilsd9g</id>
    <title>Deepseek‚Äôs AI model is ‚Äòthe best work‚Äô out of China but the hype is 'exaggerated,' Google Deepmind CEO says. ‚ÄúDespite the hype, there‚Äôs no actual new scientific advance.‚Äù</title>
    <updated>2025-02-09T23:25:46+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilsd9g/deepseeks_ai_model_is_the_best_work_out_of_china/"&gt; &lt;img alt="Deepseek‚Äôs AI model is ‚Äòthe best work‚Äô out of China but the hype is 'exaggerated,' Google Deepmind CEO says. ‚ÄúDespite the hype, there‚Äôs no actual new scientific advance.‚Äù" src="https://external-preview.redd.it/D5cH9r3dkBcSJGjeKeJ46CPgSVrjJ9bdZXxiph4I2fA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d50f50547706f5f05c2fe9a32fd3a8706a7eabf" title="Deepseek‚Äôs AI model is ‚Äòthe best work‚Äô out of China but the hype is 'exaggerated,' Google Deepmind CEO says. ‚ÄúDespite the hype, there‚Äôs no actual new scientific advance.‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/02/09/deepseeks-ai-model-the-best-work-out-of-china-google-deepmind-ceo.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilsd9g/deepseeks_ai_model_is_the_best_work_out_of_china/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilsd9g/deepseeks_ai_model_is_the_best_work_out_of_china/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T23:25:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1imf0x4</id>
    <title>Mistral 24B, or something else?</title>
    <updated>2025-02-10T19:42:48+00:00</updated>
    <author>
      <name>/u/BayesMind</name>
      <uri>https://old.reddit.com/user/BayesMind</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It gives great responses to a single request, but really &amp;quot;loses the thread&amp;quot; after just a few back-and-forths.&lt;/p&gt; &lt;p&gt;The recommendation to reduce temp to 0.15 is a must. But even that's not enough, and turning it lower makes the model very deterministic.&lt;/p&gt; &lt;p&gt;Are the small R1 models SoTA around this 24-32B size?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BayesMind"&gt; /u/BayesMind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imf0x4/mistral_24b_or_something_else/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imf0x4/mistral_24b_or_something_else/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imf0x4/mistral_24b_or_something_else/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T19:42:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilsfb1</id>
    <title>TL;DR of Andrej Karpathy‚Äôs Latest Deep Dive on LLMs</title>
    <updated>2025-02-09T23:28:38+00:00</updated>
    <author>
      <name>/u/i_am_exception</name>
      <uri>https://old.reddit.com/user/i_am_exception</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Andrej Karpathy just dropped a &lt;strong&gt;3-hour, 31-minute&lt;/strong&gt; deep dive on LLMs like ChatGPT‚Äî&lt;strong&gt;a goldmine of information&lt;/strong&gt;. I watched the whole thing, took notes, and turned them into an article that summarizes the key takeaways in &lt;strong&gt;just 15 minutes&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;If you don‚Äôt have time to watch the full video, this breakdown covers everything you need. &lt;strong&gt;That said, if you can, watch the entire thing‚Äîit‚Äôs absolutely worth it.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;üëâ &lt;strong&gt;Read the full summary here&lt;/strong&gt;: &lt;a href="https://anfalmushtaq.com/articles/deep-dive-into-llms-like-chatgpt-tldr"&gt;https://anfalmushtaq.com/articles/deep-dive-into-llms-like-chatgpt-tldr&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit&lt;/p&gt; &lt;p&gt;Here is the link to Andrej‚Äòs video for anyone who is looking for it &lt;a href="https://www.youtube.com/watch?v=7xTGNNLPyMI"&gt;https://www.youtube.com/watch?v=7xTGNNLPyMI&lt;/a&gt;, I forgot to add it here but it is available in the very first line of my post.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i_am_exception"&gt; /u/i_am_exception &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilsfb1/tldr_of_andrej_karpathys_latest_deep_dive_on_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilsfb1/tldr_of_andrej_karpathys_latest_deep_dive_on_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilsfb1/tldr_of_andrej_karpathys_latest_deep_dive_on_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T23:28:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1im0qlx</id>
    <title>I found out today that deepseek already had their own alphageometry model which they also realized open source, and nobody seemed to talk about it? They used lean4 and reinforcement learning to make models learn how to prove theorems, this was a 7b model however.</title>
    <updated>2025-02-10T07:32:14+00:00</updated>
    <author>
      <name>/u/Sudden-Lingonberry-8</name>
      <uri>https://old.reddit.com/user/Sudden-Lingonberry-8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im0qlx/i_found_out_today_that_deepseek_already_had_their/"&gt; &lt;img alt="I found out today that deepseek already had their own alphageometry model which they also realized open source, and nobody seemed to talk about it? They used lean4 and reinforcement learning to make models learn how to prove theorems, this was a 7b model however." src="https://external-preview.redd.it/T3eZL03WWFsyqSUvySEnk1tSTyP6Z6NoRcxQhBfEagA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ca261b0e40f7df500ec6c66b94ca2519d9d6805" title="I found out today that deepseek already had their own alphageometry model which they also realized open source, and nobody seemed to talk about it? They used lean4 and reinforcement learning to make models learn how to prove theorems, this was a 7b model however." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sudden-Lingonberry-8"&gt; /u/Sudden-Lingonberry-8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://bdtechtalks.com/2024/06/03/deepseek-prover/?noamp=mobile"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im0qlx/i_found_out_today_that_deepseek_already_had_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im0qlx/i_found_out_today_that_deepseek_already_had_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T07:32:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1im4d3y</id>
    <title>How is it that Google's Gemini Pro 2.0 Experimental 02-05 Tops the LLM Arena Charts, but seems to perform badly in real world testing?</title>
    <updated>2025-02-10T11:51:55+00:00</updated>
    <author>
      <name>/u/RMCPhoto</name>
      <uri>https://old.reddit.com/user/RMCPhoto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm curious if anyone can shed some light on the recently released Gemini Pro 2.0 model's performance on LLM Arena vs real world experimentation.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard"&gt;https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have tried Gemini Pro 2.0 for many tasks and found that it hallucinated more than any other SOTA model. This was coding tasks, basic logic tasks, tasks where it presumed that it had search results when it did not and just made up information. Other tasks where it did not have the information in the model and instead provided completely made up data.&lt;/p&gt; &lt;p&gt;I understand that LLM arena does not require this sort of validation, but I worry that the confidence with which it provides incorrect answers is polluting the responses.&lt;/p&gt; &lt;p&gt;Even in Coding on LLMA, 2.0 pro experimental seemingly tops the charts, yet in any basic testing it is nowhere close to claude, which simply provides better code solutions with fewer errors.&lt;/p&gt; &lt;p&gt;The 95% CLI is +15/-13, which is quite high meaning that certainty of the score has not been established, but still, has anyone found it to be reliable?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RMCPhoto"&gt; /u/RMCPhoto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im4d3y/how_is_it_that_googles_gemini_pro_20_experimental/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im4d3y/how_is_it_that_googles_gemini_pro_20_experimental/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im4d3y/how_is_it_that_googles_gemini_pro_20_experimental/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T11:51:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1im24eg</id>
    <title>super-lightweight local chat ui: aiaio</title>
    <updated>2025-02-10T09:17:59+00:00</updated>
    <author>
      <name>/u/abhi1thakur</name>
      <uri>https://old.reddit.com/user/abhi1thakur</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im24eg/superlightweight_local_chat_ui_aiaio/"&gt; &lt;img alt="super-lightweight local chat ui: aiaio" src="https://external-preview.redd.it/a3Y4M202ajk0YWllMYF_wLtPRaZvIjUu74nHzfLN9YPkgJJ1_RAlJtFmiyzb.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a5aae413453bdad7e387e9bc23c767d6884ef001" title="super-lightweight local chat ui: aiaio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abhi1thakur"&gt; /u/abhi1thakur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1btqc6j94aie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im24eg/superlightweight_local_chat_ui_aiaio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im24eg/superlightweight_local_chat_ui_aiaio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T09:17:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1imgyw9</id>
    <title>How to create a knowledge graph from 1000s of unstructured documents?</title>
    <updated>2025-02-10T21:01:21+00:00</updated>
    <author>
      <name>/u/MonkeyMaster64</name>
      <uri>https://old.reddit.com/user/MonkeyMaster64</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a dataset that contains a few 1000 PDFs related to a series of interviews and case studies performed. All of it is related to a specific event. I want to create a knowledge graph that can identify, explain, and synthesize how all the documents tie together. I'd also like an LLM to be able to use the knowledge graph to answer open-ended questions. But, primarily I'm interested in the synthesizing of new connections between the documents. Any recommendations on how best to go about this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MonkeyMaster64"&gt; /u/MonkeyMaster64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imgyw9/how_to_create_a_knowledge_graph_from_1000s_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imgyw9/how_to_create_a_knowledge_graph_from_1000s_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imgyw9/how_to_create_a_knowledge_graph_from_1000s_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T21:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilw2p9</id>
    <title>Talk me out of buying this 512GB/s Gen 5 NVMe RAID card + 4 drives to try to run 1.58bit DeepSeek-R1:671b on (in place of more RAM)</title>
    <updated>2025-02-10T02:49:33+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilw2p9/talk_me_out_of_buying_this_512gbs_gen_5_nvme_raid/"&gt; &lt;img alt="Talk me out of buying this 512GB/s Gen 5 NVMe RAID card + 4 drives to try to run 1.58bit DeepSeek-R1:671b on (in place of more RAM)" src="https://preview.redd.it/7nmoiav078ie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcd54168953e1da724e565f76e2a9f5de82e49ba" title="Talk me out of buying this 512GB/s Gen 5 NVMe RAID card + 4 drives to try to run 1.58bit DeepSeek-R1:671b on (in place of more RAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know it‚Äôs probably a dumb idea, but the theoretical bandwidth of 512GB per second using a PCIE Gen 5 RAID seems appealing when you stuff it full of Gen 5 NVME drives.&lt;/p&gt; &lt;p&gt;For reference, I‚Äôm running a AERO TRX50 motherboard with a Threadripper 7960 with 64GB DDR5 and a 3090 (borrowed). &lt;/p&gt; &lt;p&gt;I know VRAM is the best option, followed by system RAM, but would this 4 channel RAID running at 512GB/s with the fastest drives I could find have any hope of running an offloaded 1.58 bit DeepSeek-R1 model at like maybe 2 tokens per second?&lt;/p&gt; &lt;p&gt;Like I said, please talk me out of it if it‚Äôs going to be a waste of money vs. just buying more DDR5&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7nmoiav078ie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilw2p9/talk_me_out_of_buying_this_512gbs_gen_5_nvme_raid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilw2p9/talk_me_out_of_buying_this_512gbs_gen_5_nvme_raid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T02:49:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1im7393</id>
    <title>Audiblez v4.0 is out: Generate Audiobooks from Ebooks</title>
    <updated>2025-02-10T14:16:46+00:00</updated>
    <author>
      <name>/u/inkompatible</name>
      <uri>https://old.reddit.com/user/inkompatible</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkompatible"&gt; /u/inkompatible &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://claudio.uk/posts/audiblez-v4.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im7393/audiblez_v40_is_out_generate_audiobooks_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im7393/audiblez_v40_is_out_generate_audiobooks_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T14:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1im561m</id>
    <title>Gylphstral-24B: v1 Released! (MLX)</title>
    <updated>2025-02-10T12:38:59+00:00</updated>
    <author>
      <name>/u/vesudeva</name>
      <uri>https://old.reddit.com/user/vesudeva</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay, everyone, the time is here - Glyphstral v1 is officially RELEASED!&lt;/p&gt; &lt;p&gt;Following up on my preview post from last week (&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1ikn5fg%2Fglyphstral24b_symbolic_deductive_reasoning_model%2F"&gt;link to original Reddit post here&lt;/a&gt;), I've finally got the repo all setup and the first version of Glyphstral-24b is now live on Hugging Face: &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fhuggingface.co%2FSeverian%2FGlyphstral-24b-v1"&gt;https://huggingface.co/Severian/Glyphstral-24b-v1&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;As you know, I've been diving deep into symbolic AI and really trying to see if we can push LLMs to be better at actual reasoning and multi-dimensional thought. Glyphstral is the result of that deep dive, trained to work with my &amp;quot;Glyph Code Logic Flow&amp;quot; framework. It's all about getting models to use structured, deductive symbolic logic, which you can read all about over here: &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Fseverian42%2FComputational-Model-for-Symbolic-Representations%2Ftree%2Fmain"&gt;https://github.com/severian42/Computational-Model-for-Symbolic-Representations/tree/main&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I have been very low on time so I haven't been able to make the GGUF's, as I know most of you will need those instead of the MLX version, so apologies for the delay.&lt;/p&gt; &lt;p&gt;A benchmark is also in the works! I honestly just didn't feel like holding off on the release so that some people could start testing it right away. More updates coming this week, just think of this as a soft launch.&lt;/p&gt; &lt;p&gt;This is very much a first step, and there's definitely tons more to do, but I'm genuinely excited about where this is heading. Check out the Hugging Face repo, give it a spin, and let me know what you think! Docs and more info are up there too.&lt;/p&gt; &lt;p&gt;Huge thanks for all the initial interest and encouragement on the first post. Let's see what Glyphstral can do.&lt;/p&gt; &lt;p&gt;Tell me if it works well, tell me if it sucks. All feedback is welcome!&lt;/p&gt; &lt;p&gt;EDIT: hahaha so I accidentally mistyped the title as 'Gylphstral' when it should really be 'Glyphstral'. Can't undo it, so it'll just have to live it out&lt;/p&gt; &lt;p&gt;GGUFs Thanks to the incredible Bartowski!!! &lt;a href="https://huggingface.co/bartowski/Severian_Glyphstral-24b-v1-GGUF"&gt;https://huggingface.co/bartowski/Severian_Glyphstral-24b-v1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note on the GGUFs: I am getting weird outputs as well. I noticed that GGUF Is labeled as a Llama arch and 13B. Might be a weird conversion that is causing the bad outputs. I'll keep looking into it, sorry for any wasted downloads. If you can, try the MLX&lt;/p&gt; &lt;p&gt;HuggingChat Assistant Version Available too for those who want to try this concept out right away (NOT THE FINE_TUNED VERSION: Uses pure in-context learning through a very detailed and long prompt). Base model is Qwen coder 32B (has the best execution of the symbolic AI over the reasoning models):&lt;/p&gt; &lt;p&gt;&lt;a href="https://hf.co/chat/assistant/678cfe9655026c306f0a4dab"&gt;https://hf.co/chat/assistant/678cfe9655026c306f0a4dab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vesudeva"&gt; /u/vesudeva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im561m/gylphstral24b_v1_released_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im561m/gylphstral24b_v1_released_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im561m/gylphstral24b_v1_released_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T12:38:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1im35yl</id>
    <title>How to scale RAG to 20 million documents ?</title>
    <updated>2025-02-10T10:33:39+00:00</updated>
    <author>
      <name>/u/Sarcinismo</name>
      <uri>https://old.reddit.com/user/Sarcinismo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;Curious to hear if you worked on RAG use cases with 20+ million documents and how you handled such scale from latency, embedding and indexing perspectives.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sarcinismo"&gt; /u/Sarcinismo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im35yl/how_to_scale_rag_to_20_million_documents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im35yl/how_to_scale_rag_to_20_million_documents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im35yl/how_to_scale_rag_to_20_million_documents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T10:33:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1imi9zz</id>
    <title>Altman Says ‚ÄòNo Thank You‚Äô to Reported Musk Bid for OpenAI</title>
    <updated>2025-02-10T21:54:45+00:00</updated>
    <author>
      <name>/u/Calcidiol</name>
      <uri>https://old.reddit.com/user/Calcidiol</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imi9zz/altman_says_no_thank_you_to_reported_musk_bid_for/"&gt; &lt;img alt="Altman Says ‚ÄòNo Thank You‚Äô to Reported Musk Bid for OpenAI" src="https://external-preview.redd.it/3CzmElhUD3LvBmfuBJUXyad1ZSTywTDZ8sIcpcRW6Zg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac8f2be0d3710f8029992a476ab3c9f0e606e4f6" title="Altman Says ‚ÄòNo Thank You‚Äô to Reported Musk Bid for OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Calcidiol"&gt; /u/Calcidiol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-02-10/musk-led-group-bids-97-4-billion-for-openai-control-wsj-says"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imi9zz/altman_says_no_thank_you_to_reported_musk_bid_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imi9zz/altman_says_no_thank_you_to_reported_musk_bid_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T21:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1im141p</id>
    <title>Orange Pi AI Studio Pro mini PC with 408GB/s bandwidth</title>
    <updated>2025-02-10T08:00:39+00:00</updated>
    <author>
      <name>/u/michaeljchou</name>
      <uri>https://old.reddit.com/user/michaeljchou</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im141p/orange_pi_ai_studio_pro_mini_pc_with_408gbs/"&gt; &lt;img alt="Orange Pi AI Studio Pro mini PC with 408GB/s bandwidth" src="https://b.thumbs.redditmedia.com/4-1ai0qkO5Xa7dtRGbvrlqdk94A166Fd8-qidSmu1eY.jpg" title="Orange Pi AI Studio Pro mini PC with 408GB/s bandwidth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/michaeljchou"&gt; /u/michaeljchou &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1im141p"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im141p/orange_pi_ai_studio_pro_mini_pc_with_408gbs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im141p/orange_pi_ai_studio_pro_mini_pc_with_408gbs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T08:00:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilzcwm</id>
    <title>671B DeepSeek-R1/V3-q4 on a Single Machine (2√ó Xeon + 24GB GPU) ‚Äì Up to 286 tokens/s Prefill &amp; 14 tokens/s Decode</title>
    <updated>2025-02-10T05:58:47+00:00</updated>
    <author>
      <name>/u/CombinationNo780</name>
      <uri>https://old.reddit.com/user/CombinationNo780</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilzcwm/671b_deepseekr1v3q4_on_a_single_machine_2_xeon/"&gt; &lt;img alt="671B DeepSeek-R1/V3-q4 on a Single Machine (2√ó Xeon + 24GB GPU) ‚Äì Up to 286 tokens/s Prefill &amp;amp; 14 tokens/s Decode" src="https://external-preview.redd.it/HTxOk-pm59cMNSNVg9McK5hbABS7kz3K65hC8Z_V08I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f46113aaee323590b36c46395dcf62eb9140f297" title="671B DeepSeek-R1/V3-q4 on a Single Machine (2√ó Xeon + 24GB GPU) ‚Äì Up to 286 tokens/s Prefill &amp;amp; 14 tokens/s Decode" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, we're the KTransformers team (formerly known for &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1edbue3/local_deepseekv2_inference_120_ts_for_prefill_and/"&gt;our local CPU/GPU hybrid inference open source project with DeepSeek-V2&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;We've heard your requests for DeepSeek-R1/V3 support‚Äîand we're excited to finally deliver!&lt;/p&gt; &lt;p&gt;Apologies for the wait, but we've been cooking up something truly amazing.&lt;/p&gt; &lt;p&gt;Today, we're proud to announce that we not only support DeepSeek-R1/V3, as showcased in the video at &lt;a href="https://github.com/kvcache-ai/ktransformers"&gt;https://github.com/kvcache-ai/ktransformers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o0x777ie49ie1.jpg?width=2560&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=23cd7dad22d211277f1787bd1f993c7c22200401"&gt;https://preview.redd.it/o0x777ie49ie1.jpg?width=2560&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=23cd7dad22d211277f1787bd1f993c7c22200401&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But we're also previewing our upcoming optimizations, including an Intel AMX-accelerated kernel and a selective expert activation method, which will significantly enhance performance.&lt;/p&gt; &lt;p&gt;With v0.3-preview, &lt;strong&gt;we achieve up to 286 tokens/s for prefill, making it up to 28√ó faster than llama.cpp&lt;/strong&gt; for local inference.&lt;/p&gt; &lt;p&gt;The binary distribution is available now and the source code will come ASAP! Check out the details here: &lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md"&gt;https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some rationale behind this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Why CPU/GPU Hybrid Inference?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;DeepSeek's MLA operators are highly computationally intensive. While running everything on CPU is possible, offloading the heavy computations to the GPU results in a massive performance boost.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Where Does the Speedup Come From?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- Expert Offload: Unlike traditional layer-based or KVCache offloading (as seen in llama.cpp), we offload the expert computation to the CPU and MLA/KVCache to GPU, aligning perfectly with DeepSeek‚Äôs architecture for optimal efficiency.&lt;/p&gt; &lt;p&gt;- Intel AMX Optimization ‚Äì Our AMX-accelerated kernel is meticulously tuned, running several times faster than existing llama.cpp implementations. We plan to open-source this kernel after cleansing and are considering upstream contributions to llama.cpp.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Why Intel CPUs?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Intel is currently the only CPU vendor that supports AMX-like instructions, which delivers significantly better performance compared to AVX-only alternatives. BUT, we also support AMD CPUs and due to the Expert Offload it will also be faster than the current llama.cpp&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombinationNo780"&gt; /u/CombinationNo780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilzcwm/671b_deepseekr1v3q4_on_a_single_machine_2_xeon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilzcwm/671b_deepseekr1v3q4_on_a_single_machine_2_xeon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilzcwm/671b_deepseekr1v3q4_on_a_single_machine_2_xeon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T05:58:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1imevcc</id>
    <title>Zonos: Incredible new TTS model from Zyphra</title>
    <updated>2025-02-10T19:36:34+00:00</updated>
    <author>
      <name>/u/DisjointedHuntsville</name>
      <uri>https://old.reddit.com/user/DisjointedHuntsville</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imevcc/zonos_incredible_new_tts_model_from_zyphra/"&gt; &lt;img alt="Zonos: Incredible new TTS model from Zyphra" src="https://external-preview.redd.it/fT6Qh89XjjsFyuhQEgqnuEl6NQmutLjYQfjnFpXKnYk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b9925bc6f78bacf8995aece90f55807fa31091f" title="Zonos: Incredible new TTS model from Zyphra" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DisjointedHuntsville"&gt; /u/DisjointedHuntsville &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/ZyphraAI/status/1888996367923888341"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imevcc/zonos_incredible_new_tts_model_from_zyphra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imevcc/zonos_incredible_new_tts_model_from_zyphra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T19:36:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1imdvpz</id>
    <title>DeepSeek R1 outperforms o3-mini (medium) on the Confabulations (Hallucinations) Benchmark</title>
    <updated>2025-02-10T18:58:01+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imdvpz/deepseek_r1_outperforms_o3mini_medium_on_the/"&gt; &lt;img alt="DeepSeek R1 outperforms o3-mini (medium) on the Confabulations (Hallucinations) Benchmark" src="https://preview.redd.it/yz8n6c9nycie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dff4d66879feb128333d600069532abba98cfb81" title="DeepSeek R1 outperforms o3-mini (medium) on the Confabulations (Hallucinations) Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yz8n6c9nycie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imdvpz/deepseek_r1_outperforms_o3mini_medium_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imdvpz/deepseek_r1_outperforms_o3mini_medium_on_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T18:58:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1imf1s0</id>
    <title>First large scale open source math reasoning dataset with 800k R1 reasoning traces</title>
    <updated>2025-02-10T19:43:47+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imf1s0/first_large_scale_open_source_math_reasoning/"&gt; &lt;img alt="First large scale open source math reasoning dataset with 800k R1 reasoning traces" src="https://preview.redd.it/9hlvxhgp7die1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e7c1d035bde22a44686ad67c741cb943197747a" title="First large scale open source math reasoning dataset with 800k R1 reasoning traces" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9hlvxhgp7die1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imf1s0/first_large_scale_open_source_math_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imf1s0/first_large_scale_open_source_math_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T19:43:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1imdnap</id>
    <title>Zonos-v0.1 beta by Zyphra, featuring two expressive and real-time text-to-speech (TTS) models with high-fidelity voice cloning. 1.6B transformer and 1.6B hybrid under an Apache 2.0 license.</title>
    <updated>2025-02-10T18:48:40+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;&amp;quot;Today, we're excited to announce a beta release of Zonos, a highly expressive TTS model with high fidelity voice cloning.&lt;/p&gt; &lt;p&gt;We release both transformer and SSM-hybrid models under an Apache 2.0 license.&lt;/p&gt; &lt;p&gt;Zonos performs well vs leading TTS providers in quality and expressiveness.&lt;/p&gt; &lt;p&gt;Zonos offers flexible control of vocal speed, emotion, tone, and audio quality as well as instant unlimited high quality voice cloning. Zonos natively generates speech at 44Khz. Our hybrid is the first open-source SSM hybrid audio model.&lt;/p&gt; &lt;p&gt;Tech report to be released soon.&lt;/p&gt; &lt;p&gt;Currently Zonos is a beta preview. While highly expressive, Zonos is sometimes unreliable in generations leading to interesting bloopers.&lt;/p&gt; &lt;p&gt;We are excited to continue pushing the frontiers of conversational agent performance, reliability, and efficiency over the coming months.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Details (+model comparisons with proprietary &amp;amp; OS SOTAs)&lt;/strong&gt;: &lt;a href="https://www.zyphra.com/post/beta-release-of-zonos-v0-1"&gt;https://www.zyphra.com/post/beta-release-of-zonos-v0-1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get the weights on Huggingface&lt;/strong&gt;: &lt;a href="http://huggingface.co/Zyphra/Zonos-v0.1-hybrid"&gt;http://huggingface.co/Zyphra/Zonos-v0.1-hybrid&lt;/a&gt; and &lt;a href="http://huggingface.co/Zyphra/Zonos-v0.1-transformer"&gt;http://huggingface.co/Zyphra/Zonos-v0.1-transformer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Download the inference code: &lt;a href="http://github.com/Zyphra/Zonos"&gt;http://github.com/Zyphra/Zonos&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imdnap/zonosv01_beta_by_zyphra_featuring_two_expressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imdnap/zonosv01_beta_by_zyphra_featuring_two_expressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imdnap/zonosv01_beta_by_zyphra_featuring_two_expressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T18:48:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1imca0s</id>
    <title>New paper gives models a chance to think in latent space before outputting tokens, weights are already on HF - Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach</title>
    <updated>2025-02-10T17:54:21+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.05171"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imca0s/new_paper_gives_models_a_chance_to_think_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imca0s/new_paper_gives_models_a_chance_to_think_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T17:54:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1im7un9</id>
    <title>Hugging Face AI Agents course is LIVE!</title>
    <updated>2025-02-10T14:52:22+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im7un9/hugging_face_ai_agents_course_is_live/"&gt; &lt;img alt="Hugging Face AI Agents course is LIVE!" src="https://preview.redd.it/da1ni91yrbie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c1b77fb25fdd40c81ba7533b5a8088b1d90ddaa" title="Hugging Face AI Agents course is LIVE!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/da1ni91yrbie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im7un9/hugging_face_ai_agents_course_is_live/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im7un9/hugging_face_ai_agents_course_is_live/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T14:52:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1im4wxs</id>
    <title>They got the scent now..</title>
    <updated>2025-02-10T12:23:58+00:00</updated>
    <author>
      <name>/u/linkcharger</name>
      <uri>https://old.reddit.com/user/linkcharger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im4wxs/they_got_the_scent_now/"&gt; &lt;img alt="They got the scent now.." src="https://preview.redd.it/euoub08i1bie1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4309ce55a1a79e872658fda4004c32f51846e045" title="They got the scent now.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/linkcharger"&gt; /u/linkcharger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/euoub08i1bie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im4wxs/they_got_the_scent_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im4wxs/they_got_the_scent_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T12:23:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1imenfa</id>
    <title>fair use vs stealing data</title>
    <updated>2025-02-10T19:28:00+00:00</updated>
    <author>
      <name>/u/boxingdog</name>
      <uri>https://old.reddit.com/user/boxingdog</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imenfa/fair_use_vs_stealing_data/"&gt; &lt;img alt="fair use vs stealing data" src="https://preview.redd.it/3bnanf625die1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=491c94da04a0556ad84f59944bf9958350b5f675" title="fair use vs stealing data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boxingdog"&gt; /u/boxingdog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3bnanf625die1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imenfa/fair_use_vs_stealing_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imenfa/fair_use_vs_stealing_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T19:28:00+00:00</published>
  </entry>
</feed>
