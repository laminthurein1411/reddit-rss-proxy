<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-25T16:25:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ixcygz</id>
    <title>Qwq max preview released</title>
    <updated>2025-02-24T21:04:42+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1894130603513319842"&gt;https://x.com/Alibaba_Qwen/status/1894130603513319842&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixcygz/qwq_max_preview_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixcygz/qwq_max_preview_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixcygz/qwq_max_preview_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T21:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixxnag</id>
    <title>Agent browser use COURSE with smolagents on Hugging Face!</title>
    <updated>2025-02-25T15:29:26+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The hugging face agent course is getting real! This unit cover smolagents and everything from retrieval to browser use. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/agents-course"&gt;https://huggingface.co/agents-course&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This week we are releasing the first framework unit in the course and it‚Äôs on smolagents. This is what the unit covers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;why should you use smolagents vs another library?&lt;/li&gt; &lt;li&gt;how to build agents that use code&lt;/li&gt; &lt;li&gt;build multiagents systems&lt;/li&gt; &lt;li&gt;use vision language models for browser use&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixxnag/agent_browser_use_course_with_smolagents_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixxnag/agent_browser_use_course_with_smolagents_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixxnag/agent_browser_use_course_with_smolagents_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T15:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixw1uc</id>
    <title>Data extraction using local LLMs, German, models and settings?</title>
    <updated>2025-02-25T14:19:19+00:00</updated>
    <author>
      <name>/u/roverhendrix123</name>
      <uri>https://old.reddit.com/user/roverhendrix123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Reddit,&lt;/p&gt; &lt;p&gt;I‚Äôm working on a science project that involves extracting information about gene mutations from text snippets. These snippets are pulled from lab results via a keyword search (like a basic RAG approach). The texts are unstructured, and sometimes they indicate whether a mutation is present or not.&lt;/p&gt; &lt;p&gt;For example, some snippets might say:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄúTP53 Mutation p.ARG 12 VAF 14‚Äù&lt;/li&gt; &lt;li&gt;‚ÄúWe could detect the tp.53 mutation‚Äù&lt;/li&gt; &lt;li&gt;Or something like ‚Äú|TP53| was in our gene panel,‚Äù indicating that TP53 was not detected.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I developed an LLM pipeline to process these snippets. It sends each snippet to several smaller LLMs (hosted on 16 GB of VRAM) to determine if there is a mutation, then outputs a JSON like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{&amp;quot;Gen&amp;quot;: &amp;quot;TP53&amp;quot;, &amp;quot;mutation&amp;quot;: 1} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I have a lot of snippets‚Äîover 6,000 in my test run‚Äîand I need high specificity and high sensitivity. Right now, I prompt three different LLMs, and if two of them detect a mutation, I count it as a mutation. However, sensitivity is off: in about 30 cases, only one model (out of three) correctly detected an actual mutation. Also, occasionally, there‚Äôs a burst of hallucinations where a model outputs gibberish (but rarely).&lt;/p&gt; &lt;p&gt;I‚Äôm considering using five models and taking a 3-out-of-5 vote. I‚Äôm using the same temperature (0.15), top_p (0.95), and top_k (10) for all models. To make things more challenging, the text is in German.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My question:&lt;/strong&gt; Which models would be good for this task? (need to fit on 16 gig VRAM and be reasnably fast, right now the 3 models take around 4 hourse in total)&lt;br /&gt; Currently, I‚Äôm using:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Lamarck-14B-v0.7-Q6_K&lt;/li&gt; &lt;li&gt;Mistral-Small-24B-Instruct-2501-IQ4_XS&lt;/li&gt; &lt;li&gt;Qwen2.5-32B-Instruct-IQ3_XS&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I tried Llama 8B, but its performance wasn‚Äôt great for this task. &lt;/p&gt; &lt;p&gt;Should i adapt the temps and setting more?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/roverhendrix123"&gt; /u/roverhendrix123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixw1uc/data_extraction_using_local_llms_german_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixw1uc/data_extraction_using_local_llms_german_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixw1uc/data_extraction_using_local_llms_german_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T14:19:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixckba</id>
    <title>Making older LLMs (Llama 2 and Gemma 1) reason</title>
    <updated>2025-02-24T20:48:42+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixckba/making_older_llms_llama_2_and_gemma_1_reason/"&gt; &lt;img alt="Making older LLMs (Llama 2 and Gemma 1) reason" src="https://external-preview.redd.it/Y21xb3pldThnNWxlMe68FKKrQSi1VIWXGB4I0FX2lDdJRybemxt5jwSyAisL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2fc17b1b05cbeed10c64cb4b0ff38aed9587d31e" title="Making older LLMs (Llama 2 and Gemma 1) reason" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/frk5teu8g5le1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixckba/making_older_llms_llama_2_and_gemma_1_reason/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixckba/making_older_llms_llama_2_and_gemma_1_reason/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T20:48:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixwb1p</id>
    <title>Simple text conversation AI on a Raspberry PI</title>
    <updated>2025-02-25T14:31:03+00:00</updated>
    <author>
      <name>/u/malaksyan64</name>
      <uri>https://old.reddit.com/user/malaksyan64</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;Me and a couple of friends from my university want to create a joke machine as a fun project. The idea will be that the user asks questions to the ai like a magic 8ball toy and the ai answers in a funny way that is relevant to the context of the question. For example if the user says Hi or What's up the AI shouldn't answer something totally irrelevant. The questions will be small and simple and so will be the answers. The hardware is a bit limited, a Raspberry PI 3B+ with 1GB of RAM, no Internet access and a fast 128GB SD Card. I've already built the hardware (a booth with screen and keyboard that houses the PI) and the software (Chat frontend in a Wayland Cage) but I have no idea when it comes to AI. Which AI do I choose for this very low RAM, how do I train it to understand and write in Greek text, how do I train it to Greek humour and memes?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/malaksyan64"&gt; /u/malaksyan64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixwb1p/simple_text_conversation_ai_on_a_raspberry_pi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixwb1p/simple_text_conversation_ai_on_a_raspberry_pi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixwb1p/simple_text_conversation_ai_on_a_raspberry_pi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T14:31:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixuk6h</id>
    <title>I'm looking for resources to go from zero to hero for understanding LLM, transformers.</title>
    <updated>2025-02-25T13:07:56+00:00</updated>
    <author>
      <name>/u/Erdeem</name>
      <uri>https://old.reddit.com/user/Erdeem</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can you recommend some online courses or resources for leaning about LLMs, transformers, etc. I'd like to not only be able to keep up in a conversation about technical side of things, but develop enough knowledge to also contribute to projects on GitHub. &lt;/p&gt; &lt;p&gt;I know things are developing quickly and there are new acronyms for new tech being made every day, but I'd like to at least get the foundation down then move forward from there. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Erdeem"&gt; /u/Erdeem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixuk6h/im_looking_for_resources_to_go_from_zero_to_hero/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixuk6h/im_looking_for_resources_to_go_from_zero_to_hero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixuk6h/im_looking_for_resources_to_go_from_zero_to_hero/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T13:07:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixfbzd</id>
    <title>Sonnet-3.7 is best non-thinking model in the Misguided Attention eval.</title>
    <updated>2025-02-24T22:41:46+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixfbzd/sonnet37_is_best_nonthinking_model_in_the/"&gt; &lt;img alt="Sonnet-3.7 is best non-thinking model in the Misguided Attention eval." src="https://external-preview.redd.it/3Xlrhru-DocPv1ONkF-Le04N8KrkOyM1Ydkeb2ft68s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8afcf14938726714cc9d549d6ef3ea05fd4f2b3c" title="Sonnet-3.7 is best non-thinking model in the Misguided Attention eval." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/cpldcpu/MisguidedAttention"&gt;Misguided Attention&lt;/a&gt; is a collection of prompts to challenge the reasoning abilities of large language models in presence of misguiding information. It consists of slightly modified well known logical problems and riddles. Many model are overfit to these problems and will therefore report a response to the unmodified problem. &lt;/p&gt; &lt;p&gt;Claude-3.7-Sonnet was evaluated in the non-thinking mode in the long eval with 52 prompt. It almost beats o3-mini despite not using the thinking mode. This is a very impressive result. &lt;/p&gt; &lt;p&gt;I will benchmark the thinking mode once I have figured out how to activate it in the openrouter API...&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sui6i1l4z5le1.png?width=2391&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2d11d6a08386a45914660a0f576b2c4c58ae88d4"&gt;https://preview.redd.it/sui6i1l4z5le1.png?width=2391&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2d11d6a08386a45914660a0f576b2c4c58ae88d4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/e1p7r416z5le1.png?width=4170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a2cd9467c077b18212e70943d731009ff62430a6"&gt;https://preview.redd.it/e1p7r416z5le1.png?width=4170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a2cd9467c077b18212e70943d731009ff62430a6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixfbzd/sonnet37_is_best_nonthinking_model_in_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixfbzd/sonnet37_is_best_nonthinking_model_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixfbzd/sonnet37_is_best_nonthinking_model_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T22:41:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixe6yo</id>
    <title>Great announcement today. Heres how we already made it better months ago</title>
    <updated>2025-02-24T21:55:08+00:00</updated>
    <author>
      <name>/u/bmlattimer</name>
      <uri>https://old.reddit.com/user/bmlattimer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixe6yo/great_announcement_today_heres_how_we_already/"&gt; &lt;img alt="Great announcement today. Heres how we already made it better months ago" src="https://external-preview.redd.it/grYs-3O6uZipD0Sj50ba5RJGLP9auRDlnYN5RIEw2ug.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=908d238bcf53c77c3b923be9dffa7d805c8338db" title="Great announcement today. Heres how we already made it better months ago" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;JOSH: Self-Improving LLMs for Tool Use Without Human Feedback&lt;/h1&gt; &lt;p&gt;Our team released a paper a few months ago introducing JOSH (Juxtaposed Outcomes for Simulation Harvesting), a self-alignment algorithm that enables LLMs to autonomously improve their tool-using capabilities without human feedback including notably on œÑ-bench. We also have introduced an agentic tool calling dataset ToolWOZ derived from MultiWOZ. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rzfdhfkkq5le1.png?width=1906&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35804ee77ec38267881cc116304f953b5f350341"&gt;JOSH uses methods similar to Test Time Scaling to generate training data&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What JOSH does:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Uses tool calls as sparse rewards in a simulation environment to extract ideal dialogue turns&lt;/li&gt; &lt;li&gt;Trains models on their own outputs through beam search exploration (reminiscent of test time scaling methods that are currently used)&lt;/li&gt; &lt;li&gt;Significantly improves tool-based interactions across model sizes (from smaller Llama models to frontier models like GPT-4o)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key results:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;74% improvement in success rate for Llama3-8B on our ToolWOZ benchmark&lt;/li&gt; &lt;li&gt;State-of-the-art performance on œÑ-bench when applied to GPT-4o&lt;/li&gt; &lt;li&gt;Maintains general model capabilities on MT-Bench and LMSYS while specializing in tool use&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why this matters:&lt;/h1&gt; &lt;p&gt;With today's Anthropic announcement showing improvements on œÑ-bench, it's worth noting how our approach can already be applied to improve its capabilities! JOSH offers a general approach that works across model sizes and doesn't require human feedback - potentially making it more scalable as models continue to improve.&lt;/p&gt; &lt;p&gt;We've made our code and the ToolWOZ dataset publicly available: &lt;a href="https://github.com/asappresearch/josh-llm-simulation-training"&gt;GitHub repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/pdf/2409.04617"&gt;Sparse Rewards Can Self-Train Dialogue Agents&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious to hear the community's thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bmlattimer"&gt; /u/bmlattimer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixe6yo/great_announcement_today_heres_how_we_already/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixe6yo/great_announcement_today_heres_how_we_already/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixe6yo/great_announcement_today_heres_how_we_already/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T21:55:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixamd9</id>
    <title>QwQ-Max-Preview soon</title>
    <updated>2025-02-24T19:30:38+00:00</updated>
    <author>
      <name>/u/pkmxtw</name>
      <uri>https://old.reddit.com/user/pkmxtw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found that they have been updating their website on another branch:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/qwenlm.github.io/commit/5d009b319931d473211cb4225d726b322afbb734"&gt;https://github.com/QwenLM/qwenlm.github.io/commit/5d009b319931d473211cb4225d726b322afbb734&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;: Apache 2.0 licensed QwQ-Max, Qwen2.5-Max, QwQ-32B and probably other smaller QwQ variants, and an app for qwen chat.&lt;/p&gt; &lt;hr /&gt; &lt;blockquote&gt; &lt;p&gt;We‚Äôre happy to unveil QwQ-Max-Preview , the latest advancement in the Qwen series, designed to push the boundaries of deep reasoning and versatile problem-solving. Built on the robust foundation of Qwen2.5-Max , this preview model excels in mathematics, coding, and general-domain tasks, while delivering outstanding performance in Agent-related workflows. As a sneak peek into our upcoming QwQ-Max release, this version offers a glimpse of its enhanced capabilities, with ongoing refinements and an official Apache 2.0-licensed open-source launch of QwQ-Max and Qwen2.5-Max planned soon. Stay tuned for a new era of intelligent reasoning.&lt;/p&gt; &lt;p&gt;As we prepare for the official open-source release of QwQ-Max under the Apache 2.0 License, our roadmap extends beyond sharing cutting-edge research. We are committed to democratizing access to advanced reasoning capabilities and fostering innovation across diverse applications. Here‚Äôs what‚Äôs next:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;APP Release&lt;/strong&gt; To bridge the gap between powerful AI and everyday users, we will launch a dedicated APP for Qwen Chat. This intuitive interface will enable seamless interaction with the model for tasks like problem-solving, code generation, and logical reasoning‚Äîno technical expertise required. The app will prioritize real-time responsiveness and integration with popular productivity tools, making advanced AI accessible to a global audience.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Open-Sourcing Smaller Reasoning Models&lt;/strong&gt; Recognizing the need for lightweight, resource-efficient solutions, we will release a series of smaller QwQ variants , such as QwQ-32B, for local device deployment. These models will retain robust reasoning capabilities while minimizing computational demands, allowing developers to integrate them into devices. Perfect for privacy-sensitive applications or low-latency workflows, they will empower creators to build custom AI solutions.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Community-Driven Innovation&lt;/strong&gt; By open-sourcing QwQ-Max, Qwen2.5-Max, and its smaller counterparts, we aim to spark collaboration among developers, researchers, and hobbyists. We invite the community to experiment, fine-tune, and extend these models for specialized use cases‚Äîfrom education tools to autonomous agents. Our goal is to cultivate an ecosystem where innovation thrives through shared knowledge and collective problem-solving.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Stay tuned as we roll out these initiatives, designed to empower users at every level and redefine the boundaries of what AI can achieve. Together, we‚Äôre building a future where intelligence is not just powerful, but universally accessible.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pkmxtw"&gt; /u/pkmxtw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixamd9/qwqmaxpreview_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixamd9/qwqmaxpreview_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixamd9/qwqmaxpreview_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T19:30:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixyfwk</id>
    <title>Look out for the Xeon 6 6521P... 24 cores, 136 PCIe 5.0 lanes for $1250</title>
    <updated>2025-02-25T16:03:25+00:00</updated>
    <author>
      <name>/u/Relevant-Audience441</name>
      <uri>https://old.reddit.com/user/Relevant-Audience441</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Might be the best next platform for local AI builds. (And I say this as an AMD investor).&lt;br /&gt; Intel truly found the gap between Sienna and the other larger Epyc offerings.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.intel.com/content/www/us/en/products/sku/242634/intel-xeon-6521p-processor-144m-cache-2-60-ghz/specifications.html"&gt;https://www.intel.com/content/www/us/en/products/sku/242634/intel-xeon-6521p-processor-144m-cache-2-60-ghz/specifications.html&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Relevant-Audience441"&gt; /u/Relevant-Audience441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixyfwk/look_out_for_the_xeon_6_6521p_24_cores_136_pcie/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixyfwk/look_out_for_the_xeon_6_6521p_24_cores_136_pcie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixyfwk/look_out_for_the_xeon_6_6521p_24_cores_136_pcie/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T16:03:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixvlop</id>
    <title>Do you think that Mistral worked to develop Saba due to fewer AI ACT restrictions and regulatory pressures? How does this apply emergent efforts in the EU?</title>
    <updated>2025-02-25T13:58:30+00:00</updated>
    <author>
      <name>/u/RMCPhoto</name>
      <uri>https://old.reddit.com/user/RMCPhoto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral AI‚Äôs recent release of &lt;strong&gt;Mistral Saba&lt;/strong&gt;‚Äîa 24B-parameter model specialized in Middle Eastern and South Asian languages.&lt;/p&gt; &lt;p&gt;Saba‚Äôs launch (&lt;a href="https://mistral.ai/news/mistral-saba"&gt;official announcement&lt;/a&gt;) follows years of vocal criticism from Mistral about the EU AI Act‚Äôs potential to stifle innovation. C√©dric O, Mistral co-founder, warned that the EU AI Act could ‚Äúkill‚Äù European startups by imposing burdensome compliance requirements on foundation models. The Act‚Äôs strictest rules target models trained with &amp;gt;10¬≤‚Åµ FLOPs (e.g., GPT-4), but smaller models like Saba (24B params) fall under lighter transparency obligations and new oversight regarding copywritten material.&lt;/p&gt; &lt;p&gt;Saba can be deployed on-premises, potentially sidestepping EU data governance rules. &lt;/p&gt; &lt;p&gt;Independent evaluations (e.g., COMPL-AI) found Mistral‚Äôs earlier models non-compliant with EU AI Act cybersecurity and fairness standards.&lt;/p&gt; &lt;p&gt;By focusing on non-EU markets and training data, could Mistral avoid similar scrutiny for Saba?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RMCPhoto"&gt; /u/RMCPhoto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixvlop/do_you_think_that_mistral_worked_to_develop_saba/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixvlop/do_you_think_that_mistral_worked_to_develop_saba/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixvlop/do_you_think_that_mistral_worked_to_develop_saba/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T13:58:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixy5kf</id>
    <title>If you are using Linux, an AMD iGPU for running LLMs (Vulkan), and the amdgpu driver, you may want to check your GTT size</title>
    <updated>2025-02-25T15:51:25+00:00</updated>
    <author>
      <name>/u/toazd</name>
      <uri>https://old.reddit.com/user/toazd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran into a &amp;quot;problem&amp;quot; when I couldn't load Qwen2.5-7b-instruct-Q4_K_M with a context size of 32768 (using llama-cli Vulkan, insufficient memory error). Normally, you might think &amp;quot;Oh I just need different hardware for this task&amp;quot; but AMD iGPUs use system RAM for their memory and I have 16GB of that which is plenty to run that model at that context size. So, how can we &amp;quot;fix&amp;quot; this, I wondered.&lt;/p&gt; &lt;p&gt;By running &lt;code&gt;amdgpu_top&lt;/code&gt; (or &lt;code&gt;radeontop&lt;/code&gt;) you can see in the &amp;quot;Memory usage&amp;quot; section what is allocated VRAM (RAM that is dedicated to the GPU, inaccessible to the CPU/system) and what is allocated as GTT (RAM that the CPU/system can use when the GPU is not using it). It's important to know the difference between those two and when you need more of one or the other. For my use cases which are largely limited to just llama.cpp, minimum VRAM and maximum GTT is best.&lt;/p&gt; &lt;p&gt;On Arch Linux the GTT was set to 8GB by default (of 16GB available). That was my limiting factor until I did a little research. And the result of that is what I wanted to share in case it helps anyone as it did me.&lt;/p&gt; &lt;p&gt;Checking the &lt;a href="https://www.kernel.org/doc/html/v4.20/gpu/amdgpu.html"&gt;kernel docs for amdgpu&lt;/a&gt; shows that the kernel parameter &lt;code&gt;amdgpu.gttsize=X&lt;/code&gt; (where X is the size in &lt;strong&gt;MiB&lt;/strong&gt;) allows one to give the iGPU access to more (or less) system memory. I changed that number, updated grub, and rebooted and now &lt;code&gt;amdgpu_top&lt;/code&gt; shows the new GTT size and now I can load and run larger models and/or larger context sizes no problem!&lt;/p&gt; &lt;p&gt;For reference I am using an &lt;a href="https://www.amd.com/en/products/processors/laptop/ryzen/7000-series/amd-ryzen-7-7730u.html"&gt;AMD Ryzen 7 7730U&lt;/a&gt; (&lt;code&gt;gfx90c&lt;/code&gt;) 16GB RAM, 512MB VRAM, 12GB GTT.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/toazd"&gt; /u/toazd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixy5kf/if_you_are_using_linux_an_amd_igpu_for_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixy5kf/if_you_are_using_linux_an_amd_igpu_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixy5kf/if_you_are_using_linux_an_amd_igpu_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T15:51:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixl2w3</id>
    <title>Looks like Apple is not staying with Local AI in the future - they are committed to spend $500 billion (same as Stargate) on an AI farm in Texas</title>
    <updated>2025-02-25T03:09:29+00:00</updated>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixl2w3/looks_like_apple_is_not_staying_with_local_ai_in/"&gt; &lt;img alt="Looks like Apple is not staying with Local AI in the future - they are committed to spend $500 billion (same as Stargate) on an AI farm in Texas" src="https://external-preview.redd.it/12kykPoZ5ki8pkGVglj5KFbAlqCbKMl1m2OeId0Fze4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1576ef38ecd4b8b553e5065b39ebd06ab643ee8" title="Looks like Apple is not staying with Local AI in the future - they are committed to spend $500 billion (same as Stargate) on an AI farm in Texas" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://appleinsider.com/articles/25/02/24/apple-commits-over-500-billion-in-new-us-investment"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixl2w3/looks_like_apple_is_not_staying_with_local_ai_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixl2w3/looks_like_apple_is_not_staying_with_local_ai_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T03:09:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixczae</id>
    <title>QwQ-Max Preview is here...</title>
    <updated>2025-02-24T21:05:36+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixczae/qwqmax_preview_is_here/"&gt; &lt;img alt="QwQ-Max Preview is here..." src="https://external-preview.redd.it/bQFl5DBj7QNi2--7cYMNDWqUV0PSTT-usX89HeDXsMM.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1084b413b6f8c22df7000f62d2cf3888172ab3eb" title="QwQ-Max Preview is here..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://twitter.com/Alibaba_Qwen/status/1894130603513319842"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixczae/qwqmax_preview_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixczae/qwqmax_preview_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T21:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixkfcb</id>
    <title>DeepSeek 2nd OSS package - DeepEP - Expert parallel FP8 MOE kernels</title>
    <updated>2025-02-25T02:36:58+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixkfcb/deepseek_2nd_oss_package_deepep_expert_parallel/"&gt; &lt;img alt="DeepSeek 2nd OSS package - DeepEP - Expert parallel FP8 MOE kernels" src="https://external-preview.redd.it/xJj085czeH4B1zCQ9-VjIjC71RkrO3L43yqPF31kRkU.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54bdc2d3692103cd459dc9d5b21951fa5457dbea" title="DeepSeek 2nd OSS package - DeepEP - Expert parallel FP8 MOE kernels" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/deepseek_ai/status/1894211757604049133"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixkfcb/deepseek_2nd_oss_package_deepep_expert_parallel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixkfcb/deepseek_2nd_oss_package_deepep_expert_parallel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T02:36:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixefsf</id>
    <title>I created a new structured output method and it works really well</title>
    <updated>2025-02-24T22:04:49+00:00</updated>
    <author>
      <name>/u/jckwind11</name>
      <uri>https://old.reddit.com/user/jckwind11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixefsf/i_created_a_new_structured_output_method_and_it/"&gt; &lt;img alt="I created a new structured output method and it works really well" src="https://preview.redd.it/i55e55gkt5le1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88fc258672a6dae100b76e5c3df682bffb3f9b2a" title="I created a new structured output method and it works really well" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jckwind11"&gt; /u/jckwind11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i55e55gkt5le1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixefsf/i_created_a_new_structured_output_method_and_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixefsf/i_created_a_new_structured_output_method_and_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T22:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixj4bp</id>
    <title>New LiveBench results just released. Sonnet 3.7 reasoning now tops the charts and Sonnet 3.7 is also top non-reasoning model</title>
    <updated>2025-02-25T01:33:13+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixj4bp/new_livebench_results_just_released_sonnet_37/"&gt; &lt;img alt="New LiveBench results just released. Sonnet 3.7 reasoning now tops the charts and Sonnet 3.7 is also top non-reasoning model" src="https://preview.redd.it/ys8y5ndtu6le1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a842efc09f827b67552f624088ddf764f73b1a5" title="New LiveBench results just released. Sonnet 3.7 reasoning now tops the charts and Sonnet 3.7 is also top non-reasoning model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ys8y5ndtu6le1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixj4bp/new_livebench_results_just_released_sonnet_37/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixj4bp/new_livebench_results_just_released_sonnet_37/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T01:33:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixp4du</id>
    <title>QwQ-Max-Preview on LiveCodeBench where it performs on par with o1-medium</title>
    <updated>2025-02-25T07:05:15+00:00</updated>
    <author>
      <name>/u/McSnoo</name>
      <uri>https://old.reddit.com/user/McSnoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixp4du/qwqmaxpreview_on_livecodebench_where_it_performs/"&gt; &lt;img alt="QwQ-Max-Preview on LiveCodeBench where it performs on par with o1-medium" src="https://b.thumbs.redditmedia.com/jOx4TjdDbpMzX5hiEIyODbh67X61EHlfyUSc00xZVxc.jpg" title="QwQ-Max-Preview on LiveCodeBench where it performs on par with o1-medium" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/McSnoo"&gt; /u/McSnoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ixp4du"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixp4du/qwqmaxpreview_on_livecodebench_where_it_performs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixp4du/qwqmaxpreview_on_livecodebench_where_it_performs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T07:05:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixrrqb</id>
    <title>Joined the 48GB Vram Dual Hairdryer club. Frankly a bit of disappointment, deepseek-r1:70b works fine, qwen2.5:72b seems to be too big still. The 32b models apparently provide almost the same code quality and for general questions the online big LLMs are better. Meh.</title>
    <updated>2025-02-25T10:16:35+00:00</updated>
    <author>
      <name>/u/ChopSticksPlease</name>
      <uri>https://old.reddit.com/user/ChopSticksPlease</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixrrqb/joined_the_48gb_vram_dual_hairdryer_club_frankly/"&gt; &lt;img alt="Joined the 48GB Vram Dual Hairdryer club. Frankly a bit of disappointment, deepseek-r1:70b works fine, qwen2.5:72b seems to be too big still. The 32b models apparently provide almost the same code quality and for general questions the online big LLMs are better. Meh." src="https://b.thumbs.redditmedia.com/8WCwxe9biwOo4FpMYhstKuEdg_TspXpq-0hhQ6qPLXs.jpg" title="Joined the 48GB Vram Dual Hairdryer club. Frankly a bit of disappointment, deepseek-r1:70b works fine, qwen2.5:72b seems to be too big still. The 32b models apparently provide almost the same code quality and for general questions the online big LLMs are better. Meh." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChopSticksPlease"&gt; /u/ChopSticksPlease &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ixrrqb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixrrqb/joined_the_48gb_vram_dual_hairdryer_club_frankly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixrrqb/joined_the_48gb_vram_dual_hairdryer_club_frankly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T10:16:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixupja</id>
    <title>Sonnet 3.7 near clean sweep of EQ-Bench benchmarks</title>
    <updated>2025-02-25T13:15:35+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixupja/sonnet_37_near_clean_sweep_of_eqbench_benchmarks/"&gt; &lt;img alt="Sonnet 3.7 near clean sweep of EQ-Bench benchmarks" src="https://a.thumbs.redditmedia.com/6fUZh9Gsn1Cvb5T20dNRVamWAjNPiP5h-5Kb76ovqf0.jpg" title="Sonnet 3.7 near clean sweep of EQ-Bench benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ixupja"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixupja/sonnet_37_near_clean_sweep_of_eqbench_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixupja/sonnet_37_near_clean_sweep_of_eqbench_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T13:15:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixkg22</id>
    <title>DeepSeek Realse 2nd Bomb, DeepEP a communication library tailored for MoE model</title>
    <updated>2025-02-25T02:37:58+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixkg22/deepseek_realse_2nd_bomb_deepep_a_communication/"&gt; &lt;img alt="DeepSeek Realse 2nd Bomb, DeepEP a communication library tailored for MoE model" src="https://b.thumbs.redditmedia.com/6CPaOcTRlcC_uvZT3vs5YwQDjvoF9K20zY4Y3xUx5WE.jpg" title="DeepSeek Realse 2nd Bomb, DeepEP a communication library tailored for MoE model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepEP is a communication library tailored for Mixture-of-Experts (MoE) and expert parallelism (EP). It provides high-throughput and low-latency all-to-all GPU kernels, which are also as known as MoE dispatch and combine. The library also supports low-precision operations, including FP8.&lt;/p&gt; &lt;p&gt;Please note that this library still only supports GPUs with the Hopper architecture (such as H100, H200, H800). Consumer-grade graphics cards are not currently supported&lt;/p&gt; &lt;p&gt;repo: &lt;a href="https://github.com/deepseek-ai/DeepEP"&gt;https://github.com/deepseek-ai/DeepEP&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/orb7lq1m67le1.png?width=881&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c072a8df4ea13dd0baaf6dfdbc33eedfd918398"&gt;https://preview.redd.it/orb7lq1m67le1.png?width=881&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c072a8df4ea13dd0baaf6dfdbc33eedfd918398&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixkg22/deepseek_realse_2nd_bomb_deepep_a_communication/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixkg22/deepseek_realse_2nd_bomb_deepep_a_communication/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixkg22/deepseek_realse_2nd_bomb_deepep_a_communication/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T02:37:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixtug3</id>
    <title>WAN Video model launched</title>
    <updated>2025-02-25T12:29:36+00:00</updated>
    <author>
      <name>/u/BreakIt-Boris</name>
      <uri>https://old.reddit.com/user/BreakIt-Boris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Doesn't seem to be announced yet however the huggingface space is live and model weighs are released!!! Realise this isn't technically LLM however believe possibly of interest to many here.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Wan-AI/Wan2.1-T2V-14B"&gt;https://huggingface.co/Wan-AI/Wan2.1-T2V-14B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakIt-Boris"&gt; /u/BreakIt-Boris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixtug3/wan_video_model_launched/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixtug3/wan_video_model_launched/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixtug3/wan_video_model_launched/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T12:29:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixporw</id>
    <title>Alibaba video model Wan 2.1 will be released Feb 25th,2025 and is open source!</title>
    <updated>2025-02-25T07:46:26+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixporw/alibaba_video_model_wan_21_will_be_released_feb/"&gt; &lt;img alt="Alibaba video model Wan 2.1 will be released Feb 25th,2025 and is open source!" src="https://preview.redd.it/amle9h0op8le1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5133d782a13f9379a9568553fde27608d4fe653" title="Alibaba video model Wan 2.1 will be released Feb 25th,2025 and is open source!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nice to have open source. So excited for this one.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/amle9h0op8le1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixporw/alibaba_video_model_wan_21_will_be_released_feb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixporw/alibaba_video_model_wan_21_will_be_released_feb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T07:46:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixubts</id>
    <title>üá®üá≥ Sources: DeepSeek is speeding up the release of its R2 AI model, which was originally slated for May, but the company is now working to launch it sooner.</title>
    <updated>2025-02-25T12:56:10+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixubts/sources_deepseek_is_speeding_up_the_release_of/"&gt; &lt;img alt="üá®üá≥ Sources: DeepSeek is speeding up the release of its R2 AI model, which was originally slated for May, but the company is now working to launch it sooner." src="https://preview.redd.it/z11vic3x8ale1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ec3818e2a144f0e19522fbf44016cae02b88dc1" title="üá®üá≥ Sources: DeepSeek is speeding up the release of its R2 AI model, which was originally slated for May, but the company is now working to launch it sooner." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z11vic3x8ale1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixubts/sources_deepseek_is_speeding_up_the_release_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixubts/sources_deepseek_is_speeding_up_the_release_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T12:56:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixtxbw</id>
    <title>üòÇüòÇ someone made a "touch grass" app with a vLLM, you gotta go and actually touch grass to unlock your phone</title>
    <updated>2025-02-25T12:33:46+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixtxbw/someone_made_a_touch_grass_app_with_a_vllm_you/"&gt; &lt;img alt="üòÇüòÇ someone made a &amp;quot;touch grass&amp;quot; app with a vLLM, you gotta go and actually touch grass to unlock your phone" src="https://b.thumbs.redditmedia.com/5hq40VPLgBMcOH3vwQ7e1MxMGeAfqIgssUMVtLMafsg.jpg" title="üòÇüòÇ someone made a &amp;quot;touch grass&amp;quot; app with a vLLM, you gotta go and actually touch grass to unlock your phone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ixtxbw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixtxbw/someone_made_a_touch_grass_app_with_a_vllm_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixtxbw/someone_made_a_touch_grass_app_with_a_vllm_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T12:33:46+00:00</published>
  </entry>
</feed>
