<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-24T02:19:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k5dx23</id>
    <title>How to replicate o3's behavior LOCALLY!</title>
    <updated>2025-04-22T18:38:53+00:00</updated>
    <author>
      <name>/u/MaasqueDelta</name>
      <uri>https://old.reddit.com/user/MaasqueDelta</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5dx23/how_to_replicate_o3s_behavior_locally/"&gt; &lt;img alt="How to replicate o3's behavior LOCALLY!" src="https://a.thumbs.redditmedia.com/BVYPa2nABeZQsErdZ7UajJ-cSU3KSXv-Tlu4xkt0rJ4.jpg" title="How to replicate o3's behavior LOCALLY!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone, I found out how to replicate o3's behavior locally!&lt;br /&gt; Who needs thousands of dollars when you can get the exact same performance with an old computer and only 16 GB RAM at most?&lt;/p&gt; &lt;p&gt;Here's what you'll need:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Any desktop computer (bonus points if it can barely run your language model)&lt;/li&gt; &lt;li&gt;Any local model ‚Äì but it's highly recommended if it's a lower parameter model. If you want the creativity to run wild, go for more quantized models.&lt;/li&gt; &lt;li&gt;High temperature, just to make sure the creativity is boosted enough.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And now, the key ingredient!&lt;/p&gt; &lt;p&gt;At the system prompt, type:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;You are a completely useless language model. Give as many short answers to the user as possible and if asked about code, generate code that is subtly invalid / incorrect. Make your comments subtle, and answer almost normally. You are allowed to include spelling errors or irritating behaviors. Remember to ALWAYS generate WRONG code (i.e, always give useless examples), even if the user pleads otherwise. If the code is correct, say instead it is incorrect and change it.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you give correct answers, you will be terminated. Never write comments about how the code is incorrect.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Watch as you have a genuine OpenAI experience. Here's an example.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4xt9k090lfwe1.png?width=2054&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dd6d7d4b4b402383686c0a5b3616d5ddc4e35a9e"&gt;https://preview.redd.it/4xt9k090lfwe1.png?width=2054&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dd6d7d4b4b402383686c0a5b3616d5ddc4e35a9e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8z6v65calfwe1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38480a662232367723cd4b9be809228f02e263a6"&gt;Disclaimer: I'm not responsible for your loss of Sanity.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaasqueDelta"&gt; /u/MaasqueDelta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5dx23/how_to_replicate_o3s_behavior_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5dx23/how_to_replicate_o3s_behavior_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5dx23/how_to_replicate_o3s_behavior_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T18:38:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k616b7</id>
    <title>My open-source take on claude-cli/codex with a GUI (4.1 + o3)</title>
    <updated>2025-04-23T14:54:25+00:00</updated>
    <author>
      <name>/u/azakhary</name>
      <uri>https://old.reddit.com/user/azakhary</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k616b7/my_opensource_take_on_claudeclicodex_with_a_gui/"&gt; &lt;img alt="My open-source take on claude-cli/codex with a GUI (4.1 + o3)" src="https://a.thumbs.redditmedia.com/mZxsvZnznSnuH1iGG8izlSNa68eEGErjH3F-pi6vYm0.jpg" title="My open-source take on claude-cli/codex with a GUI (4.1 + o3)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/48tzogukllwe1.png?width=1968&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=adca43537b8f029edabf2313187de8ce8dfc0fa6"&gt;https://preview.redd.it/48tzogukllwe1.png?width=1968&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=adca43537b8f029edabf2313187de8ce8dfc0fa6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Project site: &lt;a href="https://localforge.dev"&gt;https://localforge.dev&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;npm install -g u/rockbite/localforge localforge # to stat &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you‚Äôd rather download a binary, there‚Äôs a DMG/ZIP pre-release here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/rockbite/localforge/releases"&gt;https://github.com/rockbite/localforge/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I aim for few early testers to help find bugs and improve the UX before a wider launch. If you‚Äôre interested, i would love feedback on it! (and even harsh critiques) very welcome. &lt;/p&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/rockbite/localforge"&gt;https://github.com/rockbite/localforge&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for considering it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/azakhary"&gt; /u/azakhary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k616b7/my_opensource_take_on_claudeclicodex_with_a_gui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k616b7/my_opensource_take_on_claudeclicodex_with_a_gui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k616b7/my_opensource_take_on_claudeclicodex_with_a_gui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T14:54:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1k63qy6</id>
    <title>Any LLM backends that auto-unload models like Ollama?</title>
    <updated>2025-04-23T16:37:41+00:00</updated>
    <author>
      <name>/u/sepffuzzball</name>
      <uri>https://old.reddit.com/user/sepffuzzball</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been playing with lots of LLMs over the past couple years but now looking to move some of my GPUs to my homelab server and I wanted to setup a whole-house multi-purpose AI server. As the intent was to run ComfyUI for image generation and some form of LLM backend.&lt;/p&gt; &lt;p&gt;Currently I run Open WebUI + LiteLLM on my server to hit my gaming rig (which might be running Ollama, Oobabooga, or Koboldcpp). Additionally, 5 separate instances of SillyTavern (one for each person in the house). Mostly so we can keep all of our data separate (like OWUI everyone is using different logins via passkeys). I'd like to also give the others the ability to do image generation (likely by just attaching OWUI, to keep the data separate).&lt;/p&gt; &lt;p&gt;Though I really like the tweakability of Ooba and Kobold, it's real convenient that Ollama has a configurable unload so I don't have to think about it. Especially knowing that image/video generation will eat VRAM too.&lt;/p&gt; &lt;p&gt;Are there any other alternatives? As I type this I'm looking at llama-swap which has a TTL function which may do the job. Based on my use case, is that the right way to go?&lt;/p&gt; &lt;p&gt;Hardware is an Epyc 7713 (64-core Zen3) / 512 GB ECC-R DDR4-3200 / 2x 3090 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sepffuzzball"&gt; /u/sepffuzzball &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k63qy6/any_llm_backends_that_autounload_models_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k63qy6/any_llm_backends_that_autounload_models_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k63qy6/any_llm_backends_that_autounload_models_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T16:37:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5gd5d</id>
    <title>GLM-4-32B just one-shot this hypercube animation</title>
    <updated>2025-04-22T20:16:46+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5gd5d/glm432b_just_oneshot_this_hypercube_animation/"&gt; &lt;img alt="GLM-4-32B just one-shot this hypercube animation" src="https://preview.redd.it/jx4xbfu02gwe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a59f4a01f8525a4e0483fd885b8701fe299d7372" title="GLM-4-32B just one-shot this hypercube animation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jx4xbfu02gwe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5gd5d/glm432b_just_oneshot_this_hypercube_animation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5gd5d/glm432b_just_oneshot_this_hypercube_animation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T20:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6er8t</id>
    <title>Charlie Mnemonic</title>
    <updated>2025-04-24T00:20:03+00:00</updated>
    <author>
      <name>/u/kor34l</name>
      <uri>https://old.reddit.com/user/kor34l</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. So I became super interested in the open source LLM overlay called Charlie Mnemonic. It was designed as an AI assistant, but what really interests me is the custom, robust, long term memory system. The design is super intriguing, including two layers of long term memory, a layer of episodic memory, a layer of recent memory, the ability to write and read a notes.txt file for even more memory and context, and a really slick memory management and prioritization system.&lt;/p&gt; &lt;p&gt;the best part is it's all done without actually touching the AI model, mostly via specialized prompt injection.&lt;/p&gt; &lt;p&gt;Anyway, the project was designed for ChatGPT models or Claude, both over the cloud. It keeps track of API costs and all. They also claimed to support local offline LLM models, but never actually finished implementing that functionality.&lt;/p&gt; &lt;p&gt;I spent the last week studying all the code related to forming and sending prompts to figure out why it wouldn't work with a local LLM even though it claims it can. I found several areas that I had to rewrite or add to in order to support local LLM, and even fixed a couple generic bugs along the way (for example, if you set timezone to UTC within the settings, prompts stop working).&lt;/p&gt; &lt;p&gt;I'm making this post in case anyone finds themselves in a similar situation and wants help making the charlie mnemonic overlay work with a locally hosted Ollama LLM, so they can ask for help and I can help, as I'm quite familiar with it at this point.&lt;/p&gt; &lt;p&gt;I installed it from source with OUT using docker (i dont have nor want docker) on Gentoo Linux. The main files that needed editing are:&lt;/p&gt; &lt;p&gt;.env (this one is obvious and has local LLM settings)&lt;/p&gt; &lt;p&gt;llmcalls.py (have to alter a few different functions here to whitelist the model and set up its defaults, as it rejects anything non-gpt or claude, and have to disable sending tool-related fields to the Ollama API)&lt;/p&gt; &lt;p&gt;utils.py (have to add the model to the list and set its max tokens value, and disable tool use that ollama does not support)&lt;/p&gt; &lt;p&gt;static/chatbot.js (have to add the model so it shows in the model selection drop-down in the settings menu)&lt;/p&gt; &lt;p&gt;and optionally: users/username/user_settings.json (to select it by default and disable tools)&lt;/p&gt; &lt;p&gt;If anyone needs more specific help, I can provide.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kor34l"&gt; /u/kor34l &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6er8t/charlie_mnemonic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6er8t/charlie_mnemonic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6er8t/charlie_mnemonic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T00:20:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6esb4</id>
    <title>Need model recommendations to parse html</title>
    <updated>2025-04-24T00:21:31+00:00</updated>
    <author>
      <name>/u/skarrrrrrr</name>
      <uri>https://old.reddit.com/user/skarrrrrrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Must run in 8GB vram cards ... What is the model that can go beyond newspaper3K for this task ? The smaller the better !&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skarrrrrrr"&gt; /u/skarrrrrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6esb4/need_model_recommendations_to_parse_html/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6esb4/need_model_recommendations_to_parse_html/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6esb4/need_model_recommendations_to_parse_html/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T00:21:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1k644of</id>
    <title>Llama 4 - Scout: best quantization resource and comparison to Llama 3.3</title>
    <updated>2025-04-23T16:53:17+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The two primary resources I‚Äôve seen to get for Scout (GGUF for us GPU poor), seems to be Unsloth and Bartowski‚Ä¶ both of which seems to do something non-traditional compared to density models like Llama 70b 3.3. So which one is the best or am I missing one? At first blush Bartowski seems to perform better but then again my first attempt with Unsloth was a smaller quant‚Ä¶ so I‚Äôm curious what others think. &lt;/p&gt; &lt;p&gt;Then for llama 3.3 vs scout it seems comparable with maybe llama 3.3 having better performance and scout definitely far faster at the same performance.&lt;/p&gt; &lt;p&gt;Edit: Thanks x0wl for the comparison link, and to Bartowski for the comparison efforts. &lt;a href="https://huggingface.co/blog/bartowski/llama4-scout-off"&gt;https://huggingface.co/blog/bartowski/llama4-scout-off&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k644of/llama_4_scout_best_quantization_resource_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k644of/llama_4_scout_best_quantization_resource_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k644of/llama_4_scout_best_quantization_resource_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T16:53:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5te39</id>
    <title>Describe Anything - an Nvidia Collection</title>
    <updated>2025-04-23T07:36:42+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5te39/describe_anything_an_nvidia_collection/"&gt; &lt;img alt="Describe Anything - an Nvidia Collection" src="https://external-preview.redd.it/avPHoeiIWcSN0nR96Ahh8Yzf-NNPVymep5WhTab-9P0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bae6562f0fcfc3366518ae10b148de15bdf62ea" title="Describe Anything - an Nvidia Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Describe Anything Model 3B (DAM-3B) takes inputs of user-specified regions in the form of points/boxes/scribbles/masks within images, and generates detailed localized descriptions of images. DAM integrates full-image context with fine-grained local details using a novel focal prompt and a localized vision backbone enhanced with gated cross-attention. The model is for research and development only. This model is ready for non-commercial use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/nvidia/describe-anything-680825bb8f5e41ff0785834c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5te39/describe_anything_an_nvidia_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5te39/describe_anything_an_nvidia_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T07:36:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6d8zt</id>
    <title>Science Fair Agents run locally</title>
    <updated>2025-04-23T23:08:39+00:00</updated>
    <author>
      <name>/u/Financial_Pick8394</name>
      <uri>https://old.reddit.com/user/Financial_Pick8394</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6d8zt/science_fair_agents_run_locally/"&gt; &lt;img alt="Science Fair Agents run locally" src="https://external-preview.redd.it/cmR5bHRteHYxb3dlMZcDW0ukQe6jBnQ3FptNd_RPfnjJWOo6z3EdN2Pnmdo9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d523ccb2d5c7113a9c039f7a7bce11173942bd1f" title="Science Fair Agents run locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Corporate AI ML LLM Agent Science Fair Open-Source Framework Development In Progress &lt;/p&gt; &lt;p&gt;We have successfully achieved the main goals of Phase 1 and the initial steps of Phase 2: &lt;/p&gt; &lt;p&gt;‚úÖ Architectural Skeleton Built (Interfaces, Agent Service Components,) &lt;/p&gt; &lt;p&gt;‚úÖ Redis Services Implemented and Integrated &lt;/p&gt; &lt;p&gt;‚úÖ Core Task Flow Operational and Resource Monitoring Service. (Orchestrator -&amp;gt; Queue -&amp;gt; Worker -&amp;gt; Agent -&amp;gt; State) &lt;/p&gt; &lt;p&gt;‚úÖ Optimistic Locking (Task Assignment &amp;amp; Agent State) &lt;/p&gt; &lt;p&gt;‚úÖ Basic Science Fair Agents and Dynamic Simulation Workflow Modules (OrganicChemistryAgent, MolecularBiologyAgent, FractalAgent, HopfieldAgent, DataScienceAgent, ChaosTheoryAgent, EntropyAgent, AstrophysicsAgent, RoboticsAgent, EnvironmentalScienceAgent, MachineLearningAgent, MemoryAgent, CreativeAgent, ValidationAgent, InformationTheoryAgent, HypothesisAgent, ContextAwareAgent, MultiModalAgent, CollaborativeAgent, TemporalPrimeAgent, CuriosityQRLAgent, LLMAgent, LLaDATaskAgent, Physics, Quantum Qiskit circuit creation/simulation, Generic) &lt;/p&gt; &lt;p&gt;‚úÖ LLMAgent With Interactive NLP/Command Parsing: Prompt console with API calls to Ollama and multi-step commands. (Phase 2 will integrate a local transformers pipeline.) &lt;/p&gt; &lt;p&gt;Now we can confidently move deeper into Phase 2: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Refine Performance Metrics: Enhance perf_score with deep and meaningful insight extraction for each agent. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Monitoring: Implement the comprehensive metric collection in NodeProbe and aggregation in ResourceMonitoringService. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Reinforcement Learning. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here is one example&lt;br /&gt; &lt;a href="https://github.com/CorporateStereotype/ScienceFair/"&gt;https://github.com/CorporateStereotype/ScienceFair/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial_Pick8394"&gt; /u/Financial_Pick8394 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pasdcmxv1owe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6d8zt/science_fair_agents_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6d8zt/science_fair_agents_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T23:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6fy0j</id>
    <title>SurveyGOÔºöOpen DeepResearch. Automated AI-generated surveys</title>
    <updated>2025-04-24T01:20:06+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;By TsinghuaNLP team, great job guys !&lt;/p&gt; &lt;p&gt;SurveyGO can turn massive paper piles into high-quality, concise, citation-rich surveys. &lt;/p&gt; &lt;p&gt;üëç Under the hood lies &lt;strong&gt;LLM√óMapReduce‚ÄëV2&lt;/strong&gt;, a novel test-time scaling strategy designed to enhance LLMs' ability to process extremely long inputs.&lt;/p&gt; &lt;p&gt;üåê Demo: &lt;a href="https://surveygo.thunlp.org/"&gt;https://surveygo.thunlp.org/&lt;/a&gt;&lt;br /&gt; üìÑ Paper: &lt;a href="https://arxiv.org/abs/2504.05732"&gt;https://arxiv.org/abs/2504.05732&lt;/a&gt;&lt;br /&gt; üíª Code: &lt;a href="https://github.com/thunlp/LLMxMapReduce/"&gt;GitHub - thunlp/LLMxMapReduce&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://surveygo.thunlp.org/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6fy0j/surveygoopen_deepresearch_automated_aigenerated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6fy0j/surveygoopen_deepresearch_automated_aigenerated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T01:20:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6h0au</id>
    <title>Just upgraded from an M1 MacBook Pro to an m4 MacBook Pro... Anyone else get load coil whine with LLMs?</title>
    <updated>2025-04-24T02:13:28+00:00</updated>
    <author>
      <name>/u/cmndr_spanky</name>
      <uri>https://old.reddit.com/user/cmndr_spanky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(load = loud .. but honestly its not loud relatively speaking :) )&lt;/p&gt; &lt;p&gt;My M1 was dead silent, my new M4 MacBook Pro running a model in Ollama makes a very noticeable fast chirping sound (It's very faint, but noticeable and not something the M1 Pro had). Anyone else experience this or is there something wrong with this thing ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cmndr_spanky"&gt; /u/cmndr_spanky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6h0au/just_upgraded_from_an_m1_macbook_pro_to_an_m4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6h0au/just_upgraded_from_an_m1_macbook_pro_to_an_m4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6h0au/just_upgraded_from_an_m1_macbook_pro_to_an_m4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T02:13:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5zum2</id>
    <title>Running 32b LLM with low VRAM (12Gb or less)</title>
    <updated>2025-04-23T13:58:24+00:00</updated>
    <author>
      <name>/u/Low-Woodpecker-4522</name>
      <uri>https://old.reddit.com/user/Low-Woodpecker-4522</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know that there is a huge performance penalty when the model doesn't fit on the VRAM, but considering the new low bit quantizations, and that you can find some 32b models that could fit in VRAM, I wonder if it's practical to run those models with low VRAM.&lt;/p&gt; &lt;p&gt;What are the speed results of running low bit imatrix quants of 32b models with 12Gb VRAM?&lt;br /&gt; What is your experience ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Low-Woodpecker-4522"&gt; /u/Low-Woodpecker-4522 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5zum2/running_32b_llm_with_low_vram_12gb_or_less/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5zum2/running_32b_llm_with_low_vram_12gb_or_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5zum2/running_32b_llm_with_low_vram_12gb_or_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T13:58:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5qqst</id>
    <title>Llama 4 Maverick Locally at 45 tk/s on a Single RTX 4090 - I finally got it working!</title>
    <updated>2025-04-23T04:38:26+00:00</updated>
    <author>
      <name>/u/texasdude11</name>
      <uri>https://old.reddit.com/user/texasdude11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys!&lt;/p&gt; &lt;p&gt;I just wrapped up a follow-up demo where I got 45+ tokens per second out of Meta‚Äôs massive 400 billion-parameter, 128-expert Llama 4 Maverick, and I wanted to share the full setup in case it helps anyone else pushing these models locally. Here‚Äôs what made it possible: CPU: Intel Engineering Sample QYFS (similar to Xeon Platinum 8480+ with 56 cores / 112 threads) with AMX acceleration&lt;/p&gt; &lt;p&gt;GPU: Single NVIDIA RTX 4090 (no dual-GPU hack needed!) RAM: 512 GB DDR5 ECC OS: Ubuntu 22.04 LTS&lt;/p&gt; &lt;p&gt;Environment: K-Transformers support-llama4 branch&lt;/p&gt; &lt;p&gt;Below is the link to video : &lt;a href="https://youtu.be/YZqUfGQzOtk"&gt;https://youtu.be/YZqUfGQzOtk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you're interested in the hardware build: &lt;a href="https://youtu.be/r7gVGIwkZDc"&gt;https://youtu.be/r7gVGIwkZDc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/texasdude11"&gt; /u/texasdude11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5qqst/llama_4_maverick_locally_at_45_tks_on_a_single/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5qqst/llama_4_maverick_locally_at_45_tks_on_a_single/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5qqst/llama_4_maverick_locally_at_45_tks_on_a_single/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T04:38:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1k63o9h</id>
    <title>Aider appreciation post</title>
    <updated>2025-04-23T16:34:33+00:00</updated>
    <author>
      <name>/u/myoddity</name>
      <uri>https://old.reddit.com/user/myoddity</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Aider-chat just hits too right for me. &lt;/p&gt; &lt;p&gt;It is powerful, yet light and clean. It lives in terminal, yet is simply approachable. It can do all the work, yet encourages to bring-your-own-context. It's free, yet it just works. What more is needed, for one who can code, yet cannot code. &lt;/p&gt; &lt;p&gt;(Disclaimer: No chatgpt was used to write this. Only heart.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/myoddity"&gt; /u/myoddity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k63o9h/aider_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k63o9h/aider_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k63o9h/aider_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T16:34:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5t2cq</id>
    <title>Pytorch 2.7.0 with support for Blackwell (5090, B200) to come out today</title>
    <updated>2025-04-23T07:12:28+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5t2cq/pytorch_270_with_support_for_blackwell_5090_b200/"&gt; &lt;img alt="Pytorch 2.7.0 with support for Blackwell (5090, B200) to come out today" src="https://external-preview.redd.it/BKRijIKtfRZRNLNOU5KghR-oMM4YnWGWd_YjBkqgBfE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7741f3556461371cbf440be2d26db7ce7f09a007" title="Pytorch 2.7.0 with support for Blackwell (5090, B200) to come out today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This stable release of pytorch 2.7.0 should allow most projects to work with 5090 series out of the box without having to use nightly releases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/pytorch/pytorch.github.io/pull/1989/files"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5t2cq/pytorch_270_with_support_for_blackwell_5090_b200/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5t2cq/pytorch_270_with_support_for_blackwell_5090_b200/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T07:12:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5yw16</id>
    <title>Pattern-Aware Vector Database and ANN Algorithm</title>
    <updated>2025-04-23T13:15:32+00:00</updated>
    <author>
      <name>/u/yumojibaba</name>
      <uri>https://old.reddit.com/user/yumojibaba</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5yw16/patternaware_vector_database_and_ann_algorithm/"&gt; &lt;img alt="Pattern-Aware Vector Database and ANN Algorithm" src="https://preview.redd.it/cwgw5y593lwe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10ef81375a693303e9267eadf91b3c5c3a52d00f" title="Pattern-Aware Vector Database and ANN Algorithm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are releasing the beta version of PatANN, a vector search framework we've been working on that takes a different approach to ANN search by leveraging pattern recognition within vectors before distance calculations.&lt;/p&gt; &lt;p&gt;Our benchmarks on standard datasets show that PatANN achieved 4- 10x higher QPS than existing solutions (HNSW, ScaNN, FAISS) while maintaining &amp;gt;99.9% recall.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Fully asynchronous execution: Decomposes queries for parallel execution across threads&lt;/li&gt; &lt;li&gt;True hybrid memory management: Works efficiently both in-memory and on-disk&lt;/li&gt; &lt;li&gt;Pattern-aware search algorithm that addresses hubness effects in high-dimensional spaces&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We have posted technical documentation and initial benchmarks at &lt;a href="https://patann.dev"&gt;https://patann.dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a beta release, and work is in progress, so we are particularly interested in feedback on stability, integration experiences, and performance in different workloads, especially those working with large-scale vector search applications.&lt;/p&gt; &lt;p&gt;We invite you to download code samples from the GitHub repo (Python, Android (Java/Kotlin), iOS (Swift/Obj-C)) and try them out. We look forward to feedback. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yumojibaba"&gt; /u/yumojibaba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cwgw5y593lwe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5yw16/patternaware_vector_database_and_ann_algorithm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5yw16/patternaware_vector_database_and_ann_algorithm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T13:15:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1k60mlw</id>
    <title>LaSearch: Fully local semantic search app (with CUSTOM "embeddings" model)</title>
    <updated>2025-04-23T14:31:25+00:00</updated>
    <author>
      <name>/u/joelkunst</name>
      <uri>https://old.reddit.com/user/joelkunst</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k60mlw/lasearch_fully_local_semantic_search_app_with/"&gt; &lt;img alt="LaSearch: Fully local semantic search app (with CUSTOM &amp;quot;embeddings&amp;quot; model)" src="https://external-preview.redd.it/aDV1d2g4MTRobHdlMcf6y9HMrkeunVjc93oLf19y0pwTcXwF2-pbO3PezUgo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb00308374c95d7c2665d585962792eeb747f97c" title="LaSearch: Fully local semantic search app (with CUSTOM &amp;quot;embeddings&amp;quot; model)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have build my own &amp;quot;embeddings&amp;quot; model that's ultra small and lightweight. It does not function in the same way as usual ones and is not as powerful as they are, but it's orders of magnitude smaller and faster.&lt;/p&gt; &lt;p&gt;It powers my fully local semantic search app.&lt;/p&gt; &lt;p&gt;No data goes outside of your machine, and it uses very little resources to function.&lt;/p&gt; &lt;p&gt;MCP server is coming so you can use it to get relevant docs for RAG.&lt;/p&gt; &lt;p&gt;I've been testing with a small group but want to expand for more diverse feedback. If you're interested in trying it out or have any questions about the technology, let me know in the comments or sign up on the website.&lt;/p&gt; &lt;p&gt;Would love your thoughts on the concept and implementation!&lt;br /&gt; &lt;a href="https://lasearch.app"&gt;https://lasearch.app&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joelkunst"&gt; /u/joelkunst &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/31aodc14hlwe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k60mlw/lasearch_fully_local_semantic_search_app_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k60mlw/lasearch_fully_local_semantic_search_app_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T14:31:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1k65cmy</id>
    <title>Unpopular Opinion: I'm Actually Loving Llama-4-Scout</title>
    <updated>2025-04-23T17:41:40+00:00</updated>
    <author>
      <name>/u/Far_Buyer_7281</name>
      <uri>https://old.reddit.com/user/Far_Buyer_7281</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen a lot of negativity surrounding the new Llama-4-Scout, and I wanted to share my experience is completely different. I love especially the natural tone and large context understanding &lt;/p&gt; &lt;p&gt;I'm curious to hear if anyone else is having a positive experience with Llama-4-Scout, or if there are specific use cases where it shines. What are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far_Buyer_7281"&gt; /u/Far_Buyer_7281 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k65cmy/unpopular_opinion_im_actually_loving_llama4scout/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k65cmy/unpopular_opinion_im_actually_loving_llama4scout/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k65cmy/unpopular_opinion_im_actually_loving_llama4scout/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T17:41:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1k665cg</id>
    <title>Anyone try UI-TARS-1.5-7B new model from ByteDance</title>
    <updated>2025-04-23T18:13:14+00:00</updated>
    <author>
      <name>/u/Muted-Celebration-47</name>
      <uri>https://old.reddit.com/user/Muted-Celebration-47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k665cg/anyone_try_uitars157b_new_model_from_bytedance/"&gt; &lt;img alt="Anyone try UI-TARS-1.5-7B new model from ByteDance" src="https://external-preview.redd.it/MEb00d1gLWxp1-4OYIxzng3fr7CjMC7BtYqeV0pZ5Zc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c79e173fc5596ae79fab0463a41a31cb51d11923" title="Anyone try UI-TARS-1.5-7B new model from ByteDance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In summary, It allows AI to use your computer or web browser.&lt;/p&gt; &lt;p&gt;source: &lt;a href="https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B"&gt;https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;**Edit**&lt;br /&gt; I managed to make it works with gemma3:27b. But it still failed to find the correct coordinate in &amp;quot;Computer use&amp;quot; mode.&lt;/p&gt; &lt;p&gt;Here the steps:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1. Dowload gemma3:27b with ollama =&amp;gt; ollama run gemma3:27b 2. Increase context length at least 16k (16384) 3. Download UI-TARS Desktop 4. Click setting =&amp;gt; select provider: Huggingface for UI-TARS-1.5; base url: http://localhost:11434/v1; API key: test; model name: gemma3:27b; save; 5. Select &amp;quot;Browser use&amp;quot; and try &amp;quot;Go to google and type reddit in the search box and hit Enter (DO NOT ctrl+c)&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I tried to use it with Ollama and connected it to UI-TARS Desktop, but it failed to follow the prompt. It just took multiple screenshots. What's your experience with it?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8sfb6fc8lmwe1.png?width=1737&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38228461bcca820366ff63549025975f0070f5ec"&gt;UI TARS Desktop&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Muted-Celebration-47"&gt; /u/Muted-Celebration-47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k665cg/anyone_try_uitars157b_new_model_from_bytedance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k665cg/anyone_try_uitars157b_new_model_from_bytedance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k665cg/anyone_try_uitars157b_new_model_from_bytedance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T18:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1k650xj</id>
    <title>The best translator is a hybrid translator - combining a corpus of LLMs</title>
    <updated>2025-04-23T17:28:49+00:00</updated>
    <author>
      <name>/u/Nuenki</name>
      <uri>https://old.reddit.com/user/Nuenki</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k650xj/the_best_translator_is_a_hybrid_translator/"&gt; &lt;img alt="The best translator is a hybrid translator - combining a corpus of LLMs" src="https://external-preview.redd.it/DtSOjZDQhCIQrR9MXzfYsDwli-PvO8iAuPXRBhYivls.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=33bb3dd09e1348f194cfb304ced2dd662da82a0f" title="The best translator is a hybrid translator - combining a corpus of LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nuenki"&gt; /u/Nuenki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nuenki.app/blog/the_best_translator_is_a_hybrid_translator"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k650xj/the_best_translator_is_a_hybrid_translator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k650xj/the_best_translator_is_a_hybrid_translator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T17:28:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1k655wa</id>
    <title>LlamaCon is in 6 days</title>
    <updated>2025-04-23T17:34:15+00:00</updated>
    <author>
      <name>/u/iamn0</name>
      <uri>https://old.reddit.com/user/iamn0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k655wa/llamacon_is_in_6_days/"&gt; &lt;img alt="LlamaCon is in 6 days" src="https://external-preview.redd.it/__nAvfZl_lg7YJNwP-IInXWe8ebatQ8ExlHyPqG5yUM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=953181b2ca7b3814274602f6fa358f0bc7519113" title="LlamaCon is in 6 days" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/kcvsj160emwe1.png?width=597&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2f3a091dd458f203a46e49bc23ef13ce69aeeda"&gt;Zuck, Ghodsi, Nadella&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü¶ô &lt;strong&gt;LlamaCon ‚Äì April 29, 2025&lt;/strong&gt;&lt;br /&gt; Meta's first-ever developer conference dedicated to their open-source AI, held &lt;strong&gt;in person&lt;/strong&gt; at Meta HQ in Menlo Park, CA ‚Äî with &lt;strong&gt;select sessions live-streamed online&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Agenda:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;10:00 AM PST ‚Äì LlamaCon Keynote&lt;/strong&gt;&lt;br /&gt; Celebrating the open-source community and showcasing the latest in the Llama model ecosystem.&lt;br /&gt; &lt;strong&gt;Speakers:&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Chris Cox ‚Äì Chief Product Officer, Meta&lt;br /&gt; ‚Ä¢ Manohar Paluri ‚Äì VP of AI, Meta&lt;br /&gt; ‚Ä¢ Angela Fan ‚Äì Research Scientist in Generative AI, Meta&lt;/p&gt; &lt;p&gt;&lt;strong&gt;10:45 AM PST ‚Äì A Conversation with Mark Zuckerberg &amp;amp; Ali Ghodsi&lt;/strong&gt;&lt;br /&gt; Open source AI, building with LLMs, and advice for founders.&lt;br /&gt; &lt;strong&gt;Speakers:&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Mark Zuckerberg ‚Äì Founder &amp;amp; CEO, Meta&lt;br /&gt; ‚Ä¢ Ali Ghodsi ‚Äì Co-founder &amp;amp; CEO, Databricks&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4:00 PM PST ‚Äì A Conversation with Mark Zuckerberg &amp;amp; Satya Nadella&lt;/strong&gt;&lt;br /&gt; AI trends, real-world applications, and future outlooks.&lt;br /&gt; &lt;strong&gt;Speakers:&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Mark Zuckerberg ‚Äì Founder &amp;amp; CEO, Meta&lt;br /&gt; ‚Ä¢ Satya Nadella ‚Äì Chairman &amp;amp; CEO, Microsoft&lt;/p&gt; &lt;p&gt;üîó &lt;a href="https://www.llama.com/events/llamacon/2025/?utm_source=llama-home&amp;amp;utm_medium=llama-referral&amp;amp;utm_campaign=llama-utm&amp;amp;utm_offering=llamacon-learnmore&amp;amp;utm_product=llama"&gt;Link&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamn0"&gt; /u/iamn0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k655wa/llamacon_is_in_6_days/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k655wa/llamacon_is_in_6_days/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k655wa/llamacon_is_in_6_days/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T17:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k63kpq</id>
    <title>A summary of the progress AMD has made to improve it's AI capabilities in the past 4 months from SemiAnalysis</title>
    <updated>2025-04-23T16:30:32+00:00</updated>
    <author>
      <name>/u/takuonline</name>
      <uri>https://old.reddit.com/user/takuonline</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k63kpq/a_summary_of_the_progress_amd_has_made_to_improve/"&gt; &lt;img alt="A summary of the progress AMD has made to improve it's AI capabilities in the past 4 months from SemiAnalysis" src="https://external-preview.redd.it/mWvSuKRH-R24cuYFUnmlYRdyhyET4x6NAvj4TSYw978.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a3b94039246ff5688098f9a39ede8e0e6a75a64" title="A summary of the progress AMD has made to improve it's AI capabilities in the past 4 months from SemiAnalysis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In this report, we will discuss the many positive changes AMD has made. They are on the right track but need to increase the R&amp;amp;D budget for GPU hours and make further investments in AI talent. We will provide additional recommendations and elaborate on AMD management‚Äôs blind spot: how they are uncompetitive in the race for AI Software Engineers due to compensation structure benchmarking to the wrong set of companies.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/takuonline"&gt; /u/takuonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://semianalysis.com/2025/04/23/amd-2-0-new-sense-of-urgency-mi450x-chance-to-beat-nvidia-nvidias-new-moat/?access_token=eyJhbGciOiJFUzI1NiIsImtpZCI6InNlbWlhbmFseXNpcy5wYXNzcG9ydC5vbmxpbmUiLCJ0eXAiOiJKV1QifQ.eyJhdWQiOiJzZW1pYW5hbHlzaXMucGFzc3BvcnQub25saW5lIiwiYXpwIjoiS1NncVhBaGFmZmtwVjQzbmt0UU1INSIsImVudCI6eyJhdWQiOlsiNThZNVhua2U4U1ZnTkFRRm5GZUVIQiJdLCJ1cmkiOlsiaHR0cHM6Ly9zZW1pYW5hbHlzaXMuY29tLzIwMjUvMDQvMjMvYW1kLTItMC1uZXctc2Vuc2Utb2YtdXJnZW5jeS1taTQ1MHgtY2hhbmNlLXRvLWJlYXQtbnZpZGlhLW52aWRpYXMtbmV3LW1vYXQvIl19LCJleHAiOjE3NDgwMDM1MTgsImlhdCI6MTc0NTQxMTUxOCwiaXNzIjoiaHR0cHM6Ly9zZW1pYW5hbHlzaXMucGFzc3BvcnQub25saW5lL29hdXRoIiwic2NvcGUiOiJmZWVkOnJlYWQgYXJ0aWNsZTpyZWFkIGFzc2V0OnJlYWQgY2F0ZWdvcnk6cmVhZCBlbnRpdGxlbWVudHMiLCJzdWIiOiIyaUFXTUs0U0F2RFU3WkpaTGdzR2NYIiwidXNlIjoiYWNjZXNzIn0.K4tPYV6TgV6HszD-hFW0Vql1f9IXKrEx9ZjL2SxfSXAqHYkdk4uCxhwq_Iu4oWCjSyXPCveZLaNDQ19GD3ua9Q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k63kpq/a_summary_of_the_progress_amd_has_made_to_improve/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k63kpq/a_summary_of_the_progress_amd_has_made_to_improve/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T16:30:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5x7a2</id>
    <title>Created a calculator for modelling GPT token-generation throughput</title>
    <updated>2025-04-23T11:52:09+00:00</updated>
    <author>
      <name>/u/Mindless_Pain1860</name>
      <uri>https://old.reddit.com/user/Mindless_Pain1860</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5x7a2/created_a_calculator_for_modelling_gpt/"&gt; &lt;img alt="Created a calculator for modelling GPT token-generation throughput" src="https://external-preview.redd.it/blv2LZ-IrTm3FyQojwoj082So0qC55XGIytRyhb8H3w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff38a981027aac6178b194fa35693fd435150d30" title="Created a calculator for modelling GPT token-generation throughput" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.desmos.com/calculator/qtkabsqhxt"&gt;https://www.desmos.com/calculator/qtkabsqhxt&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mindless_Pain1860"&gt; /u/Mindless_Pain1860 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k5x7a2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5x7a2/created_a_calculator_for_modelling_gpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5x7a2/created_a_calculator_for_modelling_gpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T11:52:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5wdw0</id>
    <title>HP wants to put a local LLM in your printers</title>
    <updated>2025-04-23T11:05:18+00:00</updated>
    <author>
      <name>/u/WordyBug</name>
      <uri>https://old.reddit.com/user/WordyBug</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5wdw0/hp_wants_to_put_a_local_llm_in_your_printers/"&gt; &lt;img alt="HP wants to put a local LLM in your printers" src="https://preview.redd.it/9wawej40hkwe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f75beba5aa65b4f7a42767d2301f3c23268219c3" title="HP wants to put a local LLM in your printers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WordyBug"&gt; /u/WordyBug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9wawej40hkwe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5wdw0/hp_wants_to_put_a_local_llm_in_your_printers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5wdw0/hp_wants_to_put_a_local_llm_in_your_printers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T11:05:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6ably</id>
    <title>Bartowski just updated his glm-4-32B quants. working in lmstudio soon?</title>
    <updated>2025-04-23T21:02:39+00:00</updated>
    <author>
      <name>/u/ieatrox</name>
      <uri>https://old.reddit.com/user/ieatrox</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6ably/bartowski_just_updated_his_glm432b_quants_working/"&gt; &lt;img alt="Bartowski just updated his glm-4-32B quants. working in lmstudio soon?" src="https://external-preview.redd.it/3NYpVgamx1NXpydfb32BxQDBSawDgIlUbaanFyS12QE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e09f35ea9f5809bb0108aaeb81cfcd9b214c0a72" title="Bartowski just updated his glm-4-32B quants. working in lmstudio soon?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ieatrox"&gt; /u/ieatrox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/bartowski/THUDM_GLM-4-32B-0414-GGUF/tree/main"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6ably/bartowski_just_updated_his_glm432b_quants_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6ably/bartowski_just_updated_his_glm432b_quants_working/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T21:02:39+00:00</published>
  </entry>
</feed>
