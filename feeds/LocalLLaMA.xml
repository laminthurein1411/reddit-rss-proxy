<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-01T21:49:01+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1l10im3</id>
    <title>Connecting two 3090s</title>
    <updated>2025-06-01T21:31:20+00:00</updated>
    <author>
      <name>/u/elchurnerista</name>
      <uri>https://old.reddit.com/user/elchurnerista</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How can I connect two 3090s in consumer hardware? My motherboard supports x8/x8, and ample cooling.&lt;/p&gt; &lt;p&gt;I was trying to connect them via an SLI/NVM Link but I don't see many resources on the topic. I've read some mentions of SLI being deprecated for FUTURE support, but I'm assuming it's still possible.&lt;/p&gt; &lt;p&gt;I am not interested in finding a different motherboard + cpu platform, trying to work with what I got.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elchurnerista"&gt; /u/elchurnerista &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l10im3/connecting_two_3090s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l10im3/connecting_two_3090s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l10im3/connecting_two_3090s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T21:31:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1l06f7r</id>
    <title>Most powerful &lt; 7b parameters model at the moment?</title>
    <updated>2025-05-31T20:14:03+00:00</updated>
    <author>
      <name>/u/ventilador_liliana</name>
      <uri>https://old.reddit.com/user/ventilador_liliana</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to know which is the best model less than 7b currently available.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ventilador_liliana"&gt; /u/ventilador_liliana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l06f7r/most_powerful_7b_parameters_model_at_the_moment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l06f7r/most_powerful_7b_parameters_model_at_the_moment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l06f7r/most_powerful_7b_parameters_model_at_the_moment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T20:14:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzzshu</id>
    <title>Google lets you run AI models locally</title>
    <updated>2025-05-31T15:29:15+00:00</updated>
    <author>
      <name>/u/dnr41418</name>
      <uri>https://old.reddit.com/user/dnr41418</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://techcrunch.com/2025/05/31/google-quietly-released-an-app-that-lets-you-download-and-run-ai-models-locally/"&gt;https://techcrunch.com/2025/05/31/google-quietly-released-an-app-that-lets-you-download-and-run-ai-models-locally/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dnr41418"&gt; /u/dnr41418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzzshu/google_lets_you_run_ai_models_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzzshu/google_lets_you_run_ai_models_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzzshu/google_lets_you_run_ai_models_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T15:29:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0cg8b</id>
    <title>AMD RX 9080 XT ES engineering sample, up to 32 GB of VRAM.</title>
    <updated>2025-06-01T00:57:59+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.notebookcheck.net/AMD-RX-9080-XT-ES-engineering-sample-could-rival-RTX-5080-Super.1027707.0.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0cg8b/amd_rx_9080_xt_es_engineering_sample_up_to_32_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0cg8b/amd_rx_9080_xt_es_engineering_sample_up_to_32_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T00:57:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0rcin</id>
    <title>Recommended setup for local LLMs</title>
    <updated>2025-06-01T15:14:12+00:00</updated>
    <author>
      <name>/u/pioni</name>
      <uri>https://old.reddit.com/user/pioni</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently running a PC with i7-8700k, 32GB of memory and Nvidia 4070 and it is clearly not fit for my needs (coding Typescript, Python and LLMs). However, I haven't found good resources on what should I upgrade next. My options at the moment are:&lt;/p&gt; &lt;p&gt;- Mac Studio M3 Ultra 96GB unified memory (or with 256GB if I manage to pay for it)&lt;br /&gt; - Mac Studio M4 Max 128GB&lt;br /&gt; - PC with 9950X3D, 128GB of DDR5 and Nvidia 5090&lt;br /&gt; - Upgrading just the GPU on my current PC, but I don't think that makes sense as the maximum RAM is still 32GB&lt;br /&gt; - making a frankenstein budget option out of extra hardware I have around, buying the parts I don't have, leading to a: PC with 5950X, 128GB of DDR4, 1080TI with 12GB of VRAM. That is the most budget friendly option here, but I'm afraid it will be even slower and the case is too small to fit that 4070 from the other PC I have. That however would run Roo Code or Cursor (which would be needed unless I get a new GPU, or a Mac I guess) just fine.&lt;/p&gt; &lt;p&gt;With my current system the biggest obstacle is that the inference speed is very slow on models larger than 8B parameters (like 2-8 tokens / second after thinking for minutes). What would be the most practical way of running larger models, and faster? You can recommend also surprise combinations if you come up with any, such as some Mac Mini configuration if the M4 Pro is fast enough for this. Also the 8B models (and smaller) have been so inaccurate that they've been effectively useless forcing me to use Cursor, which I don't exactly love either as it clears it context window constantly and I'd have to start again.&lt;/p&gt; &lt;p&gt;Note that 2nd hand computers cost the same or more than new ones due to sky high demand because of sky high umemployment and oncoming implosion of the economic system. I'm out of options there unless you can give be good European retailers that ship abroad.&lt;/p&gt; &lt;p&gt;Also I have a large Proxmox cluster that has everything I need except what I've mentioned here, database servers, dev environments, whatever I need, so that is taken care of.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pioni"&gt; /u/pioni &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0rcin/recommended_setup_for_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0rcin/recommended_setup_for_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0rcin/recommended_setup_for_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T15:14:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0x0q8</id>
    <title>3x Modded 4090 48GB or RTX Pro 6000?</title>
    <updated>2025-06-01T19:05:50+00:00</updated>
    <author>
      <name>/u/sNullp</name>
      <uri>https://old.reddit.com/user/sNullp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can source them for about the same price. I've heard there is an efficiency hit on multi card with those modded 4090. But 3 card has 144GB vram vs RTX Pro's 96GB. &lt;del&gt;And power consumption is comparable.&lt;/del&gt; Which route should I choose?&lt;/p&gt; &lt;p&gt;Edit: power consumption is obviously not comparable. I don't know what I was thinking. But it is in a colo environment so doesn't matter much for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sNullp"&gt; /u/sNullp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0x0q8/3x_modded_4090_48gb_or_rtx_pro_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0x0q8/3x_modded_4090_48gb_or_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0x0q8/3x_modded_4090_48gb_or_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T19:05:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0ylj8</id>
    <title>Pure vs. merged - and a modern leaderboard</title>
    <updated>2025-06-01T20:12:09+00:00</updated>
    <author>
      <name>/u/jaggzh</name>
      <uri>https://old.reddit.com/user/jaggzh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably been discussion about this, but I've noticed the trained-in quirks of models diminish with merged models. (Can't tell with abliterated since the only ones I've used are also mergers). Quirks include stubbornness in personality, desire consistency, to suck with certain formatting, etc. &lt;/p&gt; &lt;p&gt;Yet we have no leaderboard [that I know of] that evaluates them anymore. Most leaderboards now are quite crippled in filtering, let alone finding open models.&lt;/p&gt; &lt;p&gt;I'm trying to think of a way we could come up with basic low-energy-use community-based testing. It doesn't need to be exhaustive -- some small subsets of test types would likely satisfy for open against various mergers. &lt;/p&gt; &lt;p&gt;People can establish tests for honoring instruct, basic accuracies, math, function-calling, whatever. (Models bad at something tend to show it quite rapidly in my own experience.)&lt;/p&gt; &lt;p&gt;Being community-based (&amp;quot;crowd-sourced&amp;quot;), the system could cross-reference users' results to give a ranking reliability. Users can be get some type of reliability as well (perhaps a rank/algorithm we work on over time), to try to mitigate weirdos manipulating results (but one climbing high fraudulently would gain popularity and, thus, higher criticisms.&lt;/p&gt; &lt;p&gt;Also, since the turnover of models is quite rapid, I'm not sure if there's much risk in the system just not being that perfect anyway.&lt;/p&gt; &lt;p&gt;(It should, though, have some proper filtering and sorting in the results though!)&lt;/p&gt; &lt;p&gt;What do you all think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jaggzh"&gt; /u/jaggzh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0ylj8/pure_vs_merged_and_a_modern_leaderboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0ylj8/pure_vs_merged_and_a_modern_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0ylj8/pure_vs_merged_and_a_modern_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T20:12:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0qbot</id>
    <title>TTS support in llama.cpp?</title>
    <updated>2025-06-01T14:30:47+00:00</updated>
    <author>
      <name>/u/Disonantemus</name>
      <uri>https://old.reddit.com/user/Disonantemus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know I can do this (using &lt;code&gt;OuteTTS-0.2-500M&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-tts --tts-oute-default -p &amp;quot;Hello World&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;... and get an &lt;code&gt;output.wav&lt;/code&gt; audio file, that I can reproduce, with any terminal audio player, like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;aplay&lt;/li&gt; &lt;li&gt;play (sox)&lt;/li&gt; &lt;li&gt;paplay&lt;/li&gt; &lt;li&gt;mpv&lt;/li&gt; &lt;li&gt;ffplay&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Does llama-tts support any other TTS?&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;I saw some PR in github with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OuteTTS0.3&lt;/li&gt; &lt;li&gt;OuteTTS1.0&lt;/li&gt; &lt;li&gt;OrpheusTTS&lt;/li&gt; &lt;li&gt;SparkTTS&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But, none of those work for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disonantemus"&gt; /u/Disonantemus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0qbot/tts_support_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0qbot/tts_support_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0qbot/tts_support_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T14:30:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0jcoa</id>
    <title>How many parameters does R1 0528 have?</title>
    <updated>2025-06-01T07:44:01+00:00</updated>
    <author>
      <name>/u/Sudden-Albatross-733</name>
      <uri>https://old.reddit.com/user/Sudden-Albatross-733</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0jcoa/how_many_parameters_does_r1_0528_have/"&gt; &lt;img alt="How many parameters does R1 0528 have?" src="https://b.thumbs.redditmedia.com/WwyP2AGTFovCSAcu4ERv7Xhjnr7t6sRjYVGA0rP1BDI.jpg" title="How many parameters does R1 0528 have?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found conflicting info online, some articles say it's 685b and some say 671b, which is correct? huggingface also shows 685b (look at the attached screenshot) BUT it shows that even for the old one, which I know for sure was 671b. anyone know which is correct?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sudden-Albatross-733"&gt; /u/Sudden-Albatross-733 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1l0jcoa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0jcoa/how_many_parameters_does_r1_0528_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0jcoa/how_many_parameters_does_r1_0528_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T07:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0uccd</id>
    <title>I built a lightweight, private, MCP server to share context between AI tools</title>
    <updated>2025-06-01T17:17:13+00:00</updated>
    <author>
      <name>/u/coding9</name>
      <uri>https://old.reddit.com/user/coding9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I have seen a few projects similar to mine lately, so I decided to open source mine ASAP.&lt;/p&gt; &lt;p&gt;My approach uses a single docker command, a single 90mb service that needs to be running. So it's quite small.&lt;/p&gt; &lt;p&gt;I wanted to make a service that persists context and can recall it across any AI tools. I also want it to be a way to persist your digital life and semantic search it, all self hosted.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;One thing I saw lacking in a few other alternatives is re-embedding&lt;/strong&gt;. If you change your preferred model, the next startup will automatically re-embed all documents for you.&lt;/p&gt; &lt;p&gt;As for how it works: if I read a website about presidents, I can say &amp;quot;recall documents about government&amp;quot; in my AI tool of choice, and it would be recalled, despite an exact text match not existing.&lt;/p&gt; &lt;p&gt;I am in progress building &lt;strong&gt;Obsidian and browser extensions&lt;/strong&gt; to progress towards automatically ingesting any content for later retrieval.&lt;/p&gt; &lt;p&gt;You can bring your own AI service. I recommend Ollama or LM Studio, but you can connect it to OpenAI or any other embedding service.&lt;/p&gt; &lt;p&gt;For AI and coding specifically, &lt;strong&gt;there are getContext and setContext key / value tools that the MCP server adds&lt;/strong&gt;. You can imagine saving your project information, like what package mangers to use, in here at any time, and then any AI tool you can add it to the prompt afterwards. Some examples using Cline and Claude desktop can be found &lt;a href="https://github.com/zackify/revect?tab=readme-ov-file#-examples"&gt;at the bottom of the readme&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This service uses SQLite, so it's incredibly simple, and &lt;strong&gt;only takes up 90mb&lt;/strong&gt; for a fully complete docker container.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This means you can query your data easily, or back it up by mounting the container to an iCloud drive or Dropbox folder for example.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I have a cloud version I will launch soon, so its easy to share this between teams.&lt;/p&gt; &lt;p&gt;Most of the examples I have seen currently use multiple services and much more resources to do the same thing.&lt;/p&gt; &lt;p&gt;Let me know what you all think, the repo can be found here: &lt;a href="https://github.com/zackify/revect"&gt;https://github.com/zackify/revect&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coding9"&gt; /u/coding9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0uccd/i_built_a_lightweight_private_mcp_server_to_share/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0uccd/i_built_a_lightweight_private_mcp_server_to_share/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0uccd/i_built_a_lightweight_private_mcp_server_to_share/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T17:17:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzsa70</id>
    <title>China is leading open source</title>
    <updated>2025-05-31T08:35:25+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"&gt; &lt;img alt="China is leading open source" src="https://preview.redd.it/6stw9ivzw24f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87af4f2951867765dd0c43808b34253b587103b5" title="China is leading open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6stw9ivzw24f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T08:35:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0mo90</id>
    <title>Introducing an open source cross-platform graphical interface LLM client</title>
    <updated>2025-06-01T11:26:40+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0mo90/introducing_an_open_source_crossplatform/"&gt; &lt;img alt="Introducing an open source cross-platform graphical interface LLM client" src="https://external-preview.redd.it/asw6R0ibq6fWJLI0jTiqq5MWe_ZOda7dhXjccGwW8KM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=20d6c7b5164323d63cf76761c30754520702828d" title="Introducing an open source cross-platform graphical interface LLM client" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cherry Studio is a desktop client that supports for multiple LLM providers, available on Windows, Mac and Linux.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/CherryHQ/cherry-studio"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0mo90/introducing_an_open_source_crossplatform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0mo90/introducing_an_open_source_crossplatform/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T11:26:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0vwc1</id>
    <title>Would a laptop iGPU + 64GB RAM be good for anything, speed wise?</title>
    <updated>2025-06-01T18:20:15+00:00</updated>
    <author>
      <name>/u/ArsenicBismuth</name>
      <uri>https://old.reddit.com/user/ArsenicBismuth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;VRAM is a big limiting factor for a lot of bigger models for most of consumer GPU. So, I was wondering if my iGPU (Ryzen 5 5600H) would be capable for running some models locally using RAM?&lt;/p&gt; &lt;p&gt;Or would you think a M2 mac machine with similar RAM would be significantly better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArsenicBismuth"&gt; /u/ArsenicBismuth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0vwc1/would_a_laptop_igpu_64gb_ram_be_good_for_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0vwc1/would_a_laptop_igpu_64gb_ram_be_good_for_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0vwc1/would_a_laptop_igpu_64gb_ram_be_good_for_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T18:20:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0rvqr</id>
    <title>Old dual socket Xeon server with tons of RAM viable for LLM inference?</title>
    <updated>2025-06-01T15:35:57+00:00</updated>
    <author>
      <name>/u/jojokingxp</name>
      <uri>https://old.reddit.com/user/jojokingxp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was looking into maybe getting a used 2 socket Lga 3647 board and some Xeons wit loads of (RAM 256GB+). I don't need insane speeds, but it shouldn't take hours either. &lt;/p&gt; &lt;p&gt;It seems a lot more affordable per GB than Apple silicon and of course VRAM, but I feel like it might be too slow to really be viable or just plain not worth it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jojokingxp"&gt; /u/jojokingxp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0rvqr/old_dual_socket_xeon_server_with_tons_of_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0rvqr/old_dual_socket_xeon_server_with_tons_of_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0rvqr/old_dual_socket_xeon_server_with_tons_of_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T15:35:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0xubg</id>
    <title>Toolcalling in the reasoning trace as an alternative to agentic frameworks</title>
    <updated>2025-06-01T19:40:13+00:00</updated>
    <author>
      <name>/u/ExaminationNo8522</name>
      <uri>https://old.reddit.com/user/ExaminationNo8522</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://2084.substack.com/p/deep-reasoning-with-tools-toolcalling"&gt;Deep Reasoning With Tools: Toolcalling in the reasoning trace&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey, so I was working on training reasoning models to do interesting things, when I started wanting them to be more dynamic: not just predict based on static information but actively search the data space to get information. Thus I built this toolset to integrate toolcalling into the reasoning trace of the AI models, since then I could do wayyy more complex RL training to allow it to do stuff like reconciliation of accounts, or more complex trading. However, as I built it, I realized that its actually a nice alternative to traditional agentic frameworks - you don't have discrete steps so it can run as long or as short as you want, and it can be invoked with a single command versus having to handle multiple steps. Thoughts? What other weirder agentic frameworks have y'all seen?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExaminationNo8522"&gt; /u/ExaminationNo8522 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0xubg/toolcalling_in_the_reasoning_trace_as_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0xubg/toolcalling_in_the_reasoning_trace_as_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0xubg/toolcalling_in_the_reasoning_trace_as_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T19:40:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0y4ep</id>
    <title>A Privacy-Focused Perplexity That Runs Locally on all your devices - iPhone, Android, iPad!</title>
    <updated>2025-06-01T19:52:02+00:00</updated>
    <author>
      <name>/u/Ssjultrainstnict</name>
      <uri>https://old.reddit.com/user/Ssjultrainstnict</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt; community!&lt;/p&gt; &lt;p&gt;Following up on my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ku1444/a_privacyfocused_perplexity_that_runs_locally_on/"&gt;previous post&lt;/a&gt;- the response has been incredible! Thank you to everyone who tried it out, left reviews, and provided feedback.&lt;/p&gt; &lt;p&gt;Based on your requests, I'm excited to announce that &lt;strong&gt;MyDeviceAI is now available on iPad and Android&lt;/strong&gt;!&lt;/p&gt; &lt;h1&gt;iPad Support&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Full native iPad experience with optimized UI&lt;/li&gt; &lt;li&gt;Same lightning-fast local processing with M-series chips&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Android Release&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Available as APK on GitHub releases (v1.2)&lt;/li&gt; &lt;li&gt;Download link: &lt;a href="https://github.com/navedmerchant/MyDeviceAI/releases"&gt;https://github.com/navedmerchant/MyDeviceAI/releases&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Same core features: local AI, SearXNG integration, complete privacy&lt;/li&gt; &lt;li&gt;Works across a wide range of Android devices&lt;/li&gt; &lt;li&gt;Runs on CPU only for now, working on getting Adreno GPU support in llama.rn&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What's Next?&lt;/h1&gt; &lt;p&gt;I'm continuing to work on improvements based on your suggestions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ability to select a larger model for powerful supported devices (Qwen 3 4b)&lt;/li&gt; &lt;li&gt;Ability to add images and documents to the chat for supported devices (QwenVL support)&lt;/li&gt; &lt;li&gt;Advanced speech mode on device&lt;/li&gt; &lt;li&gt;Enhanced personalization features&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Download Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;iOS/iPad&lt;/strong&gt;: &lt;a href="https://apps.apple.com/us/app/mydeviceai/id6736578281?platform=ipad"&gt;MyDeviceAI on App Store&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Android&lt;/strong&gt;: &lt;a href="https://github.com/navedmerchant/MyDeviceAI/releases"&gt;GitHub Releases v1.2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Source Code&lt;/strong&gt;: &lt;a href="https://github.com/navedmerchant/MyDeviceAI"&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you've been waiting for Android support or want to try it on iPad, now's your chance! As always, everything remains 100% free, open source, and completely private.&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts on the new platforms, and please consider leaving a review if MyDeviceAI has been useful for you. Your support helps tremendously with continued development!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ssjultrainstnict"&gt; /u/Ssjultrainstnict &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y4ep/a_privacyfocused_perplexity_that_runs_locally_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y4ep/a_privacyfocused_perplexity_that_runs_locally_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y4ep/a_privacyfocused_perplexity_that_runs_locally_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T19:52:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0wfln</id>
    <title>Is multiple m3 ultras the move instead of 1 big one?</title>
    <updated>2025-06-01T18:42:06+00:00</updated>
    <author>
      <name>/u/AcceptableBridge7616</name>
      <uri>https://old.reddit.com/user/AcceptableBridge7616</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am seriously considering investing in a sizable m3 ultra mac studio. Looking through some of the benchmarks, it seems the m3ultra's do well but not as well in prompt processing speed. The comparisons from the 60 core to the 80 core seem to show a (surprisingly?) big boost from going up in gpu size. Given the low power usage, I think just getting more than 1 is a real option. However, I couldn't really find any comparisons comparing chained configurations, though I have seen videos of people doing it especially with the previous model. If you are in the ~10k price range, I think it's worth considering different combos:&lt;/p&gt; &lt;p&gt;one 80 core, 512gb ram- ~$9.4k&lt;/p&gt; &lt;p&gt;two 60 core, 256gb ram each - ~ $11k&lt;/p&gt; &lt;p&gt;two 60 core, 1 256gb ram, 1 96gb ram ~ $9.6k&lt;/p&gt; &lt;p&gt;three 60 core, 96gb ram each ~$12k&lt;/p&gt; &lt;p&gt;Are you losing much performance by spreading things across 2 machines? I think the biggest issue will be the annoyance of administering 2+ boxes. Having different sized boxes many even more annoying. Anyone have any experience with this who can comment? Obviously the best setup is use case dependent but I am trying to understand what I might not be taking into account here...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AcceptableBridge7616"&gt; /u/AcceptableBridge7616 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0wfln/is_multiple_m3_ultras_the_move_instead_of_1_big/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0wfln/is_multiple_m3_ultras_the_move_instead_of_1_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0wfln/is_multiple_m3_ultras_the_move_instead_of_1_big/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T18:42:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0qp75</id>
    <title>App-Use : Create virtual desktops for AI agents to focus on specific apps.</title>
    <updated>2025-06-01T14:46:57+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0qp75/appuse_create_virtual_desktops_for_ai_agents_to/"&gt; &lt;img alt="App-Use : Create virtual desktops for AI agents to focus on specific apps." src="https://external-preview.redd.it/ejV6cmV3ODZ3YjRmMYsTHh_R0WswrUJBBa-0t3y7YsS9UlwJcbvZWkm9vo2Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e17e2eb7db050248c2423a2e18a718dcda868c6" title="App-Use : Create virtual desktops for AI agents to focus on specific apps." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;App-Use lets you scope agents to just the apps they need. Instead of full desktop access, say &amp;quot;only work with Safari and Notes&amp;quot; or &amp;quot;just control iPhone Mirroring&amp;quot; - visual isolation without new processes for perfectly focused automation.&lt;/p&gt; &lt;p&gt;Running computer-use on the entire desktop often causes agent hallucinations and loss of focus when they see irrelevant windows and UI elements. App-Use solves this by creating composited views where agents only see what matters, dramatically improving task completion accuracy&lt;/p&gt; &lt;p&gt;Currently macOS-only (Quartz compositing engine). &lt;/p&gt; &lt;p&gt;Read the full guide: &lt;a href="https://trycua.com/blog/app-use"&gt;https://trycua.com/blog/app-use&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/v0fcznj6wb4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0qp75/appuse_create_virtual_desktops_for_ai_agents_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0qp75/appuse_create_virtual_desktops_for_ai_agents_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T14:46:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0n5ta</id>
    <title>Which is the best uncensored model?</title>
    <updated>2025-06-01T11:55:48+00:00</updated>
    <author>
      <name>/u/BoJackHorseMan53</name>
      <uri>https://old.reddit.com/user/BoJackHorseMan53</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to learn ethical hacking. Tried dolphin-mistral-r1 it did answer but it's answers were bad.&lt;/p&gt; &lt;p&gt;Are there any good uncensored models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoJackHorseMan53"&gt; /u/BoJackHorseMan53 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0n5ta/which_is_the_best_uncensored_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0n5ta/which_is_the_best_uncensored_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0n5ta/which_is_the_best_uncensored_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T11:55:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0m8r0</id>
    <title>104k-Token Prompt in a 110k-Token Context with DeepSeek-R1-0528-UD-IQ1_S â€“ Benchmark &amp; Impressive Results</title>
    <updated>2025-06-01T11:00:46+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;The Prompts:&lt;/strong&gt; 1. &lt;a href="https://thireus.com/REDDIT/DeepSeek_Runescape_Massive_Prompt.txt"&gt;https://thireus.com/REDDIT/DeepSeek_Runescape_Massive_Prompt.txt&lt;/a&gt; (Firefox: View -&amp;gt; Repair Text Encoding) 2. &lt;a href="https://thireus.com/REDDIT/DeepSeek_Dipiloblop_Massive_Prompt.txt"&gt;https://thireus.com/REDDIT/DeepSeek_Dipiloblop_Massive_Prompt.txt&lt;/a&gt; (Firefox: View -&amp;gt; Repair Text Encoding)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Commands (on Windows):&lt;/strong&gt; &lt;code&gt; perl -pe 's/\n/\\n/' DeepSeek_Runescape_Massive_Prompt.txt | CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0,2,1 ~/llama-b5355-bin-win-cuda12.4-x64/llama-cli -m DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf -t 36 --ctx-size 110000 -ngl 62 --flash-attn --main-gpu 0 --no-mmap --mlock -ot &amp;quot;.ffn_(up|down)_exps.=CPU&amp;quot; --simple-io &lt;/code&gt; &lt;code&gt; perl -pe 's/\n/\\n/' DeepSeek_Dipiloblop_Massive_Prompt.txt | CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0,2,1 ~/llama-b5355-bin-win-cuda12.4-x64/llama-cli -m DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf -t 36 --ctx-size 110000 -ngl 62 --flash-attn --main-gpu 0 --no-mmap --mlock -ot &amp;quot;.ffn_(up|down)_exps.=CPU&amp;quot; --simple-io &lt;/code&gt; - Tips: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kysms8"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kysms8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Answers (first time I see a model provide such a good answer):&lt;/strong&gt; - &lt;a href="https://thireus.com/REDDIT/DeepSeek_Runescape_Massive_Prompt_Answer.txt"&gt;https://thireus.com/REDDIT/DeepSeek_Runescape_Massive_Prompt_Answer.txt&lt;/a&gt; - &lt;a href="https://thireus.com/REDDIT/DeepSeek_Dipiloblop_Massive_Prompt_Answer.txt"&gt;https://thireus.com/REDDIT/DeepSeek_Dipiloblop_Massive_Prompt_Answer.txt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Hardware:&lt;/strong&gt; &lt;code&gt; i9-7980XE - 4.2Ghz on all cores 256GB DDR4 F4-3200C14Q2-256GTRS - XMP enabled 1x 5090 (x16) 1x 3090 (x16) 1x 3090 (x8) Prime-X299-A-II &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The benchmark results:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Runescape: ``` llama_perf_sampler_print: sampling time = 608.32 ms / 106524 runs ( 0.01 ms per token, 175112.36 tokens per second) llama_perf_context_print: load time = 190451.73 ms llama_perf_context_print: prompt eval time = 5188938.33 ms / 104276 tokens ( 49.76 ms per token, 20.10 tokens per second) llama_perf_context_print: eval time = 577349.77 ms / 2248 runs ( 256.83 ms per token, 3.89 tokens per second) llama_perf_context_print: total time = 5768493.07 ms / 106524 tokens&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 608.32 ms / 106524 runs ( 0.01 ms per token, 175112.36 tokens per second) llama_perf_context_print: load time = 190451.73 ms llama_perf_context_print: prompt eval time = 5188938.33 ms / 104276 tokens ( 49.76 ms per token, 20.10 tokens per second) llama_perf_context_print: eval time = 577349.77 ms / 2248 runs ( 256.83 ms per token, 3.89 tokens per second) llama_perf_context_print: total time = 5768493.22 ms / 106524 tokens &lt;code&gt; Dipiloblop: &lt;/code&gt; llama_perf_sampler_print: sampling time = 534.36 ms / 106532 runs ( 0.01 ms per token, 199364.47 tokens per second) llama_perf_context_print: load time = 177215.16 ms llama_perf_context_print: prompt eval time = 5101404.01 ms / 104586 tokens ( 48.78 ms per token, 20.50 tokens per second) llama_perf_context_print: eval time = 500475.72 ms / 1946 runs ( 257.18 ms per token, 3.89 tokens per second) llama_perf_context_print: total time = 5603899.16 ms / 106532 tokens&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 534.36 ms / 106532 runs ( 0.01 ms per token, 199364.47 tokens per second) llama_perf_context_print: load time = 177215.16 ms llama_perf_context_print: prompt eval time = 5101404.01 ms / 104586 tokens ( 48.78 ms per token, 20.50 tokens per second) llama_perf_context_print: eval time = 500475.72 ms / 1946 runs ( 257.18 ms per token, 3.89 tokens per second) llama_perf_context_print: total time = 5603899.32 ms / 106532 tokens ```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Sampler (default values were used, DeepSeek recommends temp 0.6, but 0.8 was used):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Runescape: &lt;code&gt; sampler seed: 3756224448 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 110080 top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampler chain: logits -&amp;gt; logit-bias -&amp;gt; penalties -&amp;gt; dry -&amp;gt; top-n-sigma -&amp;gt; top-k -&amp;gt; typical -&amp;gt; top-p -&amp;gt; min-p -&amp;gt; xtc -&amp;gt; temp-ext -&amp;gt; dist &lt;/code&gt; Dipiloblop: &lt;code&gt; sampler seed: 1633590497 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 110080 top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampler chain: logits -&amp;gt; logit-bias -&amp;gt; penalties -&amp;gt; dry -&amp;gt; top-n-sigma -&amp;gt; top-k -&amp;gt; typical -&amp;gt; top-p -&amp;gt; min-p -&amp;gt; xtc -&amp;gt; temp-ext -&amp;gt; dist &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The questions:&lt;/strong&gt; 1. Would 1x RTX PRO 6000 Blackwell or even 2x RTX PRO 6000 Blackwell significantly improve these metrics without any other hardware upgrade? (knowing that there would still be CPU offloading) 2. Would a different CPU, motherboard and RAM improve these metrics? 3. How to significantly improve prompt processing speed?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt; - Comparative results with Qwen3-235B-A22B-128K-UD-Q3_K_XL are here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1l0m8r0/comment/mvg5ke9/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1l0m8r0/comment/mvg5ke9/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0m8r0/104ktoken_prompt_in_a_110ktoken_context_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0m8r0/104ktoken_prompt_in_a_110ktoken_context_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0m8r0/104ktoken_prompt_in_a_110ktoken_context_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T11:00:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0v8yq</id>
    <title>I made a simple tool to test/compare your local LLMs on AIME 2024</title>
    <updated>2025-06-01T17:54:01+00:00</updated>
    <author>
      <name>/u/EntropyMagnets</name>
      <uri>https://old.reddit.com/user/EntropyMagnets</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0v8yq/i_made_a_simple_tool_to_testcompare_your_local/"&gt; &lt;img alt="I made a simple tool to test/compare your local LLMs on AIME 2024" src="https://external-preview.redd.it/-Bks8K2_TljN7hLY0DvxIu9Ncpa8BzunHNO4VODMSAA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e1cf6849b57a4c81ac6a807fbf541e56f6b4544" title="I made a simple tool to test/compare your local LLMs on AIME 2024" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made &lt;a href="https://github.com/Belluxx/LocalAIME"&gt;LocalAIME&lt;/a&gt; a simple tool that tests one or many LLMs locally or trough API (you can use any OpenAI-compatible API) on AIME 2024.&lt;/p&gt; &lt;p&gt;It is pretty useful for testing different quants of the same model or the same quant of different providers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r0xk016htc4f1.png?width=4900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5fbfc8a2d435ef0fe50a7ed0dab250cdc03e6f2c"&gt;Performance of some models i tested for each AIME 2024 problem&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think about it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EntropyMagnets"&gt; /u/EntropyMagnets &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0v8yq/i_made_a_simple_tool_to_testcompare_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0v8yq/i_made_a_simple_tool_to_testcompare_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0v8yq/i_made_a_simple_tool_to_testcompare_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T17:54:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0zsv7</id>
    <title>25L Portable NV-linked Dual 3090 LLM Rig</title>
    <updated>2025-06-01T21:01:58+00:00</updated>
    <author>
      <name>/u/Special-Wolverine</name>
      <uri>https://old.reddit.com/user/Special-Wolverine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0zsv7/25l_portable_nvlinked_dual_3090_llm_rig/"&gt; &lt;img alt="25L Portable NV-linked Dual 3090 LLM Rig" src="https://b.thumbs.redditmedia.com/SCLEVQyCptjUTsrbRVb6jCIJUk1CuEOO-Ud355bse9Q.jpg" title="25L Portable NV-linked Dual 3090 LLM Rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Main point of portability is because The workplace of the coworker I built this for is truly offline, with no potential for LAN or wifi, so to download new models and update the system periodically I need to go pick it up from him and take it home. &lt;/p&gt; &lt;p&gt;WARNING - these components don't fit if you try to copy this build. The bottom GPU is resting on the Arctic p12 slim fans at the bottom of the case and pushing up on the GPU. Also the top arctic p14 Max fans don't have mounting points for half of their screw holes, and are in place by being very tightly wedged against the motherboard, case, and PSU. Also, there 's probably way too much pressure on the pcie cables coming off the gpus when you close the glass. Also I had to daisy chain the PCIE cables because the Corsair RM 1200e only has four available on the PSU side and these particular EVGA 3090s require 3x 8pin power. Allegedly it just enforces a hardware power limit to 300 w but you should make it a little bit more safe by also enforcing the 300W power limit in Nvidia -SMI To make sure that the cards don't try to pull 450W through 300W pipes. Could have fit a bigger PSU, but then I wouldn't get that front fan which is probably crucial.&lt;/p&gt; &lt;p&gt;All that being said, with a 300w power limit applied to both gpus in a silent fan profile, this rig has surprisingly good temperatures and noise levels considering how compact it is. &lt;/p&gt; &lt;p&gt;During Cinebench 24 with both gpus being 100% utilized, the CPU runs at 63 C and both gpus at 67 Celsius somehow with almost zero gap between them and the glass closed. All the while running at about 37 to 40 decibels from 1 meter away. &lt;/p&gt; &lt;p&gt;Prompt processing and inference - the gpus run at about 63 C, CPU at 55 C, and decibels at 34. &lt;/p&gt; &lt;p&gt;Again, I don't understand why the temperatures for both are almost the same, when logically the top GPU should be much hotter. The only gap between the two gpus is the size of one of those little silicone rubber DisplayPort caps wedged into the end, right between where the pcie power cables connect to force the GPUs apart a little.&lt;/p&gt; &lt;p&gt;Everything but the case, CPU cooler, and PSU was bought used on Facebook Marketplace&lt;/p&gt; &lt;p&gt;&lt;a href="https://pcpartpicker.com/list/nQXzgn"&gt;PCPartPicker Part List&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Item&lt;/th&gt; &lt;th align="left"&gt;Price&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/qtvqqs/amd-ryzen-7-5800x-38-ghz-8-core-processor-100-100000063wof"&gt;AMD Ryzen 7 5800X 3.8 GHz 8-Core Processor&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$160.54 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU Cooler&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/HbDQzy/id-cooling-frozn-a720-black-986-cfm-cpu-cooler-frozn-a720-black"&gt;ID-COOLING FROZN A720 BLACK 98.6 CFM CPU Cooler&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$69.98 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/CLkgXL/asus-rog-strix-x570-e-gaming-atx-am4-motherboard-rog-strix-x570-e-gaming"&gt;Asus ROG Strix X570-E Gaming ATX AM4 Motherboard&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$559.00 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/6rrcCJ/corsair-memory-cmk32gx4m2b3200c16"&gt;Corsair Vengeance LPX 32 GB (2 x 16 GB) DDR4-3200 CL16 Memory&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$81.96 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/DDWBD3/samsung-980-pro-1-tb-m2-2280-nvme-solid-state-drive-mz-v8p1t0bam"&gt;Samsung 980 Pro 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$149.99 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Video Card&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/PG848d/evga-geforce-rtx-3090-24-gb-ftw3-ultra-gaming-video-card-24g-p5-3987-kr"&gt;EVGA FTW3 ULTRA GAMING GeForce RTX 3090 24 GB Video Card&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$750.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Video Card&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/PG848d/evga-geforce-rtx-3090-24-gb-ftw3-ultra-gaming-video-card-24g-p5-3987-kr"&gt;EVGA FTW3 ULTRA GAMING GeForce RTX 3090 24 GB Video Card&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$750.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Custom&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;NVlink SLI bridge&lt;/td&gt; &lt;td align="left"&gt;$90.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Custom&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Mechanic Master c34plus&lt;/td&gt; &lt;td align="left"&gt;$200.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Custom&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Corsair RM1200e&lt;/td&gt; &lt;td align="left"&gt;$210.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Custom&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2x Arctic p14 max, 3x p12, 3x p12 slim&lt;/td&gt; &lt;td align="left"&gt;$60.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Prices include shipping, taxes, rebates, and discounts&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;$3081.47&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Generated by &lt;a href="https://pcpartpicker.com"&gt;PCPartPicker&lt;/a&gt; 2025-06-01 16:48 EDT-0400&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special-Wolverine"&gt; /u/Special-Wolverine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1l0zsv7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0zsv7/25l_portable_nvlinked_dual_3090_llm_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0zsv7/25l_portable_nvlinked_dual_3090_llm_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T21:01:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0p3et</id>
    <title>Let's build a production level Small Language Model (SLM) from scratch | 3 hour workshop</title>
    <updated>2025-06-01T13:34:12+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0p3et/lets_build_a_production_level_small_language/"&gt; &lt;img alt="Let's build a production level Small Language Model (SLM) from scratch | 3 hour workshop" src="https://external-preview.redd.it/3QugVQO6P_Q3v0881CbP7ispW7LV5z9hQhVFGV8ZV58.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64639bca07382b454fb4ec613939209217564782" title="Let's build a production level Small Language Model (SLM) from scratch | 3 hour workshop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/z6y0w621jb4f1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8491a9d59f58f3082afceaa6c737005c740ad38b"&gt;https://preview.redd.it/z6y0w621jb4f1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8491a9d59f58f3082afceaa6c737005c740ad38b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I made a 3 hour workshop showing how to build an SLM from scratch. &lt;/p&gt; &lt;p&gt;Watch it here: &lt;a href="https://youtu.be/pOFcwcwtv3k?si=1UI4uCdw_HLbdQgX"&gt;https://youtu.be/pOFcwcwtv3k?si=1UI4uCdw_HLbdQgX&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here is what I cover in the workshop: &lt;/p&gt; &lt;p&gt;(a) Download a dataset with 1million+ samples&lt;/p&gt; &lt;p&gt;(b) Pre-process and tokenize the dataset&lt;/p&gt; &lt;p&gt;(c) Divide the dataset into input-target pairs&lt;/p&gt; &lt;p&gt;(d) Assemble the SLM architecture: tokenization layer, attention layer, transformer block, output layer and everything in between&lt;/p&gt; &lt;p&gt;(e) Pre-train the entire SLM&lt;/p&gt; &lt;p&gt;(f) Run inference and generate new text from your trained SLM!&lt;/p&gt; &lt;p&gt;This is not a toy project. &lt;/p&gt; &lt;p&gt;It's a production-level project with an extensive dataset. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0p3et/lets_build_a_production_level_small_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0p3et/lets_build_a_production_level_small_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0p3et/lets_build_a_production_level_small_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T13:34:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0y0wp</id>
    <title>Allowing LLM to ponder in Open WebUI</title>
    <updated>2025-06-01T19:47:52+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y0wp/allowing_llm_to_ponder_in_open_webui/"&gt; &lt;img alt="Allowing LLM to ponder in Open WebUI" src="https://external-preview.redd.it/dHd6NjY5c2JkZDRmMbDY_eAdKP8QUXyZwc-4j2cel9Olwb9ejqufCbXqijwB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d96e6747cff63170125fef17cdbcf53af47bbb3f" title="Allowing LLM to ponder in Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What is this?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A completely superficial way of letting LLM to ponder a bit before making its conversation turn. The process is streamed to an artifact within Open WebUI.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/av/harbor/blob/main/boost/src/modules/ponder.py"&gt;Code&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uoeptbsbdd4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y0wp/allowing_llm_to_ponder_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y0wp/allowing_llm_to_ponder_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T19:47:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0q2zk</id>
    <title>DeepSeek-R1-0528-UD-Q6-K-XL on 10 Year Old Hardware</title>
    <updated>2025-06-01T14:19:51+00:00</updated>
    <author>
      <name>/u/Simusid</name>
      <uri>https://old.reddit.com/user/Simusid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't expect anything useful in this post. I did it just to see if it was possible. This was on a 10+ year old system with a 6th generation i5 with 12gb of RAM. My ssd is nearly full so I had to mount an external 8TB USB drive to store the 560GB model. At least it is USB-3.&lt;/p&gt; &lt;p&gt;I made an 800GB swap file and enabled it, then launched llama-cli with a simple prompt and went to bed. I half expected that the model might not even have fully loaded when I got up but it was already part way through the response.&lt;/p&gt; &lt;p&gt;With no GPU, it seems to be about seven minutes per token.&lt;/p&gt; &lt;p&gt;Edit - I've named this system TreeBeard&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Simusid"&gt; /u/Simusid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0q2zk/deepseekr10528udq6kxl_on_10_year_old_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0q2zk/deepseekr10528udq6kxl_on_10_year_old_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0q2zk/deepseekr10528udq6kxl_on_10_year_old_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T14:19:51+00:00</published>
  </entry>
</feed>
