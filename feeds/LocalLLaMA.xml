<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-08T08:51:51+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kgxhdt</id>
    <title>Ollama vs Llama.cpp on 2x3090 and M3Max using qwen3-30b</title>
    <updated>2025-05-07T13:33:49+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone.&lt;/p&gt; &lt;p&gt;This is a comparison test between Ollama and Llama.cpp on 2 x RTX-3090 and M3-Max with 64GB using qwen3:30b-a3b-q8_0.&lt;/p&gt; &lt;p&gt;Just note, this was primarily to compare Ollama and Llama.cpp with Qwen MoE architecture. Also, this speed test won't translate to other models based on dense architecture. It'll be completely different.&lt;/p&gt; &lt;p&gt;VLLM, SGLang Exllama don't support rtx3090 with this particular Qwen MoE architecture yet. If interested, I ran a separate &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ke26sl/another_attempt_to_measure_speed_for_qwen3_moe_on/"&gt;benchmark with M3Max, rtx-4090 on MLX, Llama.cpp, VLLM SGLang&lt;/a&gt; here.&lt;/p&gt; &lt;h3&gt;Metrics&lt;/h3&gt; &lt;p&gt;To ensure consistency, I used a custom Python script that sends requests to the server via the OpenAI-compatible API. Metrics were calculated as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Time to First Token (TTFT): Measured from the start of the streaming request to the first streaming event received.&lt;/li&gt; &lt;li&gt;Prompt Processing Speed (PP): Number of prompt tokens divided by TTFT.&lt;/li&gt; &lt;li&gt;Token Generation Speed (TG): Number of generated tokens divided by (total duration - TTFT).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The displayed results were truncated to two decimal places, but the calculations used full precision. I made the script to prepend 40% new material in the beginning of next longer prompt to avoid caching effect.&lt;/p&gt; &lt;p&gt;Here's my script for anyone interest. &lt;a href="https://github.com/chigkim/prompt-test"&gt;https://github.com/chigkim/prompt-test&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It uses OpenAI API, so it should work in variety setup. Also, this tests one request at a time, so multiple parallel requests could result in higher throughput in different tests.&lt;/p&gt; &lt;h3&gt;Setup&lt;/h3&gt; &lt;p&gt;Both use the same q8_0 model from Ollama library with flash attention. I'm sure you can further optimize Llama.cpp, but I copied the flags from Ollama log in order to keep it consistent, so both use the exactly same flags when loading the model.&lt;/p&gt; &lt;p&gt;&lt;code&gt;./build/bin/llama-server --model ~/.ollama/models/blobs/sha256... --ctx-size 36000 --batch-size 512 --n-gpu-layers 49 --verbose --threads 24 --flash-attn --parallel 1 --tensor-split 25,24 --port 11434&lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama.cpp: Commit 2f54e34&lt;/li&gt; &lt;li&gt;Ollama: 0.6.8&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each row in the results represents a test (a specific combination of machine, engine, and prompt length). There are 4 tests per prompt length.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Setup 1: 2xRTX3090, Llama.cpp&lt;/li&gt; &lt;li&gt;Setup 2: 2xRTX3090, Ollama&lt;/li&gt; &lt;li&gt;Setup 3: M3Max, Llama.cpp&lt;/li&gt; &lt;li&gt;Setup 4: M3Max, Ollama&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Result&lt;/h3&gt; &lt;p&gt;Please zoom in to see the graph better.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img xcmmuk1bycze1...&lt;/em&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Machine&lt;/th&gt; &lt;th&gt;Engine&lt;/th&gt; &lt;th&gt;Prompt Tokens&lt;/th&gt; &lt;th&gt;PP/s&lt;/th&gt; &lt;th&gt;TTFT&lt;/th&gt; &lt;th&gt;Generated Tokens&lt;/th&gt; &lt;th&gt;TG/s&lt;/th&gt; &lt;th&gt;Duration&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;702&lt;/td&gt; &lt;td&gt;1663.57&lt;/td&gt; &lt;td&gt;0.42&lt;/td&gt; &lt;td&gt;1419&lt;/td&gt; &lt;td&gt;82.19&lt;/td&gt; &lt;td&gt;17.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;702&lt;/td&gt; &lt;td&gt;1595.04&lt;/td&gt; &lt;td&gt;0.44&lt;/td&gt; &lt;td&gt;1430&lt;/td&gt; &lt;td&gt;77.41&lt;/td&gt; &lt;td&gt;18.91&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;702&lt;/td&gt; &lt;td&gt;289.53&lt;/td&gt; &lt;td&gt;2.42&lt;/td&gt; &lt;td&gt;1485&lt;/td&gt; &lt;td&gt;55.60&lt;/td&gt; &lt;td&gt;29.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;702&lt;/td&gt; &lt;td&gt;288.32&lt;/td&gt; &lt;td&gt;2.43&lt;/td&gt; &lt;td&gt;1440&lt;/td&gt; &lt;td&gt;55.78&lt;/td&gt; &lt;td&gt;28.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;959&lt;/td&gt; &lt;td&gt;1768.00&lt;/td&gt; &lt;td&gt;0.54&lt;/td&gt; &lt;td&gt;1210&lt;/td&gt; &lt;td&gt;81.47&lt;/td&gt; &lt;td&gt;15.39&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;959&lt;/td&gt; &lt;td&gt;1723.07&lt;/td&gt; &lt;td&gt;0.56&lt;/td&gt; &lt;td&gt;1279&lt;/td&gt; &lt;td&gt;74.82&lt;/td&gt; &lt;td&gt;17.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;959&lt;/td&gt; &lt;td&gt;458.40&lt;/td&gt; &lt;td&gt;2.09&lt;/td&gt; &lt;td&gt;1337&lt;/td&gt; &lt;td&gt;55.28&lt;/td&gt; &lt;td&gt;26.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;959&lt;/td&gt; &lt;td&gt;459.38&lt;/td&gt; &lt;td&gt;2.09&lt;/td&gt; &lt;td&gt;1302&lt;/td&gt; &lt;td&gt;55.44&lt;/td&gt; &lt;td&gt;25.57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1306&lt;/td&gt; &lt;td&gt;1752.04&lt;/td&gt; &lt;td&gt;0.75&lt;/td&gt; &lt;td&gt;1108&lt;/td&gt; &lt;td&gt;80.95&lt;/td&gt; &lt;td&gt;14.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1306&lt;/td&gt; &lt;td&gt;1725.06&lt;/td&gt; &lt;td&gt;0.76&lt;/td&gt; &lt;td&gt;1209&lt;/td&gt; &lt;td&gt;73.83&lt;/td&gt; &lt;td&gt;17.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1306&lt;/td&gt; &lt;td&gt;455.39&lt;/td&gt; &lt;td&gt;2.87&lt;/td&gt; &lt;td&gt;1213&lt;/td&gt; &lt;td&gt;54.84&lt;/td&gt; &lt;td&gt;24.99&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1306&lt;/td&gt; &lt;td&gt;458.06&lt;/td&gt; &lt;td&gt;2.85&lt;/td&gt; &lt;td&gt;1213&lt;/td&gt; &lt;td&gt;54.96&lt;/td&gt; &lt;td&gt;24.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1774&lt;/td&gt; &lt;td&gt;1763.32&lt;/td&gt; &lt;td&gt;1.01&lt;/td&gt; &lt;td&gt;1330&lt;/td&gt; &lt;td&gt;80.44&lt;/td&gt; &lt;td&gt;17.54&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1774&lt;/td&gt; &lt;td&gt;1823.88&lt;/td&gt; &lt;td&gt;0.97&lt;/td&gt; &lt;td&gt;1370&lt;/td&gt; &lt;td&gt;78.26&lt;/td&gt; &lt;td&gt;18.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1774&lt;/td&gt; &lt;td&gt;320.44&lt;/td&gt; &lt;td&gt;5.54&lt;/td&gt; &lt;td&gt;1281&lt;/td&gt; &lt;td&gt;54.10&lt;/td&gt; &lt;td&gt;29.21&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1774&lt;/td&gt; &lt;td&gt;321.45&lt;/td&gt; &lt;td&gt;5.52&lt;/td&gt; &lt;td&gt;1281&lt;/td&gt; &lt;td&gt;54.26&lt;/td&gt; &lt;td&gt;29.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;2584&lt;/td&gt; &lt;td&gt;1776.17&lt;/td&gt; &lt;td&gt;1.45&lt;/td&gt; &lt;td&gt;1522&lt;/td&gt; &lt;td&gt;79.39&lt;/td&gt; &lt;td&gt;20.63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;2584&lt;/td&gt; &lt;td&gt;1851.35&lt;/td&gt; &lt;td&gt;1.40&lt;/td&gt; &lt;td&gt;1118&lt;/td&gt; &lt;td&gt;75.08&lt;/td&gt; &lt;td&gt;16.29&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;2584&lt;/td&gt; &lt;td&gt;445.47&lt;/td&gt; &lt;td&gt;5.80&lt;/td&gt; &lt;td&gt;1321&lt;/td&gt; &lt;td&gt;52.86&lt;/td&gt; &lt;td&gt;30.79&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;2584&lt;/td&gt; &lt;td&gt;447.47&lt;/td&gt; &lt;td&gt;5.77&lt;/td&gt; &lt;td&gt;1359&lt;/td&gt; &lt;td&gt;53.00&lt;/td&gt; &lt;td&gt;31.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;3557&lt;/td&gt; &lt;td&gt;1832.97&lt;/td&gt; &lt;td&gt;1.94&lt;/td&gt; &lt;td&gt;1500&lt;/td&gt; &lt;td&gt;77.61&lt;/td&gt; &lt;td&gt;21.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;3557&lt;/td&gt; &lt;td&gt;1928.76&lt;/td&gt; &lt;td&gt;1.84&lt;/td&gt; &lt;td&gt;1653&lt;/td&gt; &lt;td&gt;70.17&lt;/td&gt; &lt;td&gt;25.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;3557&lt;/td&gt; &lt;td&gt;444.32&lt;/td&gt; &lt;td&gt;8.01&lt;/td&gt; &lt;td&gt;1481&lt;/td&gt; &lt;td&gt;51.34&lt;/td&gt; &lt;td&gt;36.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;3557&lt;/td&gt; &lt;td&gt;442.89&lt;/td&gt; &lt;td&gt;8.03&lt;/td&gt; &lt;td&gt;1430&lt;/td&gt; &lt;td&gt;51.52&lt;/td&gt; &lt;td&gt;35.79&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;4739&lt;/td&gt; &lt;td&gt;1773.28&lt;/td&gt; &lt;td&gt;2.67&lt;/td&gt; &lt;td&gt;1279&lt;/td&gt; &lt;td&gt;76.60&lt;/td&gt; &lt;td&gt;19.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;4739&lt;/td&gt; &lt;td&gt;1910.52&lt;/td&gt; &lt;td&gt;2.48&lt;/td&gt; &lt;td&gt;1877&lt;/td&gt; &lt;td&gt;71.85&lt;/td&gt; &lt;td&gt;28.60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;4739&lt;/td&gt; &lt;td&gt;421.06&lt;/td&gt; &lt;td&gt;11.26&lt;/td&gt; &lt;td&gt;1472&lt;/td&gt; &lt;td&gt;49.97&lt;/td&gt; &lt;td&gt;40.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;4739&lt;/td&gt; &lt;td&gt;420.51&lt;/td&gt; &lt;td&gt;11.27&lt;/td&gt; &lt;td&gt;1316&lt;/td&gt; &lt;td&gt;50.16&lt;/td&gt; &lt;td&gt;37.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;6520&lt;/td&gt; &lt;td&gt;1760.68&lt;/td&gt; &lt;td&gt;3.70&lt;/td&gt; &lt;td&gt;1435&lt;/td&gt; &lt;td&gt;73.77&lt;/td&gt; &lt;td&gt;23.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;6520&lt;/td&gt; &lt;td&gt;1897.12&lt;/td&gt; &lt;td&gt;3.44&lt;/td&gt; &lt;td&gt;1781&lt;/td&gt; &lt;td&gt;68.85&lt;/td&gt; &lt;td&gt;29.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;6520&lt;/td&gt; &lt;td&gt;418.03&lt;/td&gt; &lt;td&gt;15.60&lt;/td&gt; &lt;td&gt;1998&lt;/td&gt; &lt;td&gt;47.56&lt;/td&gt; &lt;td&gt;57.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;6520&lt;/td&gt; &lt;td&gt;417.70&lt;/td&gt; &lt;td&gt;15.61&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;47.81&lt;/td&gt; &lt;td&gt;57.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;9101&lt;/td&gt; &lt;td&gt;1714.65&lt;/td&gt; &lt;td&gt;5.31&lt;/td&gt; &lt;td&gt;1528&lt;/td&gt; &lt;td&gt;70.17&lt;/td&gt; &lt;td&gt;27.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;9101&lt;/td&gt; &lt;td&gt;1881.13&lt;/td&gt; &lt;td&gt;4.84&lt;/td&gt; &lt;td&gt;1801&lt;/td&gt; &lt;td&gt;68.09&lt;/td&gt; &lt;td&gt;31.29&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;9101&lt;/td&gt; &lt;td&gt;250.25&lt;/td&gt; &lt;td&gt;36.37&lt;/td&gt; &lt;td&gt;1941&lt;/td&gt; &lt;td&gt;36.29&lt;/td&gt; &lt;td&gt;89.86&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;9101&lt;/td&gt; &lt;td&gt;244.02&lt;/td&gt; &lt;td&gt;37.30&lt;/td&gt; &lt;td&gt;1941&lt;/td&gt; &lt;td&gt;35.55&lt;/td&gt; &lt;td&gt;91.89&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;12430&lt;/td&gt; &lt;td&gt;1591.33&lt;/td&gt; &lt;td&gt;7.81&lt;/td&gt; &lt;td&gt;1001&lt;/td&gt; &lt;td&gt;66.74&lt;/td&gt; &lt;td&gt;22.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;12430&lt;/td&gt; &lt;td&gt;1805.88&lt;/td&gt; &lt;td&gt;6.88&lt;/td&gt; &lt;td&gt;1284&lt;/td&gt; &lt;td&gt;64.01&lt;/td&gt; &lt;td&gt;26.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;12430&lt;/td&gt; &lt;td&gt;280.46&lt;/td&gt; &lt;td&gt;44.32&lt;/td&gt; &lt;td&gt;1291&lt;/td&gt; &lt;td&gt;39.89&lt;/td&gt; &lt;td&gt;76.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;12430&lt;/td&gt; &lt;td&gt;278.79&lt;/td&gt; &lt;td&gt;44.58&lt;/td&gt; &lt;td&gt;1502&lt;/td&gt; &lt;td&gt;39.82&lt;/td&gt; &lt;td&gt;82.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;17078&lt;/td&gt; &lt;td&gt;1546.35&lt;/td&gt; &lt;td&gt;11.04&lt;/td&gt; &lt;td&gt;1028&lt;/td&gt; &lt;td&gt;63.55&lt;/td&gt; &lt;td&gt;27.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;17078&lt;/td&gt; &lt;td&gt;1722.15&lt;/td&gt; &lt;td&gt;9.92&lt;/td&gt; &lt;td&gt;1100&lt;/td&gt; &lt;td&gt;59.36&lt;/td&gt; &lt;td&gt;28.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;17078&lt;/td&gt; &lt;td&gt;270.38&lt;/td&gt; &lt;td&gt;63.16&lt;/td&gt; &lt;td&gt;1461&lt;/td&gt; &lt;td&gt;34.89&lt;/td&gt; &lt;td&gt;105.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;17078&lt;/td&gt; &lt;td&gt;270.49&lt;/td&gt; &lt;td&gt;63.14&lt;/td&gt; &lt;td&gt;1673&lt;/td&gt; &lt;td&gt;34.28&lt;/td&gt; &lt;td&gt;111.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;23658&lt;/td&gt; &lt;td&gt;1429.31&lt;/td&gt; &lt;td&gt;16.55&lt;/td&gt; &lt;td&gt;1039&lt;/td&gt; &lt;td&gt;58.46&lt;/td&gt; &lt;td&gt;34.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;23658&lt;/td&gt; &lt;td&gt;1586.04&lt;/td&gt; &lt;td&gt;14.92&lt;/td&gt; &lt;td&gt;1041&lt;/td&gt; &lt;td&gt;53.90&lt;/td&gt; &lt;td&gt;34.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;23658&lt;/td&gt; &lt;td&gt;241.20&lt;/td&gt; &lt;td&gt;98.09&lt;/td&gt; &lt;td&gt;1681&lt;/td&gt; &lt;td&gt;28.04&lt;/td&gt; &lt;td&gt;158.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;23658&lt;/td&gt; &lt;td&gt;240.64&lt;/td&gt; &lt;td&gt;98.31&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;27.70&lt;/td&gt; &lt;td&gt;170.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;33525&lt;/td&gt; &lt;td&gt;1293.65&lt;/td&gt; &lt;td&gt;25.91&lt;/td&gt; &lt;td&gt;1311&lt;/td&gt; &lt;td&gt;52.92&lt;/td&gt; &lt;td&gt;50.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;33525&lt;/td&gt; &lt;td&gt;1441.12&lt;/td&gt; &lt;td&gt;23.26&lt;/td&gt; &lt;td&gt;1418&lt;/td&gt; &lt;td&gt;49.76&lt;/td&gt; &lt;td&gt;51.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;33525&lt;/td&gt; &lt;td&gt;217.15&lt;/td&gt; &lt;td&gt;154.38&lt;/td&gt; &lt;td&gt;1453&lt;/td&gt; &lt;td&gt;23.91&lt;/td&gt; &lt;td&gt;215.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;33525&lt;/td&gt; &lt;td&gt;219.68&lt;/td&gt; &lt;td&gt;152.61&lt;/td&gt; &lt;td&gt;1522&lt;/td&gt; &lt;td&gt;23.84&lt;/td&gt; &lt;td&gt;216.44&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgxhdt/ollama_vs_llamacpp_on_2x3090_and_m3max_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgxhdt/ollama_vs_llamacpp_on_2x3090_and_m3max_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgxhdt/ollama_vs_llamacpp_on_2x3090_and_m3max_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T13:33:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgu4qg</id>
    <title>Qwen3-235B Q6_K ktransformers at 56t/s prefill 4.5t/s decode on Xeon 3175X (384GB DDR4-3400) and RTX 4090</title>
    <updated>2025-05-07T10:37:38+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgu4qg/qwen3235b_q6_k_ktransformers_at_56ts_prefill_45ts/"&gt; &lt;img alt="Qwen3-235B Q6_K ktransformers at 56t/s prefill 4.5t/s decode on Xeon 3175X (384GB DDR4-3400) and RTX 4090" src="https://preview.redd.it/1ijx9ffv8cze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42da044e5a349add8ff7c3733ec1c4e706676161" title="Qwen3-235B Q6_K ktransformers at 56t/s prefill 4.5t/s decode on Xeon 3175X (384GB DDR4-3400) and RTX 4090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1ijx9ffv8cze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgu4qg/qwen3235b_q6_k_ktransformers_at_56ts_prefill_45ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgu4qg/qwen3235b_q6_k_ktransformers_at_56ts_prefill_45ts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T10:37:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1khki8f</id>
    <title>EPYC 7313P - good enough?</title>
    <updated>2025-05-08T07:48:21+00:00</updated>
    <author>
      <name>/u/AfraidScheme433</name>
      <uri>https://old.reddit.com/user/AfraidScheme433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Planning a home PC build for the family and small business use. How's the EPYC 7313P? Will it be sufficient? no image generation and just a lot of admin works &lt;/p&gt; &lt;ul&gt; &lt;li&gt; CPU: EPYC 7313P (16 core)&lt;/li&gt; &lt;li&gt; Cooler: EPYC SP3 Heatpipe Dual Fan Cooler&lt;/li&gt; &lt;li&gt; Motherboard: Supermicro H12SSL-i&lt;/li&gt; &lt;li&gt; RAM: 32GB DDR4 ECC 3200MHz x 8&lt;/li&gt; &lt;li&gt; SSD: 1TB NVMe SSD (Samsung 970 EVO Plus, used)&lt;/li&gt; &lt;li&gt; HDD: Seagate 16TB&lt;/li&gt; &lt;li&gt; Case: 4U 8-bay Case&lt;/li&gt; &lt;li&gt; PSU: EVGA 1000W 80+ Gold&lt;/li&gt; &lt;li&gt; Network Card: Motherboard Integrated&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AfraidScheme433"&gt; /u/AfraidScheme433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khki8f/epyc_7313p_good_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khki8f/epyc_7313p_good_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khki8f/epyc_7313p_good_enough/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T07:48:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1khfhoh</id>
    <title>Final verdict on LLM generated confidence scores?</title>
    <updated>2025-05-08T02:32:59+00:00</updated>
    <author>
      <name>/u/sg6128</name>
      <uri>https://old.reddit.com/user/sg6128</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I remember earlier hearing the confidence scores associated with a prediction from an LLM (e.g. classify XYZ text into A,B,C categories and provide a confidence score from 0-1) are gibberish and not really useful. &lt;/p&gt; &lt;p&gt;I see them used widely though and have since seen some mixed opinions on the idea.&lt;/p&gt; &lt;p&gt;While the scores are not useful in the same way a propensity is (after all itâ€™s just tokens), they are still indicative of some sort of confidence &lt;/p&gt; &lt;p&gt;Iâ€™ve also seen that using qualitative confidence e.g. Level of confidence: low, medium, high, is better than using numbers. &lt;/p&gt; &lt;p&gt;Just wondering whatâ€™s the latest school of thought on this and whether in practice you are using confidence scores in this way, and your observations about them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sg6128"&gt; /u/sg6128 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khfhoh/final_verdict_on_llm_generated_confidence_scores/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khfhoh/final_verdict_on_llm_generated_confidence_scores/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khfhoh/final_verdict_on_llm_generated_confidence_scores/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T02:32:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgrab2</id>
    <title>Self-improving AI unlocked?</title>
    <updated>2025-05-07T07:13:24+00:00</updated>
    <author>
      <name>/u/FeathersOfTheArrow</name>
      <uri>https://old.reddit.com/user/FeathersOfTheArrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Absolute Zero: Reinforced Self-play Reasoning with Zero Data&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Abstract:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. &lt;strong&gt;Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision&lt;/strong&gt;, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, &lt;strong&gt;we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples&lt;/strong&gt;. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2505.03335"&gt;Paper&lt;/a&gt; &lt;a href="https://x.com/AndrewZ45732491/status/1919920459748909288"&gt;Thread&lt;/a&gt; &lt;a href="https://github.com/LeapLabTHU/Absolute-Zero-Reasoner"&gt;GitHub&lt;/a&gt; &lt;a href="https://huggingface.co/papers/2505.03335"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeathersOfTheArrow"&gt; /u/FeathersOfTheArrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrab2/selfimproving_ai_unlocked/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrab2/selfimproving_ai_unlocked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrab2/selfimproving_ai_unlocked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T07:13:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgt8m5</id>
    <title>nanoVLM: A minimal Vision-Language Model with a LLaMA-style decoder â€” now open source</title>
    <updated>2025-05-07T09:37:59+00:00</updated>
    <author>
      <name>/u/zKingFrist</name>
      <uri>https://old.reddit.com/user/zKingFrist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all â€” we just open-sourced &lt;strong&gt;nanoVLM&lt;/strong&gt;, a lightweight Vision-Language Model (VLM) built from scratch in &lt;strong&gt;pure PyTorch&lt;/strong&gt;, with a &lt;strong&gt;LLaMA-style decoder&lt;/strong&gt;. It's designed to be simple, hackable, and easy to train â€” the full model is just ~750 lines of code.&lt;/p&gt; &lt;p&gt;Why it's interesting:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Achieves &lt;strong&gt;35.3% on MMStar&lt;/strong&gt; with only &lt;strong&gt;6 hours of training on a single H100,&lt;/strong&gt; matching SmolVLM-256M performance â€” but using 100x fewer GPU hours.&lt;/li&gt; &lt;li&gt;Can be trained in a &lt;strong&gt;free Google Colab notebook&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Great for learning, prototyping, or building your own VLMs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Architecture:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vision encoder: &lt;strong&gt;SigLiP-ViT&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Language decoder: &lt;strong&gt;LLaMA-style&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Modality projector connecting the two&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Inspired by nanoGPT, this is like the VLM version â€” compact and easy to understand. Would love to see someone try running this on local hardware or mixing it with other projects.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/huggingface/nanoVLM"&gt;https://github.com/huggingface/nanoVLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zKingFrist"&gt; /u/zKingFrist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgt8m5/nanovlm_a_minimal_visionlanguage_model_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgt8m5/nanovlm_a_minimal_visionlanguage_model_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgt8m5/nanovlm_a_minimal_visionlanguage_model_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T09:37:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh9ape</id>
    <title>Collection of LLM System Prompts</title>
    <updated>2025-05-07T21:34:34+00:00</updated>
    <author>
      <name>/u/Haunting-Stretch8069</name>
      <uri>https://old.reddit.com/user/Haunting-Stretch8069</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9ape/collection_of_llm_system_prompts/"&gt; &lt;img alt="Collection of LLM System Prompts" src="https://external-preview.redd.it/mXIp6Z--Pl_AS_wcxpDFLh4ENIm93uSILhbV_jZ63CM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06835e2d945227966f4734fc5037151786b32e51" title="Collection of LLM System Prompts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Haunting-Stretch8069"&gt; /u/Haunting-Stretch8069 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/guy915/LLM-System-Prompts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9ape/collection_of_llm_system_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9ape/collection_of_llm_system_prompts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T21:34:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh5f4q</id>
    <title>Beelink Launches GTR9 Pro And GTR9 AI Mini PCs, Featuring AMD Ryzen AI Max+ 395 And Up To 128 GB RAM</title>
    <updated>2025-05-07T18:57:06+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh5f4q/beelink_launches_gtr9_pro_and_gtr9_ai_mini_pcs/"&gt; &lt;img alt="Beelink Launches GTR9 Pro And GTR9 AI Mini PCs, Featuring AMD Ryzen AI Max+ 395 And Up To 128 GB RAM" src="https://external-preview.redd.it/5PkORNAHF4mCCL5YAzubG_UIVEwPjYPSSg6fvkNboCI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0822aa3d4e06f9272257151f7579577e8ecd98ba" title="Beelink Launches GTR9 Pro And GTR9 AI Mini PCs, Featuring AMD Ryzen AI Max+ 395 And Up To 128 GB RAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/beelink-launches-gtr9-pro-and-gtr9-mini-pcs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh5f4q/beelink_launches_gtr9_pro_and_gtr9_ai_mini_pcs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh5f4q/beelink_launches_gtr9_pro_and_gtr9_ai_mini_pcs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T18:57:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh5vrx</id>
    <title>Trying out the Ace-Step Song Generation Model</title>
    <updated>2025-05-07T19:15:23+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh5vrx/trying_out_the_acestep_song_generation_model/"&gt; &lt;img alt="Trying out the Ace-Step Song Generation Model" src="https://external-preview.redd.it/bDY3Zm5xNjd0ZXplMfhj0slUHjrXE-UilZ0dRUIJzmh3kn39RuiWSQcdWvp9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0b49332e389d6104e19b726896e9784113cd99d" title="Trying out the Ace-Step Song Generation Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I got Gemini to whip up some lyrics for an alphabet song, and then I used ACE-Step-v1-3.5B to generate a rock-style track at 105bpm. &lt;/p&gt; &lt;p&gt;Give it a listen â€“ how does it sound to you?&lt;/p&gt; &lt;p&gt;My feeling is that some of the transitions are still a bit off, and there are issues with the pronunciation of individual lyrics. But on the whole, it's not bad! I reckon it'd be pretty smooth for making those catchy, repetitive tunes (like that &amp;quot;Shawarma Legend&amp;quot; kind of vibe).&lt;br /&gt; This was generated on HuggingFace, took about 50 seconds.&lt;/p&gt; &lt;p&gt;What are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dfm1hq67teze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh5vrx/trying_out_the_acestep_song_generation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh5vrx/trying_out_the_acestep_song_generation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T19:15:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgzskq</id>
    <title>Mistral-Medium 3 (unfortunately no local support so far)</title>
    <updated>2025-05-07T15:12:17+00:00</updated>
    <author>
      <name>/u/pier4r</name>
      <uri>https://old.reddit.com/user/pier4r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzskq/mistralmedium_3_unfortunately_no_local_support_so/"&gt; &lt;img alt="Mistral-Medium 3 (unfortunately no local support so far)" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Mistral-Medium 3 (unfortunately no local support so far)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pier4r"&gt; /u/pier4r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/mistral-medium-3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzskq/mistralmedium_3_unfortunately_no_local_support_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzskq/mistralmedium_3_unfortunately_no_local_support_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T15:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kguqmd</id>
    <title>Apriel-Nemotron-15b-Thinker - o1mini level with MIT licence (Nvidia &amp; Servicenow)</title>
    <updated>2025-05-07T11:14:13+00:00</updated>
    <author>
      <name>/u/Temporary-Size7310</name>
      <uri>https://old.reddit.com/user/Temporary-Size7310</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kguqmd/aprielnemotron15bthinker_o1mini_level_with_mit/"&gt; &lt;img alt="Apriel-Nemotron-15b-Thinker - o1mini level with MIT licence (Nvidia &amp;amp; Servicenow)" src="https://external-preview.redd.it/EuRAXuyDLNOAu2-1ktV_-X31N5aAiqTZTPHiWEhPj-E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d4cc697d8897122cd61888ad7f02b892afd49c4" title="Apriel-Nemotron-15b-Thinker - o1mini level with MIT licence (Nvidia &amp;amp; Servicenow)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Service now and Nvidia brings a new 15B thinking model with comparable performance with 32B&lt;br /&gt; Model: &lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker"&gt;https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker&lt;/a&gt; (MIT licence)&lt;br /&gt; It looks very promising (resumed by Gemini) : &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Efficiency:&lt;/strong&gt; Claimed to be half the size of some SOTA models (like QWQ-32b, EXAONE-32b) and consumes significantly fewer tokens (~40% less than QWQ-32b) for comparable tasks, directly impacting VRAM requirements and inference costs for local or self-hosted setups.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning/Enterprise:&lt;/strong&gt; Reports strong performance on benchmarks like MBPP, BFCL, Enterprise RAG, IFEval, and Multi-Challenge. The focus on Enterprise RAG is notable for business-specific applications.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Coding:&lt;/strong&gt; Competitive results on coding tasks like MBPP and HumanEval, important for development workflows.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Academic:&lt;/strong&gt; Holds competitive scores on academic reasoning benchmarks (AIME, AMC, MATH, GPQA) relative to its parameter count.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; We need to test it&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary-Size7310"&gt; /u/Temporary-Size7310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kguqmd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kguqmd/aprielnemotron15bthinker_o1mini_level_with_mit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kguqmd/aprielnemotron15bthinker_o1mini_level_with_mit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T11:14:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgzey8</id>
    <title>Run FLUX.1 losslessly on a GPU with 20GB VRAM</title>
    <updated>2025-05-07T14:57:14+00:00</updated>
    <author>
      <name>/u/arty_photography</name>
      <uri>https://old.reddit.com/user/arty_photography</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've released &lt;strong&gt;losslessly compressed versions&lt;/strong&gt; of the &lt;strong&gt;12B FLUX.1-dev&lt;/strong&gt; and &lt;strong&gt;FLUX.1-schnell&lt;/strong&gt; models using &lt;strong&gt;DFloat11&lt;/strong&gt;, a compression method that applies entropy coding to BFloat16 weights. This reduces model size by &lt;strong&gt;~30%&lt;/strong&gt; &lt;em&gt;without changing outputs&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;This brings the models down from &lt;strong&gt;24GB to ~16.3GB&lt;/strong&gt;, enabling them to run on a &lt;strong&gt;single GPU with 20GB or more of VRAM&lt;/strong&gt;, with only a &lt;strong&gt;few seconds of extra overhead per image&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;ðŸ”— Downloads &amp;amp; Resources&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Compressed FLUX.1-dev&lt;/strong&gt;: &lt;a href="https://huggingface.co/DFloat11/FLUX.1-dev-DF11"&gt;huggingface.co/DFloat11/FLUX.1-dev-DF11&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compressed FLUX.1-schnell&lt;/strong&gt;: &lt;a href="https://huggingface.co/DFloat11/FLUX.1-schnell-DF11"&gt;huggingface.co/DFloat11/FLUX.1-schnell-DF11&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Example Code&lt;/strong&gt;: &lt;a href="https://github.com/LeanModels/DFloat11/tree/master/examples/flux.1"&gt;github.com/LeanModels/DFloat11/tree/master/examples/flux.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compressed LLMs (Qwen 3, Gemma 3, etc.)&lt;/strong&gt;: &lt;a href="https://huggingface.co/DFloat11"&gt;huggingface.co/DFloat11&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Research Paper&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2504.11651"&gt;arxiv.org/abs/2504.11651&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Feedback welcome&lt;/strong&gt;! Let me know if you try them out or run into any issues!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arty_photography"&gt; /u/arty_photography &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzey8/run_flux1_losslessly_on_a_gpu_with_20gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzey8/run_flux1_losslessly_on_a_gpu_with_20gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzey8/run_flux1_losslessly_on_a_gpu_with_20gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T14:57:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh6kh3</id>
    <title>Qwen3 MMLU-Pro Computer Science LLM Benchmark Results</title>
    <updated>2025-05-07T19:43:27+00:00</updated>
    <author>
      <name>/u/WolframRavenwolf</name>
      <uri>https://old.reddit.com/user/WolframRavenwolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh6kh3/qwen3_mmlupro_computer_science_llm_benchmark/"&gt; &lt;img alt="Qwen3 MMLU-Pro Computer Science LLM Benchmark Results" src="https://preview.redd.it/3yuv5m5qxeze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ed0dfa07bd3b2e9b138176b2104e26c7a51e6e4" title="Qwen3 MMLU-Pro Computer Science LLM Benchmark Results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally finished my extensive &lt;strong&gt;Qwen 3 evaluations&lt;/strong&gt; across a range of formats and quantisations, focusing on &lt;strong&gt;MMLU-Pro&lt;/strong&gt; (Computer Science).&lt;/p&gt; &lt;p&gt;A few take-aways stood out - especially for those interested in local deployment and performance trade-offs:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Qwen3-235B-A22B&lt;/strong&gt; (via Fireworks API) tops the table at &lt;strong&gt;83.66%&lt;/strong&gt; with ~55 tok/s.&lt;/li&gt; &lt;li&gt;But the &lt;strong&gt;30B-A3B Unsloth&lt;/strong&gt; quant delivered &lt;strong&gt;82.20%&lt;/strong&gt; while running locally at ~45 tok/s and with zero API spend.&lt;/li&gt; &lt;li&gt;The same Unsloth build is ~5x faster than Qwen's &lt;strong&gt;Qwen3-32B&lt;/strong&gt;, which scores &lt;strong&gt;82.20%&lt;/strong&gt; as well yet crawls at &amp;lt;10 tok/s.&lt;/li&gt; &lt;li&gt;On Apple silicon, the &lt;strong&gt;30B MLX&lt;/strong&gt; port hits &lt;strong&gt;79.51%&lt;/strong&gt; while sustaining ~64 tok/s - arguably today's best speed/quality trade-off for Mac setups.&lt;/li&gt; &lt;li&gt;The &lt;strong&gt;0.6B&lt;/strong&gt; micro-model races above 180 tok/s but tops out at &lt;strong&gt;37.56%&lt;/strong&gt; - that's why it's not even on the graph (50 % performance cut-off).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;All local runs were done with LM Studio on an M4 MacBook Pro, using Qwen's official recommended settings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt; Quantised 30B models now get you ~98 % of frontier-class accuracy - at a fraction of the latency, cost, and energy. For most local RAG or agent workloads, they're not just good enough - they're the new default.&lt;/p&gt; &lt;p&gt;Well done, Alibaba/Qwen - you really whipped the llama's ass! And to OpenAI: for your upcoming open model, please make it MoE, with toggleable reasoning, and release it in many sizes. &lt;em&gt;This&lt;/em&gt; is the future!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WolframRavenwolf"&gt; /u/WolframRavenwolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3yuv5m5qxeze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh6kh3/qwen3_mmlupro_computer_science_llm_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh6kh3/qwen3_mmlupro_computer_science_llm_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T19:43:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh3g7f</id>
    <title>Did anyone try out Mistral Medium 3?</title>
    <updated>2025-05-07T17:38:36+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh3g7f/did_anyone_try_out_mistral_medium_3/"&gt; &lt;img alt="Did anyone try out Mistral Medium 3?" src="https://external-preview.redd.it/Z3k2eW51bjJiZXplMcUDN_ixsC3ErmhxAmaSJ8XxFt_ddYdhD_A2seyNDJhw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=380661f2e13d59675a91c29257e8bd38b702503f" title="Did anyone try out Mistral Medium 3?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I briefly tried Mistral Medium 3 on OpenRouter, and I feel its performance might not be as good as Mistral's blog claims. (The video shows the best result out of the 5 shots I ran. ) &lt;/p&gt; &lt;p&gt;Additionally, I tested having it recognize and convert the benchmark image from the blog into JSON. However, it felt like it was just randomly converting things, and not a single field matched up. Could it be that its input resolution is very low, causing compression and therefore making it unable to recognize the text in the image? &lt;/p&gt; &lt;p&gt;Also, I don't quite understand why it uses 5-shot in the GPTQ diamond and MMLU Pro benchmarks. Is that the default number of shots for these tests?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6w9w0rl2beze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh3g7f/did_anyone_try_out_mistral_medium_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh3g7f/did_anyone_try_out_mistral_medium_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T17:38:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgrjor</id>
    <title>New ""Open-Source"" Video generation model</title>
    <updated>2025-05-07T07:32:32+00:00</updated>
    <author>
      <name>/u/topiga</name>
      <uri>https://old.reddit.com/user/topiga</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrjor/new_opensource_video_generation_model/"&gt; &lt;img alt="New &amp;quot;&amp;quot;Open-Source&amp;quot;&amp;quot; Video generation model" src="https://external-preview.redd.it/ZHdlOHlodmQ5YnplMXyf8-rvm1C__Q4bDL3gJBkjO_bjkyMUPsobX80FiZpA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6db55c8236c9c875dc6ec641feb87d228687bd65" title="New &amp;quot;&amp;quot;Open-Source&amp;quot;&amp;quot; Video generation model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LTX-Video is the first DiT-based video generation model that can generate high-quality videos in &lt;em&gt;real-time&lt;/em&gt;. It can generate 30 FPS videos at 1216Ã—704 resolution, faster than it takes to watch them. The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos with realistic and diverse content.&lt;/p&gt; &lt;p&gt;The model supports text-to-image, image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.&lt;/p&gt; &lt;p&gt;To be honest, I don't view it as open-source, not even open-weight. The license is weird, not a license we know of, and there's &amp;quot;Use Restrictions&amp;quot;. By doing so, it is NOT open-source.&lt;br /&gt; Yes, the restrictions are honest, and I invite you to read them, &lt;a href="https://static.lightricks.com/legal/LTXV-13b-0.9.7-dev.pdf"&gt;here is an example&lt;/a&gt;, but I think they're just doing this to protect themselves.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Lightricks/LTX-Video"&gt;https://github.com/Lightricks/LTX-Video&lt;/a&gt;&lt;br /&gt; HF: &lt;a href="https://huggingface.co/Lightricks/LTX-Video"&gt;https://huggingface.co/Lightricks/LTX-Video&lt;/a&gt; (FP8 coming soon)&lt;br /&gt; Documentation: &lt;a href="https://www.lightricks.com/ltxv-documentation"&gt;https://www.lightricks.com/ltxv-documentation&lt;/a&gt;&lt;br /&gt; Tweet: &lt;a href="https://x.com/LTXStudio/status/1919751150888239374"&gt;https://x.com/LTXStudio/status/1919751150888239374&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/topiga"&gt; /u/topiga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i4ioviud9bze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrjor/new_opensource_video_generation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrjor/new_opensource_video_generation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T07:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1khb7rs</id>
    <title>The new MLX DWQ quant is underrated, it feels like 8bit in a 4bit quant.</title>
    <updated>2025-05-07T22:59:41+00:00</updated>
    <author>
      <name>/u/mzbacd</name>
      <uri>https://old.reddit.com/user/mzbacd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed it was added to MLX a few days ago and started using it since then. It's very impressive, like running an 8bit model in a 4bit quantization size without much performance loss, and I suspect it might even finally make the 3bit quantization usable.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;edit:&lt;br /&gt; just made a DWQ quant one from unquantized version:&lt;br /&gt; &lt;a href="https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mzbacd"&gt; /u/mzbacd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khb7rs/the_new_mlx_dwq_quant_is_underrated_it_feels_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khb7rs/the_new_mlx_dwq_quant_is_underrated_it_feels_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khb7rs/the_new_mlx_dwq_quant_is_underrated_it_feels_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T22:59:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1khbxg4</id>
    <title>QwQ Appreciation Thread</title>
    <updated>2025-05-07T23:33:37+00:00</updated>
    <author>
      <name>/u/OmarBessa</name>
      <uri>https://old.reddit.com/user/OmarBessa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khbxg4/qwq_appreciation_thread/"&gt; &lt;img alt="QwQ Appreciation Thread" src="https://external-preview.redd.it/iUbtHN7RzxrcJ1LnOytJyYZIsd6RNnT0J4eou-hgYFg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c8377a20d6ad8107b227ddbef333fbae642705" title="QwQ Appreciation Thread" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/rpteerax2gze1.png?width=1257&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfa6469374a7c33544022408885845c33043e561"&gt;https://preview.redd.it/rpteerax2gze1.png?width=1257&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfa6469374a7c33544022408885845c33043e561&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Taken from: &lt;a href="https://fiction.live/stories/Fiction-liveBench-May-06-2025/oQdzQvKHw8JyXbN87"&gt;Regarding-the-Table-Design - Fiction-liveBench-May-06-2025 - Fiction.live&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I mean guys, don't get me wrong. The new Qwen3 models are great, but QwQ still holds quite decently. If it weren't for its overly verbose thinking...yet look at this. &lt;strong&gt;It is still basically sota in long context comprehension among open-source models.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OmarBessa"&gt; /u/OmarBessa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khbxg4/qwq_appreciation_thread/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khbxg4/qwq_appreciation_thread/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khbxg4/qwq_appreciation_thread/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T23:33:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1khgir9</id>
    <title>Is GLM-4 actually a hacked GEMINI? Or just Copying their Style?</title>
    <updated>2025-05-08T03:28:06+00:00</updated>
    <author>
      <name>/u/GrungeWerX</name>
      <uri>https://old.reddit.com/user/GrungeWerX</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khgir9/is_glm4_actually_a_hacked_gemini_or_just_copying/"&gt; &lt;img alt="Is GLM-4 actually a hacked GEMINI? Or just Copying their Style?" src="https://external-preview.redd.it/eNoMp-cBhW5B3lKr_XZAgD1Qku1SepqMDIM_aLwv22o.png?width=140&amp;amp;height=82&amp;amp;crop=140:82,smart&amp;amp;auto=webp&amp;amp;s=b758fb52ae557d8179140e36108651b61ff7d08a" title="Is GLM-4 actually a hacked GEMINI? Or just Copying their Style?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Am I the only person that's noticed that GLM-4's outputs are eerily similar to Gemini Pro 2.5 in formatting? I copy/pasted a prompt in several different SOTA LLMs - GPT-4, DeepSeek, Gemini 2.5 Pro, Claude 2.7, and Grok. Then I tried it in GLM-4, and was like, wait a minute, where have I seen this formatting before? Then I checked - it was in &lt;strong&gt;Gemini 2.5 Pro&lt;/strong&gt;. Now, I'm not saying that GLM-4 is Gemini 2.5 Pro, of course not, but could it be a hacked earlier version? Or perhaps (far more likely) they used it as a template for how GLM does its outputs? Because Gemini is the &lt;strong&gt;only&lt;/strong&gt; LLM that does it this way where it gives you three Options w/parentheticals describing tone, and then finalizes it by saying &amp;quot;Choose the option that best fits your tone&amp;quot;. Like, &lt;strong&gt;almost exactly the same.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I just tested it out on Gemini 2.0 and Gemini Flash. Neither of these versions do this. This is only done by Gemini 2.5 Pro and GLM-4. None of the other Closed-source LLMs do this either, like chat-gpt, grok, deepseek, or claude.&lt;/p&gt; &lt;p&gt;I'm not complaining. And if the Chinese were to somehow hack their LLM and released a quantized open source version to the world - despite how unlikely this is - I wouldn't protest...much. &amp;gt;.&amp;gt;&lt;/p&gt; &lt;p&gt;But jokes aside, anyone else notice this?&lt;/p&gt; &lt;p&gt;Some samples:&lt;/p&gt; &lt;p&gt;Gemini Pro 2.5&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xjw45f988hze1.png?width=1267&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c85206ec3f5ebc5288c1e559c3ac2e50acb26b9d"&gt;https://preview.redd.it/xjw45f988hze1.png?width=1267&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c85206ec3f5ebc5288c1e559c3ac2e50acb26b9d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM-4&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/alnqooqa8hze1.png?width=976&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc68a36fc81af2110ac82d979e493c2889eae93e"&gt;https://preview.redd.it/alnqooqa8hze1.png?width=976&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc68a36fc81af2110ac82d979e493c2889eae93e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Gemini Pro 2.5&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0ofz0ygd8hze1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc72427d8b0b60d02c5aca3f54ac0f1e287d1e05"&gt;https://preview.redd.it/0ofz0ygd8hze1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc72427d8b0b60d02c5aca3f54ac0f1e287d1e05&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM-4&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/igddncjf8hze1.png?width=895&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ab4b4fa2f7ebdcc3a3feff0f9abbf6ce4428b4f"&gt;https://preview.redd.it/igddncjf8hze1.png?width=895&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ab4b4fa2f7ebdcc3a3feff0f9abbf6ce4428b4f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GrungeWerX"&gt; /u/GrungeWerX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khgir9/is_glm4_actually_a_hacked_gemini_or_just_copying/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khgir9/is_glm4_actually_a_hacked_gemini_or_just_copying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khgir9/is_glm4_actually_a_hacked_gemini_or_just_copying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T03:28:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh0hcd</id>
    <title>Cracking 40% on SWE-bench verified with open source models &amp; agents &amp; open-source synth data</title>
    <updated>2025-05-07T15:39:46+00:00</updated>
    <author>
      <name>/u/klieret</name>
      <uri>https://old.reddit.com/user/klieret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh0hcd/cracking_40_on_swebench_verified_with_open_source/"&gt; &lt;img alt="Cracking 40% on SWE-bench verified with open source models &amp;amp; agents &amp;amp; open-source synth data" src="https://preview.redd.it/4lwtc2sgpdze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f581dfebc0968cbf87949bad4b08918a6afa989" title="Cracking 40% on SWE-bench verified with open source models &amp;amp; agents &amp;amp; open-source synth data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We all know that finetuning &amp;amp; RL work great for getting great LMs for agents -- the problem is where to get the training data!&lt;/p&gt; &lt;p&gt;We've generated 50k+ task instances for 128 popular GitHub repositories, then trained our own LM for SWE-agent. The result? We achieve 40% pass@1 on SWE-bench Verified -- a new SoTA among open source models.&lt;/p&gt; &lt;p&gt;We've open-sourced &lt;em&gt;everything&lt;/em&gt;, and we're excited to see what you build with it! This includes the agent (SWE-agent), the framework used to generate synthetic task instances (SWE-smith), and our fine-tuned LM (SWE-agent-LM-32B)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klieret"&gt; /u/klieret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4lwtc2sgpdze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh0hcd/cracking_40_on_swebench_verified_with_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh0hcd/cracking_40_on_swebench_verified_with_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T15:39:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh9018</id>
    <title>OpenCodeReasoning - new Nemotrons by NVIDIA</title>
    <updated>2025-05-07T21:22:11+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-7B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-14B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B-IOI"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B-IOI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9018/opencodereasoning_new_nemotrons_by_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9018/opencodereasoning_new_nemotrons_by_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9018/opencodereasoning_new_nemotrons_by_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T21:22:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh579e</id>
    <title>Qwen 3 evaluations</title>
    <updated>2025-05-07T18:48:14+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh579e/qwen_3_evaluations/"&gt; &lt;img alt="Qwen 3 evaluations" src="https://preview.redd.it/8f8g366goeze1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd68bf0ab81adb00446d201fbee1d90070c68389" title="Qwen 3 evaluations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally finished my extensive Qwen 3 evaluations across a range of formats and quantisations, focusing on MMLU-Pro (Computer Science).&lt;/p&gt; &lt;p&gt;A few take-aways stood out - especially for those interested in local deployment and performance trade-offs:&lt;/p&gt; &lt;p&gt;1ï¸âƒ£ Qwen3-235B-A22B (via Fireworks API) tops the table at 83.66% with ~55 tok/s.&lt;/p&gt; &lt;p&gt;2ï¸âƒ£ But the 30B-A3B Unsloth quant delivered 82.20% while running locally at ~45 tok/s and with zero API spend.&lt;/p&gt; &lt;p&gt;3ï¸âƒ£ The same Unsloth build is ~5x faster than Qwen's Qwen3-32B, which scores 82.20% as well yet crawls at &amp;lt;10 tok/s.&lt;/p&gt; &lt;p&gt;4ï¸âƒ£ On Apple silicon, the 30B MLX port hits 79.51% while sustaining ~64 tok/s - arguably today's best speed/quality trade-off for Mac setups.&lt;/p&gt; &lt;p&gt;5ï¸âƒ£ The 0.6B micro-model races above 180 tok/s but tops out at 37.56% - that's why it's not even on the graph (50 % performance cut-off).&lt;/p&gt; &lt;p&gt;All local runs were done with @lmstudio on an M4 MacBook Pro, using Qwen's official recommended settings.&lt;/p&gt; &lt;p&gt;Conclusion: Quantised 30B models now get you ~98 % of frontier-class accuracy - at a fraction of the latency, cost, and energy. For most local RAG or agent workloads, they're not just good enough - they're the new default.&lt;/p&gt; &lt;p&gt;Well done, @Alibaba_Qwen - you really whipped the llama's ass! And to @OpenAI: for your upcoming open model, please make it MoE, with toggleable reasoning, and release it in many sizes. This is the future!&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/wolframrvnwlf/status/1920186645384478955?s=46"&gt;https://x.com/wolframrvnwlf/status/1920186645384478955?s=46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8f8g366goeze1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh579e/qwen_3_evaluations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh579e/qwen_3_evaluations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T18:48:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgzwe9</id>
    <title>New mistral model benchmarks</title>
    <updated>2025-05-07T15:16:25+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzwe9/new_mistral_model_benchmarks/"&gt; &lt;img alt="New mistral model benchmarks" src="https://preview.redd.it/hrtrvrvnmdze1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a47a4a215c33b3670819e5b09e20d25a73074d7" title="New mistral model benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hrtrvrvnmdze1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzwe9/new_mistral_model_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzwe9/new_mistral_model_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T15:16:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1khjrtj</id>
    <title>Building LLM Workflows - - some observations</title>
    <updated>2025-05-08T06:54:56+00:00</updated>
    <author>
      <name>/u/noellarkin</name>
      <uri>https://old.reddit.com/user/noellarkin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on some relatively complex LLM workflows for the past year (not continuously, on and off). Here are some conclusions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Decomposing each task to the smallest steps and prompt chaining works far better than just using a single prompt with CoT. turning each step of the CoT into its own prompt and checking/sanitizing outputs reduces errors.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Using XML tags to structure the system prompt, prompt etc works best (IMO better than JSON structure but YMMV)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;You have to remind the LLM that its only job is to work as a semantic parser of sorts, to merely understand and transform the input data and NOT introduce data from its own &amp;quot;knowledge&amp;quot; into the output.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;NLTK, SpaCY, FlairNLP are often good ways to independently verify the output of an LLM (eg: check if the LLM's output has a sequence of POS tags you want etc). The great thing about these libraries is they're fast and reliable.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;ModernBERT classifiers are often just as good at LLMs if the task is small enough. Fine-tuned BERT-style classifiers are usually better than LLM for focused, narrow tasks.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;LLM-as-judge and LLM confidence scoring is extremely unreliable, especially if there's no &amp;quot;grounding&amp;quot; for how the score is to be arrived at. Scoring on vague parameters like &amp;quot;helpfulness&amp;quot; is useless - -eg: LLMs often conflate helpfulness with professional tone and length of response. Scoring has to either be grounded in multiple examples (which has its own problems - - LLMs may make the wrong inferences from example patterns), or a fine-tuned model is needed. If you're going to fine-tune for confidence scoring, might as well use a BERT model or something similar.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;In Agentic loops, the hardest part is setting up the conditions where the LLM exits the loop - - using the LLM to decide whether or not to exit is extremely unreliable (same reason as LLM-as-judge issues).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Performance usually degrades past 4k tokens (input context window) ... this is often only seen once you've run thousands of iterations. If you have a low error threshold, even a 5% failure rate in the pipeline is unacceptable, keeping all prompts below 4k tokens helps.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;32B models are good enough and reliable enough for most tasks, if the task is structured properly.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Structured CoT (with headings and bullet points) is often better than unstructured &lt;code&gt;&amp;lt;thinking&amp;gt;Okay, so I must...etc&lt;/code&gt; tokens. Structured and concise CoT stays within the context window (in the prompt as well as examples), and doesn't waste output tokens.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Self-consistency helps, but that also means running each prompt multiple times - - forces you to use smaller models and smaller prompts.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Writing your own CoT is better than relying on a reasoning model. Reasoning models are a good way to collect different CoT paths and ideas, and then synthesize your own.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The long-term plan is always to fine-tune everything. Start with a large API-based model and few-shot examples, and keep tweaking. Once the workflows are operational, consider creating fine-tuning datasets for some of the tasks so you can shift to a smaller local LLM or BERT. Making balanced datasets isn't easy.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;when making a dataset for fine-tuning, make it balanced by setting up a categorization system/orthogonal taxonomy so you can get complete coverage of the task. Use MECE framework.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've probably missed many points, these were the first ones that came to mind.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noellarkin"&gt; /u/noellarkin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khjrtj/building_llm_workflows_some_observations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khjrtj/building_llm_workflows_some_observations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khjrtj/building_llm_workflows_some_observations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T06:54:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1khbz70</id>
    <title>Intel to announce new Intel Arc Pro GPUs at Computex 2025 (May 20-23)</title>
    <updated>2025-05-07T23:35:56+00:00</updated>
    <author>
      <name>/u/eding42</name>
      <uri>https://old.reddit.com/user/eding42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khbz70/intel_to_announce_new_intel_arc_pro_gpus_at/"&gt; &lt;img alt="Intel to announce new Intel Arc Pro GPUs at Computex 2025 (May 20-23)" src="https://external-preview.redd.it/qU1K5b6t8nYIjVW3n7kpmgcB3rS2YYANfyJcs8RztyA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4675a9fdccab6a8f9da5be381fa2c1bd1fe534bf" title="Intel to announce new Intel Arc Pro GPUs at Computex 2025 (May 20-23)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Maybe the 24 GB Arc B580 model that got leaked will be announced? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eding42"&gt; /u/eding42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/intel/status/1920241029804064796"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khbz70/intel_to_announce_new_intel_arc_pro_gpus_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khbz70/intel_to_announce_new_intel_arc_pro_gpus_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T23:35:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh9qlx</id>
    <title>No local, no care.</title>
    <updated>2025-05-07T21:53:52+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9qlx/no_local_no_care/"&gt; &lt;img alt="No local, no care." src="https://preview.redd.it/f0l4hjmklfze1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ceb732f3829a0007aaaa683f507cd9116cadc51" title="No local, no care." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f0l4hjmklfze1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9qlx/no_local_no_care/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9qlx/no_local_no_care/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T21:53:52+00:00</published>
  </entry>
</feed>
