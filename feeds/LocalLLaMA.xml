<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-14T22:48:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jz8m81</id>
    <title>Agentic QwQ-32B perfect bouncing balls</title>
    <updated>2025-04-14T20:07:57+00:00</updated>
    <author>
      <name>/u/Specific-Rub-7250</name>
      <uri>https://old.reddit.com/user/Specific-Rub-7250</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz8m81/agentic_qwq32b_perfect_bouncing_balls/"&gt; &lt;img alt="Agentic QwQ-32B perfect bouncing balls" src="https://external-preview.redd.it/Py9phHWcxHcbI8kJ7_sE7LFb3Dt1cHJ_22Az93dDwZI.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f273eef5ca122fd23b45012f802218030085b0b" title="Agentic QwQ-32B perfect bouncing balls" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;QwQ still full of surprises...&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ssakar/examples/tree/main/QwQ-32B"&gt;https://github.com/ssakar/examples/tree/main/QwQ-32B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specific-Rub-7250"&gt; /u/Specific-Rub-7250 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=eBvKa4zaaCc&amp;amp;si=hEM-LF_p557bhgHz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz8m81/agentic_qwq32b_perfect_bouncing_balls/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz8m81/agentic_qwq32b_perfect_bouncing_balls/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T20:07:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz95oz</id>
    <title>Coding-Centric LLM Benchmark: Llama 4 Underwhelms</title>
    <updated>2025-04-14T20:29:39+00:00</updated>
    <author>
      <name>/u/jj_at_rootly</name>
      <uri>https://old.reddit.com/user/jj_at_rootly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We wanted to see for ourselves what Llama 4's performances for coding were like, and we were not impressed. Here is the benchmark methodology:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We sourced 100 issues labeled &amp;quot;bug&amp;quot; from the Mastodon GitHub repository.&lt;/li&gt; &lt;li&gt;For each issue, we collected the description and the associated pull request (PR) that solved it.&lt;/li&gt; &lt;li&gt;For benchmarking, we fed models each bug description and 4 PRs to choose from as the answer, with one of them being the PR that solved the issueâ€”no codebase context was included.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Findings&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;First, we wanted to test against leading multimodal models and replicate Meta's findings. Meta found in its benchmark that Llama 4 was beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding.&lt;/p&gt; &lt;p&gt;We could not reproduce Metaâ€™s findings on Llama outperforming GPT-4o, Gemini 2.0 Flash, and DeepSeek v3.1. On our benchmark, it came last in accuracy (69.5%), 6% less than the next best performing model (DeepSeek v3.1) and 18% behind the overall top-performing model (GPT-4o).&lt;/p&gt; &lt;p&gt;Second, we wanted to test against models designed for coding tasks: Alibaba Qwen2.5-Coder, OpenAI o3-mini, and Claude 3.5 Sonnet. Unsurprisingly, Llama 4 Maverick achieved only a 70% accuracy score. Alibabaâ€™s Qwen2.5-Coder-32B topped our rankings, closely followed by OpenAI's o3-mini, both of which achieved around 90% accuracy.&lt;/p&gt; &lt;p&gt;Llama 3.3 70 B-Versatile even outperformed the latest Llama 4 models by a small yet noticeable margin (72% accuracy).&lt;/p&gt; &lt;p&gt;Are those findings surprising to you? Any benchmark methodology details that may be disadvantageous to Llama models?&lt;/p&gt; &lt;p&gt;We shared the full findings here &lt;a href="https://rootly.com/blog/llama-4-underperforms-a-benchmark-against-coding-centric-models"&gt;https://rootly.com/blog/llama-4-underperforms-a-benchmark-against-coding-centric-models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And the dataset we used for the benchmark if you want to replicate or look closer at the dataset &lt;a href="https://github.com/Rootly-AI-Labs/GMCQ-benchmark"&gt;https://github.com/Rootly-AI-Labs/GMCQ-benchmark&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jj_at_rootly"&gt; /u/jj_at_rootly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz95oz/codingcentric_llm_benchmark_llama_4_underwhelms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz95oz/codingcentric_llm_benchmark_llama_4_underwhelms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz95oz/codingcentric_llm_benchmark_llama_4_underwhelms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T20:29:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzb7u7</id>
    <title>Three reasoning workflows - Tri, Grug, Polyglot</title>
    <updated>2025-04-14T21:57:43+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzb7u7/three_reasoning_workflows_tri_grug_polyglot/"&gt; &lt;img alt="Three reasoning workflows - Tri, Grug, Polyglot" src="https://b.thumbs.redditmedia.com/kBpOuXC1zUzlz_iKxeQls9DVt4fSwJincvy-sCPPYTc.jpg" title="Three reasoning workflows - Tri, Grug, Polyglot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's a small demo of the workflows in action:&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/PZDU9MpVYP8"&gt;https://youtu.be/PZDU9MpVYP8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Very sorry for a YouTube link, there was no way to add a native Reddit video to an image post)&lt;/p&gt; &lt;p&gt;In general, all three are directed at enclosing or redirecting the activation space during inference to be different from the most typical examples seen during the pre-training.&lt;/p&gt; &lt;p&gt;Code:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/av/harbor/blob/main/boost/src/custom_modules/tri.py"&gt;Tri&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/av/harbor/blob/main/boost/src/custom_modules/grug.py"&gt;Grug&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/av/harbor/blob/main/boost/src/custom_modules/polyglot.py"&gt;Polyglot&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jzb7u7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzb7u7/three_reasoning_workflows_tri_grug_polyglot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzb7u7/three_reasoning_workflows_tri_grug_polyglot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T21:57:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz4831</id>
    <title>I'm about to ask GPT-4.1: Which do you think is bigger, GPT-4.1 or GPT-4.5?</title>
    <updated>2025-04-14T17:12:30+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz4831/im_about_to_ask_gpt41_which_do_you_think_is/"&gt; &lt;img alt="I'm about to ask GPT-4.1: Which do you think is bigger, GPT-4.1 or GPT-4.5?" src="https://preview.redd.it/0kyux96m1uue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb8985a7bf40a9732f1c24c03b40e87ac3905667" title="I'm about to ask GPT-4.1: Which do you think is bigger, GPT-4.1 or GPT-4.5?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Or are you guys really talking about GPT-4.10?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0kyux96m1uue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz4831/im_about_to_ask_gpt41_which_do_you_think_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz4831/im_about_to_ask_gpt41_which_do_you_think_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T17:12:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyz4a1</id>
    <title>Kimina-Prover Preview - New SOTA on theorem proving 80.7% miniF2F</title>
    <updated>2025-04-14T13:39:42+00:00</updated>
    <author>
      <name>/u/frunkp</name>
      <uri>https://old.reddit.com/user/frunkp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyz4a1/kiminaprover_preview_new_sota_on_theorem_proving/"&gt; &lt;img alt="Kimina-Prover Preview - New SOTA on theorem proving 80.7% miniF2F" src="https://external-preview.redd.it/LUWPRYVOciUJ48L73KckvO0xvxcEDfER5j_R7LwvqHE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=03d8c19b4f8528a22fe82f93610c49e12edac5a4" title="Kimina-Prover Preview - New SOTA on theorem proving 80.7% miniF2F" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New SOTA of 80.7% for theorem proving on `miniF2F`! &lt;/p&gt; &lt;p&gt;Idea is to combine reasoning models (o1/r1-style) with formal maths (Lean 4) and apply RL to get human-readable proofs.&lt;/p&gt; &lt;p&gt;Distilled Kimina-Prover 1.5B &amp;amp; 7B models on &lt;a href="https://huggingface.co/collections/AI-MO/kimina-prover-preview-67fb536b883d60e7ca25d7f9"&gt;ðŸ¤— Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5hxdploeysue1.png?width=1590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=81f9b08c6e6eb2382c7eecb53bd589e0f2c3e3cd"&gt;https://preview.redd.it/5hxdploeysue1.png?width=1590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=81f9b08c6e6eb2382c7eecb53bd589e0f2c3e3cd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;IMO 1968 P5 (1st part) solution found by Kimina-Prover:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/96slg6sszsue1.png?width=1654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52904f263895c9f13318e3c9fb1933855aa4c4f8"&gt;https://preview.redd.it/96slg6sszsue1.png?width=1654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52904f263895c9f13318e3c9fb1933855aa4c4f8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ns8p29lwzsue1.png?width=1652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=039dfa8aab4bc272b8578642502e1a9eb33e6aeb"&gt;https://preview.redd.it/ns8p29lwzsue1.png?width=1652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=039dfa8aab4bc272b8578642502e1a9eb33e6aeb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ðŸ“‘ Technical report: &lt;a href="https://github.com/MoonshotAI/Kimina-Prover-Preview/blob/master/Kimina_Prover_Preview.pdf"&gt;Kimina_Prover_Preview.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ðŸ¤— Models: &lt;a href="https://huggingface.co/collections/AI-MO/kimina-prover-preview-67fb536b883d60e7ca25d7f9"&gt;AI-MO/kimina-prover-preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frunkp"&gt; /u/frunkp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyz4a1/kiminaprover_preview_new_sota_on_theorem_proving/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyz4a1/kiminaprover_preview_new_sota_on_theorem_proving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyz4a1/kiminaprover_preview_new_sota_on_theorem_proving/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T13:39:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyzvcr</id>
    <title>New Tutorial on GitHub - Build an AI Agent with MCP</title>
    <updated>2025-04-14T14:13:16+00:00</updated>
    <author>
      <name>/u/Nir777</name>
      <uri>https://old.reddit.com/user/Nir777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This tutorial walks you through: Building your own MCP server with real tools (like crypto price lookup) Connecting it to Claude Desktop and also creating your own custom agent Making the agent reason when to use which tool, execute it, and explain the result what's inside:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Practical Implementation of MCP from Scratch&lt;/li&gt; &lt;li&gt;End-to-End Custom Agent with Full MCP Stack&lt;/li&gt; &lt;li&gt;Dynamic Tool Discovery and Execution Pipeline&lt;/li&gt; &lt;li&gt;Seamless Claude 3.5 Integration&lt;/li&gt; &lt;li&gt;Interactive Chat Loop with Stateful Context&lt;/li&gt; &lt;li&gt;Educational and Reusable Code Architecture&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Link to the tutorial:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/NirDiamant/GenAI_Agents/blob/main/all_agents_tutorials/mcp-tutorial.ipynb"&gt;https://github.com/NirDiamant/GenAI_Agents/blob/main/all_agents_tutorials/mcp-tutorial.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;enjoy :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nir777"&gt; /u/Nir777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyzvcr/new_tutorial_on_github_build_an_ai_agent_with_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyzvcr/new_tutorial_on_github_build_an_ai_agent_with_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyzvcr/new_tutorial_on_github_build_an_ai_agent_with_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T14:13:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyp2no</id>
    <title>If we had models like QwQ-32B and Gemma-3-27B two years ago, people would have gone crazy.</title>
    <updated>2025-04-14T03:04:47+00:00</updated>
    <author>
      <name>/u/Proud_Fox_684</name>
      <uri>https://old.reddit.com/user/Proud_Fox_684</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Imagine if we had QwQ-32B or Gemma-3-27B or some of the smaller models, 18-24 months ago. It would have been the craziest thing.&lt;/p&gt; &lt;p&gt;24 months ago, GPT-4 was released. GPT-4o was released 11 months ago. Sometimes we not only forgot how quick things have been moving, but we also forget how good these small models actually are.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proud_Fox_684"&gt; /u/Proud_Fox_684 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyp2no/if_we_had_models_like_qwq32b_and_gemma327b_two/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyp2no/if_we_had_models_like_qwq32b_and_gemma327b_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyp2no/if_we_had_models_like_qwq32b_and_gemma327b_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T03:04:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyvzqg</id>
    <title>GLM-4-0414 (9B/32B) (w. &amp; wo. reasoning) Ready to Release</title>
    <updated>2025-04-14T10:55:40+00:00</updated>
    <author>
      <name>/u/NeterOster</name>
      <uri>https://old.reddit.com/user/NeterOster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyvzqg/glm40414_9b32b_w_wo_reasoning_ready_to_release/"&gt; &lt;img alt="GLM-4-0414 (9B/32B) (w. &amp;amp; wo. reasoning) Ready to Release" src="https://external-preview.redd.it/rUMQwGzzv049_AQ65R_I2zx8r9Fk1GPQDozFx082Elc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71971286a2d2292f2a0a2b67094dc5e3c3a4b46e" title="GLM-4-0414 (9B/32B) (w. &amp;amp; wo. reasoning) Ready to Release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems the developer is making final preparations : &lt;a href="https://github.com/zRzRzRzRzRzRzR/GLM-4"&gt;https://github.com/zRzRzRzRzRzRzR/GLM-4&lt;/a&gt; (note this is developer's fork, only for reference. Also note: some benchmarks in the page are from old versions of GLM model)&lt;/p&gt; &lt;p&gt;Huggingface collection is created (but empty for now): &lt;a href="https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e"&gt;https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The release contains following models:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6j2pwsl17sue1.png?width=943&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55349ae54f8626f4a068dde1f33b750d87236395"&gt;https://preview.redd.it/6j2pwsl17sue1.png?width=943&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55349ae54f8626f4a068dde1f33b750d87236395&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeterOster"&gt; /u/NeterOster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyvzqg/glm40414_9b32b_w_wo_reasoning_ready_to_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyvzqg/glm40414_9b32b_w_wo_reasoning_ready_to_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyvzqg/glm40414_9b32b_w_wo_reasoning_ready_to_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T10:55:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz4tx9</id>
    <title>the new LLM meta is watching tech influencers get one-shot by benchmark jpegs</title>
    <updated>2025-04-14T17:37:14+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz4tx9/the_new_llm_meta_is_watching_tech_influencers_get/"&gt; &lt;img alt="the new LLM meta is watching tech influencers get one-shot by benchmark jpegs" src="https://preview.redd.it/ku1z50vm6uue1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e9ddf5705625eeb0faf0812fc229a578087ecdd" title="the new LLM meta is watching tech influencers get one-shot by benchmark jpegs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ku1z50vm6uue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz4tx9/the_new_llm_meta_is_watching_tech_influencers_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz4tx9/the_new_llm_meta_is_watching_tech_influencers_get/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T17:37:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz2tol</id>
    <title>GLM-4-0414 - a THUDM Collection</title>
    <updated>2025-04-14T16:15:11+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz2tol/glm40414_a_thudm_collection/"&gt; &lt;img alt="GLM-4-0414 - a THUDM Collection" src="https://external-preview.redd.it/CbrIZBC-MoAMjzDIDvad-loXR06ele61H5F_oGZcxJQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d2f468a7cb4f1f63cdc2c35347ae9f9d3abd7d3e" title="GLM-4-0414 - a THUDM Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz2tol/glm40414_a_thudm_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz2tol/glm40414_a_thudm_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T16:15:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jysiwc</id>
    <title>DeepSeek will open-source parts of its inference engine â€” sharing standalone features and optimizations instead of the full stack</title>
    <updated>2025-04-14T06:45:54+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jysiwc/deepseek_will_opensource_parts_of_its_inference/"&gt; &lt;img alt="DeepSeek will open-source parts of its inference engine â€” sharing standalone features and optimizations instead of the full stack" src="https://external-preview.redd.it/-j5EXG21mJ1IrGfaacZfdTPmLfMidR-DBjShQEW0nM4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8674fe0d9158595daad240e374a62be90da4c4d6" title="DeepSeek will open-source parts of its inference engine â€” sharing standalone features and optimizations instead of the full stack" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/deepseek-ai/open-infra-index/blob/main/OpenSourcing_DeepSeek_Inference_Engine/README.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jysiwc/deepseek_will_opensource_parts_of_its_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jysiwc/deepseek_will_opensource_parts_of_its_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T06:45:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz8q7a</id>
    <title>OpenAI - Wen open source tho?</title>
    <updated>2025-04-14T20:12:17+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you think, will an OpenAI model really see the light of day soon enough? Do we have any info on when that could be?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz8q7a/openai_wen_open_source_tho/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz8q7a/openai_wen_open_source_tho/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz8q7a/openai_wen_open_source_tho/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T20:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jywg95</id>
    <title>Why is Qwen 2.5 Omni not being talked about enough?</title>
    <updated>2025-04-14T11:23:03+00:00</updated>
    <author>
      <name>/u/BeetranD</name>
      <uri>https://old.reddit.com/user/BeetranD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think the Qwen models are pretty good, I've been using a lot of them locally.&lt;br /&gt; They recently (a week or some ago) released 2.5 Omni, which is a 7B real-time multimodal model, that simultaneously generates text and natural speech. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B"&gt;Qwen/Qwen2.5-Omni-7B Â· Hugging Face&lt;/a&gt;&lt;br /&gt; I think It would be great to use for something like a local AI alexa clone. But on youtube there's almost no one testing it, and even here, not a lot of people talking about it.&lt;/p&gt; &lt;p&gt;What is it?? Am I over-expecting from this model? or I'm just not well informed about alternatives, please enlighten me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BeetranD"&gt; /u/BeetranD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jywg95/why_is_qwen_25_omni_not_being_talked_about_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jywg95/why_is_qwen_25_omni_not_being_talked_about_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jywg95/why_is_qwen_25_omni_not_being_talked_about_enough/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T11:23:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz3gzd</id>
    <title>GLM-4-0414 Series Model Released!</title>
    <updated>2025-04-14T16:41:53+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz3gzd/glm40414_series_model_released/"&gt; &lt;img alt="GLM-4-0414 Series Model Released!" src="https://preview.redd.it/sr09xoehwtue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5be750e141b5100afa3a2f71eb779ee767f9fe3c" title="GLM-4-0414 Series Model Released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on official data, does GLM-4-32B-0414 outperform DeepSeek-V3-0324 and DeepSeek-R1?&lt;/p&gt; &lt;p&gt;Github Repo: &lt;a href="http://github.com/THUDM/GLM-4"&gt;github.com/THUDM/GLM-4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="http://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e"&gt;huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sr09xoehwtue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz3gzd/glm40414_series_model_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz3gzd/glm40414_series_model_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T16:41:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz4mqg</id>
    <title>Quasar Alpha = GPT-4.1</title>
    <updated>2025-04-14T17:29:09+00:00</updated>
    <author>
      <name>/u/Spirited_Salad7</name>
      <uri>https://old.reddit.com/user/Spirited_Salad7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz4mqg/quasar_alpha_gpt41/"&gt; &lt;img alt="Quasar Alpha = GPT-4.1" src="https://preview.redd.it/urj2uow45uue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=743d81bf34988394265f244939f1268d7a28bad0" title="Quasar Alpha = GPT-4.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spirited_Salad7"&gt; /u/Spirited_Salad7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/urj2uow45uue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz4mqg/quasar_alpha_gpt41/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz4mqg/quasar_alpha_gpt41/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T17:29:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz421n</id>
    <title>Drummer's Rivermindâ„¢ 12B v1, the next-generation AI thatâ€™s redefining human-machine interaction! The future is here.</title>
    <updated>2025-04-14T17:05:32+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz421n/drummers_rivermind_12b_v1_the_nextgeneration_ai/"&gt; &lt;img alt="Drummer's Rivermindâ„¢ 12B v1, the next-generation AI thatâ€™s redefining human-machine interaction! The future is here." src="https://external-preview.redd.it/TBod4kQesTgLYyjaFaYSK8iNonKB2zVdwF9pMmcEbgY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a7fd1aeef490d800d9fd946e2abe5ec70682948" title="Drummer's Rivermindâ„¢ 12B v1, the next-generation AI thatâ€™s redefining human-machine interaction! The future is here." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Rivermind-12B-v1-GGUF"&gt;https://huggingface.co/TheDrummer/Rivermind-12B-v1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Rivermind-12B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz421n/drummers_rivermind_12b_v1_the_nextgeneration_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz421n/drummers_rivermind_12b_v1_the_nextgeneration_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T17:05:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz80f1</id>
    <title>I benchmarked 7 OCR solutions on a complex academic document (with images, tables, footnotes...)</title>
    <updated>2025-04-14T19:43:56+00:00</updated>
    <author>
      <name>/u/coconautico</name>
      <uri>https://old.reddit.com/user/coconautico</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz80f1/i_benchmarked_7_ocr_solutions_on_a_complex/"&gt; &lt;img alt="I benchmarked 7 OCR solutions on a complex academic document (with images, tables, footnotes...)" src="https://b.thumbs.redditmedia.com/q-Pn0SmLZQ_hi8NQ8NNJfqDfuG1Jn_CjohIa15bdh0w.jpg" title="I benchmarked 7 OCR solutions on a complex academic document (with images, tables, footnotes...)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran a &lt;strong&gt;comparison of 7 different OCR solutions&lt;/strong&gt; using the &lt;a href="https://arxiv.org/pdf/2310.06825"&gt;Mistral 7B paper&lt;/a&gt; as a reference document (pdf), which I found complex enough to properly stress-test these tools. It's the same paper used in the team's Jupyter notebook, but whatever. &lt;strong&gt;The document includes footnotes, tables, figures, math, page numbers&lt;/strong&gt;,... making it a solid candidate to test how well these tools handle real-world complexity.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Convert a PDF document into a well-structured Markdown file, preserving text formatting, figures, tables and equations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results (Ranked):&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;MistralAPI [cloud]&lt;/strong&gt; â†’ &lt;strong&gt;BEST&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Marker + Gemini&lt;/strong&gt; (--use_llm flag) &lt;strong&gt;[cloud]&lt;/strong&gt; â†’ &lt;strong&gt;VERY GOOD&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Marker / Docling [local]&lt;/strong&gt; â†’ &lt;strong&gt;GOOD&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PyMuPDF4LLM [local]&lt;/strong&gt; â†’ &lt;strong&gt;OKAY&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 2.5 Pro [cloud]&lt;/strong&gt; â†’ &lt;strong&gt;BEST* (...but doesn't extract images)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Markitdown (without AzureAI) [local]&lt;/strong&gt; â†’ &lt;strong&gt;POOR* (doesn't extract images)&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;OCR images to compare:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g0ihgjgpruue1.png?width=5738&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94537b4d1073286c7570d8739c512bb43f4fd8aa"&gt;OCR comparison for: Mistral, Marker+Gemini, Marker, Docling, PyMuPDF4LLM, Gemini 2.5 Pro, and Markitdown&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links to tools:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://mistral.ai/news/mistral-ocr"&gt;MistralOCR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/VikParuchuri/marker"&gt;Marker&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-pro"&gt;Gemini 2.5 Pro&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/docling-project/docling"&gt;Docling&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/microsoft/markitdown"&gt;Markitdown&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pymupdf.readthedocs.io/en/latest/pymupdf4llm/"&gt;PyMuPDF4LLM&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coconautico"&gt; /u/coconautico &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz80f1/i_benchmarked_7_ocr_solutions_on_a_complex/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz80f1/i_benchmarked_7_ocr_solutions_on_a_complex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz80f1/i_benchmarked_7_ocr_solutions_on_a_complex/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T19:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz624j</id>
    <title>DeepSeek V3's strong standing here makes you wonder what v4/R2 could achieve.</title>
    <updated>2025-04-14T18:26:32+00:00</updated>
    <author>
      <name>/u/mw11n19</name>
      <uri>https://old.reddit.com/user/mw11n19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz624j/deepseek_v3s_strong_standing_here_makes_you/"&gt; &lt;img alt="DeepSeek V3's strong standing here makes you wonder what v4/R2 could achieve." src="https://preview.redd.it/tlcxh6pffuue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9761b09a84b56f8b9ad25c4ee42a925420e4fe96" title="DeepSeek V3's strong standing here makes you wonder what v4/R2 could achieve." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mw11n19"&gt; /u/mw11n19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tlcxh6pffuue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz624j/deepseek_v3s_strong_standing_here_makes_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz624j/deepseek_v3s_strong_standing_here_makes_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T18:26:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyu06v</id>
    <title>llama was so deep that now ex employee saying that we r not involved in that project</title>
    <updated>2025-04-14T08:36:06+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyu06v/llama_was_so_deep_that_now_ex_employee_saying/"&gt; &lt;img alt="llama was so deep that now ex employee saying that we r not involved in that project" src="https://preview.redd.it/49mfsia3irue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3266a093713e9cb503b3634a7a8b1f7fb0852f0" title="llama was so deep that now ex employee saying that we r not involved in that project" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/49mfsia3irue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyu06v/llama_was_so_deep_that_now_ex_employee_saying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyu06v/llama_was_so_deep_that_now_ex_employee_saying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T08:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyzl0g</id>
    <title>DGX B200 Startup ASMR</title>
    <updated>2025-04-14T14:00:52+00:00</updated>
    <author>
      <name>/u/Chemical-Mixture3481</name>
      <uri>https://old.reddit.com/user/Chemical-Mixture3481</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyzl0g/dgx_b200_startup_asmr/"&gt; &lt;img alt="DGX B200 Startup ASMR" src="https://external-preview.redd.it/YTF4eTdsdnozdHVlMTsTLvzMSe_uV5dg8VNzSYJEyMCa9wyDSSGv4dzqg19H.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=635d486d1d0cd25569bc0a249087d4f5360ec61e" title="DGX B200 Startup ASMR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just installed one of these beasts in our datacenter. Since I could not find a video that shows one of these machines running with original sound here you go!&lt;/p&gt; &lt;p&gt;Thats probably ~110dB of fan noise given that the previous generation was at around 106dB according to Nvidia. Cooling 1kW GPUs seems to be no joke given that this machine sounds like a fighter jet starting its engines next to you :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chemical-Mixture3481"&gt; /u/Chemical-Mixture3481 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yy6c2lvz3tue1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyzl0g/dgx_b200_startup_asmr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyzl0g/dgx_b200_startup_asmr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T14:00:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz1oxv</id>
    <title>NVIDIA has published new Nemotrons!</title>
    <updated>2025-04-14T15:29:07+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;what a week....!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-H-56B-Base-8K"&gt;https://huggingface.co/nvidia/Nemotron-H-56B-Base-8K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-H-47B-Base-8K"&gt;https://huggingface.co/nvidia/Nemotron-H-47B-Base-8K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-H-8B-Base-8K"&gt;https://huggingface.co/nvidia/Nemotron-H-8B-Base-8K&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz1oxv/nvidia_has_published_new_nemotrons/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz1oxv/nvidia_has_published_new_nemotrons/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz1oxv/nvidia_has_published_new_nemotrons/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T15:29:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz6rwj</id>
    <title>OpenAI released a new Prompting Cookbook with GPT 4.1</title>
    <updated>2025-04-14T18:54:15+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz6rwj/openai_released_a_new_prompting_cookbook_with_gpt/"&gt; &lt;img alt="OpenAI released a new Prompting Cookbook with GPT 4.1" src="https://external-preview.redd.it/khZZDaErUPszuYMOAnI0g6ZacxmX2AdST6xS5QZoW9g.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a79822bf5ec27d84d21f68af9b0b6792aee1dada" title="OpenAI released a new Prompting Cookbook with GPT 4.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cookbook.openai.com/examples/gpt4-1_prompting_guide"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz6rwj/openai_released_a_new_prompting_cookbook_with_gpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz6rwj/openai_released_a_new_prompting_cookbook_with_gpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T18:54:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz2iuc</id>
    <title>glm-4 0414 is out. 9b, 32b, with and without reasoning and rumination</title>
    <updated>2025-04-14T16:02:56+00:00</updated>
    <author>
      <name>/u/matteogeniaccio</name>
      <uri>https://old.reddit.com/user/matteogeniaccio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz2iuc/glm4_0414_is_out_9b_32b_with_and_without/"&gt; &lt;img alt="glm-4 0414 is out. 9b, 32b, with and without reasoning and rumination" src="https://external-preview.redd.it/CbrIZBC-MoAMjzDIDvad-loXR06ele61H5F_oGZcxJQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d2f468a7cb4f1f63cdc2c35347ae9f9d3abd7d3e" title="glm-4 0414 is out. 9b, 32b, with and without reasoning and rumination" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e"&gt;https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;6 new models and interesting benchmarks&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;GLM-Z1-32B-0414&lt;/strong&gt; is a reasoning model with deep thinking capabilities. This was developed based on GLM-4-32B-0414 through cold start, extended reinforcement learning, and further training on tasks including mathematics, code, and logic. Compared to the base model, GLM-Z1-32B-0414 significantly improves mathematical abilities and the capability to solve complex tasks. During training, we also introduced general reinforcement learning based on pairwise ranking feedback, which enhances the model's general capabilities.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GLM-Z1-Rumination-32B-0414&lt;/strong&gt; is a deep reasoning model with rumination capabilities (against OpenAI's Deep Research). Unlike typical deep thinking models, the rumination model is capable of deeper and longer thinking to solve more open-ended and complex problems (e.g., writing a comparative analysis of AI development in two cities and their future development plans). Z1-Rumination is trained through scaling end-to-end reinforcement learning with responses graded by the ground truth answers or rubrics and can make use of search tools during its deep thinking process to handle complex tasks. The model shows significant improvements in research-style writing and complex tasks.&lt;/p&gt; &lt;p&gt;Finally, &lt;strong&gt;GLM-Z1-9B-0414&lt;/strong&gt; is a surprise. We employed all the aforementioned techniques to train a small model (9B). GLM-Z1-9B-0414 exhibits excellent capabilities in mathematical reasoning and general tasks. Its overall performance is top-ranked among all open-source models of the same size. Especially in resource-constrained scenarios, this model achieves an excellent balance between efficiency and effectiveness, providing a powerful option for users seeking lightweight deployment.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1jz2iuc/video/t1b3wsidqtue1/player"&gt;write a Python program that shows a ball bouncing inside a spinning hexagon. The ball should be affected by gravity and friction, and it must bounce off the rotating walls realistically &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sk32ghamqtue1.png?width=836&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=080510c78ea7272379a4cfe5a23581e740301f9b"&gt;https://preview.redd.it/sk32ghamqtue1.png?width=836&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=080510c78ea7272379a4cfe5a23581e740301f9b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1sjbnbboqtue1.png?width=682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=60b42679e45242651bf1d91276c1d581f67839d1"&gt;https://preview.redd.it/1sjbnbboqtue1.png?width=682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=60b42679e45242651bf1d91276c1d581f67839d1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matteogeniaccio"&gt; /u/matteogeniaccio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz2iuc/glm4_0414_is_out_9b_32b_with_and_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz2iuc/glm4_0414_is_out_9b_32b_with_and_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz2iuc/glm4_0414_is_out_9b_32b_with_and_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T16:02:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jytw62</id>
    <title>DeepSeek is about to open-source their inference engine</title>
    <updated>2025-04-14T08:27:29+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jytw62/deepseek_is_about_to_opensource_their_inference/"&gt; &lt;img alt="DeepSeek is about to open-source their inference engine" src="https://preview.redd.it/1am95yongrue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=967ad74640babe443b3c9a2867547f568219bda6" title="DeepSeek is about to open-source their inference engine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek is about to open-source their inference engine, which is a modified version based on vLLM. Now, DeepSeek is preparing to contribute these modifications back to the community.&lt;/p&gt; &lt;p&gt;I really like the last sentence: 'with the goal of enabling the community to achieve state-of-the-art (SOTA) support from Day-0.'&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/deepseek-ai/open-infra-index/tree/main/OpenSourcing_DeepSeek_Inference_Engine"&gt;https://github.com/deepseek-ai/open-infra-index/tree/main/OpenSourcing_DeepSeek_Inference_Engine&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1am95yongrue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jytw62/deepseek_is_about_to_opensource_their_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jytw62/deepseek_is_about_to_opensource_their_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T08:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz5i8h</id>
    <title>Which model listened to you the best</title>
    <updated>2025-04-14T18:04:29+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz5i8h/which_model_listened_to_you_the_best/"&gt; &lt;img alt="Which model listened to you the best" src="https://preview.redd.it/r537cvlobuue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d53c87f9cd66558adbfaf7e405bbd3f001354427" title="Which model listened to you the best" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r537cvlobuue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz5i8h/which_model_listened_to_you_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz5i8h/which_model_listened_to_you_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T18:04:29+00:00</published>
  </entry>
</feed>
