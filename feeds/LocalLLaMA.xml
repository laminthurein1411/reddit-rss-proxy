<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-18T20:24:34+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jdgqcj</id>
    <title>NEW MISTRAL JUST DROPPED</title>
    <updated>2025-03-17T16:23:29+00:00</updated>
    <author>
      <name>/u/Straight-Worker-4327</name>
      <uri>https://old.reddit.com/user/Straight-Worker-4327</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgqcj/new_mistral_just_dropped/"&gt; &lt;img alt="NEW MISTRAL JUST DROPPED" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="NEW MISTRAL JUST DROPPED" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Outperforms&lt;/strong&gt; GPT-4o Mini, Claude-3.5 Haiku, and others in text, vision, and multilingual tasks.&lt;br /&gt; &lt;strong&gt;128k context window&lt;/strong&gt;, blazing &lt;strong&gt;150 tokens/sec speed&lt;/strong&gt;, and runs on a &lt;strong&gt;single RTX 4090&lt;/strong&gt; or &lt;strong&gt;Mac (32GB RAM)&lt;/strong&gt;.&lt;br /&gt; &lt;strong&gt;Apache 2.0 license&lt;/strong&gt;‚Äîfree to use, fine-tune, and deploy. Handles chatbots, docs, images, and coding.&lt;/p&gt; &lt;p&gt;&lt;a href="https://mistral.ai/fr/news/mistral-small-3-1"&gt;https://mistral.ai/fr/news/mistral-small-3-1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"&gt;https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Straight-Worker-4327"&gt; /u/Straight-Worker-4327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgqcj/new_mistral_just_dropped/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgqcj/new_mistral_just_dropped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgqcj/new_mistral_just_dropped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T16:23:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1je43n5</id>
    <title>A bit spooky... :-D</title>
    <updated>2025-03-18T12:46:20+00:00</updated>
    <author>
      <name>/u/Salty-Garage7777</name>
      <uri>https://old.reddit.com/user/Salty-Garage7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je43n5/a_bit_spooky_d/"&gt; &lt;img alt="A bit spooky... :-D" src="https://b.thumbs.redditmedia.com/UUIuwRC3OOTYRzzrEoLPMx6UcaxLLy3fUPEBqIlFQsk.jpg" title="A bit spooky... :-D" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have never seen something like it, very interesting vision of a the output of the phpinfo() method.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3qiy1piw2gpe1.png?width=3537&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c33a252814f77a10114b8b465bc3250f0a434ac"&gt;https://preview.redd.it/3qiy1piw2gpe1.png?width=3537&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c33a252814f77a10114b8b465bc3250f0a434ac&lt;/a&gt;&lt;/p&gt; &lt;p&gt;:-) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salty-Garage7777"&gt; /u/Salty-Garage7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je43n5/a_bit_spooky_d/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je43n5/a_bit_spooky_d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je43n5/a_bit_spooky_d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T12:46:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jedolw</id>
    <title>NVIDIA DGX Station (and digits officially branded DGX Spark)</title>
    <updated>2025-03-18T19:34:16+00:00</updated>
    <author>
      <name>/u/HixVAC</name>
      <uri>https://old.reddit.com/user/HixVAC</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedolw/nvidia_dgx_station_and_digits_officially_branded/"&gt; &lt;img alt="NVIDIA DGX Station (and digits officially branded DGX Spark)" src="https://external-preview.redd.it/Fdel2GSTSgP6hWqJyE6FMJmHFZF9WWafAqElhAPDZl8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a540f20a28a9c0ddb3e27a2d33afdcbac9c64c7" title="NVIDIA DGX Station (and digits officially branded DGX Spark)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HixVAC"&gt; /u/HixVAC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nvidianews.nvidia.com/news/nvidia-announces-dgx-spark-and-dgx-station-personal-ai-computers"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedolw/nvidia_dgx_station_and_digits_officially_branded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jedolw/nvidia_dgx_station_and_digits_officially_branded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:34:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1je1cus</id>
    <title>Gemma3 disappointment post</title>
    <updated>2025-03-18T09:56:52+00:00</updated>
    <author>
      <name>/u/EntertainmentBroad43</name>
      <uri>https://old.reddit.com/user/EntertainmentBroad43</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma2 was very good, but gemma3 27b just feels mediocre for STEM (finding inconsistent numbers in a medical paper). &lt;/p&gt; &lt;p&gt;I found Mistral small 3 and even phi-4 better than gemma3 27b. &lt;/p&gt; &lt;p&gt;Fwiw I tried up to q8 gguf and 8 bit mlx. &lt;/p&gt; &lt;p&gt;Is it just that gemma3 is tuned for general chat, or do you think future gguf and mlx fixes will improve it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EntertainmentBroad43"&gt; /u/EntertainmentBroad43 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je1cus/gemma3_disappointment_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je1cus/gemma3_disappointment_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je1cus/gemma3_disappointment_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T09:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdgnw5</id>
    <title>Mistrall Small 3.1 released</title>
    <updated>2025-03-17T16:20:51+00:00</updated>
    <author>
      <name>/u/Dirky_</name>
      <uri>https://old.reddit.com/user/Dirky_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnw5/mistrall_small_31_released/"&gt; &lt;img alt="Mistrall Small 3.1 released" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="Mistrall Small 3.1 released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dirky_"&gt; /u/Dirky_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/fr/news/mistral-small-3-1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnw5/mistrall_small_31_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnw5/mistrall_small_31_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T16:20:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jedtdx</id>
    <title>NVIDIA RTX PRO 6000 "Blackwell" Series Launched: Flagship GB202 GPU With 24K Cores, 96 GB VRAM</title>
    <updated>2025-03-18T19:39:33+00:00</updated>
    <author>
      <name>/u/newdoria88</name>
      <uri>https://old.reddit.com/user/newdoria88</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedtdx/nvidia_rtx_pro_6000_blackwell_series_launched/"&gt; &lt;img alt="NVIDIA RTX PRO 6000 &amp;quot;Blackwell&amp;quot; Series Launched: Flagship GB202 GPU With 24K Cores, 96 GB VRAM" src="https://external-preview.redd.it/5BAv36T_l_eU_QSz_Xpf7tieC4_Jhv4Dc1Rz9SQmcqY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=164c15c74e9b11e253100c5a08166759fbfab7a1" title="NVIDIA RTX PRO 6000 &amp;quot;Blackwell&amp;quot; Series Launched: Flagship GB202 GPU With 24K Cores, 96 GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newdoria88"&gt; /u/newdoria88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-rtx-pro-6000-blackwell-launch-flagship-gb202-gpu-24k-cores-96-gb-600w-tdp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedtdx/nvidia_rtx_pro_6000_blackwell_series_launched/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jedtdx/nvidia_rtx_pro_6000_blackwell_series_launched/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:39:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1je47w9</id>
    <title>What is the absolute best open clone of OpenAI Deep Research / Manus so far?</title>
    <updated>2025-03-18T12:52:25+00:00</updated>
    <author>
      <name>/u/hellninja55</name>
      <uri>https://old.reddit.com/user/hellninja55</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know people made some, but I don't see too much buzz about them despite being numerous:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/nickscamara/open-deep-research"&gt;https://github.com/nickscamara/open-deep-research&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/dzhng/deep-research"&gt;https://github.com/dzhng/deep-research&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mshumer/OpenDeepResearcher"&gt;https://github.com/mshumer/OpenDeepResearcher&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jina-ai/node-DeepResearch"&gt;https://github.com/jina-ai/node-DeepResearch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/atineiatte/deep-research-at-home"&gt;https://github.com/atineiatte/deep-research-at-home&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/assafelovic/gpt-researcher"&gt;https://github.com/assafelovic/gpt-researcher&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mannaandpoem/OpenManus"&gt;https://github.com/mannaandpoem/OpenManus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/The-Pocket-World/PocketManus"&gt;https://github.com/The-Pocket-World/PocketManus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Fosowl/agenticSeek"&gt;https://github.com/Fosowl/agenticSeek&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/camel-ai/owl"&gt;https://github.com/camel-ai/owl&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hellninja55"&gt; /u/hellninja55 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je47w9/what_is_the_absolute_best_open_clone_of_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je47w9/what_is_the_absolute_best_open_clone_of_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je47w9/what_is_the_absolute_best_open_clone_of_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T12:52:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdt29q</id>
    <title>LG has released their new reasoning models EXAONE-Deep</title>
    <updated>2025-03-18T00:59:59+00:00</updated>
    <author>
      <name>/u/remixer_dec</name>
      <uri>https://old.reddit.com/user/remixer_dec</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdt29q/lg_has_released_their_new_reasoning_models/"&gt; &lt;img alt="LG has released their new reasoning models EXAONE-Deep" src="https://b.thumbs.redditmedia.com/lXcocC3WrsvsaYixAdXVzQqh6xX0Tav-ZRP20RDNdvw.jpg" title="LG has released their new reasoning models EXAONE-Deep" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EXAONE reasoning model series of 2.4B, 7.8B, and 32B, optimized for reasoning tasks including math and coding&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We introduce EXAONE Deep, which exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research. Evaluation results show that 1) EXAONE Deep 2.4B outperforms other models of comparable size, 2) EXAONE Deep 7.8B outperforms not only open-weight models of comparable scale but also a proprietary reasoning model OpenAI o1-mini, and 3) EXAONE Deep 32B demonstrates competitive performance against leading open-weight models.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://www.lgresearch.ai/news/view?seq=543"&gt;Blog post&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/LGAI-EXAONE/exaone-deep-67d119918816ec6efa79a4aa"&gt;HF collection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2503.12524"&gt;Arxiv paper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/LG-AI-EXAONE/EXAONE-Deep"&gt;Github repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The models are licensed under EXAONE AI Model License Agreement 1.1 - NC&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/64raac7jpcpe1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7c4f866c5cbdcace44f4c139efcb1c7366cd952"&gt;https://preview.redd.it/64raac7jpcpe1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7c4f866c5cbdcace44f4c139efcb1c7366cd952&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;sup&gt;P.S. I made a bot that monitors fresh public releases from large companies and research labs and posts them in a&lt;/sup&gt; &lt;a href="https://remixer-dec.com/genaimon"&gt;&lt;sup&gt;tg channel&lt;/sup&gt;&lt;/a&gt;&lt;sup&gt;, feel free to join.&lt;/sup&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remixer_dec"&gt; /u/remixer_dec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdt29q/lg_has_released_their_new_reasoning_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdt29q/lg_has_released_their_new_reasoning_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdt29q/lg_has_released_their_new_reasoning_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T00:59:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdxzqt</id>
    <title>[codename] on lmarena is probably Llama4</title>
    <updated>2025-03-18T05:34:28+00:00</updated>
    <author>
      <name>/u/Most_Cap_1354</name>
      <uri>https://old.reddit.com/user/Most_Cap_1354</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i marked it as a tie, as it revealed its identity. but then i realised that it is an unreleased model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Most_Cap_1354"&gt; /u/Most_Cap_1354 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c8jfxwe9xdpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdxzqt/codename_on_lmarena_is_probably_llama4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdxzqt/codename_on_lmarena_is_probably_llama4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T05:34:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jedlum</id>
    <title>DGX Sparks / Nvidia Digits</title>
    <updated>2025-03-18T19:31:07+00:00</updated>
    <author>
      <name>/u/Temporary-Size7310</name>
      <uri>https://old.reddit.com/user/Temporary-Size7310</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedlum/dgx_sparks_nvidia_digits/"&gt; &lt;img alt="DGX Sparks / Nvidia Digits" src="https://preview.redd.it/4ydasblh2ipe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22bbb5dc941a9764664463ef883da62ce8b80a06" title="DGX Sparks / Nvidia Digits" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have now official Digits/DGX Sparks specs&lt;/p&gt; &lt;p&gt;|| || |Architecture|NVIDIA Grace Blackwell| |GPU|Blackwell Architecture| |CPU|20 core Arm, 10 Cortex-X925 + 10 Cortex-A725 Arm| |CUDA Cores|Blackwell Generation| |Tensor Cores|5th Generation| |RT Cores|4th Generation| |&lt;sup&gt;1&lt;/sup&gt;Tensor Performance |1000 AI TOPS| |System Memory|128 GB LPDDR5x, unified system memory| |Memory Interface|256-bit| |Memory Bandwidth|273 GB/s| |Storage|1 or 4 TB NVME.M2 with self-encryption| |USB|4x USB 4 TypeC (up to 40Gb/s)| |Ethernet|1x RJ-45 connector 10 GbE| |NIC|ConnectX-7 Smart NIC| |Wi-Fi|WiFi 7| |Bluetooth|BT 5.3 w/LE| |Audio-output|HDMI multichannel audio output| |Power Consumption|170W| |Display Connectors|1x HDMI 2.1a| |NVENC | NVDEC|1x | 1x| |OS|&lt;sup&gt;‚Ñ¢&lt;/sup&gt; NVIDIA DGX OS| |System Dimensions|150 mm L x 150 mm W x 50.5 mm H| |System Weight|1.2 kg|&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.nvidia.com/en-us/products/workstations/dgx-spark/"&gt;https://www.nvidia.com/en-us/products/workstations/dgx-spark/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary-Size7310"&gt; /u/Temporary-Size7310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4ydasblh2ipe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedlum/dgx_sparks_nvidia_digits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jedlum/dgx_sparks_nvidia_digits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:31:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1je2aup</id>
    <title>Kunlun Wanwei company released Skywork-R1V-38B (visual thinking chain reasoning model)</title>
    <updated>2025-03-18T11:00:49+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je2aup/kunlun_wanwei_company_released_skyworkr1v38b/"&gt; &lt;img alt="Kunlun Wanwei company released Skywork-R1V-38B (visual thinking chain reasoning model)" src="https://external-preview.redd.it/hY5-70ADAtxgy5HnAcnYxvRB7gjyBmbyBdd7tAqE0Ao.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c8f7ab157bb5241daabdeb91f42c439f60ebfc6" title="Kunlun Wanwei company released Skywork-R1V-38B (visual thinking chain reasoning model)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are thrilled to introduce Skywork R1V, the first industry open-sourced multimodal reasoning model with advanced visual chain-of-thought capabilities, pushing the boundaries of AI-driven vision and logical inference! üöÄ&lt;/p&gt; &lt;p&gt;Feature Visual Chain-of-Thought: Enables multi-step logical reasoning on visual inputs, breaking down complex image-based problems into manageable steps. Mathematical &amp;amp; Scientific Analysis: Capable of solving visual math problems and interpreting scientific/medical imagery with high precision. Cross-Modal Understanding: Seamlessly integrates text and images for richer, context-aware comprehension.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/znbbim50jfpe1.gif"&gt;https://i.redd.it/znbbim50jfpe1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d3frvoh3jfpe1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e219e876bdbe3028828509b808744bd1b076939c"&gt;https://preview.redd.it/d3frvoh3jfpe1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e219e876bdbe3028828509b808744bd1b076939c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4420a206jfpe1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d54a248fdd3474acd452353a6513153029f4cc17"&gt;https://preview.redd.it/4420a206jfpe1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d54a248fdd3474acd452353a6513153029f4cc17&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7556xizrlfpe1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b1f2d3f616c8c9aebf8d512a570539410fdf78d"&gt;https://preview.redd.it/7556xizrlfpe1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b1f2d3f616c8c9aebf8d512a570539410fdf78d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-R1V-38B"&gt;HuggingFace &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SkyworkAI/Skywork-R1V/blob/main/Skywork_R1V.pdf"&gt;Paper &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SkyworkAI/Skywork-R1V"&gt;GitHub &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je2aup/kunlun_wanwei_company_released_skyworkr1v38b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je2aup/kunlun_wanwei_company_released_skyworkr1v38b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je2aup/kunlun_wanwei_company_released_skyworkr1v38b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T11:00:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jeddoy</id>
    <title>bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF</title>
    <updated>2025-03-18T19:22:10+00:00</updated>
    <author>
      <name>/u/nicklauzon</name>
      <uri>https://old.reddit.com/user/nicklauzon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF"&gt;https://huggingface.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The man, the myth, the legend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nicklauzon"&gt; /u/nicklauzon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeddoy/bartowskimistralai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeddoy/bartowskimistralai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jeddoy/bartowskimistralai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1je7hs0</id>
    <title>ollama 0.6.2 pre-release makes Gemma 3 actually work and not suck</title>
    <updated>2025-03-18T15:22:19+00:00</updated>
    <author>
      <name>/u/vertigo235</name>
      <uri>https://old.reddit.com/user/vertigo235</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally can use Gemma 3 without memory errors when increasing context size with this new pre-release. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.6.2"&gt;https://github.com/ollama/ollama/releases/tag/v0.6.2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vertigo235"&gt; /u/vertigo235 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je7hs0/ollama_062_prerelease_makes_gemma_3_actually_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je7hs0/ollama_062_prerelease_makes_gemma_3_actually_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je7hs0/ollama_062_prerelease_makes_gemma_3_actually_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T15:22:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jedum8</id>
    <title>NVIDIA‚Äôs Llama-nemotron models</title>
    <updated>2025-03-18T19:40:57+00:00</updated>
    <author>
      <name>/u/gizcard</name>
      <uri>https://old.reddit.com/user/gizcard</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Reasoning ON/OFF. Currently on HF with entire post training data under CC-BY-4. &lt;a href="https://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b"&gt;https://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gizcard"&gt; /u/gizcard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedum8/nvidias_llamanemotron_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedum8/nvidias_llamanemotron_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jedum8/nvidias_llamanemotron_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jee2b2</id>
    <title>NVIDIA DGX Spark (Project DIGITS) Specs Are Out</title>
    <updated>2025-03-18T19:49:56+00:00</updated>
    <author>
      <name>/u/spectrography</name>
      <uri>https://old.reddit.com/user/spectrography</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nvidia.com/en-us/products/workstations/dgx-spark/"&gt;https://www.nvidia.com/en-us/products/workstations/dgx-spark/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Memory bandwidth: 273 GB/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spectrography"&gt; /u/spectrography &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jee2b2/nvidia_dgx_spark_project_digits_specs_are_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jee2b2/nvidia_dgx_spark_project_digits_specs_are_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jee2b2/nvidia_dgx_spark_project_digits_specs_are_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:49:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdidoa</id>
    <title>Victory: My wife finally recognized my silly computer hobby as useful</title>
    <updated>2025-03-17T17:28:54+00:00</updated>
    <author>
      <name>/u/StandardLovers</name>
      <uri>https://old.reddit.com/user/StandardLovers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a local LLM, LAN-accessible, with a vector database covering all tax regulations, labor laws, and compliance data. Now she sees the value. A small step for AI, a giant leap for household credibility.&lt;/p&gt; &lt;p&gt;Edit: Insane response! To everyone asking‚Äîyes, it‚Äôs just web scraping with correct layers (APIs help), embedding, and RAG. Not that hard if you structure it right. I might put together a simple guide later when i actually use a more advanced method.&lt;/p&gt; &lt;p&gt;Edit 2: I see why this blew up‚Äîthe American tax system is insanely complex. Many tax pages require a login, making a full database a massive challenge. The scale of this project for the U.S. would be huge. For context, I‚Äôm not American.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StandardLovers"&gt; /u/StandardLovers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdidoa/victory_my_wife_finally_recognized_my_silly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdidoa/victory_my_wife_finally_recognized_my_silly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdidoa/victory_my_wife_finally_recognized_my_silly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T17:28:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jedy17</id>
    <title>Nvidia digits specs released and renamed to DGX Spark</title>
    <updated>2025-03-18T19:44:57+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nvidia.com/en-us/products/workstations/dgx-spark/"&gt;https://www.nvidia.com/en-us/products/workstations/dgx-spark/&lt;/a&gt; Memory Bandwidth 273 GB/s&lt;/p&gt; &lt;p&gt;Much cheaper for running 70gb - 200 gb models than a 5090. Cost $3K. Previously nVidia claimed availability in May 2025. Will be interesting tps versus &lt;a href="https://frame.work/desktop"&gt;https://frame.work/desktop&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedy17/nvidia_digits_specs_released_and_renamed_to_dgx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedy17/nvidia_digits_specs_released_and_renamed_to_dgx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jedy17/nvidia_digits_specs_released_and_renamed_to_dgx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:44:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1je17el</id>
    <title>Open source 7.8B model beats o1 mini now on many benchmarks</title>
    <updated>2025-03-18T09:45:26+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je17el/open_source_78b_model_beats_o1_mini_now_on_many/"&gt; &lt;img alt="Open source 7.8B model beats o1 mini now on many benchmarks" src="https://preview.redd.it/211jtna16fpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e1d30576311226c74c3be9ef4b897ba63782ca9" title="Open source 7.8B model beats o1 mini now on many benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/211jtna16fpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je17el/open_source_78b_model_beats_o1_mini_now_on_many/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je17el/open_source_78b_model_beats_o1_mini_now_on_many/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T09:45:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1je7u2e</id>
    <title>ASUS DIGITS</title>
    <updated>2025-03-18T15:37:13+00:00</updated>
    <author>
      <name>/u/Cane_P</name>
      <uri>https://old.reddit.com/user/Cane_P</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je7u2e/asus_digits/"&gt; &lt;img alt="ASUS DIGITS" src="https://preview.redd.it/oidvhqtswgpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a48278147831b957d3817b71e1537dbde46ab70" title="ASUS DIGITS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When we got the online presentation, a while back, and it was in collaboration with PNY, it seemed like they would manufacture them. Now it seems like there will be more, like I guessed when I saw it.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://www.techpowerup.com/334249/asus-unveils-new-ascent-gx10-mini-pc-powered-nvidia-gb10-grace-blackwell-superchip?amp"&gt;https://www.techpowerup.com/334249/asus-unveils-new-ascent-gx10-mini-pc-powered-nvidia-gb10-grace-blackwell-superchip?amp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Archive: &lt;a href="https://web.archive.org/web/20250318102801/https://press.asus.com/news/press-releases/asus-ascent-gx10-ai-supercomputer-nvidia-gb10/"&gt;https://web.archive.org/web/20250318102801/https://press.asus.com/news/press-releases/asus-ascent-gx10-ai-supercomputer-nvidia-gb10/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cane_P"&gt; /u/Cane_P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oidvhqtswgpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je7u2e/asus_digits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je7u2e/asus_digits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T15:37:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdw7bg</id>
    <title>After these last 2 weeks of exciting releases, the only thing I know for certain is that benchmarks are largely BS</title>
    <updated>2025-03-18T03:41:11+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdw7bg/after_these_last_2_weeks_of_exciting_releases_the/"&gt; &lt;img alt="After these last 2 weeks of exciting releases, the only thing I know for certain is that benchmarks are largely BS" src="https://preview.redd.it/3lujka2ucdpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a9e581fb20610cdc9e8940f25cf7e4e9277ff42" title="After these last 2 weeks of exciting releases, the only thing I know for certain is that benchmarks are largely BS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3lujka2ucdpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdw7bg/after_these_last_2_weeks_of_exciting_releases_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdw7bg/after_these_last_2_weeks_of_exciting_releases_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T03:41:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1je4eka</id>
    <title>SmolDocling - 256M VLM for document understanding</title>
    <updated>2025-03-18T13:01:55+00:00</updated>
    <author>
      <name>/u/futterneid</name>
      <uri>https://old.reddit.com/user/futterneid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello folks! I'm andi and I work at HF for everything multimodal and vision ü§ù Yesterday with IBM we released SmolDocling, a new smol model (256M parameters ü§èüèªü§èüèª) to transcribe PDFs into markdown, it's state-of-the-art and outperforms much larger models Here's some TLDR if you're interested:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The text is rendered into markdown and has a new format called DocTags, which contains location info of objects in a PDF (images, charts), it can caption images inside PDFs Inference takes 0.35s on single A100 This model is supported by transformers and friends, and is loadable to MLX and you can serve it in vLLM Apache 2.0 licensed Very curious about your opinions ü•π&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/futterneid"&gt; /u/futterneid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je4eka/smoldocling_256m_vlm_for_document_understanding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je4eka/smoldocling_256m_vlm_for_document_understanding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je4eka/smoldocling_256m_vlm_for_document_understanding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T13:01:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1je58r5</id>
    <title>Wen GGUFs?</title>
    <updated>2025-03-18T13:42:29+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je58r5/wen_ggufs/"&gt; &lt;img alt="Wen GGUFs?" src="https://preview.redd.it/vv2vg9xbcgpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fce563f7a834755ce5916e4567c3e30f30949f6" title="Wen GGUFs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vv2vg9xbcgpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je58r5/wen_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je58r5/wen_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T13:42:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jeczzz</id>
    <title>New reasoning model from NVIDIA</title>
    <updated>2025-03-18T19:07:23+00:00</updated>
    <author>
      <name>/u/mapestree</name>
      <uri>https://old.reddit.com/user/mapestree</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeczzz/new_reasoning_model_from_nvidia/"&gt; &lt;img alt="New reasoning model from NVIDIA" src="https://external-preview.redd.it/S69zjX2lDQklr7v9YvMlzRoANZizsM0E74iOfCibG0E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2980af0c07cf4d3c4c52a935239567d59c9b3be3" title="New reasoning model from NVIDIA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mapestree"&gt; /u/mapestree &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/5kluqad.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeczzz/new_reasoning_model_from_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jeczzz/new_reasoning_model_from_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:07:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1je8axe</id>
    <title>I'm not one for dumb tests but this is a funny first impression</title>
    <updated>2025-03-18T15:56:57+00:00</updated>
    <author>
      <name>/u/MixtureOfAmateurs</name>
      <uri>https://old.reddit.com/user/MixtureOfAmateurs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je8axe/im_not_one_for_dumb_tests_but_this_is_a_funny/"&gt; &lt;img alt="I'm not one for dumb tests but this is a funny first impression" src="https://preview.redd.it/s5k3j9z70hpe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7562163bfe7eb6adc1234ea41f7c18eca73fb49c" title="I'm not one for dumb tests but this is a funny first impression" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MixtureOfAmateurs"&gt; /u/MixtureOfAmateurs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s5k3j9z70hpe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je8axe/im_not_one_for_dumb_tests_but_this_is_a_funny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je8axe/im_not_one_for_dumb_tests_but_this_is_a_funny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T15:56:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1je6ns1</id>
    <title>Meta talks about us and open source source AI for over 1 Billion downloads</title>
    <updated>2025-03-18T14:46:25+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je6ns1/meta_talks_about_us_and_open_source_source_ai_for/"&gt; &lt;img alt="Meta talks about us and open source source AI for over 1 Billion downloads" src="https://preview.redd.it/gcql3piongpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58b8393e9781f3853aac114d10af307ef017ca59" title="Meta talks about us and open source source AI for over 1 Billion downloads" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gcql3piongpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je6ns1/meta_talks_about_us_and_open_source_source_ai_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je6ns1/meta_talks_about_us_and_open_source_source_ai_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T14:46:25+00:00</published>
  </entry>
</feed>
