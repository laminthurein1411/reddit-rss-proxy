<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-28T17:23:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kxj4ne</id>
    <title>Is slower inference and non-realtime cheaper?</title>
    <updated>2025-05-28T14:53:00+00:00</updated>
    <author>
      <name>/u/AryanEmbered</name>
      <uri>https://old.reddit.com/user/AryanEmbered</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there a service that can take in my requests, and then give me the response after A WHILE, like, days later.&lt;/p&gt; &lt;p&gt;and is significantly cheaper?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AryanEmbered"&gt; /u/AryanEmbered &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxj4ne/is_slower_inference_and_nonrealtime_cheaper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxj4ne/is_slower_inference_and_nonrealtime_cheaper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxj4ne/is_slower_inference_and_nonrealtime_cheaper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T14:53:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxi7qh</id>
    <title>Llama.cpp: Does it make sense to use a larger --n-predict (-n) than --ctx-size (-c)?</title>
    <updated>2025-05-28T14:15:58+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My setup: A reasoning model eg Qwen3 32B at Q4KXL + 16k context. Those will fit snugly in 24GB VRAM and leave some room for other apps.&lt;/p&gt; &lt;p&gt;Problem: Reasoning models, 1 time out of 3 (in my use cases), will keep on thinking for longer than the 16k window, and that's why I set the -n option to prevent it from reasoning indefinitely.&lt;/p&gt; &lt;p&gt;Question: I can relax -n to perhaps 30k, which some reasoning models suggest. However, when -n is larger than -c, won't the context window shift and the response's relevance to my prompt start decreasing?&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxi7qh/llamacpp_does_it_make_sense_to_use_a_larger/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxi7qh/llamacpp_does_it_make_sense_to_use_a_larger/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxi7qh/llamacpp_does_it_make_sense_to_use_a_larger/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T14:15:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxfq8r</id>
    <title>Old model, new implementation</title>
    <updated>2025-05-28T12:24:34+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/foldl/chatllm.cpp"&gt;chatllm.cpp&lt;/a&gt; implements &lt;a href="https://huggingface.co/adept/fuyu-8b"&gt;Fuyu-8b&lt;/a&gt; as the 1st supported vision model.&lt;/p&gt; &lt;p&gt;I have search this group. Not many have tested this model due to lack of support from llama.cpp. Now, would you like to try this model? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxfq8r/old_model_new_implementation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxfq8r/old_model_new_implementation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxfq8r/old_model_new_implementation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T12:24:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxc5vo</id>
    <title>MCP Proxy â€“ Use your embedded system as an agent</title>
    <updated>2025-05-28T08:47:54+00:00</updated>
    <author>
      <name>/u/arbayi</name>
      <uri>https://old.reddit.com/user/arbayi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxc5vo/mcp_proxy_use_your_embedded_system_as_an_agent/"&gt; &lt;img alt="MCP Proxy â€“ Use your embedded system as an agent" src="https://external-preview.redd.it/QE_AwMn8Vhy9CL-rjaMkp2CgPWYkmSjtSuxPv7QHnQs.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ecb64a2bffb05e44d274eb04fb6b8576a8f1055e" title="MCP Proxy â€“ Use your embedded system as an agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/x1y4mz3mkh3f1.gif"&gt;https://i.redd.it/x1y4mz3mkh3f1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video: &lt;a href="https://www.youtube.com/watch?v=foCp3ja8FRA"&gt;https://www.youtube.com/watch?v=foCp3ja8FRA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repository: &lt;a href="https://github.com/openserv-labs/mcp-proxy"&gt;https://github.com/openserv-labs/mcp-proxy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I've been playing around with agents, MCP servers and embedded systems for a while. I was trying to figure out the best way to connect my real-time devices to agents and use them in multi-agent workflows.&lt;/p&gt; &lt;p&gt;At OpenServ, we have an API to interact with agents, so at first I thought I'd just run a specialized web server to talk to the platform. But that had its own problemsâ€”mainly memory issues and needing to customize it for each device.&lt;/p&gt; &lt;p&gt;Then we thought, why not just run a regular web server and use it as an agent? The idea is simple, and the implementation is even simpler thanks to MCP. I define my serverâ€™s endpoints as tools in the MCP server, and agents (MCP clients) can call them directly.&lt;/p&gt; &lt;p&gt;Even though the initial idea was to work with embedded systems, this can work for any backend.&lt;/p&gt; &lt;p&gt;Would love to hear your thoughtsâ€”especially around connecting agents to real-time devices to collect sensor data or control them in mutlti-agent workflows.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arbayi"&gt; /u/arbayi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxc5vo/mcp_proxy_use_your_embedded_system_as_an_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxc5vo/mcp_proxy_use_your_embedded_system_as_an_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxc5vo/mcp_proxy_use_your_embedded_system_as_an_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T08:47:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxg95a</id>
    <title>vLLM Classify Bad Results</title>
    <updated>2025-05-28T12:50:16+00:00</updated>
    <author>
      <name>/u/Upstairs-Garlic-2301</name>
      <uri>https://old.reddit.com/user/Upstairs-Garlic-2301</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxg95a/vllm_classify_bad_results/"&gt; &lt;img alt="vLLM Classify Bad Results" src="https://preview.redd.it/d9tr89iqri3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f21243a1700f4e98e4582d952577c7f25af1d879" title="vLLM Classify Bad Results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone used vLLM for classification?&lt;/p&gt; &lt;p&gt;I have a fine-tuned modernBERT model with 5 classes. During model training, the best model shows a .78 F1 score.&lt;/p&gt; &lt;p&gt;After the model is trained, I passed the test set through vLLM and Hugging Face pipelines as a test and get the screenshot above.&lt;/p&gt; &lt;p&gt;Hugging Face pipeline matches the result (F1 of .78) but vLLM is way off, with an F1 of .58.&lt;/p&gt; &lt;p&gt;Any ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Upstairs-Garlic-2301"&gt; /u/Upstairs-Garlic-2301 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d9tr89iqri3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxg95a/vllm_classify_bad_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxg95a/vllm_classify_bad_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T12:50:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxlus4</id>
    <title>Codestral Embed [embedding model specialized for code]</title>
    <updated>2025-05-28T16:40:54+00:00</updated>
    <author>
      <name>/u/pahadi_keeda</name>
      <uri>https://old.reddit.com/user/pahadi_keeda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxlus4/codestral_embed_embedding_model_specialized_for/"&gt; &lt;img alt="Codestral Embed [embedding model specialized for code]" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="Codestral Embed [embedding model specialized for code]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pahadi_keeda"&gt; /u/pahadi_keeda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/codestral-embed"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxlus4/codestral_embed_embedding_model_specialized_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxlus4/codestral_embed_embedding_model_specialized_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T16:40:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwk1jm</id>
    <title>Wife isnâ€™t home, that means H200 in the living room ;D</title>
    <updated>2025-05-27T10:40:11+00:00</updated>
    <author>
      <name>/u/Flintbeker</name>
      <uri>https://old.reddit.com/user/Flintbeker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwk1jm/wife_isnt_home_that_means_h200_in_the_living_room/"&gt; &lt;img alt="Wife isnâ€™t home, that means H200 in the living room ;D" src="https://a.thumbs.redditmedia.com/CHdnIbD-SLsvZOKpoU7Rs4hqE0GREYpW_lt-IICeGd0.jpg" title="Wife isnâ€™t home, that means H200 in the living room ;D" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally got our H200 System, until itâ€™s going in the datacenter next week that means localLLaMa with some extra power :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flintbeker"&gt; /u/Flintbeker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kwk1jm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwk1jm/wife_isnt_home_that_means_h200_in_the_living_room/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwk1jm/wife_isnt_home_that_means_h200_in_the_living_room/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T10:40:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwucpn</id>
    <title>ðŸ˜žNo hate but claude-4 is disappointing</title>
    <updated>2025-05-27T18:10:17+00:00</updated>
    <author>
      <name>/u/Rare-Programmer-1747</name>
      <uri>https://old.reddit.com/user/Rare-Programmer-1747</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwucpn/no_hate_but_claude4_is_disappointing/"&gt; &lt;img alt="ðŸ˜žNo hate but claude-4 is disappointing" src="https://preview.redd.it/9dngmfww7d3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d89328b58759f0c926b5258c859b6fbfcf5a5b32" title="ðŸ˜žNo hate but claude-4 is disappointing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean how the heck literally Is Qwen-3 better than claude-4(the Claude who used to dog walk everyone). this is just disappointing ðŸ« &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare-Programmer-1747"&gt; /u/Rare-Programmer-1747 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9dngmfww7d3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwucpn/no_hate_but_claude4_is_disappointing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwucpn/no_hate_but_claude4_is_disappointing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T18:10:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxlsvk</id>
    <title>Unsloth Devstral Q8_K_XL only 30% the speed of Q8_0?</title>
    <updated>2025-05-28T16:38:46+00:00</updated>
    <author>
      <name>/u/liquidki</name>
      <uri>https://old.reddit.com/user/liquidki</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxlsvk/unsloth_devstral_q8_k_xl_only_30_the_speed_of_q8_0/"&gt; &lt;img alt="Unsloth Devstral Q8_K_XL only 30% the speed of Q8_0?" src="https://b.thumbs.redditmedia.com/O6Jko66HZAvHYggfkAy12EWL4MQTRZR0Rs12q8Eouoo.jpg" title="Unsloth Devstral Q8_K_XL only 30% the speed of Q8_0?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/9zkgrbyytj3f1.png?width=727&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dfca0d32d1cfb27d0cf7cba227aed08915a0fc7"&gt;https://preview.redd.it/9zkgrbyytj3f1.png?width=727&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dfca0d32d1cfb27d0cf7cba227aed08915a0fc7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Dear community,&lt;/p&gt; &lt;p&gt;I was wondering if anyone could shed some light on this. I prompted all these models to create a basic snake game in python using the turtle library. Each succeeded, generating about 150-180 lines of code.&lt;/p&gt; &lt;p&gt;What was interesting and unexpected was how much slower the Q8_K_XL quant was and how fast the Q8_0 quant was in relation to the others. I would have expected at least 5 tokens/sec from the Q8_K_XL quant based on the performance drop from Q4_K_XL -&amp;gt; Q6_K_XL.&lt;/p&gt; &lt;p&gt;My setup is a Mac Mini M4 Pro, with 14 CPU cores, 20 GPU cores, and 64 GB of Unified memory.&lt;/p&gt; &lt;p&gt;Any theories?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liquidki"&gt; /u/liquidki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxlsvk/unsloth_devstral_q8_k_xl_only_30_the_speed_of_q8_0/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxlsvk/unsloth_devstral_q8_k_xl_only_30_the_speed_of_q8_0/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxlsvk/unsloth_devstral_q8_k_xl_only_30_the_speed_of_q8_0/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T16:38:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxk2zf</id>
    <title>Dual RTX 3090 users (are there many of us?)</title>
    <updated>2025-05-28T15:30:48+00:00</updated>
    <author>
      <name>/u/StandardLovers</name>
      <uri>https://old.reddit.com/user/StandardLovers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is your TDP ? (Or optimal clock speeds) What is your PCIe lane speeds ? Power supply ? Planning to upgrade or sell before prices drop ? Any other remarks ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StandardLovers"&gt; /u/StandardLovers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxk2zf/dual_rtx_3090_users_are_there_many_of_us/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxk2zf/dual_rtx_3090_users_are_there_many_of_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxk2zf/dual_rtx_3090_users_are_there_many_of_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T15:30:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxbmr9</id>
    <title>Another Ryzen Max+ 395 machine has been released. Are all the Chinese Max+ 395 machines the same?</title>
    <updated>2025-05-28T08:10:10+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Another AMD Ryzen Max+ 395 mini-pc has been released. The FEVM FA-EX9. For those who kept asking for it, this comes with Oculink. Here's a YT review.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-1kuUqp1X2I"&gt;https://www.youtube.com/watch?v=-1kuUqp1X2I&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I think all the Chinese Max+ mini-pcs are the same. I noticed again that this machine has &lt;em&gt;exactly&lt;/em&gt; the same port layout as the GMK X2. But how can that be if this has Oculink but the X2 doesn't? The Oculink is an addon. It takes up one of the NVME slots. It's just not the port layout, but the motherboards look exactly the same. Down to the same red color. Even the sound level is the same with the same fan configuration 2 blowers and one axial. So it's like one manufacturer is making the MB and then all the other companies are using that MB for their mini-pcs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxbmr9/another_ryzen_max_395_machine_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxbmr9/another_ryzen_max_395_machine_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxbmr9/another_ryzen_max_395_machine_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T08:10:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxlx46</id>
    <title>I'm building a Self-Hosted Alternative to OpenAI Code Interpreter, E2B</title>
    <updated>2025-05-28T16:43:23+00:00</updated>
    <author>
      <name>/u/NyproTheGeek</name>
      <uri>https://old.reddit.com/user/NyproTheGeek</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Could not find a simple self-hosted solution so I built one in Rust that lets you securely run untrusted/AI-generated code in micro VMs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;microsandbox&lt;/strong&gt; spins up in milliseconds, runs on your own infra, no Docker needed. And It doubles as an MCP Server so you can connect it directly with your fave MCP-enabled AI agent or app.&lt;/p&gt; &lt;p&gt;Python, Typescript and Rust SDKs are available so you can spin up vms with just 4-5 lines of code. Run code, plot charts, browser use, and so on.&lt;/p&gt; &lt;p&gt;Still early days. Lmk what you think and lend us a ðŸŒŸ star on &lt;a href="https://github.com/microsandbox/microsandbox"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NyproTheGeek"&gt; /u/NyproTheGeek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxlx46/im_building_a_selfhosted_alternative_to_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxlx46/im_building_a_selfhosted_alternative_to_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxlx46/im_building_a_selfhosted_alternative_to_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T16:43:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxk3lk</id>
    <title>Another reorg for Meta Llama: AGI team created</title>
    <updated>2025-05-28T15:31:29+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which teams are going to get the most GPUs?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.axios.com/2025/05/27/meta-ai-restructure-2025-agi-llama"&gt;https://www.axios.com/2025/05/27/meta-ai-restructure-2025-agi-llama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Llama team divided into two teams:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The AGI Foundations unit will include the company's &lt;strong&gt;Llama models&lt;/strong&gt;, as well as efforts to improve capabilities in reasoning, multimedia and voice.&lt;/li&gt; &lt;li&gt;The AI products team will be responsible for the Meta AI assistant, Meta's AI Studio and AI features within Facebook, Instagram and WhatsApp.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The company's AI research unit, known as FAIR (Fundamental AI Research), remains separate from the new organizational structure, though one specific team working on multimedia is moving to the new AGI Foundations team.&lt;/p&gt; &lt;p&gt;Meta hopes that splitting a single large organization into smaller teams will speed product development and give the company more flexibility as it adds additional technical leaders.&lt;/p&gt; &lt;p&gt;The company is also &lt;a href="https://www.businessinsider.com/meta-llama-ai-talent-mistral-2025-5"&gt;seeing key talent depart&lt;/a&gt;, including to French rival Mistral, as reported by Business Insider.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxk3lk/another_reorg_for_meta_llama_agi_team_created/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxk3lk/another_reorg_for_meta_llama_agi_team_created/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxk3lk/another_reorg_for_meta_llama_agi_team_created/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T15:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxf0ig</id>
    <title>Parakeet-TDT 0.6B v2 FastAPI STT Service (OpenAI-style API + Experimental Streaming)</title>
    <updated>2025-05-28T11:47:57+00:00</updated>
    <author>
      <name>/u/Shadowfita</name>
      <uri>https://old.reddit.com/user/Shadowfita</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I'm (finally) releasing a FastAPI wrapper around NVIDIAâ€™s Parakeet-TDT 0.6B v2 ASR model with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;REST &lt;code&gt;/transcribe&lt;/code&gt; endpoint with optional timestamps&lt;/li&gt; &lt;li&gt;Health &amp;amp; debug endpoints: &lt;code&gt;/healthz&lt;/code&gt;, &lt;code&gt;/debug/cfg&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Experimental WebSocket &lt;code&gt;/ws&lt;/code&gt; for real-time PCM streaming and partial/full transcripts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Shadowfita/parakeet-tdt-0.6b-v2-fastapi"&gt;https://github.com/Shadowfita/parakeet-tdt-0.6b-v2-fastapi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shadowfita"&gt; /u/Shadowfita &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxf0ig/parakeettdt_06b_v2_fastapi_stt_service/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxf0ig/parakeettdt_06b_v2_fastapi_stt_service/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxf0ig/parakeettdt_06b_v2_fastapi_stt_service/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T11:47:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxh07e</id>
    <title>FlashMoe support in ipex-llm allows you to run DeepSeek V3/R1 671B and Qwen3MoE 235B models with just 1 or 2 Intel Arc GPU (such as A770 and B580)</title>
    <updated>2025-05-28T13:24:16+00:00</updated>
    <author>
      <name>/u/lQEX0It_CUNTY</name>
      <uri>https://old.reddit.com/user/lQEX0It_CUNTY</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just noticed that this team claims it is possible to run the DeepSeek V1/R1 671B Q4_K_M model with two cheap Intel GPUs (and a huge amount of system RAM). I wonder if anybody has actually tried or built such a beast?&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/flashmoe_quickstart.md"&gt;https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/flashmoe_quickstart.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also see at the end the claim: For 1 ARC A770 platform, please reduce context length (e.g., 1024) to avoid OOM. Add this option &lt;code&gt;-c 1024&lt;/code&gt; at the CLI command.&lt;/p&gt; &lt;p&gt;Does this mean this implementation is effectively a box ticking exercise?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lQEX0It_CUNTY"&gt; /u/lQEX0It_CUNTY &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxh07e/flashmoe_support_in_ipexllm_allows_you_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxh07e/flashmoe_support_in_ipexllm_allows_you_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxh07e/flashmoe_support_in_ipexllm_allows_you_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T13:24:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kx9nfk</id>
    <title>Megakernel doubles Llama-1B inference speed for batch size 1</title>
    <updated>2025-05-28T05:58:06+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The authors of this &lt;a href="https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles"&gt;bloglike paper&lt;/a&gt; at Stanford found that vLLM and SGLang lose significant performance due to overhead in CUDA usage for low batch sizes - what you usually use when running locally to chat. Their improvement doubles the inference speed on a H100, which however has significantly higher memory bandwidth than a 3090 for example. It remains to be seen how this scales to user GPUs. The benefits will diminish the larger the model gets.&lt;/p&gt; &lt;p&gt;The best thing is that even with their optimizations there seems to be still some room left for further improvements - theoretically. There was also no word on llama.cpp in there. Their publication is a nice &amp;amp; easy read though.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx9nfk/megakernel_doubles_llama1b_inference_speed_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx9nfk/megakernel_doubles_llama1b_inference_speed_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kx9nfk/megakernel_doubles_llama1b_inference_speed_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T05:58:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxmgtr</id>
    <title>DeepSeek-R1-0528 VS claude-4-sonnet (still a demo)</title>
    <updated>2025-05-28T17:04:48+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxmgtr/deepseekr10528_vs_claude4sonnet_still_a_demo/"&gt; &lt;img alt="DeepSeek-R1-0528 VS claude-4-sonnet (still a demo)" src="https://external-preview.redd.it/dnJvNHd1dzkwazNmMfbq08Ky_kl08uBBBLb2R6rGiFj8hH36RtTI5_C0jZhK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c7de07fc0dd217d9dfd656339a5f8ef5292948f" title="DeepSeek-R1-0528 VS claude-4-sonnet (still a demo)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The heptagon + 20 balls benchmark can no longer measure their capabilities, so I'm preparing to try something new&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4lh915x90k3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxmgtr/deepseekr10528_vs_claude4sonnet_still_a_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxmgtr/deepseekr10528_vs_claude4sonnet_still_a_demo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T17:04:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxhmgo</id>
    <title>VideoGameBench- full code + paper release</title>
    <updated>2025-05-28T13:51:11+00:00</updated>
    <author>
      <name>/u/ofirpress</name>
      <uri>https://old.reddit.com/user/ofirpress</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1kxhmgo/video/hzjtuzzr1j3f1/player"&gt;https://reddit.com/link/1kxhmgo/video/hzjtuzzr1j3f1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;VideoGameBench&lt;/strong&gt; evaluates VLMs on Game Boy and MS-DOS games given only raw screen input, just like how a human would play. The best model (Gemini) completes just 0.48% of the benchmark. We have a bunch of clips on the website:&lt;br /&gt; &lt;a href="http://vgbench.com"&gt;vgbench.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2505.18134"&gt;https://arxiv.org/abs/2505.18134&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/alexzhang13/videogamebench"&gt;https://github.com/alexzhang13/videogamebench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Alex and I will stick around to answer questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ofirpress"&gt; /u/ofirpress &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxhmgo/videogamebench_full_code_paper_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxhmgo/videogamebench_full_code_paper_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxhmgo/videogamebench_full_code_paper_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T13:51:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxdkms</id>
    <title>Cobolt is now available on Linux! ðŸŽ‰</title>
    <updated>2025-05-28T10:23:07+00:00</updated>
    <author>
      <name>/u/ice-url</name>
      <uri>https://old.reddit.com/user/ice-url</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Remember when we said Cobolt is &amp;quot;Powered by community-driven development&amp;quot;?&lt;/p&gt; &lt;p&gt;After our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kujwzl/we_believe_the_future_of_ai_is_local_private_and"&gt;last post&lt;/a&gt; about Cobolt â€“ &lt;strong&gt;our local, private, and personalized AI assistant&lt;/strong&gt; â€“ the call for Linux support was overwhelming. Well, you asked, and we're thrilled to deliver: Cobolt is now available on Linux! ðŸŽ‰ &lt;a href="https://github.com/platinum-hill/cobolt?tab=readme-ov-file#getting-started"&gt;Get started here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are excited by your engagement and shared belief in accessible, private AI.&lt;/p&gt; &lt;p&gt;Join us in shaping the future of Cobolt on &lt;a href="https://github.com/platinum-hill/cobolt"&gt;Github&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our promise remains: Privacy by design, extensible, and personalized.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thank you for driving us forward. Let's keep building AI that serves you, now on Linux!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ice-url"&gt; /u/ice-url &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdkms/cobolt_is_now_available_on_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdkms/cobolt_is_now_available_on_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdkms/cobolt_is_now_available_on_linux/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T10:23:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxdcpi</id>
    <title>impressive streamlining in local llm deployment: gemma 3n downloading directly to my phone without any tinkering. what a time to be alive!</title>
    <updated>2025-05-28T10:08:41+00:00</updated>
    <author>
      <name>/u/thebigvsbattlesfan</name>
      <uri>https://old.reddit.com/user/thebigvsbattlesfan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdcpi/impressive_streamlining_in_local_llm_deployment/"&gt; &lt;img alt="impressive streamlining in local llm deployment: gemma 3n downloading directly to my phone without any tinkering. what a time to be alive!" src="https://preview.redd.it/sd06j27qyh3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c51a26804948f34a4686a4018dd2e02a67c40a82" title="impressive streamlining in local llm deployment: gemma 3n downloading directly to my phone without any tinkering. what a time to be alive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebigvsbattlesfan"&gt; /u/thebigvsbattlesfan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sd06j27qyh3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdcpi/impressive_streamlining_in_local_llm_deployment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdcpi/impressive_streamlining_in_local_llm_deployment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T10:08:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxgzd1</id>
    <title>Is there an open source alternative to manus?</title>
    <updated>2025-05-28T13:23:15+00:00</updated>
    <author>
      <name>/u/BoJackHorseMan53</name>
      <uri>https://old.reddit.com/user/BoJackHorseMan53</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried manus and was surprised how ahead it is of other agents at browsing the web and using files, terminal etc autonomously.&lt;/p&gt; &lt;p&gt;There is no tool I've tried before that comes close to it.&lt;/p&gt; &lt;p&gt;What's the best open source alternative to Manus that you've tried?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoJackHorseMan53"&gt; /u/BoJackHorseMan53 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxgzd1/is_there_an_open_source_alternative_to_manus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxgzd1/is_there_an_open_source_alternative_to_manus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxgzd1/is_there_an_open_source_alternative_to_manus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T13:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxa788</id>
    <title>Google AI Edge Gallery</title>
    <updated>2025-05-28T06:33:50+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxa788/google_ai_edge_gallery/"&gt; &lt;img alt="Google AI Edge Gallery" src="https://preview.redd.it/s6rgmrfawg3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4720f1c95bf832e5eacd2490cf5b69783a79a11b" title="Google AI Edge Gallery" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Explore, Experience, and Evaluate the Future of On-Device Generative AI with Google AI Edge.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Google AI Edge Gallery is an experimental app that puts the power of cutting-edge Generative AI models directly into your hands, running entirely on your Android &lt;em&gt;(available now)&lt;/em&gt; and iOS &lt;em&gt;(coming soon)&lt;/em&gt; devices. Dive into a world of creative and practical AI use cases, all running locally, without needing an internet connection once the model is loaded. Experiment with different models, chat, ask questions with images, explore prompts, and more!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/google-ai-edge/gallery?tab=readme-ov-file"&gt;https://github.com/google-ai-edge/gallery?tab=readme-ov-file&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s6rgmrfawg3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxa788/google_ai_edge_gallery/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxa788/google_ai_edge_gallery/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T06:33:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxjbb5</id>
    <title>QwQ 32B is Amazing (&amp; Sharing my 131k + Imatrix)</title>
    <updated>2025-05-28T15:00:38+00:00</updated>
    <author>
      <name>/u/crossivejoker</name>
      <uri>https://old.reddit.com/user/crossivejoker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm curious what your experience has been with QwQ 32B. I've seen really good takes on QwQ vs Qwen3, but I think they're not comparable. Here's the differences I see and I'd love feedback.&lt;/p&gt; &lt;h1&gt;When To Use Qwen3&lt;/h1&gt; &lt;p&gt;If I had to choose between QwQ 32B versus Qwen3 for daily AI assistant tasks, I'd choose Qwen3. This is because for 99% of general questions or work, Qwen3 is faster, answers just as well, and does amazing. As where QwQ 32B will do just as good, but it'll often over think and spend much longer answering any question.&lt;/p&gt; &lt;h1&gt;When To Use QwQ 32B&lt;/h1&gt; &lt;p&gt;Now for an AI agent or doing orchestration level work, I would choose QwQ all day every day. It's not that Qwen3 is bad, but it cannot handle the same level of semantic orchestration. In fact, ChatGPT 4o can't keep up with what I'm pushing QwQ to do.&lt;/p&gt; &lt;h1&gt;Benchmarks&lt;/h1&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/magiccodingman/QwQ-32B-abliterated-131k-GGUF-Yarn-Imatrix/blob/main/Benchmarks/Simulation%20Fidelity%20Benchmark.md"&gt;Simulation Fidelity Benchmark&lt;/a&gt; is something I created a long time ago. Firstly I love RP based D&amp;amp;D inspired AI simulated games. But, I've always hated how current AI systems makes me the driver, but without any gravity. Anything and everything I say goes, so years ago I made a benchmark that is meant to be a better enforcement of simulated gravity. And as I'd eventually build agents that'd do real world tasks, this test funnily was an amazing benchmark for everything. So I know it's dumb that I use something like this, but it's been a fantastic way for me to gauge the wisdom of an AI model. I've often valued wisdom over intelligence. It's not about an AI knowing a random capital of X country, it's about knowing when to Google the capital of X country. &lt;a href="https://huggingface.co/datasets/magiccodingman/QwQ-32B-abliterated-131k-GGUF-Yarn-Imatrix/tree/main/Benchmarks"&gt;Benchmark Tests&lt;/a&gt; are here. And if more details on inputs or anything are wanted, I'm more than happy to share. My system prompt was counted with GPT 4 token counter (bc I'm lazy) and it was ~6k tokens. Input was ~1.6k. The shown benchmarks was the end results. But I had tests ranging a total of ~16k tokens to ~40k tokens. I don't have the hardware to test further sadly.&lt;/p&gt; &lt;h1&gt;My Experience With QwQ 32B&lt;/h1&gt; &lt;p&gt;So, what am I doing? Why do I like QwQ? Because it's not just emulating a good story, it's remembering many dozens of semantic threads. Did an item get moved? Is the scene changing? Did the last result from context require memory changes? Does the current context provide sufficient information or is the custom RAG database created needed to be called with an optimized query based on meta data tags provided?&lt;/p&gt; &lt;p&gt;Oh I'm just getting started, but I've been pushing QwQ to the absolute edge. Because AI agents whether a dungeon master of a game, creating projects, doing research, or anything else. A single missed step is catastrophic to simulated reality. Missed contexts leads to semantic degradation in time. Because my agents have to consistently alter what it remembers or knows. I have limited context limits, so it must always tell the future version that must run what it must do for the next part of the process.&lt;/p&gt; &lt;p&gt;Qwen3, Gemma, GPT 4o, they do amazing. To a point. But they're trained to be assistants. But QwQ 32B is weird, incredibly weird. The kind of weird I love. It's an agent level battle tactician. I'm allowing my agent to constantly rewrite it's own system prompts (partially), have full access to grab or alter it's own short term and long term memory, and it's not missing a beat.&lt;/p&gt; &lt;p&gt;The perfection is what makes QwQ so very good. Near perfection is required when doing wisdom based AI agent tasks.&lt;/p&gt; &lt;h1&gt;QwQ-32B-Abliterated-131k-GGUF-Yarn-Imatrix&lt;/h1&gt; &lt;p&gt;I've enjoyed QwQ 32B so much that I made my own version. Note, this isn't a fine tune or anything like that, but my own custom GGUF converted version to run on llama.cpp. But I did do the following:&lt;/p&gt; &lt;p&gt;1.) Altered the llama.cpp conversion script to add yarn meta data tags. (TLDR, unlocked the normal 8k precision but can handle ~32k to 131,072 tokens)&lt;/p&gt; &lt;p&gt;2.) Utilized a hybrid FP16 process with all quants with embed, output, all 64 layers (attention/feed forward weights + bias).&lt;/p&gt; &lt;p&gt;3.) Q4 to Q6 were all created with a ~16M token imatrix to make them significantly better and bring the level of precision much closer to Q8. (Q8 excluded, reasons in repo).&lt;/p&gt; &lt;p&gt;The repo is here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/magiccodingman/QwQ-32B-abliterated-131k-GGUF-Yarn-Imatrix"&gt;https://huggingface.co/datasets/magiccodingman/QwQ-32B-abliterated-131k-GGUF-Yarn-Imatrix&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Have You Really Used QwQ?&lt;/h1&gt; &lt;p&gt;I've had a fantastic time with QwQ 32B so far. When I say that Qwen3 and other models can't keep up, I've genuinely tried to put each in an environment to compete on equal footing. It's not that everything else was &amp;quot;bad&amp;quot; it just wasn't as perfect as QwQ. But I'd also love feedback.&lt;/p&gt; &lt;p&gt;I'm more than open to being wrong and hearing why. Is Qwen3 able to hit just as hard? Note I did utilize Qwen3 of all sizes plus think mode.&lt;/p&gt; &lt;p&gt;But I've just been incredibly happy to use QwQ 32B because it's the first model that's open source and something I can run locally that can perform the tasks I want. So far any API based models to do the tasks I wanted would cost ~$1k minimum a month, so it's really amazing to be able to finally run something this good locally.&lt;/p&gt; &lt;p&gt;If I could get just as much power with a faster, more efficient, or smaller model, that'd be amazing. But, I can't find it.&lt;/p&gt; &lt;h1&gt;Q&amp;amp;A&lt;/h1&gt; &lt;p&gt;Just some answers to questions that are relevant:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; What's my hardware setup&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Used 2x 3090's with the following llama.cpp settings:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--no-mmap --ctx-size 32768 --n-gpu-layers 256 --tensor-split 20,20 --flash-attn &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crossivejoker"&gt; /u/crossivejoker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxjbb5/qwq_32b_is_amazing_sharing_my_131k_imatrix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxjbb5/qwq_32b_is_amazing_sharing_my_131k_imatrix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxjbb5/qwq_32b_is_amazing_sharing_my_131k_imatrix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T15:00:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxdm2z</id>
    <title>DeepSeek Announces Upgrade, Possibly Launching New Model Similar to 0324</title>
    <updated>2025-05-28T10:25:48+00:00</updated>
    <author>
      <name>/u/luckbossx</name>
      <uri>https://old.reddit.com/user/luckbossx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdm2z/deepseek_announces_upgrade_possibly_launching_new/"&gt; &lt;img alt="DeepSeek Announces Upgrade, Possibly Launching New Model Similar to 0324" src="https://b.thumbs.redditmedia.com/Dz5hcsX2WLjgLhcJJfOpbDqkyNTpx1Aiw9gp50Wdl_Q.jpg" title="DeepSeek Announces Upgrade, Possibly Launching New Model Similar to 0324" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The official DeepSeek group has issued an announcement claiming an upgrade, possibly a new model similar to the 0324 version.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/luckbossx"&gt; /u/luckbossx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kxdm2z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdm2z/deepseek_announces_upgrade_possibly_launching_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdm2z/deepseek_announces_upgrade_possibly_launching_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T10:25:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxaxw9</id>
    <title>The Economist: "Companies abandon their generative AI projects"</title>
    <updated>2025-05-28T07:23:16+00:00</updated>
    <author>
      <name>/u/mayalihamur</name>
      <uri>https://old.reddit.com/user/mayalihamur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;a href="https://archive.ph/P51MQ"&gt;recent article&lt;/a&gt; in the Economist claims that &amp;quot;the share of companies abandoning most of their generative-AI pilot projects has risen to 42%, up from 17% last year.&amp;quot; Apparently companies who invested in generative AI and slashed jobs are now disappointed and they began rehiring humans for roles.&lt;/p&gt; &lt;p&gt;The hype with the generative AI increasingly looks like a &amp;quot;we have a solution, now let's find some problems&amp;quot; scenario. Apart from software developers and graphic designers, I wonder how many professionals actually feel the impact of generative AI in their workplace?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mayalihamur"&gt; /u/mayalihamur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxaxw9/the_economist_companies_abandon_their_generative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxaxw9/the_economist_companies_abandon_their_generative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxaxw9/the_economist_companies_abandon_their_generative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T07:23:16+00:00</published>
  </entry>
</feed>
