<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-10T02:25:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1l7fg95</id>
    <title>Need feedback for a RAG using Ollama as background.</title>
    <updated>2025-06-09T20:24:27+00:00</updated>
    <author>
      <name>/u/LivingSignificant452</name>
      <uri>https://old.reddit.com/user/LivingSignificant452</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; I would like to set up a private , local notebooklm alternative. Using documents I prepare in PDF mainly ( up to 50 very long document 500pages each ). Also !! I need it to work correctly with french language.&lt;br /&gt; for the hardward part, I have a RTX 3090, so I can choose any ollama model working with up to 24Mb of vram. &lt;/p&gt; &lt;p&gt;I have &lt;strong&gt;openwebui&lt;/strong&gt;, and started to make some test with the integrated document feature, but for the option or improve it, it's difficult to understand the impact of each option&lt;/p&gt; &lt;p&gt;I have tested briefly &lt;strong&gt;PageAssist&lt;/strong&gt; in chrome, but honestly, it's like it doesn't work, despite I followed a youtube tutorial. &lt;/p&gt; &lt;p&gt;is there anything else I should try ? I saw a mention to LightRag ?&lt;br /&gt; as things are moving so fast, it's hard to know where to start, and even when it works, you don't know if you are not missing an option or a tip. thanks by advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LivingSignificant452"&gt; /u/LivingSignificant452 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7fg95/need_feedback_for_a_rag_using_ollama_as_background/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7fg95/need_feedback_for_a_rag_using_ollama_as_background/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7fg95/need_feedback_for_a_rag_using_ollama_as_background/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T20:24:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7ngkn</id>
    <title>Chonkie update.</title>
    <updated>2025-06-10T02:22:31+00:00</updated>
    <author>
      <name>/u/dnr41418</name>
      <uri>https://old.reddit.com/user/dnr41418</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Launch HN: Chonkie (YC X25) – Open-Source Library for Advanced Chunking | &lt;a href="https://news.ycombinator.com/item?id=44225930"&gt;https://news.ycombinator.com/item?id=44225930&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dnr41418"&gt; /u/dnr41418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ngkn/chonkie_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ngkn/chonkie_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ngkn/chonkie_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T02:22:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1l76cg2</id>
    <title>7900 XTX what are your go-to models for 24GB VRAM?</title>
    <updated>2025-06-09T14:32:15+00:00</updated>
    <author>
      <name>/u/BillyTheMilli</name>
      <uri>https://old.reddit.com/user/BillyTheMilli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just finished my new build with a 7900 XTX and I'm looking for some model recommendations.&lt;/p&gt; &lt;p&gt;Since most of the talk is CUDA-centric, I'm curious what my AMD users are running. I've got 24GB of VRAM to play with and I'm mainly looking for good models for general purpose chat/reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BillyTheMilli"&gt; /u/BillyTheMilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l76cg2/7900_xtx_what_are_your_goto_models_for_24gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l76cg2/7900_xtx_what_are_your_goto_models_for_24gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l76cg2/7900_xtx_what_are_your_goto_models_for_24gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T14:32:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6urvw</id>
    <title>Gemini 2.5 Flash plays Final Fantasy in real-time but gets stuck...</title>
    <updated>2025-06-09T03:29:08+00:00</updated>
    <author>
      <name>/u/ZhalexDev</name>
      <uri>https://old.reddit.com/user/ZhalexDev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6urvw/gemini_25_flash_plays_final_fantasy_in_realtime/"&gt; &lt;img alt="Gemini 2.5 Flash plays Final Fantasy in real-time but gets stuck..." src="https://external-preview.redd.it/cnVvdWR3c2RtdDVmMY0aYliuaUJ6RykjdFncok76V91JG_1sGT9Nkds3i_jF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58084eea8db99337bb3ecabbca254f7e62fa7193" title="Gemini 2.5 Flash plays Final Fantasy in real-time but gets stuck..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some more clips of frontier VLMs on games (gemini-2.5-flash-preview-04-17) on &lt;a href="https://www.vgbench.com/"&gt;VideoGameBench&lt;/a&gt;. Here is just unedited footage, where the model is able to defeat the first &amp;quot;mini-boss&amp;quot; with real-time combat but also gets stuck in the menu screens, despite having it in its prompt how to get out.&lt;/p&gt; &lt;p&gt;Generated from &lt;a href="https://github.com/alexzhang13/VideoGameBench"&gt;https://github.com/alexzhang13/VideoGameBench&lt;/a&gt; and recorded on OBS.&lt;/p&gt; &lt;p&gt;tldr; we're still pretty far from embodied intelligence&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZhalexDev"&gt; /u/ZhalexDev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kun6x1tdmt5f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6urvw/gemini_25_flash_plays_final_fantasy_in_realtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6urvw/gemini_25_flash_plays_final_fantasy_in_realtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T03:29:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7m2q7</id>
    <title>WINA from Microsoft</title>
    <updated>2025-06-10T01:13:13+00:00</updated>
    <author>
      <name>/u/mas554ter365</name>
      <uri>https://old.reddit.com/user/mas554ter365</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did anyone tested this on actual setup of the local model? Would like to know if there is possibility to spend less money on local setup and still get good output.&lt;br /&gt; &lt;a href="https://github.com/microsoft/wina"&gt;https://github.com/microsoft/wina&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mas554ter365"&gt; /u/mas554ter365 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7m2q7/wina_from_microsoft/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7m2q7/wina_from_microsoft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7m2q7/wina_from_microsoft/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T01:13:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6vc8u</id>
    <title>I made the move and I'm in love. RTX Pro 6000 Workstation</title>
    <updated>2025-06-09T04:01:23+00:00</updated>
    <author>
      <name>/u/Demonicated</name>
      <uri>https://old.reddit.com/user/Demonicated</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6vc8u/i_made_the_move_and_im_in_love_rtx_pro_6000/"&gt; &lt;img alt="I made the move and I'm in love. RTX Pro 6000 Workstation" src="https://preview.redd.it/7uu5ooyast5f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1dd757acde7feb8ae7c2f694103a8cab2dbaab8c" title="I made the move and I'm in love. RTX Pro 6000 Workstation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're running a workload that's processing millions of records and analyzing using Magentic One (autogen) and the 4090 just want cutting it. With the way scalpers are preying on would be 5090 owners, it was much easier to pick one of these up. Plus significantly less wattage. Just posting cause I'm super excited. &lt;/p&gt; &lt;p&gt;What's the best tool model I can run with this bad boy? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Demonicated"&gt; /u/Demonicated &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7uu5ooyast5f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6vc8u/i_made_the_move_and_im_in_love_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6vc8u/i_made_the_move_and_im_in_love_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T04:01:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7k3s4</id>
    <title>CLI for Chatterbox TTS</title>
    <updated>2025-06-09T23:38:48+00:00</updated>
    <author>
      <name>/u/init0</name>
      <uri>https://old.reddit.com/user/init0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7k3s4/cli_for_chatterbox_tts/"&gt; &lt;img alt="CLI for Chatterbox TTS" src="https://external-preview.redd.it/MoP6enMQ2Q6o4o23d5xCmvlBtpeCXWiqxc63UVCX5Rk.jpg?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfd7f76ac4c13cdc287edd9856ef0430dbc862a5" title="CLI for Chatterbox TTS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/init0"&gt; /u/init0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://pypi.org/project/voice-forge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7k3s4/cli_for_chatterbox_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7k3s4/cli_for_chatterbox_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T23:38:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7ab18</id>
    <title>Lightweight writing model as of June 2025</title>
    <updated>2025-06-09T17:07:34+00:00</updated>
    <author>
      <name>/u/Royal_Light_9921</name>
      <uri>https://old.reddit.com/user/Royal_Light_9921</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can you please recommend a model ? I've tried these so far :&lt;/p&gt; &lt;p&gt;Mistral Creative 24b : good overall, my favorite, quite fast, but actually lacks a bit of creativity....&lt;/p&gt; &lt;p&gt;Gemma2 Writer 9b : very fun to read, fast, but forgets everything after 3 messages. My favorite to generate ideas and create short dialogue, role play.&lt;/p&gt; &lt;p&gt;Gemma3 27b : Didn't like that much, maybe I need a finetune, but the base model is full of phrases like &amp;quot;My living room is a battlefield of controllers and empty soda cans – remnants of our nightly ritual. (AI slop i believe is what it's called?). &lt;/p&gt; &lt;p&gt;Qwen3 and QwQ just keep repeating themselves, and the reasoning in them makes things worse usually, they always come up with weird conclusions...&lt;/p&gt; &lt;p&gt;So ideally I would like something in between Mistral Creative and Gemma2 Writer. Any ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Royal_Light_9921"&gt; /u/Royal_Light_9921 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ab18/lightweight_writing_model_as_of_june_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ab18/lightweight_writing_model_as_of_june_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ab18/lightweight_writing_model_as_of_june_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T17:07:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1l748qc</id>
    <title>Why isn't it common for companies to compare the evaluation of the different quantizations of their model?</title>
    <updated>2025-06-09T13:03:06+00:00</updated>
    <author>
      <name>/u/ArcaneThoughts</name>
      <uri>https://old.reddit.com/user/ArcaneThoughts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it not as trivial as it sounds? Are they scared of showing lower scoring evaluations in case users confuse them for the original ones?&lt;/p&gt; &lt;p&gt;It would be so useful when choosing a gguf version to know how much accuracy loss each has. Like I'm sure there are many models where Qn vs Qn+1 are indistinguishable in performance so in that case you would know not to pick Qn+1 and prefer Qn.&lt;/p&gt; &lt;p&gt;Am I missing something?&lt;/p&gt; &lt;p&gt;edit: I'm referring to companies that release their own quantizations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArcaneThoughts"&gt; /u/ArcaneThoughts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l748qc/why_isnt_it_common_for_companies_to_compare_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l748qc/why_isnt_it_common_for_companies_to_compare_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l748qc/why_isnt_it_common_for_companies_to_compare_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T13:03:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7ksm0</id>
    <title>Medical language model - for STT and summarize things</title>
    <updated>2025-06-10T00:10:36+00:00</updated>
    <author>
      <name>/u/ed0c</name>
      <uri>https://old.reddit.com/user/ed0c</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I'd like to use a language model via ollama/openwebui to summarize medical reports. &lt;/p&gt; &lt;p&gt;I've tried several models, but I'm not happy with the results. I was thinking that there might be pre-trained models for this task that know medical language.&lt;/p&gt; &lt;p&gt;My goal: STT and then summarize my medical consultations, home visits, etc.&lt;/p&gt; &lt;p&gt;Note that the model must be adapted to the French language. I'm a french guy..&lt;/p&gt; &lt;p&gt;And for that I have a war machine: 5070ti with 16gb of VRAM and 32Gb of RAM.&lt;/p&gt; &lt;p&gt;Any ideas for completing this project?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ed0c"&gt; /u/ed0c &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ksm0/medical_language_model_for_stt_and_summarize/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ksm0/medical_language_model_for_stt_and_summarize/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ksm0/medical_language_model_for_stt_and_summarize/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T00:10:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6x91g</id>
    <title>Use Ollama to run agents that watch your screen! (100% Local and Open Source)</title>
    <updated>2025-06-09T05:58:30+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6x91g/use_ollama_to_run_agents_that_watch_your_screen/"&gt; &lt;img alt="Use Ollama to run agents that watch your screen! (100% Local and Open Source)" src="https://external-preview.redd.it/YjkwOGhtajRkdTVmMZ0cZOsTXi-ThTayE7iEfGGYXF4Z17hX-7dpetBO2beo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f5e81cd85f9cc73982ab5326f6dce727d4078f2" title="Use Ollama to run agents that watch your screen! (100% Local and Open Source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/tysofmj4du5f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6x91g/use_ollama_to_run_agents_that_watch_your_screen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6x91g/use_ollama_to_run_agents_that_watch_your_screen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T05:58:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1l73294</id>
    <title>H company - Holo1 7B</title>
    <updated>2025-06-09T12:06:38+00:00</updated>
    <author>
      <name>/u/TacGibs</name>
      <uri>https://old.reddit.com/user/TacGibs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l73294/h_company_holo1_7b/"&gt; &lt;img alt="H company - Holo1 7B" src="https://preview.redd.it/ph3t561w6w5f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d0039ef982c60176689cf04d36365e4bbf966f0" title="H company - Holo1 7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Hcompany/Holo1-7B"&gt;https://huggingface.co/Hcompany/Holo1-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper : &lt;a href="https://huggingface.co/papers/2506.02865"&gt;https://huggingface.co/papers/2506.02865&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The H company (a French AI startup) released this model, and I haven't seen anyone talk about it here despite the great performance showed on benchmarks for GUI agentic use.&lt;/p&gt; &lt;p&gt;Did anyone tried it ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TacGibs"&gt; /u/TacGibs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ph3t561w6w5f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l73294/h_company_holo1_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l73294/h_company_holo1_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T12:06:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7iyim</id>
    <title>Where is wizardLM now ?</title>
    <updated>2025-06-09T22:47:17+00:00</updated>
    <author>
      <name>/u/Killerx7c</name>
      <uri>https://old.reddit.com/user/Killerx7c</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone know where are these guys? I think they disappeared 2 years ago with no information &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Killerx7c"&gt; /u/Killerx7c &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7iyim/where_is_wizardlm_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7iyim/where_is_wizardlm_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7iyim/where_is_wizardlm_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T22:47:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6v37m</id>
    <title>1.93bit Deepseek R1 0528 beats Claude Sonnet 4</title>
    <updated>2025-06-09T03:46:57+00:00</updated>
    <author>
      <name>/u/BumblebeeOk3281</name>
      <uri>https://old.reddit.com/user/BumblebeeOk3281</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;1.93bit Deepseek R1 0528 beats Claude Sonnet 4 (no think) on Aiders Polygot Benchmark. Unsloth's IQ1_M GGUF at 200GB fit with 65535 context into 224gb of VRAM and scored 60% which is over Claude 4's &amp;lt;no think&amp;gt; benchmark of 56.4%. Source: &lt;a href="https://aider.chat/docs/leaderboards/"&gt;https://aider.chat/docs/leaderboards/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;── tmp.benchmarks/2025-06-07-17-01-03--R1-0528-IQ1_M ─- dirname: 2025-06-07-17-01-03--R1-0528-IQ1_M&lt;/p&gt; &lt;p&gt;test_cases: 225&lt;/p&gt; &lt;p&gt;model: unsloth/DeepSeek-R1-0528-GGUF&lt;/p&gt; &lt;p&gt;edit_format: diff&lt;/p&gt; &lt;p&gt;commit_hash: 4c161f9&lt;/p&gt; &lt;p&gt;pass_rate_1: 25.8&lt;/p&gt; &lt;p&gt;pass_rate_2: 60.0&lt;/p&gt; &lt;p&gt;pass_num_1: 58&lt;/p&gt; &lt;p&gt;pass_num_2: 135&lt;/p&gt; &lt;p&gt;percent_cases_well_formed: 96.4&lt;/p&gt; &lt;p&gt;error_outputs: 9&lt;/p&gt; &lt;p&gt;num_malformed_responses: 9&lt;/p&gt; &lt;p&gt;num_with_malformed_responses: 8&lt;/p&gt; &lt;p&gt;user_asks: 104&lt;/p&gt; &lt;p&gt;lazy_comments: 0&lt;/p&gt; &lt;p&gt;syntax_errors: 0&lt;/p&gt; &lt;p&gt;indentation_errors: 0&lt;/p&gt; &lt;p&gt;exhausted_context_windows: 0&lt;/p&gt; &lt;p&gt;prompt_tokens: 2733132&lt;/p&gt; &lt;p&gt;completion_tokens: 2482855&lt;/p&gt; &lt;p&gt;test_timeouts: 6&lt;/p&gt; &lt;p&gt;total_tests: 225&lt;/p&gt; &lt;p&gt;command: aider --model unsloth/DeepSeek-R1-0528-GGUF&lt;/p&gt; &lt;p&gt;date: 2025-06-07&lt;/p&gt; &lt;p&gt;versions: &lt;a href="http://0.84.1.dev"&gt;0.84.1.dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;seconds_per_case: 527.8&lt;/p&gt; &lt;p&gt;./build/bin/llama-server --model unsloth/DeepSeek-R1-0528-GGUF/UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00001-of-00005.gguf --threads 16 --n-gpu-layers 507 --prio 3 --temp 0.6 --top_p 0.95 --min-p 0.01 --ctx-size 65535 --host 0.0.0.0 --host 0.0.0.0 --tensor-split 0.55,0.15,0.16,0.06,0.11,0.12 -fa&lt;/p&gt; &lt;p&gt;Device 0: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes&lt;/p&gt; &lt;p&gt;Device 1: NVIDIA GeForce RTX 5090, compute capability 12.0, VMM: yes&lt;/p&gt; &lt;p&gt;Device 2: NVIDIA GeForce RTX 5090, compute capability 12.0, VMM: yes&lt;/p&gt; &lt;p&gt;Device 3: NVIDIA GeForce RTX 4080, compute capability 8.9, VMM: yes&lt;/p&gt; &lt;p&gt;Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes&lt;/p&gt; &lt;p&gt;Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BumblebeeOk3281"&gt; /u/BumblebeeOk3281 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6v37m/193bit_deepseek_r1_0528_beats_claude_sonnet_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6v37m/193bit_deepseek_r1_0528_beats_claude_sonnet_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6v37m/193bit_deepseek_r1_0528_beats_claude_sonnet_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T03:46:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1l75tp1</id>
    <title>I built a Code Agent that writes code and live-debugs itself by reading and walking the call stack.</title>
    <updated>2025-06-09T14:11:07+00:00</updated>
    <author>
      <name>/u/bn_from_zentara</name>
      <uri>https://old.reddit.com/user/bn_from_zentara</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l75tp1/i_built_a_code_agent_that_writes_code_and/"&gt; &lt;img alt="I built a Code Agent that writes code and live-debugs itself by reading and walking the call stack." src="https://external-preview.redd.it/M3NzcG44YWxzdzVmMSD9styMP4r1TEey8oyCG95ikT3wdTqvY8nF4QJV9K9T.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e8567a348bd34184b221b126559e0bca5f778522" title="I built a Code Agent that writes code and live-debugs itself by reading and walking the call stack." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bn_from_zentara"&gt; /u/bn_from_zentara &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b1pnpj9lsw5f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l75tp1/i_built_a_code_agent_that_writes_code_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l75tp1/i_built_a_code_agent_that_writes_code_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T14:11:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1l71iie</id>
    <title>Concept graph workflow in Open WebUI</title>
    <updated>2025-06-09T10:41:50+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l71iie/concept_graph_workflow_in_open_webui/"&gt; &lt;img alt="Concept graph workflow in Open WebUI" src="https://external-preview.redd.it/aXB5aHN0YTlydjVmMUDafYoOCCtYLjNpQgqDHTqQwNrdDTG86AmqQ0wIdIFQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c2a7c9d31f1ad19416b2226d54eabeba40a3068" title="Concept graph workflow in Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What is this?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Reasoning workflow where LLM thinks about the concepts that are related to the User's query and then makes a final answer based on that&lt;/li&gt; &lt;li&gt;Workflow runs within OpenAI-compatible LLM proxy. It streams a special HTML artifact that connects back to the workflow and listens for events from it to display in the visualisation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/av/harbor/blob/main/boost/src/modules/concept.py#L135"&gt;Code&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dzeqvwa9rv5f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l71iie/concept_graph_workflow_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l71iie/concept_graph_workflow_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T10:41:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7j7uk</id>
    <title>Now that 256GB DDR5 is possible on consumer hardware PC, is it worth it for inference?</title>
    <updated>2025-06-09T22:58:42+00:00</updated>
    <author>
      <name>/u/waiting_for_zban</name>
      <uri>https://old.reddit.com/user/waiting_for_zban</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The &lt;a href="https://www.amazon.com/dp/B0DSR5P84D/"&gt;128GB Kit (2x 64GB)&lt;/a&gt; are already available since early this year, making it possible to put 256 GB on consumer PC hardware. &lt;/p&gt; &lt;p&gt;Paired with a dual 3090 or dual 4090, would it be possible to load big models for inference at an acceptable speed? Or offloading will always be slow?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waiting_for_zban"&gt; /u/waiting_for_zban &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7j7uk/now_that_256gb_ddr5_is_possible_on_consumer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7j7uk/now_that_256gb_ddr5_is_possible_on_consumer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7j7uk/now_that_256gb_ddr5_is_possible_on_consumer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T22:58:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6ibwg</id>
    <title>When you figure out it’s all just math:</title>
    <updated>2025-06-08T17:53:48+00:00</updated>
    <author>
      <name>/u/Current-Ticket4214</name>
      <uri>https://old.reddit.com/user/Current-Ticket4214</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6ibwg/when_you_figure_out_its_all_just_math/"&gt; &lt;img alt="When you figure out it’s all just math:" src="https://preview.redd.it/t7ko9eywrq5f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82581f5bc2e1251bb77594995cdd04eccde6717a" title="When you figure out it’s all just math:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Current-Ticket4214"&gt; /u/Current-Ticket4214 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t7ko9eywrq5f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6ibwg/when_you_figure_out_its_all_just_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6ibwg/when_you_figure_out_its_all_just_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-08T17:53:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7mijq</id>
    <title>I found a DeepSeek-R1-0528-Distill-Qwen3-32B</title>
    <updated>2025-06-10T01:35:01+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7mijq/i_found_a_deepseekr10528distillqwen332b/"&gt; &lt;img alt="I found a DeepSeek-R1-0528-Distill-Qwen3-32B" src="https://preview.redd.it/ear6iov2706f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7cf29ee3fdbf774f271f614b7665f27ad55a954c" title="I found a DeepSeek-R1-0528-Distill-Qwen3-32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Their authors said: &lt;/p&gt; &lt;h1&gt;Our Approach to DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT:&lt;/h1&gt; &lt;p&gt;Since Qwen3 did not provide a pre-trained base for its 32B model, our initial step was to perform &lt;strong&gt;additional pre-training&lt;/strong&gt; on Qwen3-32B using a &lt;strong&gt;self-constructed multilingual pre-training dataset&lt;/strong&gt;. This was done to restore a &amp;quot;pre-training style&amp;quot; model base as much as possible, ensuring that subsequent work would not be influenced by Qwen3's inherent SFT language style. This model will also be open-sourced in the future.&lt;/p&gt; &lt;p&gt;Building on this foundation, we attempted distillation from R1-0528 and completed an early preview version: &lt;strong&gt;DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;In this version, we referred to the configuration from Fei-Fei Li's team in their work &amp;quot;s1: Simple test-time scaling.&amp;quot; We tried training with a small amount of data over multiple epochs. We discovered that by using only about &lt;strong&gt;10% of our available distillation data&lt;/strong&gt;, we could achieve a model with a language style and reasoning approach very close to the original R1-0528.&lt;/p&gt; &lt;p&gt;We have included a Chinese evaluation report in the model repository for your reference. Some datasets have also been uploaded to Hugging Face, hoping to assist other open-source enthusiasts in their work.&lt;/p&gt; &lt;h1&gt;Next Steps:&lt;/h1&gt; &lt;p&gt;Moving forward, we will further expand our distillation data and train the next version of the 32B model with a larger dataset (expected to be released within a few days). We also plan to train open-source models of different sizes, such as 4B and 72B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ear6iov2706f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7mijq/i_found_a_deepseekr10528distillqwen332b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7mijq/i_found_a_deepseekr10528distillqwen332b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T01:35:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7ek6n</id>
    <title>Apple Intelligence on device model available to developers</title>
    <updated>2025-06-09T19:50:39+00:00</updated>
    <author>
      <name>/u/Ssjultrainstnict</name>
      <uri>https://old.reddit.com/user/Ssjultrainstnict</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ek6n/apple_intelligence_on_device_model_available_to/"&gt; &lt;img alt="Apple Intelligence on device model available to developers" src="https://external-preview.redd.it/rG_nlJemzl12v1uLGyDH2DT5RlL4_1RSptq8UoALUQw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0dc187472b01eafad499fadd404dc388928bed5b" title="Apple Intelligence on device model available to developers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like they are going to expose an API that will let you use the model to build experiences. The details on it are sparse, but cool and exciting development for us LocalLlama folks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ssjultrainstnict"&gt; /u/Ssjultrainstnict &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.apple.com/newsroom/2025/06/apple-intelligence-gets-even-more-powerful-with-new-capabilities-across-apple-devices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ek6n/apple_intelligence_on_device_model_available_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ek6n/apple_intelligence_on_device_model_available_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T19:50:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7gxpb</id>
    <title>LMStudio on screen in WWDC Platform State of the Union</title>
    <updated>2025-06-09T21:22:36+00:00</updated>
    <author>
      <name>/u/Specialist_Cup968</name>
      <uri>https://old.reddit.com/user/Specialist_Cup968</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7gxpb/lmstudio_on_screen_in_wwdc_platform_state_of_the/"&gt; &lt;img alt="LMStudio on screen in WWDC Platform State of the Union" src="https://preview.redd.it/ymvbqxynxy5f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04c533a5ef6eac83396175c657f8e913c63d5e02" title="LMStudio on screen in WWDC Platform State of the Union" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its nice to see local llm support in the next version of Xcode &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist_Cup968"&gt; /u/Specialist_Cup968 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ymvbqxynxy5f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7gxpb/lmstudio_on_screen_in_wwdc_platform_state_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7gxpb/lmstudio_on_screen_in_wwdc_platform_state_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T21:22:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1l76ab7</id>
    <title>DeepSeek R1 0528 Hits 71% (+14.5 pts from R1) on Aider Polyglot Coding Leaderboard</title>
    <updated>2025-06-09T14:29:54+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l76ab7/deepseek_r1_0528_hits_71_145_pts_from_r1_on_aider/"&gt; &lt;img alt="DeepSeek R1 0528 Hits 71% (+14.5 pts from R1) on Aider Polyglot Coding Leaderboard" src="https://external-preview.redd.it/iUsfwiVJPYLjTSAVy9M84yJWl92m3NW-HLg-4yfog9U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a4c25f54ed06b5f744ff2faad7914958769cc14" title="DeepSeek R1 0528 Hits 71% (+14.5 pts from R1) on Aider Polyglot Coding Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/geu3ik68ww5f1.png?width=1346&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6b4583095291cd1f6a9a91e5fe5642965e9cfde3"&gt;https://preview.redd.it/geu3ik68ww5f1.png?width=1346&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6b4583095291cd1f6a9a91e5fe5642965e9cfde3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Full leaderboard:&lt;/em&gt; &lt;a href="https://aider.chat/docs/leaderboards/"&gt;&lt;em&gt;https://aider.chat/docs/leaderboards/&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l76ab7/deepseek_r1_0528_hits_71_145_pts_from_r1_on_aider/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l76ab7/deepseek_r1_0528_hits_71_145_pts_from_r1_on_aider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l76ab7/deepseek_r1_0528_hits_71_145_pts_from_r1_on_aider/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T14:29:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7l39m</id>
    <title>Apple's On Device Foundation Models LLM is 3B quantized to 2 bits</title>
    <updated>2025-06-10T00:25:05+00:00</updated>
    <author>
      <name>/u/iKy1e</name>
      <uri>https://old.reddit.com/user/iKy1e</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;The on-device model we just used is a large language model with &lt;strong&gt;3 billion parameters&lt;/strong&gt;, each quantized to &lt;strong&gt;2 bits&lt;/strong&gt;. It is several orders of magnitude bigger than any other models that are part of the operating system.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Source: Meet the Foundation Models framework&lt;br /&gt; Timestamp: 2:57&lt;br /&gt; URL: &lt;a href="https://developer.apple.com/videos/play/wwdc2025/286/?time=175"&gt;https://developer.apple.com/videos/play/wwdc2025/286/?time=175&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The framework also supports adapters:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;For certain common use cases, such as content tagging, we also provide specialized adapters that maximize the model’s capability in specific domains.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And structured output:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Generable type, you can make the model respond to prompts by generating an instance of your type.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And tool calling:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;At this phase, the FoundationModels framework will automatically call the code you wrote for these tools. The framework then automatically inserts the tool outputs back into the transcript. Finally, the model will incorporate the tool output along with everything else in the transcript to furnish the final response.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iKy1e"&gt; /u/iKy1e &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7l39m/apples_on_device_foundation_models_llm_is_3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7l39m/apples_on_device_foundation_models_llm_is_3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7l39m/apples_on_device_foundation_models_llm_is_3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T00:25:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1l75fc8</id>
    <title>KVzip: Query-agnostic KV Cache Eviction — 3~4× memory reduction and 2× lower decoding latency</title>
    <updated>2025-06-09T13:54:45+00:00</updated>
    <author>
      <name>/u/janghyun1230</name>
      <uri>https://old.reddit.com/user/janghyun1230</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l75fc8/kvzip_queryagnostic_kv_cache_eviction_34_memory/"&gt; &lt;img alt="KVzip: Query-agnostic KV Cache Eviction — 3~4× memory reduction and 2× lower decoding latency" src="https://preview.redd.it/bpxlu6tfnw5f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=351cc92c28950c272bbcdaec0981dd5beb03e8cc" title="KVzip: Query-agnostic KV Cache Eviction — 3~4× memory reduction and 2× lower decoding latency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! We've released KVzip, a KV cache compression method designed to support diverse future queries. You can try the demo on GitHub! Supported models include Qwen3/2.5, Gemma3, and LLaMA3. &lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/snu-mllab/KVzip"&gt;https://github.com/snu-mllab/KVzip&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2505.23416"&gt;https://arxiv.org/abs/2505.23416&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://janghyun1230.github.io/kvzip"&gt;https://janghyun1230.github.io/kvzip&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/janghyun1230"&gt; /u/janghyun1230 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bpxlu6tfnw5f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l75fc8/kvzip_queryagnostic_kv_cache_eviction_34_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l75fc8/kvzip_queryagnostic_kv_cache_eviction_34_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T13:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7dj3z</id>
    <title>China starts mass producing a Ternary AI Chip.</title>
    <updated>2025-06-09T19:11:14+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As reported earlier here.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.scmp.com/news/china/science/article/3301229/chinese-scientists-build-worlds-first-ai-chip-made-carbon-and-its-super-fast"&gt;https://www.scmp.com/news/china/science/article/3301229/chinese-scientists-build-worlds-first-ai-chip-made-carbon-and-its-super-fast&lt;/a&gt;&lt;/p&gt; &lt;p&gt;China starts mass production of a Ternary AI Chip.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.scmp.com/news/china/science/article/3313349/beyond-1s-and-0s-china-starts-mass-production-worlds-first-non-binary-ai-chip"&gt;https://www.scmp.com/news/china/science/article/3313349/beyond-1s-and-0s-china-starts-mass-production-worlds-first-non-binary-ai-chip&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wonder if Ternary models like bitnet could be run super fast on it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7dj3z/china_starts_mass_producing_a_ternary_ai_chip/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7dj3z/china_starts_mass_producing_a_ternary_ai_chip/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7dj3z/china_starts_mass_producing_a_ternary_ai_chip/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T19:11:14+00:00</published>
  </entry>
</feed>
