<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-04T10:38:09+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j3621h</id>
    <title>What prompt template should be used for finetuning on Unsloth ?</title>
    <updated>2025-03-04T08:13:02+00:00</updated>
    <author>
      <name>/u/Prior-Blood5979</name>
      <uri>https://old.reddit.com/user/Prior-Blood5979</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to finetune an llama-3b on Unsloth. I used the given alpaca format and it's training is good. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Then I thought I can improve it by using llama prompt template instead of alpaca.&lt;/strong&gt; But It's not even as good as using Alpaca format. &lt;/p&gt; &lt;p&gt;Why is this happening. Am I doing something wrong here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prior-Blood5979"&gt; /u/Prior-Blood5979 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3621h/what_prompt_template_should_be_used_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3621h/what_prompt_template_should_be_used_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3621h/what_prompt_template_should_be_used_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T08:13:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3789k</id>
    <title>How to run hardware accelerated Ollama on integrated GPU, like Radeon 780M on Linux.</title>
    <updated>2025-03-04T09:44:57+00:00</updated>
    <author>
      <name>/u/Sensitive-Leather-32</name>
      <uri>https://old.reddit.com/user/Sensitive-Leather-32</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For hardware acceleration you could use either &lt;strong&gt;ROCm&lt;/strong&gt; or &lt;strong&gt;Vulkan&lt;/strong&gt;. Ollama devs &lt;strong&gt;don't want to merge Vulkan&lt;/strong&gt; integration, so better use ROCm if you can. It has slightly worse performance, but is easier to run.&lt;/p&gt; &lt;p&gt;If you still need Vulkan, you can &lt;a href="https://github.com/whyvl/ollama-vulkan"&gt;find a fork here&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Installation&lt;/h1&gt; &lt;p&gt;I am running Archlinux, so installed ollama and ollama-rocm. Rocm dependencies are installed automatically.&lt;/p&gt; &lt;p&gt;You can also follow &lt;a href="https://github.com/ollama/ollama/blob/ddb6dc81c26721a08453f1db7f2727076e97dabc/docs/tutorials/amd-igpu-780m.md"&gt;this guide for other distributions&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Override env&lt;/h1&gt; &lt;p&gt;If you have &amp;quot;unsupported&amp;quot; GPU, set &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=11.0.2&lt;/code&gt; in &lt;code&gt;/etc/systemd/system/ollama.service.d/override.conf&lt;/code&gt; this way:&lt;/p&gt; &lt;p&gt;&lt;code&gt;[Service]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Environment=&amp;quot;your env value&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;then run &lt;code&gt;sudo systemctl daemon-reload &amp;amp;&amp;amp; sudo systemctl restart ollama.service&lt;/code&gt;&lt;/p&gt; &lt;p&gt;For different GPUs you may need to try different override values like 9.0.0, 9.4.6. Google them.)&lt;/p&gt; &lt;h1&gt;APU fix patch&lt;/h1&gt; &lt;p&gt;You probably need &lt;a href="https://github.com/ollama/ollama/pull/6282"&gt;this &lt;/a&gt;&lt;a href="https://github.com/ollama/ollama/pull/6282"&gt;patch&lt;/a&gt; until it gets merged. There is a &lt;a href="https://gitlab.com/zhaose233/ollama-rocm-package/-/jobs/9282301273/artifacts/browse"&gt;repo with CI with patched packages&lt;/a&gt; for Archlinux.&lt;/p&gt; &lt;h1&gt;Increase GTT size&lt;/h1&gt; &lt;p&gt;If you want to run big models with a bigger context, you have to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ixy5kf/comment/mfoptfw/?context=3"&gt;set GTT size according to this guide&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Amdgpu kernel bug&lt;/h1&gt; &lt;p&gt;Later during high GPU load I got freezes and graphics restarts with the &lt;a href="https://gist.github.com/AntonIXO/3f49ce8453766a4c07c8920c4c9d2b01"&gt;following logs in dmesg&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The only way to fix it is to build a &lt;a href="https://lore.kernel.org/lkml/20241127114638.11216-1-lamikr@gmail.com/T/"&gt;kernel with this patch&lt;/a&gt;.(&lt;/p&gt; &lt;h1&gt;Performance tips&lt;/h1&gt; &lt;p&gt;You can also set these env valuables to get better generation speed:&lt;/p&gt; &lt;p&gt;HSA_ENABLE_SDMA=0&lt;br /&gt; HSA_ENABLE_COMPRESSION=1&lt;br /&gt; &lt;a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-enable-flash-attention"&gt;OLLAMA_FLASH_ATTENTION=1&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-set-the-quantization-type-for-the-kv-cache"&gt;OLLAMA_KV_CACHE_TYPE=q8_0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Specify max context with: OLLAMA_CONTEXT_LENGTH=16382 # 16k (move context - more ram)&lt;/p&gt; &lt;p&gt;OLLAMA_NEW_ENGINE - does not work for me.&lt;/p&gt; &lt;p&gt;Now you got HW accelerated LLMs on your APUsüéâ Check it with &lt;strong&gt;ollama ps&lt;/strong&gt; and &lt;strong&gt;amdgpu_top&lt;/strong&gt; utility.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sensitive-Leather-32"&gt; /u/Sensitive-Leather-32 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3789k/how_to_run_hardware_accelerated_ollama_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3789k/how_to_run_hardware_accelerated_ollama_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3789k/how_to_run_hardware_accelerated_ollama_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T09:44:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2wzky</id>
    <title>Which do you think is better: Deepseek-R1-32B vs QWQ-32b-preview?</title>
    <updated>2025-03-03T23:40:14+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using QWQ-32b-preview for a lot of different things, only to find out Qwen2.5-coder-32b provides comparable performance in math and coding.&lt;/p&gt; &lt;p&gt;This made me question whether QWQ really was an effective reasoning model, but my comparison will not be against Coder, but with Deepseek-r1-32B instead. Is the latter model any better than QWQ? I would really like to settle the matter.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2wzky/which_do_you_think_is_better_deepseekr132b_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2wzky/which_do_you_think_is_better_deepseekr132b_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2wzky/which_do_you_think_is_better_deepseekr132b_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T23:40:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j37eto</id>
    <title>Are there prompt reducers that reduce the size of inputs in to LLMs?</title>
    <updated>2025-03-04T09:58:41+00:00</updated>
    <author>
      <name>/u/PulIthEld</name>
      <uri>https://old.reddit.com/user/PulIthEld</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLMs only have so much memory, so it seems we should be screening prompts that people type or copy and paste and reduce the size of them, no?&lt;/p&gt; &lt;p&gt;Does white space and variable length matter for pasted code? Would it make sense to minimize JS, for example?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PulIthEld"&gt; /u/PulIthEld &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j37eto/are_there_prompt_reducers_that_reduce_the_size_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j37eto/are_there_prompt_reducers_that_reduce_the_size_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j37eto/are_there_prompt_reducers_that_reduce_the_size_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T09:58:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2horr</id>
    <title>NLP Brain-to-Text Decoding: A Non-invasive Approach via Typing</title>
    <updated>2025-03-03T12:36:04+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2horr/nlp_braintotext_decoding_a_noninvasive_approach/"&gt; &lt;img alt="NLP Brain-to-Text Decoding: A Non-invasive Approach via Typing" src="https://preview.redd.it/8gyz8kzsygme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a88d058b7bc64d9941684f9dc53c45071cf7f231" title="NLP Brain-to-Text Decoding: A Non-invasive Approach via Typing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ai.meta.com/research/publications/brain-to-text-decoding-a-non-invasive-approach-via-typing/"&gt;https://ai.meta.com/research/publications/brain-to-text-decoding-a-non-invasive-approach-via-typing/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8gyz8kzsygme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2horr/nlp_braintotext_decoding_a_noninvasive_approach/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2horr/nlp_braintotext_decoding_a_noninvasive_approach/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T12:36:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1j36j4a</id>
    <title>I'm running Ollama for a project and I wanted to know if there's easy documentation on how to fine-tune or RAG an LLM ?</title>
    <updated>2025-03-04T08:50:42+00:00</updated>
    <author>
      <name>/u/Darkoplax</name>
      <uri>https://old.reddit.com/user/Darkoplax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw couple of videos but it wasn't intuitive so I thought I would ask here if there's an easy way to fine-tune/RAG (still dont understand the difference) an LLM that I downloaded from Ollama&lt;/p&gt; &lt;p&gt;I'm creating a chatbot ai app and I have some data that I want to insert on the LLM ... I'm mostly a Frontend/JS dev so I'm not that good at python-stuff&lt;/p&gt; &lt;p&gt;So far I got my app running locally and hooked it up with Vercel's AI SDK to my app and it works well ; I just need to insert my pdf/csv data &lt;/p&gt; &lt;p&gt;Any help is apperciated&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Darkoplax"&gt; /u/Darkoplax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j36j4a/im_running_ollama_for_a_project_and_i_wanted_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j36j4a/im_running_ollama_for_a_project_and_i_wanted_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j36j4a/im_running_ollama_for_a_project_and_i_wanted_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T08:50:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j29mi4</id>
    <title>Me Today</title>
    <updated>2025-03-03T03:38:52+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29mi4/me_today/"&gt; &lt;img alt="Me Today" src="https://preview.redd.it/qrxhvlblaeme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a2767bc89a037159368246cac9dac0d3050c85f" title="Me Today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qrxhvlblaeme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29mi4/me_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j29mi4/me_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T03:38:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j29hm0</id>
    <title>New Atom of Thoughts looks promising for helping smaller models reason</title>
    <updated>2025-03-03T03:31:16+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29hm0/new_atom_of_thoughts_looks_promising_for_helping/"&gt; &lt;img alt="New Atom of Thoughts looks promising for helping smaller models reason" src="https://preview.redd.it/xlairo4g9eme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=767c07ca77e2312ef37e77aa5686232b9b3aebb6" title="New Atom of Thoughts looks promising for helping smaller models reason" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xlairo4g9eme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29hm0/new_atom_of_thoughts_looks_promising_for_helping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j29hm0/new_atom_of_thoughts_looks_promising_for_helping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T03:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2m5a3</id>
    <title>I just made something really cursed. It's a local AI javascript library that allows for generating all of your websites styles... using text... It's like tailwind!</title>
    <updated>2025-03-03T16:08:39+00:00</updated>
    <author>
      <name>/u/valdev</name>
      <uri>https://old.reddit.com/user/valdev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2m5a3/i_just_made_something_really_cursed_its_a_local/"&gt; &lt;img alt="I just made something really cursed. It's a local AI javascript library that allows for generating all of your websites styles... using text... It's like tailwind!" src="https://external-preview.redd.it/MzV5cjJiYnAwaW1lMVP5w4KiH2jOdvVLO5M2qzHTvkueIwiHgKlPWJafTXUE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85988097e8c3bdd3a2d386e68bcd0b55c2d2f2c0" title="I just made something really cursed. It's a local AI javascript library that allows for generating all of your websites styles... using text... It's like tailwind!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valdev"&gt; /u/valdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ys7qtcbp0ime1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2m5a3/i_just_made_something_really_cursed_its_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2m5a3/i_just_made_something_really_cursed_its_a_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T16:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j32szk</id>
    <title>Any Update on the HF's Open R1 Project?</title>
    <updated>2025-03-04T04:35:42+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am curious. Following the release of Deepseek R1, Hugging Face announced a new project dubbed Open R1 attempting to reproduce R1. I haven't heard about it since.&lt;/p&gt; &lt;p&gt;Do you know if there is any meaningful step forward?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32szk/any_update_on_the_hfs_open_r1_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32szk/any_update_on_the_hfs_open_r1_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j32szk/any_update_on_the_hfs_open_r1_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T04:35:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j37ns6</id>
    <title>Future of Phi-4-multimodal</title>
    <updated>2025-03-04T10:16:18+00:00</updated>
    <author>
      <name>/u/Bitter-College8786</name>
      <uri>https://old.reddit.com/user/Bitter-College8786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for a good multilingual model for summarizing texts, writing answer e-mails that can run on a consumer laptop with 6-8GB VRAM.&lt;/p&gt; &lt;p&gt;Phi-4-multimodal looked very promising to me and I was excited to see a multimodal LLM with such capacities and such a small size.&lt;/p&gt; &lt;p&gt;But it seems like implementing a llama.cpp support is more complicated than expected: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/11292#issuecomment-2692445044"&gt;https://github.com/ggml-org/llama.cpp/pull/11292#issuecomment-2692445044&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What will this mean for support, tools, fine-tunes etc. Will using this model lead to having a niche LLM where I have to implement solutuons (e.g. work with agents) all by myself (like using Rocm a few years ago) or is it not a big thing that there is no llama.cpp support?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-College8786"&gt; /u/Bitter-College8786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j37ns6/future_of_phi4multimodal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j37ns6/future_of_phi4multimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j37ns6/future_of_phi4multimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T10:16:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2kdeb</id>
    <title>OpenBenchTable is great for trying out different compute hardware configurations. Does anyone have benchmarking tips?</title>
    <updated>2025-03-03T14:52:19+00:00</updated>
    <author>
      <name>/u/eso_logic</name>
      <uri>https://old.reddit.com/user/eso_logic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kdeb/openbenchtable_is_great_for_trying_out_different/"&gt; &lt;img alt="OpenBenchTable is great for trying out different compute hardware configurations. Does anyone have benchmarking tips?" src="https://b.thumbs.redditmedia.com/WEagdfr42ScIzJVwvfP__c7O6w-pNG7grBkUGiA0rAk.jpg" title="OpenBenchTable is great for trying out different compute hardware configurations. Does anyone have benchmarking tips?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eso_logic"&gt; /u/eso_logic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j2kdeb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kdeb/openbenchtable_is_great_for_trying_out_different/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kdeb/openbenchtable_is_great_for_trying_out_different/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T14:52:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2wf5j</id>
    <title>High demand for DIGITS</title>
    <updated>2025-03-03T23:14:14+00:00</updated>
    <author>
      <name>/u/Cane_P</name>
      <uri>https://old.reddit.com/user/Cane_P</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't know if the claim is valid, but according to Taiwanese &amp;quot;United News Media&amp;quot; (udn.com), more production capacity will be allocated to DIGITS than initially planned, because AI companies want to use it for edge computing.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://money.udn.com/money/story/5612/8581326"&gt;https://money.udn.com/money/story/5612/8581326&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Google translated version: &lt;a href="https://money-udn-com.translate.goog/money/story/5612/8581326?_x_tr_sl=auto&amp;amp;_x_tr_tl=en&amp;amp;_x_tr_hl=en-US&amp;amp;_x_tr_pto=wapp"&gt;https://money-udn-com.translate.goog/money/story/5612/8581326?_x_tr_sl=auto&amp;amp;_x_tr_tl=en&amp;amp;_x_tr_hl=en-US&amp;amp;_x_tr_pto=wapp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cane_P"&gt; /u/Cane_P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2wf5j/high_demand_for_digits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2wf5j/high_demand_for_digits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2wf5j/high_demand_for_digits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T23:14:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2xzsj</id>
    <title>I used a 100-line LLM Framework to let AI Agents build Agents for me (Step-by-Step Video Tutorial)</title>
    <updated>2025-03-04T00:27:24+00:00</updated>
    <author>
      <name>/u/Willing-Site-8137</name>
      <uri>https://old.reddit.com/user/Willing-Site-8137</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a video tutorial on a personal hack that can let Cursor AI build complex LLM Agents and greatly improve my productivity : &lt;a href="https://youtu.be/wc9O-9mcObc"&gt;https://youtu.be/wc9O-9mcObc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For example, in this tutorial, I mostly write the high-level design doc, and Cursor AI handles all the implementation and coding to build an &lt;a href="https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple"&gt;AI YouTube Summarizer&lt;/a&gt;. The secret is &lt;a href="https://github.com/The-Pocket/PocketFlow"&gt;Pocket Flow&lt;/a&gt;, a 100-line framework that fits easily into Cursor‚Äôs &lt;a href="https://docs.cursor.com/context/rules-for-ai"&gt;rules&lt;/a&gt;, remains flexible for all sorts of designs, and nudges Cursor to follow good coding practices.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Background of 100-line framework&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I built this &lt;a href="https://github.com/The-Pocket/PocketFlow"&gt;100-line LLM framework&lt;/a&gt; over Christmas. It provides the core ‚Äúgraph abstraction‚Äù that LLM workflows need‚Äîfor (&lt;a href="https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html"&gt;multi-&lt;/a&gt;)&lt;a href="https://the-pocket.github.io/PocketFlow/design_pattern/agent.html"&gt;agents&lt;/a&gt;, &lt;a href="https://the-pocket.github.io/PocketFlow/design_pattern/rag.html"&gt;Retrieval-Augmented Generation (RAG)&lt;/a&gt;, &lt;a href="https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html"&gt;workflow&lt;/a&gt;, and more. I built this because:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Most big frameworks have &lt;em&gt;messy abstractions&lt;/em&gt;, deprecated methods, and annoying dependencies that are very hard to use.&lt;/li&gt; &lt;li&gt;These issues don‚Äôt just confuse humans; they confuse AI coding assistants as well! For example, if you let &lt;em&gt;Cursor AI&lt;/em&gt; build a LLM project with those frameworks, you‚Äôll likely run into a bunch of version or deprecation errors.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So I stripped everything down to &lt;strong&gt;100 lines&lt;/strong&gt;, making it easy for AI tools (like &lt;em&gt;Cursor AI&lt;/em&gt;) to read and build on top of it as ‚Äúrules.‚Äù Surprisingly, &lt;strong&gt;Cursor understands Pocket Flow really well&lt;/strong&gt;-its generated code is modular, maintainable, and has &lt;em&gt;greatly boosted my productivity&lt;/em&gt; over the past year.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo in the YouTube Video&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;To demonstrate this further, I made &lt;a href="https://www.youtube.com/watch?v=wc9O-9mcObc"&gt;this &lt;em&gt;YouTube video&lt;/em&gt;&lt;/a&gt; showing exactly how I fed &lt;em&gt;Cursor AI&lt;/em&gt; the &lt;strong&gt;Pocket Flow&lt;/strong&gt; docs and a high-level design to build LLM apps. I asked &lt;em&gt;Cursor AI&lt;/em&gt; to create a &lt;a href="https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple"&gt;YouTube ‚Äúexplainer‚Äù agent &lt;/a&gt;that summarizes long videos into &lt;em&gt;simple ‚Äú5-year-old-friendly‚Äù terms&lt;/em&gt;‚Äîfor instance, it can condense Lex Fridman‚Äôs &lt;em&gt;5-hour DeepSeek&lt;/em&gt; interview into a &lt;a href="https://the-pocket.github.io/Tutorial-Youtube-Made-Simple/examples/DeepSeek%2C%20China%2C%20OpenAI%2C%20NVIDIA%2C%20xAI%2C%20TSMC%2C%20Stargate%2C%20and%20AI%20Megaclusters%20%7C%20Lex%20Fridman%20Podcast%20%23459.html"&gt;concise, sharp summary&lt;/a&gt;. The entire development took me &lt;em&gt;less than an hour&lt;/em&gt;‚Äîand you can do the same!&lt;/p&gt; &lt;p&gt;I‚Äôm very new to YouTube, so &lt;em&gt;please, please, please&lt;/em&gt; give me your feedback on which parts are unclear! If there‚Äôs another LLM project you‚Äôd like to see me build with Pocket Flow + Cursor, let me know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Willing-Site-8137"&gt; /u/Willing-Site-8137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2xzsj/i_used_a_100line_llm_framework_to_let_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2xzsj/i_used_a_100line_llm_framework_to_let_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2xzsj/i_used_a_100line_llm_framework_to_let_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T00:27:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2leve</id>
    <title>new Hugging Face course on building reasoning models like deepseek r1</title>
    <updated>2025-03-03T15:37:25+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new FREE and CERTIFIED course is here, and It‚Äôs called &lt;strong&gt;The Reasoning Course.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;To sign up for the course, follow the org: &lt;a href="https://huggingface.co/reasoning-course"&gt;https://huggingface.co/reasoning-course&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is what the course will cover:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It will teach you to build your own reasoning model like Deepseek r1.&lt;/li&gt; &lt;li&gt;It‚Äôs suitable for code and non-coders with separate certification.&lt;/li&gt; &lt;li&gt;The course has material and exercises from Hugging Face, Maxime Labonne, Unsloth, and Marimo notebooks. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is how the course works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sign up now, the first release is already live.&lt;/li&gt; &lt;li&gt;Each week we‚Äôll release new material and exercises. &lt;/li&gt; &lt;li&gt;We have interactive demos and quizzes&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2leve/new_hugging_face_course_on_building_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2leve/new_hugging_face_course_on_building_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2leve/new_hugging_face_course_on_building_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T15:37:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2vhhq</id>
    <title>Story writing benchmark/dataset</title>
    <updated>2025-03-03T22:32:38+00:00</updated>
    <author>
      <name>/u/CorrectLow9302</name>
      <uri>https://old.reddit.com/user/CorrectLow9302</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2vhhq/story_writing_benchmarkdataset/"&gt; &lt;img alt="Story writing benchmark/dataset" src="https://external-preview.redd.it/C3b4DWbqwScAlqlFr8UQw42SuiBsrSRBBL5H3HRmHFA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a47dcafbfc7088a32a7fc1410e5b9704fd4e1a93" title="Story writing benchmark/dataset" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/hu1npsu3xjme1.jpg?width=4665&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5ef87cc79183f0cb10bc67f5b10121bb979b878c"&gt;https://preview.redd.it/hu1npsu3xjme1.jpg?width=4665&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5ef87cc79183f0cb10bc67f5b10121bb979b878c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;dataset: &lt;a href="https://huggingface.co/datasets/lars1234/story_writing_benchmark"&gt;https://huggingface.co/datasets/lars1234/story_writing_benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Each model was instructed to write 568 short stories. Each story was then rated by 4 models: Llama 3.3 70B, Mistral Small 24B (2501), Gemma 2 9B (SPPO-Iter3), Aya Expanse 32B. The ranking correlation between the evaluators is approx. 90%. Evaluation criteria such as creativity, world-building and grammar were weighted equally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CorrectLow9302"&gt; /u/CorrectLow9302 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2vhhq/story_writing_benchmarkdataset/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2vhhq/story_writing_benchmarkdataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2vhhq/story_writing_benchmarkdataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T22:32:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j329e9</id>
    <title>ktransformers troll rig R1 671B UD-Q2_K_XL on 96GB RAM in the wild</title>
    <updated>2025-03-04T04:07:23+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j329e9/ktransformers_troll_rig_r1_671b_udq2_k_xl_on_96gb/"&gt; &lt;img alt="ktransformers troll rig R1 671B UD-Q2_K_XL on 96GB RAM in the wild" src="https://external-preview.redd.it/Iv7H9o8LgDHbszU_vTFdZ8XMQmmCZhgShm1FiEoXFQ4.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a13486bc9da50e971c1852f3ecbc2b560e6b2f4b" title="ktransformers troll rig R1 671B UD-Q2_K_XL on 96GB RAM in the wild" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=4ucmn3b44x4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j329e9/ktransformers_troll_rig_r1_671b_udq2_k_xl_on_96gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j329e9/ktransformers_troll_rig_r1_671b_udq2_k_xl_on_96gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T04:07:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2pw8i</id>
    <title>The sound Tesla P40s make while training is eerie. My apartment lights also phase pulse during passes.. ü§©</title>
    <updated>2025-03-03T18:40:59+00:00</updated>
    <author>
      <name>/u/AffectSouthern9894</name>
      <uri>https://old.reddit.com/user/AffectSouthern9894</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2pw8i/the_sound_tesla_p40s_make_while_training_is_eerie/"&gt; &lt;img alt="The sound Tesla P40s make while training is eerie. My apartment lights also phase pulse during passes.. ü§©" src="https://external-preview.redd.it/ZmV0aG1ibndyaW1lMWFNzMK25hHzjNvt4Y-OO73o5sGhDV6XnH6G0Oq87xCn.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e39fa10a7873118e9284eb276fb4ef9c51960837" title="The sound Tesla P40s make while training is eerie. My apartment lights also phase pulse during passes.. ü§©" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AffectSouthern9894"&gt; /u/AffectSouthern9894 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lakifgrwrime1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2pw8i/the_sound_tesla_p40s_make_while_training_is_eerie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2pw8i/the_sound_tesla_p40s_make_while_training_is_eerie/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T18:40:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3479c</id>
    <title>I'm working on a open source UI coding tool with artifacts, cli, agent actions, and github connection</title>
    <updated>2025-03-04T06:00:27+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3479c/im_working_on_a_open_source_ui_coding_tool_with/"&gt; &lt;img alt="I'm working on a open source UI coding tool with artifacts, cli, agent actions, and github connection" src="https://b.thumbs.redditmedia.com/FJNdfekndsHi-w5W0N_AnNYGW8A2CD2BuNGLk_uW8iI.jpg" title="I'm working on a open source UI coding tool with artifacts, cli, agent actions, and github connection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j3479c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3479c/im_working_on_a_open_source_ui_coding_tool_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3479c/im_working_on_a_open_source_ui_coding_tool_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T06:00:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1j34snr</id>
    <title>I open sourced my project to analyze your years of Apple Health data with Local Llama</title>
    <updated>2025-03-04T06:40:45+00:00</updated>
    <author>
      <name>/u/Fit_Chair2340</name>
      <uri>https://old.reddit.com/user/Fit_Chair2340</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j34snr/i_open_sourced_my_project_to_analyze_your_years/"&gt; &lt;img alt="I open sourced my project to analyze your years of Apple Health data with Local Llama" src="https://external-preview.redd.it/lLsIvxvl6coJk3dd69rue5IjQS1mwSUfJjRAQiI0jak.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0705f2c7bb303cf84d8b222fd9c148b2417ea6ce" title="I open sourced my project to analyze your years of Apple Health data with Local Llama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was playing around and found out that you can export all your Apple health data. I've been wearing an Apple watch for 8 years and whoop for 3 years. I always check my day to day and week to week stats but I never looked at the data over the years. What if I could send this data to A.I. for analysis? But I also don't want to send my private data to a public LLM. What if I could run the analysis locally?&lt;/p&gt; &lt;p&gt;I exported my data and there was 989MB of data! So I needed to write some code to break this down. The code takes in your export data and gives you options to look at Steps, Distance, Heart rate, Sleep and more. It gave me some cool charts and you can use local llama to run the A.I. analysis!&lt;/p&gt; &lt;p&gt;I was really stressed at work last 2 years.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/65612e77cmme1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=784225ea3990427860e9abb16fcd60eec3da2563"&gt;https://preview.redd.it/65612e77cmme1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=784225ea3990427860e9abb16fcd60eec3da2563&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It gave me some CRAZY insights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Seasonal Anomalies: While there's a general trend of higher activity in spring/summer, some of your most active periods occurred during winter months, particularly in December and January of recent years.&lt;/li&gt; &lt;li&gt;Reversed Weekend Pattern: Unlike most people who are more active on weekends, your data shows consistently lower step counts on weekends, suggesting your physical activity is more tied to workdays than leisure time.&lt;/li&gt; &lt;li&gt;COVID Impact: There's a clear signature of the pandemic in your data, with more erratic step patterns and changed workout routines during 2020-2021, followed by a distinct recovery pattern in late 2021.&lt;/li&gt; &lt;li&gt;Morning Consistency: Your most successful workout periods consistently occur in morning hours, with these sessions showing better heart rate performance compared to other times.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can run this on your own computer. No one can access your data. &lt;a href="https://github.com/krumjahn/applehealth"&gt;&lt;strong&gt;Here's the link to the project.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you need more guidance on how to run it (not a programmer), &lt;a href="https://rumjahn.com/how-i-used-a-i-to-analyze-8-years-of-apple-health-fitness-data-to-uncover-actionable-insights/"&gt;check out my detailed instructions here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If people like this, I will make a web app version so you can run it without using code. Give this a like if you find it useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fit_Chair2340"&gt; /u/Fit_Chair2340 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j34snr/i_open_sourced_my_project_to_analyze_your_years/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j34snr/i_open_sourced_my_project_to_analyze_your_years/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j34snr/i_open_sourced_my_project_to_analyze_your_years/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T06:40:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j345eq</id>
    <title>AMD Rocm User Forum</title>
    <updated>2025-03-04T05:57:04+00:00</updated>
    <author>
      <name>/u/Nerina23</name>
      <uri>https://old.reddit.com/user/Nerina23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j345eq/amd_rocm_user_forum/"&gt; &lt;img alt="AMD Rocm User Forum" src="https://external-preview.redd.it/y_FINKO8XMlAJhEf7w8V__pSsdOlGq2_AygT_2N_tmE.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=90054cd4dd3bf4dc8ffabe4326ea716b454230eb" title="AMD Rocm User Forum" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fingers crossed for competition to the Nvidia Dominance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nerina23"&gt; /u/Nerina23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/AMD/status/1896709832629158323"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j345eq/amd_rocm_user_forum/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j345eq/amd_rocm_user_forum/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T05:57:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2usb0</id>
    <title>Is qwen 2.5 coder still the best?</title>
    <updated>2025-03-03T22:02:25+00:00</updated>
    <author>
      <name>/u/Ambitious_Subject108</name>
      <uri>https://old.reddit.com/user/Ambitious_Subject108</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anything better been released for coding? (&amp;lt;=32b parameters)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious_Subject108"&gt; /u/Ambitious_Subject108 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2usb0/is_qwen_25_coder_still_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2usb0/is_qwen_25_coder_still_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2usb0/is_qwen_25_coder_still_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T22:02:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j32y7c</id>
    <title>Split brain (Update) - What I've learned and will improve</title>
    <updated>2025-03-04T04:43:51+00:00</updated>
    <author>
      <name>/u/Alienanthony</name>
      <uri>https://old.reddit.com/user/Alienanthony</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32y7c/split_brain_update_what_ive_learned_and_will/"&gt; &lt;img alt="Split brain (Update) - What I've learned and will improve" src="https://external-preview.redd.it/UWvmtQPs4ScGH0IthYKdfU1hrMW7JnkAzdMKFse7jL0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c740e7b82c11757b66468f76734733c2aa704f1c" title="Split brain (Update) - What I've learned and will improve" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a update post to the last one &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"&gt;Here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have uploaded a inference page to the code I had previously discussed. &lt;a href="https://github.com/alientony/Split-brain/blob/main/inference-app.py"&gt;Inference&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can download the fusion layer here. &lt;a href="https://huggingface.co/Alienanthony/Splitbrain_Fusion_model"&gt;Fusion layer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The original models can be found here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-3.2-1B"&gt;https://huggingface.co/meta-llama/Llama-3.2-1B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;So far the inference has been fascinating. Unfortunately I have only had the original gpt4all dataset on hand for training. (800mb) &lt;/p&gt; &lt;p&gt;Including I have learned that if you're doing to use a fused layer for differentiation for one model output you should probably make another. So moving forward I will update the training and attempt again. &lt;/p&gt; &lt;p&gt;BUT I am extremely fascinated by this new crazy system.&lt;/p&gt; &lt;p&gt;As you can see below. While we did not give the model on the left &amp;quot;Describe the history of chocolate chip cookies.&amp;quot; it does begin to think in that direction within it's &amp;quot;Think&amp;quot; space. &lt;/p&gt; &lt;p&gt;I have been able to replicate this sort of &amp;quot;thought directions&amp;quot; multiple times but it is very erratic. As both models are actually not on the same playing field due to the dependency in the way the architecture functions and it is asymmetrical rather than mirrored.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m19l6vxpklme1.png?width=1428&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fdbd8edda0fb1b9a6b03d5922cf233fa462911a1"&gt;https://preview.redd.it/m19l6vxpklme1.png?width=1428&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fdbd8edda0fb1b9a6b03d5922cf233fa462911a1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One major issue I need to fix is the fused layer to realign the model on the right to produce usable tokens.&lt;/p&gt; &lt;p&gt;I also need a larger dataset as this will give more of a wider branch of training for the &amp;quot;sharing of info&amp;quot; across models but I find these results majorly agreeable!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ckwixc26rlme1.png?width=1164&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4164c7d255c32c1c00275f121437c96a65eef5b4"&gt;https://preview.redd.it/ckwixc26rlme1.png?width=1164&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4164c7d255c32c1c00275f121437c96a65eef5b4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alienanthony"&gt; /u/Alienanthony &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32y7c/split_brain_update_what_ive_learned_and_will/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32y7c/split_brain_update_what_ive_learned_and_will/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j32y7c/split_brain_update_what_ive_learned_and_will/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T04:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2j7su</id>
    <title>I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities.</title>
    <updated>2025-03-03T13:57:34+00:00</updated>
    <author>
      <name>/u/w-zhong</name>
      <uri>https://old.reddit.com/user/w-zhong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/"&gt; &lt;img alt="I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." src="https://preview.redd.it/54k8f1ladhme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa20219f6ef894d7607d0ad10ab575e376420b53" title="I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w-zhong"&gt; /u/w-zhong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/54k8f1ladhme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T13:57:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1j32p97</id>
    <title>Qwen 32b coder instruct can now drive a coding agent fairly well</title>
    <updated>2025-03-04T04:29:49+00:00</updated>
    <author>
      <name>/u/ai-christianson</name>
      <uri>https://old.reddit.com/user/ai-christianson</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32p97/qwen_32b_coder_instruct_can_now_drive_a_coding/"&gt; &lt;img alt="Qwen 32b coder instruct can now drive a coding agent fairly well" src="https://external-preview.redd.it/aDJ4N25hdXlvbG1lMXyf8-rvm1C__Q4bDL3gJBkjO_bjkyMUPsobX80FiZpA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b763684453c2eb0539d13912eebe98f2d438296" title="Qwen 32b coder instruct can now drive a coding agent fairly well" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai-christianson"&gt; /u/ai-christianson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c2000d3tolme1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32p97/qwen_32b_coder_instruct_can_now_drive_a_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j32p97/qwen_32b_coder_instruct_can_now_drive_a_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T04:29:49+00:00</published>
  </entry>
</feed>
