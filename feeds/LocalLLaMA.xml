<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-22T12:26:22+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i6mjxv</id>
    <title>Deploy any LLM on Huggingface at 3-10x Speed</title>
    <updated>2025-01-21T16:30:20+00:00</updated>
    <author>
      <name>/u/avianio</name>
      <uri>https://old.reddit.com/user/avianio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6mjxv/deploy_any_llm_on_huggingface_at_310x_speed/"&gt; &lt;img alt="Deploy any LLM on Huggingface at 3-10x Speed" src="https://preview.redd.it/8dsnudtrhdee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4bf725a99b4877d47c42a9a0a7d813a8339eb266" title="Deploy any LLM on Huggingface at 3-10x Speed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avianio"&gt; /u/avianio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8dsnudtrhdee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6mjxv/deploy_any_llm_on_huggingface_at_310x_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6mjxv/deploy_any_llm_on_huggingface_at_310x_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T16:30:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i72bip</id>
    <title>‚ÄúAny Router‚Äù in v0.1.9 - unify access and observability to ollama-supported and API-based LLMs</title>
    <updated>2025-01-22T03:53:08+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i72bip/any_router_in_v019_unify_access_and_observability/"&gt; &lt;img alt="‚ÄúAny Router‚Äù in v0.1.9 - unify access and observability to ollama-supported and API-based LLMs" src="https://preview.redd.it/ks10onr1xgee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61fada01dff806ec53c0bd3dc33cb257f94507e7" title="‚ÄúAny Router‚Äù in v0.1.9 - unify access and observability to ollama-supported and API-based LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not a new project; just a feature update on egress functionally to help developers unify access and observability for ollama- supported and API-based LLMs. Coincidentally out #1 feature request last month. &lt;/p&gt; &lt;p&gt;So if you want a simple way to access any LLM and get unified tracing and logs, then this update might be useful for you.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Arch Gateway is an intelligent proxy server designed for prompts. Guides for egress LLM routing and ollama below &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/katanemo/archgw?tab=readme-ov-file#use-arch-gateway-as-llm-router"&gt;https://github.com/katanemo/archgw?tab=readme-ov-file#use-arch-gateway-as-llm-router&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/katanemo/archgw/tree/main/demos/currency_exchange_ollamau"&gt;https://github.com/katanemo/archgw/tree/main/demos/currency_exchange_ollamau&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ks10onr1xgee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i72bip/any_router_in_v019_unify_access_and_observability/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i72bip/any_router_in_v019_unify_access_and_observability/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T03:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6l3ms</id>
    <title>Thanks to DeepSeek other open model releases with "research" license will be laughable</title>
    <updated>2025-01-21T15:28:03+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Imagine labs like Mistral, Cohere (do you remember them?) release open-weight model with so called research purposes only license. Comedy Central would call them for movie rights ;) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6l3ms/thanks_to_deepseek_other_open_model_releases_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6l3ms/thanks_to_deepseek_other_open_model_releases_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6l3ms/thanks_to_deepseek_other_open_model_releases_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T15:28:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6pra7</id>
    <title>Spanish government releases some official models</title>
    <updated>2025-01-21T18:40:57+00:00</updated>
    <author>
      <name>/u/xdoso</name>
      <uri>https://old.reddit.com/user/xdoso</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spanish government has fund the training of official and public LLMs, mainly trained on Spanish and co-official spanish languages.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Main page&lt;/strong&gt;: &lt;a href="https://alia.gob.es/"&gt;https://alia.gob.es/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Huggingface models&lt;/strong&gt;: &lt;a href="https://huggingface.co/BSC-LT"&gt;https://huggingface.co/BSC-LT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The main released models are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Alia 40b&lt;/strong&gt; (base model still on training, published intermediate result; instruct version will be released in the future)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Salamandra 2b/7b&lt;/strong&gt; (base and instruct available)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The main model has been trained using the Spanish Marenostrum 5 with a total of 2048 GPUs (H100 64Gb). They are all Apache 2.0 license and most datasets have been published also. They are mainly trained on European languages.&lt;/p&gt; &lt;p&gt;Also some translation models have been published:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Salamandra TA 2b:&lt;/strong&gt; translation between 30 main European languages directly&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plume 256k, 128, 32k&lt;/strong&gt;: finetuning of gemma2 models for translation between Spanish languages.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Aina models&lt;/strong&gt;: a list of 1to1 models for translation between Spanish languages.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Alia 40b is the latest release and the most important one, although for the moments the results that we are seeing during the tests are pretty bad. &lt;/p&gt; &lt;p&gt;Posts about the results: &lt;/p&gt; &lt;p&gt;- &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i6qecq/spanish_alia_model_has_been_trained_with_porn_and/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1i6qecq/spanish_alia_model_has_been_trained_with_porn_and/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xdoso"&gt; /u/xdoso &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6pra7/spanish_government_releases_some_official_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6pra7/spanish_government_releases_some_official_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6pra7/spanish_government_releases_some_official_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T18:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6x1rz</id>
    <title>Deepseek-R1 is brittle</title>
    <updated>2025-01-21T23:45:27+00:00</updated>
    <author>
      <name>/u/girishsk</name>
      <uri>https://old.reddit.com/user/girishsk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6x1rz/deepseekr1_is_brittle/"&gt; &lt;img alt="Deepseek-R1 is brittle" src="https://b.thumbs.redditmedia.com/8NQUVLxGD7ZRf-EVvaa_9iF75S6_Ytn5AU8V2JqmcJg.jpg" title="Deepseek-R1 is brittle" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w64gxy9sofee1.png?width=2005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5ce8831dd1bd935250ebd75ccda5fdbf39ebe86"&gt;https://preview.redd.it/w64gxy9sofee1.png?width=2005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5ce8831dd1bd935250ebd75ccda5fdbf39ebe86&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/girishsk"&gt; /u/girishsk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6x1rz/deepseekr1_is_brittle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6x1rz/deepseekr1_is_brittle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6x1rz/deepseekr1_is_brittle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T23:45:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6vhzy</id>
    <title>Gemini Thinking experimental 01-21 is out!</title>
    <updated>2025-01-21T22:37:02+00:00</updated>
    <author>
      <name>/u/Salty-Garage7777</name>
      <uri>https://old.reddit.com/user/Salty-Garage7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6vhzy/gemini_thinking_experimental_0121_is_out/"&gt; &lt;img alt="Gemini Thinking experimental 01-21 is out!" src="https://preview.redd.it/lizc4v8ncfee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d15911a5fea2f0b9809f3de42a8a8e34ce0a319" title="Gemini Thinking experimental 01-21 is out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salty-Garage7777"&gt; /u/Salty-Garage7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lizc4v8ncfee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6vhzy/gemini_thinking_experimental_0121_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6vhzy/gemini_thinking_experimental_0121_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T22:37:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6x0od</id>
    <title>Local hero ports new minimax 456B model to llama.cpp</title>
    <updated>2025-01-21T23:44:01+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6x0od/local_hero_ports_new_minimax_456b_model_to/"&gt; &lt;img alt="Local hero ports new minimax 456B model to llama.cpp" src="https://external-preview.redd.it/XboNX-2aBFaEZb_pw6HGHDpi2wkmn6wYOlAdwzo9X-M.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32c1d2fa99a9246c99a12c019dd3fbea99b33755" title="Local hero ports new minimax 456B model to llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgflip.com/i/9hi08y"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6x0od/local_hero_ports_new_minimax_456b_model_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6x0od/local_hero_ports_new_minimax_456b_model_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T23:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6zbsf</id>
    <title>The distilled R1 models likely work best in workflows, so now's a great time to learn those if you haven't already!</title>
    <updated>2025-01-22T01:31:21+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Another member of our board recently pointed out that Deepseek's paper &lt;a href="https://kingy.ai/wp-content/uploads/2025/01/DeepSeek_R1.pdf"&gt;&amp;quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning&amp;quot;&lt;/a&gt; said the following: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;R1, and likely all reasoning models, are best suited for a zero-shot &amp;quot;&lt;em&gt;please think through this specific problem&lt;/em&gt;&amp;quot; sort of prompts, and you'll likely get far better results doing that than having a multi-turn conversation jammed in there while it thinks.&lt;/p&gt; &lt;p&gt;So once again, I take the opportunity to say: workflows are your friend. I know I'm always harping about workflows, but this case is a slam dunk use-case for learning to use workflows, getting comfortable with them, etc.&lt;/p&gt; &lt;p&gt;You will likely get far better results out of R1, QwQ, the R1 Distilled models, etc if you were have a workflow that did something similar to the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Summarize the what the most recent message is saying and/or asking&lt;/li&gt; &lt;li&gt;Summarize any supporting context to assist in thinking about this&lt;/li&gt; &lt;li&gt;Pass 1 and 2 into the reasoning model, and let it think of a problem&lt;/li&gt; &lt;li&gt;Respond to the user using the output of 3.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;There are 2 really valuable benefits of doing this- first: you only pass in a single scoped problem every time and second: you are hiding the full thinking logic of step 3, so that isn't kept within the conversation or agentic history.&lt;/p&gt; &lt;p&gt;It doesn't matter what workflow program you go with- n8n, langflow, wilmerai, omnichain, whatever. This is a great time to just try them out if you haven't already, and get used to working with them. I've been using workflows exclusively when using ai since at least May or June of last year, and I can't imagine going back. Many of you may end up not liking using them, but many of you might. Either way, you'll get the experience AND can use these distilled R1 models to their maximum benefit.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6zbsf/the_distilled_r1_models_likely_work_best_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6zbsf/the_distilled_r1_models_likely_work_best_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6zbsf/the_distilled_r1_models_likely_work_best_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T01:31:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1i73y5t</id>
    <title>Deepseek running in my basement finds the 5 odd numbers with the letter e in them</title>
    <updated>2025-01-22T05:24:00+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Amazing. I'm a believer. &lt;/p&gt; &lt;p&gt;It's impossible to find an odd number in English that doesn't have the letter 'e' in its spelling because all single-digit odd numbers (1, 3, 5, 7, 9) contain the letter 'e' in their names. Consequently, any odd number, regardless of its size, will include one of these digits in its ones place, resulting in the presence of the letter 'e'. Therefore, no such numbers exist.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; There are no odd numbers in English that don't have the letter 'e' in their spelling.&lt;/p&gt; &lt;p&gt;If you want to see the thinking &lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/rbvF7p2f"&gt;https://pastebin.com/rbvF7p2f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i73y5t/deepseek_running_in_my_basement_finds_the_5_odd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i73y5t/deepseek_running_in_my_basement_finds_the_5_odd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i73y5t/deepseek_running_in_my_basement_finds_the_5_odd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T05:24:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1i74jqg</id>
    <title>DeepSeek R1 32B is way better than 7B Distill, even at Q4 quant</title>
    <updated>2025-01-22T06:01:35+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been quite impressed by the model. I'm using the Qwen distill and so far it's working well, although as is typical with these models, they tend to overthink a lot! But it answered my trick question in one shot (See comments).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i74jqg/deepseek_r1_32b_is_way_better_than_7b_distill/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i74jqg/deepseek_r1_32b_is_way_better_than_7b_distill/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i74jqg/deepseek_r1_32b_is_way_better_than_7b_distill/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T06:01:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i75wyo</id>
    <title>Bytedance Model: Doubao 1.5pro - Doubao Team</title>
    <updated>2025-01-22T07:36:36+00:00</updated>
    <author>
      <name>/u/a445141126</name>
      <uri>https://old.reddit.com/user/a445141126</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a445141126"&gt; /u/a445141126 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://team.doubao.com/en/special/doubao_1_5_pro"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i75wyo/bytedance_model_doubao_15pro_doubao_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i75wyo/bytedance_model_doubao_15pro_doubao_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T07:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i78src</id>
    <title>How I Used GPT-O1 Pro to Discover My Autoimmune Disease (After Spending $100k and Visiting 30+ Hospitals with No Success)</title>
    <updated>2025-01-22T11:12:33+00:00</updated>
    <author>
      <name>/u/Dry_Steak30</name>
      <uri>https://old.reddit.com/user/Dry_Steak30</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Suffered from various health issues for 5 years, visited 30+ hospitals with no answers&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Finally diagnosed with axial spondyloarthritis through genetic testing&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built a personalized health analysis system using GPT-O1 Pro, which actually suggested this condition earlier&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm a guy in my mid-30s who started having weird health issues about 5 years ago. Nothing major, but lots of annoying symptoms - getting injured easily during workouts, slow recovery, random fatigue, and sometimes the pain was so bad I could barely walk.&lt;/p&gt; &lt;p&gt;At first, I went to different doctors for each symptom. Tried everything - MRIs, chiropractic care, meds, steroids - nothing helped. I followed every doctor's advice perfectly. Started getting into longevity medicine thinking it might be early aging. Changed my diet, exercise routine, sleep schedule - still no improvement. The cause remained a mystery.&lt;/p&gt; &lt;p&gt;Recently, after a month-long toe injury wouldn't heal, I ended up seeing a rheumatologist. They did genetic testing and boom - diagnosed with axial spondyloarthritis. This was the answer I'd been searching for over 5 years.&lt;/p&gt; &lt;p&gt;Here's the crazy part - I fed all my previous medical records and symptoms into GPT-O1 pro before the diagnosis, and it actually listed this condition as the top possibility!&lt;/p&gt; &lt;p&gt;This got me thinking - why didn't any doctor catch this earlier? Well, it's a rare condition, and autoimmune diseases affect the whole body. Joint pain isn't just joint pain, dry eyes aren't just eye problems. The usual medical workflow isn't set up to look at everything together.&lt;/p&gt; &lt;p&gt;So I had an idea: What if we created an open-source system that could analyze someone's complete medical history, including family history (which was a huge clue in my case), and create personalized health plans? It wouldn't replace doctors but could help both patients and medical professionals spot patterns.&lt;/p&gt; &lt;p&gt;Building my personal system was challenging:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Every hospital uses different formats and units for test results. Had to create a GPT workflow to standardize everything.&lt;/li&gt; &lt;li&gt;RAG wasn't enough - needed a large context window to analyze everything at once for the best results.&lt;/li&gt; &lt;li&gt;Finding reliable medical sources was tough. Combined official guidelines with recent papers and trusted YouTube content.&lt;/li&gt; &lt;li&gt;GPT-O1 pro was best at root cause analysis, Google Note LLM worked great for citations, and Examine excelled at suggesting actions.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In the end, I built a system using Google Sheets to view my data and interact with trusted medical sources. It's been incredibly helpful in managing my condition and understanding my health better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Steak30"&gt; /u/Dry_Steak30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T11:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6jbur</id>
    <title>DeepSeek R1 (Qwen 32B Distill) is now available for free on HuggingChat!</title>
    <updated>2025-01-21T14:07:01+00:00</updated>
    <author>
      <name>/u/SensitiveCranberry</name>
      <uri>https://old.reddit.com/user/SensitiveCranberry</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6jbur/deepseek_r1_qwen_32b_distill_is_now_available_for/"&gt; &lt;img alt="DeepSeek R1 (Qwen 32B Distill) is now available for free on HuggingChat!" src="https://external-preview.redd.it/8HwSiZPd8K_hder46_kXYrWF23xE0qYwa1myzoXXUfM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32931922738bce5caa4226b968a442514cf96587" title="DeepSeek R1 (Qwen 32B Distill) is now available for free on HuggingChat!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SensitiveCranberry"&gt; /u/SensitiveCranberry &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hf.co/chat/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6jbur/deepseek_r1_qwen_32b_distill_is_now_available_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6jbur/deepseek_r1_qwen_32b_distill_is_now_available_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T14:07:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6n7jf</id>
    <title>Pretty sure OpenAI has their devs working 24/7 to not lose their throne to DeepSeek üò≠</title>
    <updated>2025-01-21T16:57:48+00:00</updated>
    <author>
      <name>/u/Condomphobic</name>
      <uri>https://old.reddit.com/user/Condomphobic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And DeepSeek is making the same progress at a much faster pace than OpenAI is. They are definitely in a rock situation &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Condomphobic"&gt; /u/Condomphobic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6n7jf/pretty_sure_openai_has_their_devs_working_247_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6n7jf/pretty_sure_openai_has_their_devs_working_247_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6n7jf/pretty_sure_openai_has_their_devs_working_247_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T16:57:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6t08q</id>
    <title>DeepSeek-R1-Distill-Qwen-1.5B running 100% locally in-browser on WebGPU. Reportedly outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks (28.9% on AIME and 83.9% on MATH).</title>
    <updated>2025-01-21T20:53:47+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6t08q/deepseekr1distillqwen15b_running_100_locally/"&gt; &lt;img alt="DeepSeek-R1-Distill-Qwen-1.5B running 100% locally in-browser on WebGPU. Reportedly outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks (28.9% on AIME and 83.9% on MATH)." src="https://external-preview.redd.it/bHl5MDU0Yzl0ZWVlMQAQ0j2wFUvXTQrT52Nv81bl04kSZ1X_57NkDQOUMylE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=277e9be2fe8997f1f81f1d7306a6ef378dd50407" title="DeepSeek-R1-Distill-Qwen-1.5B running 100% locally in-browser on WebGPU. Reportedly outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks (28.9% on AIME and 83.9% on MATH)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5ei4j3c9teee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6t08q/deepseekr1distillqwen15b_running_100_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6t08q/deepseekr1distillqwen15b_running_100_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T20:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6lsgo</id>
    <title>From llama2 --&gt; DeepSeek R1 things have gone a long way in a 1 year</title>
    <updated>2025-01-21T15:58:08+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was blown away by llama2 70b when it came out. I felt so empowered having so much knowledge spun up locally on my M3 Max. &lt;/p&gt; &lt;p&gt;Just over a year, and DeepSeek R1 makes Llama 2 seem like a little child. It's crazy how good the outputs are, and how fast it spits out tokens in just 40GB.&lt;/p&gt; &lt;p&gt;Can't imagine where things will be in another year.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6lsgo/from_llama2_deepseek_r1_things_have_gone_a_long/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6lsgo/from_llama2_deepseek_r1_things_have_gone_a_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6lsgo/from_llama2_deepseek_r1_things_have_gone_a_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T15:58:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6zid8</id>
    <title>Just a comparison of US $500B Stargate AI project to other tech projects</title>
    <updated>2025-01-22T01:40:19+00:00</updated>
    <author>
      <name>/u/Shir_man</name>
      <uri>https://old.reddit.com/user/Shir_man</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Manhattan Project&lt;/strong&gt; ~$30 billion in today‚Äôs dollars [~1.5% of US GDP in the mid-1940s]&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Apollo Program&lt;/strong&gt; ~$170‚Äì$180 billion in today‚Äôs dollars [~0.5% of US GDP in the mid-1960s]&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Space Shuttle Program&lt;/strong&gt; ~$275‚Äì$300 billion in today‚Äôs dollars [~0.2% of US GDP in the early 1980s]&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interstate Highway System&lt;/strong&gt;, entire decades-long Interstate Highway System buildout, ~$500‚Äì$550 billion in today‚Äôs dollars [~0.2%‚Äì0.3% of GDP annually over multiple decades]&lt;/p&gt; &lt;p&gt;Stargate is huge AI project [~1.7% of US GDP 2024]&lt;/p&gt; &lt;p&gt;&lt;em&gt;Update:&lt;/em&gt; rough GDP calculations added&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shir_man"&gt; /u/Shir_man &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6zid8/just_a_comparison_of_us_500b_stargate_ai_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6zid8/just_a_comparison_of_us_500b_stargate_ai_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6zid8/just_a_comparison_of_us_500b_stargate_ai_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T01:40:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6urjd</id>
    <title>Billions in proprietary AI? No more.</title>
    <updated>2025-01-21T22:06:11+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6urjd/billions_in_proprietary_ai_no_more/"&gt; &lt;img alt="Billions in proprietary AI? No more." src="https://b.thumbs.redditmedia.com/9ar_9wVV9mMAYbxuIYJsMxrYp5TIPP2iGETnpq4RXng.jpg" title="Billions in proprietary AI? No more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI raised billions on promise of having and securing behind thick doors something no other is even remotely close to. The following tweet from just few days prior R1 release made me think they really have atomic bomb the world will knee for;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/25dv42dl1fee1.png?width=1209&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bcfafe3260c7a5257502850b940aa24a2f5f7ecd"&gt;https://preview.redd.it/25dv42dl1fee1.png?width=1209&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bcfafe3260c7a5257502850b940aa24a2f5f7ecd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The truth is, they have nothing. o1-level, some say human-level reasoning is reproducible, can be privately hosted by anyone, anywhere. Can't be greedily priced.&lt;/p&gt; &lt;p&gt;MIT licensed open models is the future of AI. Zero dollars is the only right price for something made on all human knowledge. It is a sum of effort of the whole civilisation, spanning many generations. Just imagine, any book that landed in the pretraining dataset influences the whole model. There is no better way to honor any author contributing to the overall model performance, knowingly or not than to make a tool that help create new knowledge, available for anyone, at no cost.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6urjd/billions_in_proprietary_ai_no_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6urjd/billions_in_proprietary_ai_no_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6urjd/billions_in_proprietary_ai_no_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T22:06:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i765q0</id>
    <title>R1-Zero: Pure RL Creates a Mind We Can‚Äôt Decode‚ÄîIs This AGI‚Äôs Dark Mirror?</title>
    <updated>2025-01-22T07:54:24+00:00</updated>
    <author>
      <name>/u/Fun_Dragonfruit_4613</name>
      <uri>https://old.reddit.com/user/Fun_Dragonfruit_4613</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The AI world is losing its mind over DeepSeek-R1-Zero, a model that skipped supervised fine-tuning (SFT) entirely and learned purely through reinforcement learning (RL). Unlike its sibling R1‚Äîwhich uses &lt;strong&gt;some&lt;/strong&gt; SFT data to stay &amp;quot;human-readable&amp;quot;‚ÄîR1-Zero‚Äôs training mirrors AlphaZero‚Äôs trial-and-error self-play. The result? &lt;strong&gt;Jaw-dropping performance&lt;/strong&gt; (AIME math scores jumped from 15.6% ‚Üí 86.7%) paired with &lt;strong&gt;bizarre, uninterpretable reasoning&lt;/strong&gt;. Researchers observed &amp;quot;aha moments&amp;quot; where it autonomously rechecked flawed logic mid-process and allocated more compute to harder problems‚Äî&lt;strong&gt;without human guidance&lt;/strong&gt;. But here‚Äôs the kicker: its outputs are riddled with garbled language mixes (e.g., Chinese/English spaghetti code) and logic leaps that even its creators can‚Äôt fully explain. &lt;/p&gt; &lt;p&gt;Meanwhile, R1 (the SFT-hybrid version) achieves similar performance &lt;strong&gt;without the chaos&lt;/strong&gt;, proving that human-curated data still tames the beast. But at what cost? R1-Zero‚Äôs pure RL approach hints at a terrifying possibility: &lt;strong&gt;minds that optimize truth beyond human comprehension&lt;/strong&gt;. And with API costs 50x cheaper than OpenAI‚Äôs, scaling this could democratize superintelligence‚Äîor unleash unreadable black-box AI. &lt;/p&gt; &lt;p&gt;If R1-Zero‚Äôs &amp;quot;alien logic&amp;quot; solves problems we can‚Äôt, does readability even matter‚Ä¶ or is this how alignment dies? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Dragonfruit_4613"&gt; /u/Fun_Dragonfruit_4613 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T07:54:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i78sfs</id>
    <title>Deepseek R1 GRPO code open sourced ü§Ø</title>
    <updated>2025-01-22T11:11:56+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"&gt; &lt;img alt="Deepseek R1 GRPO code open sourced ü§Ø" src="https://preview.redd.it/ryfnofs83jee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40e8bb4b0b8a7dd1b82c628a825c88559a17aff0" title="Deepseek R1 GRPO code open sourced ü§Ø" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ryfnofs83jee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T11:11:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6uviy</id>
    <title>R1 is mind blowing</title>
    <updated>2025-01-21T22:10:54+00:00</updated>
    <author>
      <name>/u/Not-The-Dark-Lord-7</name>
      <uri>https://old.reddit.com/user/Not-The-Dark-Lord-7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gave it a problem from my graph theory course that‚Äôs reasonably nuanced. 4o gave me the wrong answer twice, but did manage to produce the correct answer once. R1 managed to get this problem right in one shot, and also held up under pressure when I asked it to justify its answer. It also gave a great explanation that showed it really understood the nuance of the problem. I feel pretty confident in saying that AI is smarter than me. Not just closed, flagship models, but smaller models that I could run on my MacBook are probably smarter than me at this point. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Not-The-Dark-Lord-7"&gt; /u/Not-The-Dark-Lord-7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6uviy/r1_is_mind_blowing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6uviy/r1_is_mind_blowing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6uviy/r1_is_mind_blowing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T22:10:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6vnqc</id>
    <title>Trump announces a $500 billion AI infrastructure investment in the US</title>
    <updated>2025-01-21T22:43:49+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6vnqc/trump_announces_a_500_billion_ai_infrastructure/"&gt; &lt;img alt="Trump announces a $500 billion AI infrastructure investment in the US" src="https://external-preview.redd.it/qFlenD3wOMEKpyf-2qth3Zo8oJQzBNIpBFiyCeVPdPY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1661f472f059c05f183a4286b726667eb4724fc9" title="Trump announces a $500 billion AI infrastructure investment in the US" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnn.com/2025/01/21/tech/openai-oracle-softbank-trump-ai-investment/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6vnqc/trump_announces_a_500_billion_ai_infrastructure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6vnqc/trump_announces_a_500_billion_ai_infrastructure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T22:43:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1i73x81</id>
    <title>YOU CAN EXTRACT REASONING FROM R1 AND PASS IT ONTO ANY MODEL</title>
    <updated>2025-01-22T05:22:22+00:00</updated>
    <author>
      <name>/u/Sensitive-Finger-404</name>
      <uri>https://old.reddit.com/user/Sensitive-Finger-404</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i73x81/you_can_extract_reasoning_from_r1_and_pass_it/"&gt; &lt;img alt="YOU CAN EXTRACT REASONING FROM R1 AND PASS IT ONTO ANY MODEL" src="https://external-preview.redd.it/OG1uaHRydHljaGVlMeGKc_GKsNSHC_YJy3k1hv6gZ336TNH-m_F1sXruvXhI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=672f440c070400525909ae68b98c3deb34d98428" title="YOU CAN EXTRACT REASONING FROM R1 AND PASS IT ONTO ANY MODEL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;from @skirano on twitter&lt;/p&gt; &lt;p&gt;By the way, you can extract JUST the reasoning from deepseek-reasoner, which means you can send that thinking process to any model you want before they answer you. &lt;/p&gt; &lt;p&gt;Like here where I turn gpt-3.5 turbo into an absolute genius!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sensitive-Finger-404"&gt; /u/Sensitive-Finger-404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mbcqadwychee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i73x81/you_can_extract_reasoning_from_r1_and_pass_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i73x81/you_can_extract_reasoning_from_r1_and_pass_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T05:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1i75g7p</id>
    <title>I don‚Äôt believe the $500 Billion OpenAI investment</title>
    <updated>2025-01-22T07:03:02+00:00</updated>
    <author>
      <name>/u/MattDTO</name>
      <uri>https://old.reddit.com/user/MattDTO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking at this deal, several things don't add up. The $500 billion figure is wildly optimistic - that's almost double what the entire US government committed to semiconductor manufacturing through the CHIPS Act. When you dig deeper, you see lots of vague promises but no real details about where the money's coming from or how they'll actually build anything.&lt;/p&gt; &lt;p&gt;The legal language is especially fishy. Instead of making firm commitments, they're using weasel words like &amp;quot;intends to,&amp;quot; &amp;quot;evaluating,&amp;quot; and &amp;quot;potential partnerships.&amp;quot; This isn't accidental - by running everything through Stargate, a new private company, and using this careful language, they've created a perfect shield for bigger players like SoftBank and Microsoft. If things go south, they can just blame &amp;quot;market conditions&amp;quot; and walk away with minimal exposure. Private companies like Stargate don't face the same strict disclosure requirements as public ones.&lt;/p&gt; &lt;p&gt;The timing is also telling - announcing this massive investment right after Trump won the presidency was clearly designed for maximum political impact. It fits perfectly into the narrative of bringing jobs and investment back to America. Using inflated job numbers for data centers (which typically employ relatively few people once built) while making vague promises about US technological leadership? That‚Äôs politics.&lt;/p&gt; &lt;p&gt;My guess? There's probably a real data center project in the works, but it's being massively oversold for publicity and political gains. The actual investment will likely be much smaller, take longer to complete, and involve different partners than what's being claimed. This announcement just is a deal structured by lawyers who wanted to generate maximum headlines while minimizing any legal risk for their clients.‚Äã‚Äã‚Äã‚Äã&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MattDTO"&gt; /u/MattDTO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i75g7p/i_dont_believe_the_500_billion_openai_investment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i75g7p/i_dont_believe_the_500_billion_openai_investment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i75g7p/i_dont_believe_the_500_billion_openai_investment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T07:03:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1i77sy1</id>
    <title>How it feels...</title>
    <updated>2025-01-22T10:01:42+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i77sy1/how_it_feels/"&gt; &lt;img alt="How it feels..." src="https://preview.redd.it/rocl1zwsqiee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0cf91de1f8277f67834bb8422547f586738360e5" title="How it feels..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rocl1zwsqiee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i77sy1/how_it_feels/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i77sy1/how_it_feels/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T10:01:42+00:00</published>
  </entry>
</feed>
