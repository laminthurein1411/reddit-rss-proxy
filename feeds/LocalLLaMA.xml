<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-26T07:34:19+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iy0pia</id>
    <title>Minions: embracing small LMs, shifting compute on-device, and cutting cloud costs in the process</title>
    <updated>2025-02-25T17:36:21+00:00</updated>
    <author>
      <name>/u/McSnoo</name>
      <uri>https://old.reddit.com/user/McSnoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy0pia/minions_embracing_small_lms_shifting_compute/"&gt; &lt;img alt="Minions: embracing small LMs, shifting compute on-device, and cutting cloud costs in the process" src="https://external-preview.redd.it/1In3rkqkPsaqWPg4zfY3r4Mu2bX-LaExyt4GewlRPU4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=277f9cef8cbc0cdc3f7eca12af309c990dc9750e" title="Minions: embracing small LMs, shifting compute on-device, and cutting cloud costs in the process" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/McSnoo"&gt; /u/McSnoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.together.ai/blog/minions"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy0pia/minions_embracing_small_lms_shifting_compute/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy0pia/minions_embracing_small_lms_shifting_compute/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T17:36:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy7xi2</id>
    <title>Comparing Unsloth R1 dynamic quants relative performance: IQ2_XXS (183GB) beats Q2_K_XL (212GB)</title>
    <updated>2025-02-25T22:33:22+00:00</updated>
    <author>
      <name>/u/TyraVex</name>
      <uri>https://old.reddit.com/user/TyraVex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While we wait for the amazing Ktransformers devs to drop Unsloth's R1 dynamic quant support into their inference framework, I measured the relative performance of the different precisions available.&lt;/p&gt; &lt;p&gt;To do so, I used llama.cpp commit af7747c and bartowski's &lt;a href="https://gist.github.com/bartowski1182/eb213dccb3571f863da82e99418f81e8"&gt;calibration file&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Here is the table (the lower the PPL - the better):&lt;/p&gt; &lt;p&gt;Comparing to FP8:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Quant&lt;/th&gt; &lt;th&gt;Size (MB)&lt;/th&gt; &lt;th&gt;PPL&lt;/th&gt; &lt;th&gt;Size (%)&lt;/th&gt; &lt;th&gt;Accuracy (%)&lt;/th&gt; &lt;th&gt;PPL error rate&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;IQ1_S&lt;/td&gt; &lt;td&gt;133736&lt;/td&gt; &lt;td&gt;5.9582&lt;/td&gt; &lt;td&gt;20.36&lt;/td&gt; &lt;td&gt;NaN&lt;/td&gt; &lt;td&gt;0.08194&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IQ1_M&lt;/td&gt; &lt;td&gt;161092&lt;/td&gt; &lt;td&gt;5.5432&lt;/td&gt; &lt;td&gt;24.53&lt;/td&gt; &lt;td&gt;NaN&lt;/td&gt; &lt;td&gt;0.07515&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IQ2_XXS&lt;/td&gt; &lt;td&gt;187076&lt;/td&gt; &lt;td&gt;5.0739&lt;/td&gt; &lt;td&gt;28.48&lt;/td&gt; &lt;td&gt;NaN&lt;/td&gt; &lt;td&gt;0.06756&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q2_K_XL&lt;/td&gt; &lt;td&gt;216105&lt;/td&gt; &lt;td&gt;5.0812&lt;/td&gt; &lt;td&gt;32.90&lt;/td&gt; &lt;td&gt;NaN&lt;/td&gt; &lt;td&gt;0.06742&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;FP8&lt;/td&gt; &lt;td&gt;656707&lt;/td&gt; &lt;td&gt;NaN&lt;/td&gt; &lt;td&gt;100.00&lt;/td&gt; &lt;td&gt;NaN&lt;/td&gt; &lt;td&gt;NaN&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Comparing to Q2_K_XL:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Quant&lt;/th&gt; &lt;th&gt;Size (MB)&lt;/th&gt; &lt;th&gt;PPL&lt;/th&gt; &lt;th&gt;Size (%)&lt;/th&gt; &lt;th&gt;Accuracy (%)&lt;/th&gt; &lt;th&gt;PPL error rate&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;IQ1_S&lt;/td&gt; &lt;td&gt;133736&lt;/td&gt; &lt;td&gt;5.9582&lt;/td&gt; &lt;td&gt;61.88&lt;/td&gt; &lt;td&gt;85.28&lt;/td&gt; &lt;td&gt;0.08194&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IQ1_M&lt;/td&gt; &lt;td&gt;161092&lt;/td&gt; &lt;td&gt;5.5432&lt;/td&gt; &lt;td&gt;74.54&lt;/td&gt; &lt;td&gt;91.67&lt;/td&gt; &lt;td&gt;0.07515&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IQ2_XXS&lt;/td&gt; &lt;td&gt;187076&lt;/td&gt; &lt;td&gt;5.0739&lt;/td&gt; &lt;td&gt;86.57&lt;/td&gt; &lt;td&gt;100.14&lt;/td&gt; &lt;td&gt;0.06756&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q2_K_XL&lt;/td&gt; &lt;td&gt;216105&lt;/td&gt; &lt;td&gt;5.0812&lt;/td&gt; &lt;td&gt;100.00&lt;/td&gt; &lt;td&gt;100.00&lt;/td&gt; &lt;td&gt;0.06742&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Suprisingly, IQ2_XXS (183GB) beats Q2_K_XL (212GB) with 5.0812 PPL vs 5.0739 PPL. Maybe this is because of the normal IQ quants being more efficient than the normal K quants in the first place. However, Q2_K_XL is already supported by Ktransformers, so there's that.&lt;/p&gt; &lt;p&gt;As you can see, there is sadly no FP8 perplexity measurement, and so no relative performance to it (I don't have the compute, and Q2_K_XL's run took 50 hours). If anyone has the time and means, I am dying to know how close or far we are from the full FP8 when using those 20%-30% sized quants.&lt;/p&gt; &lt;p&gt;PPL logs for reproducibility: &lt;a href="https://gist.github.com/ThomasBaruzier/3f88a81b9c131cc5dad717073e05804e"&gt;https://gist.github.com/ThomasBaruzier/3f88a81b9c131cc5dad717073e05804e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Have a nice day everyone.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TyraVex"&gt; /u/TyraVex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7xi2/comparing_unsloth_r1_dynamic_quants_relative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7xi2/comparing_unsloth_r1_dynamic_quants_relative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7xi2/comparing_unsloth_r1_dynamic_quants_relative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T22:33:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy73hk</id>
    <title>reasoning without a single token</title>
    <updated>2025-02-25T21:58:19+00:00</updated>
    <author>
      <name>/u/Fun_Librarian_7699</name>
      <uri>https://old.reddit.com/user/Fun_Librarian_7699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Unlike conventional reasoning models like OpenAI's o3-mini that generate chains of thought through reasoning tokens, Huginn requires no specialized training and reasons in its neural network's latent space before producing any output.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I think this has a lot of potential and also leads to reduced costs. &lt;/p&gt; &lt;p&gt;&lt;a href="https://the-decoder.com/huginn-new-ai-model-thinks-without-words/"&gt;https://the-decoder.com/huginn-new-ai-model-thinks-without-words/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Librarian_7699"&gt; /u/Fun_Librarian_7699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy73hk/reasoning_without_a_single_token/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy73hk/reasoning_without_a_single_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy73hk/reasoning_without_a_single_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T21:58:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixtug3</id>
    <title>WAN Video model launched</title>
    <updated>2025-02-25T12:29:36+00:00</updated>
    <author>
      <name>/u/BreakIt-Boris</name>
      <uri>https://old.reddit.com/user/BreakIt-Boris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Doesn't seem to be announced yet however the huggingface space is live and model weighs are released!!! Realise this isn't technically LLM however believe possibly of interest to many here.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Wan-AI/Wan2.1-T2V-14B"&gt;https://huggingface.co/Wan-AI/Wan2.1-T2V-14B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakIt-Boris"&gt; /u/BreakIt-Boris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixtug3/wan_video_model_launched/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixtug3/wan_video_model_launched/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixtug3/wan_video_model_launched/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T12:29:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixzd4p</id>
    <title>QuantBench: Easy LLM / VLM Quantization</title>
    <updated>2025-02-25T16:41:39+00:00</updated>
    <author>
      <name>/u/Ragecommie</name>
      <uri>https://old.reddit.com/user/Ragecommie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixzd4p/quantbench_easy_llm_vlm_quantization/"&gt; &lt;img alt="QuantBench: Easy LLM / VLM Quantization" src="https://preview.redd.it/phviteh5dble1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5d2247615af9ad7a5752ddf77e7e379217c78f51" title="QuantBench: Easy LLM / VLM Quantization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The amount of low-effort, low-quality and straight up broken quants on HF is too damn high!&lt;/p&gt; &lt;p&gt;That's why we're making quantization even lower effort!&lt;/p&gt; &lt;p&gt;Check it out: &lt;a href="https://youtu.be/S9jYXYIz_d4"&gt;https://youtu.be/S9jYXYIz_d4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Currently working on VLM benchmarking, quantization code is already on GitHub: &lt;a href="https://github.com/Independent-AI-Labs/local-super-agents/tree/main/quantbench"&gt;https://github.com/Independent-AI-Labs/local-super-agents/tree/main/quantbench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thoughts and feature requests are welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ragecommie"&gt; /u/Ragecommie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/phviteh5dble1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixzd4p/quantbench_easy_llm_vlm_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixzd4p/quantbench_easy_llm_vlm_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T16:41:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy1fbe</id>
    <title>Gemini 2.0 suddenly started thinking in Chinese ðŸ˜…</title>
    <updated>2025-02-25T18:05:09+00:00</updated>
    <author>
      <name>/u/ashutrv</name>
      <uri>https://old.reddit.com/user/ashutrv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy1fbe/gemini_20_suddenly_started_thinking_in_chinese/"&gt; &lt;img alt="Gemini 2.0 suddenly started thinking in Chinese ðŸ˜…" src="https://b.thumbs.redditmedia.com/5jcZ4IDyZOqAc6EnDMCZ-oPU9MOh2Ipu1lVzAz-hfMU.jpg" title="Gemini 2.0 suddenly started thinking in Chinese ðŸ˜…" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was analysing an NFL game and suddenly it switched to thinking in Chinese ðŸ‡¨ðŸ‡³ &lt;/p&gt; &lt;p&gt;Hmm, Deepseek underneath?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashutrv"&gt; /u/ashutrv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iy1fbe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy1fbe/gemini_20_suddenly_started_thinking_in_chinese/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy1fbe/gemini_20_suddenly_started_thinking_in_chinese/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T18:05:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy5whf</id>
    <title>Now on Hugging Face: Microsoft's Magma: A Foundation Model for Multimodal AI Agents w/MIT License</title>
    <updated>2025-02-25T21:08:47+00:00</updated>
    <author>
      <name>/u/softwareweaver</name>
      <uri>https://old.reddit.com/user/softwareweaver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Magma is a multimodal agentic AI model that can generate text based on the input text and image. The model is designed for research purposes and aimed at knowledge-sharing and accelerating research in multimodal AI, in particular the multimodal agentic AI. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/microsoft/Magma-8B"&gt;https://huggingface.co/microsoft/Magma-8B&lt;/a&gt;&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=T4Xu7WMYUcc"&gt;https://www.youtube.com/watch?v=T4Xu7WMYUcc&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Digital and Physical Worlds:&lt;/strong&gt; Magma is the first-ever foundation model for multimodal AI agents, designed to handle complex interactions across both virtual and real environments!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Versatile Capabilities:&lt;/strong&gt; Magma as a single model not only possesses generic image and videos understanding ability, but also generate goal-driven visual plans and actions, making it versatile for different agentic tasks!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;State-of-the-art Performance:&lt;/strong&gt; Magma achieves state-of-the-art performance on various multimodal tasks, including UI navigation, robotics manipulation, as well as generic image and video understanding, in particular the spatial understanding and reasoning!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scalable Pretraining Strategy:&lt;/strong&gt; Magma is designed to be &lt;strong&gt;learned scalably from unlabeled videos&lt;/strong&gt; in the wild in addition to the existing agentic data, making it strong generalization ability and suitable for real-world applications!&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/softwareweaver"&gt; /u/softwareweaver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy5whf/now_on_hugging_face_microsofts_magma_a_foundation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy5whf/now_on_hugging_face_microsofts_magma_a_foundation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy5whf/now_on_hugging_face_microsofts_magma_a_foundation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T21:08:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy88jt</id>
    <title>WilmerAI: I just uploaded around 3 hours worth of video tutorials explaining the prompt routing, workflows, and walking through running it</title>
    <updated>2025-02-25T22:46:34+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy88jt/wilmerai_i_just_uploaded_around_3_hours_worth_of/"&gt; &lt;img alt="WilmerAI: I just uploaded around 3 hours worth of video tutorials explaining the prompt routing, workflows, and walking through running it" src="https://external-preview.redd.it/c6siCekRyG6Ns9HYU0MVyr293slrzeLzV1u5DrO4Ww4.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c3ffa52940a2fd8ccec5c2a1e35ab247e4ce901e" title="WilmerAI: I just uploaded around 3 hours worth of video tutorials explaining the prompt routing, workflows, and walking through running it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/playlist?list=PLjIfeYFu5Pl7J7KGJqVmHM4HU56nByb4X"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy88jt/wilmerai_i_just_uploaded_around_3_hours_worth_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy88jt/wilmerai_i_just_uploaded_around_3_hours_worth_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T22:46:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixupja</id>
    <title>Sonnet 3.7 near clean sweep of EQ-Bench benchmarks</title>
    <updated>2025-02-25T13:15:35+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixupja/sonnet_37_near_clean_sweep_of_eqbench_benchmarks/"&gt; &lt;img alt="Sonnet 3.7 near clean sweep of EQ-Bench benchmarks" src="https://a.thumbs.redditmedia.com/6fUZh9Gsn1Cvb5T20dNRVamWAjNPiP5h-5Kb76ovqf0.jpg" title="Sonnet 3.7 near clean sweep of EQ-Bench benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ixupja"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixupja/sonnet_37_near_clean_sweep_of_eqbench_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixupja/sonnet_37_near_clean_sweep_of_eqbench_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T13:15:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixporw</id>
    <title>Alibaba video model Wan 2.1 will be released Feb 25th,2025 and is open source!</title>
    <updated>2025-02-25T07:46:26+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixporw/alibaba_video_model_wan_21_will_be_released_feb/"&gt; &lt;img alt="Alibaba video model Wan 2.1 will be released Feb 25th,2025 and is open source!" src="https://preview.redd.it/amle9h0op8le1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5133d782a13f9379a9568553fde27608d4fe653" title="Alibaba video model Wan 2.1 will be released Feb 25th,2025 and is open source!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nice to have open source. So excited for this one.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/amle9h0op8le1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixporw/alibaba_video_model_wan_21_will_be_released_feb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixporw/alibaba_video_model_wan_21_will_be_released_feb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T07:46:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy13dz</id>
    <title>Free Gemini Code Assist</title>
    <updated>2025-02-25T17:51:48+00:00</updated>
    <author>
      <name>/u/Cane_P</name>
      <uri>https://old.reddit.com/user/Cane_P</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy13dz/free_gemini_code_assist/"&gt; &lt;img alt="Free Gemini Code Assist" src="https://preview.redd.it/pszm583opble1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=496bb02106a8ef0f5a34f28aaec6fc4b24d8e2ec" title="Free Gemini Code Assist" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://blog.google/technology/developers/gemini-code-assist-free/"&gt;https://blog.google/technology/developers/gemini-code-assist-free/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cane_P"&gt; /u/Cane_P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pszm583opble1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy13dz/free_gemini_code_assist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy13dz/free_gemini_code_assist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T17:51:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyen1x</id>
    <title>If claude 3.7 is the best for coding then why is it ranked low on artificial analysis coding benchmarks?</title>
    <updated>2025-02-26T03:55:56+00:00</updated>
    <author>
      <name>/u/Hv_V</name>
      <uri>https://old.reddit.com/user/Hv_V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyen1x/if_claude_37_is_the_best_for_coding_then_why_is/"&gt; &lt;img alt="If claude 3.7 is the best for coding then why is it ranked low on artificial analysis coding benchmarks?" src="https://b.thumbs.redditmedia.com/FveQ50mZ2Vdj8xoAq1CIe1I2fzt8nJFRmgyRdNll8Hs.jpg" title="If claude 3.7 is the best for coding then why is it ranked low on artificial analysis coding benchmarks?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0nan7xubpele1.png?width=1055&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1e8f5ef5414b11c52f73974b2c15216220c1d0cf"&gt;https://preview.redd.it/0nan7xubpele1.png?width=1055&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1e8f5ef5414b11c52f73974b2c15216220c1d0cf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gqpbsopcpele1.png?width=776&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=04b1bc8235d5e2795b42b2a82d999ffd2ea8bd79"&gt;https://preview.redd.it/gqpbsopcpele1.png?width=776&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=04b1bc8235d5e2795b42b2a82d999ffd2ea8bd79&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aby185kdpele1.png?width=788&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e833a4e7156d457f810a9f1b8541b2d4b0eff2c7"&gt;https://preview.redd.it/aby185kdpele1.png?width=788&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e833a4e7156d457f810a9f1b8541b2d4b0eff2c7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7mk49xgepele1.png?width=768&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=570b4ef625bcf82695b707c869471a4e337f8408"&gt;https://preview.redd.it/7mk49xgepele1.png?width=768&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=570b4ef625bcf82695b707c869471a4e337f8408&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hv_V"&gt; /u/Hv_V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyen1x/if_claude_37_is_the_best_for_coding_then_why_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyen1x/if_claude_37_is_the_best_for_coding_then_why_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyen1x/if_claude_37_is_the_best_for_coding_then_why_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T03:55:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy7mco</id>
    <title>Magma: A Foundation Model for Multimodal AI Agents</title>
    <updated>2025-02-25T22:20:01+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7mco/magma_a_foundation_model_for_multimodal_ai_agents/"&gt; &lt;img alt="Magma: A Foundation Model for Multimodal AI Agents" src="https://external-preview.redd.it/3TcBPHiZT0ehLG7Chhf6Tcy2t-RW9H63I2r1U0-0qWg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73e9bb2225d928aa47a08c9bedee0f7751c754d0" title="Magma: A Foundation Model for Multimodal AI Agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/Magma-8B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7mco/magma_a_foundation_model_for_multimodal_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7mco/magma_a_foundation_model_for_multimodal_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T22:20:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy2qhm</id>
    <title>New form factor announced for AMD MAX cpu from Framework</title>
    <updated>2025-02-25T18:58:09+00:00</updated>
    <author>
      <name>/u/takuonline</name>
      <uri>https://old.reddit.com/user/takuonline</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Framework just announced a mini desktop version of the AMD MAX CPU chip featuring up to 128GB of unified memory with up to 96GB available for graphics.&lt;/p&gt; &lt;p&gt;Edit: So apparently, this new CPU Strix CPU from AMD requires a new motherboard and device redesign for laptops which makes the products more expensive.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;This thing has a massive integrated GP that boasts performance that is similar to an RTX 4060 on integrated graphics and It even allows you to allocate up to 96 GB of its maximum 128 gigs of lpddr 5x to that GPU making it awesome for gamers creative professionals and AI developers no the disappointing thing was that this sick processor barely made it into any products all I saw at the show was one admittedly awesome laptop from HP and One gaming tablet from Asus &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Talking to those Brands they said the issue was that Strix Halo requires a complete motherboard and device redesign making its implementation in mobile devices really costly so I guess framework said screw it we're a small company and can't afford all that but what if we just made it into a desktop is that really how it went down that is literally how it went down&lt;/p&gt; &lt;p&gt;source: &lt;a href="https://youtu.be/-lErGZZgUbY?t=158"&gt;https://youtu.be/-lErGZZgUbY?t=158&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/takuonline"&gt; /u/takuonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy2qhm/new_form_factor_announced_for_amd_max_cpu_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy2qhm/new_form_factor_announced_for_amd_max_cpu_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy2qhm/new_form_factor_announced_for_amd_max_cpu_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T18:58:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixzast</id>
    <title>olmOCR-7B by Ai2 - open-source model to extract clean plain text from PDFs.</title>
    <updated>2025-02-25T16:38:55+00:00</updated>
    <author>
      <name>/u/False_Care_2957</name>
      <uri>https://old.reddit.com/user/False_Care_2957</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/allenai/olmOCR-7B-0225-preview"&gt;https://huggingface.co/allenai/olmOCR-7B-0225-preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/False_Care_2957"&gt; /u/False_Care_2957 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixzast/olmocr7b_by_ai2_opensource_model_to_extract_clean/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixzast/olmocr7b_by_ai2_opensource_model_to_extract_clean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixzast/olmocr7b_by_ai2_opensource_model_to_extract_clean/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T16:38:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixubts</id>
    <title>ðŸ‡¨ðŸ‡³ Sources: DeepSeek is speeding up the release of its R2 AI model, which was originally slated for May, but the company is now working to launch it sooner.</title>
    <updated>2025-02-25T12:56:10+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixubts/sources_deepseek_is_speeding_up_the_release_of/"&gt; &lt;img alt="ðŸ‡¨ðŸ‡³ Sources: DeepSeek is speeding up the release of its R2 AI model, which was originally slated for May, but the company is now working to launch it sooner." src="https://preview.redd.it/z11vic3x8ale1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ec3818e2a144f0e19522fbf44016cae02b88dc1" title="ðŸ‡¨ðŸ‡³ Sources: DeepSeek is speeding up the release of its R2 AI model, which was originally slated for May, but the company is now working to launch it sooner." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z11vic3x8ale1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixubts/sources_deepseek_is_speeding_up_the_release_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixubts/sources_deepseek_is_speeding_up_the_release_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T12:56:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1iybgj2</id>
    <title>TinyR1-32B-Preview (surpassing official R1 distill 32B performance)</title>
    <updated>2025-02-26T01:14:30+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iybgj2/tinyr132bpreview_surpassing_official_r1_distill/"&gt; &lt;img alt="TinyR1-32B-Preview (surpassing official R1 distill 32B performance)" src="https://external-preview.redd.it/n9Pibq5ap97rYKS_QfhVUwq5U1l5cN9jQ5aOHTyyDyg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=659b372ed30298182f662bdd30b00d3b42381833" title="TinyR1-32B-Preview (surpassing official R1 distill 32B performance)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/qihoo360/TinyR1-32B-Preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iybgj2/tinyr132bpreview_surpassing_official_r1_distill/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iybgj2/tinyr132bpreview_surpassing_official_r1_distill/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T01:14:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy7k6b</id>
    <title>Nvidia gaming GPUs modded with 2X VRAM for AI workloads â€” RTX 4090D 48GB and RTX 4080 Super 32GB go up for rent at Chinese cloud computing provider</title>
    <updated>2025-02-25T22:17:32+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7k6b/nvidia_gaming_gpus_modded_with_2x_vram_for_ai/"&gt; &lt;img alt="Nvidia gaming GPUs modded with 2X VRAM for AI workloads â€” RTX 4090D 48GB and RTX 4080 Super 32GB go up for rent at Chinese cloud computing provider" src="https://external-preview.redd.it/LHkWl_VkJgCRA11Syl07lcXlc5oC-0ZjNEgwTTmbGnM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=309552b833882287b79a1c8f0cb6385eb21a5228" title="Nvidia gaming GPUs modded with 2X VRAM for AI workloads â€” RTX 4090D 48GB and RTX 4080 Super 32GB go up for rent at Chinese cloud computing provider" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/nvidia-gaming-gpus-modded-with-2x-vram-for-ai-workloads"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7k6b/nvidia_gaming_gpus_modded_with_2x_vram_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7k6b/nvidia_gaming_gpus_modded_with_2x_vram_for_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T22:17:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixtxbw</id>
    <title>ðŸ˜‚ðŸ˜‚ someone made a "touch grass" app with a vLLM, you gotta go and actually touch grass to unlock your phone</title>
    <updated>2025-02-25T12:33:46+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixtxbw/someone_made_a_touch_grass_app_with_a_vllm_you/"&gt; &lt;img alt="ðŸ˜‚ðŸ˜‚ someone made a &amp;quot;touch grass&amp;quot; app with a vLLM, you gotta go and actually touch grass to unlock your phone" src="https://b.thumbs.redditmedia.com/5hq40VPLgBMcOH3vwQ7e1MxMGeAfqIgssUMVtLMafsg.jpg" title="ðŸ˜‚ðŸ˜‚ someone made a &amp;quot;touch grass&amp;quot; app with a vLLM, you gotta go and actually touch grass to unlock your phone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ixtxbw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixtxbw/someone_made_a_touch_grass_app_with_a_vllm_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixtxbw/someone_made_a_touch_grass_app_with_a_vllm_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T12:33:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy22ux</id>
    <title>Gemma 3 27b just dropped (Gemini API models list)</title>
    <updated>2025-02-25T18:31:30+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy22ux/gemma_3_27b_just_dropped_gemini_api_models_list/"&gt; &lt;img alt="Gemma 3 27b just dropped (Gemini API models list)" src="https://preview.redd.it/y2nlshypwble1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16e52e66ab49c258198ed2169eecedba2241176d" title="Gemma 3 27b just dropped (Gemini API models list)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y2nlshypwble1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy22ux/gemma_3_27b_just_dropped_gemini_api_models_list/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy22ux/gemma_3_27b_just_dropped_gemini_api_models_list/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T18:31:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyfvhb</id>
    <title>Perplexity is forking Chrome</title>
    <updated>2025-02-26T05:05:09+00:00</updated>
    <author>
      <name>/u/WordyBug</name>
      <uri>https://old.reddit.com/user/WordyBug</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyfvhb/perplexity_is_forking_chrome/"&gt; &lt;img alt="Perplexity is forking Chrome" src="https://preview.redd.it/ubxe59mr1fle1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1f3bdf014e84bea19fddef06263361ac5e64ab3" title="Perplexity is forking Chrome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WordyBug"&gt; /u/WordyBug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ubxe59mr1fle1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyfvhb/perplexity_is_forking_chrome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyfvhb/perplexity_is_forking_chrome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T05:05:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy7e4x</id>
    <title>RTX 4090 48GB</title>
    <updated>2025-02-25T22:10:28+00:00</updated>
    <author>
      <name>/u/xg357</name>
      <uri>https://old.reddit.com/user/xg357</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7e4x/rtx_4090_48gb/"&gt; &lt;img alt="RTX 4090 48GB" src="https://b.thumbs.redditmedia.com/i7dCpIN6G-2UgPHgeJaiSWC47liBasG9PHIrLoSF1kw.jpg" title="RTX 4090 48GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got one of these legendary 4090 with 48gb of ram from eBay. I am from Canada. &lt;/p&gt; &lt;p&gt;What do you want me to test? And any questions? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xg357"&gt; /u/xg357 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iy7e4x"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7e4x/rtx_4090_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7e4x/rtx_4090_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T22:10:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy6rid</id>
    <title>Framework Desktop 128gb Mainboard Only Costs $1,699 And Can Networked Together</title>
    <updated>2025-02-25T21:44:22+00:00</updated>
    <author>
      <name>/u/Noble00_</name>
      <uri>https://old.reddit.com/user/Noble00_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy6rid/framework_desktop_128gb_mainboard_only_costs_1699/"&gt; &lt;img alt="Framework Desktop 128gb Mainboard Only Costs $1,699 And Can Networked Together" src="https://b.thumbs.redditmedia.com/p-QWrsvBVwjKiJeQsp_Oo8L1SYP2Wqt_48i9b9Kgics.jpg" title="Framework Desktop 128gb Mainboard Only Costs $1,699 And Can Networked Together" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Noble00_"&gt; /u/Noble00_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iy6rid"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy6rid/framework_desktop_128gb_mainboard_only_costs_1699/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy6rid/framework_desktop_128gb_mainboard_only_costs_1699/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T21:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1iybcnl</id>
    <title>DeepSeek Realse 3th Bomb! DeepGEMM a library for efficient FP8 General Matrix</title>
    <updated>2025-02-26T01:09:10+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iybcnl/deepseek_realse_3th_bomb_deepgemm_a_library_for/"&gt; &lt;img alt="DeepSeek Realse 3th Bomb! DeepGEMM a library for efficient FP8 General Matrix" src="https://external-preview.redd.it/qnXEqoF7LuYpZmlhfqFWCAFt6nE0AU8_d2ok4KoHKZ0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abee123819338d8d068f68944ed6953760e40e9e" title="DeepSeek Realse 3th Bomb! DeepGEMM a library for efficient FP8 General Matrix" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepGEMM is a library designed for clean and efficient FP8 General Matrix Multiplications (GEMMs) with fine-grained scaling, as proposed in DeepSeek-V3&lt;/p&gt; &lt;p&gt;link: &lt;a href="https://github.com/deepseek-ai/DeepGEMM"&gt;https://github.com/deepseek-ai/DeepGEMM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/616ztgnjvdle1.png?width=882&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fb2a1853f514cd4f0b57cd8861518cdcfe5a8f9"&gt;https://preview.redd.it/616ztgnjvdle1.png?width=882&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fb2a1853f514cd4f0b57cd8861518cdcfe5a8f9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iybcnl/deepseek_realse_3th_bomb_deepgemm_a_library_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iybcnl/deepseek_realse_3th_bomb_deepgemm_a_library_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iybcnl/deepseek_realse_3th_bomb_deepgemm_a_library_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T01:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy2t7c</id>
    <title>Framework's new Ryzen Max desktop with 128gb 256gb/s memory is $1990</title>
    <updated>2025-02-25T19:01:07+00:00</updated>
    <author>
      <name>/u/sobe3249</name>
      <uri>https://old.reddit.com/user/sobe3249</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy2t7c/frameworks_new_ryzen_max_desktop_with_128gb/"&gt; &lt;img alt="Framework's new Ryzen Max desktop with 128gb 256gb/s memory is $1990" src="https://preview.redd.it/erki80wv1cle1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ada9d2780ffd78b32f14450c762f69f014324845" title="Framework's new Ryzen Max desktop with 128gb 256gb/s memory is $1990" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sobe3249"&gt; /u/sobe3249 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/erki80wv1cle1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy2t7c/frameworks_new_ryzen_max_desktop_with_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy2t7c/frameworks_new_ryzen_max_desktop_with_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T19:01:07+00:00</published>
  </entry>
</feed>
