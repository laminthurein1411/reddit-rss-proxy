<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-18T14:39:20+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lehe2i</id>
    <title>Local LLM Coding Setup for 8GB VRAM - Coding Models?</title>
    <updated>2025-06-18T13:40:09+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unfortunately for now, I'm limited to &lt;strong&gt;8GB VRAM&lt;/strong&gt; (&lt;strong&gt;32GB RAM&lt;/strong&gt;) with my friend's laptop - NVIDIA GeForce RTX 4060 GPU - Intel(R) Core(TM) i7-14700HX 2.10 GHz. We can't upgrade this laptop with neither RAM nor Graphics anymore.&lt;/p&gt; &lt;p&gt;I'm not expecting great performance from LLMs with this VRAM. Just decent OK performance is enough for me on coding.&lt;/p&gt; &lt;p&gt;Fortunately I'm able to load upto 14B models(I pick highest quant fit my VRAM whenever possible) with this VRAM, I use JanAI.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My use case&lt;/strong&gt; : Python, C#, Js(And Optionally Rust, Go). To develop simple utilities &amp;amp; small games.&lt;/p&gt; &lt;p&gt;Please share &lt;strong&gt;Coding Models&lt;/strong&gt;, &lt;strong&gt;Tools&lt;/strong&gt;, &lt;strong&gt;Utilities&lt;/strong&gt;, &lt;strong&gt;Resources&lt;/strong&gt;, &lt;strong&gt;etc.,&lt;/strong&gt; for this setup to help this Poor GPU.&lt;/p&gt; &lt;p&gt;Tools like OpenHands could help me newbies like me on coding better way? or AI coding assistants/agents like Roo / Cline? What else?&lt;/p&gt; &lt;p&gt;Big Thanks&lt;/p&gt; &lt;p&gt;(We don't want to invest anymore with current laptop. I can use friend's this laptop weekdays since he needs that for gaming weekends only. I'm gonna build a PC with some medium-high config for 150-200B models next year start. So for next 6-9 months, I have to use this current laptop for coding).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lehe2i/local_llm_coding_setup_for_8gb_vram_coding_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lehe2i/local_llm_coding_setup_for_8gb_vram_coding_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lehe2i/local_llm_coding_setup_for_8gb_vram_coding_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T13:40:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldv6jb</id>
    <title>Newly Released MiniMax-M1 80B vs Claude Opus 4</title>
    <updated>2025-06-17T18:40:54+00:00</updated>
    <author>
      <name>/u/Just_Lingonberry_352</name>
      <uri>https://old.reddit.com/user/Just_Lingonberry_352</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldv6jb/newly_released_minimaxm1_80b_vs_claude_opus_4/"&gt; &lt;img alt="Newly Released MiniMax-M1 80B vs Claude Opus 4" src="https://preview.redd.it/gwxrxooh8j7f1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3152d1553ef91f7e6cd9a02b26203647e9fb1e5" title="Newly Released MiniMax-M1 80B vs Claude Opus 4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Lingonberry_352"&gt; /u/Just_Lingonberry_352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gwxrxooh8j7f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldv6jb/newly_released_minimaxm1_80b_vs_claude_opus_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldv6jb/newly_released_minimaxm1_80b_vs_claude_opus_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T18:40:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1le69tx</id>
    <title>What's your analysis of unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF locally</title>
    <updated>2025-06-18T02:47:47+00:00</updated>
    <author>
      <name>/u/ready_to_fuck_yeahh</name>
      <uri>https://old.reddit.com/user/ready_to_fuck_yeahh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been almost 20 days since the release, I'm considering buying single RTX 5090 based PC this winter to use BF16 or Q_8_K_XL unsloth version, my main use case are document processing, summarization(context length will not be an issue since i'm using chunking algorithm for shorter chunks) and trading. Does it justify it's benchmark results?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ready_to_fuck_yeahh"&gt; /u/ready_to_fuck_yeahh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1le69tx/whats_your_analysis_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1le69tx/whats_your_analysis_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1le69tx/whats_your_analysis_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T02:47:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldvosh</id>
    <title>Handy - a simple, open-source offline speech-to-text app written in Rust using whisper.cpp</title>
    <updated>2025-06-17T19:00:08+00:00</updated>
    <author>
      <name>/u/sipjca</name>
      <uri>https://old.reddit.com/user/sipjca</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldvosh/handy_a_simple_opensource_offline_speechtotext/"&gt; &lt;img alt="Handy - a simple, open-source offline speech-to-text app written in Rust using whisper.cpp" src="https://external-preview.redd.it/bDzCT4ZXzfyUunJH2t6KbXRCZTeW_8YRhybKom8weVk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49821e08e57000fbfa143c29ef88ad4252597b67" title="Handy - a simple, open-source offline speech-to-text app written in Rust using whisper.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;I built a simple, offline speech-to-text app after breaking my finger - now open sourcing it&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Made a cross-platform speech-to-text app using whisper.cpp that runs completely offline. Press shortcut, speak, get text pasted anywhere. It's rough around the edges but works well and is designed to be easily modified/extended - including adding LLM calls after transcription.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I broke my finger a while back and suddenly couldn't type properly. Tried existing speech-to-text solutions but they were either subscription-based, cloud-dependent, or I couldn't modify them to work exactly how I needed for coding and daily computer use.&lt;/p&gt; &lt;p&gt;So I built Handy - intentionally simple speech-to-text that runs entirely on your machine using whisper.cpp (Whisper Small model). No accounts, no subscriptions, no data leaving your computer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Press keyboard shortcut ‚Üí speak ‚Üí press again (or use push-to-talk)&lt;/li&gt; &lt;li&gt;Transcribes with whisper.cpp and pastes directly into whatever app you're using&lt;/li&gt; &lt;li&gt;Works across Windows, macOS, Linux&lt;/li&gt; &lt;li&gt;GPU accelerated where available&lt;/li&gt; &lt;li&gt;Completely offline&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That's literally it. No fancy UI, no feature creep, just reliable local speech-to-text.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I'm sharing this&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This was my first Rust project and there are definitely rough edges, but the core functionality works well. More importantly, I designed it to be easily forkable and extensible because that's what I was looking for when I started this journey.&lt;/p&gt; &lt;p&gt;The codebase is intentionally simple - you can understand the whole thing in an afternoon. If you want to add LLM integration (calling an LLM after transcription to rewrite/enhance the text), custom post-processing, or whatever else, the foundation is there and it's straightforward to extend.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I'm hoping it might be useful for:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;People who want reliable offline speech-to-text without subscriptions&lt;/li&gt; &lt;li&gt;Developers who want to experiment with voice computing interfaces&lt;/li&gt; &lt;li&gt;Anyone who prefers tools they can actually modify instead of being stuck with someone else's feature decisions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Project Reality&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There are known bugs and architectural decisions that could be better. I'm documenting issues openly because I'd rather have people know what they're getting into. This isn't trying to compete with polished commercial solutions - it's trying to be the most hackable and modifiable foundation for people who want to build their own thing.&lt;/p&gt; &lt;p&gt;If you're looking for something perfect out of the box, this probably isn't it. If you're looking for something you can understand, modify, and make your own, it might be exactly what you need.&lt;/p&gt; &lt;p&gt;Would love feedback from anyone who tries it out, especially if you run into issues or see ways to make the codebase cleaner and more accessible for others to build on.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sipjca"&gt; /u/sipjca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://handy.computer"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldvosh/handy_a_simple_opensource_offline_speechtotext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldvosh/handy_a_simple_opensource_offline_speechtotext/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T19:00:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldjyhf</id>
    <title>Completed Local LLM Rig</title>
    <updated>2025-06-17T10:48:48+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjyhf/completed_local_llm_rig/"&gt; &lt;img alt="Completed Local LLM Rig" src="https://external-preview.redd.it/HJkY2jxSg_GtjUMbmHI4EEBqY3YefZ9gwrvbaXuZONc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a5994778697f45b5780284b90c07b85daa01d2e" title="Completed Local LLM Rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So proud it's finally done!&lt;/p&gt; &lt;p&gt;GPU: 4 x RTX 3090 CPU: TR 3945wx 12c RAM: 256GB DDR4@3200MT/s SSD: PNY 3040 2TB MB: Asrock Creator WRX80 PSU: Seasonic Prime 2200W RAD: Heatkiller MoRa 420 Case: Silverstone RV-02&lt;/p&gt; &lt;p&gt;Was a long held dream to fit 4 x 3090 in an ATX form factor, all in my good old Silverstone Raven from 2011. An absolute classic. GPU temps at 57C.&lt;/p&gt; &lt;p&gt;Now waiting for the Fractal 180mm LED fans to put into the bottom. What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ldjyhf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjyhf/completed_local_llm_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjyhf/completed_local_llm_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T10:48:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1leamks</id>
    <title>If NotebookLM were Agentic</title>
    <updated>2025-06-18T07:04:47+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1leamks/video/yak8abh4xm7f1/player"&gt;https://reddit.com/link/1leamks/video/yak8abh4xm7f1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;At &lt;a href="https://morphik.ai"&gt;Morphik&lt;/a&gt;, we're dedicated to building the best RAG and document-processing systems in the world. Morphik works particularly well with visual data. As a challenge, I was trying to get it to solve a Where's Waldo puzzle. This led me down the agent rabbit hole and culminated in an agentic document viewer which can navigate the document, zoom into pages, and search/compile information exactly the way a human would.&lt;/p&gt; &lt;p&gt;This is ideal for things like analyzing blueprints, hard to parse data-sheets, or playing Where's Waldo :) In the demo below, I ask the agent to compile information across a 42 page 10Q report from NVIDIA. &lt;/p&gt; &lt;p&gt;Test it out &lt;a href="https://morphik.ai"&gt;here&lt;/a&gt;! Soon, we'll be adding features to actually annotate the documents too - imagine filing your tax forms, legal docs, or entire applications with just a prompt. Would love your feedback, feature requests, suggestions, or comments below!&lt;/p&gt; &lt;p&gt;As always, we're open source: &lt;a href="https://github.com/morphik-org/morphik-core"&gt;https://github.com/morphik-org/morphik-core&lt;/a&gt; (Would love a ‚≠êÔ∏è!)&lt;/p&gt; &lt;p&gt;- &lt;a href="https://morphik.ai"&gt;Morphik&lt;/a&gt; Team ‚ù§Ô∏è&lt;/p&gt; &lt;p&gt;PS: We got feedback to make our installation simpler, and it is one-click for all machines now!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leamks/if_notebooklm_were_agentic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leamks/if_notebooklm_were_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1leamks/if_notebooklm_were_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T07:04:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldqroi</id>
    <title>A free goldmine of tutorials for the components you need to create production-level agents</title>
    <updated>2025-06-17T15:53:14+00:00</updated>
    <author>
      <name>/u/Nir777</name>
      <uri>https://old.reddit.com/user/Nir777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I‚Äôve just launched a free resource with 25 detailed tutorials for building comprehensive production-level AI agents, as part of my Gen AI educational initiative.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The tutorials cover all the key components you need to create agents that are ready for real-world deployment. I plan to keep adding more tutorials over time and will make sure the content stays up to date.&lt;/p&gt; &lt;p&gt;The response so far has been incredible! (the repo got nearly 500 stars in just 8 hours from launch) This is part of my broader effort to create high-quality open source educational material. I already have over 100 code tutorials on GitHub with nearly 40,000 stars.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The link is in the first comment&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The content is organized into these categories:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Orchestration&lt;/li&gt; &lt;li&gt;Tool integration&lt;/li&gt; &lt;li&gt;Observability&lt;/li&gt; &lt;li&gt;Deployment&lt;/li&gt; &lt;li&gt;Memory&lt;/li&gt; &lt;li&gt;UI &amp;amp; Frontend&lt;/li&gt; &lt;li&gt;Agent Frameworks&lt;/li&gt; &lt;li&gt;Model Customization&lt;/li&gt; &lt;li&gt;Multi-agent Coordination&lt;/li&gt; &lt;li&gt;Security&lt;/li&gt; &lt;li&gt;Evaluation&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nir777"&gt; /u/Nir777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldqroi/a_free_goldmine_of_tutorials_for_the_components/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldqroi/a_free_goldmine_of_tutorials_for_the_components/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldqroi/a_free_goldmine_of_tutorials_for_the_components/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T15:53:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1leaip7</id>
    <title>Easily run multiple local llama.cpp servers with FlexLLama</title>
    <updated>2025-06-18T06:57:55+00:00</updated>
    <author>
      <name>/u/yazoniak</name>
      <uri>https://old.reddit.com/user/yazoniak</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leaip7/easily_run_multiple_local_llamacpp_servers_with/"&gt; &lt;img alt="Easily run multiple local llama.cpp servers with FlexLLama" src="https://external-preview.redd.it/tOnGQQVBUGYTjHWnaMDYXgUV2FJ7LJdXQlMueTBtftM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f7b3d4e31fbdbcb27778d08464d0a08beb2dfe6" title="Easily run multiple local llama.cpp servers with FlexLLama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. I‚Äôve been working on a lightweight tool called &lt;strong&gt;FlexLLama&lt;/strong&gt; that makes it really easy to run multiple llama.cpp instances locally. It‚Äôs open-source and it lets you run multiple llama.cpp models at once (even on different GPUs) and puts them all behind a single OpenAI compatible API - so you never have to shut one down to use another (models are switched dynamically on the fly).&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1leaip7/video/mz0l9s88wm7f1/player"&gt;FlexLLama Dashboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Spin up several llama.cpp servers at once and distribute them across different GPUs / CPU.&lt;/li&gt; &lt;li&gt;Works with chat, completions, embeddings and reranking models.&lt;/li&gt; &lt;li&gt;Comes with a web dashboard so you can see runner status and switch models on the fly.&lt;/li&gt; &lt;li&gt;Supports automatic startup and dynamic model reloading, so it‚Äôs easy to manage a fleet of models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here‚Äôs the repo: &lt;a href="https://github.com/yazon/flexllama"&gt;https://github.com/yazon/flexllama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm open to any questions or feedback, let me know what you think.&lt;/p&gt; &lt;p&gt;Usage example:&lt;/p&gt; &lt;p&gt;OpenWebUI: All models (even those not currently running) are visible in the models list dashboard. After selecting a model and sending a prompt, the model is dynamically loaded or switched.&lt;/p&gt; &lt;p&gt;Visual Studio Code / Roo code: Different local models are assigned to different modes. In my case, Qwen3 is assigned to Architect and Orchestrator, THUDM 4 is used for Code, and OpenHands is used for Debug. When Roo switches modes, the appropriate model is automatically loaded.&lt;/p&gt; &lt;p&gt;Visual Studio Code / Continue.dev: All models are visible and run on the NVIDIA GPU. Additionally, embedding and reranker models run on the integrated AMD GPU using Vulkan. Because models are distributed to different runners, all requests (code, embedding, reranker) work simultaneously.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yazoniak"&gt; /u/yazoniak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leaip7/easily_run_multiple_local_llamacpp_servers_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leaip7/easily_run_multiple_local_llamacpp_servers_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1leaip7/easily_run_multiple_local_llamacpp_servers_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T06:57:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1le30yi</id>
    <title>Would love to know if you consider gemma27b the best small model out there?</title>
    <updated>2025-06-18T00:05:43+00:00</updated>
    <author>
      <name>/u/Ok-Internal9317</name>
      <uri>https://old.reddit.com/user/Ok-Internal9317</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Because I haven't found another that didn't have much hiccup under normal conversations and basic usage; I personally think it's the best out there, what about y'all? (Small as in like 32B max.) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Internal9317"&gt; /u/Ok-Internal9317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1le30yi/would_love_to_know_if_you_consider_gemma27b_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1le30yi/would_love_to_know_if_you_consider_gemma27b_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1le30yi/would_love_to_know_if_you_consider_gemma27b_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T00:05:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lee4pd</id>
    <title>WikipeQA : An evaluation dataset for both web-browsing agents and vector DB RAG systems</title>
    <updated>2025-06-18T10:58:26+00:00</updated>
    <author>
      <name>/u/Fit_Strawberry8480</name>
      <uri>https://old.reddit.com/user/Fit_Strawberry8480</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey fellow OSS enjoyer,&lt;/p&gt; &lt;p&gt;I've created WikipeQA, an evaluation dataset inspired by BrowseComp but designed to test a broader range of retrieval systems.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes WikipeQA different?&lt;/strong&gt; Unlike BrowseComp (which requires live web browsing), WikipeQA can evaluate BOTH:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Web-browsing agents&lt;/strong&gt;: Can your agent find the answer by searching online? (The info exists on Wikipedia and its sources)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Traditional RAG systems&lt;/strong&gt;: How well does your vector DB perform when given the full Wikipedia corpus?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This lets you directly compare different architectural approaches on the same questions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Dataset:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;3,000 complex, narrative-style questions (encrypted to prevent training contamination)&lt;/li&gt; &lt;li&gt;200 public examples to get started&lt;/li&gt; &lt;li&gt;Includes the full Wikipedia pages used as sources&lt;/li&gt; &lt;li&gt;Shows the exact chunks that generated each question&lt;/li&gt; &lt;li&gt;Short answers (1-4 words) for clear evaluation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example question:&lt;/strong&gt; &lt;em&gt;&amp;quot;Which national Antarctic research program, known for its 2021 Midterm Assessment on a 2015 Strategic Vision, places the Changing Antarctic Ice Sheets Initiative at the top of its priorities to better understand why ice sheets are changing now and how they will change in the future?&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Answer: &lt;em&gt;&amp;quot;United States Antarctic Program&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Built with Kushim&lt;/strong&gt; The entire dataset was automatically generated using Kushim, my open-source framework. This means you can create your own evaluation datasets from your own documents - perfect for domain-specific benchmarks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current Status:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dataset is ready at: &lt;a href="https://huggingface.co/datasets/teilomillet/wikipeqa"&gt;https://huggingface.co/datasets/teilomillet/wikipeqa&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Working on the eval harness (coming soon)&lt;/li&gt; &lt;li&gt;Would love to see early results if anyone runs evals!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm particularly interested in seeing:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;How traditional vector search compares to web browsing on these questions&lt;/li&gt; &lt;li&gt;Whether hybrid approaches (vector DB + web search) perform better&lt;/li&gt; &lt;li&gt;Performance differences between different chunking/embedding strategies&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you run any evals with WikipeQA, please share your results! Happy to collaborate on making this more useful for the community.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fit_Strawberry8480"&gt; /u/Fit_Strawberry8480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lee4pd/wikipeqa_an_evaluation_dataset_for_both/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lee4pd/wikipeqa_an_evaluation_dataset_for_both/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lee4pd/wikipeqa_an_evaluation_dataset_for_both/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T10:58:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lef9o7</id>
    <title>„ÄêNew release v1.7.1„ÄëDingo: A Comprehensive Data Quality Evaluation Tool</title>
    <updated>2025-06-18T12:01:24+00:00</updated>
    <author>
      <name>/u/chupei0</name>
      <uri>https://old.reddit.com/user/chupei0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/DataEval/dingo"&gt;https://github.com/DataEval/dingo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;welcome give us a star üåüüåüüåü&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chupei0"&gt; /u/chupei0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lef9o7/new_release_v171dingo_a_comprehensive_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lef9o7/new_release_v171dingo_a_comprehensive_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lef9o7/new_release_v171dingo_a_comprehensive_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T12:01:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lefgmh</id>
    <title>gpt_agents.py</title>
    <updated>2025-06-18T12:10:57+00:00</updated>
    <author>
      <name>/u/jameswdelancey</name>
      <uri>https://old.reddit.com/user/jameswdelancey</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/jameswdelancey/gpt_agents.py"&gt;https://github.com/jameswdelancey/gpt_agents.py&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A single-file, multi-agent framework for LLMs‚Äîeverything is implemented in one core file with no dependencies for maximum clarity and hackability. See the main implementation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jameswdelancey"&gt; /u/jameswdelancey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lefgmh/gpt_agentspy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lefgmh/gpt_agentspy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lefgmh/gpt_agentspy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T12:10:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1le3b9e</id>
    <title>Cheap dual Radeon, 60 tk/s Qwen3-30B-A3B</title>
    <updated>2025-06-18T00:19:28+00:00</updated>
    <author>
      <name>/u/dsjlee</name>
      <uri>https://old.reddit.com/user/dsjlee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1le3b9e/cheap_dual_radeon_60_tks_qwen330ba3b/"&gt; &lt;img alt="Cheap dual Radeon, 60 tk/s Qwen3-30B-A3B" src="https://external-preview.redd.it/dzU0NXF1ZXd3azdmMRL6N26Lhnz9zx3CK2rpMgt595CDjr45ninPojQsc6H2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d372a765787447d55212161ab23fd7b0049a0f69" title="Cheap dual Radeon, 60 tk/s Qwen3-30B-A3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got new RX 9060 XT 16GB. Kept old RX 6600 8GB to increase vram pool. Quite surprised 30B MoE model running much faster than running on CPU with GPU partial offload.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dsjlee"&gt; /u/dsjlee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fdxzcidwwk7f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1le3b9e/cheap_dual_radeon_60_tks_qwen330ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1le3b9e/cheap_dual_radeon_60_tks_qwen330ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T00:19:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldxuk1</id>
    <title>The Gemini 2.5 models are sparse mixture-of-experts (MoE)</title>
    <updated>2025-06-17T20:24:15+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldxuk1/the_gemini_25_models_are_sparse_mixtureofexperts/"&gt; &lt;img alt="The Gemini 2.5 models are sparse mixture-of-experts (MoE)" src="https://b.thumbs.redditmedia.com/HQc2i0Um5x7pBOoOn-gVnTMmIXgB-srLNatUzMp3veI.jpg" title="The Gemini 2.5 models are sparse mixture-of-experts (MoE)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From the &lt;a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf"&gt;model report&lt;/a&gt;. It should be a surprise to noone, but it's good to see this being spelled out. We barely ever learn anything about the architecture of closed models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zhyrdk2dqj7f1.png?width=1056&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca3d89968dc6bf950d030bbab25243aeb7623cf4"&gt;https://preview.redd.it/zhyrdk2dqj7f1.png?width=1056&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca3d89968dc6bf950d030bbab25243aeb7623cf4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(I am still hoping for a Gemma-3N report...)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldxuk1/the_gemini_25_models_are_sparse_mixtureofexperts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldxuk1/the_gemini_25_models_are_sparse_mixtureofexperts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldxuk1/the_gemini_25_models_are_sparse_mixtureofexperts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T20:24:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldv0hk</id>
    <title>:grab popcorn: OpenAI weighs ‚Äúnuclear option‚Äù of antitrust complaint against Microsoft</title>
    <updated>2025-06-17T18:34:19+00:00</updated>
    <author>
      <name>/u/tabspaces</name>
      <uri>https://old.reddit.com/user/tabspaces</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldv0hk/grab_popcorn_openai_weighs_nuclear_option_of/"&gt; &lt;img alt=":grab popcorn: OpenAI weighs ‚Äúnuclear option‚Äù of antitrust complaint against Microsoft" src="https://external-preview.redd.it/o-39gdKiRmg7xCtqAV9Kzd__IIP_fxUuIZpgEMOTxUU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c859f5b46d753fef35eae0d18284d780a193b411" title=":grab popcorn: OpenAI weighs ‚Äúnuclear option‚Äù of antitrust complaint against Microsoft" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabspaces"&gt; /u/tabspaces &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arstechnica.com/ai/2025/06/openai-weighs-nuclear-option-of-antitrust-complaint-against-microsoft/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldv0hk/grab_popcorn_openai_weighs_nuclear_option_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldv0hk/grab_popcorn_openai_weighs_nuclear_option_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T18:34:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1legaq8</id>
    <title>Update:My agent model now supports OpenAI function calling format! (mirau-agent-base)</title>
    <updated>2025-06-18T12:51:47+00:00</updated>
    <author>
      <name>/u/EliaukMouse</name>
      <uri>https://old.reddit.com/user/EliaukMouse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1legaq8/updatemy_agent_model_now_supports_openai_function/"&gt; &lt;img alt="Update:My agent model now supports OpenAI function calling format! (mirau-agent-base)" src="https://external-preview.redd.it/8a8-yIo-XSHNh-GrNf4RtIK3HA5ouO1zg1RogdSi4c0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd68843d35315a418aca711de9b0c6516cd2f26e" title="Update:My agent model now supports OpenAI function calling format! (mirau-agent-base)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;A while back I shared my multi-turn tool-calling model &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1l7v9gf/a_multiturn_toolcalling_base_model_for_rl_agent/"&gt;in this post&lt;/a&gt;. Based on community feedback about OpenAI compatibility, I've updated the model to support OpenAI's function calling format!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's new:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full compatibility with OpenAI's tool/function definition format&lt;/li&gt; &lt;li&gt;New model available at: &lt;a href="https://huggingface.co/eliuakk/mirau-agent-base-oai"&gt;https://huggingface.co/eliuakk/mirau-agent-base-oai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Live demo: &lt;a href="https://modelscope.cn/studios/mouseEliauk/mirau-agent-demo/summary"&gt;https://modelscope.cn/studios/mouseEliauk/mirau-agent-demo/summary&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;About the model:&lt;/strong&gt; mirau-agent-14b-base is a large language model specifically optimized for Agent scenarios, fine-tuned from Qwen2.5-14B-Instruct. This model focuses on enhancing multi-turn tool-calling capabilities, enabling it to autonomously plan, execute tasks, and handle exceptions in complex interactive environments.&lt;/p&gt; &lt;p&gt;Although named &amp;quot;base,&amp;quot; this does not refer to a pre-trained only base model. Instead, it is a &amp;quot;cold-start&amp;quot; version that has undergone Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). It provides a high-quality initial policy for subsequent reinforcement learning training. We also hope the community can further enhance it with RL.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EliaukMouse"&gt; /u/EliaukMouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/eliuakk/mirau-agent-base-oai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1legaq8/updatemy_agent_model_now_supports_openai_function/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1legaq8/updatemy_agent_model_now_supports_openai_function/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T12:51:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lehbra</id>
    <title>Built memX: a shared memory backend for LLM agents (demo + open-source code)</title>
    <updated>2025-06-18T13:37:19+00:00</updated>
    <author>
      <name>/u/Temporary-Tap-7323</name>
      <uri>https://old.reddit.com/user/Temporary-Tap-7323</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lehbra/built_memx_a_shared_memory_backend_for_llm_agents/"&gt; &lt;img alt="Built memX: a shared memory backend for LLM agents (demo + open-source code)" src="https://external-preview.redd.it/bWpmbGR5djV2bzdmMYCyFtIdy85G-V-pyC1NhTykFPs5rMNi1ya3S7fCsS5U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16d6b58da2a98255f6e48827413597a42a678592" title="Built memX: a shared memory backend for LLM agents (demo + open-source code)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone ‚Äî I built this over the weekend and wanted to share:&lt;/p&gt; &lt;p&gt;üîó &lt;a href="https://github.com/MehulG/memX"&gt;https://github.com/MehulG/memX&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;memX&lt;/strong&gt; is a shared memory layer for LLM agents ‚Äî kind of like Redis, but with real-time sync, pub/sub, schema validation, and access control.&lt;/p&gt; &lt;p&gt;Instead of having agents pass messages or follow a fixed pipeline, they just read and write to shared memory keys. It‚Äôs like a collaborative whiteboard where agents evolve context together.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt; - Real-time pub/sub - Per-key JSON schema validation - API key-based ACLs - Python SDK&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary-Tap-7323"&gt; /u/Temporary-Tap-7323 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ibq16xv5vo7f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lehbra/built_memx_a_shared_memory_backend_for_llm_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lehbra/built_memx_a_shared_memory_backend_for_llm_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T13:37:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1leea24</id>
    <title>MiniMax-M1</title>
    <updated>2025-06-18T11:06:41+00:00</updated>
    <author>
      <name>/u/David-Kunz</name>
      <uri>https://old.reddit.com/user/David-Kunz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leea24/minimaxm1/"&gt; &lt;img alt="MiniMax-M1" src="https://external-preview.redd.it/oMQEZmVHLMDxK1KRt9PeRYnQDqbulLVqufYW6GyaZ_E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e03a77037995d5532bd94e47fb40d5cd61b33f6" title="MiniMax-M1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/David-Kunz"&gt; /u/David-Kunz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MiniMax-AI/MiniMax-M1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leea24/minimaxm1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1leea24/minimaxm1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T11:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1le0mpb</id>
    <title>Llama.cpp is much faster! Any changes made recently?</title>
    <updated>2025-06-17T22:17:52+00:00</updated>
    <author>
      <name>/u/simracerman</name>
      <uri>https://old.reddit.com/user/simracerman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've ditched Ollama for about 3 months now, and been on a journey testing multiple wrappers. KoboldCPP coupled with llama swap has been good but I experienced so many hang ups (I leave my PC running 24/7 to serve AI requests), and waking up almost daily and Kobold (or in combination with AMD drivers) would not work. I had to reset llama swap or reboot the PC for it work again.&lt;/p&gt; &lt;p&gt;That said, I tried llama.cpp a few weeks ago and it wasn't smooth with Vulkan (likely some changes that was reverted back). Tried it again yesterday, and the inference speed is 20% faster on average across multiple model types and sizes.&lt;/p&gt; &lt;p&gt;Specifically for Vulkan, I didn't see anything major in the release notes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simracerman"&gt; /u/simracerman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1le0mpb/llamacpp_is_much_faster_any_changes_made_recently/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1le0mpb/llamacpp_is_much_faster_any_changes_made_recently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1le0mpb/llamacpp_is_much_faster_any_changes_made_recently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T22:17:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lecpcr</id>
    <title>NVIDIA B300 cut all INT8 and FP64 performance???</title>
    <updated>2025-06-18T09:27:17+00:00</updated>
    <author>
      <name>/u/Mindless_Pain1860</name>
      <uri>https://old.reddit.com/user/Mindless_Pain1860</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lecpcr/nvidia_b300_cut_all_int8_and_fp64_performance/"&gt; &lt;img alt="NVIDIA B300 cut all INT8 and FP64 performance???" src="https://preview.redd.it/cekoaeehmn7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2334c9b7ee829fa01a3d12ddcf78fd5d800909f0" title="NVIDIA B300 cut all INT8 and FP64 performance???" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nvidia.com/en-us/data-center/hgx/"&gt;https://www.nvidia.com/en-us/data-center/hgx/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mindless_Pain1860"&gt; /u/Mindless_Pain1860 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cekoaeehmn7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lecpcr/nvidia_b300_cut_all_int8_and_fp64_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lecpcr/nvidia_b300_cut_all_int8_and_fp64_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T09:27:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1le951x</id>
    <title>GMK X2(AMD Max+ 395 w/128GB) first impressions.</title>
    <updated>2025-06-18T05:28:44+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've had a X2 for about a day. These are my first impressions of it including a bunch of numbers comparing it to other GPUs I have.&lt;/p&gt; &lt;p&gt;First, the people who were claiming that you couldn't load a model larger than 64GB because it would need to use 64GB of RAM for the CPU too are wrong. That's simple user error. That is simply not the case.&lt;/p&gt; &lt;p&gt;Update: I'm having big model problems. I can load a big model with ROCm. But when it starts to infer, it dies with some unsupported function error. I think I need ROCm 6.4.1 for Strix Halo support. Vulkan works but there's a Vulkan memory limit of 32GB. At least with the driver I'm using under Windows. More on that down below where I talk about shared memory. ROCm does report the available amount of memory to be 110GB. I don't know how that's going to work out since only 96GB is allocated to the GPU so some of that 110GB belongs to the CPU. There's no 110GB option in the BIOS.&lt;/p&gt; &lt;p&gt;Update #2: I thought of a work around with Vulkan. It isn't pretty but it does the job. I should be able to load models up go 80GB. Here's a 50GB model. It's only a quick run since it's late. I'll do a full run tomorrow.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; | llama4 17Bx16E (Scout) Q3_K - Medium | 49.47 GiB | 107.77 B | RPC,Vulkan | 999 | 0 | pp512 | 135.93 ¬± 2.39 | | llama4 17Bx16E (Scout) Q3_K - Medium | 49.47 GiB | 107.77 B | RPC,Vulkan | 999 | 0 | tg128 | 19.99 ¬± 0.02 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Second, the GPU can use 120W. It does that when doing PP. Unfortunately, TG seems to be memory bandwidth limited and when doing that the GPU is at around 89W.&lt;/p&gt; &lt;p&gt;Third, as delivered the BIOS was not capable of allocating more than 64GB to the GPU on my 128GB machine. It needed a BIOS update. GMK should at least send email about that with a link to the correct BIOS to use. I first tried the one linked to on the GMK store page. That updated me to what it claimed was the required one, version 1.04 from 5/12 or later. The BIOS was dated 5/12. That didn't do the job. I still couldn't allocate more than 64GB to the GPU. So I dug around the GMK website and found a link to a different BIOS. It is also version 1.04 but was dated 5/14. That one worked. It took forever to flash compared to the first one and took forever to reboot, it turns out twice. There was no video signal for what felt like a long time, although it was probably only about a minute or so. But it finally showed the GMK logo only to restart again with another wait. The second time it booted back up to Windows. This time I could set the VRAM allocation to 96GB.&lt;/p&gt; &lt;p&gt;Overall, it's as I expected. So far, it's like my M1 Max with 96GB. But with about 3x the PP speed. It strangely uses more than a bit of &amp;quot;shared memory&amp;quot; for the GPU as opposed to the &amp;quot;dedicated memory&amp;quot;. Like GBs worth. Which normally would make me believe it's slowing it down, on this machine though the &amp;quot;shared&amp;quot; and &amp;quot;dedicated&amp;quot; RAM is the same. Although it's probably less efficient to go though the shared stack. I wish there was a way to turn off shared memory for a GPU in Windows. It can be done in Linux.&lt;/p&gt; &lt;p&gt;Update: I think I figured it out. There's always a little shared memory being used but what I see is that there's like 15GB of shared memory being used. It's Vulkan. It seems to top out at a 32GB allocation. Then it starts to leverage shared memory. So even though it's only using 32 out of 96GB of dedicated memory, it starts filling out the shared memory. So that limits the maximum size of the model to 47GB under Vulkan.&lt;/p&gt; &lt;p&gt;Here are a bunch of numbers. First for a small LLM that I can fit onto a 3060 12GB. Then successively bigger from there. For the 9B model, I threw in a run for the Max+ using only the CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;9B&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;**Max+** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | RPC,Vulkan | 99 | 0 | pp512 | 923.76 ¬± 2.45 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | RPC,Vulkan | 99 | 0 | tg128 | 21.22 ¬± 0.03 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | RPC,Vulkan | 99 | 0 | pp512 @ d5000 | 486.25 ¬± 1.08 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | RPC,Vulkan | 99 | 0 | tg128 @ d5000 | 12.31 ¬± 0.04 | **M1 Max** | model | size | params | backend | threads | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | ------: | ---: | --------------: | -------------------: | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | Metal,BLAS,RPC | 8 | 0 | pp512 | 335.93 ¬± 0.22 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | Metal,BLAS,RPC | 8 | 0 | tg128 | 28.08 ¬± 0.02 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | Metal,BLAS,RPC | 8 | 0 | pp512 @ d5000 | 262.21 ¬± 0.15 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | Metal,BLAS,RPC | 8 | 0 | tg128 @ d5000 | 20.07 ¬± 0.01 | **3060** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | Vulkan,RPC | 999 | 0 | pp512 | 951.23 ¬± 1.50 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | Vulkan,RPC | 999 | 0 | tg128 | 26.40 ¬± 0.12 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | Vulkan,RPC | 999 | 0 | pp512 @ d5000 | 545.49 ¬± 9.61 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | Vulkan,RPC | 999 | 0 | tg128 @ d5000 | 19.94 ¬± 0.01 | **7900xtx** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | Vulkan,RPC | 999 | 0 | pp512 | 2164.10 ¬± 3.98 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | Vulkan,RPC | 999 | 0 | tg128 | 61.94 ¬± 0.20 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | Vulkan,RPC | 999 | 0 | pp512 @ d5000 | 1197.40 ¬± 4.75 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | Vulkan,RPC | 999 | 0 | tg128 @ d5000 | 44.51 ¬± 0.08 | **Max+ CPU** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | RPC,Vulkan | 0 | 0 | pp512 | 438.57 ¬± 3.88 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | RPC,Vulkan | 0 | 0 | tg128 | 6.99 ¬± 0.01 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | RPC,Vulkan | 0 | 0 | pp512 @ d5000 | 292.43 ¬± 0.30 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | RPC,Vulkan | 0 | 0 | tg128 @ d5000 | 5.82 ¬± 0.01 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;27B Q5&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;**Max+** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | RPC,Vulkan | 99 | 0 | pp512 | 129.93 ¬± 0.08 | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | RPC,Vulkan | 99 | 0 | tg128 | 10.38 ¬± 0.01 | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | RPC,Vulkan | 99 | 0 | pp512 @ d10000 | 97.25 ¬± 0.04 | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | RPC,Vulkan | 99 | 0 | tg128 @ d10000 | 4.70 ¬± 0.01 | **M1 Max** | model | size | params | backend | threads | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | ------: | ---: | --------------: | -------------------: | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | Metal,BLAS,RPC | 8 | 0 | pp512 | 79.02 ¬± 0.02 | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | Metal,BLAS,RPC | 8 | 0 | tg128 | 10.15 ¬± 0.00 | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | Metal,BLAS,RPC | 8 | 0 | pp512 @ d10000 | 67.11 ¬± 0.04 | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | Metal,BLAS,RPC | 8 | 0 | tg128 @ d10000 | 7.39 ¬± 0.00 | **7900xtx** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | Vulkan,RPC | 999 | 0 | pp512 | 342.95 ¬± 0.13 | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | Vulkan,RPC | 999 | 0 | tg128 | 35.80 ¬± 0.01 | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | Vulkan,RPC | 999 | 0 | pp512 @ d10000 | 244.69 ¬± 1.99 | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | Vulkan,RPC | 999 | 0 | tg128 @ d10000 | 19.03 ¬± 0.05 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;27B Q8&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;**Max+** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | gemma2 27B Q8_0 | 26.94 GiB | 27.23 B | RPC,Vulkan | 99 | 0 | pp512 | 318.41 ¬± 0.71 | | gemma2 27B Q8_0 | 26.94 GiB | 27.23 B | RPC,Vulkan | 99 | 0 | tg128 | 7.61 ¬± 0.00 | | gemma2 27B Q8_0 | 26.94 GiB | 27.23 B | RPC,Vulkan | 99 | 0 | pp512 @ d10000 | 175.32 ¬± 0.08 | | gemma2 27B Q8_0 | 26.94 GiB | 27.23 B | RPC,Vulkan | 99 | 0 | tg128 @ d10000 | 3.97 ¬± 0.01 | **M1 Max** | model | size | params | backend | threads | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | ------: | ---: | --------------: | -------------------: | | gemma2 27B Q8_0 | 26.94 GiB | 27.23 B | Metal,BLAS,RPC | 8 | 0 | pp512 | 90.87 ¬± 0.24 | | gemma2 27B Q8_0 | 26.94 GiB | 27.23 B | Metal,BLAS,RPC | 8 | 0 | tg128 | 11.00 ¬± 0.00 | **7900xtx + 3060** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | gemma2 27B Q8_0 | 26.94 GiB | 27.23 B | Vulkan,RPC | 999 | 0 | pp512 | 493.75 ¬± 0.98 | | gemma2 27B Q8_0 | 26.94 GiB | 27.23 B | Vulkan,RPC | 999 | 0 | tg128 | 16.09 ¬± 0.02 | | gemma2 27B Q8_0 | 26.94 GiB | 27.23 B | Vulkan,RPC | 999 | 0 | pp512 @ d10000 | 269.98 ¬± 5.03 | | gemma2 27B Q8_0 | 26.94 GiB | 27.23 B | Vulkan,RPC | 999 | 0 | tg128 @ d10000 | 10.49 ¬± 0.02 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;32B&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;**Max+** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | qwen2 32B Q8_0 | 32.42 GiB | 32.76 B | RPC,Vulkan | 99 | 0 | pp512 | 231.05 ¬± 0.73 | | qwen2 32B Q8_0 | 32.42 GiB | 32.76 B | RPC,Vulkan | 99 | 0 | tg128 | 6.44 ¬± 0.00 | | qwen2 32B Q8_0 | 32.42 GiB | 32.76 B | RPC,Vulkan | 99 | 0 | pp512 @ d10000 | 84.68 ¬± 0.26 | | qwen2 32B Q8_0 | 32.42 GiB | 32.76 B | RPC,Vulkan | 99 | 0 | tg128 @ d10000 | 4.62 ¬± 0.01 | **7900xtx + 3060 + 2070** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | qwen2 32B Q8_0 | 32.42 GiB | 32.76 B | RPC,Vulkan | 999 | 0 | pp512 | 342.35 ¬± 17.21 | | qwen2 32B Q8_0 | 32.42 GiB | 32.76 B | RPC,Vulkan | 999 | 0 | tg128 | 11.52 ¬± 0.18 | | qwen2 32B Q8_0 | 32.42 GiB | 32.76 B | RPC,Vulkan | 999 | 0 | pp512 @ d10000 | 213.81 ¬± 3.92 | | qwen2 32B Q8_0 | 32.42 GiB | 32.76 B | RPC,Vulkan | 999 | 0 | tg128 @ d10000 | 8.27 ¬± 0.02 | &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1le951x/gmk_x2amd_max_395_w128gb_first_impressions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1le951x/gmk_x2amd_max_395_w128gb_first_impressions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1le951x/gmk_x2amd_max_395_w128gb_first_impressions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T05:28:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1led23c</id>
    <title>Local AI for a small/median accounting firm - ‚Ç¨ Buget of 10k-25k</title>
    <updated>2025-06-18T09:51:00+00:00</updated>
    <author>
      <name>/u/AFruitShopOwner</name>
      <uri>https://old.reddit.com/user/AFruitShopOwner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Our medium-sized &lt;strong&gt;accounting firm&lt;/strong&gt; (around 100 people) in the &lt;strong&gt;Netherlands&lt;/strong&gt; is looking to set up a local AI system, I'm hoping to tap into your collective wisdom for some recommendations. The &lt;strong&gt;budget&lt;/strong&gt; is roughly &lt;strong&gt;‚Ç¨10k-‚Ç¨25k.&lt;/strong&gt; This is purely for the hardware. I'll be able to build the system myself. I'll also handle the software side. I don't have a lot of experience actually running local models but I do spent a lot of my free time watching videos about it.&lt;/p&gt; &lt;p&gt;We're going local for privacy. Keeping sensitive client data in-house is paramount. My boss does not want anything going to the cloud.&lt;/p&gt; &lt;p&gt;Some more info about use cases what I had in mind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;RAG system&lt;/strong&gt; for professional questions about Dutch accounting standards and laws. (We already have an extensive librairy of documents, neatly orderd)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Analyzing and summarizing&lt;/strong&gt; various files like contracts, invoices, emails, excel sheets, word files and pdfs.&lt;/li&gt; &lt;li&gt;Developing &lt;strong&gt;AI agents&lt;/strong&gt; for more advanced task automation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Coding assistance&lt;/strong&gt; for our data analyst (mainly in Python).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm looking for broad advice on:&lt;/p&gt; &lt;p&gt;Hardware&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Go with a &lt;strong&gt;CPU&lt;/strong&gt; based or &lt;strong&gt;GPU based&lt;/strong&gt; set up?&lt;/li&gt; &lt;li&gt;If I go with GPU's should I go with a couple of consumer GPU's like 3090/4090's or maybe a single Pro 6000? Why pick one over the other (cost obviously)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Software&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Operating System:&lt;/strong&gt; Is Linux still the go-to for optimal AI performance and compatibility with frameworks?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local AI Model (LLMs):&lt;/strong&gt; What LLMs are generally recommended for a mix of RAG, summarization, agentic workflows, and coding? Or should I consider running multiple models? I've read some positive reviews about qwen3 235b. Can I even run a model like that with reasonable tps within this budget? Probably not the full 235b variant?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference Software:&lt;/strong&gt; What are the best tools for running open-source LLMs locally, from user-friendly options for beginners to high-performance frameworks for scaling?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Supporting Software:&lt;/strong&gt; What recommendations do you have for open-source tools or frameworks for building RAG systems (vector databases, RAG frameworks) and AI agents?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any general insights, experiences, or project architectural advice would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thanks in advance for your input!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Wow, thank you all for the incredible amount of feedback and advice! &lt;/p&gt; &lt;p&gt;I want to clarify a couple of things that came up in the comments:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This system will probably only be used by 20 users, with probably no more than 5 using it at the same time.&lt;/li&gt; &lt;li&gt;My boss and our IT team are aware that this is an experimental project. The goal is to build in-house knowledge, and we are prepared for some setbacks along the way. Our company already has the necessary infrastructure for security and data backups.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks again to everyone for the valuable input! It has given me a lot to think about and will be extremely helpful as I move forward with this project.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AFruitShopOwner"&gt; /u/AFruitShopOwner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1led23c/local_ai_for_a_smallmedian_accounting_firm_buget/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1led23c/local_ai_for_a_smallmedian_accounting_firm_buget/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1led23c/local_ai_for_a_smallmedian_accounting_firm_buget/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T09:51:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1leh14g</id>
    <title>Can your favourite local model solve this?</title>
    <updated>2025-06-18T13:24:24+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leh14g/can_your_favourite_local_model_solve_this/"&gt; &lt;img alt="Can your favourite local model solve this?" src="https://preview.redd.it/gkjegqtyso7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93880be720bd03128b1e673976aa49f67626b2f0" title="Can your favourite local model solve this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am interested which, if any, models this relatively simple geometry picture if you simply give it this image.&lt;/p&gt; &lt;p&gt;I don't have a big enough setup to test visual models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gkjegqtyso7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leh14g/can_your_favourite_local_model_solve_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1leh14g/can_your_favourite_local_model_solve_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T13:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1led0lb</id>
    <title>Google doubled the price of Gemini 2.5 Flash thinking output after GA from 0.15 to 0.30 what</title>
    <updated>2025-06-18T09:48:16+00:00</updated>
    <author>
      <name>/u/NoAd2240</name>
      <uri>https://old.reddit.com/user/NoAd2240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry the input**&lt;/p&gt; &lt;p&gt;&lt;a href="https://cloud.google.com/vertex-ai/generative-ai/pricing"&gt;https://cloud.google.com/vertex-ai/generative-ai/pricing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoAd2240"&gt; /u/NoAd2240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1led0lb/google_doubled_the_price_of_gemini_25_flash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1led0lb/google_doubled_the_price_of_gemini_25_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1led0lb/google_doubled_the_price_of_gemini_25_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T09:48:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lei5mb</id>
    <title>Oops</title>
    <updated>2025-06-18T14:12:39+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lei5mb/oops/"&gt; &lt;img alt="Oops" src="https://preview.redd.it/iv35yrek1p7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a1be0e37ffab5a4926e5a5a7a869b2ee3a9c853" title="Oops" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iv35yrek1p7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lei5mb/oops/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lei5mb/oops/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T14:12:39+00:00</published>
  </entry>
</feed>
