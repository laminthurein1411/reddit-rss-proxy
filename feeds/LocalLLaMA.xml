<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-30T18:25:01+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iczucy</id>
    <title>Running Deepseek R1 IQ2XXS (200GB) from SSD actually works</title>
    <updated>2025-01-29T17:51:28+00:00</updated>
    <author>
      <name>/u/Wrong-Historian</name>
      <uri>https://old.reddit.com/user/Wrong-Historian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;prompt eval time = 97774.66 ms / 367 tokens ( 266.42 ms per token, 3.75 tokens per second) eval time = 253545.02 ms / 380 tokens ( 667.22 ms per token, 1.50 tokens per second) total time = 351319.68 ms / 747 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No, not a distill, but a 2bit quantized version of the actual 671B model (&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ2_XXS"&gt;IQ2XXS&lt;/a&gt;), about 200GB large, running on a 14900K with 96GB DDR5 6800 and a single 3090 24GB (with 5 layers offloaded), and for the rest running off of PCIe 4.0 SSD (Samsung 990 pro)&lt;/p&gt; &lt;p&gt;Although of limited actual usefulness, it's just amazing that is actually works! With larger context it takes a couple of minutes just to process the prompt, token generation is actually reasonably fast.&lt;/p&gt; &lt;p&gt;Thanks &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1icrc2l/comment/m9t5cbw/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1icrc2l/comment/m9t5cbw/&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;Edit: one hour later, i've tried a bigger prompt (800 tokens input), with more tokens output (6000 tokens output)&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;prompt eval time = 210540.92 ms / 803 tokens ( 262.19 ms per token, 3.81 tokens per second)&lt;br /&gt; eval time = 6883760.49 ms / 6091 tokens ( 1130.15 ms per token, 0.88 tokens per second)&lt;br /&gt; total time = 7094301.41 ms / 6894 tokens&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It 'works'. Lets keep it at that. Usable? Meh. The main drawback is all the &amp;lt;thinking&amp;gt;... honestly. For a simple answer it does a whole lot of &amp;lt;thinking&amp;gt; and that takes a lot of tokens and thus a lot of time and context in follow-up questions taking even more time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong-Historian"&gt; /u/Wrong-Historian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iczucy/running_deepseek_r1_iq2xxs_200gb_from_ssd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iczucy/running_deepseek_r1_iq2xxs_200gb_from_ssd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iczucy/running_deepseek_r1_iq2xxs_200gb_from_ssd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T17:51:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1idiurl</id>
    <title>What about 1 TB Sys RAM system with the 7995WX to run LLMs ?</title>
    <updated>2025-01-30T08:53:33+00:00</updated>
    <author>
      <name>/u/MatrixEternal</name>
      <uri>https://old.reddit.com/user/MatrixEternal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Title Edit : ..... with a most powerful CPU ....&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Today, I tried running the DeepSeek R1 2.58-bit Quant version on a 24 vCPU, 192 GB RAM server without a GPU. I achieved a speed of about 11 tokens/second in the pg512 test. Meanwhile, four A40 GPUs produced around 33 tokens/second.&lt;/p&gt; &lt;p&gt;This got me thinking about a possible setup. For my personal needs, 11 tokens/second seems adequate. However, for a very large LLM such as R1 Q8_0, which requires 700 GB of VRAM, one would typically need eight A100 GPUs (H100s are even more expensive) and would also have to offload some layers to the CPU. That setup costs around $177,840.&lt;/p&gt; &lt;p&gt;In contrast, a Ryzen Threadripper PRO 7995WX costs around $11,500, and 1 TB of RAM is about $2,400, so the total would be roughly $14,000‚Äîabout twelve times cheaper. Of course, the inference speed would be significantly slower, and performance might suffer as the context window grows, but it‚Äôs still feasible to own a personal system.&lt;/p&gt; &lt;p&gt;I‚Äôm new to LLMs, so I‚Äôd love to hear any additional thoughts or suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MatrixEternal"&gt; /u/MatrixEternal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idiurl/what_about_1_tb_sys_ram_system_with_the_7995wx_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idiurl/what_about_1_tb_sys_ram_system_with_the_7995wx_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idiurl/what_about_1_tb_sys_ram_system_with_the_7995wx_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T08:53:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1idrxfs</id>
    <title>What is the current best small model for coding?</title>
    <updated>2025-01-30T17:13:26+00:00</updated>
    <author>
      <name>/u/Boltyx</name>
      <uri>https://old.reddit.com/user/Boltyx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For all of us with a medium level GPU (10-20 gb in VRAM), what are the current top open source models that should be tried first for a local coding assistant?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Boltyx"&gt; /u/Boltyx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idrxfs/what_is_the_current_best_small_model_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idrxfs/what_is_the_current_best_small_model_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idrxfs/what_is_the_current_best_small_model_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T17:13:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1id3ak8</id>
    <title>Ex-Google, Apple engineers launch unconditionally open source Oumi AI platform that could help to build the next DeepSeek</title>
    <updated>2025-01-29T20:10:23+00:00</updated>
    <author>
      <name>/u/Revenant013</name>
      <uri>https://old.reddit.com/user/Revenant013</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/"&gt; &lt;img alt="Ex-Google, Apple engineers launch unconditionally open source Oumi AI platform that could help to build the next DeepSeek" src="https://external-preview.redd.it/x6M3uf-eJl-Hf8TvVZ4dGYKKD1ETTCuW2FzYR0-kr7E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=941f580733ebba68fbdba4b08ca747ba331fe7f7" title="Ex-Google, Apple engineers launch unconditionally open source Oumi AI platform that could help to build the next DeepSeek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Revenant013"&gt; /u/Revenant013 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://venturebeat.com/ai/ex-google-apple-engineers-launch-unconditionally-open-source-oumi-ai-platform-that-could-help-to-build-the-next-deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T20:10:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ids22f</id>
    <title>Microsoft and University of China develop CoRAG (chain of retrieval augmented generation)</title>
    <updated>2025-01-30T17:18:32+00:00</updated>
    <author>
      <name>/u/throwawayacc201711</name>
      <uri>https://old.reddit.com/user/throwawayacc201711</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ids22f/microsoft_and_university_of_china_develop_corag/"&gt; &lt;img alt="Microsoft and University of China develop CoRAG (chain of retrieval augmented generation)" src="https://external-preview.redd.it/6eoofLfOC4_cc9PR6ZpafYjT0gy2izYp6JJgo6BzJoI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=648350a31801382dd38062c49ca776907bcc5b1a" title="Microsoft and University of China develop CoRAG (chain of retrieval augmented generation)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This could led to some awesome improvements in accuracy for RAG. This is essentially CoT applied to the queries for RAG.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/throwawayacc201711"&gt; /u/throwawayacc201711 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.marktechpost.com/2025/01/28/microsoft-ai-introduces-corag-chain-of-retrieval-augmented-generation-an-ai-framework-for-iterative-retrieval-and-reasoning-in-knowledge-intensive-tasks/?amp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ids22f/microsoft_and_university_of_china_develop_corag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ids22f/microsoft_and_university_of_china_develop_corag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T17:18:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1idltqu</id>
    <title>Fantastic summary of DeepSeek R1 and why it's such a big deal by Computerphile</title>
    <updated>2025-01-30T12:26:30+00:00</updated>
    <author>
      <name>/u/CrasHthe2nd</name>
      <uri>https://old.reddit.com/user/CrasHthe2nd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idltqu/fantastic_summary_of_deepseek_r1_and_why_its_such/"&gt; &lt;img alt="Fantastic summary of DeepSeek R1 and why it's such a big deal by Computerphile" src="https://external-preview.redd.it/-2yTeJWjY7Xy7hJ5TsvAGSJhZsAdkXzGD9s5XXxFxaY.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d8f45dbff9232c0291f5891b3546986242e2cef" title="Fantastic summary of DeepSeek R1 and why it's such a big deal by Computerphile" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CrasHthe2nd"&gt; /u/CrasHthe2nd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/gY4Z-9QlZ64"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idltqu/fantastic_summary_of_deepseek_r1_and_why_its_such/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idltqu/fantastic_summary_of_deepseek_r1_and_why_its_such/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T12:26:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1idivqe</id>
    <title>The Mac M2 Ultra is faster than 2xH100s in running Deepseek R1 IQ1_S.</title>
    <updated>2025-01-30T08:55:42+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over on the llama.cpp github, people have been benchmarking R1 IQ1_S. The M2 Ultra is faster than two H100s for TG. The M2 Ultra gets 13.88t/s. 2xH100s get in the best run 11.53t/s. That's surprising.&lt;/p&gt; &lt;p&gt;As for PP processing, that's all over the place on the 2xH100s. From 0.41 to 137.66. For the M2 Ultra it's 24.05.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggerganov/llama.cpp/issues/11474"&gt;https://github.com/ggerganov/llama.cpp/issues/11474&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idivqe/the_mac_m2_ultra_is_faster_than_2xh100s_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idivqe/the_mac_m2_ultra_is_faster_than_2xh100s_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idivqe/the_mac_m2_ultra_is_faster_than_2xh100s_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T08:55:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1icwys9</id>
    <title>Berkley AI research team claims to reproduce DeepSeek core technologies for $30</title>
    <updated>2025-01-29T15:54:59+00:00</updated>
    <author>
      <name>/u/Slasher1738</name>
      <uri>https://old.reddit.com/user/Slasher1738</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/ai-research-team-claims-to-reproduce-deepseek-core-technologies-for-usd30-relatively-small-r1-zero-model-has-remarkable-problem-solving-abilities"&gt;https://www.tomshardware.com/tech-industry/artificial-intelligence/ai-research-team-claims-to-reproduce-deepseek-core-technologies-for-usd30-relatively-small-r1-zero-model-has-remarkable-problem-solving-abilities&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;An AI research team from the University of California, Berkeley, led by Ph.D. candidate Jiayi Pan, claims to have reproduced DeepSeek R1-Zero‚Äôs core technologies for just $30, showing how advanced models could be implemented affordably. According to Jiayi Pan on &lt;a href="https://nitter.lucabased.xyz/jiayi_pirate/status/1882839370505621655"&gt;Nitter&lt;/a&gt;, their team reproduced DeepSeek R1-Zero in the Countdown game, and the small language model, with its 3 billion parameters, developed self-verification and search abilities through reinforcement learning.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;DeepSeek R1's cost advantage seems real. Not looking good for OpenAI. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slasher1738"&gt; /u/Slasher1738 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icwys9/berkley_ai_research_team_claims_to_reproduce/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icwys9/berkley_ai_research_team_claims_to_reproduce/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icwys9/berkley_ai_research_team_claims_to_reproduce/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T15:54:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1idny30</id>
    <title>mistralai/Mistral-Small-24B-Instruct-2501 ¬∑ Hugging Face</title>
    <updated>2025-01-30T14:17:54+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idny30/mistralaimistralsmall24binstruct2501_hugging_face/"&gt; &lt;img alt="mistralai/Mistral-Small-24B-Instruct-2501 ¬∑ Hugging Face" src="https://external-preview.redd.it/8WbpVBLCMGZToRjDI6ufWcY1nKKfpipz-TKy8aHrbsg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0fb438638f6c72b0b1b0db98ac490b6bda471f2f" title="mistralai/Mistral-Small-24B-Instruct-2501 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idny30/mistralaimistralsmall24binstruct2501_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idny30/mistralaimistralsmall24binstruct2501_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:17:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1idgrh4</id>
    <title>What are you *actually* using R1 for?</title>
    <updated>2025-01-30T06:35:23+00:00</updated>
    <author>
      <name>/u/PataFunction</name>
      <uri>https://old.reddit.com/user/PataFunction</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Honest question. I see the hype around R1, and I‚Äôve even downloaded and played with a couple distills myself. It‚Äôs definitely an achievement, if not for the models, then for the paper and detailed publication of the training methodology. No argument there.&lt;/p&gt; &lt;p&gt;However, I‚Äôm having difficulty understanding the mad rush to download and use these models. They are reasoning models, and as such, all they want to do is output long chains of thought full of /think tokens to solve a problem, even if the problem is simple, e.g. 2+2. As such, my assumption is they aren‚Äôt meant to be used for quick daily interactions like GPT-4o and company, but rather only to solve complex problems.&lt;/p&gt; &lt;p&gt;So I ask, what are you actually doing with R1 (other than toy ‚Äúhow many R‚Äôs in strawberry‚Äù reasoning problems) that you were previously doing with other models? What value have they added to your daily workload? I‚Äôm honestly curious, as maybe I have a misconception about their utility.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PataFunction"&gt; /u/PataFunction &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idgrh4/what_are_you_actually_using_r1_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idgrh4/what_are_you_actually_using_r1_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idgrh4/what_are_you_actually_using_r1_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T06:35:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1id5179</id>
    <title>R1 is now on Azure AI serverless. Great news if you have Azure startup credits to burn</title>
    <updated>2025-01-29T21:23:36+00:00</updated>
    <author>
      <name>/u/mesmerlord</name>
      <uri>https://old.reddit.com/user/mesmerlord</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id5179/r1_is_now_on_azure_ai_serverless_great_news_if/"&gt; &lt;img alt="R1 is now on Azure AI serverless. Great news if you have Azure startup credits to burn" src="https://preview.redd.it/u9e4zggf20ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3934cf599e32057ef689487414da48ae9ac5687" title="R1 is now on Azure AI serverless. Great news if you have Azure startup credits to burn" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mesmerlord"&gt; /u/mesmerlord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u9e4zggf20ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id5179/r1_is_now_on_azure_ai_serverless_great_news_if/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1id5179/r1_is_now_on_azure_ai_serverless_great_news_if/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T21:23:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1id7a3k</id>
    <title>I feel bad for the AI lol after seeing its chain of thought. üò≠</title>
    <updated>2025-01-29T22:58:30+00:00</updated>
    <author>
      <name>/u/Tricky_Reflection_75</name>
      <uri>https://old.reddit.com/user/Tricky_Reflection_75</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id7a3k/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt; &lt;img alt="I feel bad for the AI lol after seeing its chain of thought. üò≠" src="https://b.thumbs.redditmedia.com/Z88vfN16w6NY9yx3hsyBi8c5yYLLA29C35wbeMSZgfQ.jpg" title="I feel bad for the AI lol after seeing its chain of thought. üò≠" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/u4cm9r5oj0ge1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7cf76aa8fcfaa023df271504b53ea217f4208528"&gt;https://preview.redd.it/u4cm9r5oj0ge1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7cf76aa8fcfaa023df271504b53ea217f4208528&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tricky_Reflection_75"&gt; /u/Tricky_Reflection_75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id7a3k/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id7a3k/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1id7a3k/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T22:58:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1idny6j</id>
    <title>mistralai/Mistral-Small-24B-Instruct-2501</title>
    <updated>2025-01-30T14:18:01+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"&gt;https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501&lt;/a&gt;&lt;/p&gt; &lt;p&gt;its show time folks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idny6j/mistralaimistralsmall24binstruct2501/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idny6j/mistralaimistralsmall24binstruct2501/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idny6j/mistralaimistralsmall24binstruct2501/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:18:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1idro19</id>
    <title>DeepSeek R1 scores between o1 and o1-mini on NYT Connections</title>
    <updated>2025-01-30T17:02:39+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idro19/deepseek_r1_scores_between_o1_and_o1mini_on_nyt/"&gt; &lt;img alt="DeepSeek R1 scores between o1 and o1-mini on NYT Connections" src="https://preview.redd.it/e8ov1yb3x5ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=663523084a040c4c952820d45759a6a4e7a87469" title="DeepSeek R1 scores between o1 and o1-mini on NYT Connections" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e8ov1yb3x5ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idro19/deepseek_r1_scores_between_o1_and_o1mini_on_nyt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idro19/deepseek_r1_scores_between_o1_and_o1mini_on_nyt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T17:02:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1idqql6</id>
    <title>Mistral Small 3 24b's Context Window is Remarkably Efficient</title>
    <updated>2025-01-30T16:23:25+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idqql6/mistral_small_3_24bs_context_window_is_remarkably/"&gt; &lt;img alt="Mistral Small 3 24b's Context Window is Remarkably Efficient" src="https://b.thumbs.redditmedia.com/tUYsJoEn9u94ym2whVhsPOc7Lcfh9qD4M48XkP1073Y.jpg" title="Mistral Small 3 24b's Context Window is Remarkably Efficient" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using the Mistral Small 3 24b-q6k model with a full 32K context (Q8 KV cache), and I still have 1.6GB of VRAM left.&lt;br /&gt; In comparison, Qwen2.5 32b Q4 KL is roughly the same size, but I could only manage to get 24K context before getting dangerously close to running out of VRAM.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/adg5weajp5ge1.png?width=913&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bb3e935191ccc7000f402ce10e2c880ddb309730"&gt;https://preview.redd.it/adg5weajp5ge1.png?width=913&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bb3e935191ccc7000f402ce10e2c880ddb309730&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idqql6/mistral_small_3_24bs_context_window_is_remarkably/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idqql6/mistral_small_3_24bs_context_window_is_remarkably/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idqql6/mistral_small_3_24bs_context_window_is_remarkably/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T16:23:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1id2poe</id>
    <title>"DeepSeek produced a model close to the performance of US models 7-10 months older, for a good deal less cost (but NOT anywhere near the ratios people have suggested)" says Anthropic's CEO</title>
    <updated>2025-01-29T19:46:32+00:00</updated>
    <author>
      <name>/u/siegevjorn</name>
      <uri>https://old.reddit.com/user/siegevjorn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id2poe/deepseek_produced_a_model_close_to_the/"&gt; &lt;img alt="&amp;quot;DeepSeek produced a model close to the performance of US models 7-10 months older, for a good deal less cost (but NOT anywhere near the ratios people have suggested)&amp;quot; says Anthropic's CEO" src="https://external-preview.redd.it/33CmrJWIyiH-IL_JOc7gY-avdl30Pd-oQB-Pun7s774.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6512765018eb834d1f7c5898ca5a6e6f6fd0af6e" title="&amp;quot;DeepSeek produced a model close to the performance of US models 7-10 months older, for a good deal less cost (but NOT anywhere near the ratios people have suggested)&amp;quot; says Anthropic's CEO" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic's CEO has a word about DeepSeek. &lt;/p&gt; &lt;p&gt;Here are some of his statements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&amp;quot;Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;3.5 Sonnet did not involve a larger or more expensive model&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&amp;quot;Sonnet's training was conducted 9-12 months ago, while Sonnet remains notably ahead of DeepSeek in many internal and external evals. &amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;DeepSeek's cost efficiency is x8 compared to Sonnet, which is much less than the &amp;quot;original GPT-4 to Claude 3.5 Sonnet inference price differential (10x).&amp;quot; Yet 3.5 Sonnet is a better model than GPT-4, while DeepSeek is not.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;TL;DR: Although DeepSeekV3 was a real deal, but such innovation has been achieved regularly by U.S. AI companies. DeepSeek had enough resources to make it happen. /s&lt;/p&gt; &lt;p&gt;I guess an important distinction, that the Anthorpic CEO refuses to recognize, is the fact that DeepSeekV3 it open weight. In his mind, it is U.S. vs China. It appears that he doesn't give a fuck about local LLMs. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/siegevjorn"&gt; /u/siegevjorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/01/29/anthropics-ceo-says-deepseek-shows-that-u-s-export-rules-are-working-as-intended/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id2poe/deepseek_produced_a_model_close_to_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1id2poe/deepseek_produced_a_model_close_to_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T19:46:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1idp6n6</id>
    <title>Deepseek is hosted on Huawei cloud</title>
    <updated>2025-01-30T15:15:46+00:00</updated>
    <author>
      <name>/u/Reasonable-Climate66</name>
      <uri>https://old.reddit.com/user/Reasonable-Climate66</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on the IP resolved in China. The chat endpoints is from Huawei DC&lt;/p&gt; &lt;p&gt;DS could be using Singapore Huawei region for WW and Shanghai region for CN users.&lt;/p&gt; &lt;p&gt;So demand for Nvidia card for training and Huawei GPU for inference is real. &lt;/p&gt; &lt;p&gt;&lt;a href="https://i.postimg.cc/0QyjxTkh/Screenshot-20250130-230756.png"&gt;https://i.postimg.cc/0QyjxTkh/Screenshot-20250130-230756.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.postimg.cc/FHknCz0B/Screenshot-20250130-230812.png"&gt;https://i.postimg.cc/FHknCz0B/Screenshot-20250130-230812.png&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable-Climate66"&gt; /u/Reasonable-Climate66 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idp6n6/deepseek_is_hosted_on_huawei_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idp6n6/deepseek_is_hosted_on_huawei_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idp6n6/deepseek_is_hosted_on_huawei_cloud/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T15:15:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ideaxu</id>
    <title>Nvidia cuts FP8 training performance in half on RTX 40 and 50 series GPUs</title>
    <updated>2025-01-30T04:22:34+00:00</updated>
    <author>
      <name>/u/Emergency-Map9861</name>
      <uri>https://old.reddit.com/user/Emergency-Map9861</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ideaxu/nvidia_cuts_fp8_training_performance_in_half_on/"&gt; &lt;img alt="Nvidia cuts FP8 training performance in half on RTX 40 and 50 series GPUs" src="https://b.thumbs.redditmedia.com/SlGpr_siDY7Rr_nl1h9FbbkgpwtHXQX47AlZAVKy8LM.jpg" title="Nvidia cuts FP8 training performance in half on RTX 40 and 50 series GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to their new RTX Blackwell GPU architecture whitepaper, Nvidia appears to have cut FP8 training performance in half on RTX 40 and 50 series GPUs after DeepSeek successfully trained their SOTA V3 and R1 models using FP8. &lt;/p&gt; &lt;p&gt;In their original Ada Lovelace whitepaper, table 2 in Appendix A shows the 4090 having &lt;strong&gt;660.6 TFlops&lt;/strong&gt; of FP8 with FP32 accumulate without sparsity, which is the same as FP8 with FP16 accumulate. The new Blackwell paper shows half the performance for the 4090 at just &lt;strong&gt;330.3 TFlops&lt;/strong&gt; of FP8 with FP32 accumulate, and the 5090 has just &lt;strong&gt;419 TFlops&lt;/strong&gt; vs &lt;strong&gt;838 TFlops&lt;/strong&gt; for FP8 with FP16 accumulate. &lt;/p&gt; &lt;p&gt;FP32 accumulate is a must when it comes to training because FP16 doesn't have the necessary precision and dynamic range required. &lt;/p&gt; &lt;p&gt;If this isn't a mistake, then it means Nvidia lobotomized their Geforce lineup to further dissuade us from using them for AI/ML training, and it could potentially be reversible for the RTX 40 series at least, as this was likely done through a driver update.&lt;/p&gt; &lt;p&gt;This is quite unfortunate but not unexpected as Nvidia has a known history of artificially limiting Geforce GPUs for AI training since the Turing architecture, while their Quadro and datacenter GPUs continue to have the full performance.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x3qfea1352ge1.jpg?width=2007&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6c20a53057eb2bf15bbf65db4900af638fef9955"&gt;https://preview.redd.it/x3qfea1352ge1.jpg?width=2007&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6c20a53057eb2bf15bbf65db4900af638fef9955&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lk3ch91352ge1.jpg?width=1934&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d267c0312fe0be00175e616512101dce69113134"&gt;https://preview.redd.it/lk3ch91352ge1.jpg?width=1934&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d267c0312fe0be00175e616512101dce69113134&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sources:&lt;/p&gt; &lt;p&gt;RTX Blackwell GPU Architecture Whitepaper:&lt;/p&gt; &lt;p&gt;&lt;a href="https://images.nvidia.com/aem-dam/Solutions/geforce/blackwell/nvidia-rtx-blackwell-gpu-architecture.pdf"&gt;https://images.nvidia.com/aem-dam/Solutions/geforce/blackwell/nvidia-rtx-blackwell-gpu-architecture.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;RTX Ada Lovelace GPU Architecture Whitepaper:&lt;/p&gt; &lt;p&gt;&lt;a href="https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf"&gt;https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency-Map9861"&gt; /u/Emergency-Map9861 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ideaxu/nvidia_cuts_fp8_training_performance_in_half_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ideaxu/nvidia_cuts_fp8_training_performance_in_half_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ideaxu/nvidia_cuts_fp8_training_performance_in_half_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T04:22:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ido2up</id>
    <title>Mistral Small</title>
    <updated>2025-01-30T14:24:15+00:00</updated>
    <author>
      <name>/u/MLTyrunt</name>
      <uri>https://old.reddit.com/user/MLTyrunt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral Small&lt;/p&gt; &lt;p&gt;Apache 2.0, 81% MMLU, 150 tokens/s&lt;/p&gt; &lt;p&gt;&lt;a href="https://mistral.ai/news/mistral-small-3/"&gt;https://mistral.ai/news/mistral-small-3/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLTyrunt"&gt; /u/MLTyrunt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ido2up/mistral_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ido2up/mistral_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ido2up/mistral_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:24:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1idokcx</id>
    <title>Mistral new open models</title>
    <updated>2025-01-30T14:47:21+00:00</updated>
    <author>
      <name>/u/konilse</name>
      <uri>https://old.reddit.com/user/konilse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idokcx/mistral_new_open_models/"&gt; &lt;img alt="Mistral new open models" src="https://preview.redd.it/5nnsoy4295ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d39024b2c7d0acbb55e2f3d01eee2b120c949e0" title="Mistral new open models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral base and instruct 24B &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/konilse"&gt; /u/konilse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5nnsoy4295ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idokcx/mistral_new_open_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idokcx/mistral_new_open_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:47:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1idseqb</id>
    <title>DeepSeek R1 671B over 2 tok/sec *without* GPU on local gaming rig!</title>
    <updated>2025-01-30T17:33:04+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't rush out and buy that 5090TI just yet (if you can even find one lol)!&lt;/p&gt; &lt;p&gt;I just inferenced ~2.13 tok/sec with 2k context using a dynamic quant of the full R1 671B model (not a distill) after &lt;em&gt;disabling&lt;/em&gt; my 3090TI GPU on a 96GB RAM gaming rig. The secret trick is to &lt;em&gt;not&lt;/em&gt; load anything but kv cache into RAM and let &lt;code&gt;llama.cpp&lt;/code&gt; use its default behavior to &lt;code&gt;mmap()&lt;/code&gt; the model files off of a fast NVMe SSD. The rest of your system RAM acts as disk cache for the active weights.&lt;/p&gt; &lt;p&gt;Yesterday a bunch of folks got the dynamic quant flavors of &lt;code&gt;unsloth/DeepSeek-R1-GGUF&lt;/code&gt; running on gaming rigs in another thread here. I myself got the &lt;code&gt;DeepSeek-R1-UD-Q2_K_XL&lt;/code&gt; flavor going between 1~2 toks/sec and 2k~16k context on 96GB RAM + 24GB VRAM experimenting with context length and up to 8 concurrent slots inferencing for increased aggregate throuput.&lt;/p&gt; &lt;p&gt;After experimenting with various setups, the bottle neck is clearly my Gen 5 x4 NVMe SSD card as the CPU doesn't go over ~30%, the GPU was basically idle, and the power supply fan doesn't even come on. So while slow, it isn't heating up the room.&lt;/p&gt; &lt;p&gt;So instead of a $2k GPU what about $1.5k for 4x NVMe SSDs on an expansion card for 2TB &amp;quot;VRAM&amp;quot; giving theoretical max sequential read &amp;quot;memory&amp;quot; bandwidth of ~48GB/s? This less expensive setup would likely give better price/performance for big MoEs on home rigs. If you forgo a GPU, you could have 16 lanes of PCIe 5.0 all for NVMe drives on gamer class motherboards.&lt;/p&gt; &lt;p&gt;If anyone has a fast read IOPs drive array, I'd love to hear what kind of speeds you can get. I gotta bug Wendell over at Level1Techs lol...&lt;/p&gt; &lt;p&gt;P.S. In my opinion this quantized R1 671B beats the pants off any of the distill model toys. While slow and limited in context, it is still likely the best thing available for home users for many applications.&lt;/p&gt; &lt;p&gt;Just need to figure out how to short circuit the &lt;code&gt;&amp;lt;think&amp;gt;Blah blah&amp;lt;/think&amp;gt;&lt;/code&gt; stuff by injecting a &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; into the assistant prompt to see if it gives decent results without all the yapping haha...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T17:33:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1idp1z5</id>
    <title>No synthetic data?</title>
    <updated>2025-01-30T15:09:51+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idp1z5/no_synthetic_data/"&gt; &lt;img alt="No synthetic data?" src="https://preview.redd.it/98dq1wg2d5ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=448fe61c33c8db28d89becf7c1d0ccbcf95ea88a" title="No synthetic data?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That's reallllllly rare in 2025, did I understand this correctly? They didn't use any synthetic data to train this model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/98dq1wg2d5ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idp1z5/no_synthetic_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idp1z5/no_synthetic_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T15:09:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1idnyhh</id>
    <title>mistralai/Mistral-Small-24B-Base-2501 ¬∑ Hugging Face</title>
    <updated>2025-01-30T14:18:23+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idnyhh/mistralaimistralsmall24bbase2501_hugging_face/"&gt; &lt;img alt="mistralai/Mistral-Small-24B-Base-2501 ¬∑ Hugging Face" src="https://external-preview.redd.it/lDGKmq6pSZNpISh4piV15abwPTUoM5lDEjjJ9qZ_vd4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56053b8ce77cd587b1abeda9737783c65c0ebab8" title="mistralai/Mistral-Small-24B-Base-2501 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idnyhh/mistralaimistralsmall24bbase2501_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idnyhh/mistralaimistralsmall24bbase2501_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:18:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ido3fn</id>
    <title>Are there ¬Ω million people capable of running locally 685B params models?</title>
    <updated>2025-01-30T14:25:02+00:00</updated>
    <author>
      <name>/u/S1M0N38</name>
      <uri>https://old.reddit.com/user/S1M0N38</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ido3fn/are_there_¬Ω_million_people_capable_of_running/"&gt; &lt;img alt="Are there ¬Ω million people capable of running locally 685B params models?" src="https://b.thumbs.redditmedia.com/nUAmR_7owY5oJQcrzV0vL3H93-ccvgV-SDlaKg3CSyw.jpg" title="Are there ¬Ω million people capable of running locally 685B params models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/S1M0N38"&gt; /u/S1M0N38 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ido3fn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ido3fn/are_there_¬Ω_million_people_capable_of_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ido3fn/are_there_¬Ω_million_people_capable_of_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:25:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1idny3w</id>
    <title>Mistral Small 3</title>
    <updated>2025-01-30T14:17:56+00:00</updated>
    <author>
      <name>/u/khubebk</name>
      <uri>https://old.reddit.com/user/khubebk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idny3w/mistral_small_3/"&gt; &lt;img alt="Mistral Small 3" src="https://preview.redd.it/kj3s0jvr35ge1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0317aadc49155a8df1074618844c589ea3d2753d" title="Mistral Small 3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/khubebk"&gt; /u/khubebk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kj3s0jvr35ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idny3w/mistral_small_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idny3w/mistral_small_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:17:56+00:00</published>
  </entry>
</feed>
