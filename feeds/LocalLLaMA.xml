<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-11T07:06:06+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kj6vlj</id>
    <title>Why is adding search functionality so hard?</title>
    <updated>2025-05-10T10:09:34+00:00</updated>
    <author>
      <name>/u/iswasdoes</name>
      <uri>https://old.reddit.com/user/iswasdoes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I installed LM studio and loaded the qwen32b model easily, very impressive to have local reasoning&lt;/p&gt; &lt;p&gt;However not having web search really limits the functionality. I‚Äôve tried to add it using ChatGPT to guide me, and it‚Äôs had me creating JSON config files and getting various api tokens etc, but nothing seems to work.&lt;/p&gt; &lt;p&gt;My question is why is this seemingly obvious feature so far out of reach?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iswasdoes"&gt; /u/iswasdoes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj6vlj/why_is_adding_search_functionality_so_hard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj6vlj/why_is_adding_search_functionality_so_hard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj6vlj/why_is_adding_search_functionality_so_hard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T10:09:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjgyzp</id>
    <title>Best backend for the qwen3 moe models</title>
    <updated>2025-05-10T18:31:02+00:00</updated>
    <author>
      <name>/u/Noxusequal</name>
      <uri>https://old.reddit.com/user/Noxusequal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello I just half heared that there are a bunch of backend solutions by now that focus on moe and greatly help improve their performance when you have to split CPU gpu. I want to set up a small inference maschine for my family thinking about qwen3 30b moe. I am aware that it is light on compute anyway but I was wondering if there are any backend that help to optimize it further ? &lt;/p&gt; &lt;p&gt;Looking for something running a 3060 and a bunch of ram on a xeon platform with quad channel memory and idk 128-256gb of ram. I want to serve up to 4 concurrent users and have them be able to use decent context size idk 16-32k&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Noxusequal"&gt; /u/Noxusequal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjgyzp/best_backend_for_the_qwen3_moe_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjgyzp/best_backend_for_the_qwen3_moe_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjgyzp/best_backend_for_the_qwen3_moe_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T18:31:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjuud2</id>
    <title>Please help with model advice</title>
    <updated>2025-05-11T06:52:51+00:00</updated>
    <author>
      <name>/u/Universal_Cognition</name>
      <uri>https://old.reddit.com/user/Universal_Cognition</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've asked a few questions about hardware and received some good input, for which I thank those who helped me. Now I need some direction for which model(s) to start messing with.&lt;/p&gt; &lt;p&gt;My end goal is to have a model that has STT &amp;amp; TTS capability (I'll be building or modding speakers to interact with it) either natively or through add-on capability, and can also use the STT to interact with my Home Assistant so my smart home can be controlled completely locally. The use case would mostly include inference, but with some generative tasks as well, and smart home control. I currently have two Arc B580 gpus at my disposal, so I need something that can work with Intel and be loaded on 24gb of vram.&lt;/p&gt; &lt;p&gt;What model(s) would fit those requirements? I don't mind messing with different models, and ultimately I probably will on a separate box, but I want to start my journey going in a direction that gets me closer to my end goal.&lt;/p&gt; &lt;p&gt;TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Universal_Cognition"&gt; /u/Universal_Cognition &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjuud2/please_help_with_model_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjuud2/please_help_with_model_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjuud2/please_help_with_model_advice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T06:52:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj23yk</id>
    <title>An LLM + a selfhosted self engine looks like black magic</title>
    <updated>2025-05-10T04:40:38+00:00</updated>
    <author>
      <name>/u/marsxyz</name>
      <uri>https://old.reddit.com/user/marsxyz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj23yk/an_llm_a_selfhosted_self_engine_looks_like_black/"&gt; &lt;img alt="An LLM + a selfhosted self engine looks like black magic" src="https://external-preview.redd.it/HSJh1Glwn1cudWqMdR7v0csb93OcXPxZ1DJssuHJXOM.png?width=140&amp;amp;height=55&amp;amp;crop=140:55,smart&amp;amp;auto=webp&amp;amp;s=89b685fdf649d30b4a8df013cf2beef219f7fc0d" title="An LLM + a selfhosted self engine looks like black magic" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EDIT: I of course meant search engine.&lt;/p&gt; &lt;p&gt;In its last update, open-webui added support for Yacy as a search provider. Yacy is an open source, distributed search engine that does not rely on a central index but rely on distributed peers indexing pages themselves. I already tried Yacy in the past but the problem is that the algorithm that sorts the results is garbage and it is not really usable as a search engine. Of course a small open source software that can run on literally anything (the server it ran on for this experiment is a 12th gen Celeron with 8GB of RAM) cannot compete in term of the intelligence of the algorithm to sort the results with companies like Google or Microsoft. It was practically unusable.&lt;/p&gt; &lt;p&gt;Or It Was ! Coupled with an LLM, the LLM can sort the trash results from Yacy out and keep what is useful ! For the purpose of this exercise I used Deepseek-V3-0324 from OpenRouter but it is trivial to use local models !&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zcq88bwjvvze1.png?width=2492&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8e7c5c36e0f1770fab88f7baed53cd25e1014d07"&gt;https://preview.redd.it/zcq88bwjvvze1.png?width=2492&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8e7c5c36e0f1770fab88f7baed53cd25e1014d07&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That means that we can now have selfhosted AI models that learn from the Web ... without relying on Google or any central entity at all !&lt;/p&gt; &lt;p&gt;Some caveats; 1. Of course this is inferior to using google or even duckduckgo, I just wanted to share that here because I think you'll find it cool. 2. You need a solid CPU to have a lot of concurrent research, my Celeron gets hammered to 100% usage at each query. (open-webui and a bunch of other services are running on this server, that must not help). That's not your average LocalLLama rig costing my yearly salary ahah.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7q2mkkshvvze1.png?width=554&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56b75972d9a1e4e98c7cdfe111dad47b7f87cbeb"&gt;https://preview.redd.it/7q2mkkshvvze1.png?width=554&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56b75972d9a1e4e98c7cdfe111dad47b7f87cbeb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marsxyz"&gt; /u/marsxyz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj23yk/an_llm_a_selfhosted_self_engine_looks_like_black/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj23yk/an_llm_a_selfhosted_self_engine_looks_like_black/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj23yk/an_llm_a_selfhosted_self_engine_looks_like_black/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T04:40:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjsnqr</id>
    <title>Laptop help - lenovo or asus?</title>
    <updated>2025-05-11T04:28:22+00:00</updated>
    <author>
      <name>/u/AfraidScheme433</name>
      <uri>https://old.reddit.com/user/AfraidScheme433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Need your expertise! Looking for laptop recommendations for my younger brother to run LLMs offline (think airport/national parks).&lt;/p&gt; &lt;p&gt;I'm considering two options:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lenovo Legion Pro 7i:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; CPU: Intel Ultra 9 275HX&lt;/li&gt; &lt;li&gt; GPU: RTX 5070 Ti 12GB&lt;/li&gt; &lt;li&gt; RAM: Upgraded to 64GB (can run Qwen3-4B or DeepSeek-R1-Distill-Qwen-7B smoothly)&lt;/li&gt; &lt;li&gt; Storage: 1TB SSD Price: ~$3200&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ASUS Scar 18:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; CPU: Ultra 9 275HX&lt;/li&gt; &lt;li&gt; GPU: RTX 5090&lt;/li&gt; &lt;li&gt; RAM: 64GB&lt;/li&gt; &lt;li&gt; Storage: 4TB SSD RAID 0 Price: ~$3500+&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Based on my research, the Legion Pro 7i seems like the best value. The upgraded RAM should allow it to run the models he needs smoothly.&lt;/p&gt; &lt;p&gt;If you or anyone you know runs LLMs locally on a laptop, what computer &amp;amp; specs do you use? What would you change about your setup?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AfraidScheme433"&gt; /u/AfraidScheme433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjsnqr/laptop_help_lenovo_or_asus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjsnqr/laptop_help_lenovo_or_asus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjsnqr/laptop_help_lenovo_or_asus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T04:28:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj2j6q</id>
    <title>Seed-Coder 8B</title>
    <updated>2025-05-10T05:07:09+00:00</updated>
    <author>
      <name>/u/lly0571</name>
      <uri>https://old.reddit.com/user/lly0571</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj2j6q/seedcoder_8b/"&gt; &lt;img alt="Seed-Coder 8B" src="https://external-preview.redd.it/qN4W2OErTr-fXyFZh4FVGoCZMT9K6nHi3_DvqJJHr5c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bcd7116e2911f655490d68be32d15c7b0a893b6" title="Seed-Coder 8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bytedance has released a new 8B code-specific model that outperforms both Qwen3-8B and Qwen2.5-Coder-7B-Inst. I am curious about the performance of its base model in code FIM tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wbtmpay50wze1.jpg?width=8348&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b7e6bb5d9a152ed6594e5683f582f9d5f9fb81d9"&gt;https://preview.redd.it/wbtmpay50wze1.jpg?width=8348&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b7e6bb5d9a152ed6594e5683f582f9d5f9fb81d9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ByteDance-Seed/Seed-Coder"&gt;github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-Coder-8B-Instruct"&gt;HF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-Coder-8B-Base"&gt;Base Model HF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lly0571"&gt; /u/lly0571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj2j6q/seedcoder_8b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj2j6q/seedcoder_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj2j6q/seedcoder_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T05:07:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj89gq</id>
    <title>ManaBench: A Novel Reasoning Benchmark Based on MTG Deck Building</title>
    <updated>2025-05-10T11:40:13+00:00</updated>
    <author>
      <name>/u/Jake-Boggs</name>
      <uri>https://old.reddit.com/user/Jake-Boggs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj89gq/manabench_a_novel_reasoning_benchmark_based_on/"&gt; &lt;img alt="ManaBench: A Novel Reasoning Benchmark Based on MTG Deck Building" src="https://external-preview.redd.it/z_Ta6BgN-0E4xjWqloxN8S0IMfl-GG_lbgHPaHjOU5s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9fd95a896e5e0a2b1e6245ca34120093288ab9d" title="ManaBench: A Novel Reasoning Benchmark Based on MTG Deck Building" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm excited to share a new benchmark I've developed called &lt;strong&gt;ManaBench&lt;/strong&gt;, which tests LLM reasoning abilities using Magic: The Gathering deck building as a proxy.&lt;/p&gt; &lt;h1&gt;What is ManaBench?&lt;/h1&gt; &lt;p&gt;ManaBench evaluates an LLM's ability to reason about complex systems by presenting a simple but challenging task: given a 59-card MTG deck, select the most suitable 60th card from six options.&lt;/p&gt; &lt;p&gt;This isn't about memorizing card knowledge - all the necessary information (full card text and rules) is provided in the prompt. It's about reasoning through complex interactions, understanding strategic coherence, and making optimal choices within constraints.&lt;/p&gt; &lt;h1&gt;Why it's a good benchmark:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Strategic reasoning&lt;/strong&gt;: Requires understanding deck synergies, mana curves, and card interactions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;System optimization&lt;/strong&gt;: Tests ability to optimize within resource constraints&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Expert-aligned&lt;/strong&gt;: The &amp;quot;correct&amp;quot; answer is the card that was actually in the human-designed tournament deck&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hard to game&lt;/strong&gt;: Large labs are unlikely to optimize for this task and the questions are private&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Results for Local Models vs Cloud Models&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/adlxg53bxxze1.png?width=1065&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=39c1fe2aff1b4a5906b11bbd112d1bc53706b544"&gt;ManaBench Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Looking at these results, several interesting patterns emerge:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Llama models underperform expectations&lt;/strong&gt;: Despite their strong showing on many standard benchmarks, Llama 3.3 70B scored only 19.5% (just above random guessing at 16.67%), and Llama 4 Maverick hit only 26.5%&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Closed models dominate&lt;/strong&gt;: o3 leads the pack at 63%, followed by Claude 3.7 Sonnet at 49.5%&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance correlates with but differentiates better than LMArena scores&lt;/strong&gt;: Notice how the spread between models is much wider on ManaBench&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b3zyiwuoxxze1.png?width=814&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21d07b7fdad90b4fe3eb16b860f14617b3872fa0"&gt;ManaBench vs LMArena&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What This Means for Local Model Users&lt;/h1&gt; &lt;p&gt;If you're running models locally and working on tasks that require complex reasoning (like game strategy, system design, or multi-step planning), these results suggest that current open models may struggle more than benchmarks like MATH or LMArena would indicate.&lt;/p&gt; &lt;p&gt;This isn't to say local models aren't valuable - they absolutely are! But it's useful to understand their relative strengths and limitations compared to cloud alternatives.&lt;/p&gt; &lt;h1&gt;Looking Forward&lt;/h1&gt; &lt;p&gt;I'm curious if these findings match your experiences. The current leaderboard aligns very well with my results using many of these models personally.&lt;/p&gt; &lt;p&gt;For those interested in the technical details, my &lt;a href="https://boggs.tech/posts/evaluating-llm-reasoning-with-mtg-deck-building/"&gt;full writeup&lt;/a&gt; goes deeper into the methodology and analysis.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Note: The specific benchmark questions are not being publicly released to prevent contamination of future training data. If you are a researcher and would like access, please reach out.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jake-Boggs"&gt; /u/Jake-Boggs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj89gq/manabench_a_novel_reasoning_benchmark_based_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj89gq/manabench_a_novel_reasoning_benchmark_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj89gq/manabench_a_novel_reasoning_benchmark_based_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T11:40:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjo54e</id>
    <title>RVC to XTTS? Returning user</title>
    <updated>2025-05-11T00:11:04+00:00</updated>
    <author>
      <name>/u/santovalentino</name>
      <uri>https://old.reddit.com/user/santovalentino</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few years ago, I made a lot of audio with RVC. Cloned my own voice to sing on my favorite pop songs was one fun project. &lt;/p&gt; &lt;p&gt;Well I have a PC again. Using a 50 series isn't going well for me. New Cuda architecture isn't popular yet. Stable Diffusion is a pain with some features like Insightface/Onnx but some generous users provided forks etc..&lt;/p&gt; &lt;p&gt;Just installed SillyTavern with Kobold (ooba wouldn't work with non piper models) and it's really fun to chat with an AI assistant. &lt;/p&gt; &lt;p&gt;Now, I see RVC is kind of outdated and noticed that XTTS v2 is the new thing. But I could be wrong. What is the latest open source voice cloning technique? Especially one that runs on 12.8 nightly for my 5070! &lt;/p&gt; &lt;p&gt;TLDR: took a long break. RVC is now outdated. What's the new cloning program everyone is using for singer replacement and cloning? &lt;/p&gt; &lt;p&gt;Edit #1 - Applio updated its coding for 50 series. Cards. Using that as my new RVC. Need to find a TTS connection that integrates with ST&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/santovalentino"&gt; /u/santovalentino &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjo54e/rvc_to_xtts_returning_user/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjo54e/rvc_to_xtts_returning_user/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjo54e/rvc_to_xtts_returning_user/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T00:11:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjts8s</id>
    <title>Why is decoder architecture used for text generation according to a prompt rather than encoder-decoder architecture?</title>
    <updated>2025-05-11T05:40:51+00:00</updated>
    <author>
      <name>/u/darkGrayAdventurer</name>
      <uri>https://old.reddit.com/user/darkGrayAdventurer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Learning about LLMs for the first time, and this question is bothering me, I haven't been able to find an answer that intuitively makes sense. &lt;/p&gt; &lt;p&gt;To my understanding, encoder-decoder architectures are good for understanding the text that has been provided in a thorough manner (encoder architecture) as well as for building off of given text (decoder architecture). Using decoder-only will detract from the model's ability to gain a thorough understanding of what is being asked of it -- something that is achieved when using an encoder. &lt;/p&gt; &lt;p&gt;So, why aren't encoder-decoder architectures popular for LLMs when they are used for other common tasks, such as translation and summarization of input texts?&lt;/p&gt; &lt;p&gt;Thank you!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkGrayAdventurer"&gt; /u/darkGrayAdventurer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjts8s/why_is_decoder_architecture_used_for_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjts8s/why_is_decoder_architecture_used_for_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjts8s/why_is_decoder_architecture_used_for_text/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T05:40:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjkmzl</id>
    <title>Generating MP3 from epubs (local)?</title>
    <updated>2025-05-10T21:19:06+00:00</updated>
    <author>
      <name>/u/Affectionate-Bus4123</name>
      <uri>https://old.reddit.com/user/Affectionate-Bus4123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love listening to stories via text to speech on my android phone. It hits Google's generous APIs but I don't think that's available on a linux PC.&lt;/p&gt; &lt;p&gt;Ideally, I'd like to bulk convert an epub into a set of MP3s to listen to later...&lt;/p&gt; &lt;p&gt;There seems to have been a lot of progress on local audio models, and I'm not looking for perfection.&lt;/p&gt; &lt;p&gt;Based on your experiments with local audio models, which one would be best for generating not annoying, not too robotic audio from text? Doesn't need to be real time, doesn't need to be tiny.&lt;/p&gt; &lt;p&gt;Note - asking about models not tools - although if you have a solution already that would be lovely I'm really looking for an underlying model. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Affectionate-Bus4123"&gt; /u/Affectionate-Bus4123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjkmzl/generating_mp3_from_epubs_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjkmzl/generating_mp3_from_epubs_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjkmzl/generating_mp3_from_epubs_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T21:19:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjaf6b</id>
    <title>128GB DDR4, 2950x CPU, 1x3090 24gb Qwen3-235B-A22B-UD-Q3_K_XL 7Tokens/s</title>
    <updated>2025-05-10T13:33:51+00:00</updated>
    <author>
      <name>/u/ciprianveg</name>
      <uri>https://old.reddit.com/user/ciprianveg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share, maybe it helps others with only 24gb vram, this is what i had to send to ram to use almost all my 24gb. If you have suggestions for increasing the prompt processing, please suggest :) I get cca. 12tok/s.&lt;br /&gt; This is the experssion used: -ot &amp;quot;blk\.(?:[7-9]|[1-9][0-8])\.ffn.*=CPU&amp;quot;&lt;br /&gt; and this is my whole command:&lt;br /&gt; ./llama-cli -m ~/ai/models/unsloth_Qwen3-235B-A22B-UD-Q3_K_XL-GGUF/Qwen3-235B-A22B-UD-Q3_K_XL-00001-of-00003.gguf -ot &amp;quot;blk\.(?:[7-9]|[1-9][0-8])\.ffn.*=CPU&amp;quot; -c 16384 -n 16384 --prio 2 --threads 20 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 --color -if -ngl 99 -fa&lt;br /&gt; My DDR4 runs at 2933MT/s and the cpu is an AMD 2950x&lt;/p&gt; &lt;p&gt;L. E. --threads 15 as suggested below for my 16 cores cpu changed it to 7.5 tokens/sec and 12.3t/s for prompt processing&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ciprianveg"&gt; /u/ciprianveg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjaf6b/128gb_ddr4_2950x_cpu_1x3090_24gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjaf6b/128gb_ddr4_2950x_cpu_1x3090_24gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjaf6b/128gb_ddr4_2950x_cpu_1x3090_24gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T13:33:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjshnd</id>
    <title>Is it possible to generate my own dynamic quant?</title>
    <updated>2025-05-11T04:17:56+00:00</updated>
    <author>
      <name>/u/Lissanro</name>
      <uri>https://old.reddit.com/user/Lissanro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dynamic quants by unsloth are quite good, but they are not available for every model. For example, DeepSeek R1T Chimera has only one Q4_K_M quant (by bullerwins on huggingface) but it fails many tests like solving mazes or have lesser success rate than my own Q6_K quant that I generated locally, which can consistently solve the maze. So I know it is quant issue and not a model issue. Usually failure to solve the maze indicates too much quantization or that it wasn't done perfectly. Unsloth's old R1 quant at Q4_K_M level did not have such issue, and dynamic quants are supposed to be even better. This is why I am interested in learning from their experience creating quants.&lt;/p&gt; &lt;p&gt;I am currently trying to figure out the best way to generate similar high quality Q4 for the Chimera model, so I would like to ask was creation of Dynamic Quants documented somewhere?&lt;/p&gt; &lt;p&gt;I tried searching but I did not find an answer, hence I would like to ask here in the hope someone knows. If it wasn't documented yet, I probably will try experimenting myself with existing Q4 and IQ4 quantization methods and see what gives me the best result.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lissanro"&gt; /u/Lissanro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjshnd/is_it_possible_to_generate_my_own_dynamic_quant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjshnd/is_it_possible_to_generate_my_own_dynamic_quant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjshnd/is_it_possible_to_generate_my_own_dynamic_quant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T04:17:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjtd88</id>
    <title>Any news on INTELLECT-2?</title>
    <updated>2025-05-11T05:13:17+00:00</updated>
    <author>
      <name>/u/Amon_star</name>
      <uri>https://old.reddit.com/user/Amon_star</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They finished the training, does anyone know when the model will be published?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amon_star"&gt; /u/Amon_star &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjtd88/any_news_on_intellect2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjtd88/any_news_on_intellect2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjtd88/any_news_on_intellect2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T05:13:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjgvm8</id>
    <title>For such a small model, Qwen 3 8b is excellent! With 2 short prompts it made a playable HTML keyboard for me! This is the Q6_K Quant.</title>
    <updated>2025-05-10T18:26:48+00:00</updated>
    <author>
      <name>/u/c64z86</name>
      <uri>https://old.reddit.com/user/c64z86</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjgvm8/for_such_a_small_model_qwen_3_8b_is_excellent/"&gt; &lt;img alt="For such a small model, Qwen 3 8b is excellent! With 2 short prompts it made a playable HTML keyboard for me! This is the Q6_K Quant." src="https://external-preview.redd.it/yC8tUfmmqcWrWbYfoMczWi5TEVmr7UUYVX_Lfvwk9qM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ba7786134790ce98dbb7a8be7c736175837add8" title="For such a small model, Qwen 3 8b is excellent! With 2 short prompts it made a playable HTML keyboard for me! This is the Q6_K Quant." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/c64z86"&gt; /u/c64z86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=Jda1Z40Xcfs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjgvm8/for_such_a_small_model_qwen_3_8b_is_excellent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjgvm8/for_such_a_small_model_qwen_3_8b_is_excellent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T18:26:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjb9zs</id>
    <title>Using llama.cpp-vulkan on an AMD GPU? You can finally use FlashAttention!</title>
    <updated>2025-05-10T14:14:50+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It might be a year late, but Vulkan FA implementation was merged into llama.cpp just a few hours ago. It works! And I'm happy to double the context size thanks to Q8 KV Cache quantization.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: Might've found an issue. I get the following error when some layers are loaded on system RAM, rather than 100% GPU offloading: &lt;code&gt;swapState() Unexpected current state starting, expected stopped&lt;/code&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjb9zs/using_llamacppvulkan_on_an_amd_gpu_you_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjb9zs/using_llamacppvulkan_on_an_amd_gpu_you_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjb9zs/using_llamacppvulkan_on_an_amd_gpu_you_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T14:14:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjoc3n</id>
    <title>Is there a specific reason thinking models don't seem to exist in the (or near) 70b parameter range?</title>
    <updated>2025-05-11T00:21:28+00:00</updated>
    <author>
      <name>/u/wh33t</name>
      <uri>https://old.reddit.com/user/wh33t</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems 30b or less or 200b+. Am I missing something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wh33t"&gt; /u/wh33t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjoc3n/is_there_a_specific_reason_thinking_models_dont/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjoc3n/is_there_a_specific_reason_thinking_models_dont/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjoc3n/is_there_a_specific_reason_thinking_models_dont/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T00:21:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj7l8p</id>
    <title>AMD eGPU over USB3 for Apple Silicon by Tiny Corp</title>
    <updated>2025-05-10T10:58:23+00:00</updated>
    <author>
      <name>/u/zdy132</name>
      <uri>https://old.reddit.com/user/zdy132</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj7l8p/amd_egpu_over_usb3_for_apple_silicon_by_tiny_corp/"&gt; &lt;img alt="AMD eGPU over USB3 for Apple Silicon by Tiny Corp" src="https://external-preview.redd.it/2BON-N6TCd_ctm0tqr4moZr228fviTa5r-AUavBUN3Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de7825d238122dad4a6420788d2290a151b8da31" title="AMD eGPU over USB3 for Apple Silicon by Tiny Corp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zdy132"&gt; /u/zdy132 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/__tinygrad__/status/1920960070055080107"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj7l8p/amd_egpu_over_usb3_for_apple_silicon_by_tiny_corp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj7l8p/amd_egpu_over_usb3_for_apple_silicon_by_tiny_corp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T10:58:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjd8tg</id>
    <title>Absolute_Zero_Reasoner-Coder-14b / 7b / 3b</title>
    <updated>2025-05-10T15:44:01+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjd8tg/absolute_zero_reasonercoder14b_7b_3b/"&gt; &lt;img alt="Absolute_Zero_Reasoner-Coder-14b / 7b / 3b" src="https://external-preview.redd.it/c2vPUFXKhvD_gZRfZocGG6ne7L_maCxsQIvkq5lx_Ec.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c6db7591f54489669f2ba77fd228c4034fbc9225" title="Absolute_Zero_Reasoner-Coder-14b / 7b / 3b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/andrewzh/absolute-zero-reasoner-68139b2bca82afb00bc69e5b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjd8tg/absolute_zero_reasonercoder14b_7b_3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjd8tg/absolute_zero_reasonercoder14b_7b_3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T15:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjilvd</id>
    <title>AMD's "Strix Halo" APUs Are Being Apparently Sold Separately In China; Starting From $550</title>
    <updated>2025-05-10T19:45:57+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjilvd/amds_strix_halo_apus_are_being_apparently_sold/"&gt; &lt;img alt="AMD's &amp;quot;Strix Halo&amp;quot; APUs Are Being Apparently Sold Separately In China; Starting From $550" src="https://external-preview.redd.it/H2PilUsCPrB61jYHu-ehMJ7ez-2xqBlQGK2jAXQtDYs.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=17801eef3d0bc54803f8d5da3d4b1af4af3e68ce" title="AMD's &amp;quot;Strix Halo&amp;quot; APUs Are Being Apparently Sold Separately In China; Starting From $550" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-strix-halo-apus-are-being-sold-separately-in-china/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjilvd/amds_strix_halo_apus_are_being_apparently_sold/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjilvd/amds_strix_halo_apus_are_being_apparently_sold/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T19:45:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjoyrc</id>
    <title>How about this Ollama Chat portal?</title>
    <updated>2025-05-11T00:56:14+00:00</updated>
    <author>
      <name>/u/Ordinary_Mud7430</name>
      <uri>https://old.reddit.com/user/Ordinary_Mud7430</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjoyrc/how_about_this_ollama_chat_portal/"&gt; &lt;img alt="How about this Ollama Chat portal?" src="https://preview.redd.it/0iyghlhuw10f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=595fe97e58b00087e5706293c48dd73242bd16f9" title="How about this Ollama Chat portal?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Greetings everyone, I'm sharing a modern web chat interface for local LLMs, inspired by the visual style and user experience of Claude from Anthropic. It is super easy to use. Supports *.txt file upload, conversation history and Systemas Prompts. &lt;/p&gt; &lt;p&gt;You can play all you want with this üòÖ&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Oft3r/Ollama-Chat"&gt;https://github.com/Oft3r/Ollama-Chat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ordinary_Mud7430"&gt; /u/Ordinary_Mud7430 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0iyghlhuw10f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjoyrc/how_about_this_ollama_chat_portal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjoyrc/how_about_this_ollama_chat_portal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T00:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kju0ty</id>
    <title>Why new models feel dumber?</title>
    <updated>2025-05-11T05:57:09+00:00</updated>
    <author>
      <name>/u/SrData</name>
      <uri>https://old.reddit.com/user/SrData</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it just me, or do the new models feel‚Ä¶ dumber?&lt;/p&gt; &lt;p&gt;I‚Äôve been testing Qwen 3 across different sizes, expecting a leap forward. Instead, I keep circling back to Qwen 2.5. It just feels sharper, more coherent, less‚Ä¶ bloated. Same story with Llama. I‚Äôve had long, surprisingly good conversations with 3.1. But 3.3? Or Llama 4? It‚Äôs like the lights are on but no one‚Äôs home.&lt;/p&gt; &lt;p&gt;Some flaws I have found: They lose thread persistence. They forget earlier parts of the convo. They repeat themselves more. Worse, they feel like they‚Äôre trying to sound smarter instead of being coherent.&lt;/p&gt; &lt;p&gt;So I‚Äôm curious: Are you seeing this too? Which models are you sticking with, despite the version bump? Any new ones that have genuinely impressed you, especially in longer sessions?&lt;/p&gt; &lt;p&gt;Because right now, it feels like we‚Äôre in this strange loop of releasing ‚Äúsmarter‚Äù models that somehow forget how to talk. And I‚Äôd love to know I‚Äôm not the only one noticing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SrData"&gt; /u/SrData &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kju0ty/why_new_models_feel_dumber/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kju0ty/why_new_models_feel_dumber/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kju0ty/why_new_models_feel_dumber/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T05:57:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjigz3</id>
    <title>What happened to Black Forest Labs?</title>
    <updated>2025-05-10T19:39:31+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;theyve been totally silent since november of last year with the release of flux tools and remember when flux 1 first came out they teased that a video generation model was coming soon? what happened with that? Same with stability AI, do they do anything anymore?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjigz3/what_happened_to_black_forest_labs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjigz3/what_happened_to_black_forest_labs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjigz3/what_happened_to_black_forest_labs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T19:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjlq7g</id>
    <title>I am GPU poor.</title>
    <updated>2025-05-10T22:09:28+00:00</updated>
    <author>
      <name>/u/Khipu28</name>
      <uri>https://old.reddit.com/user/Khipu28</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjlq7g/i_am_gpu_poor/"&gt; &lt;img alt="I am GPU poor." src="https://preview.redd.it/o61lr9f3310f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71a4af796f1b72d4787c4fcfbebb5815223c9d7a" title="I am GPU poor." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently, I am very GPU poor. How many GPUs of what type can I fit into this available space of the Jonsbo N5 case? All the slots are 5.0x16 the leftmost two slots have re-timers on board. I can provide 1000W for the cards.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Khipu28"&gt; /u/Khipu28 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o61lr9f3310f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjlq7g/i_am_gpu_poor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjlq7g/i_am_gpu_poor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T22:09:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kju1y1</id>
    <title>Unsloth's Qwen3 GGUFs are updated with a new improved calibration dataset</title>
    <updated>2025-05-11T05:59:20+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kju1y1/unsloths_qwen3_ggufs_are_updated_with_a_new/"&gt; &lt;img alt="Unsloth's Qwen3 GGUFs are updated with a new improved calibration dataset" src="https://external-preview.redd.it/8ePyWxYJavtNkgThp-DI68bW9d5fj-oFIybzu4pnoUM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7b2715032a28656454c9bee39e79aafee721d37" title="Unsloth's Qwen3 GGUFs are updated with a new improved calibration dataset" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-128K-GGUF/discussions/3#681edd400153e42b1c7168e9"&gt;https://huggingface.co/unsloth/Qwen3-30B-A3B-128K-GGUF/discussions/3#681edd400153e42b1c7168e9&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We've uploaded them all now&lt;/p&gt; &lt;p&gt;Also with a new improved calibration dataset :)&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/51rr8j7qd30f1.png?width=362&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7e0b8891020518424f286d35814501b87cbd9cc0"&gt;https://preview.redd.it/51rr8j7qd30f1.png?width=362&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7e0b8891020518424f286d35814501b87cbd9cc0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They updated All Qwen3 ggufs&lt;/p&gt; &lt;p&gt;Plus more gguf variants for Qwen3-30B-A3B&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ckx6zfn0e30f1.png?width=397&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3dde922fd59d02d5223680a6d584758387bdc476"&gt;https://preview.redd.it/ckx6zfn0e30f1.png?width=397&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3dde922fd59d02d5223680a6d584758387bdc476&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/models?sort=modified&amp;amp;search=unsloth+qwen3+gguf"&gt;https://huggingface.co/models?sort=modified&amp;amp;search=unsloth+qwen3+gguf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kju1y1/unsloths_qwen3_ggufs_are_updated_with_a_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kju1y1/unsloths_qwen3_ggufs_are_updated_with_a_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kju1y1/unsloths_qwen3_ggufs_are_updated_with_a_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T05:59:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjn2wv</id>
    <title>Cheap 48GB official Blackwell yay!</title>
    <updated>2025-05-10T23:16:22+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjn2wv/cheap_48gb_official_blackwell_yay/"&gt; &lt;img alt="Cheap 48GB official Blackwell yay!" src="https://external-preview.redd.it/sC0_RV1rBP5Nka4zzrlrlknHQcvT_QUrChxq3hP_lVg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e745729c3f7132892c715292c6b31f385f223e8f" title="Cheap 48GB official Blackwell yay!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.nvidia.com/en-us/products/workstations/professional-desktop-gpus/rtx-pro-5000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjn2wv/cheap_48gb_official_blackwell_yay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjn2wv/cheap_48gb_official_blackwell_yay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T23:16:22+00:00</published>
  </entry>
</feed>
