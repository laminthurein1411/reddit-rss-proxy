<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-14T07:34:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1javx8d</id>
    <title>What is the best LLM based OCR open source available now?</title>
    <updated>2025-03-14T04:36:39+00:00</updated>
    <author>
      <name>/u/seeker_deeplearner</name>
      <uri>https://old.reddit.com/user/seeker_deeplearner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to deploy a local LLM based OCR for reading thorugh my docs and then putting it into a vector DB. Mistral OCR is making news but I cannot deploy it locally yet. Any recommendations?&lt;/p&gt; &lt;p&gt;i have 48gb vram. will be getting additional 48gb soon. I couldnt make it run to connect to vllm. if somehow i can covert that into ollama model. then life would be so much easier for me. Any help regarding that? I can rent a H100 cluster for a few hours to convert it. or can i just request it from someone.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seeker_deeplearner"&gt; /u/seeker_deeplearner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1javx8d/what_is_the_best_llm_based_ocr_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1javx8d/what_is_the_best_llm_based_ocr_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1javx8d/what_is_the_best_llm_based_ocr_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T04:36:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ja2ers</id>
    <title>The duality of man</title>
    <updated>2025-03-13T03:00:08+00:00</updated>
    <author>
      <name>/u/jhanjeek</name>
      <uri>https://old.reddit.com/user/jhanjeek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja2ers/the_duality_of_man/"&gt; &lt;img alt="The duality of man" src="https://preview.redd.it/1ukvrj06hdoe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=392789bc2b1887610312054c20af823db4ab6078" title="The duality of man" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jhanjeek"&gt; /u/jhanjeek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1ukvrj06hdoe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja2ers/the_duality_of_man/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ja2ers/the_duality_of_man/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T03:00:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaftxa</id>
    <title>DeepHermes - a NousResearch Collection</title>
    <updated>2025-03-13T16:19:02+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaftxa/deephermes_a_nousresearch_collection/"&gt; &lt;img alt="DeepHermes - a NousResearch Collection" src="https://external-preview.redd.it/1IlQBWESV8QV8X9vVxfshuMcnWyH3f4Z3MrqfDWjQNw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=994f341d0495a385f29282ceb0cf8ce537d0f6df" title="DeepHermes - a NousResearch Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/NousResearch/deephermes-67d2ff8c9246cc09a7bd8add"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaftxa/deephermes_a_nousresearch_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaftxa/deephermes_a_nousresearch_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T16:19:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaxcla</id>
    <title>Speculative Decoding Not Useful On Apple Silicon?</title>
    <updated>2025-03-14T06:14:31+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm wondering why I'm only seeing very little speed improvement using speculative decoding with llama.cpp on an M3 Max. I only get about a 2% increase—my test below shows just a 5-second improvement (from 4:18 to 4:13).&lt;/p&gt; &lt;p&gt;Also, speculative decoding seems to require significantly more memory. If I don't set --batch to match --context-size, it crashes. Without speculative decoding, I can run with 32k context, but with it, I'm limited to around 10k.&lt;/p&gt; &lt;p&gt;Is speculative decoding just not effective on Mac, or am I doing something wrong?&lt;/p&gt; &lt;p&gt;Here's my log for the test.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;time ./llama.cpp/build/bin/llama-cli -m ./models/bartowski/Llama-3.3-70B-Instruct-Q4_K_M.gguf --ctx-size 10000 --n-predict 2000 --temp 0.0 --top_p 0.9 --seed 1000 --flash-attn -no-cnv --file prompt-test/steps/8013.txt llama_perf_sampler_print: sampling time = 40.56 ms / 8958 runs ( 0.00 ms per token, 220868.88 tokens per second) llama_perf_context_print: load time = 1310.40 ms llama_perf_context_print: prompt eval time = 124793.12 ms / 8013 tokens ( 15.57 ms per token, 64.21 tokens per second) llama_perf_context_print: eval time = 131607.76 ms / 944 runs ( 139.42 ms per token, 7.17 tokens per second) llama_perf_context_print: total time = 256578.30 ms / 8957 tokens ggml_metal_free: deallocating ./llama.cpp/build/bin/llama-cli -m --ctx-size 10000 --n-predict 2000 --temp 1.29s user 1.22s system 0% cpu 4:17.98 total time ./llama.cpp/build/bin/llama-speculative -m ./models/bartowski/Llama-3.3-70B-Instruct-Q4_K_M.gguf -md ./models/bartowski/Llama-3.2-3B-Instruct-Q4_K_M.gguf --ctx-size 10000 -b 10000 --n-predict 2000 --temp 0.0 --top_p 0.9 --seed 1000 --flash-attn --draft-max 8 --draft-min 1 --file prompt-test/steps/8013.txt encoded 8013 tokens in 130.314 seconds, speed: 61.490 t/s decoded 912 tokens in 120.857 seconds, speed: 7.546 t/s n_draft = 8 n_predict = 912 n_drafted = 1320 n_accept = 746 accept = 56.515% draft: llama_perf_context_print: load time = 318.02 ms llama_perf_context_print: prompt eval time = 112632.33 ms / 8342 tokens ( 13.50 ms per token, 74.06 tokens per second) llama_perf_context_print: eval time = 13570.99 ms / 1155 runs ( 11.75 ms per token, 85.11 tokens per second) llama_perf_context_print: total time = 251179.59 ms / 9497 tokens target: llama_perf_sampler_print: sampling time = 39.52 ms / 912 runs ( 0.04 ms per token, 23078.09 tokens per second) llama_perf_context_print: load time = 1313.45 ms llama_perf_context_print: prompt eval time = 233357.84 ms / 9498 tokens ( 24.57 ms per token, 40.70 tokens per second) llama_perf_context_print: eval time = 0.00 ms / 1 runs ( 0.00 ms per token, inf tokens per second) llama_perf_context_print: total time = 251497.67 ms / 9499 tokens ggml_metal_free: deallocating ggml_metal_free: deallocating ./llama.cpp/build/bin/llama-speculative -m -md --ctx-size 10000 -b 10000 1.51s user 1.32s system 1% cpu 4:12.95 total &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaxcla/speculative_decoding_not_useful_on_apple_silicon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaxcla/speculative_decoding_not_useful_on_apple_silicon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaxcla/speculative_decoding_not_useful_on_apple_silicon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T06:14:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jacyqt</id>
    <title>Check out the new theme of my open sourced desktop app, you can run LLMs locally with built-in RAG knowledge base and note-taking capabilities.</title>
    <updated>2025-03-13T14:15:51+00:00</updated>
    <author>
      <name>/u/w-zhong</name>
      <uri>https://old.reddit.com/user/w-zhong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jacyqt/check_out_the_new_theme_of_my_open_sourced/"&gt; &lt;img alt="Check out the new theme of my open sourced desktop app, you can run LLMs locally with built-in RAG knowledge base and note-taking capabilities." src="https://preview.redd.it/kzrghtnotgoe1.gif?width=640&amp;amp;crop=smart&amp;amp;s=464ed1f234a1c459c1c1cc69de42594f8ce42dd7" title="Check out the new theme of my open sourced desktop app, you can run LLMs locally with built-in RAG knowledge base and note-taking capabilities." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w-zhong"&gt; /u/w-zhong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kzrghtnotgoe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jacyqt/check_out_the_new_theme_of_my_open_sourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jacyqt/check_out_the_new_theme_of_my_open_sourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T14:15:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jatq0e</id>
    <title>Transformers without Normalization</title>
    <updated>2025-03-14T02:33:06+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.10622"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jatq0e/transformers_without_normalization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jatq0e/transformers_without_normalization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T02:33:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaje3r</id>
    <title>SoftWhisper update – Transcribe 2 hours in 2 minutes!</title>
    <updated>2025-03-13T18:44:31+00:00</updated>
    <author>
      <name>/u/Substantial_Swan_144</name>
      <uri>https://old.reddit.com/user/Substantial_Swan_144</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaje3r/softwhisper_update_transcribe_2_hours_in_2_minutes/"&gt; &lt;img alt="SoftWhisper update – Transcribe 2 hours in 2 minutes!" src="https://external-preview.redd.it/SIM5YSKyDgEjqYlw7LlJtYqDH0a_HqoimuzQ3YBsYqM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ca1b81fb8c954c5523b93a2e91ddae911cbd7f6" title="SoftWhisper update – Transcribe 2 hours in 2 minutes!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After a long wait, a new release of SoftWhisper, your frontend to the Whisper API, is out! &lt;strong&gt;And what is best, NO MORE PYTORCH DEPENDENCIES! Now it's just install and run.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;[ Github link: &lt;a href="https://github.com/NullMagic2/SoftWhisper"&gt;https://github.com/NullMagic2/SoftWhisper&lt;/a&gt; ]&lt;/p&gt; &lt;p&gt;The changes to the frontend are minimal, but in the backend they are quite drastic. The dependencies on Pytorch made this program much more complicated to install and run to the average user than they should – which is why I decided to remove them!&lt;/p&gt; &lt;p&gt;Originally, I would use the original OpenAI AI + ZLUDA, but unfortunately Pytorch support is not quite there yet. So I decided to use Whisper.cpp as a backend. And this proved to be a good decision: now, we can transcribe 2 hours of video in around 2-3 minutes!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/75ueh1bk4ioe1.png?width=2008&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e39af5204135b1cf82c6bba2d37fe5c61762e7ce"&gt;https://preview.redd.it/75ueh1bk4ioe1.png?width=2008&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e39af5204135b1cf82c6bba2d37fe5c61762e7ce&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Installation steps:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Windows users:&lt;/strong&gt; just click on &lt;code&gt;SoftWhisper.bat&lt;/code&gt;. The script will check if any dependencies are missing and will attempt installing them for you. If that fails or you prefer the old method, just run pip install -r requirements.txt under the console.&lt;/p&gt; &lt;p&gt;If you use Windows, I have already provided a prebuilt release of Whisper.cpp as a backend with Vulkan support, so no extra steps are necessary: just download SoftWhisper and run it with:&lt;/p&gt; &lt;p&gt;For now, a Linux script is missing, but you can still run pip as usual and run the program the usual way, with &lt;code&gt;python SoftWhisper.py&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;python&lt;/strong&gt; &lt;a href="http://SoftWhisper.py"&gt;&lt;strong&gt;SoftWhisper.py&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unfortunately, I haven't tested this software under Linux. I do plan to provide a prebuilt static version of Whisper.cpp for Linux as well, but in the meantime, Linux users can compile Whisper.cpp themselves and add the executable at the field &amp;quot;Whisper.cpp executable.&amp;quot;&lt;/p&gt; &lt;p&gt;Please also note that I couldn't get speaker diarization working in this release, so I had to remove it. I might add it back in the future. However, considering the performance increase, it is a small price to pay.&lt;/p&gt; &lt;p&gt;Enjoy, and let me know if you have any questions.&lt;/p&gt; &lt;p&gt;[Link to the original release: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1fvncqc/comment/mh7t4z7/?context=3"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1fvncqc/comment/mh7t4z7/?context=3&lt;/a&gt; ]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Swan_144"&gt; /u/Substantial_Swan_144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaje3r/softwhisper_update_transcribe_2_hours_in_2_minutes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaje3r/softwhisper_update_transcribe_2_hours_in_2_minutes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaje3r/softwhisper_update_transcribe_2_hours_in_2_minutes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T18:44:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jajyz3</id>
    <title>Gemma 3 27B scores on four independent benchmarks: wide variation depending on the eval</title>
    <updated>2025-03-13T19:08:40+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jajyz3/gemma_3_27b_scores_on_four_independent_benchmarks/"&gt; &lt;img alt="Gemma 3 27B scores on four independent benchmarks: wide variation depending on the eval" src="https://a.thumbs.redditmedia.com/VpvpROyRpiHrwFqib5SAlcggW0k_M2aiS5pJIm9FZv4.jpg" title="Gemma 3 27B scores on four independent benchmarks: wide variation depending on the eval" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jajyz3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jajyz3/gemma_3_27b_scores_on_four_independent_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jajyz3/gemma_3_27b_scores_on_four_independent_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T19:08:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jabj70</id>
    <title>New model from Cohere: Command A!</title>
    <updated>2025-03-13T13:07:26+00:00</updated>
    <author>
      <name>/u/slimyXD</name>
      <uri>https://old.reddit.com/user/slimyXD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Command A is our new state-of-the-art addition to Command family optimized for demanding enterprises that require fast, secure, and high-quality models. &lt;/p&gt; &lt;p&gt;It offers maximum performance with minimal hardware costs when compared to leading proprietary and open-weights models, such as GPT-4o and DeepSeek-V3.&lt;/p&gt; &lt;p&gt;It features 111b, a 256k context window, with: * inference at a rate of up to 156 tokens/sec which is 1.75x higher than GPT-4o and 2.4x higher than DeepSeek-V3 * excelling performance on business-critical agentic and multilingual tasks * minimal hardware needs - its deployable on just two GPUs, compared to other models that typically require as many as 32&lt;/p&gt; &lt;p&gt;Check out our full report: &lt;a href="https://cohere.com/blog/command-a"&gt;https://cohere.com/blog/command-a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And the model card: &lt;a href="https://huggingface.co/CohereForAI/c4ai-command-a-03-2025"&gt;https://huggingface.co/CohereForAI/c4ai-command-a-03-2025&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's available to everyone now via Cohere API as &lt;code&gt;command-a-03-2025&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slimyXD"&gt; /u/slimyXD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabj70/new_model_from_cohere_command_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabj70/new_model_from_cohere_command_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jabj70/new_model_from_cohere_command_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T13:07:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jabh4m</id>
    <title>CohereForAI/c4ai-command-a-03-2025 · Hugging Face</title>
    <updated>2025-03-13T13:04:32+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabh4m/cohereforaic4aicommanda032025_hugging_face/"&gt; &lt;img alt="CohereForAI/c4ai-command-a-03-2025 · Hugging Face" src="https://external-preview.redd.it/8uC-fRvLBGmT6kbHk8wyBrnHknVe8WnRlviRost1iDM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=316e90209b2b29e953d13bdd363e048d518bc1d0" title="CohereForAI/c4ai-command-a-03-2025 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/CohereForAI/c4ai-command-a-03-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabh4m/cohereforaic4aicommanda032025_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jabh4m/cohereforaic4aicommanda032025_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T13:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaiux8</id>
    <title>The first Gemma3 finetune</title>
    <updated>2025-03-13T18:22:47+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a really nice formatted post, but for some reason locallama auto bans it, and only approves low effort posts. So here's the short version: a new Gemma3 tune is up.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Oni_Mitsubishi_12B"&gt;https://huggingface.co/SicariusSicariiStuff/Oni_Mitsubishi_12B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaiux8/the_first_gemma3_finetune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaiux8/the_first_gemma3_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaiux8/the_first_gemma3_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T18:22:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jag07t</id>
    <title>Nous Deephermes 24b and 3b are out !</title>
    <updated>2025-03-13T16:26:02+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;24b: &lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3b: &lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-3B-Preview"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-3B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Official gguf:&lt;/p&gt; &lt;p&gt;24b: &lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview-GGUF"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3b:&lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-3B-Preview-GGUF"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-3B-Preview-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jag07t/nous_deephermes_24b_and_3b_are_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jag07t/nous_deephermes_24b_and_3b_are_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jag07t/nous_deephermes_24b_and_3b_are_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T16:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaqpiu</id>
    <title>Mac Speed Comparison: M2 Ultra vs M3 Ultra using KoboldCpp</title>
    <updated>2025-03-14T00:01:14+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqpiu/mac_speed_comparison_m2_ultra_vs_m3_ultra_using/"&gt; &lt;img alt="Mac Speed Comparison: M2 Ultra vs M3 Ultra using KoboldCpp" src="https://a.thumbs.redditmedia.com/OTEjpiItOaaKB7vj3kGES8cMsYYXecIqD-eyJx7Qt88.jpg" title="Mac Speed Comparison: M2 Ultra vs M3 Ultra using KoboldCpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;: Running ggufs in Koboldcpp, the M3 is marginally... slower? Slightly faster prompt processing, but slower prompt writing across all models&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; &lt;em&gt;I added a comparison Llama.cpp run at the bottom; same speed as Kobold, give or take.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Setup:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Inference engine: Koboldcpp 1.85.1&lt;/li&gt; &lt;li&gt;Text: Same text on ALL models. Token size differences are due to tokenizer differences&lt;/li&gt; &lt;li&gt;Temp: 0.01; all other samplers disabled&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Computers:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;M3 Ultra 512GB 80 GPU Cores&lt;/li&gt; &lt;li&gt;M2 Ultra 192GB 76 GPU Cores&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jfhw63feojoe1.png?width=464&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=792fc47a7c5a6b51d619cefa7c6f83f31a4f438a"&gt;https://preview.redd.it/jfhw63feojoe1.png?width=464&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=792fc47a7c5a6b51d619cefa7c6f83f31a4f438a&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Notes:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Qwen2.5 Coder and Llama 3.1 8b are more sensitive to temp than Llama 3.3 70b&lt;/li&gt; &lt;li&gt;All inference was first prompt after model load&lt;/li&gt; &lt;li&gt;All models are q8, as on Mac q8 is the fastest gguf quant &lt;em&gt;(see my previous posts on Mac speeds)&lt;/em&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Llama 3.1 8b q8&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;M2 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:12433/32768, Amt:386/4000, Init:0.02s, Process:13.56s (1.1ms/T = 888.55T/s), Generate:14.41s (37.3ms/T = 26.79T/s), Total:27.96s (13.80T/s) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:12408/32768, Amt:361/4000, Init:0.01s, Process:12.05s (1.0ms/T = 999.75T/s), Generate:13.62s (37.7ms/T = 26.50T/s), Total:25.67s (14.06T/s) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Mistral Small 24b q8&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;M2 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:13300/32768, Amt:661/4000, Init:0.07s, Process:34.86s (2.8ms/T = 362.50T/s), Generate:45.43s (68.7ms/T = 14.55T/s), Total:80.29s (8.23T/s) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:13300/32768, Amt:661/4000, Init:0.04s, Process:31.97s (2.5ms/T = 395.28T/s), Generate:46.27s (70.0ms/T = 14.29T/s), Total:78.24s (8.45T/s) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Qwen2.5 32b Coder q8 with 1.5b speculative decoding&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;M2 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:13215/32768, Amt:473/4000, Init:0.06s, Process:59.38s (4.7ms/T = 214.59T/s), Generate:34.70s (73.4ms/T = 13.63T/s), Total:94.08s (5.03T/s) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:13271/32768, Amt:529/4000, Init:0.05s, Process:52.97s (4.2ms/T = 240.56T/s), Generate:43.58s (82.4ms/T = 12.14T/s), Total:96.55s (5.48T/s) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Qwen2.5 32b Coder q8 WITHOUT speculative decoding&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;M2 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:13315/32768, Amt:573/4000, Init:0.07s, Process:53.44s (4.2ms/T = 238.42T/s), Generate:64.77s (113.0ms/T = 8.85T/s), Total:118.21s (4.85T/s) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:13285/32768, Amt:543/4000, Init:0.04s, Process:49.35s (3.9ms/T = 258.22T/s), Generate:62.51s (115.1ms/T = 8.69T/s), Total:111.85s (4.85T/s) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Llama 3.3 70b q8 with 3b speculative decoding&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;M2 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:12519/32768, Amt:472/4000, Init:0.04s, Process:116.18s (9.6ms/T = 103.69T/s), Generate:54.99s (116.5ms/T = 8.58T/s), Total:171.18s (2.76T/s) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:12519/32768, Amt:472/4000, Init:0.02s, Process:103.12s (8.6ms/T = 116.77T/s), Generate:63.74s (135.0ms/T = 7.40T/s), Total:166.86s (2.83T/s) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Llama 3.3 70b q8 WITHOUT speculative decoding&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;M2 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:12519/32768, Amt:472/4000, Init:0.03s, Process:104.74s (8.7ms/T = 115.01T/s), Generate:98.15s (207.9ms/T = 4.81T/s), Total:202.89s (2.33T/s) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:12519/32768, Amt:472/4000, Init:0.01s, Process:96.67s (8.0ms/T = 124.62T/s), Generate:103.09s (218.4ms/T = 4.58T/s), Total:199.76s (2.36T/s) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;#####&lt;/p&gt; &lt;h1&gt;Llama.cpp Server Comparison Run :: Llama 3.3 70b q8 WITHOUT Speculative Decoding&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;M2 Ultra&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 105195.24 ms / 12051 tokens ( 8.73 ms per token, 114.56 tokens per second) eval time = 78102.11 ms / 377 tokens ( 207.17 ms per token, 4.83 tokens per second) total time = 183297.35 ms / 12428 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 96696.48 ms / 12051 tokens ( 8.02 ms per token, 124.63 tokens per second) eval time = 82026.89 ms / 377 tokens ( 217.58 ms per token, 4.60 tokens per second) total time = 178723.36 ms / 12428 tokens &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqpiu/mac_speed_comparison_m2_ultra_vs_m3_ultra_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqpiu/mac_speed_comparison_m2_ultra_vs_m3_ultra_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqpiu/mac_speed_comparison_m2_ultra_vs_m3_ultra_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T00:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1jal0yx</id>
    <title>There it is https://github.com/SesameAILabs/csm</title>
    <updated>2025-03-13T19:53:22+00:00</updated>
    <author>
      <name>/u/muxxington</name>
      <uri>https://old.reddit.com/user/muxxington</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...almost. Hugginface link is still 404ing. Let's wait some minutes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muxxington"&gt; /u/muxxington &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jal0yx/there_it_is_httpsgithubcomsesameailabscsm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jal0yx/there_it_is_httpsgithubcomsesameailabscsm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jal0yx/there_it_is_httpsgithubcomsesameailabscsm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T19:53:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jauy8d</id>
    <title>Giving "native" tool calling to Gemma 3 (or really any model)</title>
    <updated>2025-03-14T03:40:19+00:00</updated>
    <author>
      <name>/u/logkn</name>
      <uri>https://old.reddit.com/user/logkn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma 3 is great at following instructions, but doesn't have &amp;quot;native&amp;quot; tool/function calling. Let's change that (at least as best we can).&lt;/p&gt; &lt;p&gt;(Quick note, I'm going to be using Ollama as the example here, but this works equally well with Jinja templates, just need to change the syntax a bit.)&lt;/p&gt; &lt;h1&gt;Defining Tools&lt;/h1&gt; &lt;p&gt;Let's start by figuring out how 'native' function calling works in Ollama. Here's qwen2.5's chat template:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{{- if or .System .Tools }}&amp;lt;|im_start|&amp;gt;system {{- if .System }} {{ .System }} {{- end }} {{- if .Tools }} # Tools You may call one or more functions to assist with the user query. You are provided with function signatures within &amp;lt;tools&amp;gt;&amp;lt;/tools&amp;gt; XML tags: &amp;lt;tools&amp;gt; {{- range .Tools }} {&amp;quot;type&amp;quot;: &amp;quot;function&amp;quot;, &amp;quot;function&amp;quot;: {{ .Function }}} {{- end }} &amp;lt;/tools&amp;gt; For each function call, return a json object with function name and arguments within &amp;lt;tool_call&amp;gt;&amp;lt;/tool_call&amp;gt; XML tags: &amp;lt;tool_call&amp;gt; {&amp;quot;name&amp;quot;: &amp;lt;function-name&amp;gt;, &amp;quot;arguments&amp;quot;: &amp;lt;args-json-object&amp;gt;} &amp;lt;/tool_call&amp;gt; {{- end }}&amp;lt;|im_end|&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you think this looks like the second half of your average homebrew tool calling system prompt, you're spot on. &lt;strong&gt;This is literally appending markdown-formatted instructions on what tools are available and how to call them to the end of the system prompt.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Already, Ollama will recognize the tools you give it in the `tools` part of your OpenAI completions request, and inject them into the system prompt.&lt;/p&gt; &lt;h1&gt;Parsing Tools&lt;/h1&gt; &lt;p&gt;Let's scroll down a bit and see how tool call messages are handled:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{{ else if eq .Role &amp;quot;assistant&amp;quot; }}&amp;lt;|im_start|&amp;gt;assistant {{ if .Content }}{{ .Content }} {{- else if .ToolCalls }}&amp;lt;tool_call&amp;gt; {{ range .ToolCalls }}{&amp;quot;name&amp;quot;: &amp;quot;{{ .Function.Name }}&amp;quot;, &amp;quot;arguments&amp;quot;: {{ .Function.Arguments }}} {{ end }}&amp;lt;/tool_call&amp;gt; {{- end }}{{ if not $last }}&amp;lt;|im_end|&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is the &lt;strong&gt;tool call parser&lt;/strong&gt;. If the first token (or couple tokens) that the model outputs is &lt;code&gt;&amp;lt;tool_call&amp;gt;&lt;/code&gt;, Ollama handles the parsing of the tool calls. Assuming the model is decent at following instructions, &lt;em&gt;this means the tool calls will actually populate the&lt;/em&gt; &lt;code&gt;tool_calls&lt;/code&gt; &lt;em&gt;field rather than&lt;/em&gt; &lt;code&gt;content&lt;/code&gt;.&lt;/p&gt; &lt;h1&gt;Demonstration&lt;/h1&gt; &lt;p&gt;So just for gits and shiggles, let's see if we can get Gemma 3 to call tools properly. I adapted the same concepts from qwen2.5's chat template to Gemma 3's chat template. Before I show that template, let me show you that it works.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import ollama def add_two_numbers(a: int, b: int) -&amp;gt; int: &amp;quot;&amp;quot;&amp;quot; Add two numbers Args: a: The first integer number b: The second integer number Returns: int: The sum of the two numbers &amp;quot;&amp;quot;&amp;quot; return a + b response = ollama.chat( 'gemma3-tools', messages=[{'role': 'user', 'content': 'What is 10 + 10?'}], tools=[add_two_numbers], ) print(response) # model='gemma3-tools' created_at='2025-03-14T02:47:29.234101Z' # done=True done_reason='stop' total_duration=19211740040 # load_duration=8867467023 prompt_eval_count=79 # prompt_eval_duration=6591000000 eval_count=35 # eval_duration=3736000000 # message=Message(role='assistant', content='', images=None, # tool_calls=[ToolCall(function=Function(name='add_two_numbers', # arguments={'a': 10, 'b': 10}))]) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Booyah! Native function calling with Gemma 3.&lt;/p&gt; &lt;p&gt;It's not bullet-proof, mainly because it's not strictly enforcing a grammar. But assuming the model follows instructions, it should work *most* of the time.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Here's the template I used. It's very much like qwen2.5 in terms of the structure and logic, but using the tags of Gemma 3. Give it a shot, and better yet adapt this pattern to other models that you wish had tools.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;TEMPLATE &amp;quot;&amp;quot;&amp;quot;{{- if .Messages }} {{- if or .System .Tools }}&amp;lt;start_of_turn&amp;gt;user {{- if .System}} {{ .System }} {{- end }} {{- if .Tools }} # Tools You may call one or more functions to assist with the user query. You are provided with function signatures within &amp;lt;tools&amp;gt;&amp;lt;/tools&amp;gt; XML tags: &amp;lt;tools&amp;gt; {{- range $.Tools }} {&amp;quot;type&amp;quot;: &amp;quot;function&amp;quot;, &amp;quot;function&amp;quot;: {{ .Function }}} {{- end }} &amp;lt;/tools&amp;gt; For each function call, return a json object with function name and arguments within &amp;lt;tool_call&amp;gt;&amp;lt;/tool_call&amp;gt; XML tags: &amp;lt;tool_call&amp;gt; {&amp;quot;name&amp;quot;: &amp;lt;function-name&amp;gt;, &amp;quot;arguments&amp;quot;: &amp;lt;args-json-object&amp;gt;} &amp;lt;/tool_call&amp;gt; {{- end }}&amp;lt;end_of_turn&amp;gt; {{ end }} {{- range $i, $_ := .Messages }} {{- $last := eq (len (slice $.Messages $i)) 1 -}} {{- if eq .Role &amp;quot;user&amp;quot; }}&amp;lt;start_of_turn&amp;gt;user {{ .Content }}&amp;lt;end_of_turn&amp;gt; {{ else if eq .Role &amp;quot;assistant&amp;quot; }}&amp;lt;start_of_turn&amp;gt;model {{ if .Content }}{{ .Content }} {{- else if .ToolCalls }}&amp;lt;tool_call&amp;gt; {{ range .ToolCalls }}{&amp;quot;name&amp;quot;: &amp;quot;{{ .Function.Name }}&amp;quot;, &amp;quot;arguments&amp;quot;: {{ .Function.Arguments}}} {{ end }}&amp;lt;/tool_call&amp;gt; {{- end }}{{ if not $last }}&amp;lt;end_of_turn&amp;gt; {{ end }} {{- else if eq .Role &amp;quot;tool&amp;quot; }}&amp;lt;start_of_turn&amp;gt;user &amp;lt;tool_response&amp;gt; {{ .Content }} &amp;lt;/tool_response&amp;gt;&amp;lt;end_of_turn&amp;gt; {{ end }} {{- if and (ne .Role &amp;quot;assistant&amp;quot;) $last }}&amp;lt;start_of_turn&amp;gt;model {{ end }} {{- end }} {{- else }} {{- if .System }}&amp;lt;start_of_turn&amp;gt;user {{ .System }}&amp;lt;end_of_turn&amp;gt; {{ end }}{{ if .Prompt }}&amp;lt;start_of_turn&amp;gt;user {{ .Prompt }}&amp;lt;end_of_turn&amp;gt; {{ end }}&amp;lt;start_of_turn&amp;gt;model {{ end }}{{ .Response }}{{ if .Response }}&amp;lt;end_of_turn&amp;gt;{{ end }}&amp;quot;&amp;quot;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/logkn"&gt; /u/logkn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jauy8d/giving_native_tool_calling_to_gemma_3_or_really/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jauy8d/giving_native_tool_calling_to_gemma_3_or_really/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jauy8d/giving_native_tool_calling_to_gemma_3_or_really/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T03:40:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1janir5</id>
    <title>End of the Open LLM Leaderboard</title>
    <updated>2025-03-13T21:38:32+00:00</updated>
    <author>
      <name>/u/clefourrier</name>
      <uri>https://old.reddit.com/user/clefourrier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1janir5/end_of_the_open_llm_leaderboard/"&gt; &lt;img alt="End of the Open LLM Leaderboard" src="https://external-preview.redd.it/uNVcFTJjErtGHcv_RU8nJOqopdFi5HpQXXuBPYA8mRk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78ec61fc7857881ca74251621f68697e9ed6557a" title="End of the Open LLM Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clefourrier"&gt; /u/clefourrier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard/discussions/1135"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1janir5/end_of_the_open_llm_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1janir5/end_of_the_open_llm_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T21:38:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jabmwz</id>
    <title>AMA with the Gemma Team</title>
    <updated>2025-03-13T13:12:40+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLlama! During the next day, the Gemma research and product team from DeepMind will be around to answer with your questions! Looking forward to them!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Technical Report: &lt;a href="https://goo.gle/Gemma3Report"&gt;https://goo.gle/Gemma3Report&lt;/a&gt;&lt;/li&gt; &lt;li&gt;AI Studio: &lt;a href="https://aistudio.google.com/prompts/new_chat?model=gemma-3-27b-it"&gt;https://aistudio.google.com/prompts/new_chat?model=gemma-3-27b-it&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Technical blog post &lt;a href="https://developers.googleblog.com/en/introducing-gemma3/"&gt;https://developers.googleblog.com/en/introducing-gemma3/&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Kaggle &lt;a href="https://www.kaggle.com/models/google/gemma-3"&gt;https://www.kaggle.com/models/google/gemma-3&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Hugging Face &lt;a href="https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d"&gt;https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ollama &lt;a href="https://ollama.com/library/gemma3"&gt;https://ollama.com/library/gemma3&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabmwz/ama_with_the_gemma_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabmwz/ama_with_the_gemma_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jabmwz/ama_with_the_gemma_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T13:12:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jao3fg</id>
    <title>Qwq-32b just got updated Livebench.</title>
    <updated>2025-03-13T22:03:20+00:00</updated>
    <author>
      <name>/u/Amazing_Gate_9984</name>
      <uri>https://old.reddit.com/user/Amazing_Gate_9984</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jao3fg/qwq32b_just_got_updated_livebench/"&gt; &lt;img alt="Qwq-32b just got updated Livebench." src="https://b.thumbs.redditmedia.com/y-9o2Am61okZEVvbetInDUVF7Va9XDotcOFbj-bL_LI.jpg" title="Qwq-32b just got updated Livebench." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to the full results: &lt;a href="https://livebench.ai/#/"&gt;Livebench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wvsprzpa5joe1.jpg?width=766&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=198d4ef5ec0b493a9d57dae2a989bdb5039d9f29"&gt;https://preview.redd.it/wvsprzpa5joe1.jpg?width=766&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=198d4ef5ec0b493a9d57dae2a989bdb5039d9f29&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazing_Gate_9984"&gt; /u/Amazing_Gate_9984 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jao3fg/qwq32b_just_got_updated_livebench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jao3fg/qwq32b_just_got_updated_livebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jao3fg/qwq32b_just_got_updated_livebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T22:03:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaqylp</id>
    <title>LLM must pass a skill check to talk to me</title>
    <updated>2025-03-14T00:13:29+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqylp/llm_must_pass_a_skill_check_to_talk_to_me/"&gt; &lt;img alt="LLM must pass a skill check to talk to me" src="https://external-preview.redd.it/Y3U2cGt3NmNzam9lMRWrFMxzjmNxpTNLvYX1gtD81VUNlzdlH0AiqtOky6_L.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a45045d6f36cca8e45bf1337eccb796748268735" title="LLM must pass a skill check to talk to me" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/w7dney6csjoe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqylp/llm_must_pass_a_skill_check_to_talk_to_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqylp/llm_must_pass_a_skill_check_to_talk_to_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T00:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaoc8n</id>
    <title>QwQ on LiveBench (update) - is better than DeepSeek R1!</title>
    <updated>2025-03-13T22:14:11+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoc8n/qwq_on_livebench_update_is_better_than_deepseek_r1/"&gt; &lt;img alt="QwQ on LiveBench (update) - is better than DeepSeek R1!" src="https://preview.redd.it/sb78tt607joe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22464da14ee7a94ffe6b7ad76b52c34bab00a921" title="QwQ on LiveBench (update) - is better than DeepSeek R1!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sb78tt607joe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoc8n/qwq_on_livebench_update_is_better_than_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoc8n/qwq_on_livebench_update_is_better_than_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T22:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jahs0b</id>
    <title>OpenAI calls DeepSeek 'state-controlled,' calls for bans on 'PRC-produced' models | TechCrunch</title>
    <updated>2025-03-13T17:38:10+00:00</updated>
    <author>
      <name>/u/Qaxar</name>
      <uri>https://old.reddit.com/user/Qaxar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jahs0b/openai_calls_deepseek_statecontrolled_calls_for/"&gt; &lt;img alt="OpenAI calls DeepSeek 'state-controlled,' calls for bans on 'PRC-produced' models | TechCrunch" src="https://external-preview.redd.it/YuFnFIavAP98hFeGzOxLZQ1jrf6fXSzPC6RHQ4YO4ew.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2312e28fa573cb9d493e784a1275b4624e4c905" title="OpenAI calls DeepSeek 'state-controlled,' calls for bans on 'PRC-produced' models | TechCrunch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qaxar"&gt; /u/Qaxar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/03/13/openai-calls-deepseek-state-controlled-calls-for-bans-on-prc-produced-models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jahs0b/openai_calls_deepseek_statecontrolled_calls_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jahs0b/openai_calls_deepseek_statecontrolled_calls_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T17:38:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaxec3</id>
    <title>Sesame CSM 1B Voice Cloning</title>
    <updated>2025-03-14T06:18:15+00:00</updated>
    <author>
      <name>/u/Internal_Brain8420</name>
      <uri>https://old.reddit.com/user/Internal_Brain8420</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaxec3/sesame_csm_1b_voice_cloning/"&gt; &lt;img alt="Sesame CSM 1B Voice Cloning" src="https://external-preview.redd.it/JaEGat2-q67uEqWoPpGo5Nx0tvU4ZMhHe5tLQNQxW9w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c285788573980e7289358fbf97c0847d9b60866" title="Sesame CSM 1B Voice Cloning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Internal_Brain8420"&gt; /u/Internal_Brain8420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/isaiahbjork/csm-voice-cloning"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaxec3/sesame_csm_1b_voice_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaxec3/sesame_csm_1b_voice_cloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T06:18:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1janmn8</id>
    <title>SESAME IS HERE</title>
    <updated>2025-03-13T21:43:12+00:00</updated>
    <author>
      <name>/u/Straight-Worker-4327</name>
      <uri>https://old.reddit.com/user/Straight-Worker-4327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sesame just released their 1B CSM.&lt;br /&gt; Sadly parts of the pipeline are missing.&lt;/p&gt; &lt;p&gt;Try it here:&lt;br /&gt; &lt;a href="https://huggingface.co/spaces/sesame/csm-1b"&gt;https://huggingface.co/spaces/sesame/csm-1b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Installation steps here:&lt;br /&gt; &lt;a href="https://github.com/SesameAILabs/csm"&gt;https://github.com/SesameAILabs/csm&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Straight-Worker-4327"&gt; /u/Straight-Worker-4327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1janmn8/sesame_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1janmn8/sesame_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1janmn8/sesame_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T21:43:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaoy9g</id>
    <title>Meme i made</title>
    <updated>2025-03-13T22:41:19+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoy9g/meme_i_made/"&gt; &lt;img alt="Meme i made" src="https://external-preview.redd.it/a3h0bzNwMWxiam9lMWaOI-rE6YlXiP74zpe4ixbVM_QsxQQzHzv1tNet8B-Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aee2797aa64747b42b65d38774f6590f3d0a9e9d" title="Meme i made" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vzku6n1lbjoe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoy9g/meme_i_made/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoy9g/meme_i_made/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T22:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaj6gc</id>
    <title>AI2 releases OLMo 32B - Truly open source</title>
    <updated>2025-03-13T18:35:40+00:00</updated>
    <author>
      <name>/u/Initial-Image-1015</name>
      <uri>https://old.reddit.com/user/Initial-Image-1015</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaj6gc/ai2_releases_olmo_32b_truly_open_source/"&gt; &lt;img alt="AI2 releases OLMo 32B - Truly open source" src="https://preview.redd.it/4puob2w24ioe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebfe792d0c0462bf8dcf9f5a45f17815829f617d" title="AI2 releases OLMo 32B - Truly open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;&amp;quot;OLMo 2 32B: First fully open model to outperform GPT 3.5 and GPT 4o mini&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;OLMo is a fully open model: [they] release all artifacts. Training code, pre- &amp;amp; post-train data, model weights, and a recipe on how to reproduce it yourself.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Links: - &lt;a href="https://allenai.org/blog/olmo2-32B"&gt;https://allenai.org/blog/olmo2-32B&lt;/a&gt; - &lt;a href="https://x.com/natolambert/status/1900249099343192573"&gt;https://x.com/natolambert/status/1900249099343192573&lt;/a&gt; - &lt;a href="https://x.com/allen_ai/status/1900248895520903636"&gt;https://x.com/allen_ai/status/1900248895520903636&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Initial-Image-1015"&gt; /u/Initial-Image-1015 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4puob2w24ioe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaj6gc/ai2_releases_olmo_32b_truly_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaj6gc/ai2_releases_olmo_32b_truly_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T18:35:40+00:00</published>
  </entry>
</feed>
