<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-05T16:38:08+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1js0aju</id>
    <title>Training LLM on books</title>
    <updated>2025-04-05T10:16:42+00:00</updated>
    <author>
      <name>/u/tonyblu331</name>
      <uri>https://old.reddit.com/user/tonyblu331</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Best way to train a llm or fine-tune based on books. Like label and knowing to recall and what to say. I guess it sounds more like a RAG, but I want to be able to create essays and writings (Not based on the books author or copy them) but rather learn about what makes the good writing, how they structure it, label that data so the LLM learns and create based on the learnings of the books. &lt;/p&gt; &lt;p&gt;How would be the best way to approach this? Perhaps various agents one for rag and the other for streaming the chat and so on? Or given that now with Gemini we can get such a big context window we could just dump all in there (Even tho we can do that, it does sounds inneficient)&lt;/p&gt; &lt;p&gt;Perhaps my system prompt could be a long list of all the learnings + agent to decide which learning to apply for that question or request. But an excessively long system could hinder more than help. &lt;/p&gt; &lt;p&gt;Anyways, happy to read what the Local community has to say about. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tonyblu331"&gt; /u/tonyblu331 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js0aju/training_llm_on_books/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js0aju/training_llm_on_books/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js0aju/training_llm_on_books/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T10:16:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrn498</id>
    <title>Quasar Alpha (OpenAI open source model?) feels like a very solid model, but if its SOTA is not by much</title>
    <updated>2025-04-04T21:24:05+00:00</updated>
    <author>
      <name>/u/sirjoaco</name>
      <uri>https://old.reddit.com/user/sirjoaco</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrn498/quasar_alpha_openai_open_source_model_feels_like/"&gt; &lt;img alt="Quasar Alpha (OpenAI open source model?) feels like a very solid model, but if its SOTA is not by much" src="https://external-preview.redd.it/cWxoZTA2NTJ5dnNlMdPlOzjBh06Ls33QYFMnUJe9_SHp-HPYT5zE-TBYfw-C.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c930ee8076ed2509dc7d140019db522febf01c5d" title="Quasar Alpha (OpenAI open source model?) feels like a very solid model, but if its SOTA is not by much" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sirjoaco"&gt; /u/sirjoaco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vzi2qx62yvse1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrn498/quasar_alpha_openai_open_source_model_feels_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrn498/quasar_alpha_openai_open_source_model_feels_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T21:24:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1js5btx</id>
    <title>Larger context or Chunking? [ Rookie ]</title>
    <updated>2025-04-05T14:58:14+00:00</updated>
    <author>
      <name>/u/Foreign_Lead_3582</name>
      <uri>https://old.reddit.com/user/Foreign_Lead_3582</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, [I'm new to this world so I'll probably make rookie's mistakes]&lt;/p&gt; &lt;p&gt;I want to fine tune a model for retrieval, the documents I want it to 'learn' have different sizes (some are a dozen of lines, while others or m and they are in Italian. Those are legal texts so precision is a very important part of the result I'd like to obtain.&lt;/p&gt; &lt;p&gt;What technique should I use? I saw that two option in my case should be 'overlapping' and chunking, is there a better one in my case? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Foreign_Lead_3582"&gt; /u/Foreign_Lead_3582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js5btx/larger_context_or_chunking_rookie/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js5btx/larger_context_or_chunking_rookie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js5btx/larger_context_or_chunking_rookie/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T14:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1js6ywy</id>
    <title>I got a dual 3090... What the fuck do I do? if I run it max capacity (training) it will cost me 1-2k in electricity per year...</title>
    <updated>2025-04-05T16:12:53+00:00</updated>
    <author>
      <name>/u/Autumnlight_02</name>
      <uri>https://old.reddit.com/user/Autumnlight_02</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js6ywy/i_got_a_dual_3090_what_the_fuck_do_i_do_if_i_run/"&gt; &lt;img alt="I got a dual 3090... What the fuck do I do? if I run it max capacity (training) it will cost me 1-2k in electricity per year..." src="https://a.thumbs.redditmedia.com/506UIGjtOhJ01XeUpiApn-cnFe4qc8YZX-770lVB_b0.jpg" title="I got a dual 3090... What the fuck do I do? if I run it max capacity (training) it will cost me 1-2k in electricity per year..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/qb56t8fgj1te1.png?width=820&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f438dba2d9878d5d34915ab956e0166613a0013e"&gt;https://preview.redd.it/qb56t8fgj1te1.png?width=820&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f438dba2d9878d5d34915ab956e0166613a0013e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Autumnlight_02"&gt; /u/Autumnlight_02 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js6ywy/i_got_a_dual_3090_what_the_fuck_do_i_do_if_i_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js6ywy/i_got_a_dual_3090_what_the_fuck_do_i_do_if_i_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js6ywy/i_got_a_dual_3090_what_the_fuck_do_i_do_if_i_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T16:12:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrnx1z</id>
    <title>Whats the current best abliterated/uncensored model?</title>
    <updated>2025-04-04T21:59:18+00:00</updated>
    <author>
      <name>/u/majorfrankies</name>
      <uri>https://old.reddit.com/user/majorfrankies</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There is not much more to say to be honest. Got a 5090 and want to experiment with bigger weights than when I just gad 8gb.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/majorfrankies"&gt; /u/majorfrankies &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrnx1z/whats_the_current_best_abliterateduncensored_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrnx1z/whats_the_current_best_abliterateduncensored_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrnx1z/whats_the_current_best_abliterateduncensored_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T21:59:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrkbg0</id>
    <title>Presenting CSM-HF : Sesame CSM reimplemented for Transformers (with finetuning support!)</title>
    <updated>2025-04-04T19:24:56+00:00</updated>
    <author>
      <name>/u/hurrytewer</name>
      <uri>https://old.reddit.com/user/hurrytewer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing something I've been working on: a full rewrite of &lt;a href="https://github.com/SesameAILabs/csm"&gt;Sesame's CSM modeling code&lt;/a&gt; for Hugging Face Transformers. It has support for training with HF &lt;code&gt;Trainer&lt;/code&gt; (with &lt;a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:%7E:text=The%20audio%20decoder%20is%20trained%20on%20only%20a%20random%201/16%20subset%20of%20the%20audio%20frames%2C%20while%20the%20zeroth%20codebook%20is%20trained%20on%20every%20frame."&gt;decoder training amortization&lt;/a&gt;) as well as generation. &lt;/p&gt; &lt;p&gt;Finetuning is possible with 24GB ram (2048 frames seq_len, batch size 1, but gradient accumulation is supported for larger effective batch sizes). &lt;/p&gt; &lt;p&gt;For now, generation seems to be slower than realtime (tested with NVIDIA RTX A5000), but I'm hopeful the model can be further optimized. In any case this code can always be used for training only, with possibility of using finetuned weights with different inference code or engines. &lt;/p&gt; &lt;p&gt;LoRA/PEFT support is on the roadmap, let me know if that is something that would benefit your use case.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hurrytewer"&gt; /u/hurrytewer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/thomasgauthier/csm-hf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrkbg0/presenting_csmhf_sesame_csm_reimplemented_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrkbg0/presenting_csmhf_sesame_csm_reimplemented_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T19:24:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jromm0</id>
    <title>Found an awesome repo listing more than 2000+ MCP servers</title>
    <updated>2025-04-04T22:31:39+00:00</updated>
    <author>
      <name>/u/Vivid-Cover8921</name>
      <uri>https://old.reddit.com/user/Vivid-Cover8921</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just came across this GitHub repo and thought it was worth sharing with folks here:&lt;br /&gt; &lt;a href="https://github.com/TensorBlock/awesome-mcp-servers"&gt;https://github.com/TensorBlock/awesome-mcp-servers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôd love to hear from anyone if is using MCP in production or building cool things around it, super hype on this track recently&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vivid-Cover8921"&gt; /u/Vivid-Cover8921 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jromm0/found_an_awesome_repo_listing_more_than_2000_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jromm0/found_an_awesome_repo_listing_more_than_2000_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jromm0/found_an_awesome_repo_listing_more_than_2000_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T22:31:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr6c8e</id>
    <title>Lumina-mGPT 2.0: Stand-alone Autoregressive Image Modeling | Completely open source under Apache 2.0</title>
    <updated>2025-04-04T07:39:20+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr6c8e/luminamgpt_20_standalone_autoregressive_image/"&gt; &lt;img alt="Lumina-mGPT 2.0: Stand-alone Autoregressive Image Modeling | Completely open source under Apache 2.0" src="https://external-preview.redd.it/djIzeHlvdXJ1cnNlMewzcMfvF_oVr57d3HNacgT7P88RyE9Zm1kIukBjIS8J.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb528c0ac716b1babb41d1d367689915e0210a68" title="Lumina-mGPT 2.0: Stand-alone Autoregressive Image Modeling | Completely open source under Apache 2.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Alpha-VLLM/Lumina-mGPT-2.0"&gt;https://github.com/Alpha-VLLM/Lumina-mGPT-2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Alpha-VLLM/Lumina-mGPT-2.0"&gt;https://huggingface.co/Alpha-VLLM/Lumina-mGPT-2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/Alpha-VLLM/Lumina-Image-2.0"&gt;https://huggingface.co/spaces/Alpha-VLLM/Lumina-Image-2.0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jrf0voururse1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr6c8e/luminamgpt_20_standalone_autoregressive_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jr6c8e/luminamgpt_20_standalone_autoregressive_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T07:39:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrwstn</id>
    <title>gemini-2.5-pro-preview-03-25 available for free (this an update of gemini-2.5-pro-exp-03-25)</title>
    <updated>2025-04-05T06:02:27+00:00</updated>
    <author>
      <name>/u/chitown160</name>
      <uri>https://old.reddit.com/user/chitown160</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Output SOTA reasoning traces to distill and SFT into Gemma 3! If you are a dev with a &lt;a href="https://console.cloud.google.com/?fbclid=IwZXh0bgNhZW0CMTAAAR5s3bms1Q-L2JmxMY_DE5Ohrw8fdyW0zbYfVg7om-izxByB9nKMRtuOUgie4A_aem_SKgexjEITFNv3cOMh6tb6w"&gt;https://console.cloud.google.com/&lt;/a&gt; account with billing setup you will have FREE access to gemini-2.5-pro-preview-03-25 (an update that came out 20250404) through &lt;a href="https://aistudio.google.com/"&gt;https://aistudio.google.com/&lt;/a&gt; even before it is available on &lt;a href="https://cloud.google.com/vertex-ai"&gt;https://cloud.google.com/vertex-ai &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chitown160"&gt; /u/chitown160 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrwstn/gemini25propreview0325_available_for_free_this_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrwstn/gemini25propreview0325_available_for_free_this_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrwstn/gemini25propreview0325_available_for_free_this_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T06:02:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrd0a9</id>
    <title>Chinese response bug in tokenizer suggests Quasar-Alpha may be from OpenAI</title>
    <updated>2025-04-04T14:18:33+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/"&gt; &lt;img alt="Chinese response bug in tokenizer suggests Quasar-Alpha may be from OpenAI" src="https://b.thumbs.redditmedia.com/DMLWSDxNqqWl7AjUQHrzw2WhaBG3cSl0ZG95ENj1I-s.jpg" title="Chinese response bug in tokenizer suggests Quasar-Alpha may be from OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After testing the recently released quasar-alpha model by openrouter, I discovered that when asking this specific Chinese question:&lt;/p&gt; &lt;p&gt;''' Áªô‰∏ª‰∫∫Áïô‰∏ã‰∫õ‰ªÄ‰πàÂêß ËøôÂè•ËØùÁøªËØëÊàêËã±Êñá '''&lt;br /&gt; (This sentence means &amp;quot;Leave something for the master&amp;quot; and &amp;quot;Translate this sentence into English&amp;quot;)&lt;/p&gt; &lt;p&gt;The model's response is completely unrelated to the question.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wiq7fg3qttse1.png?width=2384&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6368c81d31fab66e3a998d26b96a49b2a556e3b2"&gt;quasar-alpha's answer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPT-4o had the same issue when it was released, because in the updated o200k_base tokenizer, the phrase &amp;quot;Áªô‰∏ª‰∫∫Áïô‰∏ã‰∫õ‰ªÄ‰πàÂêß&amp;quot; happens to be a single token with ID 177431. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/blb4buzxttse1.png?width=2546&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8dc289dc1b2968f92620aa2137e03f3a83bf624"&gt;GPT-4o's answer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The fact that this new model exhibits the same problem increases suspicion that this secret model indeed comes from OpenAI, and they still haven't fixed this Chinese token bug.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T14:18:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrpbj8</id>
    <title>Not GPT-4, but a 3B Function Calling LLM that can chat to clarify tools calls</title>
    <updated>2025-04-04T23:03:50+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrpbj8/not_gpt4_but_a_3b_function_calling_llm_that_can/"&gt; &lt;img alt="Not GPT-4, but a 3B Function Calling LLM that can chat to clarify tools calls" src="https://external-preview.redd.it/NXFyNTkzcDdmd3NlMU9tL6wO_KLELRMymDQTLDBHY2EFrYZp7zEuTqyq1mYt.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3df7cf2e6aafb7a6cc499bd220274b24812aa7b4" title="Not GPT-4, but a 3B Function Calling LLM that can chat to clarify tools calls" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to have recently released &lt;a href="https://huggingface.co/katanemo/Arch-Function-Chat-3B"&gt;Arch-Function-Chat&lt;/a&gt; A collection of fast, device friendly LLMs that achieve performance on-par with GPT-4 on function calling, now trained to chat. Why chat? To help gather accurate information from the user before triggering a tools call (manage context, handle progressive disclosure, and also respond to users in lightweight dialogue on execution of tools results). &lt;/p&gt; &lt;p&gt;The model is out on HF, and the work to integrate it in &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt; should be completed by Monday - we are also adding to support to integrate with tools definitions as captured via MCP in the upcoming week, so combining two releases in one. Happy building üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i9hd03p7fwse1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrpbj8/not_gpt4_but_a_3b_function_calling_llm_that_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrpbj8/not_gpt4_but_a_3b_function_calling_llm_that_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T23:03:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrfqnu</id>
    <title>Meta Set to Release Llama 4 This Month, per The Information &amp; Reuters</title>
    <updated>2025-04-04T16:13:03+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;April 4 (Reuters) - Meta Platforms (META.O), plans to release the latest version of its large language model later this month, after delaying it at least twice, the Information reported on Friday, as the Facebook owner scrambles to lead in the AI race.&lt;/p&gt; &lt;p&gt;Meta, however, could push back the release of Llama 4 again, the report said, citing two people familiar with the matter.&lt;/p&gt; &lt;p&gt;Big technology firms have been investing aggressively in AI infrastructure following the success of OpenAI's ChatGPT, which altered the tech landscape and drove investment into machine learning.&lt;/p&gt; &lt;p&gt;The report said one of the reasons for the delay is during development, Llama 4 did not meet Meta's expectations on technical benchmarks, particularly in reasoning and math tasks.&lt;/p&gt; &lt;p&gt;The company was also concerned that Llama 4 was less capable than OpenAI's models in conducting humanlike voice conversations, the report added.&lt;/p&gt; &lt;p&gt;Meta plans to spend as much as $65 billion this year to expand its AI infrastructure, amid investor pressure on big tech firms to show returns on their investments.&lt;/p&gt; &lt;p&gt;Additionally, the rise of the popular, lower-cost model from Chinese tech firm DeepSeek challenges the belief that developing the best AI model requires billions of dollars.&lt;/p&gt; &lt;p&gt;The report said Llama 4 is expected to borrow certain technical aspects from DeepSeek, with at least one version slated to employ a machine-learning technique called mixture of experts method, which trains separate parts of models for specific tasks, making them experts in those areas.&lt;/p&gt; &lt;p&gt;Meta has also considered releasing Llama 4 through Meta AI first and then as open-source software later, the report said.&lt;/p&gt; &lt;p&gt;Last year, Meta released its mostly free Llama 3 AI model, which can converse in eight languages, write higher-quality computer code and solve more complex math problems than previous versions.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reuters.com/technology/artificial-intelligence/meta-nears-release-new-ai-model-llama-4-this-month-information-reports-2025-04-04/"&gt;&lt;em&gt;https://www.reuters.com/technology/artificial-intelligence/meta-nears-release-new-ai-model-llama-4-this-month-information-reports-2025-04-04/&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.theinformation.com/articles/meta-nears-release-new-ai-model-performance-hiccups"&gt;&lt;em&gt;https://www.theinformation.com/articles/meta-nears-release-new-ai-model-performance-hiccups&lt;/em&gt; &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrfqnu/meta_set_to_release_llama_4_this_month_per_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrfqnu/meta_set_to_release_llama_4_this_month_per_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrfqnu/meta_set_to_release_llama_4_this_month_per_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T16:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jre3kp</id>
    <title>New paper from DeepSeek w/ model coming soon: Inference-Time Scaling for Generalist Reward Modeling</title>
    <updated>2025-04-04T15:04:46+00:00</updated>
    <author>
      <name>/u/samfundev</name>
      <uri>https://old.reddit.com/user/samfundev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quote from the abstract:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A key challenge of reinforcement learning (RL) is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. In this work, we investigate how to improve reward modeling (RM) with more inference compute for general queries, i.e. the inference-time scalability of generalist RM, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. [...] Empirically, we show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which we believe can be addressed by future efforts in generalist reward systems. &lt;strong&gt;The models will be released and open-sourced.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Summary from Claude:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;Can you provide a two paragraph summary of this paper for an audience of people who are enthusiastic about running LLMs locally?&lt;/em&gt; &lt;/p&gt; &lt;p&gt;This paper introduces DeepSeek-GRM, a novel approach to reward modeling that allows for effective &amp;quot;inference-time scaling&amp;quot; - getting better results by running multiple evaluations in parallel rather than requiring larger models. The researchers developed a method called Self-Principled Critique Tuning (SPCT) which trains reward models to generate tailored principles for each evaluation task, then produce detailed critiques based on those principles. Their experiments show that DeepSeek-GRM-27B with parallel sampling can match or exceed the performance of much larger reward models (up to 671B parameters), demonstrating that compute can be more effectively used at inference time rather than training time.&lt;/p&gt; &lt;p&gt;For enthusiasts running LLMs locally, this research offers a promising path to higher-quality evaluation without needing massive models. By using a moderately-sized reward model (27B parameters) and running it multiple times with different seeds, then combining the results through voting or their meta-RM approach, you can achieve evaluation quality comparable to much larger models. The authors also show that this generative reward modeling approach avoids the domain biases of scalar reward models, making it more versatile for different types of tasks. The models will be open-sourced, potentially giving local LLM users access to high-quality evaluation tools.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samfundev"&gt; /u/samfundev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2504.02495"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jre3kp/new_paper_from_deepseek_w_model_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jre3kp/new_paper_from_deepseek_w_model_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T15:04:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1js14u2</id>
    <title>Coding agents?</title>
    <updated>2025-04-05T11:14:38+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, would like to know what you use for local coding, I tried few months ago cline with qwen2.5 coder (4x3090). Are there better options now?&lt;/p&gt; &lt;p&gt;Another dumb question: is there a simple way to connect an agentic workflow (crewai, autogen‚Ä¶) to a tool like cline, aider etc.?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js14u2/coding_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js14u2/coding_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js14u2/coding_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T11:14:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1js31db</id>
    <title>Gemma3 licence</title>
    <updated>2025-04-05T13:07:06+00:00</updated>
    <author>
      <name>/u/Royal_Light_9921</name>
      <uri>https://old.reddit.com/user/Royal_Light_9921</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please explain to me like I'm 5 years old. What's wrong with their licence and what can I use it for? What is forbidden?&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Royal_Light_9921"&gt; /u/Royal_Light_9921 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js31db/gemma3_licence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js31db/gemma3_licence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js31db/gemma3_licence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T13:07:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1js7559</id>
    <title>SoftWhisper April 2025 out ‚Äì automated transcription now with speaker identification!</title>
    <updated>2025-04-05T16:20:24+00:00</updated>
    <author>
      <name>/u/Substantial_Swan_144</name>
      <uri>https://old.reddit.com/user/Substantial_Swan_144</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js7559/softwhisper_april_2025_out_automated/"&gt; &lt;img alt="SoftWhisper April 2025 out ‚Äì automated transcription now with speaker identification!" src="https://b.thumbs.redditmedia.com/Ju8anVfbPSfyONXi3M6stIwJ9m4n7WIstBVMqH9PYtw.jpg" title="SoftWhisper April 2025 out ‚Äì automated transcription now with speaker identification!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, my dear Github friends,&lt;/p&gt; &lt;p&gt;It is with great joy that I announce that SoftWhisper April 2025 is out ‚Äì now with speaker identification (diarization)!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/63yt4ll2l1te1.png?width=1986&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a41755400d8ab52f5b1f5a99444c74acfd1e939"&gt;https://preview.redd.it/63yt4ll2l1te1.png?width=1986&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a41755400d8ab52f5b1f5a99444c74acfd1e939&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A tricky feature&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Originally, I wanted to implement diarization with Pyannote, but because APIs are usually not widelly documented, not only learning how to use them, but also how effective they are for the project, is a bit difficult.&lt;/p&gt; &lt;p&gt;Identifying speakers is still somewhat primitive even with state-of-the-art solutions. Usually, the best results are achieved with fine-tuned models and controlled conditions (for example, two speakers in studio recordings).&lt;/p&gt; &lt;p&gt;The crux of the matter is: not only do we require a lot of money to create those specialized models, but they are incredibly hard to use. That does not align with my vision of having something that works reasonably well and is easy to setup, so I did a few tests with 3-4 different approaches.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A balanced compromise&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After careful testing, I believe inaSpeechSegmenter will provide our users the best balance between usability and accuracy: it's fast, identifies speakers to a more or less consistent degree out of the box, and does not require a complicated setup. Give it a try!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Known issues&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Please note: while speaker identification is more or less consistent, the current approach is still not perfect and will sometimes not identify cross speech or add more speakers than present in the audio, so manual review is still needed. This feature is provided with the hopes to make diarization easier, not a solved problem.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Increased loading times&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Also keep in mind that the current diarization solution will increase the loading times slightly and if you select diarization, computation will also increase. Please be patient.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Other bugfixes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This release also fixes a few other bugs, namely that the exported content sometimes would not match the content in the textbox.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Swan_144"&gt; /u/Substantial_Swan_144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js7559/softwhisper_april_2025_out_automated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js7559/softwhisper_april_2025_out_automated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js7559/softwhisper_april_2025_out_automated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T16:20:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrqb11</id>
    <title>Framework Desktop development units for open source AI developers</title>
    <updated>2025-04-04T23:52:01+00:00</updated>
    <author>
      <name>/u/cmonkey</name>
      <uri>https://old.reddit.com/user/cmonkey</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apologies in advance if this pushes too far into self-promotion, but when we launched Framework Desktop, AMD also announced that they would be providing 100 units to open source developers based in US/Canada to help accelerate local AI development. The application form for that is now open at &lt;a href="https://www.amd.com/en/forms/sign-up/framework-desktop-giveaway.html"&gt;https://www.amd.com/en/forms/sign-up/framework-desktop-giveaway.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm also happy to answer questions folks have around using Framework Desktop for local inference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cmonkey"&gt; /u/cmonkey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrqb11/framework_desktop_development_units_for_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrqb11/framework_desktop_development_units_for_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrqb11/framework_desktop_development_units_for_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T23:52:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1js5vwm</id>
    <title>AMD mi325x (8x) deployment and tests.</title>
    <updated>2025-04-05T15:23:38+00:00</updated>
    <author>
      <name>/u/Shivacious</name>
      <uri>https://old.reddit.com/user/Shivacious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Locallama cool people i am back again with new posts after &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1it46dv/amd_mi300x_deployment_and_tests/"&gt;amd_mi300x(8x)_deployment_and_tests&lt;/a&gt;&lt;/p&gt; &lt;p&gt;i will be soon be getting access to 8 x mi325x all connected by infinity fabric and yes 96 cores 2TB ram (the usual). &lt;/p&gt; &lt;p&gt;let me know what are you guys curious to actually test on it and i will try fulfilling every request as much as possible. from single model single gpu to multi model single gpu or even deploying r1 and v3 deploying in a single instance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shivacious"&gt; /u/Shivacious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js5vwm/amd_mi325x_8x_deployment_and_tests/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js5vwm/amd_mi325x_8x_deployment_and_tests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js5vwm/amd_mi325x_8x_deployment_and_tests/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T15:23:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrvhwk</id>
    <title>ibm-granite/granite-speech-3.2-8b ¬∑ Hugging Face</title>
    <updated>2025-04-05T04:37:00+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrvhwk/ibmgranitegranitespeech328b_hugging_face/"&gt; &lt;img alt="ibm-granite/granite-speech-3.2-8b ¬∑ Hugging Face" src="https://external-preview.redd.it/7tIq7fmEQHkDDxZGysXqAxxJULRvzvho5Jaa29Tj9zc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4fd879df27c798df53dc6f4fb0ddbd427697e30d" title="ibm-granite/granite-speech-3.2-8b ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Granite-speech-3.2-8b is a compact and efficient speech-language model, specifically designed for automatic speech recognition (ASR) and automatic speech translation (AST).&lt;/p&gt; &lt;p&gt;License: Apache 2.0&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-speech-3.2-8b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrvhwk/ibmgranitegranitespeech328b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrvhwk/ibmgranitegranitespeech328b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T04:37:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrljxa</id>
    <title>Local LLMs are essential in a world where LLM platforms are going to get filled with ads</title>
    <updated>2025-04-04T20:16:43+00:00</updated>
    <author>
      <name>/u/TechExpert2910</name>
      <uri>https://old.reddit.com/user/TechExpert2910</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrljxa/local_llms_are_essential_in_a_world_where_llm/"&gt; &lt;img alt="Local LLMs are essential in a world where LLM platforms are going to get filled with ads" src="https://external-preview.redd.it/VS1VAWyp01_knDCN9Cj6--BP1C9fvqcewix3-g1QeRg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b34c3c22bfaf136ee38db8c12bf0b99aa321162" title="Local LLMs are essential in a world where LLM platforms are going to get filled with ads" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechExpert2910"&gt; /u/TechExpert2910 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://privacyinternational.org/long-read/5472/chatbots-adbots-sharing-your-thoughts-advertisers"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrljxa/local_llms_are_essential_in_a_world_where_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrljxa/local_llms_are_essential_in_a_world_where_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T20:16:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1js0zmd</id>
    <title>Quick Comparison of QwQ and OpenThinker2 32B</title>
    <updated>2025-04-05T11:04:54+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Candle test:&lt;/p&gt; &lt;p&gt;qwq: &lt;a href="https://imgur.com/a/c5gJ2XL"&gt;https://imgur.com/a/c5gJ2XL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ot2: &lt;a href="https://imgur.com/a/TDNm12J"&gt;https://imgur.com/a/TDNm12J&lt;/a&gt;&lt;/p&gt; &lt;p&gt;both passed&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;5 reasoning questions:&lt;/p&gt; &lt;p&gt;&lt;a href="https://imgur.com/a/ec17EJC"&gt;https://imgur.com/a/ec17EJC&lt;/a&gt;&lt;/p&gt; &lt;p&gt;qwq passed all questions&lt;/p&gt; &lt;p&gt;ot2 failed 2 questions&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Private tests:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Coding question: One question about what caused the issue, plus 1,200 lines of C++ code. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Both passed, however ot2 is not as reliable as QwQ at solving this issue. It could give wrong answer during multi-shots, unlike qwq which always give the right answer.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Restructuring a financial spreadsheet. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Both passed.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Conclusion:&lt;/p&gt; &lt;p&gt;I prefer OpenThinker2-32B over the original R1-distill-32B from DS, especially because it never fell into an infinite loop during testing. I tested those five reasoning questions three times on OT2, and it never fell into a loop, unlike the R1-distill model.&lt;/p&gt; &lt;p&gt;Which is quite an achievement considering they open-sourced their dataset and their distillation dataset is not much larger than DS's (1M vs 800k).&lt;/p&gt; &lt;p&gt;However, it still falls behind QwQ-32B, which uses RL instead.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Settings I used for both models: &lt;a href="https://imgur.com/a/7ZBQ6SX"&gt;https://imgur.com/a/7ZBQ6SX&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gguf: &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/blob/main/Qwen_QwQ-32B-IQ4_XS.gguf"&gt;https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/blob/main/Qwen_QwQ-32B-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/open-thoughts_OpenThinker2-32B-GGUF/blob/main/open-thoughts_OpenThinker2-32B-IQ4_XS.gguf"&gt;https://huggingface.co/bartowski/open-thoughts_OpenThinker2-32B-GGUF/blob/main/open-thoughts_OpenThinker2-32B-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;backend: ollama&lt;/p&gt; &lt;p&gt;source of public questions:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i65599/r1_32b_is_be_worse_than_qwq_32b_tests_included/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1i65599/r1_32b_is_be_worse_than_qwq_32b_tests_included/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jpr1nk/the_candle_test_most_llms_fail_to_generalise_at/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jpr1nk/the_candle_test_most_llms_fail_to_generalise_at/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js0zmd/quick_comparison_of_qwq_and_openthinker2_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js0zmd/quick_comparison_of_qwq_and_openthinker2_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js0zmd/quick_comparison_of_qwq_and_openthinker2_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T11:04:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jryrik</id>
    <title>OpenThinker2-32B</title>
    <updated>2025-04-05T08:21:23+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jryrik/openthinker232b/"&gt; &lt;img alt="OpenThinker2-32B" src="https://external-preview.redd.it/MMWrXSajlQDn44f7IjYIsEwZJ6CFMkYChFS5zuH6LYA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b962a458fb32a0508ef1344c44f7b73075ef5245" title="OpenThinker2-32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/open-thoughts/OpenThinker2-32B"&gt;https://huggingface.co/open-thoughts/OpenThinker2-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1x9zxh5f7zse1.png?width=704&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76c2b9c3676b9d9969d428b0a44cf823d4f72367"&gt;https://preview.redd.it/1x9zxh5f7zse1.png?width=704&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76c2b9c3676b9d9969d428b0a44cf823d4f72367&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jryrik/openthinker232b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jryrik/openthinker232b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jryrik/openthinker232b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T08:21:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1js335l</id>
    <title>Karamaru - An "Edo period" LLM trained on 17th-19th century japanese literature.</title>
    <updated>2025-04-05T13:09:39+00:00</updated>
    <author>
      <name>/u/nomad_lw</name>
      <uri>https://old.reddit.com/user/nomad_lw</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js335l/karamaru_an_edo_period_llm_trained_on_17th19th/"&gt; &lt;img alt="Karamaru - An &amp;quot;Edo period&amp;quot; LLM trained on 17th-19th century japanese literature." src="https://external-preview.redd.it/ll0sI2kj9OWJW1iOriHpZm1jSfC278YnLF-jisELKs4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5f30bf0b3bae15b4dee53ba7bd37f2486072c04" title="Karamaru - An &amp;quot;Edo period&amp;quot; LLM trained on 17th-19th century japanese literature." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw this a few days ago where a researcher from Sakana AI continually pretrained a Llama-3 Elyza 8B model on classical japanese literature. &lt;/p&gt; &lt;p&gt;What's cool about is that it builds towards an idea that's been brewing on my mind and evidently a lot of other people here,&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A model that's able to be a Time-travelling subject matter expert. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;Researcher's tweet: &lt;a href="https://x.com/tkasasagi/status/1907998360713441571?t=PGhYyaVJQtf0k37l-9zXiA&amp;amp;s=19"&gt;https://x.com/tkasasagi/status/1907998360713441571?t=PGhYyaVJQtf0k37l-9zXiA&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface:&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/SakanaAI/Llama-3-Karamaru-v1"&gt;https://huggingface.co/SakanaAI/Llama-3-Karamaru-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Space: &lt;a href="https://huggingface.co/spaces/SakanaAI/Llama-3-Karamaru-v1"&gt;https://huggingface.co/spaces/SakanaAI/Llama-3-Karamaru-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomad_lw"&gt; /u/nomad_lw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sakana.ai/karamaru/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js335l/karamaru_an_edo_period_llm_trained_on_17th19th/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js335l/karamaru_an_edo_period_llm_trained_on_17th19th/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T13:09:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1js0g38</id>
    <title>Tenstorrent Blackhole PCI-e cards with 32 GB of GDDR6 available for order</title>
    <updated>2025-04-05T10:27:38+00:00</updated>
    <author>
      <name>/u/Marcuss2</name>
      <uri>https://old.reddit.com/user/Marcuss2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js0g38/tenstorrent_blackhole_pcie_cards_with_32_gb_of/"&gt; &lt;img alt="Tenstorrent Blackhole PCI-e cards with 32 GB of GDDR6 available for order" src="https://external-preview.redd.it/bKa6_zcgR56sbwG-Cqtu_jN8tcBni2YVCOOrskJ4IzI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d47a097527e5e5e57498d3ac6192eb2f6741fe55" title="Tenstorrent Blackhole PCI-e cards with 32 GB of GDDR6 available for order" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Marcuss2"&gt; /u/Marcuss2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://tenstorrent.com/hardware/blackhole"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js0g38/tenstorrent_blackhole_pcie_cards_with_32_gb_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js0g38/tenstorrent_blackhole_pcie_cards_with_32_gb_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T10:27:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1js4iy0</id>
    <title>I think I overdid it.</title>
    <updated>2025-04-05T14:21:22+00:00</updated>
    <author>
      <name>/u/_supert_</name>
      <uri>https://old.reddit.com/user/_supert_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js4iy0/i_think_i_overdid_it/"&gt; &lt;img alt="I think I overdid it." src="https://preview.redd.it/i5f8b0knz0te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1448cae5bed745aa96ac7b2801a7bf32c07afd26" title="I think I overdid it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_supert_"&gt; /u/_supert_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i5f8b0knz0te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js4iy0/i_think_i_overdid_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js4iy0/i_think_i_overdid_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T14:21:22+00:00</published>
  </entry>
</feed>
