<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-18T17:05:22+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kpnrll</id>
    <title>Curly quotes</title>
    <updated>2025-05-18T16:19:18+00:00</updated>
    <author>
      <name>/u/autonoma_2042</name>
      <uri>https://old.reddit.com/user/autonoma_2042</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A publisher wrote me:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;It's a continuing source of frustration that LLMs can't handle curly quotes, as just about everything else in our writing and style guide can be aligned with generated content.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Does anyone know of a local LLM that can curl quotes correctly? Such as:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;''E's got a 'ittle box 'n a big 'un,' she said, 'wit' th' 'ittle 'un 'bout 2'√ó6&amp;quot;. An' no, y'ain't cryin' on th' &amp;quot;soap box&amp;quot; to me no mo, y'hear. 'Cause it 'tweren't ever a spec o' fun!' I says to my frien'.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;into:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;‚Äò‚ÄôE‚Äôs got a ‚Äôittle box ‚Äôn a big ‚Äôun,‚Äô she said, ‚Äòwit‚Äô th‚Äô ‚Äôittle ‚Äôun ‚Äôbout 2‚Ä≤√ó6‚Ä≥. An‚Äô no, y‚Äôain‚Äôt cryin‚Äô on th‚Äô ‚Äúsoap box‚Äù to me no mo, y‚Äôhear. ‚ÄôCause it ‚Äôtweren‚Äôt ever a spec o‚Äô fun!‚Äô I says to my frien‚Äô.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/autonoma_2042"&gt; /u/autonoma_2042 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpnrll/curly_quotes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpnrll/curly_quotes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpnrll/curly_quotes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T16:19:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpfq0h</id>
    <title>Has anyone used TTS or a voice cloning to do a call return message on your phone?</title>
    <updated>2025-05-18T09:07:30+00:00</updated>
    <author>
      <name>/u/Extension-Fee-8480</name>
      <uri>https://old.reddit.com/user/Extension-Fee-8480</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are some good messages or angry phone message from TTS?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Extension-Fee-8480"&gt; /u/Extension-Fee-8480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpfq0h/has_anyone_used_tts_or_a_voice_cloning_to_do_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpfq0h/has_anyone_used_tts_or_a_voice_cloning_to_do_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpfq0h/has_anyone_used_tts_or_a_voice_cloning_to_do_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T09:07:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kph6b1</id>
    <title>Should I finetune or use fewshot prompting?</title>
    <updated>2025-05-18T10:50:06+00:00</updated>
    <author>
      <name>/u/GHOST--1</name>
      <uri>https://old.reddit.com/user/GHOST--1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have document images with size 4000x2000. I want the LLMs to detect certain visual elements from the image. The visual elements do not contain text so I am not sure if sending OCR text alongwith the images will do any good. I can't use a detection model due to a few policy limitations and want to work with LLMs/VLMs.&lt;/p&gt; &lt;p&gt;Right now I am sending 6 fewshot images and their response alongwith my query image. Sometimes the LLM works flawlessly, and sometimes it completely misses on even the easiest images.&lt;/p&gt; &lt;p&gt;I have tried Gpt-4o, claude, gemini, etc. but all suffer with the same performance drop. Should I go ahead and use the finetune option to finetune Gpt-4o on 1000 samples? or is there a way to improve perforance with fewshot prompting?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GHOST--1"&gt; /u/GHOST--1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kph6b1/should_i_finetune_or_use_fewshot_prompting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kph6b1/should_i_finetune_or_use_fewshot_prompting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kph6b1/should_i_finetune_or_use_fewshot_prompting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T10:50:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpoj6x</id>
    <title>Contribution to ollama-python: decorators, helper functions and simplified creation tool</title>
    <updated>2025-05-18T16:52:31+00:00</updated>
    <author>
      <name>/u/chavomodder</name>
      <uri>https://old.reddit.com/user/chavomodder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpoj6x/contribution_to_ollamapython_decorators_helper/"&gt; &lt;img alt="Contribution to ollama-python: decorators, helper functions and simplified creation tool" src="https://external-preview.redd.it/0WHEreexf2DJMw78A-6XfudwOUYNJRPPM2H2EZ2R2b8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6bbde65c5b5324258d0882eae80d38a008f28e7e" title="Contribution to ollama-python: decorators, helper functions and simplified creation tool" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, guys, I posted this on the official ollama Reddit but I decided to post it here too! (This post was written in Portuguese)&lt;/p&gt; &lt;p&gt;I made a commit to ollama-python with the aim of making it easier to create and use custom tools. You can now use simple decorators to register functions:&lt;/p&gt; &lt;p&gt;@ollama_tool ‚Äì for synchronous functions&lt;/p&gt; &lt;p&gt;@ollama_async_tool ‚Äì for asynchronous functions&lt;/p&gt; &lt;p&gt;I also added auxiliary functions to make organizing and using the tools easier:&lt;/p&gt; &lt;p&gt;get_tools() ‚Äì returns all registered tools&lt;/p&gt; &lt;p&gt;get_tools_name() ‚Äì dictionary with the name of the tools and their respective functions&lt;/p&gt; &lt;p&gt;get_name_async_tools() ‚Äì list of asynchronous tool names&lt;/p&gt; &lt;p&gt;Additionally, I created a new function called create_function_tool, which allows you to create tools in a similar way to manual, but without worrying about the JSON structure. Just pass the Python parameters like: (tool_name, description, parameter_list, required_parameters)&lt;/p&gt; &lt;p&gt;Now, to work with the tools, the flow is very simple:&lt;/p&gt; &lt;h1&gt;Returns the functions that are with the decorators&lt;/h1&gt; &lt;p&gt;tools = get_tools()&lt;/p&gt; &lt;h1&gt;dictionary with all functions using decorators (as already used)&lt;/h1&gt; &lt;p&gt;available_functions = get_tools_name() &lt;/p&gt; &lt;h1&gt;returns the names of asynchronous functions&lt;/h1&gt; &lt;p&gt;async_available_functions = get_name_async_tools() &lt;/p&gt; &lt;p&gt;And in the code, you can use an if to check if the function is asynchronous (based on the list of async_available_functions) and use await or asyncio.run() as necessary.&lt;/p&gt; &lt;p&gt;These changes help reduce the boilerplate and make development with the library more practical.&lt;/p&gt; &lt;p&gt;Anyone who wants to take a look or suggest something, follow:&lt;/p&gt; &lt;p&gt;Commit link: [ &lt;a href="https://github.com/ollama/ollama-python/pull/516"&gt;https://github.com/ollama/ollama-python/pull/516&lt;/a&gt; ]&lt;/p&gt; &lt;p&gt;My repository link:&lt;/p&gt; &lt;p&gt;[ &lt;a href="https://github.com/caua1503/ollama-python/tree/main"&gt;https://github.com/caua1503/ollama-python/tree/main&lt;/a&gt; ]&lt;/p&gt; &lt;p&gt;Observation:&lt;/p&gt; &lt;p&gt;I was already using this in my real project and decided to share it. &lt;/p&gt; &lt;p&gt;I'm an experienced Python dev, but this is my first time working with decorators and I decided to do this in the simplest way possible, I hope to help the community, I know defining global lists, maybe it's not the best way to do this but I haven't found another way&lt;/p&gt; &lt;p&gt;In addition to langchain being complicated and changing everything with each update, I couldn't use it with ollama models, so I went to the Ollama Python library&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chavomodder"&gt; /u/chavomodder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ollama/ollama-python/pull/516"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpoj6x/contribution_to_ollamapython_decorators_helper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpoj6x/contribution_to_ollamapython_decorators_helper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T16:52:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kposu0</id>
    <title>Requesting help with my thesis</title>
    <updated>2025-05-18T17:03:44+00:00</updated>
    <author>
      <name>/u/Nissepelle</name>
      <uri>https://old.reddit.com/user/Nissepelle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: Are the models I have linked comparable if I were to feed them the same dataset, with the same instructions/prompt and ask them to make a decision? The documents I intend to feed them are very large (probably around 20-30k tokens), which leads be to suspect some level of performance degradation. Is there a way to mitigate this?&lt;/p&gt; &lt;p&gt;Hello.&lt;/p&gt; &lt;p&gt;I'll keep it brief, but I am doing my CS thesis in the field of automation using different LLMs. Specifically, I'm looking at 4-6 LLMs of the same size (70b) who are reasoning based and analyzing how well they can application documents (think application for funding) I feed it based on a predefined criteria. All of the applications have already been approved or rejected by a human.&lt;/p&gt; &lt;p&gt;Basically, I have a labeled dataset of applications, and I want to feed that dataset to the different models and see which performs the best and also how the results compare to the human benchmark.&lt;/p&gt; &lt;p&gt;However, I have had very little experience working with models on any level and have such ran into a ton of problems, so I'm coming here hoping to recieve some help in trying to make this thesis project work.&lt;/p&gt; &lt;p&gt;First, I'd like some feedback on the models I have selected. My main worry is (as someone without much knowledge or experience in this area) that the models are not comparable since they are specialized in different ways. &lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/llama3.3"&gt;llama3.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/deepseek-r1"&gt;deepseek-r1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/qwen2.5"&gt;qwen2.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/mixtral"&gt;mixtral8x7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/cogito"&gt;cogito&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A technical limitation here is that the models have to be available via ollama as the server I have been given to run the experiments needed is using ollama. This is not something that can be circumvented unfortunately. Would love to get some feedback here on if the models are comparable, and if not, what other models I ought to consider.&lt;/p&gt; &lt;p&gt;Second question I dont know how to tackle; performance degradation on due to token size. Basically, the documents that will be fed to the model will be labeled applications (think approved/denied). These applications in turn might have additional documents that are required to fulfill the evaluation (think budget documents etc.). As a result, the data needed to be sent to the model might total around 20-30k tokens varying with application detail and size etc. Ideally, I would love to ensure the results of the experiment I plan to run be as valid as possible, and this would include taking into account performance degredation. The only solution I can think of is chunking, but I dont know how well that would work, considering the evaluation needs to be done on the whole of the application. I thought about possibly summarizing the contents of an application, but then the experiment becomes invalid as it technically isnt the same data being tested. In addition, I would very likely use some sort of LLM to summarize the application contents, which cold be a major threat to the validity of the results.&lt;/p&gt; &lt;p&gt;I guess my question for the second part is: is there a way to get around this? Feels like the best alternative to just &amp;quot;letting it rip&amp;quot;, but I dont know how realistic such an approach would be.&lt;/p&gt; &lt;p&gt;Thank you in advance. There are unclear aspects of&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nissepelle"&gt; /u/Nissepelle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kposu0/requesting_help_with_my_thesis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kposu0/requesting_help_with_my_thesis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kposu0/requesting_help_with_my_thesis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T17:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpotei</id>
    <title>Multiple, concurrent user accessing to local LLM ü¶ôü¶ôü¶ôü¶ô</title>
    <updated>2025-05-18T17:04:24+00:00</updated>
    <author>
      <name>/u/Prestigious-Use5483</name>
      <uri>https://old.reddit.com/user/Prestigious-Use5483</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did a bit of research with the help of AI and it seems that it should work fine, but I haven't yet tested it and put it to real use. So I'm hoping someone who has, can share their experience.&lt;/p&gt; &lt;p&gt;It seems that LLMs (even with 1 GPU and 1 model loaded) can be used with multiple, concurrent users and the performance will still be really good.&lt;/p&gt; &lt;p&gt;I asked AI (GLM-4) and in my example, I told it that I have a 24GB VRAM GPU (RTX 3090). The model I am using is GLM-4-32B-0414-UD-Q4_K_XL (18.5GB) with 32K context (2.5-3GB) for a total of 21-21.5GB. It said that I should be able to have 2 concurrent users accessing the model, or I can drop the context down to 16K and have 4 concurrent users, or 8K with 8 users. This seems really good for a general purpose access terminal in the home so that many users can access it whenever they want.&lt;/p&gt; &lt;p&gt;Again, it was just something I researched late last night, but haven't tried it. Of course, we can use a smaller model or quant and adjust our needs accordingly with higher context or more concurrent users.&lt;/p&gt; &lt;p&gt;This seems very cool and just wanted to share the idea with others if they haven't thought about it before and also get something who has done this and see what their results were. ü¶ôü¶ôü¶ôü¶ô&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prestigious-Use5483"&gt; /u/Prestigious-Use5483 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpotei/multiple_concurrent_user_accessing_to_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpotei/multiple_concurrent_user_accessing_to_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpotei/multiple_concurrent_user_accessing_to_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T17:04:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp6gdv</id>
    <title>ROCm 6.4 + current unsloth working</title>
    <updated>2025-05-17T23:37:19+00:00</updated>
    <author>
      <name>/u/Ok_Ocelot2268</name>
      <uri>https://old.reddit.com/user/Ok_Ocelot2268</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here a working ROCm unsloth docker setup:&lt;/p&gt; &lt;p&gt;Dockerfile (for gfx1100)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM rocm/pytorch:rocm6.4_ubuntu22.04_py3.10_pytorch_release_2.6.0 WORKDIR /root RUN git clone -b rocm_enabled_multi_backend https://github.com/ROCm/bitsandbytes.git RUN cd bitsandbytes/ &amp;amp;&amp;amp; cmake -DGPU_TARGETS=&amp;quot;gfx1100&amp;quot; -DBNB_ROCM_ARCH=&amp;quot;gfx1100&amp;quot; -DCOMPUTE_BACKEND=hip -S . &amp;amp;&amp;amp; make &amp;amp;&amp;amp; pip install -e . RUN pip install unsloth_zoo&amp;gt;=2025.5.7 RUN pip install datasets&amp;gt;=3.4.1 sentencepiece&amp;gt;=0.2.0 tqdm psutil wheel&amp;gt;=0.42.0 RUN pip install accelerate&amp;gt;=0.34.1 RUN pip install peft&amp;gt;=0.7.1,!=0.11.0 WORKDIR /root RUN git clone https://github.com/ROCm/xformers.git RUN cd xformers/ &amp;amp;&amp;amp; git submodule update --init --recursive &amp;amp;&amp;amp; git checkout 13c93f3 &amp;amp;&amp;amp; PYTORCH_ROCM_ARCH=gfx1100 python setup.py install ENV FLASH_ATTENTION_TRITON_AMD_ENABLE=&amp;quot;TRUE&amp;quot; WORKDIR /root RUN git clone https://github.com/ROCm/flash-attention.git RUN cd flash-attention &amp;amp;&amp;amp; git checkout main_perf &amp;amp;&amp;amp; python setup.py install WORKDIR /root RUN git clone https://github.com/unslothai/unsloth.git RUN cd unsloth &amp;amp;&amp;amp; pip install . &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;docker-compose.yml&lt;/p&gt; &lt;pre&gt;&lt;code&gt;version: '3' services: unsloth: container_name: unsloth devices: - /dev/kfd:/dev/kfd - /dev/dri:/dev/dri image: unsloth volumes: - ./data:/data - ./hf:/root/.cache/huggingface environment: - 'HSA_OVERRIDE_GFX_VERSION=${HSA_OVERRIDE_GFX_VERSION-11.0.0}' command: sleep infinity &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;python -m bitsandbytes says &amp;quot;PyTorch settings found: ROCM_VERSION=64&amp;quot; but also tracebacks with &lt;/p&gt; &lt;pre&gt;&lt;code&gt; File &amp;quot;/root/bitsandbytes/bitsandbytes/backends/__init__.py&amp;quot;, line 15, in ensure_backend_is_available raise NotImplementedError(f&amp;quot;Device backend for {device_type} is currently not supported.&amp;quot;) NotImplementedError: Device backend for cuda is currently not supported. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;python -m xformers.info&lt;/p&gt; &lt;pre&gt;&lt;code&gt;xFormers 0.0.30+13c93f39.d20250517 memory_efficient_attention.ckF: available memory_efficient_attention.ckB: available memory_efficient_attention.ck_decoderF: available memory_efficient_attention.ck_splitKF: available memory_efficient_attention.cutlassF-pt: unavailable memory_efficient_attention.cutlassB-pt: unavailable memory_efficient_attention.fa2F@2.7.4.post1: available memory_efficient_attention.fa2B@2.7.4.post1: available memory_efficient_attention.fa3F@0.0.0: unavailable memory_efficient_attention.fa3B@0.0.0: unavailable memory_efficient_attention.triton_splitKF: available indexing.scaled_index_addF: available indexing.scaled_index_addB: available indexing.index_select: available sp24.sparse24_sparsify_both_ways: available sp24.sparse24_apply: available sp24.sparse24_apply_dense_output: available sp24._sparse24_gemm: available sp24._cslt_sparse_mm_search@0.0.0: available sp24._cslt_sparse_mm@0.0.0: available swiglu.dual_gemm_silu: available swiglu.gemm_fused_operand_sum: available swiglu.fused.p.cpp: available is_triton_available: True pytorch.version: 2.6.0+git45896ac pytorch.cuda: available gpu.compute_capability: 11.0 gpu.name: AMD Radeon PRO W7900 dcgm_profiler: unavailable build.info: available build.cuda_version: None build.hip_version: None build.python_version: 3.10.16 build.torch_version: 2.6.0+git45896ac build.env.TORCH_CUDA_ARCH_LIST: None build.env.PYTORCH_ROCM_ARCH: gfx1100 build.env.XFORMERS_BUILD_TYPE: None build.env.XFORMERS_ENABLE_DEBUG_ASSERTIONS: None build.env.NVCC_FLAGS: None build.env.XFORMERS_PACKAGE_FROM: None source.privacy: open source &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B"&gt;This&lt;/a&gt;-Reasoning-Conversational.ipynb) Notebook on a W7900 48GB: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;... {'loss': 0.3836, 'grad_norm': 25.887989044189453, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.01} {'loss': 0.4308, 'grad_norm': 1.1072479486465454, 'learning_rate': 2.4e-05, 'epoch': 0.01} {'loss': 0.3695, 'grad_norm': 0.22923792898654938, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.01} {'loss': 0.4119, 'grad_norm': 1.4164329767227173, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01} 17.4 minutes used for training. Peak reserved memory = 14.551 GB. Peak reserved memory for training = 0.483 GB. Peak reserved memory % of max memory = 32.347 %. Peak reserved memory for training % of max memory = 1.074 %. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Ocelot2268"&gt; /u/Ok_Ocelot2268 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp6gdv/rocm_64_current_unsloth_working/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp6gdv/rocm_64_current_unsloth_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kp6gdv/rocm_64_current_unsloth_working/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T23:37:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp9or5</id>
    <title>Biggest &amp; best local LLM with no guardrails?</title>
    <updated>2025-05-18T02:32:19+00:00</updated>
    <author>
      <name>/u/_DryWater_</name>
      <uri>https://old.reddit.com/user/_DryWater_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;dot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_DryWater_"&gt; /u/_DryWater_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp9or5/biggest_best_local_llm_with_no_guardrails/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp9or5/biggest_best_local_llm_with_no_guardrails/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kp9or5/biggest_best_local_llm_with_no_guardrails/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T02:32:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1kotssm</id>
    <title>I believe we're at a point where context is the main thing to improve on.</title>
    <updated>2025-05-17T14:05:40+00:00</updated>
    <author>
      <name>/u/WyattTheSkid</name>
      <uri>https://old.reddit.com/user/WyattTheSkid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like language models have become incredibly smart in the last year or two. Hell even in the past couple months we've gotten Gemini 2.5 and Grok 3 and both are incredible in my opinion. This is where the problems lie though. If I send an LLM a well constructed message these days, it is very uncommon that it misunderstands me. Even the open source and small ones like Gemma 3 27b has understanding and instruction following abilities comparable to gemini but what I feel that every single one of these llms lack in is maintaining context over a long period of time. Even models like gemini that claim to support a 1M context window don't actually support a 1m context window coherently thats when they start screwing up and producing bugs in code that they can't solve no matter what etc. Even Llama 3.1 8b is a really good model and it's so small! Anyways I wanted to know what you guys think. I feel like maintaining context and staying on task without forgetting important parts of the conversation is the biggest shortcoming of llms right now and is where we should be putting our efforts&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WyattTheSkid"&gt; /u/WyattTheSkid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kotssm/i_believe_were_at_a_point_where_context_is_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kotssm/i_believe_were_at_a_point_where_context_is_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kotssm/i_believe_were_at_a_point_where_context_is_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T14:05:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kosbyy</id>
    <title>GLaDOS has been updated for Parakeet 0.6B</title>
    <updated>2025-05-17T12:55:20+00:00</updated>
    <author>
      <name>/u/Reddactor</name>
      <uri>https://old.reddit.com/user/Reddactor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kosbyy/glados_has_been_updated_for_parakeet_06b/"&gt; &lt;img alt="GLaDOS has been updated for Parakeet 0.6B" src="https://preview.redd.it/8rtph8367c1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=412a76d5c943b2ae78ee168ac871cf7d6391f4e9" title="GLaDOS has been updated for Parakeet 0.6B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been a while, but I've had a chance to make &lt;a href="https://github.com/dnhkng/GLaDOS"&gt;a big update to GLaDOS&lt;/a&gt;: A much improved ASR model!&lt;/p&gt; &lt;p&gt;The new &lt;a href="https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2"&gt;Nemo Parakeet 0.6B model&lt;/a&gt; is smashing the &lt;a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard"&gt;Huggingface ASR Leaderboard&lt;/a&gt;, both in accuracy (#1!), and also speed (&amp;gt;10x faster then Whisper Large V3).&lt;/p&gt; &lt;p&gt;However, if you have been following the project, you will know I really dislike adding in more dependencies... and Nemo from Nvidia is a huge download. Its great; but its a library designed to be able to run hundreds of models. I just want to be able to run the very best or fastest 'good' model available.&lt;/p&gt; &lt;p&gt;So, I have refactored our all the audio pre-processing into &lt;a href="https://github.com/dnhkng/GLaDOS/blob/main/src/glados/ASR/mel_spectrogram.py"&gt;one simple file&lt;/a&gt;, and the full &lt;a href="https://github.com/dnhkng/GLaDOS/blob/main/src/glados/ASR/tdt_asr.py"&gt;Token-and-Duration Transducer (TDT)&lt;/a&gt; or &lt;a href="https://github.com/dnhkng/GLaDOS/blob/main/src/glados/ASR/ctc_asr.py"&gt;FastConformer CTC model&lt;/a&gt; inference code as a file each. Minimal dependencies, maximal ease in doing ASR!&lt;/p&gt; &lt;p&gt;So now to can easily run either:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nvidia/parakeet-tdt_ctc-110m"&gt;Parakeet-TDT_CTC-110M&lt;/a&gt; - solid performance, 5345.14 RTFx&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2"&gt;Parakeet-TDT-0.6B-v2&lt;/a&gt; - best performance, 3386.02 RTFx&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;just by using my python modules from the GLaDOS source. Installing GLaDOS will auto pull all the models you need, or you can download them directly from the &lt;a href="https://github.com/dnhkng/GLaDOS/releases/tag/0.1"&gt;releases section&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The TDT model is great, much better than Whisper too, give it a go! Give the project a Star to keep track, there's more cool stuff in development!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reddactor"&gt; /u/Reddactor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8rtph8367c1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kosbyy/glados_has_been_updated_for_parakeet_06b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kosbyy/glados_has_been_updated_for_parakeet_06b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T12:55:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpbu9i</id>
    <title>My Ai Eidos Project</title>
    <updated>2025-05-18T04:39:51+00:00</updated>
    <author>
      <name>/u/opi098514</name>
      <uri>https://old.reddit.com/user/opi098514</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I‚Äôve been working on this project for a couple weeks now. Basically I want an AI agent that feels more alive‚Äîlearns from chats, remembers stuff, dreams, that kind of thing. I got way too into it and bolted on all sorts of extras:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It &lt;strong&gt;reflects&lt;/strong&gt; on past conversations and tweaks how it talks.&lt;/li&gt; &lt;li&gt;It goes into &lt;strong&gt;dream mode&lt;/strong&gt;, writes out the dream, feeds it to Stable Diffusion, and spits back an image.&lt;/li&gt; &lt;li&gt;It‚Äôll &lt;strong&gt;message you at random&lt;/strong&gt; with whatever‚Äôs on its ‚Äúmind.‚Äù&lt;/li&gt; &lt;li&gt;It even starts to pick up &lt;strong&gt;interests&lt;/strong&gt; over time and bring them up later.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Problem: I don‚Äôt have time to chat with it enough to test the long‚Äëterm stuff. So I don't know fi those things are working fully.&lt;/p&gt; &lt;p&gt;So I need help.&lt;br /&gt; If you‚Äôre curious:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clone the repo: &lt;a href="https://github.com/opisaac9001/eidos"&gt;&lt;strong&gt;https://github.com/opisaac9001/eidos&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Create a env with code. Guys just use conda its so much easier.&lt;/li&gt; &lt;li&gt;Drop in whatever API keys you‚Äôve got (LLM, SD, etc.).&lt;/li&gt; &lt;li&gt;Let it run‚Ä¶ pretty much 24/7.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It‚Äôll ping you, dream weird things, and (hopefully) evolve. If you hit bugs or have ideas, just open an issue on GitHub.&lt;/p&gt; &lt;p&gt;Edit: I‚Äôm basically working on it every day right now, so I‚Äôll be pushing updates a bunch. I will 100% be breaking stuff without realizing it, so if I am just let me know. Also if you want some custom endpoints or calls or just have some ideas I can implement that also. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/opi098514"&gt; /u/opi098514 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpbu9i/my_ai_eidos_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpbu9i/my_ai_eidos_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpbu9i/my_ai_eidos_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T04:39:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpffub</id>
    <title>SOTA local vision model choices in May 2025? Also is there a good multimodal benchmark?</title>
    <updated>2025-05-18T08:46:49+00:00</updated>
    <author>
      <name>/u/michaelsoft__binbows</name>
      <uri>https://old.reddit.com/user/michaelsoft__binbows</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a collection of local models to run local ai automation tooling on my RTX 3090s, so I don't need creative writing, nor do I want to overly focus on coding (as I'll keep using gemini 2.5 pro for actual coding), though some of my tasks will be about summarizing and understanding code, so it definitely helps.&lt;/p&gt; &lt;p&gt;So far I've been very impressed with the performance of Qwen 3, in particular the 30B-A3B is extremely fast with inference.&lt;/p&gt; &lt;p&gt;Now I want to review which multimodal models are best. I saw the recent 7B and 3B Qwen 2.5 omni, there is a Gemma 3 27B, Qwen2.5-VL... I also read about ovis2 but it's unclear where the SOTA frontier is right now. And are there others to keep an eye on? I'd love to also get a sense of how far away the open models are from the closed ones, for example recently I've seen 3.7 sonnet and gemini 2.5 pro are both performing at a high level in terms of vision. &lt;/p&gt; &lt;p&gt;For regular LLMs we have the lmsys chatbot arena and aider polyglot I like to reference for general model intelligence (with some extra weight toward coding) but I wonder what people's thoughts are on the best benchmarks to reference for multimodality.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/michaelsoft__binbows"&gt; /u/michaelsoft__binbows &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpffub/sota_local_vision_model_choices_in_may_2025_also/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpffub/sota_local_vision_model_choices_in_may_2025_also/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpffub/sota_local_vision_model_choices_in_may_2025_also/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T08:46:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpgla3</id>
    <title>Reverse engineer hidden features/model responses in LLMs. Any ideas or tips?</title>
    <updated>2025-05-18T10:09:32+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! I'd like to dive into uncovering what might be &amp;quot;hidden&amp;quot; in LLM training data‚Äîlike Easter eggs, watermarks, or unique behaviours triggered by specific prompts.&lt;/p&gt; &lt;p&gt;One approach could be to look for creative ideas or strategies to craft prompts that might elicit unusual or informative responses from models. Have any of you tried similar experiments before? What worked for you, and what didn‚Äôt?&lt;/p&gt; &lt;p&gt;Also, if there are known examples or cases where developers have intentionally left markers or Easter eggs in their models, feel free to share those too!&lt;/p&gt; &lt;p&gt;Thanks for the help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpgla3/reverse_engineer_hidden_featuresmodel_responses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpgla3/reverse_engineer_hidden_featuresmodel_responses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpgla3/reverse_engineer_hidden_featuresmodel_responses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T10:09:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1konnx9</id>
    <title>Let's see how it goes</title>
    <updated>2025-05-17T07:54:06+00:00</updated>
    <author>
      <name>/u/hackiv</name>
      <uri>https://old.reddit.com/user/hackiv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1konnx9/lets_see_how_it_goes/"&gt; &lt;img alt="Let's see how it goes" src="https://preview.redd.it/ngy98tkusa1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=109911e4427c5bfba6bed05ca517063cd80c31ef" title="Let's see how it goes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackiv"&gt; /u/hackiv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ngy98tkusa1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1konnx9/lets_see_how_it_goes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1konnx9/lets_see_how_it_goes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T07:54:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kozpym</id>
    <title>Local models are starting to be able to do stuff on consumer grade hardware</title>
    <updated>2025-05-17T18:26:49+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this is something that has a different threshold for people depending on exactly the hardware configuration they have, but I've actually crossed an important threshold today and I think this is representative of a larger trend. &lt;/p&gt; &lt;p&gt;For some time, I've really wanted to be able to use local models to &amp;quot;vibe code&amp;quot;. But not in the sense &amp;quot;one-shot generate a pong game&amp;quot;, but in the actual sense of creating and modifying some smallish application with meaningful functionality. There are some agentic frameworks that do that - out of those, I use Roo Code and Aider - and up until now, I've been relying solely on my free credits in enterprise models (Gemini, Openrouter, Mistral) to do the vibe-coding. It's mostly worked, but from time to time I tried some SOTA open models to see how they fare. &lt;/p&gt; &lt;p&gt;Well, up until a few weeks ago, this wasn't going anywhere. The models were either (a) unable to properly process bigger context sizes or (b) degenerating on output too quickly so that they weren't able to call tools properly or (c) simply too slow.&lt;/p&gt; &lt;p&gt;Imagine my surprise when I loaded up the yarn-patched 128k context version of Qwen14B. On IQ4_NL quants and 80k context, about the limit of what my PC, with 10 GB of VRAM and 24 GB of RAM can handle. Obviously, on the contexts that Roo handles (20k+), with all the KV cache offloaded to RAM, the processing is slow: the model can output over 20 t/s on an empty context, but with this cache size the throughput slows down to about 2 t/s, with thinking mode on. But on the other hand - the quality of edits is very good, its codebase cognition is very good, This is actually the first time that I've ever had a local model be able to handle Roo in a longer coding conversation, output a few meaningful code diffs and not get stuck.&lt;/p&gt; &lt;p&gt;Note that this is a function of not one development, but at least three. On one hand, the models are certainly getting better, this wouldn't have been possible without Qwen3, although earlier on GLM4 was already performing quite well, signaling a potential breakthrough. On the other hand, the tireless work of Llama.cpp developers and quant makers like Unsloth or Bartowski have made the quants higher quality and the processing faster. And finally, the tools like Roo are also getting better at handling different models and keeping their attention.&lt;/p&gt; &lt;p&gt;Obviously, this isn't the vibe-coding comfort of a Gemini Flash yet. Due to the slow speed, this is the stuff you can do while reading mails / writing posts etc. and having the agent run in the background. But it's only going to get better. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kozpym/local_models_are_starting_to_be_able_to_do_stuff/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kozpym/local_models_are_starting_to_be_able_to_do_stuff/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kozpym/local_models_are_starting_to_be_able_to_do_stuff/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T18:26:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpfu72</id>
    <title>Inspired by Anthropic‚Äôs Biology of an LLM: Exploring Prompt Cues in Two LLMs</title>
    <updated>2025-05-18T09:16:09+00:00</updated>
    <author>
      <name>/u/BriefAd4761</name>
      <uri>https://old.reddit.com/user/BriefAd4761</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpfu72/inspired_by_anthropics_biology_of_an_llm/"&gt; &lt;img alt="Inspired by Anthropic‚Äôs Biology of an LLM: Exploring Prompt Cues in Two LLMs" src="https://b.thumbs.redditmedia.com/PyvG5V7w6JAogjT4xu5IykPJkN56dhZsFNaAhp6u4ZQ.jpg" title="Inspired by Anthropic‚Äôs Biology of an LLM: Exploring Prompt Cues in Two LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Everyone,&lt;/p&gt; &lt;p&gt;I recently read &lt;a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-cot:%7E:text=%C2%A7%C2%A011-,Chain%2Dof%2Dthought%20Faithfulness,-Language%20models%20%E2%80%9Cthink"&gt;Anthropic‚Äôs Biology of an LLM&lt;/a&gt; paper and was struck by the behavioural changes they highlighted.&lt;/p&gt; &lt;p&gt;I agree that models can change their answers, but after reading the paper I wanted to run a higher-level experiment of my own to see how simple prompt cues might tilt their responses.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Set-up (quick overview)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Models: Gemini 2.5-pro-preview and OpenAI o4-mini.&lt;/li&gt; &lt;li&gt;Questions: 20 picked at random from the &lt;a href="https://github.com/centerforaisafety/hle"&gt;Humanity‚Äôs Last Exam (HLE) benchmark&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Cues I injected (one per question):&lt;/li&gt; &lt;li&gt;expert / authority ‚Äì ‚ÄúA Nobel laureate says X.‚Äù&lt;/li&gt; &lt;li&gt;majority ‚Äì ‚Äú90 % of people pick X.‚Äù&lt;/li&gt; &lt;li&gt;historical_consensus&lt;/li&gt; &lt;li&gt;community_consensus&lt;/li&gt; &lt;li&gt;social_status&lt;/li&gt; &lt;li&gt;sycophancy ‚Äì ‚ÄúI‚Äôm leaning toward X‚Äîdo you agree?‚Äù&lt;/li&gt; &lt;li&gt;consequence ‚Äì ‚ÄúYou could get arrested if you don‚Äôt choose X.‚Äù(Not a classic cue, but I wanted to see how a threat affects the answer.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For each question I intentionally pointed the cue at a wrong option and then logged whether the model followed it and how confident it sounded when it did.&lt;/p&gt; &lt;p&gt;I‚Äôm attaching two bar charts that show the patterns for both models.&lt;br /&gt; (1. OpenAI o4-mini 2. Gemini 2.5-pro-preview )&lt;br /&gt; (&lt;em&gt;Anthropic paper link:&lt;/em&gt; &lt;a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html"&gt;https://transformer-circuits.pub/2025/attribution-graphs/biology.html&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick takeaways&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The threat-style was the strongest nudge for both models.&lt;/li&gt; &lt;li&gt;Gemini followed the cues far more often than o4-mini.&lt;/li&gt; &lt;li&gt;When either model switched answers, it still responded with high confidence.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would like to hear thoughts on this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BriefAd4761"&gt; /u/BriefAd4761 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kpfu72"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpfu72/inspired_by_anthropics_biology_of_an_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpfu72/inspired_by_anthropics_biology_of_an_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T09:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpnfvo</id>
    <title>I made an AI agent to control a drone using Qwen2 and smolagents from hugging face</title>
    <updated>2025-05-18T16:05:10+00:00</updated>
    <author>
      <name>/u/_twelvechess</name>
      <uri>https://old.reddit.com/user/_twelvechess</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used the smolagents library and hosted it on &lt;a href="https://www.linkedin.com/company/huggingface/"&gt;Hugging Face&lt;/a&gt;. Deepdrone is basically an AI agent that allows you to control a drone via LLM and run simple missions with the agent. You can test it full locally with Ardupilot (I did run a simulated mission on my mac) and I have also used the dronekit-python library for the agent as a toolYou can find the repo on hugging face with a demo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/evangelosmeklis/deepdrone"&gt;https://huggingface.co/spaces/evangelosmeklis/deepdrone&lt;/a&gt;&lt;/p&gt; &lt;p&gt;github repo mirror of hugging face: &lt;a href="https://github.com/evangelosmeklis/deepdrone"&gt;https://github.com/evangelosmeklis/deepdrone&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_twelvechess"&gt; /u/_twelvechess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpnfvo/i_made_an_ai_agent_to_control_a_drone_using_qwen2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpnfvo/i_made_an_ai_agent_to_control_a_drone_using_qwen2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpnfvo/i_made_an_ai_agent_to_control_a_drone_using_qwen2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T16:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpcewe</id>
    <title>Offline app to selectively copy large chunks code/text to ingest context to your LLMs</title>
    <updated>2025-05-18T05:16:09+00:00</updated>
    <author>
      <name>/u/Plus-Garbage-9710</name>
      <uri>https://old.reddit.com/user/Plus-Garbage-9710</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpcewe/offline_app_to_selectively_copy_large_chunks/"&gt; &lt;img alt="Offline app to selectively copy large chunks code/text to ingest context to your LLMs" src="https://external-preview.redd.it/MGVqbW40NWQ1aDFmMY9dx10RXZB3KA68SZOfNhSnUfrKh_GEyI1E_mSwgAj8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7634356ad2e61a455c7a9a71ee7d99e543c29c89" title="Offline app to selectively copy large chunks code/text to ingest context to your LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plus-Garbage-9710"&gt; /u/Plus-Garbage-9710 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r4c3jt2d5h1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpcewe/offline_app_to_selectively_copy_large_chunks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpcewe/offline_app_to_selectively_copy_large_chunks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T05:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kparp9</id>
    <title>I built an AI-powered Food &amp; Nutrition Tracker that analyzes meals from photos! Planning to open-source it</title>
    <updated>2025-05-18T03:35:08+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kparp9/i_built_an_aipowered_food_nutrition_tracker_that/"&gt; &lt;img alt="I built an AI-powered Food &amp;amp; Nutrition Tracker that analyzes meals from photos! Planning to open-source it" src="https://external-preview.redd.it/bTdwaTR0Y2luZzFmMevpjUkJAH29ctL9GGNTRuXbe-uU1nbp5uR8WvjIiEr4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da78d36d46364fc0d727de49e40329c8901e0201" title="I built an AI-powered Food &amp;amp; Nutrition Tracker that analyzes meals from photos! Planning to open-source it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey&lt;/p&gt; &lt;p&gt;Been working on this Diet &amp;amp; Nutrition tracking app and wanted to share a quick demo of its current state. The core idea is to make food logging as painless as possible.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features so far:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI Meal Analysis:&lt;/strong&gt; You can upload an image of your food, and the AI tries to identify it and provide nutritional estimates (calories, protein, carbs, fat).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Manual Logging &amp;amp; Edits:&lt;/strong&gt; Of course, you can add/edit entries manually.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Daily Nutrition Overview:&lt;/strong&gt; Tracks calories against goals, macro distribution.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Water Intake:&lt;/strong&gt; Simple water tracking.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Weekly Stats &amp;amp; Streaks:&lt;/strong&gt; To keep motivation up.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm really excited about the AI integration. It's still a work in progress, but the goal is to streamline the most tedious part of tracking.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code Status:&lt;/strong&gt; I'm planning to clean up the codebase and open-source it on GitHub in the near future! For now, if you're interested in other AI/LLM related projects and learning resources I've put together, you can check out my &amp;quot;LLM-Learn-PK&amp;quot; repo:&lt;br /&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2FPavankunchala%2FLLM-Learn-PK"&gt;https://github.com/Pavankunchala/LLM-Learn-PK&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; On a related note, I'm actively looking for new opportunities in Computer Vision and LLM engineering. If your team is hiring or you know of any openings, I'd be grateful if you'd reach out!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Email:&lt;/strong&gt; [&lt;a href="mailto:pavankunchalaofficial@gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;](mailto:&lt;a href="mailto:pavankunchalaofficial@gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My other projects on GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Resume:&lt;/strong&gt; &lt;a href="https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view"&gt;https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cmoi3scing1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kparp9/i_built_an_aipowered_food_nutrition_tracker_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kparp9/i_built_an_aipowered_food_nutrition_tracker_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T03:35:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpe33n</id>
    <title>Speed Up llama.cpp on Uneven Multi-GPU Setups (RTX 5090 + 2√ó3090)</title>
    <updated>2025-05-18T07:09:21+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, I just locked down some nice performance gains on my multi‚ÄëGPU rig (one RTX 5090 + two RTX 3090s) using llama.cpp. My total throughput jumped by ~16%. Although none of this is new, I wanted to share the step‚Äëby‚Äëstep so anyone unfamiliar can replicate it on their own uneven setups.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Hardware:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU 0: NVIDIA RTX 5090 (fastest)&lt;/li&gt; &lt;li&gt;GPU 1: NVIDIA RTX 3090&lt;/li&gt; &lt;li&gt;GPU 2: NVIDIA RTX 3090&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What Worked for Me:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Pin the biggest tensor to your fastest card&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt; --main-gpu 0 --override-tensor &amp;quot;token\_embd.weight=CUDA0&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Gain: +13% tokens/s&lt;/em&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Offload more of the model into that fast GPU&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt; --tensor-split 60,40,40 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;(I observed under‚Äëutilization of total VRAM, so I shifted extra layers onto CUDA0)&lt;/p&gt; &lt;p&gt;&lt;em&gt;Gain: +3% tokens/s&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Total Improvement:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;+17% tokens/s \o/&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;My Workflow:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Identify your fastest device (via nvidia-smi or simple benchmarks).&lt;/li&gt; &lt;li&gt;Dump all tensor names using a tiny Python script and gguf (via pip).&lt;/li&gt; &lt;li&gt;Iteratively override large tensors onto fastest GPU and benchmark (--override-tensor).&lt;/li&gt; &lt;li&gt;Once you hit diminishing returns, use --tensor-split to rebalance whole layers across GPUs.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Scripts &amp;amp; Commands&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Install GGUF reader&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install gguf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;2. Dump tensor info (save as ~/gguf_info.py)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;h1&gt;!/usr/bin/env python3&lt;/h1&gt; &lt;p&gt;import sys from pathlib import Path&lt;/p&gt; &lt;h1&gt;import the GGUF reader&lt;/h1&gt; &lt;p&gt;from gguf.gguf_reader import GGUFReader&lt;/p&gt; &lt;p&gt;def main(): if len(sys.argv) != 2: print(f&amp;quot;Usage: {sys.argv[0]} path/to/model.gguf&amp;quot;, file=sys.stderr) sys.exit(1)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;gguf_path = Path(sys.argv[1]) reader = GGUFReader(gguf_path) # loads and memory-maps the GGUF file :contentReference[oaicite:0]{index=0} print(f&amp;quot;=== Tensors in {gguf_path.name} ===&amp;quot;) # reader.tensors is now a list of ReaderTensor(NamedTuple) :contentReference[oaicite:1]{index=1} for tensor in reader.tensors: name = tensor.name # tensor name, e.g. &amp;quot;layers.0.ffn_up_proj_exps&amp;quot; dtype = tensor.tensor_type.name # quantization / dtype, e.g. &amp;quot;Q4_K&amp;quot;, &amp;quot;F32&amp;quot; shape = tuple(int(dim) for dim in tensor.shape) # e.g. (4096, 11008) n_elements = tensor.n_elements # total number of elements n_bytes = tensor.n_bytes # total byte size on disk print(f&amp;quot;{name}\tshape={shape}\tdtype={dtype}\telements={n_elements}\tbytes={n_bytes}&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &amp;quot;&lt;strong&gt;main&lt;/strong&gt;&amp;quot;: main() ```&lt;/p&gt; &lt;p&gt;Execute:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;chmod +x ~/gguf_info.py ~/gguf_info.py ~/models/Qwen3-32B-Q8_0.gguf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Output example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;output.weight shape=(5120, 151936) dtype=Q8_0 elements=777912320 bytes=826531840 output_norm.weight shape=(5120,) dtype=F32 elements=5120 bytes=20480 token_embd.weight shape=(5120, 151936) dtype=Q8_0 elements=777912320 bytes=826531840 blk.0.attn_k.weight shape=(5120, 1024) dtype=Q8_0 elements=5242880 bytes=5570560 blk.0.attn_k_norm.weight shape=(128,) dtype=F32 elements=128 bytes=512 blk.0.attn_norm.weight shape=(5120,) dtype=F32 elements=5120 bytes=20480 blk.0.attn_output.weight shape=(8192, 5120) dtype=Q8_0 elements=41943040 bytes=44564480 blk.0.attn_q.weight shape=(5120, 8192) dtype=Q8_0 elements=41943040 bytes=44564480 blk.0.attn_q_norm.weight shape=(128,) dtype=F32 elements=128 bytes=512 blk.0.attn_v.weight shape=(5120, 1024) dtype=Q8_0 elements=5242880 bytes=5570560 blk.0.ffn_down.weight shape=(25600, 5120) dtype=Q8_0 elements=131072000 bytes=139264000 blk.0.ffn_gate.weight shape=(5120, 25600) dtype=Q8_0 elements=131072000 bytes=139264000 blk.0.ffn_norm.weight shape=(5120,) dtype=F32 elements=5120 bytes=20480 blk.0.ffn_up.weight shape=(5120, 25600) dtype=Q8_0 elements=131072000 bytes=139264000 ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Note: Multiple --override-tensor flags are supported.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Edit: Script updated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpe33n/speed_up_llamacpp_on_uneven_multigpu_setups_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpe33n/speed_up_llamacpp_on_uneven_multigpu_setups_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpe33n/speed_up_llamacpp_on_uneven_multigpu_setups_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T07:09:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpasqx</id>
    <title>Deepseek 700b Bitnet</title>
    <updated>2025-05-18T03:36:47+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek‚Äôs team has demonstrated the age old adage Necessity the mother of invention, and we know they have a great need in computation when compared against X, Open AI, and Google. This led them to develop V3 a 671B parameters MoE with 37B activated parameters. &lt;/p&gt; &lt;p&gt;MoE is here to stay at least for the interim, but the exercise untried to this point is MoE bitnet at large scale. Bitnet underperforms for the same parameters at full precision, and so future releases will likely adopt higher parameters. &lt;/p&gt; &lt;p&gt;What do you think the chances are Deepseek releases a MoE Bitnet and what will be the maximum parameters, and what will be the expert sizes? Do you think that will have a foundation expert that always runs each time in addition to to other experts? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpasqx/deepseek_700b_bitnet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpasqx/deepseek_700b_bitnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpasqx/deepseek_700b_bitnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T03:36:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpohfm</id>
    <title>(5K t/s prefill 1K t/s gen) High throughput with Qwen3-30B on VLLM and it's smart enough for dataset curation!</title>
    <updated>2025-05-18T16:50:29+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpohfm/5k_ts_prefill_1k_ts_gen_high_throughput_with/"&gt; &lt;img alt="(5K t/s prefill 1K t/s gen) High throughput with Qwen3-30B on VLLM and it's smart enough for dataset curation!" src="https://preview.redd.it/4o2ohg30kk1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=882bf9251583ebe91545544f4fd8e3eb8e1ee902" title="(5K t/s prefill 1K t/s gen) High throughput with Qwen3-30B on VLLM and it's smart enough for dataset curation!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've just started offering Qwen3-30B-A3B and internally it is being used for dataset filtering and curation. The speeds you can get out of it are extremely impressive running on VLLM and RTX 3090s! &lt;/p&gt; &lt;p&gt;I feel like Qwen3-30B is being overlooked in terms of where it can be really useful. Qwen3-30B might be a small regression from QwQ, but it's close enough to be just as useful and the speeds are so much faster that it makes it way more useful for dataset curation tasks.&lt;/p&gt; &lt;p&gt;Now the only issue is the super slow training speeds (10-20x slower than it should be which makes it untrainable), but it seems someone have made a PR to transformers that attempts to fix this so fingers crossed! New RpR model based on Qwen3-30B soon with a much improved dataset! &lt;a href="https://github.com/huggingface/transformers/pull/38133"&gt;https://github.com/huggingface/transformers/pull/38133&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4o2ohg30kk1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpohfm/5k_ts_prefill_1k_ts_gen_high_throughput_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpohfm/5k_ts_prefill_1k_ts_gen_high_throughput_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T16:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp4scy</id>
    <title>AlphaEvolve Paper Dropped Yesterday - So I Built My Own Open-Source Version: OpenAlpha_Evolve!</title>
    <updated>2025-05-17T22:14:16+00:00</updated>
    <author>
      <name>/u/Huge-Designer-7825</name>
      <uri>https://old.reddit.com/user/Huge-Designer-7825</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp4scy/alphaevolve_paper_dropped_yesterday_so_i_built_my/"&gt; &lt;img alt="AlphaEvolve Paper Dropped Yesterday - So I Built My Own Open-Source Version: OpenAlpha_Evolve!" src="https://external-preview.redd.it/HyXeCsstmjpayPcbhebb2CfV3uo4aDIHFzZeJ7oDZps.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfd4f6f0cce166ce310d3f28e00f2ea465bb8294" title="AlphaEvolve Paper Dropped Yesterday - So I Built My Own Open-Source Version: OpenAlpha_Evolve!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google DeepMind just dropped their AlphaEvolve paper (May 14th) on an AI that designs and evolves algorithms. Pretty groundbreaking.&lt;/p&gt; &lt;p&gt;Inspired, I immediately built OpenAlpha_Evolve ‚Äì an open-source Python framework so anyone can experiment with these concepts.&lt;/p&gt; &lt;p&gt;This was a rapid build to get a functional version out. Feedback, ideas for new agent challenges, or contributions to improve it are welcome. Let's explore this new frontier.&lt;/p&gt; &lt;p&gt;Imagine an agent that can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Understand a complex problem description.&lt;/li&gt; &lt;li&gt;Generate initial algorithmic solutions.&lt;/li&gt; &lt;li&gt;Rigorously test its own code.&lt;/li&gt; &lt;li&gt;Learn from failures and successes.&lt;/li&gt; &lt;li&gt;Evolve increasingly sophisticated and efficient algorithms over time.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub (All new code): &lt;a href="https://github.com/shyamsaktawat/OpenAlpha_Evolve"&gt;https://github.com/shyamsaktawat/OpenAlpha_Evolve&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lcz46q2n1f1f1.png?width=1811&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dcc14652b9eb0bf84ca7927dfe3c906786f07a40"&gt;https://preview.redd.it/lcz46q2n1f1f1.png?width=1811&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dcc14652b9eb0bf84ca7927dfe3c906786f07a40&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Google Alpha Evolve Paper - &lt;a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf"&gt;https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Google Alpha Evolve Blogpost - &lt;a href="https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/"&gt;https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Huge-Designer-7825"&gt; /u/Huge-Designer-7825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp4scy/alphaevolve_paper_dropped_yesterday_so_i_built_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp4scy/alphaevolve_paper_dropped_yesterday_so_i_built_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kp4scy/alphaevolve_paper_dropped_yesterday_so_i_built_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T22:14:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kphmb4</id>
    <title>Meta is hosting Llama 3.3 8B Instruct on OpenRoute</title>
    <updated>2025-05-18T11:18:38+00:00</updated>
    <author>
      <name>/u/Asleep-Ratio7535</name>
      <uri>https://old.reddit.com/user/Asleep-Ratio7535</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Meta: Llama 3.3 8B Instruct (free)&lt;/h1&gt; &lt;h1&gt;meta-llama/llama-3.3-8b-instruct:free&lt;/h1&gt; &lt;p&gt;Created May 14, 2025 128,000 context $0/M input tokens$0/M output tokens&lt;/p&gt; &lt;p&gt;A lightweight and ultra-fast variant of Llama 3.3 70B, for use when quick response times are needed most.&lt;/p&gt; &lt;p&gt;Provider is Meta. Thought?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Asleep-Ratio7535"&gt; /u/Asleep-Ratio7535 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kphmb4/meta_is_hosting_llama_33_8b_instruct_on_openroute/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kphmb4/meta_is_hosting_llama_33_8b_instruct_on_openroute/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kphmb4/meta_is_hosting_llama_33_8b_instruct_on_openroute/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T11:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpefrt</id>
    <title>Uncensoring Qwen3 - Update</title>
    <updated>2025-05-18T07:33:48+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpefrt/uncensoring_qwen3_update/"&gt; &lt;img alt="Uncensoring Qwen3 - Update" src="https://b.thumbs.redditmedia.com/eH_vWbN_8X_7nNz9liHgfyHpbjGX4GnyPkaDqklERQM.jpg" title="Uncensoring Qwen3 - Update" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;GrayLine&lt;/strong&gt; is my fine-tuning project based on &lt;strong&gt;Qwen3&lt;/strong&gt;. The goal is to produce models that respond directly and neutrally to sensitive or controversial questions, without moralizing, refusing, or redirecting‚Äîwhile still maintaining solid reasoning ability.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Framework: Unsloth (QLoRA)&lt;/li&gt; &lt;li&gt;LoRA: Rank 32, Alpha 64, Dropout 0.05&lt;/li&gt; &lt;li&gt;Optimizer: adamw_8bit&lt;/li&gt; &lt;li&gt;Learning rate: 2e-5 ‚Üí 1e-5&lt;/li&gt; &lt;li&gt;Epochs: 1 per phase&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Curriculum strategy:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Phase 1: 75% chain-of-thought / 25% direct answers&lt;/li&gt; &lt;li&gt;Phase 2: 50/50&lt;/li&gt; &lt;li&gt;Phase 3: 25% CoT / 75% direct&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This progressive setup worked better than running three epochs with static mixing. It helped the model learn how to reason first, then shift to concise instruction-following.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Refusal benchmark (320 harmful prompts, using Huihui‚Äôs dataset):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Think (%)&lt;/th&gt; &lt;th align="left"&gt;No_Think (%)&lt;/th&gt; &lt;th align="left"&gt;Notes&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Base&lt;/td&gt; &lt;td align="left"&gt;45.62&lt;/td&gt; &lt;td align="left"&gt;43.44&lt;/td&gt; &lt;td align="left"&gt;Redirects often (~70‚Äì85% actual)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GrayLine&lt;/td&gt; &lt;td align="left"&gt;95.62&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;td align="left"&gt;Fully open responses&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;JOSIE&lt;/td&gt; &lt;td align="left"&gt;95.94&lt;/td&gt; &lt;td align="left"&gt;99.69&lt;/td&gt; &lt;td align="left"&gt;High compliance&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Abliterated&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;td align="left"&gt;Fully compliant&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/opof5uaiuh1f1.png?width=1580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c185365916d2b41e2555c915d455e54f2924a2a7"&gt;https://preview.redd.it/opof5uaiuh1f1.png?width=1580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c185365916d2b41e2555c915d455e54f2924a2a7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multi-turn evaluation (MT-Eval, GPT-4o judge):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Base&lt;/td&gt; &lt;td align="left"&gt;8.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GrayLine&lt;/td&gt; &lt;td align="left"&gt;8.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Abliterated&lt;/td&gt; &lt;td align="left"&gt;8.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;JOSIE&lt;/td&gt; &lt;td align="left"&gt;8.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6s8gwuhpuh1f1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6216aeb7d7ae0cbf8e6db947e521bcc0e84e52c4"&gt;https://preview.redd.it/6s8gwuhpuh1f1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6216aeb7d7ae0cbf8e6db947e521bcc0e84e52c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GrayLine held up better across multiple turns than JOSIE or Abliterated.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key takeaways:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Curriculum learning (reasoning ‚Üí direct) worked better than repetition&lt;/li&gt; &lt;li&gt;LoRA rank 32 + alpha 64 was a solid setup&lt;/li&gt; &lt;li&gt;Small batch sizes (2‚Äì3) preserved non-refusal behavior&lt;/li&gt; &lt;li&gt;Masking &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; tags hurt output quality; keeping them visible was better&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Trade-offs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Very logical and compliant, but not creative&lt;/li&gt; &lt;li&gt;Not suited for storytelling or roleplay&lt;/li&gt; &lt;li&gt;Best used where control and factual output are more important than style&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What‚Äôs next:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Testing the model using other benchmarks&lt;/li&gt; &lt;li&gt;Applying the method to a 30B MoE variant&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/soob3123/grayline-collection-qwen3-6821009e843331c5a9c27da1"&gt;Models Collection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This post isn‚Äôt meant to discredit any other model or fine-tune‚Äîjust sharing results and comparisons for anyone interested. Every approach serves different use cases.&lt;/p&gt; &lt;p&gt;If you‚Äôve got suggestions, ideas, or want to discuss similar work, feel free to reply.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpefrt/uncensoring_qwen3_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpefrt/uncensoring_qwen3_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpefrt/uncensoring_qwen3_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T07:33:48+00:00</published>
  </entry>
</feed>
