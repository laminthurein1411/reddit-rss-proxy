<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-24T21:23:05+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ktiq99</id>
    <title>I accidentally too many P100</title>
    <updated>2025-05-23T12:48:51+00:00</updated>
    <author>
      <name>/u/TooManyPascals</name>
      <uri>https://old.reddit.com/user/TooManyPascals</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/"&gt; &lt;img alt="I accidentally too many P100" src="https://b.thumbs.redditmedia.com/IdF4SU4XHKp-_JI6o-Y6kol8-cLrv94jdBxKlq9CTYI.jpg" title="I accidentally too many P100" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I had quite positive results with a P100 last summer, so when R1 came out, I decided to try if I could put 16 of them in a single pc... and I could.&lt;/p&gt; &lt;p&gt;Not the fastest think in the universe, and I am not getting awesome PCIE speed (2@4x). But it works, is still cheaper than a 5090, and I hope I can run stuff with large contexts.&lt;/p&gt; &lt;p&gt;I hoped to run llama4 with large context sizes, and scout runs almost ok, but llama4 as a model is abysmal. I tried to run Qwen3-235B-A22B, but the performance with llama.cpp is pretty terrible, and I haven't been able to get it working with the vllm-pascal (ghcr.io/sasha0552/vllm:latest).&lt;/p&gt; &lt;p&gt;If you have any pointers on getting Qwen3-235B to run with any sort of parallelism, or want me to benchmark any model, just say so!&lt;/p&gt; &lt;p&gt;The MB is a 2014 intel S2600CW with dual 8-core xeons, so CPU performance is rather low. I also tried to use MB with an EPYC, but it doesn't manage to allocate the resources to all PCIe devices.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TooManyPascals"&gt; /u/TooManyPascals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ktiq99"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T12:48:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ku7qe6</id>
    <title>AMD GPU support</title>
    <updated>2025-05-24T09:36:49+00:00</updated>
    <author>
      <name>/u/Fade_Yeti</name>
      <uri>https://old.reddit.com/user/Fade_Yeti</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all. &lt;/p&gt; &lt;p&gt;I am looking to upgrade the GPU in my server with something with more than 8GB VRAM. How is AMD in the space at the moment in regards to support on linux? &lt;/p&gt; &lt;p&gt;Here are the 3 options:&lt;/p&gt; &lt;p&gt;Radeon RX 7800 XT 16GB&lt;/p&gt; &lt;p&gt;GeForce RTX 4060 Ti 16GB&lt;/p&gt; &lt;p&gt;GeForce RTX 5060 Ti OC 16G&lt;/p&gt; &lt;p&gt;Any advice would be greatly appreciated&lt;/p&gt; &lt;p&gt;EDIT: Thanks for all the advice. I picked up a 4060 Ti 16GB for $370ish&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fade_Yeti"&gt; /u/Fade_Yeti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku7qe6/amd_gpu_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku7qe6/amd_gpu_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ku7qe6/amd_gpu_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T09:36:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kufdow</id>
    <title>Best model for captioning?</title>
    <updated>2025-05-24T16:17:39+00:00</updated>
    <author>
      <name>/u/thetobesgeorge</name>
      <uri>https://old.reddit.com/user/thetobesgeorge</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What‚Äôs the best model right now for captioning pictures?&lt;br /&gt; I‚Äôm just interested in playing around and captioning individual pictures on a one by one basis&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thetobesgeorge"&gt; /u/thetobesgeorge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kufdow/best_model_for_captioning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kufdow/best_model_for_captioning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kufdow/best_model_for_captioning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T16:17:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktsqit</id>
    <title>Best Vibe Code tools (like Cursor) but are free and use your own local LLM?</title>
    <updated>2025-05-23T19:46:58+00:00</updated>
    <author>
      <name>/u/StartupTim</name>
      <uri>https://old.reddit.com/user/StartupTim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen Cursor and how it works, and it looks pretty cool, but I rather use my own local hosted LLMs and not pay a usage fee to a 3rd party company.&lt;/p&gt; &lt;p&gt;Does anybody know of any good Vibe Coding tools, as good or better than Cursor, that run on your own local LLMs?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;p&gt;EDIT: Especially tools that integrate with ollama's API.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StartupTim"&gt; /u/StartupTim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktsqit/best_vibe_code_tools_like_cursor_but_are_free_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktsqit/best_vibe_code_tools_like_cursor_but_are_free_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktsqit/best_vibe_code_tools_like_cursor_but_are_free_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T19:46:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ku5cfe</id>
    <title>What Models for C/C++?</title>
    <updated>2025-05-24T06:48:07+00:00</updated>
    <author>
      <name>/u/Aroochacha</name>
      <uri>https://old.reddit.com/user/Aroochacha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using unsloth/Qwen2.5-Coder-32B-Instruct-128K-GGUF (int 8.) Worked great for small stuff (one header/.c implementation) moreover it hallucinated when I had it evaluate a kernel api I wrote. (6 files.)&lt;/p&gt; &lt;p&gt;What are people using? I am curious about any model that are good at C. Bonus if they are good at shader code.&lt;/p&gt; &lt;p&gt;I am running a RTX A6000 PRO 96GB card in a Razer Core X. Replaced my 3090 in the TB enclosure. Have a 4090 in the gaming rig.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aroochacha"&gt; /u/Aroochacha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku5cfe/what_models_for_cc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku5cfe/what_models_for_cc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ku5cfe/what_models_for_cc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T06:48:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuhlho</id>
    <title>How to get started with Local LLMs</title>
    <updated>2025-05-24T17:53:36+00:00</updated>
    <author>
      <name>/u/bull_bear25</name>
      <uri>https://old.reddit.com/user/bull_bear25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am python coder with good understanding of FastAPI and Pandas &lt;/p&gt; &lt;p&gt;I want to start on Local LLMs for building AI Agents. How do I get started&lt;/p&gt; &lt;p&gt;Do I need GPUs&lt;/p&gt; &lt;p&gt;Which are good resources?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bull_bear25"&gt; /u/bull_bear25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuhlho/how_to_get_started_with_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuhlho/how_to_get_started_with_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuhlho/how_to_get_started_with_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T17:53:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ku1444</id>
    <title>A Privacy-Focused Perplexity That Runs Locally on Your Phone</title>
    <updated>2025-05-24T02:28:43+00:00</updated>
    <author>
      <name>/u/Ssjultrainstnict</name>
      <uri>https://old.reddit.com/user/Ssjultrainstnict</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku1444/a_privacyfocused_perplexity_that_runs_locally_on/"&gt; &lt;img alt="A Privacy-Focused Perplexity That Runs Locally on Your Phone" src="https://external-preview.redd.it/H2OOCv1bv050E4CBNwcheCR0p5galvx3UpT4d2t0NLs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0dc808e4d075ada1aefee047e33becc1859e20d5" title="A Privacy-Focused Perplexity That Runs Locally on Your Phone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1ku1444/video/e80rh7mb5n2f1/player"&gt;https://reddit.com/link/1ku1444/video/e80rh7mb5n2f1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! üëã&lt;/p&gt; &lt;p&gt;I wanted to share &lt;strong&gt;MyDeviceAI&lt;/strong&gt; - a completely private alternative to Perplexity that runs entirely on your device. If you're tired of your search queries being sent to external servers and want the power of AI search without the privacy trade-offs, this might be exactly what you're looking for.&lt;/p&gt; &lt;h1&gt;What Makes This Different&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Complete Privacy&lt;/strong&gt;: Unlike Perplexity or other AI search tools, MyDeviceAI keeps everything local. Your search queries, the results, and all processing happen on your device. No data leaves your phone, period.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SearXNG Integration&lt;/strong&gt;: The app now comes with built-in SearXNG search - no configuration needed. You get comprehensive search results with image previews, all while maintaining complete privacy. SearXNG aggregates results from multiple search engines without tracking you.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Local AI Processing&lt;/strong&gt;: Powered by Qwen 3, the AI model runs entirely on your device. Modern iPhones get lightning-fast responses, and even older models are fully supported (just a bit slower).&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;100% Free &amp;amp; Open Source&lt;/strong&gt;: Check out the code at &lt;a href="http://github.com/navedmerchant/MyDeviceAI"&gt;MyDeviceAI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Web Search + AI&lt;/strong&gt;: Get the best of both worlds - current information from the web processed by local AI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat History&lt;/strong&gt;: 30+ days of conversation history, all stored locally&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Thinking Mode&lt;/strong&gt;: Complex reasoning capabilities for challenging problems&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero Wait Time&lt;/strong&gt;: Model loads asynchronously in the background&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Personalization&lt;/strong&gt;: Beta feature for custom user contexts&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Recent Updates&lt;/h1&gt; &lt;p&gt;The latest release includes a prettier UI, out-of-the-box SearXNG integration, image previews with search results, and tons of bug fixes.&lt;/p&gt; &lt;p&gt;This app has completely replaced ChatGPT for me, I am a very curious person and keep using it for looking up things that come to my mind, and its always spot on. I also compared it with Perplexity and while Perplexity has a slight edge in some cases, MyDeviceAI generally gives me the correct information and completely to the point. Download at: &lt;a href="https://apps.apple.com/us/app/mydeviceai/id6736578281"&gt;MyDeviceAI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking forward to your feedback. Please leave a review on the AppStore if this worked for you and solved a problem, and if you like to support further development of this App!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ssjultrainstnict"&gt; /u/Ssjultrainstnict &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku1444/a_privacyfocused_perplexity_that_runs_locally_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku1444/a_privacyfocused_perplexity_that_runs_locally_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ku1444/a_privacyfocused_perplexity_that_runs_locally_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T02:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kty4mh</id>
    <title>Anyone else prefering non thinking models ?</title>
    <updated>2025-05-23T23:50:26+00:00</updated>
    <author>
      <name>/u/StandardLovers</name>
      <uri>https://old.reddit.com/user/StandardLovers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So far Ive experienced non CoT models to have more curiosity and asking follow up questions. Like gemma3 or qwen2.5 72b. Tell them about something and they ask follow up questions, i think CoT models ask them selves all the questions and end up very confident. I also understand the strength of CoT models for problem solving, and perhaps thats where their strength is.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StandardLovers"&gt; /u/StandardLovers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kty4mh/anyone_else_prefering_non_thinking_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kty4mh/anyone_else_prefering_non_thinking_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kty4mh/anyone_else_prefering_non_thinking_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T23:50:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuf20u</id>
    <title>Best small model for code auto-completion?</title>
    <updated>2025-05-24T16:03:36+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am currently using the &lt;a href="http://continue.dev"&gt;continue.dev&lt;/a&gt; extension for VS Code. I want to use a small model for code autocompletion, something that is 3B or less as I intend to run it locally using llama.cpp (no gpu).&lt;/p&gt; &lt;p&gt;What would be a good model for such a use case?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuf20u/best_small_model_for_code_autocompletion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuf20u/best_small_model_for_code_autocompletion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuf20u/best_small_model_for_code_autocompletion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T16:03:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuk6ke</id>
    <title>Manifold v0.12.0 - ReAct Agent with MCP tools access.</title>
    <updated>2025-05-24T19:49:22+00:00</updated>
    <author>
      <name>/u/LocoMod</name>
      <uri>https://old.reddit.com/user/LocoMod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuk6ke/manifold_v0120_react_agent_with_mcp_tools_access/"&gt; &lt;img alt="Manifold v0.12.0 - ReAct Agent with MCP tools access." src="https://b.thumbs.redditmedia.com/uTr-VNiKMMa4VuWfO7BRJdQKVVJGuVCjgm0WHHZIa0s.jpg" title="Manifold v0.12.0 - ReAct Agent with MCP tools access." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Manifold is a platform for workflow automation using AI assistants. Please view the README for more example images. This has been mostly a solo effort and the scope is quite large so view this as an experimental hobby project not meant to be deployed to production systems (today). The documentation is non-existent, but I‚Äôm working on that. Manifold works with the popular public services as well as local OpenAI compatible endpoints such as llama.cpp and mlx_lm.server.&lt;/p&gt; &lt;p&gt;I highly recommend using capable OpenAI models, or Claude 3.7 for the agent configuration. I have also tested it with local models with success, but your configurations will vary. Gemma3 QAT with the latest improvements in llama.cpp also make it a great combination.&lt;/p&gt; &lt;p&gt;Be mindful that the MCP servers you configure will have a big impact on how the agent behaves. It is instructed to develop its own tool if a suitable one is not available. Manifold ships with a Dockerfile you can build with some basic MCP tools.&lt;/p&gt; &lt;p&gt;I highly recommend a good filesystem server such as &lt;a href="https://github.com/mark3labs/mcp-filesystem-server"&gt;https://github.com/mark3labs/mcp-filesystem-server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also highly recommend the official Playwright MCP server, NOT running in headless mode to let the agent reference web content as needed.&lt;/p&gt; &lt;p&gt;There are a lot of knobs to turn that I have not exposed to the frontend, but for advanced users that self host you can simply launch your endpoint with the ideal params. I will expose those to the UI in future updates.&lt;/p&gt; &lt;p&gt;Creative use of the nodes can yield some impressive results, once the flow based thought process clicks for you.&lt;/p&gt; &lt;p&gt;Have fun.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LocoMod"&gt; /u/LocoMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kuk6ke"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuk6ke/manifold_v0120_react_agent_with_mcp_tools_access/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuk6ke/manifold_v0120_react_agent_with_mcp_tools_access/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T19:49:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktlz3w</id>
    <title>96GB VRAM! What should run first?</title>
    <updated>2025-05-23T15:10:20+00:00</updated>
    <author>
      <name>/u/Mother_Occasion_8076</name>
      <uri>https://old.reddit.com/user/Mother_Occasion_8076</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktlz3w/96gb_vram_what_should_run_first/"&gt; &lt;img alt="96GB VRAM! What should run first?" src="https://preview.redd.it/co0zhh06sj2f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64b43f0124c5d5b397b2efd848e6e83c1dcfcfdc" title="96GB VRAM! What should run first?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had to make a fake company domain name to order this from a supplier. They wouldn‚Äôt even give me a quote with my Gmail address. I got the card though!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mother_Occasion_8076"&gt; /u/Mother_Occasion_8076 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/co0zhh06sj2f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktlz3w/96gb_vram_what_should_run_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktlz3w/96gb_vram_what_should_run_first/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T15:10:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kua2u0</id>
    <title>On the go native GPU inference and chatting with Gemma 3n E4B on an old S21 Ultra Snapdragon!</title>
    <updated>2025-05-24T12:07:15+00:00</updated>
    <author>
      <name>/u/lets_theorize</name>
      <uri>https://old.reddit.com/user/lets_theorize</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kua2u0/on_the_go_native_gpu_inference_and_chatting_with/"&gt; &lt;img alt="On the go native GPU inference and chatting with Gemma 3n E4B on an old S21 Ultra Snapdragon!" src="https://preview.redd.it/elvym2oe0q2f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0b066e579f5d26e7e58dbeb2bdf0effb4c91efc" title="On the go native GPU inference and chatting with Gemma 3n E4B on an old S21 Ultra Snapdragon!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lets_theorize"&gt; /u/lets_theorize &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/elvym2oe0q2f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kua2u0/on_the_go_native_gpu_inference_and_chatting_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kua2u0/on_the_go_native_gpu_inference_and_chatting_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T12:07:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kudhxg</id>
    <title>Cosmos-Reason1: Physical AI Common Sense and Embodied Reasoning Models</title>
    <updated>2025-05-24T14:55:18+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Cosmos-Reason1-7B"&gt;https://huggingface.co/nvidia/Cosmos-Reason1-7B&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Description:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Cosmos-Reason1 Models&lt;/strong&gt;: Physical AI models understand physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.&lt;/p&gt; &lt;p&gt;The Cosmos-Reason1 models are post-trained with physical common sense and embodied reasoning data with supervised fine-tuning and reinforcement learning. These are Physical AI models that can understand space, time, and fundamental physics, and can serve as planning models to reason about the next steps of an embodied agent.&lt;/p&gt; &lt;p&gt;The models are ready for commercial use.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It's based on Qwen2.5 VL&lt;/p&gt; &lt;p&gt;ggufs already available:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/models?other=base_model:quantized:nvidia/Cosmos-Reason1-7B"&gt;https://huggingface.co/models?other=base_model:quantized:nvidia/Cosmos-Reason1-7B&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kudhxg/cosmosreason1_physical_ai_common_sense_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kudhxg/cosmosreason1_physical_ai_common_sense_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kudhxg/cosmosreason1_physical_ai_common_sense_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T14:55:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ku8861</id>
    <title>MCP server to connect LLM agents to any database</title>
    <updated>2025-05-24T10:10:30+00:00</updated>
    <author>
      <name>/u/RaeudigerRaffi</name>
      <uri>https://old.reddit.com/user/RaeudigerRaffi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, my startup sadly failed, so I decided to convert it to an open source project since we actually built alot of internal tools. The result is todays release &lt;a href="https://github.com/raeudigerRaeffi/turbular"&gt;Turbular&lt;/a&gt;. Turbular is an MCP server under the MIT license that allows you to connect your LLM agent to any database. Additional features are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Schema normalizes: translates schemas into proper naming conventions (LLMs perform very poorly on non standard schema naming conventions)&lt;/li&gt; &lt;li&gt;Query optimization: optimizes your LLM generated queries and renormalizes them&lt;/li&gt; &lt;li&gt;Security: All your queries (except for Bigquery) are run with autocommit off meaning your LLM agent can not wreak havoc on your database&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know what you think and I would be happy about any suggestions in which direction to move this project&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RaeudigerRaffi"&gt; /u/RaeudigerRaffi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku8861/mcp_server_to_connect_llm_agents_to_any_database/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku8861/mcp_server_to_connect_llm_agents_to_any_database/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ku8861/mcp_server_to_connect_llm_agents_to_any_database/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T10:10:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ku6wol</id>
    <title>How much VRAM would even a smaller model take to get 1 million context model like Gemini 2.5 flash/pro?</title>
    <updated>2025-05-24T08:38:08+00:00</updated>
    <author>
      <name>/u/TumbleweedDeep825</name>
      <uri>https://old.reddit.com/user/TumbleweedDeep825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to convince myself not to waste money on a localLLM setup that I don't need since gemini 2.5 flash is cheaper and probably faster than anything I could build.&lt;/p&gt; &lt;p&gt;Let's say 1 million context is impossible. What about 200k context?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TumbleweedDeep825"&gt; /u/TumbleweedDeep825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku6wol/how_much_vram_would_even_a_smaller_model_take_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku6wol/how_much_vram_would_even_a_smaller_model_take_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ku6wol/how_much_vram_would_even_a_smaller_model_take_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T08:38:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kui73k</id>
    <title>Why arent llms pretrained at fp8?</title>
    <updated>2025-05-24T18:19:47+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There must be some reason but the fact that models are always shrunk to q8 or lower at inference got me wondering why we need higher bpw in the first place.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kui73k/why_arent_llms_pretrained_at_fp8/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kui73k/why_arent_llms_pretrained_at_fp8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kui73k/why_arent_llms_pretrained_at_fp8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T18:19:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ku95nk</id>
    <title>LLM long-term memory improvement.</title>
    <updated>2025-05-24T11:12:20+00:00</updated>
    <author>
      <name>/u/Dem0lari</name>
      <uri>https://old.reddit.com/user/Dem0lari</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working on a concept for a node-based memory architecture for LLMs, inspired by cognitive maps, biological memory networks, and graph-based data storage.&lt;/p&gt; &lt;p&gt;Instead of treating memory as a flat log or embedding space, this system stores contextual knowledge as a web of tagged nodes, connected semantically. Each node contains small, modular pieces of memory (like past conversation fragments, facts, or concepts) and metadata like topic, source, or character reference (in case of storytelling use). This structure allows LLMs to selectively retrieve relevant context without scanning the entire conversation history, potentially saving tokens and improving relevance.&lt;/p&gt; &lt;p&gt;I've documented the concept and included an example in this repo:&lt;/p&gt; &lt;p&gt;üîó &lt;a href="https://github.com/Demolari/node-memory-system"&gt;https://github.com/Demolari/node-memory-system&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love to hear feedback, criticism, or any related ideas. Do you think something like this could enhance the memory capabilities of current or future LLMs?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dem0lari"&gt; /u/Dem0lari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku95nk/llm_longterm_memory_improvement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku95nk/llm_longterm_memory_improvement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ku95nk/llm_longterm_memory_improvement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T11:12:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kujwzl</id>
    <title>We believe the future of AI is local, private, and personalized.</title>
    <updated>2025-05-24T19:36:41+00:00</updated>
    <author>
      <name>/u/ice-url</name>
      <uri>https://old.reddit.com/user/ice-url</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That‚Äôs why we built &lt;strong&gt;Cobolt&lt;/strong&gt; ‚Äî a free cross-platform AI assistant that runs entirely on your device.&lt;/p&gt; &lt;p&gt;Cobolt represents our vision for the future of AI assistants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Privacy by design (everything runs locally)&lt;/li&gt; &lt;li&gt;Extensible through Model Context Protocol (MCP)&lt;/li&gt; &lt;li&gt;Personalized without compromising your data&lt;/li&gt; &lt;li&gt;Powered by community-driven development&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We're looking for contributors, testers, and fellow privacy advocates to join us in building the future of personal AI.&lt;/p&gt; &lt;p&gt;ü§ù Contributions Welcome! üåü Star us on &lt;a href="https://github.com/platinum-hill/cobolt"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üì• Try Cobolt on &lt;a href="https://github.com/platinum-hill/cobolt/releases/download/v0.0.3/Cobolt-0.0.3.dmg"&gt;macOS&lt;/a&gt; or &lt;a href="https://github.com/platinum-hill/cobolt/releases/download/v0.0.3/Cobolt-Setup-0.0.3.exe"&gt;Windows&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let's build AI that serves you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ice-url"&gt; /u/ice-url &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kujwzl/we_believe_the_future_of_ai_is_local_private_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kujwzl/we_believe_the_future_of_ai_is_local_private_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kujwzl/we_believe_the_future_of_ai_is_local_private_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T19:36:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1kukjoe</id>
    <title>46pct Aider Polyglot in 16GB VRAM with Qwen3-14B</title>
    <updated>2025-05-24T20:06:20+00:00</updated>
    <author>
      <name>/u/andrewmobbs</name>
      <uri>https://old.reddit.com/user/andrewmobbs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After some tuning, and a tiny hack to aider, I have achieved a Aider Polyglot benchmark of pass_rate_2: 45.8 with 100% of cases well-formed, using nothing more than a 16GB 5070 Ti and Qwen3-14b, with the model running entirely offloaded to GPU.&lt;/p&gt; &lt;p&gt;That result is on a par with &amp;quot;chatgpt-4o-latest (2025-03-29)&amp;quot; on the &lt;a href="https://aider.chat/docs/leaderboards/"&gt;Aider Leaderboard&lt;/a&gt;. When allowed 3 tries at the solution, rather than the 2 tries on the benchmark, the pass rate increases to 59.1% nearly matching the &amp;quot;claude-3-7-sonnet-20250219 (no thinking)&amp;quot; result (which, to be clear, only needed 2 tries to get 60.4%). I think this is useful, as it reflects how a user may interact with a local LLM, since more tries only cost time.&lt;/p&gt; &lt;p&gt;The method was to start with the &lt;a href="https://huggingface.co/Qwen/Qwen3-14B-GGUF/blob/main/Qwen3-14B-Q6_K.gguf"&gt;Qwen3-14B Q6_K&lt;/a&gt; GGUF, set the context to the full 40960 tokens, and quantized the KV cache to Q8_0/Q5_1. To do this, I used llama.cpp server, compiled with GGML_CUDA_FA_ALL_QUANTS=ON. (Q8_0 for both K and V does &lt;em&gt;just&lt;/em&gt; fit in 16GB, but doesn't leave much spare VRAM. To allow for Gnome desktop, VS Code and a browser I dropped the V cache to Q5_1, which doesn't seem to do much relative harm to quality.)&lt;/p&gt; &lt;p&gt;Aider was then configured to use the &amp;quot;/think&amp;quot; reasoning token and use &amp;quot;architect&amp;quot; edit mode. The editor model was the same Qwen3-14B Q6, but the &amp;quot;tiny hack&amp;quot; mentioned was to ensure that the editor coder used the &amp;quot;/nothink&amp;quot; token and to extend the chat timeout from the 600s default.&lt;/p&gt; &lt;p&gt;Eval performance averaged 43 tokens per second.&lt;/p&gt; &lt;p&gt;Full details in comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andrewmobbs"&gt; /u/andrewmobbs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kukjoe/46pct_aider_polyglot_in_16gb_vram_with_qwen314b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kukjoe/46pct_aider_polyglot_in_16gb_vram_with_qwen314b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kukjoe/46pct_aider_polyglot_in_16gb_vram_with_qwen314b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T20:06:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktzwgq</id>
    <title>Ollama finally acknowledged llama.cpp officially</title>
    <updated>2025-05-24T01:22:35+00:00</updated>
    <author>
      <name>/u/simracerman</name>
      <uri>https://old.reddit.com/user/simracerman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the 0.7.1 release, they introduce the capabilities of their multimodal engine. At the end in the &lt;a href="https://imgur.com/a/zKMizcr"&gt;acknowledgments section&lt;/a&gt; they thanked the GGML project.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/blog/multimodal-models"&gt;https://ollama.com/blog/multimodal-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simracerman"&gt; /u/simracerman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktzwgq/ollama_finally_acknowledged_llamacpp_officially/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktzwgq/ollama_finally_acknowledged_llamacpp_officially/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktzwgq/ollama_finally_acknowledged_llamacpp_officially/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T01:22:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuejfp</id>
    <title>New gemma 3n is amazing, wish they suported pc gpu inference</title>
    <updated>2025-05-24T15:41:07+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there at least a workaround to run .task models on pc? Works great on my android phone but id love to play around and deploy it on a local server&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuejfp/new_gemma_3n_is_amazing_wish_they_suported_pc_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuejfp/new_gemma_3n_is_amazing_wish_they_suported_pc_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuejfp/new_gemma_3n_is_amazing_wish_they_suported_pc_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T15:41:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuimwg</id>
    <title>NVLink vs No NVLink: Devstral Small 2x RTX 3090 Inference Benchmark with vLLM</title>
    <updated>2025-05-24T18:39:28+00:00</updated>
    <author>
      <name>/u/Traditional-Gap-3313</name>
      <uri>https://old.reddit.com/user/Traditional-Gap-3313</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR: NVLink provides only ~5% performance improvement for inference on 2x RTX 3090s. Probably not worth the premium unless you already have it. Also, Mistral API is crazy cheap.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This model seems like a holy grail for people with 2x24GB, but considering the price of the Mistral API, this really isn't very cost effective. The test took about 15-16 minutes and generated 82k tokens. The electricity cost me more than the API would.&lt;/p&gt; &lt;h2&gt;Setup&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model&lt;/strong&gt;: Devstral-Small-2505-Q8_0 (GGUF)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: 2x RTX 3090 (24GB each), NVLink bridge, ROMED8-2T, both cards on PCIE 4.0 x16 directly on the mobo (no risers)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Framework&lt;/strong&gt;: vLLM with tensor parallelism (TP=2)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Test&lt;/strong&gt;: 50 complex code generation prompts, avg ~1650 tokens per response&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I asked Claude to generate 50 code generation prompts to make Devstral sweat. I didn't actually look at the output, only benchmarked throughput.&lt;/p&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;h3&gt;üîó With NVLink&lt;/h3&gt; &lt;p&gt;&lt;code&gt; Tokens/sec: 85.0 Total tokens: 82,438 Average response time: 149.6s 95th percentile: 239.1s &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;‚ùå Without NVLink&lt;/h3&gt; &lt;p&gt;&lt;code&gt; Tokens/sec: 81.1 Total tokens: 84,287 Average response time: 160.3s 95th percentile: 277.6s &lt;/code&gt;&lt;/p&gt; &lt;p&gt;NVLink gave us 85.0 vs 81.1 tokens/sec = &lt;strong&gt;~5% improvement&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;NVLink showed better consistency with lower 95th percentile times (239s vs 278s)&lt;/p&gt; &lt;p&gt;Even without NVLink, PCIe x16 handled tensor parallelism just fine for inference&lt;/p&gt; &lt;p&gt;I've managed to score 4-slot NVLink recently for 200‚Ç¨ (not cheap but ebay is even more expensive), so I'm trying to see if those 200‚Ç¨ were wasted. For inference workloads, NVLink seems like a &amp;quot;nice to have&amp;quot; rather than essential. &lt;/p&gt; &lt;p&gt;This confirms that the NVLink bandwidth advantage doesn't translate to massive inference gains like it does for training, not even with tensor parallel.&lt;/p&gt; &lt;p&gt;If you're buying hardware specifically for inference: - ‚úÖ Save money and skip NVLink - ‚úÖ Put that budget toward more VRAM or better GPUs - ‚úÖ NVLink matters more for training huge models&lt;/p&gt; &lt;p&gt;If you already have NVLink cards lying around: - ‚úÖ Use them, you'll get a small but consistent boost - ‚úÖ Better latency consistency is nice for production&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;vLLM command: ```bash CUDA_VISIBLE_DEVICES=0,2 CUDA_DEVICE_ORDER=PCI_BUS_ID vllm serve /home/myusername/unsloth/Devstral-Small-2505-GGUF/Devstral-Small-2505-Q8_0.gguf --max-num-seqs 4 --max-model-len 64000 --gpu-memory-utilization 0.95 --enable-auto-tool-choice --tool-call-parser mistral --quantization gguf --tool-call-parser mistral --enable-sleep-mode --enable-chunked-prefill --tensor-parallel-size 2 --max-num-batched-tokens 16384 &lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Testing script was generated by Claude.&lt;/p&gt; &lt;p&gt;The 3090s handled the 22B-ish parameter model (in Q8) without issues on both setups. Memory wasn't the bottleneck here.&lt;/p&gt; &lt;p&gt;Anyone else have similar NVLink vs non-NVLink benchmarks? Curious to see if this pattern holds across different model sizes and GPUs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Traditional-Gap-3313"&gt; /u/Traditional-Gap-3313 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuimwg/nvlink_vs_no_nvlink_devstral_small_2x_rtx_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuimwg/nvlink_vs_no_nvlink_devstral_small_2x_rtx_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuimwg/nvlink_vs_no_nvlink_devstral_small_2x_rtx_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T18:39:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kug045</id>
    <title>Cua : Docker Container for Computer Use Agents</title>
    <updated>2025-05-24T16:44:45+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kug045/cua_docker_container_for_computer_use_agents/"&gt; &lt;img alt="Cua : Docker Container for Computer Use Agents" src="https://external-preview.redd.it/bnFrNDRtOXdkcjJmMcsvHa0C_XuOSkhUSfxPH2wNUS_IzERNrp7qS2qcV3Nx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15606b7fbeffdaf14d86c1b274034d173ab85280" title="Cua : Docker Container for Computer Use Agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cua is the Docker for Computer-Use Agent, an open-source framework that enables AI agents to control full operating systems within high-performance, lightweight virtual containers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2ibhpziwdr2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kug045/cua_docker_container_for_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kug045/cua_docker_container_for_computer_use_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T16:44:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktx15j</id>
    <title>Guys! I managed to build a 100% fully local voice AI with Ollama that can have full conversations, control all my smart devices AND now has both short term + long term memory. ü§ò</title>
    <updated>2025-05-23T22:56:42+00:00</updated>
    <author>
      <name>/u/RoyalCities</name>
      <uri>https://old.reddit.com/user/RoyalCities</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktx15j/guys_i_managed_to_build_a_100_fully_local_voice/"&gt; &lt;img alt="Guys! I managed to build a 100% fully local voice AI with Ollama that can have full conversations, control all my smart devices AND now has both short term + long term memory. ü§ò" src="https://external-preview.redd.it/b3A3aWt5dmIzbTJmMSKAZduYkWK-j-eA22aXbm6vzflALmDerWrgdNPvGQZJ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=179ccde64a277eb295ce8f54c6f88facef7ddb65" title="Guys! I managed to build a 100% fully local voice AI with Ollama that can have full conversations, control all my smart devices AND now has both short term + long term memory. ü§ò" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found out recently that Amazon/Alexa is going to use ALL users vocal data with ZERO opt outs for their new Alexa+ service so I decided to build my own that is 1000x better and runs fully local. &lt;/p&gt; &lt;p&gt;The stack uses Home Assistant directly tied into Ollama. The long and short term memory is a custom automation design that I'll be documenting soon and providing for others. &lt;/p&gt; &lt;p&gt;This entire set up runs 100% local and you could probably get away with the whole thing working within / under 16 gigs of VRAM. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RoyalCities"&gt; /u/RoyalCities &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/iigum5tb3m2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktx15j/guys_i_managed_to_build_a_100_fully_local_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktx15j/guys_i_managed_to_build_a_100_fully_local_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T22:56:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kui17w</id>
    <title>OpenHands + Devstral is utter crap as of May 2025 (24G VRAM)</title>
    <updated>2025-05-24T18:12:37+00:00</updated>
    <author>
      <name>/u/foobarg</name>
      <uri>https://old.reddit.com/user/foobarg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following the recent &lt;a href="https://mistral.ai/news/devstral"&gt;announcement of Devstral&lt;/a&gt;, I gave &lt;a href="https://github.com/All-Hands-AI/OpenHands?tab=readme-ov-file#-running-openhands-locally"&gt;OpenHands&lt;/a&gt; + Devstral (Q4_K_M on &lt;a href="https://ollama.com/library/devstral:24b"&gt;Ollama&lt;/a&gt;) a try for a fully offline code agent experience.&lt;/p&gt; &lt;h1&gt;OpenHands&lt;/h1&gt; &lt;p&gt;Meh. I won't comment much, it's a reasonable web frontend, neatly packaged as a single podman/docker container. This could use &lt;em&gt;a lot&lt;/em&gt; more polish (the configuration through environment variables is broken for example) but once you've painfully reverse-engineered the incantation to make ollama work from the non-existing documentation, it's fairly out your way.&lt;/p&gt; &lt;p&gt;I don't like the fact you must give it access to your podman/docker installation (by mounting the socket in the container) which is technically equivalent to giving this huge pile of untrusted code root access to your host. &lt;a href="https://github.com/All-Hands-AI/OpenHands/issues/5269"&gt;This is necessary&lt;/a&gt; because OpenHands needs to spawn a &lt;em&gt;runtime&lt;/em&gt; for each &amp;quot;project&amp;quot;, and the runtime is itself its own container. Surely there must be a better way?&lt;/p&gt; &lt;h1&gt;Devstral (Mistral AI)&lt;/h1&gt; &lt;p&gt;Don't get me wrong, it's awesome to have companies releasing models to the general public. I'll be blunt though: this first iteration is useless. Devstral is supposed to have been trained/fine-tuned &lt;em&gt;precisely&lt;/em&gt; to be good at the agentic behaviors that OpenHands promises. This means having access to tools like bash, a browser, and primitives to read &amp;amp; edit files. Devstral &lt;a href="https://huggingface.co/mistralai/Devstral-Small-2505/blob/main/SYSTEM_PROMPT.txt"&gt;system prompt&lt;/a&gt; references OpenHands by name. The &lt;a href="https://mistral.ai/news/devstral"&gt;press release&lt;/a&gt; boasts:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Devstral is light enough to run on a single RTX 4090. [‚Ä¶] The performance [‚Ä¶] makes it a suitable choice for agentic coding on privacy-sensitive repositories in enterprises&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It does not. I tried a few primitive tasks and it utterly failed almost all of them while burning through the whole 380 watts my GPU demands.&lt;/p&gt; &lt;p&gt;It sometimes manages to run one or two basic commands in a row, but it often takes more than one try, hence is slow and frustrating:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Clone the git repository [url] and run &lt;a href="http://build.sh"&gt;build.sh&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The most basic commands and text manipulation tasks all failed and I had to interrupt its desperate attempts. I ended up telling myself it would have been faster to do it myself, saving the Amazon rainforest as an added bonus.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Asked it to extract the JS from a short HTML file which had a single &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag. It created the file successfully (but transformed it against my will), then wasn't able to remove the tag from the HTML as the proposed edits wouldn't pass OpenHands' correctness checks.&lt;/li&gt; &lt;li&gt;Asked it to remove comments from a short file. Same issue, &lt;code&gt;ERROR: No replacement was performed, old_str [...] did not appear verbatim in /workspace/...&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Asked it to bootstrap a minimal todo app. It got stuck in a loop trying to invoke interactive &lt;code&gt;create-app&lt;/code&gt; tools from the cursed JS ecosystem, which require arrow keys to navigate menus‚Äìdid I mention I hate those wizards?&lt;/li&gt; &lt;li&gt;Prompt adhesion is bad. Even when you try to help by providing the &lt;em&gt;exact command&lt;/em&gt;, it randomly removes dashes and other important bits, and then proceeds to comfortably heat up my room trying to debug the inevitable errors.&lt;/li&gt; &lt;li&gt;OpenHands includes two random TCP ports in the prompt, to use for HTTP servers (like Vite or uvicorn) that are forwarded to the host. The model fails to understand to use them and spawns servers on the default port, making them inaccessible.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As a point of comparison, I tried those using one of the cheaper proprietary models out there (Gemini Flash) which obviously is general-purpose and &lt;em&gt;not&lt;/em&gt; tuned to OpenHands particularities. It had no issue adhering to OpenHands' prompt and blasted through the tasks‚Äìincluding tweaking the HTTP port mentioned above.&lt;/p&gt; &lt;p&gt;Perhaps this is meant to run on more expensive hardware that can run the larger flavors. If &amp;quot;all&amp;quot; you have is 24G VRAM, prepare to be disappointed. Local agentic programming is not there yet. Did anyone else try it, and does your experience match?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foobarg"&gt; /u/foobarg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T18:12:37+00:00</published>
  </entry>
</feed>
