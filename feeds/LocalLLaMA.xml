<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-22T16:37:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i6x1rz</id>
    <title>Deepseek-R1 is brittle</title>
    <updated>2025-01-21T23:45:27+00:00</updated>
    <author>
      <name>/u/girishsk</name>
      <uri>https://old.reddit.com/user/girishsk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6x1rz/deepseekr1_is_brittle/"&gt; &lt;img alt="Deepseek-R1 is brittle" src="https://b.thumbs.redditmedia.com/8NQUVLxGD7ZRf-EVvaa_9iF75S6_Ytn5AU8V2JqmcJg.jpg" title="Deepseek-R1 is brittle" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w64gxy9sofee1.png?width=2005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5ce8831dd1bd935250ebd75ccda5fdbf39ebe86"&gt;https://preview.redd.it/w64gxy9sofee1.png?width=2005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5ce8831dd1bd935250ebd75ccda5fdbf39ebe86&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/girishsk"&gt; /u/girishsk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6x1rz/deepseekr1_is_brittle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6x1rz/deepseekr1_is_brittle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6x1rz/deepseekr1_is_brittle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T23:45:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7cj11</id>
    <title>AMD HX370 LLM performance</title>
    <updated>2025-01-22T14:38:34+00:00</updated>
    <author>
      <name>/u/ivoras</name>
      <uri>https://old.reddit.com/user/ivoras</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got an AMD HX 370-based MiniPC, and at this time (January 2025), it's not really suitable for serious LLM work. The NPU isn't supported even by AMD's ROCm, so it's basically useless.&lt;/p&gt; &lt;p&gt;CPU-based inference with ollama, with deepseek-r1:14b, results in 7.5 tok/s.&lt;/p&gt; &lt;p&gt;GPU-based inference with llama.cpp and the Vulkan API yields almost the same result, 7.8 tok/s (leaving CPU cores free to do other work).&lt;/p&gt; &lt;p&gt;q4 in both cases.&lt;/p&gt; &lt;p&gt;The similarity of the results suggest that memory bandwidth is the probable bottleneck. I did these tests on a stock configuration with LPDDR5x 7500 MT/s, arranged in 4 channels of 8 GB, but the bus is 32-bit so it's like 128-bit total width. AIDA64 reports less than 90 GB/s memory read performance.&lt;/p&gt; &lt;p&gt;AMD calls it an &amp;quot;AI&amp;quot; chip, but - no it's not. At least not until drivers start supporting the NPU.&lt;/p&gt; &lt;p&gt;OTOH, by every other benchmark, it's blazing fast!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivoras"&gt; /u/ivoras &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7cj11/amd_hx370_llm_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7cj11/amd_hx370_llm_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7cj11/amd_hx370_llm_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T14:38:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6zbsf</id>
    <title>The distilled R1 models likely work best in workflows, so now's a great time to learn those if you haven't already!</title>
    <updated>2025-01-22T01:31:21+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Another member of our board recently pointed out that Deepseek's paper &lt;a href="https://kingy.ai/wp-content/uploads/2025/01/DeepSeek_R1.pdf"&gt;&amp;quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning&amp;quot;&lt;/a&gt; said the following: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;R1, and likely all reasoning models, are best suited for a zero-shot &amp;quot;&lt;em&gt;please think through this specific problem&lt;/em&gt;&amp;quot; sort of prompts, and you'll likely get far better results doing that than having a multi-turn conversation jammed in there while it thinks.&lt;/p&gt; &lt;p&gt;So once again, I take the opportunity to say: workflows are your friend. I know I'm always harping about workflows, but this case is a slam dunk use-case for learning to use workflows, getting comfortable with them, etc.&lt;/p&gt; &lt;p&gt;You will likely get far better results out of R1, QwQ, the R1 Distilled models, etc if you were have a workflow that did something similar to the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Summarize the what the most recent message is saying and/or asking&lt;/li&gt; &lt;li&gt;Summarize any supporting context to assist in thinking about this&lt;/li&gt; &lt;li&gt;Pass 1 and 2 into the reasoning model, and let it think of a problem&lt;/li&gt; &lt;li&gt;Respond to the user using the output of 3.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;There are 2 really valuable benefits of doing this- first: you only pass in a single scoped problem every time and second: you are hiding the full thinking logic of step 3, so that isn't kept within the conversation or agentic history.&lt;/p&gt; &lt;p&gt;It doesn't matter what workflow program you go with- n8n, langflow, wilmerai, omnichain, whatever. This is a great time to just try them out if you haven't already, and get used to working with them. I've been using workflows exclusively when using ai since at least May or June of last year, and I can't imagine going back. Many of you may end up not liking using them, but many of you might. Either way, you'll get the experience AND can use these distilled R1 models to their maximum benefit.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6zbsf/the_distilled_r1_models_likely_work_best_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6zbsf/the_distilled_r1_models_likely_work_best_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6zbsf/the_distilled_r1_models_likely_work_best_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T01:31:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1i75wyo</id>
    <title>Bytedance Model: Doubao 1.5pro - Doubao Team</title>
    <updated>2025-01-22T07:36:36+00:00</updated>
    <author>
      <name>/u/a445141126</name>
      <uri>https://old.reddit.com/user/a445141126</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a445141126"&gt; /u/a445141126 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://team.doubao.com/en/special/doubao_1_5_pro"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i75wyo/bytedance_model_doubao_15pro_doubao_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i75wyo/bytedance_model_doubao_15pro_doubao_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T07:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6jbur</id>
    <title>DeepSeek R1 (Qwen 32B Distill) is now available for free on HuggingChat!</title>
    <updated>2025-01-21T14:07:01+00:00</updated>
    <author>
      <name>/u/SensitiveCranberry</name>
      <uri>https://old.reddit.com/user/SensitiveCranberry</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6jbur/deepseek_r1_qwen_32b_distill_is_now_available_for/"&gt; &lt;img alt="DeepSeek R1 (Qwen 32B Distill) is now available for free on HuggingChat!" src="https://external-preview.redd.it/8HwSiZPd8K_hder46_kXYrWF23xE0qYwa1myzoXXUfM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32931922738bce5caa4226b968a442514cf96587" title="DeepSeek R1 (Qwen 32B Distill) is now available for free on HuggingChat!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SensitiveCranberry"&gt; /u/SensitiveCranberry &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hf.co/chat/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6jbur/deepseek_r1_qwen_32b_distill_is_now_available_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6jbur/deepseek_r1_qwen_32b_distill_is_now_available_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T14:07:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6n7jf</id>
    <title>Pretty sure OpenAI has their devs working 24/7 to not lose their throne to DeepSeek ðŸ˜­</title>
    <updated>2025-01-21T16:57:48+00:00</updated>
    <author>
      <name>/u/Condomphobic</name>
      <uri>https://old.reddit.com/user/Condomphobic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And DeepSeek is making the same progress at a much faster pace than OpenAI is. They are definitely in a rock situation &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Condomphobic"&gt; /u/Condomphobic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6n7jf/pretty_sure_openai_has_their_devs_working_247_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6n7jf/pretty_sure_openai_has_their_devs_working_247_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6n7jf/pretty_sure_openai_has_their_devs_working_247_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T16:57:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6t08q</id>
    <title>DeepSeek-R1-Distill-Qwen-1.5B running 100% locally in-browser on WebGPU. Reportedly outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks (28.9% on AIME and 83.9% on MATH).</title>
    <updated>2025-01-21T20:53:47+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6t08q/deepseekr1distillqwen15b_running_100_locally/"&gt; &lt;img alt="DeepSeek-R1-Distill-Qwen-1.5B running 100% locally in-browser on WebGPU. Reportedly outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks (28.9% on AIME and 83.9% on MATH)." src="https://external-preview.redd.it/bHl5MDU0Yzl0ZWVlMQAQ0j2wFUvXTQrT52Nv81bl04kSZ1X_57NkDQOUMylE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=277e9be2fe8997f1f81f1d7306a6ef378dd50407" title="DeepSeek-R1-Distill-Qwen-1.5B running 100% locally in-browser on WebGPU. Reportedly outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks (28.9% on AIME and 83.9% on MATH)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5ei4j3c9teee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6t08q/deepseekr1distillqwen15b_running_100_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6t08q/deepseekr1distillqwen15b_running_100_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T20:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6lsgo</id>
    <title>From llama2 --&gt; DeepSeek R1 things have gone a long way in a 1 year</title>
    <updated>2025-01-21T15:58:08+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was blown away by llama2 70b when it came out. I felt so empowered having so much knowledge spun up locally on my M3 Max. &lt;/p&gt; &lt;p&gt;Just over a year, and DeepSeek R1 makes Llama 2 seem like a little child. It's crazy how good the outputs are, and how fast it spits out tokens in just 40GB.&lt;/p&gt; &lt;p&gt;Can't imagine where things will be in another year.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6lsgo/from_llama2_deepseek_r1_things_have_gone_a_long/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6lsgo/from_llama2_deepseek_r1_things_have_gone_a_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6lsgo/from_llama2_deepseek_r1_things_have_gone_a_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T15:58:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1i73y5t</id>
    <title>Deepseek running in my basement finds the 5 odd numbers with the letter e in them</title>
    <updated>2025-01-22T05:24:00+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Amazing. I'm a believer. &lt;/p&gt; &lt;p&gt;It's impossible to find an odd number in English that doesn't have the letter 'e' in its spelling because all single-digit odd numbers (1, 3, 5, 7, 9) contain the letter 'e' in their names. Consequently, any odd number, regardless of its size, will include one of these digits in its ones place, resulting in the presence of the letter 'e'. Therefore, no such numbers exist.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt; There are no odd numbers in English that don't have the letter 'e' in their spelling.&lt;/p&gt; &lt;p&gt;If you want to see the thinking &lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/rbvF7p2f"&gt;https://pastebin.com/rbvF7p2f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i73y5t/deepseek_running_in_my_basement_finds_the_5_odd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i73y5t/deepseek_running_in_my_basement_finds_the_5_odd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i73y5t/deepseek_running_in_my_basement_finds_the_5_odd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T05:24:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7aoet</id>
    <title>Editing &lt;thinking&gt; process is a super effective way to guide model output</title>
    <updated>2025-01-22T13:06:20+00:00</updated>
    <author>
      <name>/u/Berberis</name>
      <uri>https://old.reddit.com/user/Berberis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;R1 is my first real foray into reasoning models. One thing I have gotten in the habit of with Claude is to edit my prior prompt to guide model output, rather than engaging in a long back and forth. This is even more powerful with reasoning models, as the thinking tags act as a scaffold for model output. You can edit, or simply add on, new thinking content in the reasoning section and re-run the completion to get what you want. In my experience, it is a bit more efficient than modifying the prompt directly, as it's already interpreted the prompt and so the thinking tags are closer to the output than the prompt is (so less of a chance to misinterpret the user instructions, and more of an opportunity to subtly guide output).&lt;/p&gt; &lt;p&gt;Here is an example. I'm a scientist, so my default test of the model is to have it output a classic model in evolutionary biology, the Price Equation applied to multilevel selection. The details don't really matter here, but in the revision, I simply added a thinking tag telling it to output a numerical example. This is a coarse example that definitely could have been accomplished in the prompt, but I figured for this example something coarse is probably easier to understand by those unfamiliar with the subject than something subtle. Still, you get the idea!&lt;/p&gt; &lt;p&gt;For this test I am using R1-70b distill, q8, Mac Studio in LM Studio.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Version 1.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;User&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Use the Price equation to partition selection acting on individual bees in a colony from that acting on the colony as a whole.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Assistant&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;lt;think&amp;gt;&lt;/strong&gt;First, I remember that the Price equation is used in evolutionary biology to partition selection into different components. It's about HOW selection acts on traits, whether at the individual level or group level.&lt;/p&gt; &lt;p&gt;So, applying it here: we have a honeybee colony with many workers. Each bee has some traitâ€”maybe foraging behaviorâ€”that varies among them. The success of the colony depends on how good the bees are at this trait.&lt;/p&gt; &lt;p&gt;The Price equation basically breaks down change in a trait into two parts: selection and transmission. Wait, noâ€”I think it's more about genetic variation within groups and between groups. Oh right, the equation is Î”z = V_S + VT.&lt;/p&gt; &lt;p&gt;Wait, actually, the Price equation formula is typically written as:&lt;/p&gt; &lt;p&gt;Î”zÌ„ = (Cov(w, z) / wÌ„ )&lt;/p&gt; &lt;p&gt;Where w is fitness, z is the trait. But when applied hierarchically, it can be split into within-group and between-group components, right?&lt;/p&gt; &lt;p&gt;So in this case, individual selection would act on the variation among bees within a colony, affecting their relative contributions (maybe how many offspring each contributes). Colony-level selection would be about how well each colony does compared to others based on average traits.&lt;/p&gt; &lt;p&gt;For honeybees, maybe it's about how efficient foragers are. Some colonies have more efficient workers on average, so those colonies survive or propagate better. But within each colony, bees with better traits might contribute more via reproduction, but wait, in bees, queens are the ones who reproduce, not workers. Oh, that complicates things.&lt;/p&gt; &lt;p&gt;Wait, so worker bees don't have direct reproduction since they're sterile. So individual selection on them is...Hmm. Maybe this is a way to model group-level adaptation through individual contributions, even if those individuals aren't reproducing themselves. Or perhaps the Price equation can still be applied to see how colony-level fitness aggregates from individual behaviors.&lt;/p&gt; &lt;p&gt;So structure-wise: Each colony has bees with trait z_i. The average of z_i in a colony is Z_j. Colony j's fitness W_j depends on Z_j.&lt;/p&gt; &lt;p&gt;The overall change in trait would come from two sources: selection within colonies (how each bee's trait affects the colony's success) and selection between colonies based on their averages.&lt;/p&gt; &lt;p&gt;Putting it into equations, I need to show the Price equation for both individual and group levels.&lt;/p&gt; &lt;p&gt;But wait, since worker bees don't reproduce directly, what constitutes 'fitness' w for them? Maybe it's about how much they contribute to the colony's fitness. So each bee contributes to W_j based on z_i.&lt;/p&gt; &lt;p&gt;So in that case, within-colony selection is based on individual contributions to Wj, and between-colony selection is based on colonies with higher average Zj having higher fitness.&lt;/p&gt; &lt;p&gt;Yeah, so I think that's how it breaks down: part of the equation captures variation among bees in how they help their colony (within-colony), and another part captures how colonies' averages affect their success (between-colonies). &lt;strong&gt;&amp;lt;/think&amp;gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;To apply the Price Equation to partition selection acting on individual bees within a hive versus selection at the colony level, we consider the following model:&lt;/p&gt; &lt;p&gt;Let ( z_i ) represent an altruistic trait of bee ( i ) in colony ( j ). The fitness ( w_{i} ) contributes to mortality/survival rates depending on this trait. Colony reproductive success is proportional to ( w_j = average(w_i for all bees in colony j) ).&lt;/p&gt; &lt;p&gt;The Price Equation quantifies the change in mean trait from one generation to the next as:&lt;/p&gt; &lt;p&gt;[ \Delta zÌ„ = \frac{\text{Cov}(z, w)}{\bar{w}} + (\bar{w}_j - \bar{w}) ]&lt;/p&gt; &lt;p&gt;Here, the first term represents individual-level selection: covariation between bee trait ( z_i ) and their contribution to colony success. The second term is colony-level selection, where colonies with higher average ( w_j ) contribute disproportionately more to new colonies.&lt;/p&gt; &lt;p&gt;Thus, selection on bees has two components:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Within-colony&lt;/strong&gt;: The effect of each bee's individual traits on its own survival/efficiency.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Between-colonies&lt;/strong&gt;: The fitness difference between colonies due to their collective trait averages.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This partition highlights how both levels contribute to evolutionary change in the population, even if worker bees themselves do not reproduce directly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Version 2&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I just added this tot he end of the thinking tags, deleted the output below the thinking tags, and then re-ran the completion in LM Studio:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Once I have derived the general case, I will provide a fully worked numerical example, to make it clear how this is used. &amp;lt;/think&amp;gt;&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;To apply the Price equation to understand selection acting on individual bees versus the colony as a whole, we need to partition selective forces into within-group and between-group components. Letâ€™s consider a population of honeybee colonies where each colony comprises multiple worker bees exhibiting some heritable trait ( z ) (e.g., foraging efficiency). The fitness of a colony ( j ), denoted ( W_j ), depends on the average value of ( z ) across its workers, as well as potentially other factors such as environmental conditions and inter-colony competition.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step 1: Define Variables&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Let ( Z_j = \frac{1}{N_j} \sum_{i=1}^{N_j} z_i ), where ( N_j ) is the number of workers in colony ( j ).&lt;/li&gt; &lt;li&gt;The aggregate fitness of colony ( j ) is some function of ( Z_j ), i.e., ( W_j = f(Z_j) ).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Step 2: Apply the Price Equation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The standard form of the Price equation is:&lt;/p&gt; &lt;p&gt;[ \Delta z = \frac{1}{\overline{W}} \sum_{j=1}^{M} (W_j - \overline{W}) Z_j + \frac{1}{\overline{W}} \sum_{j=1}^{M} W_j \left( z_j - Z_j \right). ]&lt;/p&gt; &lt;p&gt;Here, ( z ) is the trait of interest, ( W ) is fitness, and a bar denotes an average over all colonies.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step 3: Interpretation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The first term in the equation represents selection acting at the colony level (between-group selection). This measures how differences in the average trait (( Z_j )) between colonies contribute to differences in colony fitness (( W_j )). Colonies with higher values of ( Z_j ) will have higher fitness, so this term quantifies multilevel selection at the colony level.&lt;/p&gt; &lt;p&gt;The second term represents within-group selection. This measures how much individual deviations from the group mean (( z_j - Z_j )) are associated with differences in individual contributions to colony success. If certain workers contribute more to colony-level traits (e.g., foraging efficiency), this difference contributes to anisman evolutionary outcome at this level.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step 4: Connection Back to Honeybees&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For honeybees, the within-colony term can be thought of as selection acting on worker behaviors that enhance colony success. For example, more efficient foragers contribute disproportionately to colony resources and survival. The between-colony term reflects differences in overall colony performance based on the average traits of their workers.&lt;/p&gt; &lt;p&gt;By partitioning these effects using the Price equation, we can quantify the relative strength of individual versus group selection in shaping evolutionary outcomes in social bees.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step 5: Numerical Example&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Assume we have three colonies with different mean foraging efficiencies (( Z_1 = 3 ), ( Z_2 = 4 ), ( Z_3 = 5 )) on a scale where higher values represent better efficiency. Let the fitness of each colony be ( W_1 = 10 ), ( W_2 = 20 ), and ( W_3 = 30 ). Suppose the mean fitness across all colonies is ( \overline{W} = (10 + 20 + 30)/3 = \frac{60}{3} = 20 ).&lt;/p&gt; &lt;p&gt;Compute each term:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Between-group selection:&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;[ \frac{1}{\overline{W}} \sum_{j=1}^{M} (W_j - \overline{W}) Z_j = \frac{1}{20} [(10-20) \cdot 3 + (20-20) \cdot 4 + (30-20) \cdot 5] = \frac{1}{20} [(-10)\cdot3 + 0\cdot4 + 10\cdot5] = \frac{1}{20} (-30 + 50) = \frac{20}{20} = 1. ]&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Within-group selection:&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Assume within-colony deviations (( z_j - Z_j )) are uncorrelated with colony fitness for simplicity, so this term is negligible (or zero).&lt;/p&gt; &lt;p&gt;Thus, the overall change in ( z ) comes entirely from between-group selection:( \Delta z = 1 + 0 = 1 ).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Berberis"&gt; /u/Berberis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7aoet/editing_thinking_process_is_a_super_effective_way/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7aoet/editing_thinking_process_is_a_super_effective_way/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7aoet/editing_thinking_process_is_a_super_effective_way/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T13:06:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i74jqg</id>
    <title>DeepSeek R1 32B is way better than 7B Distill, even at Q4 quant</title>
    <updated>2025-01-22T06:01:35+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been quite impressed by the model. I'm using the Qwen distill and so far it's working well, although as is typical with these models, they tend to overthink a lot! But it answered my trick question in one shot (See comments).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i74jqg/deepseek_r1_32b_is_way_better_than_7b_distill/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i74jqg/deepseek_r1_32b_is_way_better_than_7b_distill/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i74jqg/deepseek_r1_32b_is_way_better_than_7b_distill/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T06:01:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6zid8</id>
    <title>Just a comparison of US $500B Stargate AI project to other tech projects</title>
    <updated>2025-01-22T01:40:19+00:00</updated>
    <author>
      <name>/u/Shir_man</name>
      <uri>https://old.reddit.com/user/Shir_man</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Manhattan Project&lt;/strong&gt; ~$30 billion in todayâ€™s dollars [~1.5% of US GDP in the mid-1940s]&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Apollo Program&lt;/strong&gt; ~$170â€“$180 billion in todayâ€™s dollars [~0.5% of US GDP in the mid-1960s]&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Space Shuttle Program&lt;/strong&gt; ~$275â€“$300 billion in todayâ€™s dollars [~0.2% of US GDP in the early 1980s]&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interstate Highway System&lt;/strong&gt;, entire decades-long Interstate Highway System buildout, ~$500â€“$550 billion in todayâ€™s dollars [~0.2%â€“0.3% of GDP annually over multiple decades]&lt;/p&gt; &lt;p&gt;Stargate is huge AI project [~1.7% of US GDP 2024]&lt;/p&gt; &lt;p&gt;&lt;em&gt;Update:&lt;/em&gt; rough GDP calculations added&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shir_man"&gt; /u/Shir_man &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6zid8/just_a_comparison_of_us_500b_stargate_ai_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6zid8/just_a_comparison_of_us_500b_stargate_ai_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6zid8/just_a_comparison_of_us_500b_stargate_ai_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T01:40:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7d9di</id>
    <title>Anthill (OpenAI Swarm fork) running with DeepSeek R1 32b</title>
    <updated>2025-01-22T15:11:41+00:00</updated>
    <author>
      <name>/u/rodrigobaron</name>
      <uri>https://old.reddit.com/user/rodrigobaron</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7d9di/anthill_openai_swarm_fork_running_with_deepseek/"&gt; &lt;img alt="Anthill (OpenAI Swarm fork) running with DeepSeek R1 32b" src="https://external-preview.redd.it/c2pnYTNvcmE5a2VlMbtJ-UCX3bJjUP4scuQf4OZlvJhBFK2cUZcpqay5Dq4B.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1be0529a52645fd8c9742eeef7297cc9fde62ce" title="Anthill (OpenAI Swarm fork) running with DeepSeek R1 32b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rodrigobaron"&gt; /u/rodrigobaron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/khbkhsra9kee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7d9di/anthill_openai_swarm_fork_running_with_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7d9di/anthill_openai_swarm_fork_running_with_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T15:11:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7adec</id>
    <title>Lightweight web scraping with ollama</title>
    <updated>2025-01-22T12:49:48+00:00</updated>
    <author>
      <name>/u/Financial-Article-12</name>
      <uri>https://old.reddit.com/user/Financial-Article-12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLLaMA people,&lt;/p&gt; &lt;p&gt;Itâ€™s been almost 6 months since Iâ€™ve shared my lightweight library for web scraping with LLMs. Today, I want to share some updates, starting with the fact that &lt;strong&gt;Parsera now officially supports Ollama!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Some notable changes include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cookies support&lt;/strong&gt; for accessing login-required content&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced page rendering&lt;/strong&gt; to ensure no content is missed&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom script execution&lt;/strong&gt;, allowing scraping of interactive elements&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Page chunking&lt;/strong&gt;, to process pages that exceed the modelâ€™s context limits&lt;/li&gt; &lt;li&gt;Plus many other minor enhancements and fixes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check it out on GitHub: &lt;a href="https://github.com/raznem/parsera"&gt;https://github.com/raznem/parsera&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P. S. We are now using Parsera as the foundation for our upcoming Scraping Agent, so hope Iâ€™ll share something exciting soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial-Article-12"&gt; /u/Financial-Article-12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7adec/lightweight_web_scraping_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7adec/lightweight_web_scraping_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7adec/lightweight_web_scraping_with_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T12:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6urjd</id>
    <title>Billions in proprietary AI? No more.</title>
    <updated>2025-01-21T22:06:11+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6urjd/billions_in_proprietary_ai_no_more/"&gt; &lt;img alt="Billions in proprietary AI? No more." src="https://b.thumbs.redditmedia.com/9ar_9wVV9mMAYbxuIYJsMxrYp5TIPP2iGETnpq4RXng.jpg" title="Billions in proprietary AI? No more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI raised billions on promise of having and securing behind thick doors something no other is even remotely close to. The following tweet from just few days prior R1 release made me think they really have atomic bomb the world will knee for;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/25dv42dl1fee1.png?width=1209&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bcfafe3260c7a5257502850b940aa24a2f5f7ecd"&gt;https://preview.redd.it/25dv42dl1fee1.png?width=1209&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bcfafe3260c7a5257502850b940aa24a2f5f7ecd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The truth is, they have nothing. o1-level, some say human-level reasoning is reproducible, can be privately hosted by anyone, anywhere. Can't be greedily priced.&lt;/p&gt; &lt;p&gt;MIT licensed open models is the future of AI. Zero dollars is the only right price for something made on all human knowledge. It is a sum of effort of the whole civilisation, spanning many generations. Just imagine, any book that landed in the pretraining dataset influences the whole model. There is no better way to honor any author contributing to the overall model performance, knowingly or not than to make a tool that help create new knowledge, available for anyone, at no cost.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6urjd/billions_in_proprietary_ai_no_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6urjd/billions_in_proprietary_ai_no_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6urjd/billions_in_proprietary_ai_no_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T22:06:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7aqhg</id>
    <title>Memory bandwidth of Nvidia RTX Laptop graphics compared</title>
    <updated>2025-01-22T13:09:26+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7aqhg/memory_bandwidth_of_nvidia_rtx_laptop_graphics/"&gt; &lt;img alt="Memory bandwidth of Nvidia RTX Laptop graphics compared" src="https://preview.redd.it/3ec8i695ojee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fbd835d18c9f6df088cfd0931409590eaf41d47" title="Memory bandwidth of Nvidia RTX Laptop graphics compared" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3ec8i695ojee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7aqhg/memory_bandwidth_of_nvidia_rtx_laptop_graphics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7aqhg/memory_bandwidth_of_nvidia_rtx_laptop_graphics/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T13:09:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6uviy</id>
    <title>R1 is mind blowing</title>
    <updated>2025-01-21T22:10:54+00:00</updated>
    <author>
      <name>/u/Not-The-Dark-Lord-7</name>
      <uri>https://old.reddit.com/user/Not-The-Dark-Lord-7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gave it a problem from my graph theory course thatâ€™s reasonably nuanced. 4o gave me the wrong answer twice, but did manage to produce the correct answer once. R1 managed to get this problem right in one shot, and also held up under pressure when I asked it to justify its answer. It also gave a great explanation that showed it really understood the nuance of the problem. I feel pretty confident in saying that AI is smarter than me. Not just closed, flagship models, but smaller models that I could run on my MacBook are probably smarter than me at this point. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Not-The-Dark-Lord-7"&gt; /u/Not-The-Dark-Lord-7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6uviy/r1_is_mind_blowing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6uviy/r1_is_mind_blowing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6uviy/r1_is_mind_blowing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T22:10:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7b3r1</id>
    <title>I did a quick test of MacBook M4 Max 128 GB token/second throughput across a few popular local LLMs (in the MLX format)</title>
    <updated>2025-01-22T13:28:52+00:00</updated>
    <author>
      <name>/u/Pure_Refrigerator988</name>
      <uri>https://old.reddit.com/user/Pure_Refrigerator988</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sharing this in case you were wondering what kind of throughput you might expect to get on a machine like this. E.g. if you are considering whether it's worth buying or not (as for me, I have no regrets, I'm loving this beast). Same single query, set the context to 8K, tok/sec numbers are reported below, as measured by LMStudio:&lt;/p&gt; &lt;p&gt;LLaMA 3.2 3B 4bit -- 181&lt;br /&gt; LLaMA 3 8B 8bit -- 55&lt;br /&gt; LLaMA 3.3 70B 4bit -- 11.8&lt;br /&gt; Mistral Large 123B 4bit -- 6.6&lt;br /&gt; Mistral Nemo 12B 4bit -- 63&lt;br /&gt; Mistral Nemo 12B 8bit -- 36&lt;br /&gt; Mistral Small 22B 4bit -- 34.5&lt;br /&gt; Mistral Small 22B 8bit -- 19.6&lt;br /&gt; Qwen2.5 32B 4bit -- 24&lt;br /&gt; Qwen2.5 32B 8bit -- 13.5&lt;br /&gt; Qwen2.5 72B 4bit -- 10.9&lt;br /&gt; Qwen2.5 72B 8bit -- 6.2&lt;br /&gt; WizardLM-2 8x22B 4bit -- 19.4!! &lt;/p&gt; &lt;p&gt;For comparison, here are some numbers obtained in the same setting on my other MacBook, M1 Pro with 32 GB:&lt;/p&gt; &lt;p&gt;Mistral Nemo 12B 4bit -- 22.8&lt;br /&gt; Mistral Small 22B 4bit -- 12.9&lt;br /&gt; Qwen2.5 32B 4bit -- 8.8 &lt;/p&gt; &lt;p&gt;Hope it's interesting / useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pure_Refrigerator988"&gt; /u/Pure_Refrigerator988 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7b3r1/i_did_a_quick_test_of_macbook_m4_max_128_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7b3r1/i_did_a_quick_test_of_macbook_m4_max_128_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7b3r1/i_did_a_quick_test_of_macbook_m4_max_128_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T13:28:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6vnqc</id>
    <title>Trump announces a $500 billion AI infrastructure investment in the US</title>
    <updated>2025-01-21T22:43:49+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6vnqc/trump_announces_a_500_billion_ai_infrastructure/"&gt; &lt;img alt="Trump announces a $500 billion AI infrastructure investment in the US" src="https://external-preview.redd.it/qFlenD3wOMEKpyf-2qth3Zo8oJQzBNIpBFiyCeVPdPY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1661f472f059c05f183a4286b726667eb4724fc9" title="Trump announces a $500 billion AI infrastructure investment in the US" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnn.com/2025/01/21/tech/openai-oracle-softbank-trump-ai-investment/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6vnqc/trump_announces_a_500_billion_ai_infrastructure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6vnqc/trump_announces_a_500_billion_ai_infrastructure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T22:43:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1i765q0</id>
    <title>R1-Zero: Pure RL Creates a Mind We Canâ€™t Decodeâ€”Is This AGIâ€™s Dark Mirror?</title>
    <updated>2025-01-22T07:54:24+00:00</updated>
    <author>
      <name>/u/Fun_Dragonfruit_4613</name>
      <uri>https://old.reddit.com/user/Fun_Dragonfruit_4613</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The AI world is losing its mind over DeepSeek-R1-Zero, a model that skipped supervised fine-tuning (SFT) entirely and learned purely through reinforcement learning (RL). Unlike its sibling R1â€”which uses &lt;strong&gt;some&lt;/strong&gt; SFT data to stay &amp;quot;human-readable&amp;quot;â€”R1-Zeroâ€™s training mirrors AlphaZeroâ€™s trial-and-error self-play. The result? &lt;strong&gt;Jaw-dropping performance&lt;/strong&gt; (AIME math scores jumped from 15.6% â†’ 86.7%) paired with &lt;strong&gt;bizarre, uninterpretable reasoning&lt;/strong&gt;. Researchers observed &amp;quot;aha moments&amp;quot; where it autonomously rechecked flawed logic mid-process and allocated more compute to harder problemsâ€”&lt;strong&gt;without human guidance&lt;/strong&gt;. But hereâ€™s the kicker: its outputs are riddled with garbled language mixes (e.g., Chinese/English spaghetti code) and logic leaps that even its creators canâ€™t fully explain. &lt;/p&gt; &lt;p&gt;Meanwhile, R1 (the SFT-hybrid version) achieves similar performance &lt;strong&gt;without the chaos&lt;/strong&gt;, proving that human-curated data still tames the beast. But at what cost? R1-Zeroâ€™s pure RL approach hints at a terrifying possibility: &lt;strong&gt;minds that optimize truth beyond human comprehension&lt;/strong&gt;. And with API costs 50x cheaper than OpenAIâ€™s, scaling this could democratize superintelligenceâ€”or unleash unreadable black-box AI. &lt;/p&gt; &lt;p&gt;If R1-Zeroâ€™s &amp;quot;alien logic&amp;quot; solves problems we canâ€™t, does readability even matterâ€¦ or is this how alignment dies? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Dragonfruit_4613"&gt; /u/Fun_Dragonfruit_4613 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T07:54:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i78src</id>
    <title>How I Used GPT-O1 Pro to Discover My Autoimmune Disease (After Spending $100k and Visiting 30+ Hospitals with No Success)</title>
    <updated>2025-01-22T11:12:33+00:00</updated>
    <author>
      <name>/u/Dry_Steak30</name>
      <uri>https://old.reddit.com/user/Dry_Steak30</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Suffered from various health issues for 5 years, visited 30+ hospitals with no answers&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Finally diagnosed with axial spondyloarthritis through genetic testing&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built a personalized health analysis system using GPT-O1 Pro, which actually suggested this condition earlier&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm a guy in my mid-30s who started having weird health issues about 5 years ago. Nothing major, but lots of annoying symptoms - getting injured easily during workouts, slow recovery, random fatigue, and sometimes the pain was so bad I could barely walk.&lt;/p&gt; &lt;p&gt;At first, I went to different doctors for each symptom. Tried everything - MRIs, chiropractic care, meds, steroids - nothing helped. I followed every doctor's advice perfectly. Started getting into longevity medicine thinking it might be early aging. Changed my diet, exercise routine, sleep schedule - still no improvement. The cause remained a mystery.&lt;/p&gt; &lt;p&gt;Recently, after a month-long toe injury wouldn't heal, I ended up seeing a rheumatologist. They did genetic testing and boom - diagnosed with axial spondyloarthritis. This was the answer I'd been searching for over 5 years.&lt;/p&gt; &lt;p&gt;Here's the crazy part - I fed all my previous medical records and symptoms into GPT-O1 pro before the diagnosis, and it actually listed this condition as the top possibility!&lt;/p&gt; &lt;p&gt;This got me thinking - why didn't any doctor catch this earlier? Well, it's a rare condition, and autoimmune diseases affect the whole body. Joint pain isn't just joint pain, dry eyes aren't just eye problems. The usual medical workflow isn't set up to look at everything together.&lt;/p&gt; &lt;p&gt;So I had an idea: What if we created an open-source system that could analyze someone's complete medical history, including family history (which was a huge clue in my case), and create personalized health plans? It wouldn't replace doctors but could help both patients and medical professionals spot patterns.&lt;/p&gt; &lt;p&gt;Building my personal system was challenging:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Every hospital uses different formats and units for test results. Had to create a GPT workflow to standardize everything.&lt;/li&gt; &lt;li&gt;RAG wasn't enough - needed a large context window to analyze everything at once for the best results.&lt;/li&gt; &lt;li&gt;Finding reliable medical sources was tough. Combined official guidelines with recent papers and trusted YouTube content.&lt;/li&gt; &lt;li&gt;GPT-O1 pro was best at root cause analysis, Google Note LLM worked great for citations, and Examine excelled at suggesting actions.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In the end, I built a system using Google Sheets to view my data and interact with trusted medical sources. It's been incredibly helpful in managing my condition and understanding my health better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Steak30"&gt; /u/Dry_Steak30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T11:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i73x81</id>
    <title>YOU CAN EXTRACT REASONING FROM R1 AND PASS IT ONTO ANY MODEL</title>
    <updated>2025-01-22T05:22:22+00:00</updated>
    <author>
      <name>/u/Sensitive-Finger-404</name>
      <uri>https://old.reddit.com/user/Sensitive-Finger-404</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i73x81/you_can_extract_reasoning_from_r1_and_pass_it/"&gt; &lt;img alt="YOU CAN EXTRACT REASONING FROM R1 AND PASS IT ONTO ANY MODEL" src="https://external-preview.redd.it/OG1uaHRydHljaGVlMeGKc_GKsNSHC_YJy3k1hv6gZ336TNH-m_F1sXruvXhI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=672f440c070400525909ae68b98c3deb34d98428" title="YOU CAN EXTRACT REASONING FROM R1 AND PASS IT ONTO ANY MODEL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;from @skirano on twitter&lt;/p&gt; &lt;p&gt;By the way, you can extract JUST the reasoning from deepseek-reasoner, which means you can send that thinking process to any model you want before they answer you. &lt;/p&gt; &lt;p&gt;Like here where I turn gpt-3.5 turbo into an absolute genius!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sensitive-Finger-404"&gt; /u/Sensitive-Finger-404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mbcqadwychee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i73x81/you_can_extract_reasoning_from_r1_and_pass_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i73x81/you_can_extract_reasoning_from_r1_and_pass_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T05:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1i75g7p</id>
    <title>I donâ€™t believe the $500 Billion OpenAI investment</title>
    <updated>2025-01-22T07:03:02+00:00</updated>
    <author>
      <name>/u/MattDTO</name>
      <uri>https://old.reddit.com/user/MattDTO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking at this deal, several things don't add up. The $500 billion figure is wildly optimistic - that's almost double what the entire US government committed to semiconductor manufacturing through the CHIPS Act. When you dig deeper, you see lots of vague promises but no real details about where the money's coming from or how they'll actually build anything.&lt;/p&gt; &lt;p&gt;The legal language is especially fishy. Instead of making firm commitments, they're using weasel words like &amp;quot;intends to,&amp;quot; &amp;quot;evaluating,&amp;quot; and &amp;quot;potential partnerships.&amp;quot; This isn't accidental - by running everything through Stargate, a new private company, and using this careful language, they've created a perfect shield for bigger players like SoftBank and Microsoft. If things go south, they can just blame &amp;quot;market conditions&amp;quot; and walk away with minimal exposure. Private companies like Stargate don't face the same strict disclosure requirements as public ones.&lt;/p&gt; &lt;p&gt;The timing is also telling - announcing this massive investment right after Trump won the presidency was clearly designed for maximum political impact. It fits perfectly into the narrative of bringing jobs and investment back to America. Using inflated job numbers for data centers (which typically employ relatively few people once built) while making vague promises about US technological leadership? Thatâ€™s politics.&lt;/p&gt; &lt;p&gt;My guess? There's probably a real data center project in the works, but it's being massively oversold for publicity and political gains. The actual investment will likely be much smaller, take longer to complete, and involve different partners than what's being claimed. This announcement just is a deal structured by lawyers who wanted to generate maximum headlines while minimizing any legal risk for their clients.â€‹â€‹â€‹â€‹&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MattDTO"&gt; /u/MattDTO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i75g7p/i_dont_believe_the_500_billion_openai_investment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i75g7p/i_dont_believe_the_500_billion_openai_investment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i75g7p/i_dont_believe_the_500_billion_openai_investment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T07:03:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1i78sfs</id>
    <title>Deepseek R1 GRPO code open sourced ðŸ¤¯</title>
    <updated>2025-01-22T11:11:56+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"&gt; &lt;img alt="Deepseek R1 GRPO code open sourced ðŸ¤¯" src="https://preview.redd.it/ryfnofs83jee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40e8bb4b0b8a7dd1b82c628a825c88559a17aff0" title="Deepseek R1 GRPO code open sourced ðŸ¤¯" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ryfnofs83jee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T11:11:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i77sy1</id>
    <title>How it feels...</title>
    <updated>2025-01-22T10:01:42+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i77sy1/how_it_feels/"&gt; &lt;img alt="How it feels..." src="https://preview.redd.it/rocl1zwsqiee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0cf91de1f8277f67834bb8422547f586738360e5" title="How it feels..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rocl1zwsqiee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i77sy1/how_it_feels/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i77sy1/how_it_feels/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T10:01:42+00:00</published>
  </entry>
</feed>
