<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-28T09:34:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ibxj3a</id>
    <title>Trump to impose 25% to 100% tariffs on Taiwan-made chips, impacting TSMC</title>
    <updated>2025-01-28T09:04:57+00:00</updated>
    <author>
      <name>/u/noblex33</name>
      <uri>https://old.reddit.com/user/noblex33</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxj3a/trump_to_impose_25_to_100_tariffs_on_taiwanmade/"&gt; &lt;img alt="Trump to impose 25% to 100% tariffs on Taiwan-made chips, impacting TSMC" src="https://external-preview.redd.it/AH_s6Lnngj4fg7u4p7ikli1G9UIpzFPfjMk_755j9_E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93f0138e6b1b669eee32d0888eddee9317da1a1b" title="Trump to impose 25% to 100% tariffs on Taiwan-made chips, impacting TSMC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noblex33"&gt; /u/noblex33 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/tech-industry/trump-to-impose-25-percent-100-percent-tariffs-on-taiwan-made-chips-impacting-tsmc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxj3a/trump_to_impose_25_to_100_tariffs_on_taiwanmade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxj3a/trump_to_impose_25_to_100_tariffs_on_taiwanmade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T09:04:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibu95h</id>
    <title>Baichuan-Omni-1.5: Open-source Omni-modal Foundation Model Supporting Text, Image, Video, and Audio Inputs as Well as Text and Audio Outputs</title>
    <updated>2025-01-28T05:10:23+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibu95h/baichuanomni15_opensource_omnimodal_foundation/"&gt; &lt;img alt="Baichuan-Omni-1.5: Open-source Omni-modal Foundation Model Supporting Text, Image, Video, and Audio Inputs as Well as Text and Audio Outputs" src="https://external-preview.redd.it/3fzIyV4anasZ5nx3BLJQChdXCbx3mvV0tNzVxaZzP68.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5065e6fdbd8d6865ade8ce0c089a659591780282" title="Baichuan-Omni-1.5: Open-source Omni-modal Foundation Model Supporting Text, Image, Video, and Audio Inputs as Well as Text and Audio Outputs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/baichuan-inc/Baichuan-Omni-1.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibu95h/baichuanomni15_opensource_omnimodal_foundation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibu95h/baichuanomni15_opensource_omnimodal_foundation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T05:10:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibc2vx</id>
    <title>Deepseek currently restricts new registrations to Chinese phone numbers only</title>
    <updated>2025-01-27T15:43:22+00:00</updated>
    <author>
      <name>/u/SysPsych</name>
      <uri>https://old.reddit.com/user/SysPsych</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See: &lt;a href="https://i.imgur.com/9WLAnko.png"&gt;https://i.imgur.com/9WLAnko.png&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SysPsych"&gt; /u/SysPsych &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibc2vx/deepseek_currently_restricts_new_registrations_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibc2vx/deepseek_currently_restricts_new_registrations_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibc2vx/deepseek_currently_restricts_new_registrations_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T15:43:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibhcsy</id>
    <title>Janus-Pro-7B first tests</title>
    <updated>2025-01-27T19:14:43+00:00</updated>
    <author>
      <name>/u/HugoDzz</name>
      <uri>https://old.reddit.com/user/HugoDzz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhcsy/januspro7b_first_tests/"&gt; &lt;img alt="Janus-Pro-7B first tests" src="https://preview.redd.it/24o1wy1z5lfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dedf99adb87d986aed80bf3ef760307490ebb0a4" title="Janus-Pro-7B first tests" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HugoDzz"&gt; /u/HugoDzz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/24o1wy1z5lfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhcsy/januspro7b_first_tests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhcsy/januspro7b_first_tests/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T19:14:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib4ksj</id>
    <title>How *exactly* is Deepseek so cheap?</title>
    <updated>2025-01-27T09:40:04+00:00</updated>
    <author>
      <name>/u/micamecava</name>
      <uri>https://old.reddit.com/user/micamecava</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek's all the rage. I get it, 95-97% reduction in costs. &lt;/p&gt; &lt;p&gt;How *exactly*? &lt;/p&gt; &lt;p&gt;Aside from cheaper training (not doing RLHF), quantization, and caching (semantic input HTTP caching I guess?), where's the reduction coming from? &lt;/p&gt; &lt;p&gt;This can't be all, because supposedly R1 isn't quantized. Right?&lt;/p&gt; &lt;p&gt;Is it subsidized? Is OpenAI/Anthropic just...charging too much? What's the deal?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/micamecava"&gt; /u/micamecava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4ksj/how_exactly_is_deepseek_so_cheap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4ksj/how_exactly_is_deepseek_so_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4ksj/how_exactly_is_deepseek_so_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T09:40:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibmser</id>
    <title>Good way of comparing robustness between R1 and its distills: division accuracy</title>
    <updated>2025-01-27T22:57:37+00:00</updated>
    <author>
      <name>/u/PC_Screen</name>
      <uri>https://old.reddit.com/user/PC_Screen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmser/good_way_of_comparing_robustness_between_r1_and/"&gt; &lt;img alt="Good way of comparing robustness between R1 and its distills: division accuracy" src="https://preview.redd.it/49ld6dks9mfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e2b4259f807f8d639b6383915783d0e37d2c8a0e" title="Good way of comparing robustness between R1 and its distills: division accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://x.com/TheXeophon/status/1883933054366015545"&gt;https://x.com/TheXeophon/status/1883933054366015545&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This shows that despite looking good on benchmarks (and being pretty good overall) the distilled versions are not nearly as robust as a model trained with actual rl (please ignore the fact a calculator would ace this). &lt;/p&gt; &lt;p&gt;The distills would almost certainly perform a lot better and more robustly if you did rl on them instead of just sft even if benchmarks stayed mostly the same.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PC_Screen"&gt; /u/PC_Screen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/49ld6dks9mfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmser/good_way_of_comparing_robustness_between_r1_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmser/good_way_of_comparing_robustness_between_r1_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T22:57:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibhco4</id>
    <title>Qwen2.5-VL are here</title>
    <updated>2025-01-27T19:14:36+00:00</updated>
    <author>
      <name>/u/x0wl</name>
      <uri>https://old.reddit.com/user/x0wl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhco4/qwen25vl_are_here/"&gt; &lt;img alt="Qwen2.5-VL are here" src="https://preview.redd.it/4x3qcn4y5lfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e30cac9b913dab31060cd7e7517ab0833399410" title="Qwen2.5-VL are here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/x0wl"&gt; /u/x0wl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4x3qcn4y5lfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhco4/qwen25vl_are_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhco4/qwen25vl_are_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T19:14:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibkydm</id>
    <title>1 Million Token Context Length üî•</title>
    <updated>2025-01-27T21:41:21+00:00</updated>
    <author>
      <name>/u/CelebrationClean7309</name>
      <uri>https://old.reddit.com/user/CelebrationClean7309</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibkydm/1_million_token_context_length/"&gt; &lt;img alt="1 Million Token Context Length üî•" src="https://preview.redd.it/uicxkqj6wlfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=701568383b2f300b0a9c905c9deac48d6fab5673" title="1 Million Token Context Length üî•" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CelebrationClean7309"&gt; /u/CelebrationClean7309 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uicxkqj6wlfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibkydm/1_million_token_context_length/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibkydm/1_million_token_context_length/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T21:41:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibm5u3</id>
    <title>How can we be so sure the training of Deepseek R1 is around $6 million?</title>
    <updated>2025-01-27T22:30:42+00:00</updated>
    <author>
      <name>/u/scmlfty</name>
      <uri>https://old.reddit.com/user/scmlfty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I heard their parent company is a quant fund that may be one of the the contributors that slashed the NVDA price today.&lt;/p&gt; &lt;p&gt;Besides this, how do we estimate this is possible? Or not far from achievable? Since the model does not include training dataset, is there a way for any organizations to do an estimation about it? Alex Wang said Deepseek has at least 50k H100, maybe more, and NVDA sold 20% of H100 to Singapore last year, which most of the cards could be used by Chinese companies.&lt;/p&gt; &lt;p&gt;What if today's NVDA price is just a sophisticated plot to make money for their quant fund?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/scmlfty"&gt; /u/scmlfty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibm5u3/how_can_we_be_so_sure_the_training_of_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibm5u3/how_can_we_be_so_sure_the_training_of_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibm5u3/how_can_we_be_so_sure_the_training_of_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T22:30:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibb8rr</id>
    <title>Qwen3.0 MOE? New Reasoning Model?</title>
    <updated>2025-01-27T15:09:24+00:00</updated>
    <author>
      <name>/u/Vishnu_One</name>
      <uri>https://old.reddit.com/user/Vishnu_One</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibb8rr/qwen30_moe_new_reasoning_model/"&gt; &lt;img alt="Qwen3.0 MOE? New Reasoning Model?" src="https://preview.redd.it/0vnua5vqxjfe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12861d7e6664e9cd7e45dd0710b87280d3a92aff" title="Qwen3.0 MOE? New Reasoning Model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vishnu_One"&gt; /u/Vishnu_One &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0vnua5vqxjfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibb8rr/qwen30_moe_new_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibb8rr/qwen30_moe_new_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T15:09:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibe7dn</id>
    <title>Nvidia faces $465 billion loss as DeepSeek disrupts AI market, largest in US market history</title>
    <updated>2025-01-27T17:09:57+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.financialexpress.com/business/investing-abroad-nvidia-faces-465-billion-loss-as-deepseek-disrupts-ai-market-3728093/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe7dn/nvidia_faces_465_billion_loss_as_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe7dn/nvidia_faces_465_billion_loss_as_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:09:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibqgpv</id>
    <title>LOCAL SUNO MUSIC GEN IS HERE!</title>
    <updated>2025-01-28T01:48:11+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibqgpv/local_suno_music_gen_is_here/"&gt; &lt;img alt="LOCAL SUNO MUSIC GEN IS HERE!" src="https://external-preview.redd.it/hyJLchSWtfVD2lbkrTS5uGW2psnrGe7122Ht2t5Tm1g.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ed8b7d59fa25922de70ea1f4d1976a3aa91d3a9" title="LOCAL SUNO MUSIC GEN IS HERE!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/_akhaliq/status/1884053159175414203"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibqgpv/local_suno_music_gen_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibqgpv/local_suno_music_gen_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T01:48:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibnso0</id>
    <title>Janus Pro 1B running 100% locally in-browser on WebGPU, powered by Transformers.js</title>
    <updated>2025-01-27T23:41:14+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/"&gt; &lt;img alt="Janus Pro 1B running 100% locally in-browser on WebGPU, powered by Transformers.js" src="https://external-preview.redd.it/cWlzM29xamVobWZlMRI2sWnlfg2FDSrRjInkFzMs3EdEEOYCx8go5Q48Mfdn.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9d961d6b16b8c23081f7f86352af9e4389b5637" title="Janus Pro 1B running 100% locally in-browser on WebGPU, powered by Transformers.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9v3xkqjehmfe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T23:41:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibd5x0</id>
    <title>DeepSeek releases deepseek-ai/Janus-Pro-7B (unified multimodal model).</title>
    <updated>2025-01-27T16:28:21+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibd5x0/deepseek_releases_deepseekaijanuspro7b_unified/"&gt; &lt;img alt="DeepSeek releases deepseek-ai/Janus-Pro-7B (unified multimodal model)." src="https://external-preview.redd.it/n5r1wVoNriwXCNjXkrw2Ab2zRN5UbL6aXFXA0wRQWRU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef80d96659edc7101cd569ddae687c24437596ba" title="DeepSeek releases deepseek-ai/Janus-Pro-7B (unified multimodal model)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/Janus-Pro-7B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibd5x0/deepseek_releases_deepseekaijanuspro7b_unified/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibd5x0/deepseek_releases_deepseekaijanuspro7b_unified/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T16:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibeub5</id>
    <title>llama.cpp PR with 99% of code written by Deepseek-R1</title>
    <updated>2025-01-27T17:35:22+00:00</updated>
    <author>
      <name>/u/nelson_moondialu</name>
      <uri>https://old.reddit.com/user/nelson_moondialu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibeub5/llamacpp_pr_with_99_of_code_written_by_deepseekr1/"&gt; &lt;img alt="llama.cpp PR with 99% of code written by Deepseek-R1" src="https://preview.redd.it/pfm0xpbaokfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3147a5b22a1d22f5ba48a7737734a1af6de28d53" title="llama.cpp PR with 99% of code written by Deepseek-R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nelson_moondialu"&gt; /u/nelson_moondialu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pfm0xpbaokfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibeub5/llamacpp_pr_with_99_of_code_written_by_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibeub5/llamacpp_pr_with_99_of_code_written_by_deepseekr1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibhew9</id>
    <title>Qwen Just launced a new SOTA multimodal model!, rivaling claude Sonnet and GPT-4o and it has open weights.</title>
    <updated>2025-01-27T19:17:00+00:00</updated>
    <author>
      <name>/u/brawll66</name>
      <uri>https://old.reddit.com/user/brawll66</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhew9/qwen_just_launced_a_new_sota_multimodal_model/"&gt; &lt;img alt="Qwen Just launced a new SOTA multimodal model!, rivaling claude Sonnet and GPT-4o and it has open weights." src="https://preview.redd.it/8811npnd6lfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e1491fe046f7dfb7fa4f728b0b182b4fae6b44b" title="Qwen Just launced a new SOTA multimodal model!, rivaling claude Sonnet and GPT-4o and it has open weights." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brawll66"&gt; /u/brawll66 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8811npnd6lfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhew9/qwen_just_launced_a_new_sota_multimodal_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhew9/qwen_just_launced_a_new_sota_multimodal_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T19:17:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibud4z</id>
    <title>This is my Japanese fine-tune of R1's Qwen 7B distil. It now outputs its thinking in Japanese, making it understandable for a Japanese audience. Model, code, and data all open source. I'd love to collab with y'all to make a more multilingual model.</title>
    <updated>2025-01-28T05:16:50+00:00</updated>
    <author>
      <name>/u/Peter_Lightblue</name>
      <uri>https://old.reddit.com/user/Peter_Lightblue</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibud4z/this_is_my_japanese_finetune_of_r1s_qwen_7b/"&gt; &lt;img alt="This is my Japanese fine-tune of R1's Qwen 7B distil. It now outputs its thinking in Japanese, making it understandable for a Japanese audience. Model, code, and data all open source. I'd love to collab with y'all to make a more multilingual model." src="https://external-preview.redd.it/E20r3Reg7pmGZc2kfkV-eNcOmjkN431vAWro6XAlkY4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aba1ff0b1621d1a37d4f41aba54db46dace6f4c5" title="This is my Japanese fine-tune of R1's Qwen 7B distil. It now outputs its thinking in Japanese, making it understandable for a Japanese audience. Model, code, and data all open source. I'd love to collab with y'all to make a more multilingual model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Peter_Lightblue"&gt; /u/Peter_Lightblue &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibud4z/this_is_my_japanese_finetune_of_r1s_qwen_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibud4z/this_is_my_japanese_finetune_of_r1s_qwen_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T05:16:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibmubo</id>
    <title>Just canceled my OpenAI Plus subscription (for now). Been running DeepSeek-R1 14b locally on my home workstation. I'll probably renew it if OpenAI launches something worthy for Plus tier by then.</title>
    <updated>2025-01-27T22:59:53+00:00</updated>
    <author>
      <name>/u/CarbonTail</name>
      <uri>https://old.reddit.com/user/CarbonTail</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmubo/just_canceled_my_openai_plus_subscription_for_now/"&gt; &lt;img alt="Just canceled my OpenAI Plus subscription (for now). Been running DeepSeek-R1 14b locally on my home workstation. I'll probably renew it if OpenAI launches something worthy for Plus tier by then." src="https://preview.redd.it/m45p2d9t9mfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d2ca8fc18451f5e82d99e58f0fde7474c05578c" title="Just canceled my OpenAI Plus subscription (for now). Been running DeepSeek-R1 14b locally on my home workstation. I'll probably renew it if OpenAI launches something worthy for Plus tier by then." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CarbonTail"&gt; /u/CarbonTail &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m45p2d9t9mfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmubo/just_canceled_my_openai_plus_subscription_for_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmubo/just_canceled_my_openai_plus_subscription_for_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T22:59:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibppfk</id>
    <title>Trump says deepseek is a very good thing</title>
    <updated>2025-01-28T01:10:34+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibppfk/trump_says_deepseek_is_a_very_good_thing/"&gt; &lt;img alt="Trump says deepseek is a very good thing" src="https://external-preview.redd.it/NTBqampmaml4bWZlMYjuPz-QA6cKORKkwtlZ19r0QTZXNxSGyroi1Nf1fV7W.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad8964c6efb2411c6e2c635b98018c3ddd3eda8d" title="Trump says deepseek is a very good thing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mn710sfgxmfe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibppfk/trump_says_deepseek_is_a_very_good_thing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibppfk/trump_says_deepseek_is_a_very_good_thing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T01:10:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibe1ro</id>
    <title>Thoughts? I kinda feel happy about this...</title>
    <updated>2025-01-27T17:03:47+00:00</updated>
    <author>
      <name>/u/Butefluko</name>
      <uri>https://old.reddit.com/user/Butefluko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe1ro/thoughts_i_kinda_feel_happy_about_this/"&gt; &lt;img alt="Thoughts? I kinda feel happy about this..." src="https://preview.redd.it/6b78kpulikfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f14a6edf107ecbf10ce8c437a2e0826bef5af67" title="Thoughts? I kinda feel happy about this..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Butefluko"&gt; /u/Butefluko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6b78kpulikfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe1ro/thoughts_i_kinda_feel_happy_about_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe1ro/thoughts_i_kinda_feel_happy_about_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:03:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibbloy</id>
    <title>1.58bit DeepSeek R1 - 131GB Dynamic GGUF</title>
    <updated>2025-01-27T15:24:00+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt; &lt;img alt="1.58bit DeepSeek R1 - 131GB Dynamic GGUF" src="https://external-preview.redd.it/uHOmNdCTHW-Q1CBdw01aifeSpeyvgfhjJI_lcC-SH5c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc7cd6ab7b35a273b107dce1a4113ba2c9dcca51" title="1.58bit DeepSeek R1 - 131GB Dynamic GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! I managed to &lt;strong&gt;dynamically quantize&lt;/strong&gt; the full DeepSeek R1 671B MoE to 1.58bits in GGUF format. The trick is &lt;strong&gt;not to quantize all layers&lt;/strong&gt;, but quantize only the MoE layers to 1.5bit, and leave attention and other layers in 4 or 6bit.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;MoE Bits&lt;/th&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Disk Size&lt;/th&gt; &lt;th align="left"&gt;Accuracy&lt;/th&gt; &lt;th align="left"&gt;HF Link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1.58bit&lt;/td&gt; &lt;td align="left"&gt;IQ1_S&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;131GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Fair&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1.73bit&lt;/td&gt; &lt;td align="left"&gt;IQ1_M&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;158GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Good&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_M"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.22bit&lt;/td&gt; &lt;td align="left"&gt;IQ2_XXS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;183GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Better&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ2_XXS"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.51bit&lt;/td&gt; &lt;td align="left"&gt;Q2_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;212GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Best&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;You can get &lt;strong&gt;140 tokens / s&lt;/strong&gt; on 2x H100 80GB GPUs with all layers offloaded. A 24GB GPU like RTX 4090 should be able to get at least 1 to 3 tokens / s.&lt;/p&gt; &lt;p&gt;If we naively quantize all layers to 1.5bit (-1, 0, 1), the model will fail dramatically, since it'll produce &lt;strong&gt;gibberish&lt;/strong&gt; and &lt;strong&gt;infinite repetitions&lt;/strong&gt;. I selectively leave all attention layers in 4/6bit, and leave the first 3 transformer dense layers in 4/6bit. The MoE layers take up 88% of all space, so we can leave them in 1.5bit. We get in total a weighted sum of 1.58bits!&lt;/p&gt; &lt;p&gt;I asked it the 1.58bit model to create Flappy Bird with 10 conditions (like random colors, a best score etc), and it did pretty well! Using a generic non dynamically quantized model will fail miserably - there will be no output at all!&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/k8nfun2ezjfe1.gif"&gt;Flappy Bird game made by 1.58bit R1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There's more details in the blog here: &lt;a href="https://unsloth.ai/blog/deepseekr1-dynamic"&gt;https://unsloth.ai/blog/deepseekr1-dynamic&lt;/a&gt; The link to the 1.58bit GGUF is here: &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S"&gt;https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S&lt;/a&gt; You should be able to run it in your favorite inference tool if it supports i matrix quants. No need to re-update llama.cpp.&lt;/p&gt; &lt;p&gt;A reminder on DeepSeek's chat template (for distilled versions as well) - it auto adds a BOS - do not add it manually!&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;lt;ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú&amp;gt;&amp;lt;ÔΩúUserÔΩú&amp;gt;What is 1+1?&amp;lt;ÔΩúAssistantÔΩú&amp;gt;It's 2.&amp;lt;ÔΩúend‚ñÅof‚ñÅsentenceÔΩú&amp;gt;&amp;lt;ÔΩúUserÔΩú&amp;gt;Explain more!&amp;lt;ÔΩúAssistantÔΩú&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;To know how many layers to offload to the GPU, I approximately calculated it as below:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;File Size&lt;/th&gt; &lt;th align="left"&gt;24GB GPU&lt;/th&gt; &lt;th align="left"&gt;80GB GPU&lt;/th&gt; &lt;th align="left"&gt;2x80GB GPU&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1.58bit&lt;/td&gt; &lt;td align="left"&gt;131GB&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;33&lt;/td&gt; &lt;td align="left"&gt;All layers 61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1.73bit&lt;/td&gt; &lt;td align="left"&gt;158GB&lt;/td&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;26&lt;/td&gt; &lt;td align="left"&gt;57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.22bit&lt;/td&gt; &lt;td align="left"&gt;183GB&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;22&lt;/td&gt; &lt;td align="left"&gt;49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.51bit&lt;/td&gt; &lt;td align="left"&gt;212GB&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;19&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;All other GGUFs for R1 are here: &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-R1-GGUF&lt;/a&gt; There's also GGUFs and dynamic 4bit bitsandbytes quants and others for all other distilled versions (Qwen, Llama etc) at &lt;a href="https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5"&gt;https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T15:24:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibmflv</id>
    <title>Deepseek censorship is more tolerable than Western censorship</title>
    <updated>2025-01-27T22:42:20+00:00</updated>
    <author>
      <name>/u/CreepyMan121</name>
      <uri>https://old.reddit.com/user/CreepyMan121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry, but I just had to say it. I can't speak for non westerners but as a U.S citizen I find that Deepseek is able to handle &amp;quot;sensitive topics&amp;quot; much better than SOTA models made here in America. I hate when people claim that Deepseeks censorship is worse than ours when in reality we both suffer from some kind of censorship. And please dont say &amp;quot;well Deepseek is controlled by CCP and stare sponsored cenorship&amp;quot;, because I don't really care, it doesn't effect me. Anyways, another win for Deepseek!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CreepyMan121"&gt; /u/CreepyMan121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmflv/deepseek_censorship_is_more_tolerable_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmflv/deepseek_censorship_is_more_tolerable_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmflv/deepseek_censorship_is_more_tolerable_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T22:42:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibldkl</id>
    <title>OpenAI reaction to Deepseek</title>
    <updated>2025-01-27T21:58:37+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibldkl/openai_reaction_to_deepseek/"&gt; &lt;img alt="OpenAI reaction to Deepseek" src="https://external-preview.redd.it/aHF1eWU4Zzl6bGZlMf4MrbzIQ8hanARSUc7k95AH77zTyfar1-pm8LqZJoap.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a2f1c75737234c364bf60db1855daaf3f441a02f" title="OpenAI reaction to Deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rf7tj9l9zlfe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibldkl/openai_reaction_to_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibldkl/openai_reaction_to_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T21:58:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibk9us</id>
    <title>Meta is reportedly scrambling multiple ‚Äòwar rooms‚Äô of engineers to figure out how DeepSeek‚Äôs AI is beating everyone else at a fraction of the price</title>
    <updated>2025-01-27T21:13:50+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibk9us/meta_is_reportedly_scrambling_multiple_war_rooms/"&gt; &lt;img alt="Meta is reportedly scrambling multiple ‚Äòwar rooms‚Äô of engineers to figure out how DeepSeek‚Äôs AI is beating everyone else at a fraction of the price" src="https://external-preview.redd.it/Brnl3ltRvrwiYwAXRD8-9ZQzXA_EE-2JvCrM0Zi5k8U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1046aa83b70828043ace549a5075989da27f1ff4" title="Meta is reportedly scrambling multiple ‚Äòwar rooms‚Äô of engineers to figure out how DeepSeek‚Äôs AI is beating everyone else at a fraction of the price" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From the article: &amp;quot;Of the four war rooms Meta has created to respond to DeepSeek‚Äôs potential breakthrough, two teams will try to decipher how High-Flyer lowered the cost of training and running DeepSeek with the goal of using those tactics for Llama, the outlet reported citing one anonymous Meta employee. &lt;/p&gt; &lt;p&gt;Among the remaining two teams, one will try to find out which data DeepSeek used to train its model, and the other will consider how Llama can restructure its models based on attributes of the DeepSeek models, The Information reported.&amp;quot;&lt;/p&gt; &lt;p&gt;I am actually excited by this. If Meta can figure it out, it means Llama 4 or 4.x will be substantially better. Hopefully we'll get a 70B dense model that's on part with DeepSeek.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://fortune.com/2025/01/27/mark-zuckerberg-meta-llama-assembling-war-rooms-engineers-deepseek-ai-china/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibk9us/meta_is_reportedly_scrambling_multiple_war_rooms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibk9us/meta_is_reportedly_scrambling_multiple_war_rooms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T21:13:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibej82</id>
    <title>OpenAI employee‚Äôs reaction to Deepseek</title>
    <updated>2025-01-27T17:23:12+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibej82/openai_employees_reaction_to_deepseek/"&gt; &lt;img alt="OpenAI employee‚Äôs reaction to Deepseek" src="https://preview.redd.it/ij7ubrn3mkfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db93fc1e3aea11120926d14eefcc127a43118a66" title="OpenAI employee‚Äôs reaction to Deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ij7ubrn3mkfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibej82/openai_employees_reaction_to_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibej82/openai_employees_reaction_to_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:23:12+00:00</published>
  </entry>
</feed>
