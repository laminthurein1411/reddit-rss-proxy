<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-13T14:06:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1l9fec7</id>
    <title>OpenAI delays their open source model claiming to add "something amazing" to it</title>
    <updated>2025-06-12T06:26:23+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9fec7/openai_delays_their_open_source_model_claiming_to/"&gt; &lt;img alt="OpenAI delays their open source model claiming to add &amp;quot;something amazing&amp;quot; to it" src="https://external-preview.redd.it/R_3_vozrpyNLk-RuPK719qfww-pDxCPb4GbyGYYEwIQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35f17ffcf6a5b86edfcd5643785889b2dd77fb23" title="OpenAI delays their open source model claiming to add &amp;quot;something amazing&amp;quot; to it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/06/10/openais-open-model-is-delayed"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9fec7/openai_delays_their_open_source_model_claiming_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l9fec7/openai_delays_their_open_source_model_claiming_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T06:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9ucsv</id>
    <title>Drummer's Agatha 111B v1 - Command A tune with less positivity and better creativity!</title>
    <updated>2025-06-12T18:43:11+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9ucsv/drummers_agatha_111b_v1_command_a_tune_with_less/"&gt; &lt;img alt="Drummer's Agatha 111B v1 - Command A tune with less positivity and better creativity!" src="https://external-preview.redd.it/8SUvc_SntqJPYJMpYlLHwIvKojeS37Q9MlW_-GMIUcs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9aa0d10c56f1afbb644a1a90d88ca6c1bbc9317" title="Drummer's Agatha 111B v1 - Command A tune with less positivity and better creativity!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;PSA! My testers at BeaverAI are pooped!&lt;/p&gt; &lt;p&gt;Cydonia needs your help! We're looking to release a v3.1 but came up with several candidates with their own strengths and weaknesses. They've all got tons of potential but we can only have ONE v3.1.&lt;/p&gt; &lt;p&gt;Help me pick the winner from these:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/BeaverAI/Cydonia-24B-v3j-GGUF"&gt;https://huggingface.co/BeaverAI/Cydonia-24B-v3j-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/BeaverAI/Cydonia-24B-v3i-GGUF"&gt;https://huggingface.co/BeaverAI/Cydonia-24B-v3i-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/BeaverAI/Cydonia-24B-v3h-GGUF"&gt;https://huggingface.co/BeaverAI/Cydonia-24B-v3h-GGUF&lt;/a&gt; (May ignore?)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/BeaverAI/Cydonia-24B-v3g-GGUF"&gt;https://huggingface.co/BeaverAI/Cydonia-24B-v3g-GGUF&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Agatha-111B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9ucsv/drummers_agatha_111b_v1_command_a_tune_with_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l9ucsv/drummers_agatha_111b_v1_command_a_tune_with_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T18:43:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1laa9bj</id>
    <title>New VS Code update supports all MCP features (tools, prompts, sampling, resources, auth)</title>
    <updated>2025-06-13T07:55:10+00:00</updated>
    <author>
      <name>/u/isidor_n</name>
      <uri>https://old.reddit.com/user/isidor_n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laa9bj/new_vs_code_update_supports_all_mcp_features/"&gt; &lt;img alt="New VS Code update supports all MCP features (tools, prompts, sampling, resources, auth)" src="https://external-preview.redd.it/4t6GOGdTOsYCXJc0r80Taopgc8TuG7QgRWRArsQ4GFY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da6a231d5b5f944073ecacc611122a4b945f2dd3" title="New VS Code update supports all MCP features (tools, prompts, sampling, resources, auth)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you have any questions about the release, let me know.&lt;/p&gt; &lt;p&gt;--vscode pm&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/isidor_n"&gt; /u/isidor_n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://code.visualstudio.com/updates/v1_101"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laa9bj/new_vs_code_update_supports_all_mcp_features/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1laa9bj/new_vs_code_update_supports_all_mcp_features/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T07:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1laf96d</id>
    <title>Mac Mini for local LLM? ü§î</title>
    <updated>2025-06-13T12:57:26+00:00</updated>
    <author>
      <name>/u/matlong</name>
      <uri>https://old.reddit.com/user/matlong</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not much of an IT guy. Example: I bought a Synology because I wanted a home server, but didn't want to fiddle with things beyond me too much.&lt;/p&gt; &lt;p&gt;That being said, I am a programmer that uses a Macbook every day.&lt;/p&gt; &lt;p&gt;Is it possible to go the on-prem home LLM route using a Mac Mini?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matlong"&gt; /u/matlong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laf96d/mac_mini_for_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laf96d/mac_mini_for_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1laf96d/mac_mini_for_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T12:57:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9yk8v</id>
    <title>Is AMD Ryzen AI Max+ 395 really the only consumer option for running Llama 70B locally?</title>
    <updated>2025-06-12T21:32:14+00:00</updated>
    <author>
      <name>/u/Single-Blackberry866</name>
      <uri>https://old.reddit.com/user/Single-Blackberry866</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Researching hardware for Llama 70B and keep hitting the same conclusion. AMD Ryzen AI Max+ 395 in Framework Desktop with 128GB unified memory seems like the only consumer device that can actually run 70B locally. RTX 4090 maxes at 24GB, Jetson AGX Orin hits 64GB, everything else needs rack servers with cooling and noise. The Framework setup should handle 70B in a quiet desktop form factor for around $3,000.&lt;/p&gt; &lt;p&gt;Is there something I'm missing? Other consumer hardware with enough memory? Anyone running 70B on less memory with extreme tricks? Or is 70B overkill vs 13B/30B for local use?&lt;/p&gt; &lt;p&gt;Reports say it should output 4-8 tokens per second, which seems slow for this price tag. Are my expectations too high? Any catch with this AMD solution?&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Thanks for responses! Should clarify my use case - looking for an always-on edge device that can sit quietish in a living room.&lt;/p&gt; &lt;p&gt;Requirements: - Linux-based (rules out Mac ecosystem) - Quietish operation (shouldn't cause headaches) - Lowish power consumption (always-on device) - Consumer form factor (not rack mount or multi-GPU)&lt;/p&gt; &lt;p&gt;The 2x3090 suggestions seem good for performance but would be like a noisy space heater. Maybe liquid cooling will help, but still be hot. Same issue with any multi-GPU setups - more like basement/server room solutions. Other GPU solutions seem expensive. Are they worth it?&lt;/p&gt; &lt;p&gt;I should reconsider whether 70B is necessary. If Qwen 32B performs similarly, that opens up devices like Jetson AGX Orin.&lt;/p&gt; &lt;p&gt;Anyone running 32B models on quiet, always-on setups? What's your experience with performance and noise levels?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Single-Blackberry866"&gt; /u/Single-Blackberry866 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9yk8v/is_amd_ryzen_ai_max_395_really_the_only_consumer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9yk8v/is_amd_ryzen_ai_max_395_really_the_only_consumer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l9yk8v/is_amd_ryzen_ai_max_395_really_the_only_consumer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T21:32:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1la6h3y</id>
    <title>[First Release!] Serene Pub - 0.1.0 Alpha - Linux/MacOS/Windows - Silly Tavern alternative</title>
    <updated>2025-06-13T03:56:02+00:00</updated>
    <author>
      <name>/u/doolijb</name>
      <uri>https://old.reddit.com/user/doolijb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1la6h3y/first_release_serene_pub_010_alpha/"&gt; &lt;img alt="[First Release!] Serene Pub - 0.1.0 Alpha - Linux/MacOS/Windows - Silly Tavern alternative" src="https://external-preview.redd.it/xLYJQVPvAzJH6LTy7XZaM08Yzp0dQfVS5ZcYjGMavrE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5d7f9507f484edd0bd289052a401f28884994be2" title="[First Release!] Serene Pub - 0.1.0 Alpha - Linux/MacOS/Windows - Silly Tavern alternative" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;# Introduction &lt;/p&gt; &lt;p&gt;Hey everyone! I got some moderate interest when I posted a week back about Serene Pub.&lt;/p&gt; &lt;p&gt;I'm proud to say that I've finally reached a point where I can release the first Alpha version of this app for preview, testing and feedback!&lt;/p&gt; &lt;p&gt;This is in development, there will be bugs!&lt;/p&gt; &lt;p&gt;There are releases for Linux, MacOS and Windows. I run Linux and can only test Mac and Windows in virtual machines, so I could use help testing with that. Thanks!&lt;/p&gt; &lt;p&gt;Currently, only Ollama is officially supported via ollama-js. Support for other connections are coming soon once Serene Tavern's connection API becomes more final.&lt;/p&gt; &lt;p&gt;# Screenshots&lt;/p&gt; &lt;p&gt;Attached are a handful of misc screenshots, showing mobile themes and desktop layouts.&lt;/p&gt; &lt;p&gt;# Download&lt;/p&gt; &lt;p&gt;- &lt;a href="https://github.com/doolijb/serene-pub/releases/tag/v0.1.0-alpha"&gt;Download here, for your favorite OS!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &lt;a href="https://github.com/doolijb/serene-pub/tree/v0.1.0-alpha"&gt;Download here, if you prefer running source code!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &lt;a href="https://github.com/doolijb/serene-pub"&gt;Repository home and readme.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;# Excerpt&lt;/p&gt; &lt;p&gt;Serene Pub is a modern, customizable chat application designed for immersive roleplay and creative conversations. Inspired by Silly Tavern, it aims to be more intuitive, responsive, and simple to configure.&lt;/p&gt; &lt;p&gt;Primary concerns Serene Pub aims to address:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Reduce the number of nested menus and settings.&lt;/li&gt; &lt;li&gt;Reduced visual clutter.&lt;/li&gt; &lt;li&gt;Manage settings server-side to prevent configurations from changing because the user switched windows/devices.&lt;/li&gt; &lt;li&gt;Make API calls &amp;amp; chat completion requests asyncronously server-side so they process regardless of window/device state.&lt;/li&gt; &lt;li&gt;Use sockets for all data, the user will see the same information updated across all windows/devices.&lt;/li&gt; &lt;li&gt;Have compatibility with the majority of Silly Tavern import/exports, i.e. Character Cards&lt;/li&gt; &lt;li&gt;Overall be a well rounded app with a suite of features. Use SillyTavern if you want the most options, features and plugin-support.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doolijb"&gt; /u/doolijb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1la6h3y"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1la6h3y/first_release_serene_pub_010_alpha/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1la6h3y/first_release_serene_pub_010_alpha/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T03:56:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ladl6d</id>
    <title>Finetune a model to think and use tools</title>
    <updated>2025-06-13T11:32:09+00:00</updated>
    <author>
      <name>/u/LostDog_88</name>
      <uri>https://old.reddit.com/user/LostDog_88</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im very new to Local AI tools, recently built a small Agno Team with agents to do a certain task, and its sort of good. I think it will improve after fine tuning on the tasks related to my prompts(code completion). Right now im using Qwen3:6b which can think and use tools.&lt;/p&gt; &lt;p&gt;1) How do i train models? I know Ollama is meant to run models, dont know which platform to use to train the models locally&lt;/p&gt; &lt;p&gt;2) How do i structure my data to train the models to have a chain of thought/think, and to use tools?&lt;/p&gt; &lt;p&gt;3) Do ya'll have any tips on how to grammatically structure the chain of thoughts/thinking?&lt;/p&gt; &lt;p&gt;Thank you so much!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostDog_88"&gt; /u/LostDog_88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ladl6d/finetune_a_model_to_think_and_use_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ladl6d/finetune_a_model_to_think_and_use_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ladl6d/finetune_a_model_to_think_and_use_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T11:32:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ladv5b</id>
    <title>Qwen2.5 VL</title>
    <updated>2025-06-13T11:47:19+00:00</updated>
    <author>
      <name>/u/Odd_Industry_2376</name>
      <uri>https://old.reddit.com/user/Odd_Industry_2376</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;Has anyone used this LLM for UI/UX? I would like a general opinion on it as I would like to set it up and fine-tune it for such purposes.&lt;/p&gt; &lt;p&gt;If you know models that are better for UI/UX, I would ask if you could recommend me some.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Industry_2376"&gt; /u/Odd_Industry_2376 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ladv5b/qwen25_vl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ladv5b/qwen25_vl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ladv5b/qwen25_vl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T11:47:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1labpb1</id>
    <title>Local Alternative to NotebookLM</title>
    <updated>2025-06-13T09:36:06+00:00</updated>
    <author>
      <name>/u/sv723</name>
      <uri>https://old.reddit.com/user/sv723</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm looking to run a local alternative to Google Notebook LM on a M2 with 32GB RAM in a one user scenario but with a lot of documents (~2k PDFs). Has anybody tried this? Are you aware of any tutorials?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sv723"&gt; /u/sv723 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1labpb1/local_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1labpb1/local_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1labpb1/local_alternative_to_notebooklm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T09:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9hzb5</id>
    <title>Google and Microsoft vs OpenAI and Anthropic, a fun visualization of their open releases on Hugging Face in the past year (Julien Chaumond on LinkedIn)</title>
    <updated>2025-06-12T09:21:44+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9hzb5/google_and_microsoft_vs_openai_and_anthropic_a/"&gt; &lt;img alt="Google and Microsoft vs OpenAI and Anthropic, a fun visualization of their open releases on Hugging Face in the past year (Julien Chaumond on LinkedIn)" src="https://preview.redd.it/2vdfa3f5sg6f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8cc62e350d3b3d31d64be11a8e4372ca1fc5f0e7" title="Google and Microsoft vs OpenAI and Anthropic, a fun visualization of their open releases on Hugging Face in the past year (Julien Chaumond on LinkedIn)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2vdfa3f5sg6f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9hzb5/google_and_microsoft_vs_openai_and_anthropic_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l9hzb5/google_and_microsoft_vs_openai_and_anthropic_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T09:21:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9rejn</id>
    <title>Qwen3-72B-Embiggened</title>
    <updated>2025-06-12T16:49:08+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9rejn/qwen372bembiggened/"&gt; &lt;img alt="Qwen3-72B-Embiggened" src="https://external-preview.redd.it/3jemtoTl3dbvGWwls0qD8rxMoJ2jFMtej9rCleQmntc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=caeaed61b9102d58a91296d431d16a9370486b24" title="Qwen3-72B-Embiggened" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/cognitivecomputations/Qwen3-72B-Embiggened"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9rejn/qwen372bembiggened/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l9rejn/qwen372bembiggened/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T16:49:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lafs7r</id>
    <title>Qwen3 embedding/reranker padding token error?</title>
    <updated>2025-06-13T13:21:23+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm new to embedding and rerankers. On paper they seem pretty straightforward:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;The embedding model turns tokens into numbers so models can process them more efficiently for retrieval. The embeddings are stored in an index. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The reranker simply ranks the text by similarity to the query. Its not perfect, but its a start. &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I tried experimenting with that over the last two days and the results are pretty good, but progress was stalled because I ran into this error after embedding a large text file and attempting to generate a query with &lt;code&gt;llamaindex&lt;/code&gt;:&lt;/p&gt; &lt;p&gt;&lt;code&gt; An error occurred: Cannot handle batch sizes &amp;gt; 1 if no padding token is defined. &lt;/code&gt;&lt;/p&gt; &lt;p&gt;As soon as I sent my query, I got this. The text was already indexed so I was hoping &lt;code&gt;llamaindex&lt;/code&gt; would use its &lt;code&gt;query engine&lt;/code&gt; to do everything after setting everything up. Here's what I did:&lt;/p&gt; &lt;p&gt;1 - Create the embeddings using &lt;code&gt;Qwen3-embeddings-0.6B&lt;/code&gt; and store the embeddings in an index file - this was done quickly. I used &lt;code&gt;llama index&lt;/code&gt;'s &lt;code&gt;SemanticDoubleMergingSplitterNodeParser&lt;/code&gt; with a maximum chunk size of 8192 tokens, the same amount as the context length set for &lt;code&gt;Qwen3-embeddings-0.6B&lt;/code&gt;, to intelligently chunk the text. This is a more advanced form of semantic chunking that not only chunks based on similarity to its immediate neighbor, but also looks two chunks ahead to see if the second chunk ahead is similar to the first one, merging all three within a set threshold if they line up. &lt;/p&gt; &lt;p&gt;This is good for breaking up related sequences of paragraphs and is usually my go-to chunker, like a paragraph of text describing a math formula, then displaying the formula before elaborating further in a subsequent paragraph.&lt;/p&gt; &lt;p&gt;2 - Load that same index with the same embedding model, then try to rerank the query using &lt;code&gt;qwen3-Reranker-4b&lt;/code&gt; and send it to &lt;code&gt;Qwen3-4b-q8_0&lt;/code&gt; for Q&amp;amp;A sessions. This would all be handle with three components:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;llamaindex's &lt;code&gt;Ollama&lt;/code&gt; class for LLM.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The &lt;code&gt;VectorIndexRetriever&lt;/code&gt; class.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The &lt;code&gt;RetrieverQueryEngine&lt;/code&gt; class to serve as the retriever, at which point you would send the query to and receive a response.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The error message I encountered above was related to a 500-page pdf file in which I used &lt;code&gt;Gemma3-27b-it-qat&lt;/code&gt; on Ollama to read the entire document's contents via OCR and convert it into text and save it as a markdown file, with highly accurate results, except for the occasional infinite loop that I would max out the output at around 1600 tokens. &lt;/p&gt; &lt;p&gt;But when I took another pre-written &lt;code&gt;.md&lt;/code&gt; file, a one-page &lt;code&gt;.md&lt;/code&gt; file, Everything worked just fine. &lt;/p&gt; &lt;p&gt;So this leads me to two possible culprits:&lt;/p&gt; &lt;p&gt;1 - The file was too big or its contents were too difficult for the &lt;code&gt;SemanticDoubleMergingSplitterNodeParser&lt;/code&gt; class to chunk effectively or it was too difficult for the embedding model to process effectively. &lt;/p&gt; &lt;p&gt;2 - The original &lt;code&gt;.md&lt;/code&gt; file's indexed contents were messing something up on the tokenization side of things, since the &lt;code&gt;.md&lt;/code&gt; file was all text, but contained a lot of links, drawn tables by &lt;code&gt;Gemma3&lt;/code&gt; and a lot of other contents. &lt;/p&gt; &lt;p&gt;This is a little confusing to me, but I think I'm on the right track. I like &lt;code&gt;llamaindex&lt;/code&gt; because its modular, with lots of plug-and-play features that I can add to the script. &lt;/p&gt; &lt;p&gt;EDIT: Mixed up model names.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lafs7r/qwen3_embeddingreranker_padding_token_error/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lafs7r/qwen3_embeddingreranker_padding_token_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lafs7r/qwen3_embeddingreranker_padding_token_error/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T13:21:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9p54x</id>
    <title>Nanonets-OCR-s: An Open-Source Image-to-Markdown Model with LaTeX, Tables, Signatures, checkboxes &amp; More</title>
    <updated>2025-06-12T15:19:41+00:00</updated>
    <author>
      <name>/u/SouvikMandal</name>
      <uri>https://old.reddit.com/user/SouvikMandal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9p54x/nanonetsocrs_an_opensource_imagetomarkdown_model/"&gt; &lt;img alt="Nanonets-OCR-s: An Open-Source Image-to-Markdown Model with LaTeX, Tables, Signatures, checkboxes &amp;amp; More" src="https://external-preview.redd.it/_xcgPKgts5yF5jBIuB89LBBM6G1OD9-qimuRMUyq8jY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a45bdc43de827d3d2f41c39e7eb1c65d82e73b20" title="Nanonets-OCR-s: An Open-Source Image-to-Markdown Model with LaTeX, Tables, Signatures, checkboxes &amp;amp; More" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're excited to share &lt;strong&gt;Nanonets-OCR-s&lt;/strong&gt;, a powerful and lightweight (3B) VLM model that converts documents into clean, structured &lt;strong&gt;Markdown&lt;/strong&gt;. This model is trained to understand document structure and content context (like tables, equations, images, plots, watermarks, checkboxes, etc.).&lt;/p&gt; &lt;p&gt;üîç &lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;LaTeX Equation Recognition&lt;/strong&gt; Converts inline and block-level math into properly formatted LaTeX, distinguishing between &lt;code&gt;$...$&lt;/code&gt; and &lt;code&gt;$$...$$&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Image Descriptions for LLMs&lt;/strong&gt; Describes embedded images using structured &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; tags. Handles logos, charts, plots, and so on.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Signature Detection &amp;amp; Isolation&lt;/strong&gt; Finds and tags signatures in scanned documents, outputting them in &lt;code&gt;&amp;lt;signature&amp;gt;&lt;/code&gt; blocks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Watermark Extraction&lt;/strong&gt; Extracts watermark text and stores it within &lt;code&gt;&amp;lt;watermark&amp;gt;&lt;/code&gt; tag for traceability.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Checkbox &amp;amp; Radio Button Handling&lt;/strong&gt; Converts checkboxes to Unicode symbols like ‚òë, ‚òí, and ‚òê for reliable parsing in downstream apps.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complex Table Extraction&lt;/strong&gt; Handles multi-row/column tables, preserving structure and outputting both &lt;strong&gt;Markdown&lt;/strong&gt; and &lt;strong&gt;HTML&lt;/strong&gt; formats.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Huggingface / GitHub / Try it out&lt;/strong&gt;:&lt;br /&gt; &lt;a href="https://huggingface.co/nanonets/Nanonets-OCR-s"&gt;Huggingface Model Card&lt;/a&gt;&lt;br /&gt; &lt;a href="https://nanonets.com/research/nanonets-ocr-s/"&gt;Read the full announcement&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/NanoNets/docext/blob/main/PDF2MD_README.md#quickstart"&gt;Try it with Docext in Colab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9r53s8oxii6f1.png?width=1762&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d401151504b45c6c7b7aa49342a2a40bff19a3d"&gt;Document with checkbox and radio buttons&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ky28mxc1ji6f1.jpg?width=3938&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0f0ba053a366c0fd0885aea5785b5c040ff590fd"&gt;Document with image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yfrazoi3ji6f1.png?width=3640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4215d426d90f153c5a477f140aa47312e153aab8"&gt;Document with equations&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/am74wtm5ji6f1.jpg?width=1533&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=380788a942ee270cf73ddc7e968773948b4f74f0"&gt;Document with watermark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6g80yoj9ji6f1.png?width=3482&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d6008f9e56e03e7589f38a16ba1917e4e0c419b"&gt;Document with tables&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to try it out and share your feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SouvikMandal"&gt; /u/SouvikMandal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9p54x/nanonetsocrs_an_opensource_imagetomarkdown_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9p54x/nanonetsocrs_an_opensource_imagetomarkdown_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l9p54x/nanonetsocrs_an_opensource_imagetomarkdown_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T15:19:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1la3xni</id>
    <title>Happy Birthday Transformers!</title>
    <updated>2025-06-13T01:40:38+00:00</updated>
    <author>
      <name>/u/sksq9</name>
      <uri>https://old.reddit.com/user/sksq9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1la3xni/happy_birthday_transformers/"&gt; &lt;img alt="Happy Birthday Transformers!" src="https://external-preview.redd.it/-WGucDJrfXWGk82HcV3sYvI56KgBvSq6Ts-J6hHyLl0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9dc544bc2b53c9634b6322ef6ef5b0018f8e039" title="Happy Birthday Transformers!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sksq9"&gt; /u/sksq9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/sksq96/status/1933335774100857090?s=46"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1la3xni/happy_birthday_transformers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1la3xni/happy_birthday_transformers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T01:40:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1la3uvz</id>
    <title>3.53bit R1 0528 scores 68% on the Aider Polygot</title>
    <updated>2025-06-13T01:36:43+00:00</updated>
    <author>
      <name>/u/BumblebeeOk3281</name>
      <uri>https://old.reddit.com/user/BumblebeeOk3281</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;3.53bit R1 0528 scores 68% on the Aider Polyglot benchmark.&lt;/p&gt; &lt;p&gt;ram/vram required: 300GB&lt;/p&gt; &lt;p&gt;context size used: 40960 with flash attention&lt;/p&gt; &lt;p&gt;Edit 1: Polygot &amp;gt;&amp;gt; Polyglot :-)&lt;/p&gt; &lt;p&gt;Edit 2: *this was a download from a few days before the &amp;lt;tool\_calling&amp;gt; improvements Unsloth did 2 days ago. We will maybe do one more benchmark perhaps the updated &amp;quot;UD-IQ2_M&amp;quot;.&lt;/p&gt; &lt;p&gt;Edit 3: Unsloth 1.93bit UD_IQ1_M scored 60%&lt;/p&gt; &lt;p&gt;‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ- dirname: 2025-06-11-04-03-18--unsloth-DeepSeek-R1-0528-GGUF-UD-Q3_K_XL&lt;/p&gt; &lt;p&gt;test_cases: 225&lt;/p&gt; &lt;p&gt;model: openai/unsloth/DeepSeek-R1-0528-GGUF/UD-Q3_K_XL&lt;/p&gt; &lt;p&gt;edit_format: diff&lt;/p&gt; &lt;p&gt;commit_hash: 4c161f9-dirty&lt;/p&gt; &lt;p&gt;pass_rate_1: 32.9&lt;/p&gt; &lt;p&gt;pass_rate_2: 68.0&lt;/p&gt; &lt;p&gt;pass_num_1: 74&lt;/p&gt; &lt;p&gt;pass_num_2: 153&lt;/p&gt; &lt;p&gt;percent_cases_well_formed: 96.4&lt;/p&gt; &lt;p&gt;error_outputs: 15&lt;/p&gt; &lt;p&gt;num_malformed_responses: 15&lt;/p&gt; &lt;p&gt;num_with_malformed_responses: 8&lt;/p&gt; &lt;p&gt;user_asks: 72&lt;/p&gt; &lt;p&gt;lazy_comments: 0&lt;/p&gt; &lt;p&gt;syntax_errors: 0&lt;/p&gt; &lt;p&gt;indentation_errors: 0&lt;/p&gt; &lt;p&gt;exhausted_context_windows: 0&lt;/p&gt; &lt;p&gt;prompt_tokens: 2596907&lt;/p&gt; &lt;p&gt;completion_tokens: 2297409&lt;/p&gt; &lt;p&gt;test_timeouts: 2&lt;/p&gt; &lt;p&gt;total_tests: 225&lt;/p&gt; &lt;p&gt;command: aider --model openai/unsloth/DeepSeek-R1-0528-GGUF/UD-Q3_K_XL&lt;/p&gt; &lt;p&gt;date: 2025-06-11&lt;/p&gt; &lt;p&gt;versions: &lt;a href="http://0.84.1.dev"&gt;0.84.1.dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;seconds_per_case: 485.7&lt;/p&gt; &lt;p&gt;total_cost: 0.0000&lt;/p&gt; &lt;p&gt;‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BumblebeeOk3281"&gt; /u/BumblebeeOk3281 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1la3uvz/353bit_r1_0528_scores_68_on_the_aider_polygot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1la3uvz/353bit_r1_0528_scores_68_on_the_aider_polygot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1la3uvz/353bit_r1_0528_scores_68_on_the_aider_polygot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T01:36:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1la1v4d</id>
    <title>llama.cpp adds support to two new quantization format, tq1_0 and tq2_0</title>
    <updated>2025-06-12T23:59:21+00:00</updated>
    <author>
      <name>/u/Remarkable-Pea645</name>
      <uri>https://old.reddit.com/user/Remarkable-Pea645</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;which can be found at tools/convert_hf_to_gguf.py on github.&lt;/p&gt; &lt;p&gt;tq means ternary quantization, what's this? is for consumer device?&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; I have tried tq1_0 both llama.cpp on qwen3-8b and sd.cpp on flux. despite quantizing is fast, tq1_0 is hard to work at now time: qwen3 outputs messy chars while flux is 30x slower than k-quants after dequantizing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Pea645"&gt; /u/Remarkable-Pea645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1la1v4d/llamacpp_adds_support_to_two_new_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1la1v4d/llamacpp_adds_support_to_two_new_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1la1v4d/llamacpp_adds_support_to_two_new_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T23:59:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lafihl</id>
    <title>Struggling on local multi-user inference? Llama.cpp GGUF vs VLLM AWQ/GPTQ.</title>
    <updated>2025-06-13T13:09:07+00:00</updated>
    <author>
      <name>/u/SomeRandomGuuuuuuy</name>
      <uri>https://old.reddit.com/user/SomeRandomGuuuuuuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I tested VLLM and Llama.cpp and got much better results from GGUF than AWQ and GPTQ (it was also hard to find this format for VLLM). I used the same system prompts and saw really crazy bad results on Gemma in GPTQ: higher VRAM usage, slower inference, and worse output quality.&lt;/p&gt; &lt;p&gt;Now my project is moving to multiple concurrent users, so I will need parallelism. I'm using either A10 AWS instances or L40s etc.&lt;/p&gt; &lt;p&gt;From my understanding, Llama.cpp is not optimal for the efficiency and concurrency I need, as I want to squeeze the as much request with same or smillar time for one and minimize VRAM usage if possible. I like GGUF as it's so easy to find good quantizations, but I'm wondering if I should switch back to VLLM.&lt;/p&gt; &lt;p&gt;I also considered Triton / NVIDIA Inference Server / Dynamo, but I'm not sure what's currently the best option for this workload.&lt;/p&gt; &lt;p&gt;Here is my current Docker setup for llama.cpp:&lt;/p&gt; &lt;p&gt;&lt;code&gt;cpp_3.1.8B:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;image:&lt;/code&gt; &lt;a href="http://ghcr.io/ggml-org/llama.cpp:server-cuda"&gt;&lt;code&gt;ghcr.io/ggml-org/llama.cpp:server-cuda&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;container_name: cpp_3.1.8B&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ports:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- 8003:8003&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;volumes:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- ./models/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf:/model/model.gguf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;environment:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;LLAMA_ARG_MODEL: /model/model.gguf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;LLAMA_ARG_CTX_SIZE: 4096&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;LLAMA_ARG_N_PARALLEL: 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;LLAMA_ARG_MAIN_GPU: 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;LLAMA_ARG_N_GPU_LAYERS: 99&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;LLAMA_ARG_ENDPOINT_METRICS: 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;LLAMA_ARG_PORT: 8003&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;LLAMA_ARG_FLASH_ATTN: 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;GGML_CUDA_FORCE_MMQ: 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;GGML_CUDA_FORCE_CUBLAS: 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;deploy:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;resources:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;reservations:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;devices:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- driver: nvidia&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;count: all&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;capabilities: [gpu]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And for vllm:&lt;br /&gt; &lt;code&gt;sudo docker run --runtime nvidia --gpus all \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-v ~/.cache/huggingface:/root/.cache/huggingface \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--env &amp;quot;HUGGING_FACE_HUB_TOKEN= \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-p 8003:8000 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--ipc=host \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--name gemma12bGPTQ \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--user 0 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;vllm/vllm-openai:latest \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--model circulus/gemma-3-12b-it-gptq \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--gpu_memory_utilization=0.80 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--max_model_len=4096&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I would greatly appreciate feedback from people who have been through this ‚Äî what stack works best for you today for maximum concurrent users? Should I fully switch back to VLLM? Is Triton / Nvidia NIM / Dynamo inference worth exploring or smth else?&lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeRandomGuuuuuuy"&gt; /u/SomeRandomGuuuuuuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lafihl/struggling_on_local_multiuser_inference_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lafihl/struggling_on_local_multiuser_inference_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lafihl/struggling_on_local_multiuser_inference_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T13:09:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1labaqn</id>
    <title>Introducing the Hugging Face MCP Server - find, create and use AI models directly from VSCode, Cursor, Claude or other clients! ü§ó</title>
    <updated>2025-06-13T09:08:15+00:00</updated>
    <author>
      <name>/u/vaibhavs10</name>
      <uri>https://old.reddit.com/user/vaibhavs10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey hey, everyone, I'm VB from Hugging Face. We're tinkering a lot with MCP at HF these days and are quite excited to host our official MCP server accessible at `hf.co/mcp` üî•&lt;/p&gt; &lt;p&gt;Here's what you can do today with it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;You can run semantic search on datasets, spaces and models (find the correct artefact just with text)&lt;/li&gt; &lt;li&gt;Get detailed information about these artefacts&lt;/li&gt; &lt;li&gt;My favorite: Use any MCP compatible space directly in your downstream clients (let our GPUs run wild and free üòà) &lt;a href="https://huggingface.co/spaces?filter=mcp-server"&gt;https://huggingface.co/spaces?filter=mcp-server&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Bonus: We provide ready to use snippets to use it in VSCode, Cursor, Claude and any other client!&lt;/p&gt; &lt;p&gt;This is still an early beta version, but we're excited to see how you'd play with it today. Excited to hear your feedback or comments about it! Give it a shot @ &lt;a href="http://hf.co/mcp"&gt;hf.co/mcp&lt;/a&gt; ü§ó&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaibhavs10"&gt; /u/vaibhavs10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1labaqn/introducing_the_hugging_face_mcp_server_find/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1labaqn/introducing_the_hugging_face_mcp_server_find/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1labaqn/introducing_the_hugging_face_mcp_server_find/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T09:08:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9lddr</id>
    <title>Petition: Ban 'announcement of announcement' posts</title>
    <updated>2025-06-12T12:36:45+00:00</updated>
    <author>
      <name>/u/RangaRea</name>
      <uri>https://old.reddit.com/user/RangaRea</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's no reason to have 5 posts a week about OpenAI announcing that they will release a model then delaying the release date it then announcing it's gonna be &lt;em&gt;amazing&lt;/em&gt;&lt;strong&gt;‚Ñ¢&lt;/strong&gt; then announcing they will announce a new update in a month ad infinitum. Fuck those grifters.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RangaRea"&gt; /u/RangaRea &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9lddr/petition_ban_announcement_of_announcement_posts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9lddr/petition_ban_announcement_of_announcement_posts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l9lddr/petition_ban_announcement_of_announcement_posts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T12:36:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9wbaw</id>
    <title>Meta Is Offering Nine Figure Salaries to Build Superintelligent AI. Mark going All In.</title>
    <updated>2025-06-12T20:00:39+00:00</updated>
    <author>
      <name>/u/Neon_Nomad45</name>
      <uri>https://old.reddit.com/user/Neon_Nomad45</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.entrepreneur.com/business-news/meta-is-offering-nine-figure-pay-for-superintelligence-team/493040"&gt;https://www.entrepreneur.com/business-news/meta-is-offering-nine-figure-pay-for-superintelligence-team/493040&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neon_Nomad45"&gt; /u/Neon_Nomad45 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9wbaw/meta_is_offering_nine_figure_salaries_to_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9wbaw/meta_is_offering_nine_figure_salaries_to_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l9wbaw/meta_is_offering_nine_figure_salaries_to_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T20:00:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1la91hz</id>
    <title>Llama-Server Launcher (Python with performance CUDA focus)</title>
    <updated>2025-06-13T06:31:45+00:00</updated>
    <author>
      <name>/u/LA_rent_Aficionado</name>
      <uri>https://old.reddit.com/user/LA_rent_Aficionado</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1la91hz/llamaserver_launcher_python_with_performance_cuda/"&gt; &lt;img alt="Llama-Server Launcher (Python with performance CUDA focus)" src="https://preview.redd.it/lwjqunrt0n6f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4334cf1869792277cb832843144b80fa08950e05" title="Llama-Server Launcher (Python with performance CUDA focus)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share a llama-server launcher I put together for my personal use. I got tired of maintaining bash scripts and notebook files and digging through my gaggle of model folders while testing out models and turning performance. Hopefully this helps make someone else's life easier, it certainly has for me. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github repo:&lt;/strong&gt; &lt;a href="https://github.com/thad0ctor/llama-server-launcher"&gt;https://github.com/thad0ctor/llama-server-launcher&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;üß© Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üñ•Ô∏è &lt;strong&gt;Clean GUI&lt;/strong&gt; with tabs for: &lt;ul&gt; &lt;li&gt;Basic settings (model, paths, context, batch)&lt;/li&gt; &lt;li&gt;GPU/performance tuning (offload, FlashAttention, tensor split, batches, etc.)&lt;/li&gt; &lt;li&gt;Chat template selection (predefined, model default, or custom Jinja2)&lt;/li&gt; &lt;li&gt;Environment variables (GGML_CUDA_*, custom vars)&lt;/li&gt; &lt;li&gt;Config management (save/load/import/export)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;üß† &lt;strong&gt;Auto GPU + system info&lt;/strong&gt; via PyTorch or manual override&lt;/li&gt; &lt;li&gt;üßæ &lt;strong&gt;Model analyzer&lt;/strong&gt; for GGUF (layers, size, type) with fallback support&lt;/li&gt; &lt;li&gt;üíæ &lt;strong&gt;Script generation&lt;/strong&gt; (.ps1 / .sh) from your launch settings&lt;/li&gt; &lt;li&gt;üõ†Ô∏è &lt;strong&gt;Cross-platform:&lt;/strong&gt; Works on Windows/Linux (macOS untested)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;üì¶ Recommended Python deps:&lt;/strong&gt;&lt;br /&gt; &lt;code&gt;torch&lt;/code&gt;, &lt;code&gt;llama-cpp-python&lt;/code&gt;, &lt;code&gt;psutil&lt;/code&gt; (optional but useful for calculating gpu layers and selecting GPUs)&lt;/p&gt; &lt;p&gt;![Advanced Settings](&lt;a href="https://raw.githubusercontent.com/thad0ctor/llama-server-launcher/main/images/advanced.png"&gt;https://raw.githubusercontent.com/thad0ctor/llama-server-launcher/main/images/advanced.png&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;![Chat Templates](&lt;a href="https://raw.githubusercontent.com/thad0ctor/llama-server-launcher/main/images/chat-templates.png"&gt;https://raw.githubusercontent.com/thad0ctor/llama-server-launcher/main/images/chat-templates.png&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;![Configuration Management](&lt;a href="https://raw.githubusercontent.com/thad0ctor/llama-server-launcher/main/images/configs.png"&gt;https://raw.githubusercontent.com/thad0ctor/llama-server-launcher/main/images/configs.png&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;![Environment Variables](&lt;a href="https://raw.githubusercontent.com/thad0ctor/llama-server-launcher/main/images/env.png"&gt;https://raw.githubusercontent.com/thad0ctor/llama-server-launcher/main/images/env.png&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LA_rent_Aficionado"&gt; /u/LA_rent_Aficionado &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lwjqunrt0n6f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1la91hz/llamaserver_launcher_python_with_performance_cuda/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1la91hz/llamaserver_launcher_python_with_performance_cuda/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T06:31:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1laazto</id>
    <title>The EuroLLM team released preview versions of several new models</title>
    <updated>2025-06-13T08:47:09+00:00</updated>
    <author>
      <name>/u/sommerzen</name>
      <uri>https://old.reddit.com/user/sommerzen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They released a 22b version, 2 vision models (1.7b, 9b, based on the older EuroLLMs) and a small MoE with 0.6b active and 2.6b total parameters. The MoE seems to be surprisingly good for its size in my limited testing. They seem to be Apache-2.0 licensed.&lt;/p&gt; &lt;p&gt;EuroLLM 22b instruct preview: &lt;a href="https://huggingface.co/utter-project/EuroLLM-22B-Instruct-Preview"&gt;https://huggingface.co/utter-project/EuroLLM-22B-Instruct-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EuroLLM 22b base preview: &lt;a href="https://huggingface.co/utter-project/EuroLLM-22B-Preview"&gt;https://huggingface.co/utter-project/EuroLLM-22B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EuroMoE 2.6B-A0.6B instruct preview: &lt;a href="https://huggingface.co/utter-project/EuroMoE-2.6B-A0.6B-Instruct-Preview"&gt;https://huggingface.co/utter-project/EuroMoE-2.6B-A0.6B-Instruct-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EuroMoE 2.6B-A0.6B base preview: &lt;a href="https://huggingface.co/utter-project/EuroMoE-2.6B-A0.6B-Preview"&gt;https://huggingface.co/utter-project/EuroMoE-2.6B-A0.6B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EuroVLM 1.7b instruct preview: &lt;a href="https://huggingface.co/utter-project/EuroVLM-1.7B-Preview"&gt;https://huggingface.co/utter-project/EuroVLM-1.7B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EuroVLM 9b instruct preview: &lt;a href="https://huggingface.co/utter-project/EuroVLM-9B-Preview"&gt;https://huggingface.co/utter-project/EuroVLM-9B-Preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sommerzen"&gt; /u/sommerzen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laazto/the_eurollm_team_released_preview_versions_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laazto/the_eurollm_team_released_preview_versions_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1laazto/the_eurollm_team_released_preview_versions_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T08:47:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lacqxh</id>
    <title>Against the Apple's paper: LLM can solve new complex problems</title>
    <updated>2025-06-13T10:44:17+00:00</updated>
    <author>
      <name>/u/WackyConundrum</name>
      <uri>https://old.reddit.com/user/WackyConundrum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lacqxh/against_the_apples_paper_llm_can_solve_new/"&gt; &lt;img alt="Against the Apple's paper: LLM can solve new complex problems" src="https://external-preview.redd.it/diKLwubmbtEotRQN3uKfVq9qOQ6ZisOE0UOpfwELjRg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65a0f7db35926d7e688235ee33fc2c2411b4b025" title="Against the Apple's paper: LLM can solve new complex problems" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/omx5izshbo6f1.png?width=1132&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5ac1691fa08d5bfcf23930daef24e448934734c3"&gt;https://preview.redd.it/omx5izshbo6f1.png?width=1132&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5ac1691fa08d5bfcf23930daef24e448934734c3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/rohanpaul_ai/status/1933296859730301353"&gt;Explanation by Rohan Paul from Twitter&lt;/a&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A follow-up study on Apple's &amp;quot;Illusion of Thinking&amp;quot; Paper is published now. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Shows the same models succeed once the format lets them give compressed answers, proving the earlier collapse was a measurement artifact. &lt;/p&gt; &lt;p&gt;Token limits, not logic, froze the models. &lt;/p&gt; &lt;p&gt;Collapse vanished once the puzzles fit the context window. &lt;/p&gt; &lt;p&gt;So Models failed the rubric, not the reasoning. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Core Concepts&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Large Reasoning Models add chain-of-thought tokens and self-checks on top of standard language models. The Illusion of Thinking paper pushed them through four controlled puzzles, steadily raising complexity to track how accuracy and token use scale. The authors saw accuracy plunge to zero and reasoned that thinking itself had hit a hard limit. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Puzzle-Driven Evaluation&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Tower of Hanoi forced models to print every move; River Crossing demanded safe boat trips under strict capacity. Because a solution for forty-plus moves already eats thousands of tokens, the move-by-move format made token budgets explode long before reasoning broke. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Collapse Appeared&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;The comment paper pinpoints three test artifacts: token budgets were exceeded, evaluation scripts flagged deliberate truncation as failure, and some River Crossing instances were mathematically unsolvable yet still graded. Together these artifacts masqueraded as cognitive limits. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fixing the Test&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;When researchers asked the same models to output a compact Lua function that generates the Hanoi solution, models solved fifteen-disk cases in under five thousand tokens with high accuracy, overturning the zero-score narrative.&lt;/p&gt; &lt;p&gt;Abstract:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit &amp;quot;accuracy collapse&amp;quot; on planning puzzles beyond certain complexity thresholds. We demonstrate that their findings primarily reflect experimental design limitations rather than fundamental reasoning failures. Our analysis reveals three critical issues: (1) Tower of Hanoi experiments systematically exceed model output token limits at reported failure points, with models explicitly acknowledging these constraints in their outputs; (2) The authors' automated evaluation framework fails to distinguish between reasoning failures and practical constraints, leading to misclassification of model capabilities; (3) Most concerningly, their River Crossing benchmarks include mathematically impossible instances for N &amp;gt; 5 due to insufficient boat capacity, yet models are scored as failures for not solving these unsolvable problems. When we control for these experimental artifacts, by requesting generating functions instead of exhaustive move lists, preliminary experiments across multiple models indicate high accuracy on Tower of Hanoi instances previously reported as complete failures. These findings highlight the importance of careful experimental design when evaluating AI reasoning capabilities.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The paper:&lt;/p&gt; &lt;p&gt;Shojaee, P., Mirzadeh, I., Alizadeh, K., Horton, M., Bengio, S., &amp;amp; Farajtabar, M. (2025). The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity. &lt;em&gt;arXiv preprint arXiv:2506.06941&lt;/em&gt;. &lt;a href="https://arxiv.org/abs/2506.09250"&gt;https://arxiv.org/abs/2506.09250&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WackyConundrum"&gt; /u/WackyConundrum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lacqxh/against_the_apples_paper_llm_can_solve_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lacqxh/against_the_apples_paper_llm_can_solve_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lacqxh/against_the_apples_paper_llm_can_solve_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T10:44:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1laavph</id>
    <title>Finally, Zen 6, per-socket memory bandwidth to 1.6 TB/s</title>
    <updated>2025-06-13T08:39:05+00:00</updated>
    <author>
      <name>/u/On1ineAxeL</name>
      <uri>https://old.reddit.com/user/On1ineAxeL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laavph/finally_zen_6_persocket_memory_bandwidth_to_16_tbs/"&gt; &lt;img alt="Finally, Zen 6, per-socket memory bandwidth to 1.6 TB/s" src="https://external-preview.redd.it/WQB4YYDDJWqV1l5CZF1V17S1yCdW1pO-9wS0zX4_i0Y.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fbf587ec8d49fef7fc461c839cfe256b80cfd21" title="Finally, Zen 6, per-socket memory bandwidth to 1.6 TB/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.tomshardware.com/pc-components/cpus/amds-256-core-epyc-venice-cpu-in-the-labs-now-coming-in-2026"&gt;https://www.tomshardware.com/pc-components/cpus/amds-256-core-epyc-venice-cpu-in-the-labs-now-coming-in-2026&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Perhaps more importantly, the new EPYC 'Venice' processor will more than double per-socket memory bandwidth to 1.6 TB/s (up from 614 GB/s in case of the company's existing CPUs) to keep those high-performance Zen 6 cores fed with data all the time. AMD did not disclose how it plans to achieve the 1.6 TB/s bandwidth, though it is reasonable to assume that the new EPYC ‚ÄòVenice‚Äô CPUS will support advanced memory modules like like &lt;a href="https://www.tomshardware.com/news/amd-advocates-ddr5-mrdimms-with-speeds-up-to-17600-mts"&gt;MR-DIMM&lt;/a&gt; and &lt;a href="https://www.tomshardware.com/news/sk-hynix-develops-mcr-dimm"&gt;MCR-DIMM&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/us3k64mzon6f1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=de757984360f7d9597d9583a7f95f0d8400ddcb9"&gt;https://preview.redd.it/us3k64mzon6f1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=de757984360f7d9597d9583a7f95f0d8400ddcb9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Greatest hardware news&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/On1ineAxeL"&gt; /u/On1ineAxeL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laavph/finally_zen_6_persocket_memory_bandwidth_to_16_tbs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laavph/finally_zen_6_persocket_memory_bandwidth_to_16_tbs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1laavph/finally_zen_6_persocket_memory_bandwidth_to_16_tbs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T08:39:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1laee7q</id>
    <title>Got a tester version of the open-weight OpenAI model. Very lean inference engine!</title>
    <updated>2025-06-13T12:14:51+00:00</updated>
    <author>
      <name>/u/Firepal64</name>
      <uri>https://old.reddit.com/user/Firepal64</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laee7q/got_a_tester_version_of_the_openweight_openai/"&gt; &lt;img alt="Got a tester version of the open-weight OpenAI model. Very lean inference engine!" src="https://external-preview.redd.it/YTZ6aWx2ODdxbzZmMZP4_Zg7YIqZNzvbtM-0NW72ki5jdKm1HMEQNOp3yi9R.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=68f2539f409c852b055ce84c62425320bcc7860f" title="Got a tester version of the open-weight OpenAI model. Very lean inference engine!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;span class="md-spoiler-text"&gt;Silkposting in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;? I'd never&lt;/span&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Firepal64"&gt; /u/Firepal64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3r075o87qo6f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laee7q/got_a_tester_version_of_the_openweight_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1laee7q/got_a_tester_version_of_the_openweight_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T12:14:51+00:00</published>
  </entry>
</feed>
