<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-09T09:06:34+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hwf4jm</id>
    <title>[Second Take] Kokoro-82M is an Apache TTS model</title>
    <updated>2025-01-08T08:16:07+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I trained this model recently: &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;https://huggingface.co/hexgrad/Kokoro-82M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everything is in the README there, TLDR: Kokoro is a TTS model that is very good for its size.&lt;/p&gt; &lt;p&gt;Apologies for the double-post, but the first one was cooking, and it suddenly got `ledeted` by `domeration` (yes, I'm `simpelling` on purpose, it will make sense soon).&lt;/p&gt; &lt;p&gt;Last time I tried giving longer, meaningful replies to people in the comments, which kept getting `dashow-nabbed`, and when I edited to the OP to include that word which must not be named, the whole post was poofed. This time I will shut up and let the post speak for itself, and you can find me on `sidcord` where we can speak more freely, since I appear to have GTA 5 stars over here.&lt;/p&gt; &lt;p&gt;Finally, I am also collecting synthetic audio, see &lt;a href="https://hf.co/posts/hexgrad/418806998707773"&gt;https://hf.co/posts/hexgrad/418806998707773&lt;/a&gt; if interested.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf4jm/second_take_kokoro82m_is_an_apache_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf4jm/second_take_kokoro82m_is_an_apache_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf4jm/second_take_kokoro82m_is_an_apache_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwvyze</id>
    <title>Creative Writing</title>
    <updated>2025-01-08T21:49:10+00:00</updated>
    <author>
      <name>/u/Iamblichos</name>
      <uri>https://old.reddit.com/user/Iamblichos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone found a model that excels at creative writing yet? I've been trying to work with various models in a &amp;quot;follow your own adventure&amp;quot; type setup set in a fantasy world, but generally the products are poor-to-awful. Even when a given model can remember what's going on - no guarantees, no matter what the context is set to - it defaults to a Moorcock/de Camp style high fantasy setting, forcing you into the role of the hero, and I'm trying to do the Low Fantasy Just Some Dude story. Most of the models get furious with me refusing to follow The Very Important Quest that the wizard bursts into the tavern to demand help with, in a thunderstorm, while the bard plays a song of ancient brave heros, etc. etc. ad nauseam.&lt;/p&gt; &lt;p&gt;Even worse, most of the &amp;quot;storytelling&amp;quot; models like Aura and Wizard seem to have been overtrained to throw potential sexual encounters at the main character at literally any moment. Constantly. I'm fine to have NSFW encounters in a story if they follow the plot, but if I wanted to read PWP I'd just go to AO3 or Nifty.&lt;/p&gt; &lt;p&gt;I realize this is probably a niche ask, but has anyone had any luck with a similar project? The best luck I've had is with a very small quant of Mistral Large (by, of course, bartowski - the GOAT of LLMs), but NeMo was horribly bad. Tips?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iamblichos"&gt; /u/Iamblichos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwvyze/creative_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwvyze/creative_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwvyze/creative_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T21:49:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwsuuf</id>
    <title>The phi family model: Acing tests but failing real use cases?</title>
    <updated>2025-01-08T19:38:57+00:00</updated>
    <author>
      <name>/u/fewsats</name>
      <uri>https://old.reddit.com/user/fewsats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve really tried to like this family of models, including the newly released phi 4. Itâ€™s trained on synthetic data and academic textbooks, which sounds great in theory. But in practice, they all seem to fall short when it comes to real-world applications. For starters, no function calling (at least in the current Ollama version).&lt;/p&gt; &lt;p&gt;The phi model reminds me of that one smart kid in class who always nails the tests but struggles with anything outside of that structured environment. On paper, itâ€™s brilliant. In reality, it just doesnâ€™t measure up.&lt;/p&gt; &lt;p&gt;Curious if anyone else has had the same experience or sees things differently. Is it just me, or is this a recurring issue with these kinds of models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fewsats"&gt; /u/fewsats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwsuuf/the_phi_family_model_acing_tests_but_failing_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwsuuf/the_phi_family_model_acing_tests_but_failing_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwsuuf/the_phi_family_model_acing_tests_but_failing_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T19:38:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwp00r</id>
    <title>Interesting Solution to the problem of Misguided Attention: "Mindful Attention"</title>
    <updated>2025-01-08T17:01:17+00:00</updated>
    <author>
      <name>/u/vesudeva</name>
      <uri>https://old.reddit.com/user/vesudeva</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwp00r/interesting_solution_to_the_problem_of_misguided/"&gt; &lt;img alt="Interesting Solution to the problem of Misguided Attention: &amp;quot;Mindful Attention&amp;quot;" src="https://external-preview.redd.it/okxb_U2_vWgnDkR1V08qly94GnsdGTVu0C8vagF8k-M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d9d2562f385dca6a43d030b7263e0bc59a432aa" title="Interesting Solution to the problem of Misguided Attention: &amp;quot;Mindful Attention&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vesudeva"&gt; /u/vesudeva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/posts/Severian/375067343900874"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwp00r/interesting_solution_to_the_problem_of_misguided/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwp00r/interesting_solution_to_the_problem_of_misguided/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T17:01:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwfblu</id>
    <title>NVIDIA Open Model License: NVIDIA Cosmos is a world foundation model trained on 20 million hours of video to build virtual worlds and generate photo-real, physically-based synthetic data for scientific and industrial testing.</title>
    <updated>2025-01-08T08:29:57+00:00</updated>
    <author>
      <name>/u/Powerful-Solution646</name>
      <uri>https://old.reddit.com/user/Powerful-Solution646</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfblu/nvidia_open_model_license_nvidia_cosmos_is_a/"&gt; &lt;img alt="NVIDIA Open Model License: NVIDIA Cosmos is a world foundation model trained on 20 million hours of video to build virtual worlds and generate photo-real, physically-based synthetic data for scientific and industrial testing." src="https://external-preview.redd.it/dGFrdTNlbm5kcWJlMeFSSXTYDvjzDDIYxHTRsBuU24PYEoa111CihFQLGiR7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=950ef6aabdd5309efe066d1a0cf14319e919f04f" title="NVIDIA Open Model License: NVIDIA Cosmos is a world foundation model trained on 20 million hours of video to build virtual worlds and generate photo-real, physically-based synthetic data for scientific and industrial testing." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nvidia.com/en-in/ai/cosmos/"&gt;https://www.nvidia.com/en-in/ai/cosmos/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Solution646"&gt; /u/Powerful-Solution646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lfzohbxndqbe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfblu/nvidia_open_model_license_nvidia_cosmos_is_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfblu/nvidia_open_model_license_nvidia_cosmos_is_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwkjq0</id>
    <title>MiniThinky 1B - My first trial to make a reasoning model</title>
    <updated>2025-01-08T13:44:30+00:00</updated>
    <author>
      <name>/u/MediocreProgrammer99</name>
      <uri>https://old.reddit.com/user/MediocreProgrammer99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;This is my first trial to fine tune a small model, adding the reasoning capability.&lt;/p&gt; &lt;p&gt;I took Llama 3.2 1B as the base model, so the size is very small.&lt;/p&gt; &lt;p&gt;Check it out here ==&amp;gt; &lt;a href="https://huggingface.co/ngxson/MiniThinky-v2-1B-Llama-3.2"&gt;https://huggingface.co/ngxson/MiniThinky-v2-1B-Llama-3.2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF version (runnable directly via ollama): &lt;a href="https://huggingface.co/ngxson/MiniThinky-v2-1B-Llama-3.2-Q8_0-GGUF"&gt;https://huggingface.co/ngxson/MiniThinky-v2-1B-Llama-3.2-Q8_0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MediocreProgrammer99"&gt; /u/MediocreProgrammer99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwkjq0/minithinky_1b_my_first_trial_to_make_a_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwkjq0/minithinky_1b_my_first_trial_to_make_a_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwkjq0/minithinky_1b_my_first_trial_to_make_a_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T13:44:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwjzw1</id>
    <title>A Recipe for a Better Code Generator with RAG</title>
    <updated>2025-01-08T13:15:30+00:00</updated>
    <author>
      <name>/u/agbell</name>
      <uri>https://old.reddit.com/user/agbell</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwjzw1/a_recipe_for_a_better_code_generator_with_rag/"&gt; &lt;img alt="A Recipe for a Better Code Generator with RAG" src="https://external-preview.redd.it/LXLJBuPegXr_jr-v3g2QasDpI6sYCw7nkCBT17bt6Qs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25403c01daf573ed3ba6a7bc1d3fb35fac180834" title="A Recipe for a Better Code Generator with RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/agbell"&gt; /u/agbell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.pulumi.com/blog/codegen-learnings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwjzw1/a_recipe_for_a_better_code_generator_with_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwjzw1/a_recipe_for_a_better_code_generator_with_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T13:15:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwz324</id>
    <title>What happened to AiTracker?</title>
    <updated>2025-01-08T23:57:34+00:00</updated>
    <author>
      <name>/u/whotookthecandyjar</name>
      <uri>https://old.reddit.com/user/whotookthecandyjar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About 7 months ago, &lt;a href="https://AiTracker.art"&gt;AiTracker.art&lt;/a&gt; was announced here as a torrent tracker for AI models. It was a fairly useful resource, but I noticed it's no longer accessible. The torrents still work of course, but does anyone know what happened to it? Is it just down for maintenance, or gone forever (the hostname hasn't been resolving for the past week or two)?&lt;/p&gt; &lt;p&gt;Link to original announcement: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1dc1nxg/aitrackerart_a_torrent_tracker_for_ai_models/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1dc1nxg/aitrackerart_a_torrent_tracker_for_ai_models/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/whotookthecandyjar"&gt; /u/whotookthecandyjar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwz324/what_happened_to_aitracker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwz324/what_happened_to_aitracker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwz324/what_happened_to_aitracker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T23:57:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx8nex</id>
    <title>"rStar-Math demonstrates that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising "deep thinking" through Monte Carlo Tree Search (MCTS)....."</title>
    <updated>2025-01-09T08:51:09+00:00</updated>
    <author>
      <name>/u/Powerful-Solution646</name>
      <uri>https://old.reddit.com/user/Powerful-Solution646</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx8nex/rstarmath_demonstrates_that_small_language_models/"&gt; &lt;img alt="&amp;quot;rStar-Math demonstrates that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising &amp;quot;deep thinking&amp;quot; through Monte Carlo Tree Search (MCTS).....&amp;quot;" src="https://b.thumbs.redditmedia.com/HJz73domuc-71OAJfI-YWNodO1IrhFgRKSFmybqiieU.jpg" title="&amp;quot;rStar-Math demonstrates that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising &amp;quot;deep thinking&amp;quot; through Monte Carlo Tree Search (MCTS).....&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising &amp;quot;deep thinking&amp;quot; through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids naÃ¯ve step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students. &lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2501.04519"&gt;https://arxiv.org/abs/2501.04519&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Solution646"&gt; /u/Powerful-Solution646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1hx8nex"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx8nex/rstarmath_demonstrates_that_small_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx8nex/rstarmath_demonstrates_that_small_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T08:51:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwn90v</id>
    <title>Phi 4 MIT licensed - its show time folks</title>
    <updated>2025-01-08T15:50:30+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/phi-4"&gt;https://huggingface.co/microsoft/phi-4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwn90v/phi_4_mit_licensed_its_show_time_folks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwn90v/phi_4_mit_licensed_its_show_time_folks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwn90v/phi_4_mit_licensed_its_show_time_folks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T15:50:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx6sjc</id>
    <title>Weirdly good finetune - QwQ-LCoT-7B-Instruct</title>
    <updated>2025-01-09T06:33:12+00:00</updated>
    <author>
      <name>/u/1ncehost</name>
      <uri>https://old.reddit.com/user/1ncehost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use a lot of complex, large-context coding prompts that are high on the difficulty scale using &lt;a href="https://github.com/curvedinf/dir-assistant"&gt;https://github.com/curvedinf/dir-assistant&lt;/a&gt; . I've been using APIs for a number of months since prices have come down, but I just did a round of tests in the 7B-14B range. I found this tune randomly while browsing huggingface and it has a whole 304 downloads, but damn is it good. Its consistently outperforming newer 32B models, and older 70B models in my tests. I don't know what the secret is here, but I just wanted to pass this along. I test a LOT of models, and this one is weirdly good for coding.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/prithivMLmods/QwQ-LCoT-7B-Instruct"&gt;https://huggingface.co/prithivMLmods/QwQ-LCoT-7B-Instruct&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/bartowski/QwQ-LCoT-7B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/QwQ-LCoT-7B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1ncehost"&gt; /u/1ncehost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx6sjc/weirdly_good_finetune_qwqlcot7binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx6sjc/weirdly_good_finetune_qwqlcot7binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx6sjc/weirdly_good_finetune_qwqlcot7binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T06:33:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwfm8k</id>
    <title>Tech lead of Qwen Team, Alibaba Group: "I often recommend people to read the blog of Anthropic to learn more about what agent really is. Then you will realize you should invest on it as much as possible this year." Blog linked in body text.</title>
    <updated>2025-01-08T08:50:35+00:00</updated>
    <author>
      <name>/u/Powerful-Solution646</name>
      <uri>https://old.reddit.com/user/Powerful-Solution646</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfm8k/tech_lead_of_qwen_team_alibaba_group_i_often/"&gt; &lt;img alt="Tech lead of Qwen Team, Alibaba Group: &amp;quot;I often recommend people to read the blog of Anthropic to learn more about what agent really is. Then you will realize you should invest on it as much as possible this year.&amp;quot; Blog linked in body text." src="https://preview.redd.it/5lmmx4qchqbe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f0c18af44997be0f0628d875475982b7bf3b877" title="Tech lead of Qwen Team, Alibaba Group: &amp;quot;I often recommend people to read the blog of Anthropic to learn more about what agent really is. Then you will realize you should invest on it as much as possible this year.&amp;quot; Blog linked in body text." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/JustinLin610/status/1876324689657954413?t=rQiJk8V8N9-Rd8dcWJedww&amp;amp;s=19"&gt;https://x.com/JustinLin610/status/1876324689657954413?t=rQiJk8V8N9-Rd8dcWJedww&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.anthropic.com/research/building-effective-agents"&gt;https://www.anthropic.com/research/building-effective-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Solution646"&gt; /u/Powerful-Solution646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5lmmx4qchqbe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfm8k/tech_lead_of_qwen_team_alibaba_group_i_often/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfm8k/tech_lead_of_qwen_team_alibaba_group_i_often/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:50:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx5rak</id>
    <title>If you are trying to learn about a specific topic that your model does not know a lot about, create a glossary. If the glossary is small enough, enter it into a system instructions. If not, insert as a prompt before asking your question....</title>
    <updated>2025-01-09T05:27:05+00:00</updated>
    <author>
      <name>/u/Powerful-Solution646</name>
      <uri>https://old.reddit.com/user/Powerful-Solution646</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used to enter dozens of pages worth of tokens. This confused all models except Claude and maybe sometimes Gemini.&lt;/p&gt; &lt;p&gt;All open source models mostly suck at long context in context understanding. I even tried the big ones on open router as I cannot run the big ones locally.&lt;/p&gt; &lt;p&gt;Even LLaMA 3.1 405B struggles.&lt;/p&gt; &lt;p&gt;But now that I enter the glossary of the topic I am learning, it immediately understands the definitions and reduces hallucinations a lot because it knows what I am asking about. Saves a ton of time and compute / money.&lt;/p&gt; &lt;p&gt;Also the model starts admitting what it doesn't know which stops it from making bs hallucinations that we are likely to fall for.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Solution646"&gt; /u/Powerful-Solution646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx5rak/if_you_are_trying_to_learn_about_a_specific_topic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx5rak/if_you_are_trying_to_learn_about_a_specific_topic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx5rak/if_you_are_trying_to_learn_about_a_specific_topic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T05:27:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwe9mf</id>
    <title>HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it</title>
    <updated>2025-01-08T07:20:14+00:00</updated>
    <author>
      <name>/u/quantier</name>
      <uri>https://old.reddit.com/user/quantier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"&gt; &lt;img alt="HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it" src="https://external-preview.redd.it/2Uxr2fZXgwYpxUcnSif2gZmNvP23o2dpwlhS4x1dHZA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54d46261045d9a2cee779ef1547c528c90021757" title="HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;96 GB out of the 128GB can be allocated to use VRAM making it able to run 70B models q8 with ease.&lt;/p&gt; &lt;p&gt;I am pretty sure Digits will use CUDA and/or TensorRT for optimization of inferencing.&lt;/p&gt; &lt;p&gt;I am wondering if this will use RocM or if we can just use CPU inferencing - wondering what the acceleration will be here. Anyone able to share insights?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantier"&gt; /u/quantier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://aecmag.com/workstations/hp-amd-ryzen-ai-max-pro-hp-zbook-ultra-g1a-hp-z2-mini-g1a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T07:20:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwx8ah</id>
    <title>ROG Flow Z13 2025 has Ryzen AI Max+ 395 and 128 GB LPDDR5X??</title>
    <updated>2025-01-08T22:42:25+00:00</updated>
    <author>
      <name>/u/Daniel_H212</name>
      <uri>https://old.reddit.com/user/Daniel_H212</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://rog.asus.com/laptops/rog-flow/rog-flow-z13-2025/spec/"&gt;https://rog.asus.com/laptops/rog-flow/rog-flow-z13-2025/spec/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Apparently it is 8000MHz quad channel, must be reallocated between CPU and GPU (which you can only do after restarting), so I'm guessing similar to the lenovo system from that other post where you can allocate say up to 96 GB.&lt;/p&gt; &lt;p&gt;They also claim the NPU has 50 TOPs.&lt;/p&gt; &lt;p&gt;Is this the Windows answer to Apple Silicon's unified memory?&lt;/p&gt; &lt;p&gt;Obviously support will be a big issue but man if this thing runs like they advertise it will run, this tablet will be more versatile than the majority of home GPU-based systems, just off of the memory config alone, and won't be slow either.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daniel_H212"&gt; /u/Daniel_H212 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwx8ah/rog_flow_z13_2025_has_ryzen_ai_max_395_and_128_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwx8ah/rog_flow_z13_2025_has_ryzen_ai_max_395_and_128_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwx8ah/rog_flow_z13_2025_has_ryzen_ai_max_395_and_128_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T22:42:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx6utz</id>
    <title>Help Me Decide: RTX 3060 12GB vs. RTX 4060 Ti 16GB for ML and Occasional Gaming</title>
    <updated>2025-01-09T06:37:35+00:00</updated>
    <author>
      <name>/u/zaid2801</name>
      <uri>https://old.reddit.com/user/zaid2801</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I could really use some GPU advice. I primarily do machine learning/model training but also game casually (League of Legends at 60 FPS is more than enough for me). Due to local market constraints, Iâ€™ve narrowed it down to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;RTX 3060 12GB&lt;/strong&gt; (MSI Ventus 2X) â€“ $365 &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.techpowerup.com/gpu-specs/msi-rtx-3060-ventus-2x.b8614"&gt;Specs&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RTX 4060 Ti 16GB&lt;/strong&gt; (ZOTAC AMP) â€“ $510 &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.techpowerup.com/gpu-specs/zotac-rtx-4060-ti-amp-16-gb.b11324"&gt;Specs&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My current system is an &lt;strong&gt;i5-12400&lt;/strong&gt; with &lt;strong&gt;32GB of RAM&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Iâ€™m Torn:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The 4060 Ti has more VRAM (16GB vs. 12GB) and higher CUDA core count, which can help with bigger ML models.&lt;/li&gt; &lt;li&gt;However, itâ€™s got a narrower memory bus (128-bit vs. 192-bit on the 3060).&lt;/li&gt; &lt;li&gt;Thereâ€™s also a significant price difference ($510 vs. $365).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Use Cases:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Machine Learning / Model Training&lt;/strong&gt;: Primarily in TensorFlow/PyTorch. VRAM size is important for handling larger models, but memory bandwidth can also be a factor.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gaming&lt;/strong&gt;: Mostly League of Legends (60 FPS is plenty). Iâ€™m not aiming for ultra settings in AAA titles.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;How much does the narrower bus on the 4060 Ti matter for ML workloads in practice?&lt;/li&gt; &lt;li&gt;Is it worth paying the extra $145 for the 4060 Ti for the additional VRAM and performance uplift?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Iâ€™d really appreciate any insights or experiences you might have. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zaid2801"&gt; /u/zaid2801 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx6utz/help_me_decide_rtx_3060_12gb_vs_rtx_4060_ti_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx6utz/help_me_decide_rtx_3060_12gb_vs_rtx_4060_ti_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx6utz/help_me_decide_rtx_3060_12gb_vs_rtx_4060_ti_16gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T06:37:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwlka6</id>
    <title>I made the world's first AI meeting copilot, and open sourced it!</title>
    <updated>2025-01-08T14:33:52+00:00</updated>
    <author>
      <name>/u/stealthanthrax</name>
      <uri>https://old.reddit.com/user/stealthanthrax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of relying on clunky SaaS tools for meeting transcriptions that didnâ€™t respect my privacy or workflow. Everyone I tried had issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bots awkwardly join meetings and announce themselves.&lt;/li&gt; &lt;li&gt;Poor transcription quality.&lt;/li&gt; &lt;li&gt;No flexibility to tweak things to fit &lt;em&gt;my&lt;/em&gt; setup.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I built &lt;strong&gt;Amurex&lt;/strong&gt;, a self-hosted solution that actually works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Records meetings quietly, with no bots interrupting.&lt;/li&gt; &lt;li&gt;Delivers clean, accurate diarized transcripts right after the meeting.&lt;/li&gt; &lt;li&gt;Does late meeting summaries. i.e. a recap for a meeting if I am late&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But most importantly, it has it is the only meeting tool in the world that can give&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time suggestions to stay engaged in boring meetings.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Itâ€™s completely open source and designed for self-hosting, so you control your data and your workflow. No subscriptions, and no vendor lock-in.&lt;/p&gt; &lt;p&gt;I would love to know what you all think of it. It only works on Google Meet for now but I will be scaling it to all the famous meeting providers.&lt;/p&gt; &lt;p&gt;Github - &lt;a href="https://github.com/thepersonalaicompany/amurex"&gt;https://github.com/thepersonalaicompany/amurex&lt;/a&gt;&lt;br /&gt; Website - &lt;a href="https://www.amurex.ai/"&gt;https://www.amurex.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stealthanthrax"&gt; /u/stealthanthrax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlka6/i_made_the_worlds_first_ai_meeting_copilot_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlka6/i_made_the_worlds_first_ai_meeting_copilot_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlka6/i_made_the_worlds_first_ai_meeting_copilot_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T14:33:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx1qn2</id>
    <title>Now that Phi-4 has been out for a while what do you think?</title>
    <updated>2025-01-09T02:03:50+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;on real world use cases does it perform well and what tasks have you tried it on so far?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx1qn2/now_that_phi4_has_been_out_for_a_while_what_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx1qn2/now_that_phi4_has_been_out_for_a_while_what_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx1qn2/now_that_phi4_has_been_out_for_a_while_what_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T02:03:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx5i8u</id>
    <title>Phi 4 is just 14B But Better than llama 3.1 70b for several tasks.</title>
    <updated>2025-01-09T05:12:08+00:00</updated>
    <author>
      <name>/u/Vishnu_One</name>
      <uri>https://old.reddit.com/user/Vishnu_One</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx5i8u/phi_4_is_just_14b_but_better_than_llama_31_70b/"&gt; &lt;img alt="Phi 4 is just 14B But Better than llama 3.1 70b for several tasks. " src="https://preview.redd.it/uwfo8ig8jwbe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06707621fd601095edb87fea07007bbf976f658b" title="Phi 4 is just 14B But Better than llama 3.1 70b for several tasks. " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vishnu_One"&gt; /u/Vishnu_One &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uwfo8ig8jwbe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx5i8u/phi_4_is_just_14b_but_better_than_llama_31_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx5i8u/phi_4_is_just_14b_but_better_than_llama_31_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T05:12:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwthrq</id>
    <title>Why I think that NVIDIA Project DIGITS will have 273 GB/s of memory bandwidth</title>
    <updated>2025-01-08T20:05:14+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwthrq/why_i_think_that_nvidia_project_digits_will_have/"&gt; &lt;img alt="Why I think that NVIDIA Project DIGITS will have 273 GB/s of memory bandwidth" src="https://b.thumbs.redditmedia.com/Hq1kyt3KW_h0jhHPVRH0mhDgkE3V6Gg4-cxEb1LPpSg.jpg" title="Why I think that NVIDIA Project DIGITS will have 273 GB/s of memory bandwidth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Used the following image from NVIDIA CES presentation:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o0dkapeqltbe1.png?width=1257&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=46b818ce812eed4ec1607595836eaccbe0da852d"&gt;Project DIGITS board&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Applied some GIMP magic to reset perspective (not perfect but close enough), used a photo of Grace chip die from the same presentation to make sure the aspect ratio is correct:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jfdlbef8mtbe1.png?width=1257&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a188f8b18c6a2ff8560b93c5ce84e20427d62f64"&gt;Project DIGITS - corrected perspective&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Then I measured dimensions of memory chips on this image:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;102 x 85 px&lt;/li&gt; &lt;li&gt;103 x 85 px&lt;/li&gt; &lt;li&gt;103 x 86 px&lt;/li&gt; &lt;li&gt;103 x 87 px&lt;/li&gt; &lt;li&gt;103 x 87 px&lt;/li&gt; &lt;li&gt;104 x 87 px&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looks consistent, so let's calculate the average aspect ratio of the chip dimensions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;102 / 85 = 1.2&lt;/li&gt; &lt;li&gt;103 / 85 = 1.211&lt;/li&gt; &lt;li&gt;103 / 86 = 1.198&lt;/li&gt; &lt;li&gt;103 / 87 = 1.184&lt;/li&gt; &lt;li&gt;103 / 87 = 1.184&lt;/li&gt; &lt;li&gt;104 / 87 = 1.195&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Average is 1.195&lt;/p&gt; &lt;p&gt;Now let's see what are the possible dimensions of &lt;a href="https://www.micron.com/products/memory/dram-components/lpddr5x/part-catalog?density=128Gb"&gt;Micron 128Gb LPDDR5X chips&lt;/a&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;496-ball packages (x64 bus): 14.00 x 12.40 mm. Aspect ratio = 1.13&lt;/li&gt; &lt;li&gt;441-ball packages (x64 bus): 14.00 x 14.00 mm. Aspect ratio = 1.0&lt;/li&gt; &lt;li&gt;315-ball packages (x32 bus): 12.40 x 15.00 mm. Aspect ratio = 1.21&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So the closest match (I guess 1-2% measurement errors are possible) is 315-ball x32 bus package. With 8 chips the memory bus width will be 8 * 32 = 256 bits. With 8533MT/s that's 273 GB/s max. So basically the same as Strix Halo.&lt;/p&gt; &lt;p&gt;Another reason is that they didn't mention the memory bandwidth during presentation. I'm sure they would have mentioned it if it was exceptionally high.&lt;/p&gt; &lt;p&gt;Hopefully I'm wrong! ðŸ˜¢&lt;/p&gt; &lt;p&gt;...or there are 8 more memory chips underneath the board and I just wasted a hour of my life. ðŸ˜†&lt;/p&gt; &lt;p&gt;Edit - that's unlikely, as there are only 8 identical high bandwidth memory I/O structures on the chip die.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwthrq/why_i_think_that_nvidia_project_digits_will_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwthrq/why_i_think_that_nvidia_project_digits_will_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwthrq/why_i_think_that_nvidia_project_digits_will_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T20:05:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx60t4</id>
    <title>New Microsoft research - rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking</title>
    <updated>2025-01-09T05:43:37+00:00</updated>
    <author>
      <name>/u/Ok_Landscape_6819</name>
      <uri>https://old.reddit.com/user/Ok_Landscape_6819</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2501.04519"&gt;https://arxiv.org/abs/2501.04519&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen2.5-Math-7B reaches 90% on MATH with this new technique. Phi3-mini-3.8B reaches 86.4%..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Landscape_6819"&gt; /u/Ok_Landscape_6819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx60t4/new_microsoft_research_rstarmath_small_llms_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx60t4/new_microsoft_research_rstarmath_small_llms_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx60t4/new_microsoft_research_rstarmath_small_llms_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T05:43:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwzmqc</id>
    <title>Phi-4 Llamafied + 4 Bug Fixes + GGUFs, Dynamic 4bit Quants</title>
    <updated>2025-01-09T00:20:50+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwzmqc/phi4_llamafied_4_bug_fixes_ggufs_dynamic_4bit/"&gt; &lt;img alt="Phi-4 Llamafied + 4 Bug Fixes + GGUFs, Dynamic 4bit Quants" src="https://b.thumbs.redditmedia.com/0aD2C0A02Gg08d6G5AgWeGQyp6FZmThRfyFVw15avlQ.jpg" title="Phi-4 Llamafied + 4 Bug Fixes + GGUFs, Dynamic 4bit Quants" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ! I've uploaded &lt;strong&gt;fixed versions&lt;/strong&gt; of &lt;a href="https://unsloth.ai/blog/phi4"&gt;Phi-4&lt;/a&gt;, including GGUF + 4-bit + 16-bit versions on HuggingFace!&lt;/p&gt; &lt;p&gt;Weâ€™ve fixed over &lt;strong&gt;4 bugs (3 major ones)&lt;/strong&gt; in Phi-4, mainly related to tokenizers and chat templates which affected inference and finetuning workloads. If you were experiencing poor results, we recommend trying our GGUF upload. A detailed post on the fixes will be released tomorrow.&lt;/p&gt; &lt;p&gt;We also &lt;strong&gt;Llamafied&lt;/strong&gt; the model meaning it should work out of the box with every framework including &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;. Fine-tuning is &lt;strong&gt;2x faster, uses 70% VRAM&lt;/strong&gt; &amp;amp; has 9x longer context lengths with Unsloth.&lt;/p&gt; &lt;p&gt;View all Phi-4 versions with our bug fixes: &lt;a href="https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa"&gt;https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Phi-4 Uploads (with our bug fixes)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4-GGUF"&gt;GGUFs&lt;/a&gt; including 2, 3, 4, 5, 6, 8, 16-bit&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4-unsloth-bnb-4bit"&gt;Unsloth Dynamic 4-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4-bnb-4bit"&gt;4-bit Bnb&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4"&gt;Original 16-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I uploaded Q2_K_L quants which works well as well - they are Q2_K quants, but leaves the embedding as Q4 and lm_head as Q6 - this should increase accuracy by a bit!&lt;/p&gt; &lt;p&gt;To use Phi-4 in llama.cpp, do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama.cpp/llama-cli --model unsloth/phi-4-GGUF/phi-4-Q2_K_L.gguf --prompt '&amp;lt;|im_start|&amp;gt;user&amp;lt;|im_sep|&amp;gt;Provide all combinations of a 5 bit binary number.&amp;lt;|im_end|&amp;gt;&amp;lt;|im_start|&amp;gt;assistant&amp;lt;|im_sep|&amp;gt;' --threads 16 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Which will produce:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;A 5-bit binary number consists of 5 positions, each of which can be either 0 or 1. Therefore, there are \(2^5 = 32\) possible combinations. Here they are, listed in ascending order: 1. 00000 2. 00001 3. 00010 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I also uploaded &lt;strong&gt;Dynamic 4bit quants&lt;/strong&gt; which don't quantize every layer to 4bit, and leaves some in 16bit - by using only an extra 1GB of VRAM, you get superior accuracy, especially for finetuning! - Head over to &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt; to finetune LLMs and Vision models 2x faster and use 70% less VRAM!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/74tja0m83vbe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=99a09dc6e9ad8c1d8e8e19ad519ccaaabebde3a8"&gt;Dynamic 4bit quants leave some layers as 16bit and not 4bit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwzmqc/phi4_llamafied_4_bug_fixes_ggufs_dynamic_4bit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwzmqc/phi4_llamafied_4_bug_fixes_ggufs_dynamic_4bit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwzmqc/phi4_llamafied_4_bug_fixes_ggufs_dynamic_4bit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T00:20:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwmy39</id>
    <title>Phi-4 has been released</title>
    <updated>2025-01-08T15:37:07+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwmy39/phi4_has_been_released/"&gt; &lt;img alt="Phi-4 has been released" src="https://external-preview.redd.it/gF2xHg5eGxs-B6pIwB6VodFWhOArLu2_8o4wQP6siP8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed435e811f517c43ffac1607f9719679d8dd1b5d" title="Phi-4 has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/phi-4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwmy39/phi4_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwmy39/phi4_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T15:37:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx7421</id>
    <title>TransPixar: a new generative model that preserves transparency,</title>
    <updated>2025-01-09T06:55:51+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx7421/transpixar_a_new_generative_model_that_preserves/"&gt; &lt;img alt="TransPixar: a new generative model that preserves transparency," src="https://external-preview.redd.it/aHFsc2gwdXExeGJlMYcphE9YFRyNCrr76DvwiShDtswDzb2s93cwaOHelUg2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd6433e0c057e8d365fab3d5289d9a28fb641a1f" title="TransPixar: a new generative model that preserves transparency," /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8fhb41uq1xbe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx7421/transpixar_a_new_generative_model_that_preserves/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx7421/transpixar_a_new_generative_model_that_preserves/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T06:55:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwwvuz</id>
    <title>This sums my experience with models on Groq</title>
    <updated>2025-01-08T22:27:22+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwwvuz/this_sums_my_experience_with_models_on_groq/"&gt; &lt;img alt="This sums my experience with models on Groq" src="https://preview.redd.it/7tqzm8bsiube1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bebd5aeeb53de86390096a6377b9d96fe453c674" title="This sums my experience with models on Groq" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7tqzm8bsiube1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwwvuz/this_sums_my_experience_with_models_on_groq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwwvuz/this_sums_my_experience_with_models_on_groq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T22:27:22+00:00</published>
  </entry>
</feed>
