<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-10T14:06:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jvw049</id>
    <title>MCP Limitations: Async, External Triggers, Security Concerns</title>
    <updated>2025-04-10T11:51:50+00:00</updated>
    <author>
      <name>/u/toolhouseai</name>
      <uri>https://old.reddit.com/user/toolhouseai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems like many AI workflow tools including MCP are having limitations when handling things asynchronously and triggering workflows from external events. Calling it/them just an &amp;quot;API wrapper&amp;quot; might be a bit harsh but i think its creating a gap for building complex, event-driven AI systems.&lt;/p&gt; &lt;p&gt;I've also seen around about security concerns when it comes to MCP what's that all about?&lt;br /&gt; &lt;strong&gt;What needs to be improved? What alternative approaches or tools handle async better?&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/toolhouseai"&gt; /u/toolhouseai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvw049/mcp_limitations_async_external_triggers_security/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvw049/mcp_limitations_async_external_triggers_security/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvw049/mcp_limitations_async_external_triggers_security/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T11:51:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv9s6q</id>
    <title>LMSYS WebDev Arena updated with DeepSeek-V3-0324 and Llama 4 models.</title>
    <updated>2025-04-09T16:18:25+00:00</updated>
    <author>
      <name>/u/jpydych</name>
      <uri>https://old.reddit.com/user/jpydych</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv9s6q/lmsys_webdev_arena_updated_with_deepseekv30324/"&gt; &lt;img alt="LMSYS WebDev Arena updated with DeepSeek-V3-0324 and Llama 4 models." src="https://preview.redd.it/ew55ayg24ute1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb7de8d9615d9f552f6a7a05c87acda4c5a6a656" title="LMSYS WebDev Arena updated with DeepSeek-V3-0324 and Llama 4 models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jpydych"&gt; /u/jpydych &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ew55ayg24ute1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv9s6q/lmsys_webdev_arena_updated_with_deepseekv30324/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv9s6q/lmsys_webdev_arena_updated_with_deepseekv30324/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T16:18:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv71su</id>
    <title>Granite 3.3 imminent?</title>
    <updated>2025-04-09T14:24:45+00:00</updated>
    <author>
      <name>/u/das_rdsm</name>
      <uri>https://old.reddit.com/user/das_rdsm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv71su/granite_33_imminent/"&gt; &lt;img alt="Granite 3.3 imminent?" src="https://preview.redd.it/g2ceteotjtte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60dab73cdbdeadc070ee093587327ecdf15ea289" title="Granite 3.3 imminent?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apparently they added and then edited the collection. maybe it will be released today?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/das_rdsm"&gt; /u/das_rdsm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g2ceteotjtte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv71su/granite_33_imminent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv71su/granite_33_imminent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T14:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv7x6l</id>
    <title>Hogwild! Inference: Parallel LLM Generation via Concurrent Attention</title>
    <updated>2025-04-09T15:01:53+00:00</updated>
    <author>
      <name>/u/Psychological-Tea652</name>
      <uri>https://old.reddit.com/user/Psychological-Tea652</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv7x6l/hogwild_inference_parallel_llm_generation_via/"&gt; &lt;img alt="Hogwild! Inference: Parallel LLM Generation via Concurrent Attention" src="https://external-preview.redd.it/b3BjbHE0c2ZwdHRlMWcmQ4x0UIQwBXGX5ihDQRS0yvkPTeRAH8Mf_AVWxETI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49a6c2066bf6d904c4233f96449ce01283d4b000" title="Hogwild! Inference: Parallel LLM Generation via Concurrent Attention" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The paper modifies LLM attention so multiple &amp;quot;workers&amp;quot; can see each other's thoughts (KV) in real time. They generate text in parallel like humans use Google Docs. Turns out, they can self-organize, split the work and cross-verify. Works with open-source models like QwQ-32B. Check it out!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Paper &amp;amp; code:&lt;/strong&gt; &lt;a href="https://huggingface.co/papers/2504.06261"&gt;https://huggingface.co/papers/2504.06261&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Project page:&lt;/strong&gt; &lt;a href="https://eqimp.github.io/hogwild_llm"&gt;https://eqimp.github.io/hogwild_llm&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological-Tea652"&gt; /u/Psychological-Tea652 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/q36zd4sfptte1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv7x6l/hogwild_inference_parallel_llm_generation_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv7x6l/hogwild_inference_parallel_llm_generation_via/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T15:01:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvpko7</id>
    <title>The Ultimate MCP Client</title>
    <updated>2025-04-10T04:31:19+00:00</updated>
    <author>
      <name>/u/dicklesworth</name>
      <uri>https://old.reddit.com/user/dicklesworth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvpko7/the_ultimate_mcp_client/"&gt; &lt;img alt="The Ultimate MCP Client" src="https://external-preview.redd.it/nocIlb0TD3sqK4UU27jSBiEel3Kt83uLvo9UMt7ojGk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=33c431aaacd4e100ace18968a06867f8300bf396" title="The Ultimate MCP Client" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the past couple weeks, I've been really immersed in learning about MCP, a new protocol for equipping any LLM with a set of tools that can run on your own machine or a remote server you control and give all kinds of superpowers to AI agents to do things like search, etc. &lt;/p&gt; &lt;p&gt;As part of that research, I've already built one very fleshed-out and useful MCP server that I've shared here (I've added much more to it recently though!), the LLM Gateway MCP Server, which lets you use a big model to delegate to a cheaper model (and many more things in addition to that, like running automated multi-round LLM Tournaments, which I also posted about here recently).&lt;/p&gt; &lt;p&gt;To actually use these MCP servers though, you need an MCP client. Most people seem to be using the Claude Desktop app. I tried this and got it to work just fine, but it was a bit annoying to set up and there were lots of things I didn't like about it. I wanted something better. &lt;/p&gt; &lt;p&gt;So two days ago I began work on what I call the Ultimate MCP Client. After ~24 hours of work, it's working and ready and I'm really proud of how amazingly well it turned out. This is going to be a workhorse tool for me personally. &lt;/p&gt; &lt;p&gt;It's pure python and all in a single large .py file which can be deployed as a self-contained uv script if you want. It offers all kinds of features and very rich console output for use interactively in a terminal, along with a CLI. But it can also be used in the background. &lt;/p&gt; &lt;p&gt;That kind of background functionality, orchestrating and coordinating several MCP servers nicely, is how I mostly intend on using it. But once I saw how nice the interactive terminal experience was, I realized that I could slap a FastAPI server on top of it and make a web GUI. &lt;/p&gt; &lt;p&gt;Because I hate unneeded complexity so much, I made the WebGUI a single self-contained HTML file you can just open in your browser (similar to my Your-Source-to-Prompt tool), and it looks awesome using Alpine and Daisy and other nice UI libraries, all loaded via CDN.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dicklesworth"&gt; /u/dicklesworth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Dicklesworthstone/ultimate_mcp_client"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvpko7/the_ultimate_mcp_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvpko7/the_ultimate_mcp_client/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T04:31:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvbhlp</id>
    <title>I actually really like Llama 4 scout</title>
    <updated>2025-04-09T17:28:00+00:00</updated>
    <author>
      <name>/u/d13f00l</name>
      <uri>https://old.reddit.com/user/d13f00l</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running it on a 64 core Ampere Altra arm system with 128GB ram, no GPU, in llama.cpp with q6_k quant. It averages about 10 tokens a second which is great for personal use. It is answering coding questions and technical questions well. I have run Llama 3.3 70b, Mixtral 8x7b, Qwen 2.5 72b, some of the PHI models. The performance of scout is really good. Anecdotally it seems to be answering things at least as good as Llama 3.3 70b or Qwen 2.5 72b, at higher speeds. People aren't liking the model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/d13f00l"&gt; /u/d13f00l &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvbhlp/i_actually_really_like_llama_4_scout/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvbhlp/i_actually_really_like_llama_4_scout/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvbhlp/i_actually_really_like_llama_4_scout/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T17:28:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvpujt</id>
    <title>LiveIdeaBench-v2 Update: Dataset &amp; Leaderboard</title>
    <updated>2025-04-10T04:48:43+00:00</updated>
    <author>
      <name>/u/realJoeTrump</name>
      <uri>https://old.reddit.com/user/realJoeTrump</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvpujt/liveideabenchv2_update_dataset_leaderboard/"&gt; &lt;img alt="LiveIdeaBench-v2 Update: Dataset &amp;amp; Leaderboard" src="https://b.thumbs.redditmedia.com/LGJ1ScwO4k72rTTpki5DjoJP8nqKAjK5kArrwFknjjU.jpg" title="LiveIdeaBench-v2 Update: Dataset &amp;amp; Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/5o2vubvstxte1.png?width=1602&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=789f2bc6b2439f1b77f9c00accf62e040a2a0c1b"&gt;https://preview.redd.it/5o2vubvstxte1.png?width=1602&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=789f2bc6b2439f1b77f9c00accf62e040a2a0c1b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i44km9vutxte1.png?width=1191&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3cfcfc7797c67acba22804035594c781ebab09e6"&gt;https://preview.redd.it/i44km9vutxte1.png?width=1191&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3cfcfc7797c67acba22804035594c781ebab09e6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Leaderboard:&lt;/p&gt; &lt;p&gt;&lt;a href="https://liveideabench.com/"&gt;https://liveideabench.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Dataset: &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/6cf/liveideabench-v2"&gt;https://huggingface.co/datasets/6cf/liveideabench-v2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realJoeTrump"&gt; /u/realJoeTrump &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvpujt/liveideabenchv2_update_dataset_leaderboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvpujt/liveideabenchv2_update_dataset_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvpujt/liveideabenchv2_update_dataset_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T04:48:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv5xv7</id>
    <title>Google Ironwood TPU (7th generation) introduction</title>
    <updated>2025-04-09T13:35:39+00:00</updated>
    <author>
      <name>/u/zimmski</name>
      <uri>https://old.reddit.com/user/zimmski</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/"&gt;https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When i see Google's TPUs, i always ask myself if there is any company working on a local variant that us mortals can buy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zimmski"&gt; /u/zimmski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5xv7/google_ironwood_tpu_7th_generation_introduction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5xv7/google_ironwood_tpu_7th_generation_introduction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5xv7/google_ironwood_tpu_7th_generation_introduction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T13:35:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvxi5f</id>
    <title>New coding model DeepCoder-14B-Preview</title>
    <updated>2025-04-10T13:09:25+00:00</updated>
    <author>
      <name>/u/mrskeptical00</name>
      <uri>https://old.reddit.com/user/mrskeptical00</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvxi5f/new_coding_model_deepcoder14bpreview/"&gt; &lt;img alt="New coding model DeepCoder-14B-Preview" src="https://external-preview.redd.it/YkKjX2lPMDEhqwzwRn4cpEMg8i531yZ0i7k6psvFYo8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7b9a413e72aba377960a99362f5936db3ce66d4b" title="New coding model DeepCoder-14B-Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A joint collab between the Agentica team and Together AI based on finetune of DeepSeek-R1-Distill-Qwen-14B. They claim it‚Äôs as good at o3-mini. &lt;/p&gt; &lt;p&gt;HuggingFace URL: &lt;a href="https://huggingface.co/agentica-org/DeepCoder-14B-Preview"&gt;https://huggingface.co/agentica-org/DeepCoder-14B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/bartowski/agentica-org_DeepCoder-14B-Preview-GGUF"&gt;https://huggingface.co/bartowski/agentica-org_DeepCoder-14B-Preview-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrskeptical00"&gt; /u/mrskeptical00 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.together.ai/blog/deepcoder"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvxi5f/new_coding_model_deepcoder14bpreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvxi5f/new_coding_model_deepcoder14bpreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T13:09:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvc768</id>
    <title>Google just launched the A2A protocol were AI agents from any framework can work together</title>
    <updated>2025-04-09T17:56:16+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvc768/google_just_launched_the_a2a_protocol_were_ai/"&gt; &lt;img alt="Google just launched the A2A protocol were AI agents from any framework can work together" src="https://preview.redd.it/azpf25q5lute1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c562212b7e129e18a030a5673a2221e17473b30" title="Google just launched the A2A protocol were AI agents from any framework can work together" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're working on an even more MCP-oriented approach to this problem and are building in the open here if anyone is interested, would love to see peoples opinions on both approaches to see what you think it all. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/azpf25q5lute1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvc768/google_just_launched_the_a2a_protocol_were_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvc768/google_just_launched_the_a2a_protocol_were_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T17:56:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvp7fo</id>
    <title>Long context summarization: Qwen2.5-1M vs Gemma3 vs Mistral 3.1</title>
    <updated>2025-04-10T04:08:38+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tested long context summarization of these models, using ollama as backend:&lt;/p&gt; &lt;p&gt;Qwen2.5-14b-1m Q8&lt;/p&gt; &lt;p&gt;Gemma3 27b Q4KM (ollama gguf)&lt;/p&gt; &lt;p&gt;Mistral 3.1 24b Q4KM&lt;/p&gt; &lt;p&gt;Using the transcription of this 4hr Wan show video, it's about 55k~63k tokens for these 3 models:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=mk05ddf3mqg"&gt;https://www.youtube.com/watch?v=mk05ddf3mqg&lt;/a&gt;&lt;/p&gt; &lt;p&gt;System prompt: &lt;a href="https://pastebin.com/e4mKCAMk"&gt;https://pastebin.com/e4mKCAMk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;Qwen2.5 &lt;a href="https://pastebin.com/C4Ss67Ed"&gt;https://pastebin.com/C4Ss67Ed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Gemma3 &lt;a href="https://pastebin.com/btTv6RCT"&gt;https://pastebin.com/btTv6RCT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mistral 3.1 &lt;a href="https://pastebin.com/rMp9KMhE"&gt;https://pastebin.com/rMp9KMhE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Observation&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;Qwen2.5 did okay, mistral 3.1 still has the same repetition issue as 3&lt;/p&gt; &lt;p&gt;idk if there is something wrong with ollama's implementation, but gemma3 is really bad at this, like it even didn't mention the AMD card at all.&lt;/p&gt; &lt;p&gt;So I also tested gemma3 in &lt;strong&gt;google&lt;/strong&gt; ai studio which should has the best implementation for gemma3:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;An internal error has occured&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Then I tried open router:&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/Y1gX0bVb"&gt;https://pastebin.com/Y1gX0bVb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And it's waaaay better then ollama Q4, consider how mistral's Q4 is doing way better than gemma q4, &lt;strong&gt;I guess there is still some bugs in ollama's gemma3 implementation and you should avoid using it for long context tasks&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvp7fo/long_context_summarization_qwen251m_vs_gemma3_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvp7fo/long_context_summarization_qwen251m_vs_gemma3_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvp7fo/long_context_summarization_qwen251m_vs_gemma3_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T04:08:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvchif</id>
    <title>How we used NVIDIA TensorRT-LLM with Blackwell B200 to achieve 303 output tokens per second on DeepSeek R1</title>
    <updated>2025-04-09T18:07:37+00:00</updated>
    <author>
      <name>/u/avianio</name>
      <uri>https://old.reddit.com/user/avianio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvchif/how_we_used_nvidia_tensorrtllm_with_blackwell/"&gt; &lt;img alt="How we used NVIDIA TensorRT-LLM with Blackwell B200 to achieve 303 output tokens per second on DeepSeek R1" src="https://external-preview.redd.it/Rb0aJ2SdT0s5u9VBtMk4r2KGdiNryvu9_uLzVS6423c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1de9d7d850207207499d5314450055f45c8256e" title="How we used NVIDIA TensorRT-LLM with Blackwell B200 to achieve 303 output tokens per second on DeepSeek R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a technical blog post on how the team at Avian collaborated with Nvidia to achieve 303 output tokens per second, using FP4 quantization and their new Pytorch runtime.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avianio"&gt; /u/avianio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://new.avian.io/blog/article/deepseek_r1_303"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvchif/how_we_used_nvidia_tensorrtllm_with_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvchif/how_we_used_nvidia_tensorrtllm_with_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T18:07:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv5vic</id>
    <title>Alibaba AI Conference happening today! We may see Qwen3 in a few hours!</title>
    <updated>2025-04-09T13:32:27+00:00</updated>
    <author>
      <name>/u/MushroomGecko</name>
      <uri>https://old.reddit.com/user/MushroomGecko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5vic/alibaba_ai_conference_happening_today_we_may_see/"&gt; &lt;img alt="Alibaba AI Conference happening today! We may see Qwen3 in a few hours!" src="https://preview.redd.it/w79q9k0katte1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a242dd332547aef1d5aceaf7a7c766055e6c50e" title="Alibaba AI Conference happening today! We may see Qwen3 in a few hours!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MushroomGecko"&gt; /u/MushroomGecko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w79q9k0katte1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5vic/alibaba_ai_conference_happening_today_we_may_see/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5vic/alibaba_ai_conference_happening_today_we_may_see/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T13:32:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvlf6m</id>
    <title>Llama 4 Scout sub 50GB GGUF Quantization showdown (aka I did some KLD comparisons)</title>
    <updated>2025-04-10T00:43:16+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry in advanced if you've seen this already, wanted to post it here first but it got caught in auto-mod so I threw it up elsewhere, reposting now with permission&lt;/p&gt; &lt;p&gt;Big fat disclaimer, KLD is not everything, PPL is even less so, Top P is.. somewhat useful&lt;/p&gt; &lt;p&gt;Also huge thanks to Artus at BeaverAI Club for helping run the KLD for the full BF16 model, would have taken me days probably :D&lt;/p&gt; &lt;p&gt;Before working on Maverick, I decided to blow some compute on calculating the PPL/KLD/Top P of several small Scout quants, the ones I published, same setup but minus my PR changes (so what main would produce), and even threw in some of Unsloth's quants.&lt;/p&gt; &lt;p&gt;This is an effort to see if the PR changes I made are overall beneficial or detract. I don't love how much larger they get, we're losing some of the meaning of &amp;quot;IQ1_M&amp;quot; (which is supposed to average 1.75BPW..) and such, but nevertheless I figured it was worth finding out if these changes are worth pursuing and applying to Maverick&lt;/p&gt; &lt;p&gt;For reference, BF16's PPL is 8.6, so we expect all quant numbers to be pretty high. 8.6 PPL is not inherently bad for wikitext, it's odd, but also not a number worth reading into because all it really means is Scout wouldn't tend to arbitrarily spit out wikitext ü§∑‚Äç‚ôÇÔ∏è&lt;/p&gt; &lt;p&gt;Raw data (I'm so sorry mobile users):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Measurement&lt;/th&gt; &lt;th&gt;IQ1_M (mine)&lt;/th&gt; &lt;th&gt;IQ1_M (main)&lt;/th&gt; &lt;th&gt;IQ2_XXS (mine)&lt;/th&gt; &lt;th&gt;IQ2_XXS (main)&lt;/th&gt; &lt;th&gt;IQ2_S (mine)&lt;/th&gt; &lt;th&gt;UD-IQ1_M (unsloth)&lt;/th&gt; &lt;th&gt;Q2_K_L (mine)&lt;/th&gt; &lt;th&gt;Q2_K_L (main)&lt;/th&gt; &lt;th&gt;UD-Q2_K_XL (unsloth)&lt;/th&gt; &lt;th&gt;IQ3_XXS (mine)&lt;/th&gt; &lt;th&gt;IQ3_XXS (main)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Size (GB)&lt;/td&gt; &lt;td&gt;26.32&lt;/td&gt; &lt;td&gt;24.57&lt;/td&gt; &lt;td&gt;30.17&lt;/td&gt; &lt;td&gt;28.56&lt;/td&gt; &lt;td&gt;34.34&lt;/td&gt; &lt;td&gt;35.4&lt;/td&gt; &lt;td&gt;44&lt;/td&gt; &lt;td&gt;40.57&lt;/td&gt; &lt;td&gt;42.6&lt;/td&gt; &lt;td&gt;44.96&lt;/td&gt; &lt;td&gt;41.66&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mean PPL&lt;/td&gt; &lt;td&gt;11.81&lt;/td&gt; &lt;td&gt;13.79&lt;/td&gt; &lt;td&gt;10.55&lt;/td&gt; &lt;td&gt;11.66&lt;/td&gt; &lt;td&gt;9.85&lt;/td&gt; &lt;td&gt;10.30&lt;/td&gt; &lt;td&gt;9.02&lt;/td&gt; &lt;td&gt;9.88&lt;/td&gt; &lt;td&gt;9.31&lt;/td&gt; &lt;td&gt;9.266434&lt;/td&gt; &lt;td&gt;9.76184&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;KLD&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mean&lt;/td&gt; &lt;td&gt;0.691&lt;/td&gt; &lt;td&gt;0.933&lt;/td&gt; &lt;td&gt;0.464&lt;/td&gt; &lt;td&gt;0.664&lt;/td&gt; &lt;td&gt;0.361&lt;/td&gt; &lt;td&gt;0.376&lt;/td&gt; &lt;td&gt;0.217&lt;/td&gt; &lt;td&gt;0.332&lt;/td&gt; &lt;td&gt;0.185&lt;/td&gt; &lt;td&gt;0.164&lt;/td&gt; &lt;td&gt;0.244&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Max&lt;/td&gt; &lt;td&gt;17.819&lt;/td&gt; &lt;td&gt;23.806&lt;/td&gt; &lt;td&gt;26.647&lt;/td&gt; &lt;td&gt;26.761&lt;/td&gt; &lt;td&gt;17.597&lt;/td&gt; &lt;td&gt;21.264&lt;/td&gt; &lt;td&gt;24.180&lt;/td&gt; &lt;td&gt;17.556&lt;/td&gt; &lt;td&gt;23.286&lt;/td&gt; &lt;td&gt;28.166&lt;/td&gt; &lt;td&gt;25.849&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;99.9%&lt;/td&gt; &lt;td&gt;9.912&lt;/td&gt; &lt;td&gt;10.822&lt;/td&gt; &lt;td&gt;7.897&lt;/td&gt; &lt;td&gt;10.029&lt;/td&gt; &lt;td&gt;6.693&lt;/td&gt; &lt;td&gt;6.995&lt;/td&gt; &lt;td&gt;11.729&lt;/td&gt; &lt;td&gt;12.766&lt;/td&gt; &lt;td&gt;4.213&lt;/td&gt; &lt;td&gt;4.232&lt;/td&gt; &lt;td&gt;4.964&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;99%&lt;/td&gt; &lt;td&gt;5.463&lt;/td&gt; &lt;td&gt;6.250&lt;/td&gt; &lt;td&gt;4.084&lt;/td&gt; &lt;td&gt;5.094&lt;/td&gt; &lt;td&gt;3.237&lt;/td&gt; &lt;td&gt;3.560&lt;/td&gt; &lt;td&gt;2.108&lt;/td&gt; &lt;td&gt;2.966&lt;/td&gt; &lt;td&gt;1.844&lt;/td&gt; &lt;td&gt;1.600&lt;/td&gt; &lt;td&gt;2.178&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;median&lt;/td&gt; &lt;td&gt;0.315&lt;/td&gt; &lt;td&gt;0.503&lt;/td&gt; &lt;td&gt;0.187&lt;/td&gt; &lt;td&gt;0.336&lt;/td&gt; &lt;td&gt;0.141&lt;/td&gt; &lt;td&gt;0.131&lt;/td&gt; &lt;td&gt;0.067&lt;/td&gt; &lt;td&gt;0.125&lt;/td&gt; &lt;td&gt;0.060&lt;/td&gt; &lt;td&gt;0.056&lt;/td&gt; &lt;td&gt;0.099&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;10%&lt;/td&gt; &lt;td&gt;0.0053&lt;/td&gt; &lt;td&gt;0.0099&lt;/td&gt; &lt;td&gt;0.002&lt;/td&gt; &lt;td&gt;0.004&lt;/td&gt; &lt;td&gt;0.0012&lt;/td&gt; &lt;td&gt;0.0012&lt;/td&gt; &lt;td&gt;0.0005&lt;/td&gt; &lt;td&gt;0.0009&lt;/td&gt; &lt;td&gt;0.0004&lt;/td&gt; &lt;td&gt;0.0004&lt;/td&gt; &lt;td&gt;0.0005&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5%&lt;/td&gt; &lt;td&gt;0.00097&lt;/td&gt; &lt;td&gt;0.00179&lt;/td&gt; &lt;td&gt;0.0003&lt;/td&gt; &lt;td&gt;0.00064&lt;/td&gt; &lt;td&gt;0.00019&lt;/td&gt; &lt;td&gt;0.00018&lt;/td&gt; &lt;td&gt;0.00008&lt;/td&gt; &lt;td&gt;0.00013&lt;/td&gt; &lt;td&gt;0.00005&lt;/td&gt; &lt;td&gt;0.00005&lt;/td&gt; &lt;td&gt;0.00007&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1%&lt;/td&gt; &lt;td&gt;0.000046&lt;/td&gt; &lt;td&gt;0.000073&lt;/td&gt; &lt;td&gt;0.000011&lt;/td&gt; &lt;td&gt;0.000030&lt;/td&gt; &lt;td&gt;0.000007&lt;/td&gt; &lt;td&gt;0.000007&lt;/td&gt; &lt;td&gt;0.000003&lt;/td&gt; &lt;td&gt;0.000004&lt;/td&gt; &lt;td&gt;0.000001&lt;/td&gt; &lt;td&gt;0.000001&lt;/td&gt; &lt;td&gt;0.000002&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Delta probs&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mean&lt;/td&gt; &lt;td&gt;-8.03%&lt;/td&gt; &lt;td&gt;-10.30%&lt;/td&gt; &lt;td&gt;-4.62%&lt;/td&gt; &lt;td&gt;-6.70%&lt;/td&gt; &lt;td&gt;-3.38%&lt;/td&gt; &lt;td&gt;-3.46%&lt;/td&gt; &lt;td&gt;-2.14%&lt;/td&gt; &lt;td&gt;-2.37%&lt;/td&gt; &lt;td&gt;-1.38%&lt;/td&gt; &lt;td&gt;-1.13%&lt;/td&gt; &lt;td&gt;-1.57%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Max&lt;/td&gt; &lt;td&gt;99.67%&lt;/td&gt; &lt;td&gt;98.73%&lt;/td&gt; &lt;td&gt;99.81%&lt;/td&gt; &lt;td&gt;99.81%&lt;/td&gt; &lt;td&gt;99.13%&lt;/td&gt; &lt;td&gt;98.90%&lt;/td&gt; &lt;td&gt;99.88%&lt;/td&gt; &lt;td&gt;99.81%&lt;/td&gt; &lt;td&gt;99.83%&lt;/td&gt; &lt;td&gt;99.91%&lt;/td&gt; &lt;td&gt;99.89%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;99.9%&lt;/td&gt; &lt;td&gt;77.40%&lt;/td&gt; &lt;td&gt;79.77%&lt;/td&gt; &lt;td&gt;76.36%&lt;/td&gt; &lt;td&gt;79.42%&lt;/td&gt; &lt;td&gt;75.03%&lt;/td&gt; &lt;td&gt;76.59%&lt;/td&gt; &lt;td&gt;69.34%&lt;/td&gt; &lt;td&gt;75.65%&lt;/td&gt; &lt;td&gt;69.69%&lt;/td&gt; &lt;td&gt;65.60%&lt;/td&gt; &lt;td&gt;71.73%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;99%&lt;/td&gt; &lt;td&gt;42.37%&lt;/td&gt; &lt;td&gt;47.40%&lt;/td&gt; &lt;td&gt;41.62%&lt;/td&gt; &lt;td&gt;47.11%&lt;/td&gt; &lt;td&gt;40.06%&lt;/td&gt; &lt;td&gt;40.50%&lt;/td&gt; &lt;td&gt;32.34%&lt;/td&gt; &lt;td&gt;41.88%&lt;/td&gt; &lt;td&gt;33.46%&lt;/td&gt; &lt;td&gt;31.38%&lt;/td&gt; &lt;td&gt;37.88%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;95.00%&lt;/td&gt; &lt;td&gt;15.79%&lt;/td&gt; &lt;td&gt;18.51%&lt;/td&gt; &lt;td&gt;16.32%&lt;/td&gt; &lt;td&gt;19.86%&lt;/td&gt; &lt;td&gt;16.05%&lt;/td&gt; &lt;td&gt;15.56%&lt;/td&gt; &lt;td&gt;12.41%&lt;/td&gt; &lt;td&gt;17.30%&lt;/td&gt; &lt;td&gt;12.83%&lt;/td&gt; &lt;td&gt;12.71%&lt;/td&gt; &lt;td&gt;16.04%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;90.00%&lt;/td&gt; &lt;td&gt;6.59%&lt;/td&gt; &lt;td&gt;7.56%&lt;/td&gt; &lt;td&gt;7.69%&lt;/td&gt; &lt;td&gt;9.05%&lt;/td&gt; &lt;td&gt;7.62%&lt;/td&gt; &lt;td&gt;7.33%&lt;/td&gt; &lt;td&gt;5.92%&lt;/td&gt; &lt;td&gt;8.86%&lt;/td&gt; &lt;td&gt;6.43%&lt;/td&gt; &lt;td&gt;6.50%&lt;/td&gt; &lt;td&gt;8.23%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;75.00%&lt;/td&gt; &lt;td&gt;0.16%&lt;/td&gt; &lt;td&gt;0.13%&lt;/td&gt; &lt;td&gt;0.44%&lt;/td&gt; &lt;td&gt;0.35%&lt;/td&gt; &lt;td&gt;0.54%&lt;/td&gt; &lt;td&gt;0.51%&lt;/td&gt; &lt;td&gt;0.53%&lt;/td&gt; &lt;td&gt;0.89%&lt;/td&gt; &lt;td&gt;0.70%&lt;/td&gt; &lt;td&gt;0.70%&lt;/td&gt; &lt;td&gt;0.86%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Median&lt;/td&gt; &lt;td&gt;-0.78%&lt;/td&gt; &lt;td&gt;-1.21%&lt;/td&gt; &lt;td&gt;-0.18%&lt;/td&gt; &lt;td&gt;-0.42%&lt;/td&gt; &lt;td&gt;-0.09%&lt;/td&gt; &lt;td&gt;-0.09%&lt;/td&gt; &lt;td&gt;-0.03%&lt;/td&gt; &lt;td&gt;-0.02%&lt;/td&gt; &lt;td&gt;-0.01%&lt;/td&gt; &lt;td&gt;-0.01%&lt;/td&gt; &lt;td&gt;-0.01%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;25.00%&lt;/td&gt; &lt;td&gt;-11.66%&lt;/td&gt; &lt;td&gt;-15.85%&lt;/td&gt; &lt;td&gt;-6.11%&lt;/td&gt; &lt;td&gt;-9.93%&lt;/td&gt; &lt;td&gt;-4.65%&lt;/td&gt; &lt;td&gt;-4.56%&lt;/td&gt; &lt;td&gt;-2.86%&lt;/td&gt; &lt;td&gt;-3.40%&lt;/td&gt; &lt;td&gt;-2.11%&lt;/td&gt; &lt;td&gt;-1.96%&lt;/td&gt; &lt;td&gt;-2.66%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;10.00%&lt;/td&gt; &lt;td&gt;-35.57%&lt;/td&gt; &lt;td&gt;-46.38%&lt;/td&gt; &lt;td&gt;-23.74%&lt;/td&gt; &lt;td&gt;-34.08%&lt;/td&gt; &lt;td&gt;-19.19%&lt;/td&gt; &lt;td&gt;-18.97%&lt;/td&gt; &lt;td&gt;-12.61%&lt;/td&gt; &lt;td&gt;-16.60%&lt;/td&gt; &lt;td&gt;-10.76%&lt;/td&gt; &lt;td&gt;-10.12%&lt;/td&gt; &lt;td&gt;-13.68%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.00%&lt;/td&gt; &lt;td&gt;-56.91%&lt;/td&gt; &lt;td&gt;-68.67%&lt;/td&gt; &lt;td&gt;-40.94%&lt;/td&gt; &lt;td&gt;-53.40%&lt;/td&gt; &lt;td&gt;-33.86%&lt;/td&gt; &lt;td&gt;-34.31%&lt;/td&gt; &lt;td&gt;-23.01%&lt;/td&gt; &lt;td&gt;-30.06%&lt;/td&gt; &lt;td&gt;-20.07%&lt;/td&gt; &lt;td&gt;-18.53%&lt;/td&gt; &lt;td&gt;-24.41%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.00%&lt;/td&gt; &lt;td&gt;-91.25%&lt;/td&gt; &lt;td&gt;-95.39%&lt;/td&gt; &lt;td&gt;-80.42%&lt;/td&gt; &lt;td&gt;-87.98%&lt;/td&gt; &lt;td&gt;-70.51%&lt;/td&gt; &lt;td&gt;-73.12%&lt;/td&gt; &lt;td&gt;-55.83%&lt;/td&gt; &lt;td&gt;-67.16%&lt;/td&gt; &lt;td&gt;-49.11%&lt;/td&gt; &lt;td&gt;-44.35%&lt;/td&gt; &lt;td&gt;-53.65%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.10%&lt;/td&gt; &lt;td&gt;-99.61%&lt;/td&gt; &lt;td&gt;-99.87%&lt;/td&gt; &lt;td&gt;-98.74%&lt;/td&gt; &lt;td&gt;-99.76%&lt;/td&gt; &lt;td&gt;-95.85%&lt;/td&gt; &lt;td&gt;-95.98%&lt;/td&gt; &lt;td&gt;-99.92%&lt;/td&gt; &lt;td&gt;-99.92%&lt;/td&gt; &lt;td&gt;-82.64%&lt;/td&gt; &lt;td&gt;-78.71%&lt;/td&gt; &lt;td&gt;-86.82%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Minimum&lt;/td&gt; &lt;td&gt;-100.00%&lt;/td&gt; &lt;td&gt;-100.00%&lt;/td&gt; &lt;td&gt;-100.00%&lt;/td&gt; &lt;td&gt;-100.00%&lt;/td&gt; &lt;td&gt;-99.95%&lt;/td&gt; &lt;td&gt;-99.99%&lt;/td&gt; &lt;td&gt;-100.00%&lt;/td&gt; &lt;td&gt;-100.00%&lt;/td&gt; &lt;td&gt;-99.90%&lt;/td&gt; &lt;td&gt;-100.00%&lt;/td&gt; &lt;td&gt;-100.00%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RMS Œîp&lt;/td&gt; &lt;td&gt;23.63%&lt;/td&gt; &lt;td&gt;27.63%&lt;/td&gt; &lt;td&gt;19.13%&lt;/td&gt; &lt;td&gt;23.06%&lt;/td&gt; &lt;td&gt;16.88%&lt;/td&gt; &lt;td&gt;17.16%&lt;/td&gt; &lt;td&gt;13.55%&lt;/td&gt; &lt;td&gt;16.31%&lt;/td&gt; &lt;td&gt;12.16%&lt;/td&gt; &lt;td&gt;11.30%&lt;/td&gt; &lt;td&gt;13.69%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Same top&lt;/td&gt; &lt;td&gt;68.58%&lt;/td&gt; &lt;td&gt;62.65%&lt;/td&gt; &lt;td&gt;74.02%&lt;/td&gt; &lt;td&gt;67.77%&lt;/td&gt; &lt;td&gt;76.74%&lt;/td&gt; &lt;td&gt;77.00%&lt;/td&gt; &lt;td&gt;82.92%&lt;/td&gt; &lt;td&gt;77.85%&lt;/td&gt; &lt;td&gt;83.42%&lt;/td&gt; &lt;td&gt;84.28%&lt;/td&gt; &lt;td&gt;80.08%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Image of the above:&lt;/p&gt; &lt;p&gt;&lt;del&gt;&lt;a href="https://i.imgur.com/35GAKe5.png"&gt;https://i.imgur.com/35GAKe5.png&lt;/a&gt;&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;EDIT&lt;/em&gt;: Messed up some of the lower calculations! (that's why i included the raw data haha..) here's an updated image:&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/hFkza66.png"&gt;https://i.imgur.com/hFkza66.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also added a logit for the Top P for the size (and made it clearer by multiplying by 100 after), since I think this paints a more clear image for Top P.. Obviously if the model is extremely tiny but sometimes gives the right answer, it'll get a super high Top P/GB, but as the Top P gets closer to 100, that's where the differences matter more. The logit calculation gives a better picture of the differences IMO&lt;/p&gt; &lt;p&gt;I added at the bottom some &amp;quot;metrics&amp;quot;, like 1/PPL/MB (since GB was a tiny number)&lt;/p&gt; &lt;p&gt;For all of these, bigger is better (I inversed PPL, KLD, and RMS to get meaningful results, since smaller per GB is a weird metric to look at)&lt;/p&gt; &lt;p&gt;I added some colour to highlight a few things, but DON'T read too much into them, it's purely informational. I can't REALLY say which values are more important (though I will say PPL itself seems pretty useless when even the full BF16 model got over 8)&lt;/p&gt; &lt;p&gt;KLD, RMS, and Top P are all relevant regardless of the PPL, simply because they tell you how similarly a quantization performs to the full model weights. This doesn't mean that one that's closer is strictly better, just more similar&lt;/p&gt; &lt;p&gt;And I share the full information because there are distinct sections where each quant performs admirably&lt;/p&gt; &lt;p&gt;In terms of performance per GB, my IQ3_XXS seems to come out on top (by a hair), but it has by far the worst MAX KLD value.. That's not super concerning since the 99.9% is very reasonable, but it's worth noting that no quant is best across the board.. maybe something to continue striving towards! My optimization search is ongoing :)&lt;/p&gt; &lt;p&gt;More than anything it looks like my IQ3_XXS and Unsloth's UD-Q2_K_XL are the kings of sub 50GB, trading blows across the chart&lt;/p&gt; &lt;p&gt;And if you need even less weight, both my IQ2_S and Unsloth's UD-1Q_M offer pretty great performance for around 35GB!&lt;/p&gt; &lt;p&gt;Anyways, hope someone finds something interesting in the charts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvlf6m/llama_4_scout_sub_50gb_gguf_quantization_showdown/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvlf6m/llama_4_scout_sub_50gb_gguf_quantization_showdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvlf6m/llama_4_scout_sub_50gb_gguf_quantization_showdown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T00:43:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvi860</id>
    <title>PSA: Gemma 3 QAT gguf models have some wrongly configured tokens</title>
    <updated>2025-04-09T22:08:06+00:00</updated>
    <author>
      <name>/u/dampflokfreund</name>
      <uri>https://old.reddit.com/user/dampflokfreund</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;so as I loaded my 12B IT q4_0 QAT model, I've noticed a strage error in llama.cpp: &amp;quot;load: control-looking token: 106 '' was not control-type; this is probably a bug in the model. its type will be overridden&amp;quot;&lt;/p&gt; &lt;p&gt;So I've wondered, is this normal and loaded a Bartowski file, and indeed, that error was nowhere to be seen. After that, I did some digging and came across this post by the guy who implemented Gemma 3 and LLama 4 support in llama.cpp: &lt;a href="https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/discussions/3#67f6a2e0207b4bceea793151"&gt;https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/discussions/3#67f6a2e0207b4bceea793151&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This looked awfully similar to my error, so what I did was set both token 105 and 106 to control (which are &amp;lt;start\_of\_turn&amp;gt; and &amp;lt;end\_of\_turn&amp;gt; btw) instead of normal (like it's the case with the bartowski files too) using the huggingface gguf editor. Not only that, the image start and end tokens were also not set to control, unlike the original. I've fixed that and noticed a boost in the image capabilities immediately.&lt;/p&gt; &lt;p&gt;If you have noticed weirdness with the QAT models in comparison to the older bart models, then it was most likely due to that. On top of that, the name metadata was missing as well which I've added back, apparently some inference backends need it. &lt;/p&gt; &lt;p&gt;I have uploaded it here: &lt;a href="https://huggingface.co/Dampfinchen/google-gemma-3-12b-it-qat-q4_0-gguf-small-fix"&gt;https://huggingface.co/Dampfinchen/google-gemma-3-12b-it-qat-q4_0-gguf-small-fix&lt;/a&gt; Note that it is based on &lt;a href="https://huggingface.co/stduhpf"&gt;stduhpf&lt;/a&gt;'s version which is faster without any compromise to performance.&lt;/p&gt; &lt;p&gt;Happy testing!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dampflokfreund"&gt; /u/dampflokfreund &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvi860/psa_gemma_3_qat_gguf_models_have_some_wrongly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvi860/psa_gemma_3_qat_gguf_models_have_some_wrongly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvi860/psa_gemma_3_qat_gguf_models_have_some_wrongly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T22:08:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvgpju</id>
    <title>Moonshot AI released Kimi-VL MoE (3B/16B) Thinking</title>
    <updated>2025-04-09T21:02:16+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvgpju/moonshot_ai_released_kimivl_moe_3b16b_thinking/"&gt; &lt;img alt="Moonshot AI released Kimi-VL MoE (3B/16B) Thinking" src="https://b.thumbs.redditmedia.com/eEN3TB95dTXyXnsBnqObPtpp29ara6PvOVYX7yFQo1c.jpg" title="Moonshot AI released Kimi-VL MoE (3B/16B) Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Moonshot AI's Kimi-VL and Kimi-VL-Thinking! &lt;/p&gt; &lt;p&gt;üí° An MoE VLM and an MoE Reasoning VLM with only ~3B activated parameters (total 16B) üß† Strong multimodal reasoning (36.8% on MathVision, on par with 10x larger models) and agent skills (34.5% on ScreenSpot-Pro) üñºÔ∏è Handles high-res visuals natively with MoonViT (867 on OCRBench) üßæ Supports long context windows up to 128K (35.1% on MMLongBench-Doc, 64.5% on LongVideoBench) üèÜ Outperforms larger models like GPT-4o on key benchmarks&lt;/p&gt; &lt;p&gt;üìú Paper: &lt;a href="https://github.com/MoonshotAI/Kimi-VL/blob/main/Kimi-VL.pdf"&gt;https://github.com/MoonshotAI/Kimi-VL/blob/main/Kimi-VL.pdf&lt;/a&gt; ü§ó Huggingface: &lt;a href="https://huggingface.co/collections/moonshotai/kimi-vl-a3b-67f67b6ac91d3b03d382dd85"&gt;https://huggingface.co/collections/moonshotai/kimi-vl-a3b-67f67b6ac91d3b03d382dd85&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jvgpju"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvgpju/moonshot_ai_released_kimivl_moe_3b16b_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvgpju/moonshot_ai_released_kimivl_moe_3b16b_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T21:02:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv5uk8</id>
    <title>OmniSVG: A Unified Scalable Vector Graphics Generation Model</title>
    <updated>2025-04-09T13:31:15+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/"&gt; &lt;img alt="OmniSVG: A Unified Scalable Vector Graphics Generation Model" src="https://external-preview.redd.it/MHI3ZzMzc3Q5dHRlMexiJYH3Awkmn9VEWXtNssspPIW9nVy43T4cWZBoNTdU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df299d5217413408d56d4fe1cca2c1dc834179f7" title="OmniSVG: A Unified Scalable Vector Graphics Generation Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saw this on X. If this is true, this SVG generation capability is really amazing, and I can't wait to run it locally. I checked and it seems the model weights haven't been released on Hugging Face yet.&lt;/p&gt; &lt;p&gt;site: &lt;a href="http://omnisvg.github.io"&gt;omnisvg.github.io&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jk6dp2st9tte1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T13:31:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvknex</id>
    <title>I've realized that Llama 4's odd architecture makes it perfect for my Mac and my workflows</title>
    <updated>2025-04-10T00:02:58+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm a huge workflow enthusiast when it comes to LLMs, and believe the appropriate application of iterating through a problem + tightly controlled steps can solve just about anything. I'm also a Mac user. For a while my main machine was an M2 Ultra Mac Studio, but recently I got the 512GB M3 Ultra Mac Studio, &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jaqpiu/mac_speed_comparison_m2_ultra_vs_m3_ultra_using/"&gt;which honestly I had a little bit of buyer's remorse for.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The thing about workflows is that speed is the biggest pain point; and when you use a Mac, you don't get a lot of speed, but you have memory to spare. It's really not a great matchup.&lt;/p&gt; &lt;p&gt;Speed is important because you can take even some of the weakest models and, with workflows, make them do amazing things just by scoping their thinking into multi-step problem solving, and having them validate themselves constantly along the way.&lt;/p&gt; &lt;p&gt;But again- the problem is speed. On my mac, my complex coding workflow can take up to 20-30 minutes to run using 32b-70b models, which is absolutely miserable. I'll ask it a question and then go take a shower, eat food, etc.&lt;/p&gt; &lt;p&gt;For a long time, I kept telling myself that I'd just use 8-14b models in my workflows. With the speed those models would run at, I could run really complex workflows easily... but I could never convince myself to stick with them, since any workflow that makes the 14b great would make the 32b even better. It's always been hard to pass that quality up.&lt;/p&gt; &lt;p&gt;Enter Llama 4. &lt;strong&gt;Llama 4 Maverick Q8&lt;/strong&gt; fits on my M3 Studio, and the speed is very acceptable for its 400b size.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Maverick Q8 in KoboldCpp- 9.3k context, 270 token response.&lt;/em&gt; &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;CtxLimit:9378/32768,&lt;br /&gt; Amt:270/300, Init:0.18s,&lt;br /&gt; Process:62.05s (146.69T/s),&lt;br /&gt; Generate:16.06s (16.81T/s),&lt;br /&gt; Total:78.11s &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This model basically has the memory footprint of a 400b, but otherwise is a supercharged 17b. And since memory footprint was never a pain on the Mac, but speed is? That's the perfect combination for my use-case.&lt;/p&gt; &lt;p&gt;I know this model is weird, and the benchmarks don't remotely line up to the memory requirements. But for me? I realized today that this thing is exactly what I've been wanting... though I do think it still has a tokenizer issue or something. &lt;/p&gt; &lt;p&gt;Honestly, I doubt they'll go with this architecture again due to its poor reception, but for now... I'm quite happy with this model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: &lt;em&gt;I did try MLX; y'all actually talked me into using it, and I'm really liking it. But Maverick and Scout were both broken for me last time I tried it. I pulled down the PR branch for it, but the model would not shut up for anything in the world. It will talk until it hits the token limit.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Alternatively, Unsloth's GGUFs seem to work great.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvknex/ive_realized_that_llama_4s_odd_architecture_makes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvknex/ive_realized_that_llama_4s_odd_architecture_makes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvknex/ive_realized_that_llama_4s_odd_architecture_makes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T00:02:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvqqvm</id>
    <title>Second Me : Fully Local AI Self with Identity &amp; Memory Modeling‚Äî‚Äîwith Docker &amp; API support now live</title>
    <updated>2025-04-10T05:48:28+00:00</updated>
    <author>
      <name>/u/DontPlayMeLikeAFool</name>
      <uri>https://old.reddit.com/user/DontPlayMeLikeAFool</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvqqvm/second_me_fully_local_ai_self_with_identity/"&gt; &lt;img alt="Second Me : Fully Local AI Self with Identity &amp;amp; Memory Modeling‚Äî‚Äîwith Docker &amp;amp; API support now live" src="https://preview.redd.it/zc18secp4yte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=38886c3f5d327276b3971dbaf783a586924f1f1a" title="Second Me : Fully Local AI Self with Identity &amp;amp; Memory Modeling‚Äî‚Äîwith Docker &amp;amp; API support now live" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I'm one of the contributors to &lt;a href="https://github.com/Mindverse/Second-Me"&gt;Second Me&lt;/a&gt;, an open-source, fully local AI project designed for personal memory, reasoning, and identity modeling. Think of it as a customizable ‚ÄúAI self‚Äù ‚Äî trained on your data, aligned with your values, and fully under your control (not OpenAI‚Äôs).&lt;/p&gt; &lt;p&gt;We hit &lt;strong&gt;6,000+ stars in 7 days&lt;/strong&gt;, which is wild ‚Äî but what‚Äôs even cooler is what‚Äôs been happening &lt;strong&gt;after&lt;/strong&gt; launch:&lt;/p&gt; &lt;h1&gt;üîß What It Does (tl;dr):&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Personal AI&lt;/strong&gt;, locally trained and run. 100% privacy with full local execution.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hierarchical Memory Modeling (HMM)&lt;/strong&gt; for authentic, long-term personalization.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Me-alignment&lt;/strong&gt; structure that mirrors individual values and identity.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Second Me Protocol (SMP)&lt;/strong&gt; for decentralized, peer-to-peer AI interaction.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;New in this release:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full Docker support for macOS (Apple Silicon), Windows, and Linux&lt;/li&gt; &lt;li&gt;OpenAI-Compatible API Interface&lt;/li&gt; &lt;li&gt;MLX training support (Beta)&lt;/li&gt; &lt;li&gt;Significant performance enhancements&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üíª Community Contributions&lt;/h1&gt; &lt;p&gt;In just 2 weeks post-launch:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;60+ PRs, 70+ issues&lt;/li&gt; &lt;li&gt;Contributors from Tokyo to Dubai: students, academics, enterprise devs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Highlights from the GitHub:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ü§ñ WeChat bot integration ‚Äî #81 by &lt;a href="/u/Zero-coder"&gt;u/Zero-coder&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üåè Japanese README localization ‚Äî #115 by &lt;a href="/u/eltociear"&gt;u/eltociear&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìÅ Improved file resource management ‚Äî #74 by &lt;a href="/u/mahdirahimi1999"&gt;u/mahdirahimi1999&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üîê File name validation for added security ‚Äî #62 by &lt;a href="/u/umutcrs"&gt;u/umutcrs&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks to their and others‚Äô feedback, features like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üîÑ Multi-platform deployment&lt;/li&gt; &lt;li&gt;üìù Note-based continuous training&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚Ä¶have been added to the roadmap.&lt;/p&gt; &lt;p&gt;üìà &lt;strong&gt;In the Wild&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Tech creator &lt;a href="/u/GOROman"&gt;u/GOROman&lt;/a&gt; documented a full workflow for deploying Second Me, training it on &lt;strong&gt;75GB of his own X posts since 2007&lt;/strong&gt; ‚Äî and even bought a Mac Studio just for it.&lt;/p&gt; &lt;p&gt;Inspired by his post, &lt;a href="/u/Yuzunose"&gt;u/Yuzunose&lt;/a&gt; envisioned linking Second Me with &lt;strong&gt;VRChat&lt;/strong&gt;, giving AI a persistent virtual persona to interact with others on your behalf.&lt;/p&gt; &lt;h1&gt;‚è≠Ô∏è What‚Äôs Next?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Your Identity as an Interface&lt;/strong&gt;: Use your AI self as a consistent entry point across platforms ‚Äî carrying your identity, memory, and thought process ‚Äî accessible by other users.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deep Reasoning &amp;amp; Continuous Learning&lt;/strong&gt;: We‚Äôre integrating Chain of Thought-style reasoning (think OpenAI o1 / DeepSeek R1) and one-click continuous training. The more data you feed it, the more your Second Me evolves to think like &lt;em&gt;you&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üîó GitHub: &lt;a href="https://github.com/Mindverse/Second-Me"&gt;https://github.com/Mindverse/Second-Me&lt;/a&gt;&lt;br /&gt; üìÑ Paper: &lt;a href="https://arxiv.org/abs/2503.08102"&gt;https://arxiv.org/abs/2503.08102&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We‚Äôre building Second Me so that your AI extends &lt;em&gt;your&lt;/em&gt; capabilities ‚Äî not someone else‚Äôs platform. If you value privacy, customization, and digital freedom, we‚Äôd love your thoughts, feedback, or contributions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DontPlayMeLikeAFool"&gt; /u/DontPlayMeLikeAFool &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zc18secp4yte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvqqvm/second_me_fully_local_ai_self_with_identity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvqqvm/second_me_fully_local_ai_self_with_identity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T05:48:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvx09w</id>
    <title>Model Context Protocol (MCP) Explained</title>
    <updated>2025-04-10T12:45:22+00:00</updated>
    <author>
      <name>/u/Nir777</name>
      <uri>https://old.reddit.com/user/Nir777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone‚Äôs talking about MCP these days. But‚Ä¶ what is MCP? (Spoiler: it‚Äôs the new standard for how AI systems connect with tools.)&lt;/p&gt; &lt;p&gt;üß† When should you use it?&lt;/p&gt; &lt;p&gt;üõ†Ô∏è How can you create your own server?&lt;/p&gt; &lt;p&gt;üîå How can you connect to existing ones?&lt;/p&gt; &lt;p&gt;I covered it all in detail in this (Free) article, which took me a long time to write.&lt;/p&gt; &lt;p&gt;Enjoy! üôå&lt;/p&gt; &lt;p&gt;&lt;a href="https://open.substack.com/pub/diamantai/p/model-context-protocol-mcp-explained?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false"&gt;Link to the full blog post&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nir777"&gt; /u/Nir777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvx09w/model_context_protocol_mcp_explained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvx09w/model_context_protocol_mcp_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvx09w/model_context_protocol_mcp_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T12:45:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvsvzj</id>
    <title>Just did a deep dive into Google's Agent Development Kit (ADK). Here are some thoughts, nitpicks, and things I loved (unbiased)</title>
    <updated>2025-04-10T08:23:51+00:00</updated>
    <author>
      <name>/u/Any-Cockroach-3233</name>
      <uri>https://old.reddit.com/user/Any-Cockroach-3233</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;The CLI is excellent. adk web, adk run, and api_server make it super smooth to start building and debugging. It feels like a proper developer-first tool. Love this part.&lt;/li&gt; &lt;li&gt;The docs have some unnecessary setup steps‚Äîlike creating folders manually - that add friction for no real benefit.&lt;/li&gt; &lt;li&gt;Support for multiple model providers is impressive. Not just Gemini, but also GPT-4o, Claude Sonnet, LLaMA, etc, thanks to LiteLLM. Big win for flexibility.&lt;/li&gt; &lt;li&gt;Async agents and conversation management introduce unnecessary complexity. It‚Äôs powerful, but the developer experience really suffers here.&lt;/li&gt; &lt;li&gt;Artifact management is a great addition. Being able to store/load files or binary data tied to a session is genuinely useful for building stateful agents.&lt;/li&gt; &lt;li&gt;The different types of agents feel a bit overengineered. LlmAgent works but could‚Äôve stuck to a cleaner interface. Sequential, Parallel, and Loop agents are interesting, but having three separate interfaces instead of a unified workflow concept adds cognitive load. Custom agents are nice in theory, but I‚Äôd rather just plug in a Python function.&lt;/li&gt; &lt;li&gt;AgentTool is a standout. Letting one agent use another as a tool is a smart, modular design.&lt;/li&gt; &lt;li&gt;Eval support is there, but again, the DX doesn‚Äôt feel intuitive or smooth.&lt;/li&gt; &lt;li&gt;Guardrail callbacks are a great idea, but their implementation is more complex than it needs to be. This could be simplified without losing flexibility.&lt;/li&gt; &lt;li&gt;Session state management is one of the weakest points right now. It‚Äôs just not easy to work with.&lt;/li&gt; &lt;li&gt;Deployment options are solid. Being able to deploy via Agent Engine (GCP handles everything) or use Cloud Run (for control over infra) gives developers the right level of control.&lt;/li&gt; &lt;li&gt;Callbacks, in general, feel like a strong foundation for building event-driven agent applications. There‚Äôs a lot of potential here.&lt;/li&gt; &lt;li&gt;Minor nitpick: the artifacts documentation currently points to a 404.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Final thoughts&lt;/p&gt; &lt;p&gt;Frameworks like ADK are most valuable when they empower beginners and intermediate developers to build confidently. But right now, the developer experience feels like it's optimized for advanced users only. The ideas are strong, but the complexity and boilerplate may turn away the very people who‚Äôd benefit most. A bit of DX polish could make ADK the go-to framework for building agentic apps at scale.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Cockroach-3233"&gt; /u/Any-Cockroach-3233 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvsvzj/just_did_a_deep_dive_into_googles_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvsvzj/just_did_a_deep_dive_into_googles_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvsvzj/just_did_a_deep_dive_into_googles_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T08:23:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvtn1t</id>
    <title>"Dragontail" model at LMarena is a potential beast</title>
    <updated>2025-04-10T09:19:23+00:00</updated>
    <author>
      <name>/u/IrisColt</name>
      <uri>https://old.reddit.com/user/IrisColt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm curious if anyone has any suspicions about the true identity behind the Dragontail model at LMArena. From what I've seen so far, this mysterious model performs on par with top-tier models like o3-mini-high and claude-3-7-sonnet-20250219-thinking-32k, but what it sets it apart from them is that it consistently delivers the correct answers (tedious mathematical problems). Sadly, open weights models such as DeepSeek V3 or R1, Llama4, Cohere's, are not even close to be able to solve them. There is also a (slightly worse) Shadebrook model that I suspect is also related to it.&lt;/p&gt; &lt;p&gt;Does anyone have any theories or insights about which model might actually be powering this beast?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IrisColt"&gt; /u/IrisColt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvtn1t/dragontail_model_at_lmarena_is_a_potential_beast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvtn1t/dragontail_model_at_lmarena_is_a_potential_beast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvtn1t/dragontail_model_at_lmarena_is_a_potential_beast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T09:19:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvpwgc</id>
    <title>Bindu Reddy, CEO of AbacusAI (LiveBench) states Qwen 3 ‚Äúis coming in hours‚Äù</title>
    <updated>2025-04-10T04:52:17+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvpwgc/bindu_reddy_ceo_of_abacusai_livebench_states_qwen/"&gt; &lt;img alt="Bindu Reddy, CEO of AbacusAI (LiveBench) states Qwen 3 ‚Äúis coming in hours‚Äù" src="https://external-preview.redd.it/95PFso5FRvaqkffIUa3P3b8toY1_H2zLUQj524lS6zs.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61d55dbb224ea9b043b89c3770cb1097d7dc7459" title="Bindu Reddy, CEO of AbacusAI (LiveBench) states Qwen 3 ‚Äúis coming in hours‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/bindureddy/status/1910185483545776630?s=46"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvpwgc/bindu_reddy_ceo_of_abacusai_livebench_states_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvpwgc/bindu_reddy_ceo_of_abacusai_livebench_states_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T04:52:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvw91v</id>
    <title>Notes on Llama 4: The hits, the misses, and the disasters</title>
    <updated>2025-04-10T12:05:25+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Llama 4 is here, but definitely not in the shape everyone wanted. There‚Äôs only negative sentiment towards it. Nobody seems to say good things about it except for a few Meta employees.&lt;/p&gt; &lt;p&gt;They seriously rushed the launch, but I am still not sure why. If the models were bad, why not postpone it? Was it something to do with tariffs, the anticipation of Monday market crash, to cushion their stock? &lt;/p&gt; &lt;p&gt;The entire launch was muddled with controversies, from poor models and false claims to bungled-up benchmarks. But are there any good Llama 4 models? If you search hard enough, there are a few.&lt;/p&gt; &lt;p&gt;Here is an overview of the Llama 4 models.&lt;/p&gt; &lt;h1&gt;The Hits&lt;/h1&gt; &lt;p&gt;There‚Äôs a very few good things about the Llama 4 models.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;10 million context window in Scout and 1 million in Maverick. Good at the needle in the haystack tests I have done.&lt;/li&gt; &lt;li&gt;The Maverick seems to be a model created for agentic use cases, and it performs well on the function-calling benchmarks.&lt;/li&gt; &lt;li&gt;It‚Äôs very fast and cheap, again compliments function calling use cases.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Misses&lt;/h1&gt; &lt;p&gt;A lot of misses, indeed&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Starting with a restrictive, not-so-open-source Llama Licence. Still a mystery why it is when Deepseek models are MIT.&lt;/li&gt; &lt;li&gt;The 400b Maverick doesn‚Äôt justify its size. I'm not sure why they went with 17b active parameters; it‚Äôs worse than QwQ 32b in reasoning.&lt;/li&gt; &lt;li&gt;It neither offers the best code gen, writing, or reasoning.&lt;/li&gt; &lt;li&gt;The biggest miss is that there is no paper, no system card, just a blog post. Everyone looked up to Meta for this, and now they have botched this.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Disasters&lt;/h1&gt; &lt;p&gt;They are not recovering from this ever again.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;They literally gamed the Lmsys the sloppiest benchmark just to appear good. It‚Äôs sad at this point. I'm not sure if they cooked up other benchmarks mentioned in their release blog post.&lt;/li&gt; &lt;li&gt;Meta has tarnished their image again. They had the people's mandate, and they chose to squander it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Being a long-time Llama appreciator, the Llama 4 launch was such a letdown. It would have been still fine and forgotten if it was just a bad model, but cooking up benchmarks to appear that they are still in the AI race is horrible. &lt;/p&gt; &lt;p&gt;Full write-up on the Llama 4 launch here: &lt;a href="https://composio.dev/blog/notes-on-llama-4-the-hits-the-misses-and-the-disasters/"&gt;Notes on Llama 4: The Hits, the Misses, and the Disasters&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would love to know your opinions on Llama 4 and would be interested to hear if you found anything good with these models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvw91v/notes_on_llama_4_the_hits_the_misses_and_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvw91v/notes_on_llama_4_the_hits_the_misses_and_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvw91v/notes_on_llama_4_the_hits_the_misses_and_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T12:05:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvs66w</id>
    <title>Qwen Dev: Qwen3 not gonna release "in hours", still need more time</title>
    <updated>2025-04-10T07:29:02+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvs66w/qwen_dev_qwen3_not_gonna_release_in_hours_still/"&gt; &lt;img alt="Qwen Dev: Qwen3 not gonna release &amp;quot;in hours&amp;quot;, still need more time" src="https://preview.redd.it/3kcfx9xnmyte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acd7d81fbe37029fcaaed94d7eab854f4dea3442" title="Qwen Dev: Qwen3 not gonna release &amp;quot;in hours&amp;quot;, still need more time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3kcfx9xnmyte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvs66w/qwen_dev_qwen3_not_gonna_release_in_hours_still/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvs66w/qwen_dev_qwen3_not_gonna_release_in_hours_still/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T07:29:02+00:00</published>
  </entry>
</feed>
