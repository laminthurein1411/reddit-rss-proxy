<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-13T12:26:25+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1inch7r</id>
    <title>A new paper demonstrates that LLMs could "think" in latent space, effectively decoupling internal reasoning from visible context tokens. This breakthrough suggests that even smaller models can achieve remarkable performance without relying on extensive context windows.</title>
    <updated>2025-02-11T23:14:51+00:00</updated>
    <author>
      <name>/u/tehbangere</name>
      <uri>https://old.reddit.com/user/tehbangere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/"&gt; &lt;img alt="A new paper demonstrates that LLMs could &amp;quot;think&amp;quot; in latent space, effectively decoupling internal reasoning from visible context tokens. This breakthrough suggests that even smaller models can achieve remarkable performance without relying on extensive context windows." src="https://external-preview.redd.it/lsXw1VKNR0EoTFYgDUro5o8By4n9gHC7i_cxDktIeuo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7f243d34bc596be68af0031b70b22b21c475830" title="A new paper demonstrates that LLMs could &amp;quot;think&amp;quot; in latent space, effectively decoupling internal reasoning from visible context tokens. This breakthrough suggests that even smaller models can achieve remarkable performance without relying on extensive context windows." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tehbangere"&gt; /u/tehbangere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2502.05171"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T23:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1inmkbc</id>
    <title>agentica-org/DeepScaleR-1.5B-Preview</title>
    <updated>2025-02-12T08:39:47+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inmkbc/agenticaorgdeepscaler15bpreview/"&gt; &lt;img alt="agentica-org/DeepScaleR-1.5B-Preview" src="https://preview.redd.it/3fm88arb7oie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=094bb1f83ed48f6b26b3ca5b52f7cdfb742b34e0" title="agentica-org/DeepScaleR-1.5B-Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3fm88arb7oie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inmkbc/agenticaorgdeepscaler15bpreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inmkbc/agenticaorgdeepscaler15bpreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T08:39:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioh11i</id>
    <title>Best llm app for android?</title>
    <updated>2025-02-13T11:29:26+00:00</updated>
    <author>
      <name>/u/S-m-a-r-t-y</name>
      <uri>https://old.reddit.com/user/S-m-a-r-t-y</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did a bit of research and got stuck in between ChatterUI and PocketPal. Need some inputs or other better recommendations.&lt;/p&gt; &lt;p&gt;Cheers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/S-m-a-r-t-y"&gt; /u/S-m-a-r-t-y &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioh11i/best_llm_app_for_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioh11i/best_llm_app_for_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioh11i/best_llm_app_for_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T11:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iohaj5</id>
    <title>Correct my prompts for text summaries (Llama, but suggestions are welcome)</title>
    <updated>2025-02-13T11:46:44+00:00</updated>
    <author>
      <name>/u/brian-the-porpoise</name>
      <uri>https://old.reddit.com/user/brian-the-porpoise</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to build a little handy assistant that summarizes my digital notes for me. &lt;/p&gt; &lt;p&gt;I am using llama.cpp with llama-3.2-3b-instruct-q8_0. &lt;/p&gt; &lt;p&gt;No matter what I try as prompts, the result is not adhering to the requirements I set.&lt;br /&gt; E.g. I want it to narrate from the first person perspective, but it never does it. I specifically asked it to refer to me as &amp;quot;you&amp;quot;, doesnt do it either. &lt;/p&gt; &lt;p&gt;I have found that after going back and forth with it, it will eventually do as I require, but I would need a one-shot prompt so I can automate the whole thing. &lt;/p&gt; &lt;p&gt;The core issue is that it does not narrate from the desired POV, and continues to refer to the subject as &amp;quot;the speaker&amp;quot;, and that it keeps summarizing in bullet points. &lt;/p&gt; &lt;p&gt;Here are things I have tried: &lt;/p&gt; &lt;p&gt;&lt;code&gt;the following is a diary entry written by me. summarize it in 200 words. No bullet points. First person narration:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Assume you wrote the following text, but now need to make it more concise, 200 words or less.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Trying to assign a role, as suggested by other posts:&lt;/p&gt; &lt;p&gt;&lt;code&gt;You are an expert psychologist who needs to summarize your own diary entry in 200 words. This is the entry:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Turn the following text into a diary entry of max 200 words using &amp;quot;I&amp;quot; language/point of view:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Turn the following text into a diary entry narrated from YOUR perspective. Max 200 words:&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brian-the-porpoise"&gt; /u/brian-the-porpoise &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohaj5/correct_my_prompts_for_text_summaries_llama_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohaj5/correct_my_prompts_for_text_summaries_llama_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iohaj5/correct_my_prompts_for_text_summaries_llama_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T11:46:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1inos01</id>
    <title>Some details on Project Digits from PNY presentation</title>
    <updated>2025-02-12T11:32:04+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inos01/some_details_on_project_digits_from_pny/"&gt; &lt;img alt="Some details on Project Digits from PNY presentation" src="https://b.thumbs.redditmedia.com/pzMMeiqVpng-Evo7PS_VS5BolvYAdkUVtZ-ex05okEA.jpg" title="Some details on Project Digits from PNY presentation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These are my meeting notes, unedited:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ Only 19 people attended the presentation?!!! Some left mid-way.. ‚Ä¢ Presentation by PNY DGX EMEA lead ‚Ä¢ PNY takes Nvidia DGX ecosystemto market ‚Ä¢ Memory is DDR5x, 128GB &amp;quot;initially&amp;quot; ‚óã No comment on memory speed or bandwidth. ‚óã The memory is on the same fabric, connected to CPU and GPU. ‚óã &amp;quot;we don't have the specific bandwidth specification&amp;quot; ‚Ä¢ Also include a dual port QSFP networking, includes a Mellanox chip, supports infiniband and ethernet. Expetced at least 100gb/port, not yet confirmed by Nvidia. ‚Ä¢ Brand new ARM processor built for the Digits, never released before product (processor, not core). ‚Ä¢ Real product pictures, not rendering. ‚Ä¢ &amp;quot;what makes it special is the software stack&amp;quot; ‚Ä¢ Will run a Ubuntu based OS. Software stack shared with the rest of the nvidia ecosystem. ‚Ä¢ Digits is to be the first product of a new line within nvidia. ‚Ä¢ No dedicated power connector could be seen, USB-C powered? ‚óã &amp;quot;I would assume it is USB-C powered&amp;quot; ‚Ä¢ Nvidia indicated two maximum can be stacked. There is a possibility to cluster more. ‚óã The idea is to use it as a developer kit, not or production workloads. ‚Ä¢ &amp;quot;hopefully May timeframe to market&amp;quot;. ‚Ä¢ Cost: circa $3k RRP. Can be more depending on software features required, some will be paid. ‚Ä¢ &amp;quot;significantly more powerful than what we've seen on Jetson products&amp;quot; ‚óã &amp;quot;exponentially faster than Jetson&amp;quot; ‚óã &amp;quot;everything you can run on DGX, you can run on this, obviously slower&amp;quot; ‚óã Targeting universities and researchers. ‚Ä¢ &amp;quot;set expectations:&amp;quot; ‚óã It's a workstation ‚óã It can work standalone, or can be connected to another device to offload processing. ‚óã Not a replacement for a &amp;quot;full-fledged&amp;quot; multi-GPU workstation &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A few of us pushed on how the performance compares to a RTX 5090. No clear answer given beyond talking about 5090 not designed for enterprise workload, and power consumption&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1inos01"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inos01/some_details_on_project_digits_from_pny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inos01/some_details_on_project_digits_from_pny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T11:32:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioewvw</id>
    <title>WebRover 2.0 - AI Copilot for Browser Automation and Research Workflows</title>
    <updated>2025-02-13T08:53:08+00:00</updated>
    <author>
      <name>/u/Elegant_Fish_3822</name>
      <uri>https://old.reddit.com/user/Elegant_Fish_3822</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioewvw/webrover_20_ai_copilot_for_browser_automation_and/"&gt; &lt;img alt="WebRover 2.0 - AI Copilot for Browser Automation and Research Workflows" src="https://external-preview.redd.it/PMbSHk0WW6PoDIccKf_6k0rFhzH7cvXADJSNQbeOQeM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5df5a740fb78975f904e1d12f013d08df810dc2" title="WebRover 2.0 - AI Copilot for Browser Automation and Research Workflows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever wondered if AI could autonomously navigate the web to perform complex research tasks‚Äîtasks that might take you hours or even days‚Äîwithout stumbling over context limitations like existing large language models?&lt;/p&gt; &lt;p&gt;Introducing WebRover 2.0, an open-source web automation agent that efficiently orchestrates complex research tasks using Langchains's agentic framework, LangGraph, and retrieval-augmented generation (RAG) pipelines. Simply provide the agent with a topic, and watch as it takes control of your browser to conduct human-like research.&lt;/p&gt; &lt;p&gt;I welcome your feedback, suggestions, and contributions to enhance WebRover further. Let's collaborate to push the boundaries of autonomous AI agents! üöÄ&lt;/p&gt; &lt;p&gt;Explore the the project on Github : &lt;a href="https://github.com/hrithikkoduri/WebRover"&gt;https://github.com/hrithikkoduri/WebRover&lt;/a&gt;&lt;/p&gt; &lt;p&gt;[Curious to see it in action? üé• In the demo video below, I prompted the deep research agent to write a detailed report on AI systems in healthcare. It autonomously browses the web, opens links, reads through webpages, self-reflects, and infers to build a comprehensive report with references. Additionally, it also opens Google Docs and types down the entire report for you to use later.]&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ioewvw/video/jzfc8ncjevie1/player"&gt;https://reddit.com/link/1ioewvw/video/jzfc8ncjevie1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Elegant_Fish_3822"&gt; /u/Elegant_Fish_3822 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioewvw/webrover_20_ai_copilot_for_browser_automation_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioewvw/webrover_20_ai_copilot_for_browser_automation_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioewvw/webrover_20_ai_copilot_for_browser_automation_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T08:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1io1txa</id>
    <title>OpenAI's plans for GPT4.5/GPT-5 - ETA weeks / months according to Sam</title>
    <updated>2025-02-12T21:09:09+00:00</updated>
    <author>
      <name>/u/rajwanur</name>
      <uri>https://old.reddit.com/user/rajwanur</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io1txa/openais_plans_for_gpt45gpt5_eta_weeks_months/"&gt; &lt;img alt="OpenAI's plans for GPT4.5/GPT-5 - ETA weeks / months according to Sam" src="https://external-preview.redd.it/C21I1UZsCNPoAR2CpLpEnL-d9RF9Rx4gseKID9bem40.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92adc0a85147e7c6ef3687d2dd3114dd7c01753f" title="OpenAI's plans for GPT4.5/GPT-5 - ETA weeks / months according to Sam" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/2skvct3pwrie1.png?width=602&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4d2bdff65bcb8e941840064badc168abfdc6db9"&gt;https://preview.redd.it/2skvct3pwrie1.png?width=602&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4d2bdff65bcb8e941840064badc168abfdc6db9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3k86ernrwrie1.png?width=594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b1690aa3128a046ca97dda9c08b1a4f5df72cf2"&gt;https://preview.redd.it/3k86ernrwrie1.png?width=594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b1690aa3128a046ca97dda9c08b1a4f5df72cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Link to the post: &lt;a href="https://x.com/sama/status/1889755723078443244"&gt;https://x.com/sama/status/1889755723078443244&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rajwanur"&gt; /u/rajwanur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io1txa/openais_plans_for_gpt45gpt5_eta_weeks_months/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io1txa/openais_plans_for_gpt45gpt5_eta_weeks_months/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io1txa/openais_plans_for_gpt45gpt5_eta_weeks_months/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T21:09:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1io9urf</id>
    <title>Anyone repurpose gaming consoles ?</title>
    <updated>2025-02-13T03:22:53+00:00</updated>
    <author>
      <name>/u/Axelni98</name>
      <uri>https://old.reddit.com/user/Axelni98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With all the Nvidia GPUs selling like hotcakes. Has anyone bought the PS5 or the latest Xbox and used it ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Axelni98"&gt; /u/Axelni98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io9urf/anyone_repurpose_gaming_consoles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io9urf/anyone_repurpose_gaming_consoles/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io9urf/anyone_repurpose_gaming_consoles/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T03:22:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1io8qe0</id>
    <title>AceInstruct 1.5B / 7B / 72B by Nvidia</title>
    <updated>2025-02-13T02:24:12+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io8qe0/aceinstruct_15b_7b_72b_by_nvidia/"&gt; &lt;img alt="AceInstruct 1.5B / 7B / 72B by Nvidia" src="https://external-preview.redd.it/AW9WUUjiULOHbAfYY66Sx6D3OmGPFlGm47TagKzBqgo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94eb08024b4ddeaf5f136dca632fc922d506f5fb" title="AceInstruct 1.5B / 7B / 72B by Nvidia" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/nvidia/AceInstruct-1.5B"&gt;https://huggingface.co/nvidia/AceInstruct-1.5B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/AceInstruct-7B"&gt;https://huggingface.co/nvidia/AceInstruct-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/AceInstruct-72B"&gt;https://huggingface.co/nvidia/AceInstruct-72B&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We introduce AceInstruct, a family of advanced SFT models for coding, mathematics, and general-purpose tasks. The AceInstruct family, which includes AceInstruct-1.5B, 7B, and 72B, is &lt;strong&gt;Improved using Qwen&lt;/strong&gt;. These models are fine-tuned on Qwen2.5-Base using &lt;a href="https://huggingface.co/datasets/nvidia/AceMath-Instruct-Training-Data"&gt;general SFT datasets&lt;/a&gt;. These same datasets are also used in the training of &lt;a href="https://huggingface.co/nvidia/AceMath-72B-Instruct"&gt;AceMath-Instruct&lt;/a&gt;. Different from AceMath-Instruct which is specialized for math questions, AceInstruct is versatile and can be applied to a wide range of domains. Benchmark evaluations across coding, mathematics, and general knowledge tasks demonstrate that AceInstruct delivers performance comparable to Qwen2.5-Instruct.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5v30ob7mgtie1.png?width=708&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c419909e48136207192ee44705b79c037068d73"&gt;https://preview.redd.it/5v30ob7mgtie1.png?width=708&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c419909e48136207192ee44705b79c037068d73&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Bruh, from 1.5b to 7b and then straight up to 72b, it's the same disappointing release strategy as Meta Llama. I guess I'll keep using Qwen 2.5 32b until Qwen 3.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io8qe0/aceinstruct_15b_7b_72b_by_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io8qe0/aceinstruct_15b_7b_72b_by_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io8qe0/aceinstruct_15b_7b_72b_by_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T02:24:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iohbij</id>
    <title>I built a knowledge management system that enables you to connect knowledge to any RAG</title>
    <updated>2025-02-13T11:48:33+00:00</updated>
    <author>
      <name>/u/Outside-Project-1451</name>
      <uri>https://old.reddit.com/user/Outside-Project-1451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm excited to introduce Simba ‚Äì an open-source solution I developed to simplify managing and leveraging knowledge in Retrieval-Augmented Generation (RAG) systems.&lt;/p&gt; &lt;p&gt;In simple terms, Simba enables you to structure and connect a knowledge base (Word, PDF, PowerPoint documents, etc.) to any chatbot.&lt;/p&gt; &lt;p&gt;üîç Why Simba?&lt;/p&gt; &lt;p&gt;While working on AI projects, I frequently encountered challenges such as:&lt;/p&gt; &lt;p&gt;üìÇ Handling long, complex documents (including tables, images, multiple sections‚Ä¶)&lt;/p&gt; &lt;p&gt;üîé Indexing and structuring information for effective retrieval&lt;/p&gt; &lt;p&gt;üõ†Ô∏è Controlling the sources that a chatbot uses&lt;/p&gt; &lt;p&gt;Simba addresses these issues with:&lt;/p&gt; &lt;p&gt;‚úÖ Advanced parsing that automatically structures documents using state-of-the-art algorithms&lt;/p&gt; &lt;p&gt;‚úÖ An intuitive interface to visualize, modify, and organize data chunks&lt;/p&gt; &lt;p&gt;‚úÖ Precise knowledge control to include or exclude sources as needed&lt;/p&gt; &lt;p&gt;‚úÖ A flexible architecture allowing you to choose your LLMs, vector databases, chunking strategies, and parsers&lt;/p&gt; &lt;p&gt;üìå When to Use Simba?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For long and complex documents (tables, images, multiple sections‚Ä¶)&lt;/li&gt; &lt;li&gt;When you need granular control over which sources are included during conversations&lt;/li&gt; &lt;li&gt;When managing data access is critical (permissions and roles ‚Äì a feature coming soon)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üéØ Who Is Simba For?&lt;/p&gt; &lt;p&gt;Simba is crafted for developers aiming to integrate a structured knowledge base into their RAG systems.&lt;/p&gt; &lt;p&gt;üõ†Ô∏è Although the project is still evolving and doesn‚Äôt yet cover every planned feature, it‚Äôs on track to become a powerful tool for the community.&lt;/p&gt; &lt;p&gt;üí° Feedback Is a Gift!&lt;/p&gt; &lt;p&gt;The magic of open source lies in collaboration. If you encounter bugs, unclear areas, or simply have suggestions, please share your feedback. You can propose improvements, bug fixes, or new features directly on GitHub.&lt;/p&gt; &lt;p&gt;Check out the repository here: &lt;a href="https://github.com/GitHamza0206/simba"&gt;https://github.com/GitHamza0206/simba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚≠ê Simba is nearing 100 stars on GitHub, and the goal is to reach 1000 stars within the next 2 months! If you appreciate the project, please give it a star ‚≠ê ‚Äì your support means a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outside-Project-1451"&gt; /u/Outside-Project-1451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohbij/i_built_a_knowledge_management_system_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohbij/i_built_a_knowledge_management_system_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iohbij/i_built_a_knowledge_management_system_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T11:48:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1io2vq5</id>
    <title>Promptable object tracking robots with Moondream VLM &amp; OpenCV Optical Flow (open source)</title>
    <updated>2025-02-12T21:53:10+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2vq5/promptable_object_tracking_robots_with_moondream/"&gt; &lt;img alt="Promptable object tracking robots with Moondream VLM &amp;amp; OpenCV Optical Flow (open source)" src="https://external-preview.redd.it/N2xjdjR4MG80c2llMTEDy-zmwY-2zxEHn6L-Fnq1X838PMp4mnmxIFCi0bu_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35f82c0e1ecceb7465617120ea97715cdb5a48e9" title="Promptable object tracking robots with Moondream VLM &amp;amp; OpenCV Optical Flow (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/z5buym0o4sie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2vq5/promptable_object_tracking_robots_with_moondream/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io2vq5/promptable_object_tracking_robots_with_moondream/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T21:53:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1iofdch</id>
    <title>VRAM Requirements for Training a 70B Model with GRPO &amp; ZeRO-3?</title>
    <updated>2025-02-13T09:28:43+00:00</updated>
    <author>
      <name>/u/thanhdouwu</name>
      <uri>https://old.reddit.com/user/thanhdouwu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to estimate VRAM usage when training a 70B parameter model with GRPO. If I use ZeRO-3, set the context length to 8k or 16k, and use a rule-based reward model, how much VRAM would I need?&lt;/p&gt; &lt;p&gt;Additionally, if you've trained with LoRA or different model sizes, I'd love to hear about your experience‚ÄîVRAM consumption, setup details, and any optimizations you found helpful. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thanhdouwu"&gt; /u/thanhdouwu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iofdch/vram_requirements_for_training_a_70b_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iofdch/vram_requirements_for_training_a_70b_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iofdch/vram_requirements_for_training_a_70b_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T09:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1iohk4o</id>
    <title>Let's build DeepSeek from Scratch | Taught by MIT PhD graduate</title>
    <updated>2025-02-13T12:03:45+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"&gt; &lt;img alt="Let's build DeepSeek from Scratch | Taught by MIT PhD graduate" src="https://external-preview.redd.it/pAa68GpmjnpZeahm_YMGQkYTs9KtW9HemhGbAYHU02s.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=555355166a247eb92939344c89b96ed48dd7655a" title="Let's build DeepSeek from Scratch | Taught by MIT PhD graduate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/vjwhw6ticwie1.gif"&gt;https://i.redd.it/vjwhw6ticwie1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Join us for the 6pm Youtube premier here: &lt;a href="https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ"&gt;https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ever since DeepSeek was launched, everyone is focused on: &lt;/p&gt; &lt;p&gt;- Flashy headlines&lt;/p&gt; &lt;p&gt;- Company wars&lt;/p&gt; &lt;p&gt;- Building LLM applications powered by DeepSeek&lt;/p&gt; &lt;p&gt;I very strongly think that students, researchers, engineers and working professionals should focus on the foundations. &lt;/p&gt; &lt;p&gt;The real question we should ask ourselves is: &lt;/p&gt; &lt;p&gt;‚ÄúCan I build the DeepSeek architecture and model myself, from scratch?‚Äù&lt;/p&gt; &lt;p&gt;If you ask this question, you will discover that to make DeepSeek work, there are a number of key ingredients which play a role:&lt;/p&gt; &lt;p&gt;(1) Mixture of Experts (MoE)&lt;/p&gt; &lt;p&gt;(2) Multi-head Latent Attention (MLA)&lt;/p&gt; &lt;p&gt;(3) Rotary Positional Encodings (RoPE)&lt;/p&gt; &lt;p&gt;(4) Multi-token prediction (MTP)&lt;/p&gt; &lt;p&gt;(5) Supervised Fine-Tuning (SFT)&lt;/p&gt; &lt;p&gt;(6) Group Relative Policy Optimisation (GRPO)&lt;/p&gt; &lt;p&gt;My aim with the ‚ÄúBuild DeepSeek from Scratch‚Äù playlist is: &lt;/p&gt; &lt;p&gt;- To teach you the mathematical foundations behind all the 6 ingredients above.&lt;/p&gt; &lt;p&gt;- To code all 6 ingredients above, from scratch.&lt;/p&gt; &lt;p&gt;- To assemble these ingredients and to run a ‚Äúmini Deep-Seek‚Äù on your own.&lt;/p&gt; &lt;p&gt;After this, you will among the top 0.1%. of ML/LLM engineers who can build DeepSeek ingredients on their own.&lt;/p&gt; &lt;p&gt;This playlist won‚Äôt be a 1 hour or 2 hour video. This will be a mega playlist of 35-40 videos with a duration of 40+ hours. &lt;/p&gt; &lt;p&gt;It will be in-depth. No fluff. Solid content. &lt;/p&gt; &lt;p&gt;Join us for the 6pm premier here: &lt;a href="https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ"&gt;https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S: Attached is a small GIF showing the notes we have made. This is just 5-10% of the total amount of notes and material we have prepared for this series!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T12:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1inoui5</id>
    <title>AMD reportedly working on gaming Radeon RX 9070 XT GPU with 32GB memory</title>
    <updated>2025-02-12T11:36:29+00:00</updated>
    <author>
      <name>/u/noiserr</name>
      <uri>https://old.reddit.com/user/noiserr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoui5/amd_reportedly_working_on_gaming_radeon_rx_9070/"&gt; &lt;img alt="AMD reportedly working on gaming Radeon RX 9070 XT GPU with 32GB memory" src="https://external-preview.redd.it/qxSKCWeduksNqEDRWvwQaww7R41JuTdE_uY1z8NDX_M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b97591e394a959b1d54b453c3148692e6cab6ca" title="AMD reportedly working on gaming Radeon RX 9070 XT GPU with 32GB memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noiserr"&gt; /u/noiserr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/amd-reportedly-working-on-gaming-radeon-rx-9000-gpu-with-32gb-memory"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoui5/amd_reportedly_working_on_gaming_radeon_rx_9070/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inoui5/amd_reportedly_working_on_gaming_radeon_rx_9070/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T11:36:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1io4x5c</id>
    <title>OpenThinker-32B &amp; 7B</title>
    <updated>2025-02-12T23:21:11+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/open-thoughts/OpenThinker-32B"&gt;https://huggingface.co/open-thoughts/OpenThinker-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/open-thoughts/OpenThinker-7B"&gt;https://huggingface.co/open-thoughts/OpenThinker-7B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io4x5c/openthinker32b_7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io4x5c/openthinker32b_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io4x5c/openthinker32b_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T23:21:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1io811j</id>
    <title>Who builds PCs that can handle 70B local LLMs?</title>
    <updated>2025-02-13T01:48:52+00:00</updated>
    <author>
      <name>/u/Moist-Mongoose4467</name>
      <uri>https://old.reddit.com/user/Moist-Mongoose4467</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are only a few videos on YouTube that show folks buying old server hardware and cobbling together affordable PCs with a bunch of cores, RAM, and CPU RAM. Is there a company or person that does that for a living (or side hustle)? I don't have $10,000 to $50,000 for a home server with multiple high-end GPUs. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moist-Mongoose4467"&gt; /u/Moist-Mongoose4467 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io811j/who_builds_pcs_that_can_handle_70b_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io811j/who_builds_pcs_that_can_handle_70b_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io811j/who_builds_pcs_that_can_handle_70b_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T01:48:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iofe4w</id>
    <title>[update] aiaio: simple, lightweight ui with more features now</title>
    <updated>2025-02-13T09:30:25+00:00</updated>
    <author>
      <name>/u/abhi1thakur</name>
      <uri>https://old.reddit.com/user/abhi1thakur</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iofe4w/update_aiaio_simple_lightweight_ui_with_more/"&gt; &lt;img alt="[update] aiaio: simple, lightweight ui with more features now" src="https://external-preview.redd.it/eWJ1dWZtaTlsdmllMTTMNvywGLfHKtiMdeeDDuKKJ-xtwCq_lpvrE6nUhuq6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f5f1683ed07dbea3c137ac3ecb29bfaf68079ce" title="[update] aiaio: simple, lightweight ui with more features now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abhi1thakur"&gt; /u/abhi1thakur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1bduxmi9lvie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iofe4w/update_aiaio_simple_lightweight_ui_with_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iofe4w/update_aiaio_simple_lightweight_ui_with_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T09:30:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1io655d</id>
    <title>The endgame of Tool-Use, toolmaking</title>
    <updated>2025-02-13T00:17:33+00:00</updated>
    <author>
      <name>/u/fractalcrust</name>
      <uri>https://old.reddit.com/user/fractalcrust</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Stop making bespoke tools for every usecase. What we need is a tool-making tool, enabling LLMs to create their own tools to solve their tasks. Nothing could possibly go wrong and I'm 100% comfortable leaving my LLM unsupervised &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fractalcrust"&gt; /u/fractalcrust &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io655d/the_endgame_of_tooluse_toolmaking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io655d/the_endgame_of_tooluse_toolmaking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io655d/the_endgame_of_tooluse_toolmaking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T00:17:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1io9lfc</id>
    <title>DeepSeek Distilled Qwen 1.5B on NPU for Windows on Snapdragon</title>
    <updated>2025-02-13T03:09:14+00:00</updated>
    <author>
      <name>/u/SkyFeistyLlama8</name>
      <uri>https://old.reddit.com/user/SkyFeistyLlama8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft just released a Qwen 1.5B DeepSeek Distilled local model that targets the Hexagon NPU on Snapdragon X Plus/Elite laptops. Finally, we have an LLM that officially runs on the NPU for prompt eval (inference runs on CPU). &lt;/p&gt; &lt;p&gt;To run it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;run VS Code under Windows on ARM&lt;/li&gt; &lt;li&gt;download the AI Toolkit extension&lt;/li&gt; &lt;li&gt;Ctrl-Shift-P to load the command palette, type &amp;quot;Load Model Catalog&amp;quot;&lt;/li&gt; &lt;li&gt;scroll down to the DeepSeek (NPU Optimized) card, click +Add. The extension then downloads a bunch of ONNX files.&lt;/li&gt; &lt;li&gt;to run inference, Ctrl-Shift-P to load the command palette, then type &amp;quot;Focus on my models view&amp;quot; to load, then have fun in the chat playground&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Task Manager shows NPU usage at 50% and CPU at 25% during inference so it's working as intended. Larger Qwen and Llama models are coming so we finally have multiple performant inference stacks on Snapdragon.&lt;/p&gt; &lt;p&gt;The actual executable is in the &amp;quot;ai-studio&amp;quot; directory under VS Code's extensions directory. There's an ONNX runtime .exe along with a bunch of QnnHtp DLLs. It might be interesting to code up a PowerShell workflow for this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SkyFeistyLlama8"&gt; /u/SkyFeistyLlama8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io9lfc/deepseek_distilled_qwen_15b_on_npu_for_windows_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io9lfc/deepseek_distilled_qwen_15b_on_npu_for_windows_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io9lfc/deepseek_distilled_qwen_15b_on_npu_for_windows_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T03:09:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioclzg</id>
    <title>InternVideo2.5 releasedÔºÅHas anyone tried it out? How well does it perform?</title>
    <updated>2025-02-13T06:04:38+00:00</updated>
    <author>
      <name>/u/vansinhu</name>
      <uri>https://old.reddit.com/user/vansinhu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioclzg/internvideo25_releasedhas_anyone_tried_it_out_how/"&gt; &lt;img alt="InternVideo2.5 releasedÔºÅHas anyone tried it out? How well does it perform?" src="https://external-preview.redd.it/Y7gp2ezADJTiI3oUU3P5TMgIEsAjig-29MVwWQpiG_c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de07f828e6653fd6667798fac5614b252c311381" title="InternVideo2.5 releasedÔºÅHas anyone tried it out? How well does it perform?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/47l93seekuie1.png?width=592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5b0206424f31cc8412405cdc02e780ce5763b9b"&gt;https://preview.redd.it/47l93seekuie1.png?width=592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5b0206424f31cc8412405cdc02e780ce5763b9b&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Handles videos 6x longer than predecessors&lt;/li&gt; &lt;li&gt;Pinpoints objects/actions with surgical precision&lt;/li&gt; &lt;li&gt;Trained on 300K+ hours of diverse video data&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Outperforms SOTA on multiple benchmarks &amp;amp; unlocks possibilities for Autonomous Driving, VR, and more!Code: &lt;a href="https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5"&gt;https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2501.12386"&gt;https://arxiv.org/abs/2501.12386&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://huggingface.co/OpenGVLab/InternVideo2_5_Chat_8B"&gt;https://huggingface.co/OpenGVLab/InternVideo2_5_Chat_8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xieqwfmhkuie1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c9e39c7d538478ba3387fa5077d06c0017df073"&gt;https://preview.redd.it/xieqwfmhkuie1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c9e39c7d538478ba3387fa5077d06c0017df073&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vansinhu"&gt; /u/vansinhu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioclzg/internvideo25_releasedhas_anyone_tried_it_out_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioclzg/internvideo25_releasedhas_anyone_tried_it_out_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioclzg/internvideo25_releasedhas_anyone_tried_it_out_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T06:04:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1iof0r2</id>
    <title>When it comes to fine-tuning LLMs, the training dataset isn‚Äôt just a factor‚Äîit‚Äôs the kingmaker.</title>
    <updated>2025-02-13T09:01:31+00:00</updated>
    <author>
      <name>/u/Excellent_Delay_3701</name>
      <uri>https://old.reddit.com/user/Excellent_Delay_3701</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let‚Äôs take a look at the current SOTA models‚ÄîLlama 3.x, DeepSeek, Mistral, and others. (Don't forget I am talking about fine-tune for specific tasks, not pre-train)&lt;/p&gt; &lt;p&gt;The real kingmaker for top performance? A meticulously cleaned, balanced, and well-structured dataset. Even if a ‚Äúperfect‚Äù dataset doesn‚Äôt exist, getting as close as possible makes all the difference.&lt;/p&gt; &lt;p&gt;Sure, training variables and hyperparameters impact an LLM‚Äôs performance. But in the end, isn‚Äôt the dataset everything?&lt;/p&gt; &lt;p&gt;If you‚Äôre fine-tuning an LLM or SLM for a specific task and not seeing the results you want after a few iterations, the first place you should look is the dataset. &lt;/p&gt; &lt;p&gt;How many of you changes model architectures, apply something new?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Delay_3701"&gt; /u/Excellent_Delay_3701 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iof0r2/when_it_comes_to_finetuning_llms_the_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iof0r2/when_it_comes_to_finetuning_llms_the_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iof0r2/when_it_comes_to_finetuning_llms_the_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T09:01:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1io4s4s</id>
    <title>This paper might be a breakthrough Google doesn't know they have</title>
    <updated>2025-02-12T23:14:52+00:00</updated>
    <author>
      <name>/u/Ok-Possibility-5586</name>
      <uri>https://old.reddit.com/user/Ok-Possibility-5586</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2105.03824"&gt;2105.03824&lt;/a&gt;&lt;/p&gt; &lt;p&gt;FNet: Mixing Tokens with Fourier Transforms&lt;/p&gt; &lt;p&gt;^^^ this paper is from 2022 before LLMs blew up in the public imagination.&lt;/p&gt; &lt;p&gt;If someone is able to replicate this, maybe by training a smaller model and cutting out the layers and splicing into a bigger model (or something else, I'm winging it here) then maybe we get some big speedups. According to the paper (from Google) it's looking at a 90% speedup and memory reduction.&lt;/p&gt; &lt;p&gt;&lt;a href="/u/danielhanchen"&gt;u/danielhanchen&lt;/a&gt; have you seen this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Possibility-5586"&gt; /u/Ok-Possibility-5586 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io4s4s/this_paper_might_be_a_breakthrough_google_doesnt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io4s4s/this_paper_might_be_a_breakthrough_google_doesnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io4s4s/this_paper_might_be_a_breakthrough_google_doesnt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T23:14:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1io3hn2</id>
    <title>NoLiMa: Long-Context Evaluation Beyond Literal Matching - Finally a good benchmark that shows just how bad LLM performance is at long context. Massive drop at just 32k context for all models.</title>
    <updated>2025-02-12T22:19:00+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io3hn2/nolima_longcontext_evaluation_beyond_literal/"&gt; &lt;img alt="NoLiMa: Long-Context Evaluation Beyond Literal Matching - Finally a good benchmark that shows just how bad LLM performance is at long context. Massive drop at just 32k context for all models." src="https://preview.redd.it/95ysyjzs8sie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=846630231480ed6a71d97aeaed4938ab9b5cc355" title="NoLiMa: Long-Context Evaluation Beyond Literal Matching - Finally a good benchmark that shows just how bad LLM performance is at long context. Massive drop at just 32k context for all models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/95ysyjzs8sie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io3hn2/nolima_longcontext_evaluation_beyond_literal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io3hn2/nolima_longcontext_evaluation_beyond_literal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T22:19:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1io5o9a</id>
    <title>How do LLMs actually do this?</title>
    <updated>2025-02-12T23:56:05+00:00</updated>
    <author>
      <name>/u/No-Conference-8133</name>
      <uri>https://old.reddit.com/user/No-Conference-8133</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io5o9a/how_do_llms_actually_do_this/"&gt; &lt;img alt="How do LLMs actually do this?" src="https://preview.redd.it/m6rfcv5tqsie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97e3e2e816211a62c38e8c3c60368cca7c8d38d4" title="How do LLMs actually do this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The LLM can‚Äôt actually see or look close. It can‚Äôt zoom in the picture and count the fingers carefully or slower.&lt;/p&gt; &lt;p&gt;My guess is that when I say &amp;quot;look very close&amp;quot; it just adds a finger and assumes a different answer. Because LLMs are all about matching patterns. When I tell someone to look very close, the answer usually changes.&lt;/p&gt; &lt;p&gt;Is this accurate or am I totally off?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Conference-8133"&gt; /u/No-Conference-8133 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m6rfcv5tqsie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io5o9a/how_do_llms_actually_do_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io5o9a/how_do_llms_actually_do_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T23:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1io2ija</id>
    <title>Is Mistral's Le Chat truly the FASTEST?</title>
    <updated>2025-02-12T21:37:41+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2ija/is_mistrals_le_chat_truly_the_fastest/"&gt; &lt;img alt="Is Mistral's Le Chat truly the FASTEST?" src="https://preview.redd.it/zk2uyy142sie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abb4eab5a990f54584b5bb28366386e39bb58419" title="Is Mistral's Le Chat truly the FASTEST?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zk2uyy142sie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2ija/is_mistrals_le_chat_truly_the_fastest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io2ija/is_mistrals_le_chat_truly_the_fastest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T21:37:41+00:00</published>
  </entry>
</feed>
