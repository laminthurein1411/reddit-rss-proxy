<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-24T18:07:56+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ix5ofd</id>
    <title>Has anyone ran the 1.58 and 2.51bit quants of DeepSeek R1 using KTransformers?</title>
    <updated>2025-02-24T16:11:14+00:00</updated>
    <author>
      <name>/u/tim1234525</name>
      <uri>https://old.reddit.com/user/tim1234525</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Also is there any data of comparisons of the pp and tg using different CPUs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tim1234525"&gt; /u/tim1234525 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5ofd/has_anyone_ran_the_158_and_251bit_quants_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5ofd/has_anyone_ran_the_158_and_251bit_quants_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5ofd/has_anyone_ran_the_158_and_251bit_quants_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T16:11:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix2ey5</id>
    <title>What about combining two RTX 4060 TI with 16 GB VRAM (each)?</title>
    <updated>2025-02-24T13:47:51+00:00</updated>
    <author>
      <name>/u/Repsol_Honda_PL</name>
      <uri>https://old.reddit.com/user/Repsol_Honda_PL</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you think about combining two RTX 4060TI cards with 16 GB VRAM each, together I would get a memory the size of one RTX 5090, which is quite decent. I already have one 4060 TI (Gigabyte Gaming OC arrived today) and I'm slowly thinking about the second one - good direction?&lt;/p&gt; &lt;p&gt;The other option is to stay with one card and in, say, half a year when the GPU market stabilizes (if it happens at all ;) ) I would swap the 4060 Ti for the 5090.&lt;/p&gt; &lt;p&gt;For simple work on small models with unsloth 16 GB should be enough, but it is also tempting to expand the memory.&lt;/p&gt; &lt;p&gt;Another thing, does the CPU (number of cores), RAM (frequency) and SSD performance matter very much here - or does it not matter much? (I know that sometimes some calculations are delegated to the CPU, not everything can be computed on the GPU).&lt;/p&gt; &lt;p&gt;I am on &lt;strong&gt;AMD AM4 platform.&lt;/strong&gt; But might upgrade to AM5 with 7900 if it is recommended.&lt;/p&gt; &lt;p&gt;Thank you for the hints!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Repsol_Honda_PL"&gt; /u/Repsol_Honda_PL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix2ey5/what_about_combining_two_rtx_4060_ti_with_16_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix2ey5/what_about_combining_two_rtx_4060_ti_with_16_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix2ey5/what_about_combining_two_rtx_4060_ti_with_16_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T13:47:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix40jp</id>
    <title>Best local (reliable) LLM/RAG agent on a MacBook M3pro 18GB RAM?</title>
    <updated>2025-02-24T15:00:48+00:00</updated>
    <author>
      <name>/u/Onetimehelper</name>
      <uri>https://old.reddit.com/user/Onetimehelper</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have local sensitive documents that I want summarized locally, any best fits? Would like something simple, the document would probably be one time use only (other programs I would use would keep the documents in a database and refer back to them, messing up summaries for specific documents). I also have another PC with a 7900XTX, but obviously not as portable for work. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Onetimehelper"&gt; /u/Onetimehelper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix40jp/best_local_reliable_llmrag_agent_on_a_macbook/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix40jp/best_local_reliable_llmrag_agent_on_a_macbook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix40jp/best_local_reliable_llmrag_agent_on_a_macbook/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T15:00:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwwt1i</id>
    <title>Vulkan oddness with llama.cpp and how to get best tokens/second with my setup</title>
    <updated>2025-02-24T07:50:30+00:00</updated>
    <author>
      <name>/u/billblake2018</name>
      <uri>https://old.reddit.com/user/billblake2018</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was trying to decide if using the Intel Graphics for its GPU would be worthwhile. My machine is an HP ProBook with 32G running FreeBSD 14.1. When llama-bench is run with Vulkan, it says:&lt;/p&gt; &lt;p&gt;ggml_vulkan: 0 = Intel(R) UHD Graphics 620 (WHL GT2) (Intel open-source Mesa driver) | uma: 1 | fp16: 1 | warp size: 32 | shared memory: 65536 | matrix cores: none&lt;/p&gt; &lt;p&gt;Results from earlier versions of llama.cpp were inconsistent and confusing, including various abort()s from llama.cpp after a certain number of layers in the GPU had been specified. I grabbed b4762, compiled it, and had a go. The model I'm using is llama 3B Q8_0, says llama-bench. I ran with 7 threads, as that was a bit faster than running with 8, the system number. (Later results suggest that, if I'm using Vulkan, a smaller number of threads work as well, but I'll ignore that for this post.)&lt;/p&gt; &lt;p&gt;The first oddity is that llama.cpp compiled without Vulkan support is faster than llama.cpp compiled with Vulkan support and -ngl 0 (all numbers are token/second).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vulkan pp512 tg128&lt;/li&gt; &lt;li&gt;w/o 20.30 7.06&lt;/li&gt; &lt;li&gt;with 17.76 6.45&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The next oddity is that, as I increased -ngl, the pp512 numbers stayed more or less constant until around 15 layers, when they started increasing, ending up about 40% larger than -ngl 0. By contrast, the tg128 numbers &lt;em&gt;decreased&lt;/em&gt; to about 40% of the -ngl 0 value. Here's some of the results (these are with -r 1, since I was only interested in the general trend):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ngl pp512 tg128&lt;/li&gt; &lt;li&gt; 1 18.07 6.52&lt;/li&gt; &lt;li&gt;23 20.39 2.80&lt;/li&gt; &lt;li&gt;28 25.43 2.68&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If I understand this correctly, I get faster prompt processing the more layers I offload to the GPU but slower token generation the more layers I offload to the GPU.&lt;/p&gt; &lt;p&gt;My first question is, is that the correct interpretation? My second question is, how might I tune or hack llama.cpp so that I get that high tg128 figure that I got with no Vulkan support but also that high pp512 figure that I got with offloading all layers to the GPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/billblake2018"&gt; /u/billblake2018 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwwt1i/vulkan_oddness_with_llamacpp_and_how_to_get_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwwt1i/vulkan_oddness_with_llamacpp_and_how_to_get_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwwt1i/vulkan_oddness_with_llamacpp_and_how_to_get_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T07:50:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix3cxy</id>
    <title>I updated my personal open source Chat UI to support reasoning models.</title>
    <updated>2025-02-24T14:31:27+00:00</updated>
    <author>
      <name>/u/DivineAscension</name>
      <uri>https://old.reddit.com/user/DivineAscension</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3cxy/i_updated_my_personal_open_source_chat_ui_to/"&gt; &lt;img alt="I updated my personal open source Chat UI to support reasoning models." src="https://external-preview.redd.it/hrIhTomEzBpvg6FqJVsML_mUgvpVbATgJKk-LAnZKxE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97f0ff1022e7b1bac913b25515c4392fafa4d943" title="I updated my personal open source Chat UI to support reasoning models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Toy-97/Chat-WebUI"&gt;Here is the link to the open source repos&lt;/a&gt;. I've posted about my personal Chat UI before, and now I've updated it to support reasoning models. I use this personally because this has built-in tools to summarize YouTube videos and perform online web searches. There have been tons of improvements made too, so this version should be extremely stable. I hope you guys find it useful!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DivineAscension"&gt; /u/DivineAscension &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3cxy/i_updated_my_personal_open_source_chat_ui_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3cxy/i_updated_my_personal_open_source_chat_ui_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3cxy/i_updated_my_personal_open_source_chat_ui_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T14:31:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwolln</id>
    <title>Fine tune your own LLM for any GitHub repository â€“ Introducing KoloLLM</title>
    <updated>2025-02-24T00:07:00+00:00</updated>
    <author>
      <name>/u/Maxwell10206</name>
      <uri>https://old.reddit.com/user/Maxwell10206</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am releasing &lt;strong&gt;KoloLLM&lt;/strong&gt; today! It is a fine tuned &lt;strong&gt;8B Llama 3.1&lt;/strong&gt; model that you can download from Ollama. I trained it using approx. &lt;strong&gt;10,000 synthetically generated Q&amp;amp;A prompts&lt;/strong&gt; based on the &lt;strong&gt;Kolo GitHub repository&lt;/strong&gt;, so you can ask it anything about the repo, and itâ€™ll do its best to answer.&lt;/p&gt; &lt;p&gt;ðŸ”¹ &lt;strong&gt;Download the model from Ollama:&lt;/strong&gt; &lt;a href="https://ollama.com/MaxHastings/KoloLLM"&gt;KoloLLM&lt;/a&gt;&lt;br /&gt; ðŸ”¹ &lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/MaxHastings/Kolo"&gt;Kolo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can use Kolo to help you synthetically generate training data and fine tune your own LLM to be an expert for any GitHub repository!&lt;/p&gt; &lt;p&gt;Please share your thoughts and feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxwell10206"&gt; /u/Maxwell10206 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwolln/fine_tune_your_own_llm_for_any_github_repository/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwolln/fine_tune_your_own_llm_for_any_github_repository/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwolln/fine_tune_your_own_llm_for_any_github_repository/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T00:07:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix5rj9</id>
    <title>Dockerfile for running Unsloth GGUF Deepseek R1 quants on 4xL40S</title>
    <updated>2025-02-24T16:14:53+00:00</updated>
    <author>
      <name>/u/tempNull</name>
      <uri>https://old.reddit.com/user/tempNull</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Works for &lt;strong&gt;g6e.12xlarge&lt;/strong&gt; instances and above with a &lt;strong&gt;context size of 5k&lt;/strong&gt; and single request throughput of &lt;strong&gt;25tok/seconds.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;--------Dockerfile ---------&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM ghcr.io/ggerganov/llama.cpp:full-cuda # Set environment variables ENV CUDA_VISIBLE_DEVICES=0,1,2,3 ENV GGML_CUDA_MAX_STREAMS=16 ENV GGML_CUDA_MMQ_Y=1 ENV HF_HUB_ENABLE_HF_TRANSFER=1 WORKDIR /app # Install dependencies RUN apt-get update &amp;amp;&amp;amp; \ apt-get install -y python3-pip &amp;amp;&amp;amp; \ pip3 install huggingface_hub hf-transfer # Copy and set permissions COPY entrypoint.sh . RUN chmod +x /app/entrypoint.sh EXPOSE 8080 ENTRYPOINT [&amp;quot;/app/entrypoint.sh&amp;quot;] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;-----------------------------entrypoint.sh--------------------------&lt;/p&gt; &lt;pre&gt;&lt;code&gt;#!/bin/bash set -e # Download model shards if missing if [ ! -d &amp;quot;/app/DeepSeek-R1-GGUF&amp;quot; ]; then echo &amp;quot;Downloading model...&amp;quot; python3 -c &amp;quot; from huggingface_hub import snapshot_download snapshot_download( repo_id='unsloth/DeepSeek-R1-GGUF', local_dir='DeepSeek-R1-GGUF', allow_patterns=['*UD-IQ1_S*'] )&amp;quot; fi echo &amp;quot;Downloading model finished. Now waiting to start the llama server with optimisations for one batch latency&amp;quot; # Start server with single-request optimizations ./llama-server \ --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf\ --host 0.0.0.0 \ --port 8080 \ --n-gpu-layers 62 \ --parallel 4 \ --ctx-size 5120 \ --mlock \ --threads 42 \ --tensor-split 1,1,1,1 \ --no-mmap \ --rope-freq-base 1000000 \ --rope-freq-scale 0.25 \ --metrics &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Originally posted here: &lt;a href="https://tensorfuse.io/docs/guides/integrations/llama_cpp"&gt;https://tensorfuse.io/docs/guides/integrations/llama_cpp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tempNull"&gt; /u/tempNull &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5rj9/dockerfile_for_running_unsloth_gguf_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5rj9/dockerfile_for_running_unsloth_gguf_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5rj9/dockerfile_for_running_unsloth_gguf_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T16:14:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwyv0c</id>
    <title>Creative Reasoning Assistants: An other Fine-Tuned LLMs for Storytelling</title>
    <updated>2025-02-24T10:17:23+00:00</updated>
    <author>
      <name>/u/molbal</name>
      <uri>https://old.reddit.com/user/molbal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR: I combined reasoning with creative writing. I like the outcome. Models on HF:&lt;/strong&gt; &lt;a href="https://huggingface.co/collections/molbal/creative-reasoning-assistant-67bb91ba4a1e1803da997c5f"&gt;&lt;strong&gt;https://huggingface.co/collections/molbal/creative-reasoning-assistant-67bb91ba4a1e1803da997c5f&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Abstract&lt;/h1&gt; &lt;p&gt;This post presents a methodology for fine-tuning large language models to improve context-aware story continuation by incorporating reasoning steps. The approach leverages publicly available books from the Project Gutenberg corpus, processes them into structured training data, and fine-tunes models like Qwen2.5 Instruct (7B and 32B) using a cost-effective pipeline (qLoRA). The resulting models demonstrate improved story continuation capabilities, generating a few sentences at a time while maintaining narrative coherence. The fine-tuned models are made available in GGUF format for accessibility and experimentation. This work is planned to be part of writer-assistant tools (to be developer and published later) and encourages community feedback for further refinement.&lt;/p&gt; &lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;While text continuation is literally the main purpose of LLMs, story continuation is still a challenging task, as it requires understanding narrative context, characters' motivations, and plot progression. While existing models can generate text, they often lack the ability to progress the story's flow just in the correct amount when continuing it, they often do nothing to progress to plot, or too much in a short amount of time. This post introduces a fine-tuning methodology that combines reasoning steps with story continuation, enabling models to better understand context and produce more coherent outputs. The approach is designed to be cost-effective, leveraging free and low-cost resources while only using public domain or synthetic training data.&lt;/p&gt; &lt;h1&gt;Methodology&lt;/h1&gt; &lt;h1&gt;1. Data Collection and Preprocessing&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Source Data:&lt;/strong&gt; Public domain books from the Project Gutenberg corpus, written before the advent of LLMs were used to make avoid contamination from modern AI-generated text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chunking:&lt;/strong&gt; Each book was split into chunks of ~100 sentences, where 80 sentences were used as context and the subsequent 20 sentences as the continuation target.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. Thought Process Generation&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Prompt Design:&lt;/strong&gt; Two prompt templates were used: &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Thought Process Template:&lt;/strong&gt; Encourages the model to reason about the story's flow, character motivations, and interactions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Continuation Template:&lt;/strong&gt; Combines the generated reasoning with the original continuation to create a structured training example. This becomes the final training data, which is built from 4 parts: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Static part:&lt;/strong&gt; System prompt and Task parts are fix.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context:&lt;/strong&gt; Context is the first 80 sentences of the chunk (Human-written data)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning:&lt;/strong&gt; Synthetic reasoning part, written DeepSeek v3 model on OpenRouter was used to generate thought processes for each chunk, because it follows instructions very well and it is cheap.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Response:&lt;/strong&gt; The last 20 sentences of the training data&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. Fine-Tuning&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Selection:&lt;/strong&gt; Qwen2.5 Instruct (7B and 32B) was chosen for fine-tuning due to its already strong performance and permissive licensing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Pipeline:&lt;/strong&gt; LoRA (Low-Rank Adaptation) training was performed on &lt;a href="http://Fireworks.ai"&gt;Fireworks.ai&lt;/a&gt;, as currently their new fine-tuning service is free.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Note:&lt;/strong&gt; Please note that GRPO (Used for reasoning models like DeepSeek R1) was not used for this experiment.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Model Deployment&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Quantization:&lt;/strong&gt; Fireworks' output are safetensor adapters, these were first converted to GGUF adapters, then merged into the base model. For the 7B variant, the adapter was merged into the F16 base model, then quantized into Q4, with the 32B model, the adapter was directly merged into Q4 base model. Conversion and merging was done with llama.cpp.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Distribution:&lt;/strong&gt; Models were uploaded to Ollama and Hugging Face for easy access and experimentation.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;p&gt;The fine-tuned models demonstrated improvements in story continuation tasks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Contextual Understanding:&lt;/strong&gt; The models effectively used reasoning steps to understand narrative context before generating continuations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Coherence:&lt;/strong&gt; Generated continuations were more coherent and aligned with the story's flow compared to baseline models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficiency:&lt;/strong&gt; The 7B model with 16k context fully offloads to my laptop's GPU (RTX 3080 8GB) and manages ~50 tokens/sec, which I am satisfied with.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Using the model&lt;/h1&gt; &lt;p&gt;I invite the community to try the fine-tuned models and provide feedback. The models are available on Ollama Hub (&lt;a href="https://ollama.com/molbal/cra-v1-7b"&gt;7B&lt;/a&gt;, &lt;a href="https://ollama.com/molbal/cra-v1-32b"&gt;32B&lt;/a&gt;) and Hugging Face (&lt;a href="https://huggingface.co/molbal/CRA-v1-7B"&gt;7B&lt;/a&gt;, &lt;a href="https://huggingface.co/molbal/CRA-v1-32B"&gt;32B&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;For best results, please keep the following prompt format. Do not omit the System part either.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;### System: You are a writerâ€™s assistant. ### Task: Understand how the story flows, what motivations the characters have and how they will interact with each other and the world as a step by step thought process before continuing the story. ### Context: {context} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The model will reliably respond in the following format&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;reasoning&amp;gt; Chain of thought. &amp;lt;/reasoning&amp;gt; &amp;lt;answer&amp;gt; Text completion &amp;lt;/answer&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Using the model with the following parameters work:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;num_ctx: 16384,&lt;/li&gt; &lt;li&gt;repeat_penalty: 1.05,&lt;/li&gt; &lt;li&gt;temperature: 0.7,&lt;/li&gt; &lt;li&gt;top_p: 0.8&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Scripts used during the pipeline are uploaded to GitHub: &lt;a href="https://github.com/molbal/creative-reasoning-assistant-v1"&gt;molbal/creative-reasoning-assistant-v1: Fine-Tuning LLMs for Context-Aware Story Continuation with Reasoning&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Examples&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/molbal/creative-reasoning-assistant-v1/blob/master/examples/example-1.md"&gt;Example 1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/molbal/creative-reasoning-assistant-v1/blob/master/examples/example-2.md"&gt;Example 2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/molbal/creative-reasoning-assistant-v1/blob/master/examples/example-3.md"&gt;Example 3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/molbal/creative-reasoning-assistant-v1/blob/master/examples/example-4.md"&gt;Example 4&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/molbal/creative-reasoning-assistant-v1/blob/master/examples/example-5.md"&gt;Example 5&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/molbal"&gt; /u/molbal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwyv0c/creative_reasoning_assistants_an_other_finetuned/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwyv0c/creative_reasoning_assistants_an_other_finetuned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwyv0c/creative_reasoning_assistants_an_other_finetuned/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T10:17:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix3xn4</id>
    <title>200 Combinatorial Identities and Theorems Dataset for LLM finetuning [Dataset]</title>
    <updated>2025-02-24T14:57:24+00:00</updated>
    <author>
      <name>/u/DataBaeBee</name>
      <uri>https://old.reddit.com/user/DataBaeBee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3xn4/200_combinatorial_identities_and_theorems_dataset/"&gt; &lt;img alt="200 Combinatorial Identities and Theorems Dataset for LLM finetuning [Dataset]" src="https://external-preview.redd.it/hWfudl7kX8SLOLHp3nzBGVBq-MkP2SBqtM5dmm1DsOk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a2aa6e2090df556a49e222ec8d0b21997e38277" title="200 Combinatorial Identities and Theorems Dataset for LLM finetuning [Dataset]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataBaeBee"&gt; /u/DataBaeBee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://leetarxiv.substack.com/p/oeic"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3xn4/200_combinatorial_identities_and_theorems_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3xn4/200_combinatorial_identities_and_theorems_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T14:57:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwn617</id>
    <title>Benchmarks are a lie, and I have some examples</title>
    <updated>2025-02-23T23:00:15+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This was talked about a lot, but the recent HuggingFace eval results still took me by surprise.&lt;/p&gt; &lt;p&gt;My favorite RP model- &lt;strong&gt;Midnight Miqu 1.5&lt;/strong&gt; got &lt;strong&gt;LOWER&lt;/strong&gt; benchmarks all across the board than my own &lt;strong&gt;Wingless_Imp_8B&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;As much as I'd like to say &amp;quot;Yeah guys, my 8B model &lt;strong&gt;outperforms the legendary Miqu&lt;/strong&gt;&amp;quot;, &lt;strong&gt;no&lt;/strong&gt;, it &lt;strong&gt;does not&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It's not even close. &lt;strong&gt;Midnight Miqu&lt;/strong&gt; (1.5) is orders of magnitude &lt;strong&gt;better than ANY 8B&lt;/strong&gt; model, it's not even remotely close.&lt;/p&gt; &lt;p&gt;Now, I know exactly what went into &lt;strong&gt;Wingless_Imp_8B&lt;/strong&gt;, and &lt;strong&gt;I did NOT benchmaxxed&lt;/strong&gt;, as I simply do not care for these things, I started doing the evals only recently, and solely because people asked for it. What I am saying is:&lt;/p&gt; &lt;p&gt;1) Wingless_Imp_8B high benchmarks results were &lt;strong&gt;NOT cooked&lt;/strong&gt; (not on purpose anyway)&lt;br /&gt; 2) Even despite it was not benchmaxxed, and the results are &amp;quot;&lt;strong&gt;organic&lt;/strong&gt;&amp;quot;, they &lt;strong&gt;still do not reflect actual smarts&lt;/strong&gt;&lt;br /&gt; 2) The high benchmarks are &lt;strong&gt;randomly high&lt;/strong&gt;, while in practice have &lt;strong&gt;ALMOST no correlation to actual &amp;quot;organic&amp;quot; smarts&lt;/strong&gt; vs ANY 70B model, especially midnight miqu&lt;/p&gt; &lt;p&gt;Now, this case above is sus in itself, but the following case should settle it once and for all, the case of &lt;strong&gt;Phi-Lthy&lt;/strong&gt; and &lt;strong&gt;Phi-Line_14B&lt;/strong&gt; (TL;DR 1 is lobotomized, the other is not, the lobotmized is better at following instructions):&lt;/p&gt; &lt;p&gt;I used the &lt;strong&gt;exact same dataset for both&lt;/strong&gt;, but for Phi-Lthy, &lt;strong&gt;I literally lobotomized it by yeeting 8 layers&lt;/strong&gt; out of its brain, &lt;strong&gt;yet&lt;/strong&gt; its IFeval is &lt;strong&gt;significantly higher than the unlobotomized model&lt;/strong&gt;. How does &lt;strong&gt;removing 8 layers&lt;/strong&gt; out of 40 make it follow instructions &lt;strong&gt;better&lt;/strong&gt;?&lt;/p&gt; &lt;p&gt;I believe we should have a serious discussion about whether benchmarks for LLMs even hold any weight anymore, because I am straight up doubting their accuracy to reflect model capabilities alltogether at this point. A model can be in practice almost orders of magnitude smarter than the rest, yet people will ignore it because of low benchmarks. There might be somewhere in hugging face a real SOTA model, yet we might just dismiss it due to mediocre benchmarks.&lt;/p&gt; &lt;p&gt;What if I told you last year that I have &lt;strong&gt;the best roleplay model in the world&lt;/strong&gt;, but when you'd look at its benchmarks, you would see that the &amp;quot;best roleplay model in the world, of 70B size, &lt;strong&gt;has worst benchmarks than a shitty 8B model&lt;/strong&gt;&amp;quot;, most would have called BS. &lt;/p&gt; &lt;p&gt;That model was &lt;strong&gt;Midnight Miqu&lt;/strong&gt; (1.5) 70B, and I still think it blows away many 'modern' models even today.&lt;/p&gt; &lt;p&gt;The unlobtomized Phi-4:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Phi-Line_14B"&gt;https://huggingface.co/SicariusSicariiStuff/Phi-Line_14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The lobtomized Phi-4:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Phi-lthy4"&gt;https://huggingface.co/SicariusSicariiStuff/Phi-lthy4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwn617/benchmarks_are_a_lie_and_i_have_some_examples/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwn617/benchmarks_are_a_lie_and_i_have_some_examples/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwn617/benchmarks_are_a_lie_and_i_have_some_examples/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T23:00:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix3lsw</id>
    <title>Tutorial: 100 Lines to Let Cursor AI Build Agents for You</title>
    <updated>2025-02-24T14:42:36+00:00</updated>
    <author>
      <name>/u/Willing-Site-8137</name>
      <uri>https://old.reddit.com/user/Willing-Site-8137</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3lsw/tutorial_100_lines_to_let_cursor_ai_build_agents/"&gt; &lt;img alt="Tutorial: 100 Lines to Let Cursor AI Build Agents for You" src="https://external-preview.redd.it/owYpiYxzMbFoIy2mEgwHQu4HBDMye8iivJkM1wZ1bsE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fff8bc2c79be0acb98d587fba0d423ac51045588" title="Tutorial: 100 Lines to Let Cursor AI Build Agents for You" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Willing-Site-8137"&gt; /u/Willing-Site-8137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=0Pv5HVoVBYE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3lsw/tutorial_100_lines_to_let_cursor_ai_build_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3lsw/tutorial_100_lines_to_let_cursor_ai_build_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T14:42:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix5vgm</id>
    <title>TIP: Open WebUI "Overview" mode</title>
    <updated>2025-02-24T16:19:18+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5vgm/tip_open_webui_overview_mode/"&gt; &lt;img alt="TIP: Open WebUI &amp;quot;Overview&amp;quot; mode" src="https://a.thumbs.redditmedia.com/xhTnipJlQp9kwvDNpjx8F2VXJDkn8RgN7RNScLB8_00.jpg" title="TIP: Open WebUI &amp;quot;Overview&amp;quot; mode" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As Google added &lt;a href="https://nitter.net/OfficialLoganK/status/1894049802557456669"&gt;branching support&lt;/a&gt; for its AI Studio product, I think the crown in terms of implementation is still held by the Open WebUI.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nfdla1lq34le1.png?width=2492&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70419e34b5c6474913c5d005bf6a5125561d8302"&gt;Overview mode&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;To activate: click &amp;quot;...&amp;quot; at the top right and select &amp;quot;Overview&amp;quot; in the menu&lt;/li&gt; &lt;li&gt;Clicking any leaf node in the graph will update the chat state accordingly&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5vgm/tip_open_webui_overview_mode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5vgm/tip_open_webui_overview_mode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5vgm/tip_open_webui_overview_mode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T16:19:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix11go</id>
    <title>aspen - Open-source voice assistant you can call, at only $0.01025/min!</title>
    <updated>2025-02-24T12:37:28+00:00</updated>
    <author>
      <name>/u/thooton</name>
      <uri>https://old.reddit.com/user/thooton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix11go/aspen_opensource_voice_assistant_you_can_call_at/"&gt; &lt;img alt="aspen - Open-source voice assistant you can call, at only $0.01025/min!" src="https://external-preview.redd.it/Y8cU497M8VmMsSvykiiACmZpJ9cu5NkYzryYit_2lHY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b00ce71267ae00f4c36a6263a4dd0cc5d7b9aee" title="aspen - Open-source voice assistant you can call, at only $0.01025/min!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1ix11go/video/ohkvv8g9z2le1/player"&gt;https://reddit.com/link/1ix11go/video/ohkvv8g9z2le1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;hi everyone, hope you're all doing great :) I thought I'd share a little project that I've been working on for the past few days. It's a voice assistant that uses Twilio's API to be accessible through a real phone number, so you can call it just like a person!&lt;/p&gt; &lt;p&gt;Using Groq's STT free tier and Google's TTS free tier, the only costs come from Twilio and Anthropic and add up to about $0.01025/min, which is a lot cheaper than the conversational agents from ElevenLabs or PlayAI which approach $0.10/min or $0.18/min respectively.&lt;/p&gt; &lt;p&gt;I wrote the code to be as modular as possible so it should be easy to modify it to use your own local LLM or whatever you like! all PRs are welcome :)&lt;/p&gt; &lt;p&gt;have an awesome day!!!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/thooton/aspen"&gt;https://github.com/thooton/aspen&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thooton"&gt; /u/thooton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix11go/aspen_opensource_voice_assistant_you_can_call_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix11go/aspen_opensource_voice_assistant_you_can_call_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix11go/aspen_opensource_voice_assistant_you_can_call_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T12:37:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix6bjw</id>
    <title>Is there any image models coming out?</title>
    <updated>2025-02-24T16:37:38+00:00</updated>
    <author>
      <name>/u/hoja_nasredin</name>
      <uri>https://old.reddit.com/user/hoja_nasredin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We were extremely spoiled this summer with Flux and SD3.1 coming out. But was anything else have been released since? Flux cannot be trained in a serious way apparently since it is distilled, and SD3 is hated by the community (or it might have some other issues I'm not aware). &lt;/p&gt; &lt;p&gt;What is happening with the image models right now? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hoja_nasredin"&gt; /u/hoja_nasredin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix6bjw/is_there_any_image_models_coming_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix6bjw/is_there_any_image_models_coming_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix6bjw/is_there_any_image_models_coming_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T16:37:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix46wr</id>
    <title>R1 for Spatial Reasoning</title>
    <updated>2025-02-24T15:08:13+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix46wr/r1_for_spatial_reasoning/"&gt; &lt;img alt="R1 for Spatial Reasoning" src="https://b.thumbs.redditmedia.com/jyqIGUOvKtGUJMPde7zV_1b5qWCF6lHqv8cfpN-Wi_Q.jpg" title="R1 for Spatial Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing an experiment in data synthesis for R1-style reasoning in my VLM, fine-tuned for enhanced spatial reasoning, more in &lt;a href="https://huggingface.co/spaces/open-r1/README/discussions/10"&gt;this discussion&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;After finding &lt;a href="https://spatial-vlm.github.io/"&gt;SpatialVLM&lt;/a&gt; last year, we open-sourced a similar 3D scene reconstruction pipeline: &lt;a href="https://github.com/remyxai/VQASynth"&gt;VQASynth&lt;/a&gt; to generate instruction following data for spatial reasoning.&lt;/p&gt; &lt;p&gt;Inspired by &lt;a href="https://typefly.github.io/"&gt;TypeFly&lt;/a&gt;, we tried applying &lt;a href="https://x.com/smellslikeml/status/1790069289413914956"&gt;this idea to VLMs&lt;/a&gt;, but it wasn't robust enough to fly our drone.&lt;/p&gt; &lt;p&gt;With R1-style reasoning, can't we ground our response on a set of observations from the VQASynth pipeline to train a VLM for better scene understanding and planning?&lt;/p&gt; &lt;p&gt;That's the goal for an upcoming VLM release &lt;a href="https://colab.research.google.com/drive/1R64daHgR50GnxH3yn7mcs8rnldWL1ZxF"&gt;based on this&lt;/a&gt; colab.&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts on making a dataset and VLM which could power the next generation of more reliable embodied AI applications, join us on &lt;a href="https://github.com/remyxai/VQASynth/issues/36"&gt;github&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rwcajdccv3le1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d924f00d5dd56b8bb5c6900799071783993fe1a4"&gt;https://preview.redd.it/rwcajdccv3le1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d924f00d5dd56b8bb5c6900799071783993fe1a4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gzs74o4dv3le1.png?width=754&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d20c61ca9f0cb8e8f3d97b21be473c6002abdc3a"&gt;https://preview.redd.it/gzs74o4dv3le1.png?width=754&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d20c61ca9f0cb8e8f3d97b21be473c6002abdc3a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix46wr/r1_for_spatial_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix46wr/r1_for_spatial_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix46wr/r1_for_spatial_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T15:08:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwhfl5</id>
    <title>96GB modded RTX 4090 for $4.5k</title>
    <updated>2025-02-23T18:55:09+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwhfl5/96gb_modded_rtx_4090_for_45k/"&gt; &lt;img alt="96GB modded RTX 4090 for $4.5k" src="https://preview.redd.it/5rf8m3k1rxke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d35cdb0e62ea887c4da38324fff2ccbbf226f9f" title="96GB modded RTX 4090 for $4.5k" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5rf8m3k1rxke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwhfl5/96gb_modded_rtx_4090_for_45k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwhfl5/96gb_modded_rtx_4090_for_45k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T18:55:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix3gao</id>
    <title>nvidia / Evo 2 Protein Design</title>
    <updated>2025-02-24T14:35:47+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3gao/nvidia_evo_2_protein_design/"&gt; &lt;img alt="nvidia / Evo 2 Protein Design" src="https://preview.redd.it/fp2o6r9ql3le1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=174a8acebfd2e90b81df658a8e8c6f3c7d031293" title="nvidia / Evo 2 Protein Design" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://build.nvidia.com/nvidia/evo2-protein-design/blueprintcard"&gt;https://build.nvidia.com/nvidia/evo2-protein-design/blueprintcard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fp2o6r9ql3le1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3gao/nvidia_evo_2_protein_design/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3gao/nvidia_evo_2_protein_design/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T14:35:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix5np0</id>
    <title>I built OLLAMA GUI in next.js how do you like it?</title>
    <updated>2025-02-24T16:10:21+00:00</updated>
    <author>
      <name>/u/Itsaliensbro453</name>
      <uri>https://old.reddit.com/user/Itsaliensbro453</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5np0/i_built_ollama_gui_in_nextjs_how_do_you_like_it/"&gt; &lt;img alt="I built OLLAMA GUI in next.js how do you like it?" src="https://preview.redd.it/f0j99mmn24le1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=494ec50651dc68222e262f195d12282a270ea7e0" title="I built OLLAMA GUI in next.js how do you like it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hellou guys im a developer trying to land my first job so im creating projects for my portfolio!&lt;/p&gt; &lt;p&gt;I have built this OLLAMA GUI with Next.js and Typescrypt!ðŸ˜€&lt;/p&gt; &lt;p&gt;How do you like it? Feel free to use the app and contribute its 100% free and open source! &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Ablasko32/Project-Shard---GUI-for-local-LLM-s"&gt;https://github.com/Ablasko32/Project-Shard---GUI-for-local-LLM-s&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Itsaliensbro453"&gt; /u/Itsaliensbro453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f0j99mmn24le1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5np0/i_built_ollama_gui_in_nextjs_how_do_you_like_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5np0/i_built_ollama_gui_in_nextjs_how_do_you_like_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T16:10:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwvo4b</id>
    <title>An Open-Source Implementation of Deep Research using Gemini Flash 2.0</title>
    <updated>2025-02-24T06:32:56+00:00</updated>
    <author>
      <name>/u/CarpetNo5579</name>
      <uri>https://old.reddit.com/user/CarpetNo5579</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an open source version of deep research using Gemini Flash 2.0!&lt;/p&gt; &lt;p&gt;Feed it any topic and it'll explore it thoroughly, building and displaying a research tree in real-time as it works. &lt;/p&gt; &lt;p&gt;This implementation has three research modes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fast (1-3min): Quick surface research, perfect for initial exploration&lt;/li&gt; &lt;li&gt;Balanced (3-6min): Moderate depth, explores main concepts and relationships&lt;/li&gt; &lt;li&gt;Comprehensive (5-12min): Deep recursive research, builds query trees, explores counter-arguments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The coolest part is watching it think - it prints out the research tree as it explores, so you can see exactly how it's approaching your topic.&lt;/p&gt; &lt;p&gt;I built this because I haven't seen any implementation that uses Gemini and its built in search tool and thought others might find it useful too.&lt;/p&gt; &lt;p&gt;Here's the github link: &lt;a href="https://github.com/eRuaro/open-gemini-deep-research"&gt;https://github.com/eRuaro/open-gemini-deep-research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CarpetNo5579"&gt; /u/CarpetNo5579 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwvo4b/an_opensource_implementation_of_deep_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwvo4b/an_opensource_implementation_of_deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwvo4b/an_opensource_implementation_of_deep_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T06:32:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix1ddj</id>
    <title>ragit 0.3.0 released</title>
    <updated>2025-02-24T12:55:49+00:00</updated>
    <author>
      <name>/u/baehyunsol</name>
      <uri>https://old.reddit.com/user/baehyunsol</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix1ddj/ragit_030_released/"&gt; &lt;img alt="ragit 0.3.0 released" src="https://external-preview.redd.it/TipJWadkvg51FCh2k5yn7L-J4VOuk7fkeumbFTDL_OM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91ff6bfc615eb15181fb1437f1d20a7a4e5656c6" title="ragit 0.3.0 released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on this open source RAG solution for a while.&lt;/p&gt; &lt;p&gt;It gives you a simple CLI for local rag, without any need for writing code!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/baehyunsol"&gt; /u/baehyunsol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/baehyunsol/ragit"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix1ddj/ragit_030_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix1ddj/ragit_030_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T12:55:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwtl7f</id>
    <title>Most people are worried about LLM's executing code. Then theres me...... ðŸ˜‚</title>
    <updated>2025-02-24T04:24:24+00:00</updated>
    <author>
      <name>/u/DataScientist305</name>
      <uri>https://old.reddit.com/user/DataScientist305</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwtl7f/most_people_are_worried_about_llms_executing_code/"&gt; &lt;img alt="Most people are worried about LLM's executing code. Then theres me...... ðŸ˜‚" src="https://preview.redd.it/92abn3ekk0le1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09547c1d04e4bb052014aac1d2d58fba8d76d0ee" title="Most people are worried about LLM's executing code. Then theres me...... ðŸ˜‚" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataScientist305"&gt; /u/DataScientist305 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/92abn3ekk0le1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwtl7f/most_people_are_worried_about_llms_executing_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwtl7f/most_people_are_worried_about_llms_executing_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T04:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix0d4z</id>
    <title>Polish Ministry of Digital Affairs shared PLLuM model family on HF</title>
    <updated>2025-02-24T11:57:56+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/CYFRAGOVPL"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix0d4z/polish_ministry_of_digital_affairs_shared_pllum/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix0d4z/polish_ministry_of_digital_affairs_shared_pllum/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T11:57:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwvvmy</id>
    <title>Qwen is releasing something tonight!</title>
    <updated>2025-02-24T06:46:53+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwvvmy/qwen_is_releasing_something_tonight/"&gt; &lt;img alt="Qwen is releasing something tonight!" src="https://external-preview.redd.it/vArUV2h82u8EtPauQRu5bQrqvRa1QZ1C_bg0wPIoH5o.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=263b5e9873bf302385907f40a338f7412dc9b280" title="Qwen is releasing something tonight!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://twitter.com/Alibaba_Qwen/status/1893907569724281088"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwvvmy/qwen_is_releasing_something_tonight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwvvmy/qwen_is_releasing_something_tonight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T06:46:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwqf3z</id>
    <title>FlashMLA - Day 1 of OpenSourceWeek</title>
    <updated>2025-02-24T01:37:17+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwqf3z/flashmla_day_1_of_opensourceweek/"&gt; &lt;img alt="FlashMLA - Day 1 of OpenSourceWeek" src="https://preview.redd.it/to631nzvqzke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3551b21c98cfb01ba242529b337443a5c85b4481" title="FlashMLA - Day 1 of OpenSourceWeek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/deepseek-ai/FlashMLA"&gt;https://github.com/deepseek-ai/FlashMLA&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/to631nzvqzke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwqf3z/flashmla_day_1_of_opensourceweek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwqf3z/flashmla_day_1_of_opensourceweek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T01:37:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwzuqb</id>
    <title>Claude Sonnet 3.7 soon</title>
    <updated>2025-02-24T11:24:48+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwzuqb/claude_sonnet_37_soon/"&gt; &lt;img alt="Claude Sonnet 3.7 soon" src="https://preview.redd.it/agru3m1in2le1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=753ac58c540176e9b69e887c77f5a13fea6f9608" title="Claude Sonnet 3.7 soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/agru3m1in2le1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwzuqb/claude_sonnet_37_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwzuqb/claude_sonnet_37_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T11:24:48+00:00</published>
  </entry>
</feed>
