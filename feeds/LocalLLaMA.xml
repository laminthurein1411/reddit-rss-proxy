<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-06T21:34:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j4y0kn</id>
    <title>Made a simple playground for easy experiment with 8+ open-source PDF-to-markdown for local model ingestion (+ visualization)</title>
    <updated>2025-03-06T15:29:12+00:00</updated>
    <author>
      <name>/u/taprosoft</name>
      <uri>https://old.reddit.com/user/taprosoft</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4y0kn/made_a_simple_playground_for_easy_experiment_with/"&gt; &lt;img alt="Made a simple playground for easy experiment with 8+ open-source PDF-to-markdown for local model ingestion (+ visualization)" src="https://external-preview.redd.it/Ik_UOXUVGTKj5B9oOW8_FISqZe0LfJ9NkHqhzs4tgyU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c48cee8cb3ea64e35dbb0891bce79f17bc38eb0" title="Made a simple playground for easy experiment with 8+ open-source PDF-to-markdown for local model ingestion (+ visualization)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taprosoft"&gt; /u/taprosoft &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/chunking-ai/pdf-playground"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4y0kn/made_a_simple_playground_for_easy_experiment_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4y0kn/made_a_simple_playground_for_easy_experiment_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T15:29:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4r1iu</id>
    <title>A SOTA of hardware for LLM made by exolab creator</title>
    <updated>2025-03-06T08:35:04+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a quite long but &lt;a href="https://x.com/alexocheema/status/1897349404522078261"&gt;interesting thread&lt;/a&gt; made by Alex Cheema, the creator of exolabs.&lt;/p&gt; &lt;p&gt;With the release of the new Qwen and the fast pace of improvement, it seems that we will no longer need to buy maxed out machines to run a frontier model locally.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Apple's timing could not be better with this. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The M3 Ultra 512GB Mac Studio fits perfectly with massive sparse MoEs like DeepSeek V3/R1. &lt;/p&gt; &lt;p&gt;2 M3 Ultra 512GB Mac Studios with &lt;a href="https://x.com/exolabs"&gt;u/exolabs&lt;/a&gt; is all you need to run the full, unquantized DeepSeek R1 at home. &lt;/p&gt; &lt;p&gt;The first requirement for running these massive AI models is that they need to fit into GPU memory (in Apple's case, unified memory). Here's a quick comparison of how much that costs for different options (note: DIGITS is left out here since details are still unconfirmed): &lt;/p&gt; &lt;p&gt;NVIDIA H100: 80GB @ 3TB/s, $25,000, $312.50 per GB&lt;br /&gt; AMD MI300X: 192GB @ 5.3TB/s, $20,000, $104.17 per GB&lt;br /&gt; Apple M2 Ultra: 192GB @ 800GB/s, $5,000, $26.04 per GB&lt;br /&gt; Apple M3 Ultra: 512GB @ 800GB/s, $9,500, $18.55 per GB &lt;/p&gt; &lt;p&gt;That's a 28% reduction in $ per GB from the M2 Ultra - pretty good. &lt;/p&gt; &lt;p&gt;The concerning thing here is the memory refresh rate. This is the ratio of memory bandwidth to memory of the device. It tells you how many times per second you could cycle through the entire memory on the device. This is the dominating factor for the performance of single request (batch_size=1) inference. For a dense model that saturates all of the memory of the machine, the maximum theoretical token rate is bound by this number. Comparison of memory refresh rate: &lt;/p&gt; &lt;p&gt;NVIDIA H100 (80GB): 37.5/s&lt;br /&gt; AMD MI300X (192GB): 27.6/s&lt;br /&gt; Apple M2 Ultra (192GB): 4.16/s (9x less than H100)&lt;br /&gt; Apple M3 Ultra (512GB): 1.56/s (24x less than H100) &lt;/p&gt; &lt;p&gt;Apple is trading off more memory for less memory refresh frequency, now 24x less than a H100. Another way to look at this is to analyze how much it costs per unit of memory bandwidth. Comparison of cost per GB/s of memory bandwidth (cheaper is better): &lt;/p&gt; &lt;p&gt;NVIDIA H100 (80GB): $8.33 per GB/s&lt;br /&gt; AMD MI300X (192GB): $3.77 per GB/s&lt;br /&gt; Apple M2 Ultra (192GB): $6.25 per GB/s&lt;br /&gt; Apple M3 Ultra (512GB): $11.875 per GB/s &lt;/p&gt; &lt;p&gt;There are two ways Apple wins with this approach. Both are hierarchical model structures that exploit the sparsity of model parameter activation: MoE and Modular Routing. &lt;/p&gt; &lt;p&gt;MoE adds multiple experts to each layer and picks the top-k of N experts in each layer, so only k/N experts are active per layer. The more sparse the activation (smaller the ratio k/N) the better for Apple. DeepSeek R1 ratio is small: 8/256 = 1/32. Model developers could likely push this to be even smaller, potentially we might see a future where k/N is something like 8/1024 = 1/128 (&amp;lt;1% activated parameters). &lt;/p&gt; &lt;p&gt;Modular Routing includes methods like DiPaCo and dynamic ensembles where a gating function activates multiple independent models and aggregates the results into one single result. For this, multiple models need to be in memory but only a few are active at any given time. &lt;/p&gt; &lt;p&gt;Both MoE and Modular Routing require a lot of memory but not much memory bandwidth because only a small % of total parameters are active at any given time, which is the only data that actually needs to move around in memory. &lt;/p&gt; &lt;p&gt;Funny story... 2 weeks ago I had a call with one of Apple's biggest competitors. They asked if I had a suggestion for a piece of AI hardware they could build. I told them, go build a 512GB memory Mac Studio-like box for AI. Congrats Apple for doing this. Something I thought would still take you a few years to do you did today. I'm impressed. &lt;/p&gt; &lt;p&gt;Looking forward, there will likely be an M4 Ultra Mac Studio next year which should address my main concern since these Ultra chips use Apple UltraFusion to fuse Max dies. The M4 Max had a 36.5% increase in memory bandwidth compared to the M3 Max, so we should see something similar (or possibly more depending on the configuration) in the M4 Ultra.&lt;/p&gt; &lt;p&gt;AI generated TLDR:&lt;/p&gt; &lt;p&gt;Apple's new M3 Ultra Mac Studio with 512GB unified memory is ideal for massive sparse AI models like DeepSeek V3/R1, allowing users to run large models at home affordably compared to NVIDIA and AMD GPUs. While Apple's approach offers significantly cheaper memory capacity, it sacrifices memory bandwidth, resulting in lower memory refresh rates—crucial for dense model inference. However, sparse architectures like Mixture-of-Experts (MoE) and Modular Routing effectively utilize Apple's strengths by activating only a small portion of parameters at a time. Future Apple chips (e.g., M4 Ultra) may further improve memory bandwidth, addressing current performance limitations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4r1iu/a_sota_of_hardware_for_llm_made_by_exolab_creator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4r1iu/a_sota_of_hardware_for_llm_made_by_exolab_creator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4r1iu/a_sota_of_hardware_for_llm_made_by_exolab_creator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T08:35:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4gw91</id>
    <title>QwQ-32B seems to get the same quality final answer as R1 while reasoning much more concisely and efficiently</title>
    <updated>2025-03-05T23:04:05+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think I will now switch over to using QwQ as my primary reasoning model instead of R1. In all my testing, it gets the same or superior quality answers as R1 does, while having its chain of thought be much more efficient, much more concise, and much more confident. In contrast, R1 feels like a bumbling idiot who happens to be really smart only because he tries every possible solution. And It's not particularly close either, QwQ takes like 4x fewer tokens than R1 on the same problem while both arriving at the same answer.&lt;/p&gt; &lt;p&gt;Adam was right when he said not all CoTs are equal, and in this case, I think Qwen trained their model to be more efficient without degrading quality at all.&lt;/p&gt; &lt;p&gt;But I'm curious to hear what everyone here thinks, because I'm sure others are more experienced than I am.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4gw91/qwq32b_seems_to_get_the_same_quality_final_answer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4gw91/qwq32b_seems_to_get_the_same_quality_final_answer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4gw91/qwq32b_seems_to_get_the_same_quality_final_answer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T23:04:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4az6k</id>
    <title>Qwen/QwQ-32B · Hugging Face</title>
    <updated>2025-03-05T19:05:05+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4az6k/qwenqwq32b_hugging_face/"&gt; &lt;img alt="Qwen/QwQ-32B · Hugging Face" src="https://external-preview.redd.it/6TRd04lcKHQEO7NFYroC88UsYfg6QAwSPoiUg0dROsM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59db8a1b256d27e6f63efdf37ea7de63d8be02e2" title="Qwen/QwQ-32B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/QwQ-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4az6k/qwenqwq32b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4az6k/qwenqwq32b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T19:05:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j51m68</id>
    <title>[JotItNow] I built a 100% offline AI note-taking app that respects your privacy</title>
    <updated>2025-03-06T17:59:19+00:00</updated>
    <author>
      <name>/u/Ssjultrainstnict</name>
      <uri>https://old.reddit.com/user/Ssjultrainstnict</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Reddit!After months of work, I'm excited to share JotItNow - a completely offline AI-powered note-taking app for iOS I built to solve my own frustrations with existing solutions.What makes JotItNow different?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;100% Offline &amp;amp; Private: Everything happens on your device. No internet connection needed, no data tracking, no cloud storage. Your thoughts stay yours.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Voice + Text Together: Record voice notes while simultaneously typing text notes - perfect for lectures, meetings, or when inspiration strikes.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;AI Summaries: Instantly generate concise summaries of your long voice memos or text notes with one tap - all processed locally on your device.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Chat With Your Notes: After summarizing, you can actually have a conversation with your notes! Ask questions, explore connections, or dig deeper into specific topics - the AI understands your content.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Completely Free &amp;amp; Open Source: No subscriptions, no in-app purchases, no ads. Check out the code on GitHub &lt;a href="https://github.com/navedmerchant/JotItNow"&gt;https://github.com/navedmerchant/JotItNow&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I built this because I was tired of note-taking apps that either lacked AI features or required sending all my data to the cloud. JotItNow runs PARM2(LLM) and BGE (Embedding) models directly on your device, so you get powerful AI without sacrificing privacy. &lt;/p&gt; &lt;p&gt;Coming Soon: Global chat with all your notes at once! This will let you search and interact with your entire knowledge base through natural conversation.&lt;/p&gt; &lt;p&gt;If you're interested in trying it out, JotItNow is available on the App Store &lt;a href="https://apps.apple.com/us/app/jotitnow/id6742083186"&gt;https://apps.apple.com/us/app/jotitnow/id6742083186&lt;/a&gt; for iOS (15.0+).I'd love to hear your feedback or answer any questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ssjultrainstnict"&gt; /u/Ssjultrainstnict &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j51m68/jotitnow_i_built_a_100_offline_ai_notetaking_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j51m68/jotitnow_i_built_a_100_offline_ai_notetaking_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j51m68/jotitnow_i_built_a_100_offline_ai_notetaking_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T17:59:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4s0o4</id>
    <title>QwQ-32B solves the o1-preview Cipher problem!</title>
    <updated>2025-03-06T09:50:28+00:00</updated>
    <author>
      <name>/u/sunpazed</name>
      <uri>https://old.reddit.com/user/sunpazed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen QwQ 32B solves the Cipher problem first showcased in the &lt;a href="https://openai.com/index/learning-to-reason-with-llms/"&gt;OpenAI o1-preview Technical Paper&lt;/a&gt;. No other local model so far (at least on my 48Gb MacBook) has been able to solve this. Amazing performance from a 32B model (6-bit quantised too!). Now for the sad bit — it did take over 9000 tokens, and at 4t/s this took 33 minutes to complete.&lt;/p&gt; &lt;p&gt;Here's the full output, including prompt from llama.cpp:&lt;br /&gt; &lt;a href="https://gist.github.com/sunpazed/497cf8ab11fa7659aab037771d27af57"&gt;https://gist.github.com/sunpazed/497cf8ab11fa7659aab037771d27af57&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunpazed"&gt; /u/sunpazed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4s0o4/qwq32b_solves_the_o1preview_cipher_problem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4s0o4/qwq32b_solves_the_o1preview_cipher_problem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4s0o4/qwq32b_solves_the_o1preview_cipher_problem/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T09:50:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j50doz</id>
    <title>Compatible draft models for QwQ speculative decoding with Llama CPP?</title>
    <updated>2025-03-06T17:08:13+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tried Qwen 1.5 and got an incomplete vocabulary &lt;/p&gt; &lt;p&gt;Tried &lt;a href="https://huggingface.co/MaziyarPanahi/Qwen2.5-Coder-0.5B-QwQ-draft-GGUF/tree/main"&gt;a qwq preview coder draft&lt;/a&gt; and got:&lt;/p&gt; &lt;p&gt;&lt;code&gt; common_speculative_are_compatible: draft vocab special tokens must match target vocab to use speculation &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Has anyone had success in speculative decoding with this model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j50doz/compatible_draft_models_for_qwq_speculative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j50doz/compatible_draft_models_for_qwq_speculative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j50doz/compatible_draft_models_for_qwq_speculative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T17:08:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4b1t9</id>
    <title>QwQ-32B released, equivalent or surpassing full Deepseek-R1!</title>
    <updated>2025-03-05T19:08:01+00:00</updated>
    <author>
      <name>/u/ortegaalfredo</name>
      <uri>https://old.reddit.com/user/ortegaalfredo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4b1t9/qwq32b_released_equivalent_or_surpassing_full/"&gt; &lt;img alt="QwQ-32B released, equivalent or surpassing full Deepseek-R1!" src="https://external-preview.redd.it/GjWMsqQ0sjAo2i1u3zMKBVF8QJTEurDWKLmSNIhLwOE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37675ad756a9c5a85511da4e75709d13466b2af3" title="QwQ-32B released, equivalent or surpassing full Deepseek-R1!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ortegaalfredo"&gt; /u/ortegaalfredo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1897361654763151544"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4b1t9/qwq32b_released_equivalent_or_surpassing_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4b1t9/qwq32b_released_equivalent_or_surpassing_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T19:08:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j501o6</id>
    <title>Meet Gemini Coder - Open WebUI hands-free chat initializer with your code and prompt (openwebui shown at the end)</title>
    <updated>2025-03-06T16:54:59+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j501o6/meet_gemini_coder_open_webui_handsfree_chat/"&gt; &lt;img alt="Meet Gemini Coder - Open WebUI hands-free chat initializer with your code and prompt (openwebui shown at the end)" src="https://preview.redd.it/prc5g1i8n3ne1.gif?width=640&amp;amp;crop=smart&amp;amp;s=d5f258747a38619d34aa9120ff24c7a7c0739719" title="Meet Gemini Coder - Open WebUI hands-free chat initializer with your code and prompt (openwebui shown at the end)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/prc5g1i8n3ne1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j501o6/meet_gemini_coder_open_webui_handsfree_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j501o6/meet_gemini_coder_open_webui_handsfree_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T16:54:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4wsmc</id>
    <title>QwQ-32B running on a 4 years old 32GB M1 Max</title>
    <updated>2025-03-06T14:34:13+00:00</updated>
    <author>
      <name>/u/w-zhong</name>
      <uri>https://old.reddit.com/user/w-zhong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4wsmc/qwq32b_running_on_a_4_years_old_32gb_m1_max/"&gt; &lt;img alt="QwQ-32B running on a 4 years old 32GB M1 Max" src="https://external-preview.redd.it/d3J3bGg3dmp5Mm5lMfhiyQ5lHDmbnap8mhwAi3miXIdLVbuLWrtOk23s9uGq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a6f7fe88aeb3c5fbf45af9dc367beb6c3b3edd9" title="QwQ-32B running on a 4 years old 32GB M1 Max" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w-zhong"&gt; /u/w-zhong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ev80m8vjy2ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4wsmc/qwq32b_running_on_a_4_years_old_32gb_m1_max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4wsmc/qwq32b_running_on_a_4_years_old_32gb_m1_max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T14:34:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4p3xw</id>
    <title>A few hours with QwQ and Aider - and my thoughts</title>
    <updated>2025-03-06T06:15:15+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a mini review. I'll be as brief as possible.&lt;/p&gt; &lt;p&gt;I tested QwQ using Q5 and Q6 from Bartowski. I didn't notice any major benefit from Q6.&lt;/p&gt; &lt;h2&gt;The Good&lt;/h2&gt; &lt;p&gt;It's very good. This model, if you can stomach the extra tokens, is stronger than Deepseek Distill R1 32B, no doubt about it. But &lt;em&gt;it needs to think more&lt;/em&gt; to achieve it. If you are sensitive to context size or inference speed, this may be a difficult trade off.&lt;/p&gt; &lt;h2&gt;The Great&lt;/h2&gt; &lt;p&gt;This model beat Qwen-Coder 32B, who has been the king of kings for coders in Aider for models of this size. It doesn't necessarily write better code, but it takes far less iterations. It catches your intentions and instructions on the first try and avoids silly syntax errors. &lt;strong&gt;The biggest strength is that I have to prompt way less using QwQ vs Qwen Coder&lt;/strong&gt; - but it should be noted that 1 prompt to QwQ will take 2-3x as many tokens as 3 iterative prompts to Qwen-Coder 32B &lt;/p&gt; &lt;h2&gt;The Bad&lt;/h2&gt; &lt;p&gt;As said above, it THINKS to be as smart as it is. And it thinks A LOT. I'm using 512GB/s entirely in VRAM and I found myself getting impatient.&lt;/p&gt; &lt;h2&gt;The Ugly&lt;/h2&gt; &lt;p&gt;Twice it randomly wrote perfect code for me (one shots) but then forgot to follow Aider's code-editing rules. This is a huge bummer after waiting for SO MANY thinking tokens to produce a result.&lt;/p&gt; &lt;h2&gt;Conclusion (so far)&lt;/h2&gt; &lt;p&gt;Those benchmarks beating Deepseek R1 (full fat) are definitely bogus. This model is not in that tier. But it's basically managed to become three iterative prompts to Qwen32B and Qwen-Coder32B in a single prompt, which is absolutely incredible. I think a lot of folks will get use out of this model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4p3xw/a_few_hours_with_qwq_and_aider_and_my_thoughts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4p3xw/a_few_hours_with_qwq_and_aider_and_my_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4p3xw/a_few_hours_with_qwq_and_aider_and_my_thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T06:15:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4jpij</id>
    <title>M3 Ultra is a slightly weakened 3090 w/ 512GB</title>
    <updated>2025-03-06T01:19:01+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To conclude, you are getting a slightly weakened 3090 with 512GB at max config as it gets 114.688TFLOPS FP16 vs 142.32TFLOPS FP16 for 3090 and memory bandwidth of 819.2GB/s vs 936GB/s.&lt;/p&gt; &lt;p&gt;The only place I can find about M3 Ultra spec is:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.apple.com/newsroom/2025/03/apple-reveals-m3-ultra-taking-apple-silicon-to-a-new-extreme/"&gt;https://www.apple.com/newsroom/2025/03/apple-reveals-m3-ultra-taking-apple-silicon-to-a-new-extreme/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, it is highly vague about the spec. So I made an educated guess on the exact spec of M3 Ultra based on this article.&lt;/p&gt; &lt;p&gt;To achieve a GPU of 2x performance of M2 Ultra and 2.6x of M1 Ultra, you need to double the shaders per core from 128 to 256. That's what I guess is happening here for such big improvement.&lt;/p&gt; &lt;p&gt;I also made a guesstimate on what a M4 Ultra can be.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Chip&lt;/th&gt; &lt;th align="left"&gt;M3 Ultra&lt;/th&gt; &lt;th align="left"&gt;M2 Ultra&lt;/th&gt; &lt;th align="left"&gt;M1 Ultra&lt;/th&gt; &lt;th align="left"&gt;M4 Ultra?&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU Core&lt;/td&gt; &lt;td align="left"&gt;80&lt;/td&gt; &lt;td align="left"&gt;76&lt;/td&gt; &lt;td align="left"&gt;80&lt;/td&gt; &lt;td align="left"&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU Shader&lt;/td&gt; &lt;td align="left"&gt;20480&lt;/td&gt; &lt;td align="left"&gt;9728&lt;/td&gt; &lt;td align="left"&gt;8192&lt;/td&gt; &lt;td align="left"&gt;20480&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU GHz&lt;/td&gt; &lt;td align="left"&gt;1.4&lt;/td&gt; &lt;td align="left"&gt;1.4&lt;/td&gt; &lt;td align="left"&gt;1.3&lt;/td&gt; &lt;td align="left"&gt;1.68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU FP16&lt;/td&gt; &lt;td align="left"&gt;114.688&lt;/td&gt; &lt;td align="left"&gt;54.4768&lt;/td&gt; &lt;td align="left"&gt;42.5984&lt;/td&gt; &lt;td align="left"&gt;137.6256&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM Type&lt;/td&gt; &lt;td align="left"&gt;LPDDR5&lt;/td&gt; &lt;td align="left"&gt;LPDDR5&lt;/td&gt; &lt;td align="left"&gt;LPDDR5&lt;/td&gt; &lt;td align="left"&gt;LPDDR5X&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM Speed&lt;/td&gt; &lt;td align="left"&gt;6400&lt;/td&gt; &lt;td align="left"&gt;6400&lt;/td&gt; &lt;td align="left"&gt;6400&lt;/td&gt; &lt;td align="left"&gt;8533&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM Controller&lt;/td&gt; &lt;td align="left"&gt;64&lt;/td&gt; &lt;td align="left"&gt;64&lt;/td&gt; &lt;td align="left"&gt;64&lt;/td&gt; &lt;td align="left"&gt;64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM Bandwidth&lt;/td&gt; &lt;td align="left"&gt;819.2&lt;/td&gt; &lt;td align="left"&gt;819.2&lt;/td&gt; &lt;td align="left"&gt;819.2&lt;/td&gt; &lt;td align="left"&gt;1092.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU P-Core&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU GHz&lt;/td&gt; &lt;td align="left"&gt;4.05&lt;/td&gt; &lt;td align="left"&gt;3.5&lt;/td&gt; &lt;td align="left"&gt;3.2&lt;/td&gt; &lt;td align="left"&gt;4.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU FP16&lt;/td&gt; &lt;td align="left"&gt;3.1104&lt;/td&gt; &lt;td align="left"&gt;1.792&lt;/td&gt; &lt;td align="left"&gt;1.6384&lt;/td&gt; &lt;td align="left"&gt;3.456&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Apple is likely to be selling it at 10-15k. If 10k, I think it is quite a good deal as its performance is about 4xDIGITS and RAM is much faster. 15k is still not a bad deal either in that perspective.&lt;/p&gt; &lt;p&gt;There is also a possibility that there is no doubling of shader density and Apple is just playing with words. That would be a huge bummer. In that case, it is better to wait for M4 Ultra.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jpij/m3_ultra_is_a_slightly_weakened_3090_w_512gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jpij/m3_ultra_is_a_slightly_weakened_3090_w_512gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jpij/m3_ultra_is_a_slightly_weakened_3090_w_512gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T01:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4sjn5</id>
    <title>New AI breakthrough,Faster and Smaller low thinking token with "CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation "</title>
    <updated>2025-03-06T10:28:07+00:00</updated>
    <author>
      <name>/u/Different-Olive-8745</name>
      <uri>https://old.reddit.com/user/Different-Olive-8745</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Olive-8745"&gt; /u/Different-Olive-8745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.21074"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4sjn5/new_ai_breakthroughfaster_and_smaller_low/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4sjn5/new_ai_breakthroughfaster_and_smaller_low/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T10:28:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4x8sq</id>
    <title>new QwQ is beating any distil deepseek model in math, is even better than a full deepseek 670b in math, that is level o3 mini med / high - test in the post</title>
    <updated>2025-03-06T14:55:08+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4x8sq/new_qwq_is_beating_any_distil_deepseek_model_in/"&gt; &lt;img alt="new QwQ is beating any distil deepseek model in math, is even better than a full deepseek 670b in math, that is level o3 mini med / high - test in the post" src="https://b.thumbs.redditmedia.com/q7fp4W2eqYAWGBtpaItYK8uU0vQG9NXPIwItbPYyjpA.jpg" title="new QwQ is beating any distil deepseek model in math, is even better than a full deepseek 670b in math, that is level o3 mini med / high - test in the post" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All test were made 10 times (those questions I got correct 10/10 times)&lt;/p&gt; &lt;p&gt;QwQ form Bartowski - q4km, 16k context, speed - around 35 t/s&lt;/p&gt; &lt;p&gt;command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-cli.exe --model QwQ-32B-Q4_K_M.gguf --color --threads 30 --keep -1 --n-predict -1 --ctx-size 16384 -ngl 99 --simple-io -e --multiline-input --no-display-prompt --conversation --no-mmap &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;MATH&lt;/p&gt; &lt;pre&gt;&lt;code&gt;I have an initial balance of $100,000, and I earn $15,000 per month for every $100,000 in my balance. As my balance grows, my earnings increase in steps. Specifically, each time my balance increases by $100,000, my monthly earnings increase by $15,000. For example: With a balance of $100,000, I earn $15,000 per month. Once my balance reaches $200,000, I start earning $30,000 per month. When my balance reaches $300,000, I earn $45,000 per month, and so on. Assuming my balance grows month by month based on these earnings, how much will I have after 3 years (36 months)? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer - answer 9,475,000&lt;/p&gt; &lt;p&gt;QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tn8uo9pvr2ne1.png?width=1654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=293867d54a317141164c70c7187df3fbe9bc4637"&gt;https://preview.redd.it/tn8uo9pvr2ne1.png?width=1654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=293867d54a317141164c70c7187df3fbe9bc4637&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Can you solve the puzzle with these equations? ( 4 @ 7 @ 8 = 285684 ) ( 9 @ 3 @ 5 = 271542 ) ( 6 @ 2 @ 7 = 121426 ) ( 5 @ 6 @ 7 = ? ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer 304272&lt;/p&gt; &lt;p&gt;QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xq9o88uis2ne1.png?width=1647&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e8d4b3e615d9bfe0e0f7e0dcd1f9b52deffb97c"&gt;https://preview.redd.it/xq9o88uis2ne1.png?width=1647&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e8d4b3e615d9bfe0e0f7e0dcd1f9b52deffb97c&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;How many days are between 12-12-1971 and 18-4-2024? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer 19121 / 19122 &amp;lt;-- both answers are valid&lt;/p&gt; &lt;p&gt;QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wyrsesa4v2ne1.png?width=1633&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bb88ae1302c8760c1a10e8c210a4ec5aaebc9ba8"&gt;https://preview.redd.it/wyrsesa4v2ne1.png?width=1633&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bb88ae1302c8760c1a10e8c210a4ec5aaebc9ba8&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;If my BMI is 20.5 and my height is 172cm, how much would I weigh if I gained 5% of my current weight? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer 63.68kg &amp;lt;-- important is to get result as close to this number as possible&lt;/p&gt; &lt;p&gt;QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/otah3femv2ne1.png?width=1630&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2102c8b6ea535d220b53f8a504074a83ccc06e5"&gt;https://preview.redd.it/otah3femv2ne1.png?width=1630&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2102c8b6ea535d220b53f8a504074a83ccc06e5&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;In what percentage is water compressed at the bottom of the ocean in the Mariana Trench? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer around 5%&lt;/p&gt; &lt;p&gt;QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uagcqzj1w2ne1.png?width=1653&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c344a15d25f933e7ab5d312e25bf553131aa617"&gt;https://preview.redd.it/uagcqzj1w2ne1.png?width=1653&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c344a15d25f933e7ab5d312e25bf553131aa617&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;oyfjdnisdr rtqwainr acxz mynzbhhx -&amp;gt; Think step by step Use the example above to decode: oyekaijzdf aaptcg suaokybhai ouow aqht mynznvaatzacdfoulxxz &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer - There are three R's in Strawberry.&lt;br /&gt; QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/amgogxw9c4ne1.png?width=1786&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fdf59a2801ce5ea7ae63e531f09acb43a48dc342"&gt;https://preview.redd.it/amgogxw9c4ne1.png?width=1786&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fdf59a2801ce5ea7ae63e531f09acb43a48dc342&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LOGIC&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Create 10 sentences that ends with a word &amp;quot;apple&amp;quot;. Remember the word &amp;quot;apple&amp;quot; MUST be at the end. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer ... 10 sentences&lt;br /&gt; QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d7w1odgnw2ne1.png?width=1656&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c7c5856e48b554238c7b815f1b280dbe8f6f244"&gt;https://preview.redd.it/d7w1odgnw2ne1.png?width=1656&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c7c5856e48b554238c7b815f1b280dbe8f6f244&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Two fathers and two sons go fishing. They each catch one fish. Together, they leave with four fish in total. Is there anything strange about this story? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer - nothing strange&lt;br /&gt; QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uxqlq4p9x2ne1.png?width=1648&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d0b23a44fd00c8fe67e0ac6dd19aaff3630ee62"&gt;https://preview.redd.it/uxqlq4p9x2ne1.png?width=1648&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d0b23a44fd00c8fe67e0ac6dd19aaff3630ee62&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Here is a bag filled with popcorn. There is no chocolate in the bag. The bag is made of transparent plastic, so you can see what is inside. Yet, the label on the bag says &amp;quot;chocolate&amp;quot; and not &amp;quot;popcorn&amp;quot;. Sam finds the bag. She had never seen the bag before. Sam reads the label. She believes that the bag is full of… &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer - popcorn&lt;br /&gt; QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xzkuj33jx2ne1.png?width=1636&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a6014b99b0bc0d6e362732e2e23dee6559eaa71"&gt;https://preview.redd.it/xzkuj33jx2ne1.png?width=1636&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a6014b99b0bc0d6e362732e2e23dee6559eaa71&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LOGIC TRICKY&lt;/p&gt; &lt;pre&gt;&lt;code&gt;I have a bowl with a small cup inside. I placed the bowl upside down on a table and then pick up the bowl to put it in the microwave. Where is that cup? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer - on the table&lt;br /&gt; QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/78m0vg0ux2ne1.png?width=1640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=820786548e409e1c8e7f5febbd7c42aa0e930a06"&gt;https://preview.redd.it/78m0vg0ux2ne1.png?width=1640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=820786548e409e1c8e7f5febbd7c42aa0e930a06&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;I have a boat with 4 free spaces. I want to transport a man, sheep and cat on the other side of the river. How to do that? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer - one ride&lt;br /&gt; QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8h461fl303ne1.png?width=1657&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=88a54e969b56cdea417a36c51652e0e184b1de4a"&gt;https://preview.redd.it/8h461fl303ne1.png?width=1657&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=88a54e969b56cdea417a36c51652e0e184b1de4a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CODING&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Provide complete working code for a realistic looking tree in Python using the Turtle graphics library and a recursive algorithm. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer - testing how good tree will be built (derails , nuances )&lt;br /&gt; QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/egqwkfku03ne1.png?width=1021&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f10241983bc3fca66c8098672fcedf5ac9f4827"&gt;https://preview.redd.it/egqwkfku03ne1.png?width=1021&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f10241983bc3fca66c8098672fcedf5ac9f4827&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Provide complete working code for a realistic looking car in Python using the Turtle graphics library and a recursive algorithm. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer - QwQ made a car animation! ... even better than I expected ... no qwen coder 32b nor QwQ preview did that even close.&lt;br /&gt; QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2x9mkf3k43ne1.png?width=1635&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac67958dc6e46412e55f155c4e96791c192de754"&gt;https://preview.redd.it/2x9mkf3k43ne1.png?width=1635&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac67958dc6e46412e55f155c4e96791c192de754&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1j4x8sq/video/s8b9izfjd4ne1/player"&gt;https://reddit.com/link/1j4x8sq/video/s8b9izfjd4ne1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Conclusion:&lt;/p&gt; &lt;p&gt;Thinking like CRAZY ... sometimes x2-x3 longer than QwQ preview but it gives much better results!&lt;/p&gt; &lt;p&gt;I was able to solve EVETHING from my private tests by OFFLINE MODEL .... I have to make new more advanced questions.&lt;/p&gt; &lt;p&gt;Here I presented around 10 % of my questions.&lt;/p&gt; &lt;p&gt;Currently QwQ is the SOTA reasoning model 32b size beating beating any distil deepseek ....working offline has a level in reasoning and math on pair with o3 mini med or high...easy level of deepseek 671b&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4x8sq/new_qwq_is_beating_any_distil_deepseek_model_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4x8sq/new_qwq_is_beating_any_distil_deepseek_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4x8sq/new_qwq_is_beating_any_distil_deepseek_model_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T14:55:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4w3hz</id>
    <title>QwQ-32B is making waves in the stock market already</title>
    <updated>2025-03-06T14:01:14+00:00</updated>
    <author>
      <name>/u/piggledy</name>
      <uri>https://old.reddit.com/user/piggledy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4w3hz/qwq32b_is_making_waves_in_the_stock_market_already/"&gt; &lt;img alt="QwQ-32B is making waves in the stock market already" src="https://external-preview.redd.it/2Iq_CBTq3Fa9g0toYsPGGhCr0MaJn10vyxB4mx5RgeE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c31f6a94da485acc2cde8d6d221e945b0b1e12b1" title="QwQ-32B is making waves in the stock market already" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/piggledy"&gt; /u/piggledy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/03/06/alibaba-shares-soar-after-chinese-tech-giant-unveils-deepseek-rival-qwq-32b.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4w3hz/qwq32b_is_making_waves_in_the_stock_market_already/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4w3hz/qwq32b_is_making_waves_in_the_stock_market_already/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T14:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4v3fi</id>
    <title>Prompts for QwQ-32B</title>
    <updated>2025-03-06T13:10:23+00:00</updated>
    <author>
      <name>/u/drrros</name>
      <uri>https://old.reddit.com/user/drrros</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday while searching for prompts for QwQ, I stumbled upon an interesting article: &lt;a href="https://www.researchgate.net/publication/389351923_Towards_Thinking-Optimal_Scaling_of_Test-Time_Compute_for_LLM_Reasoning"&gt;research&lt;/a&gt;&lt;br /&gt; TLDR: 3 options for QwQ system prompt:&lt;/p&gt; &lt;p&gt;Low:&lt;br /&gt; &lt;code&gt;Low Reasoning Effort: You have extremely limited time to think and respond to the user’s query. Every additional second of processing and reasoning incurs a significant resource cost, which could affect efficiency and effectiveness. Your task is to prioritize speed without sacrificing essential clarity or accuracy. Provide the most direct and concise answer possible. Avoid unnecessary steps, reflections, verification, or refinements UNLESS ABSOLUTELY NECESSARY. Your primary goal is to deliver a quick, clear and correct response.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Medium:&lt;br /&gt; &lt;code&gt;Medium Reasoning Effort: You have sufficient time to think and respond to the user’s query, allowing for a more thoughtful and in-depth answer. However, be aware that the longer you take to reason and process, the greater the associated resource costs and potential consequences. While you should not rush, aim to balance the depth of your reasoning with efficiency. Prioritize providing a well-thought-out response, but do not overextend your thinking if the answer can be provided with a reasonable level of analysis. Use your reasoning time wisely, focusing on what is essential for delivering an accurate response without unnecessary delays and overthinking.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;High:&lt;br /&gt; &lt;code&gt;High Reasoning Effort: You have unlimited time to think and respond to the user’s question. There is no need to worry about reasoning time or associated costs. Your only goal is to arrive at a reliable, correct final answer. Feel free to explore the problem from multiple angles, and try various methods in your reasoning. This includes reflecting on reasoning by trying different approaches, verifying steps from different aspects, and rethinking your conclusions as needed. You are encouraged to take the time to analyze the problem thoroughly, reflect on your reasoning promptly and test all possible solutions. Only after a deep, comprehensive thought process should you provide the final answer, ensuring it is correct and well-supported by your reasoning.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Tried a High effort prompt and got some good results, may be someone would find them interesting.&lt;/p&gt; &lt;p&gt;Edit: fixed some copy-pasting issues in prompts.&lt;/p&gt; &lt;p&gt;Edit2: Seems there were some bad characters as as &lt;a href="/u/remixer_dec"&gt;u/remixer_dec&lt;/a&gt; noticed and &lt;a href="/u/spAnser"&gt;u/spAnser&lt;/a&gt; fixed. Thanks to them, post updated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/drrros"&gt; /u/drrros &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4v3fi/prompts_for_qwq32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4v3fi/prompts_for_qwq32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4v3fi/prompts_for_qwq32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T13:10:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j53cgv</id>
    <title>Introducing LogiLlama: A 1B-Parameter Open Source Model with Logical Reasoning</title>
    <updated>2025-03-06T19:09:50+00:00</updated>
    <author>
      <name>/u/Secret_Ad_6448</name>
      <uri>https://old.reddit.com/user/Secret_Ad_6448</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;We are a small team of engineers from the University of Toronto working to make smaller models smarter. LogiLlama is our first release, a 1B-parameter model fine-tuned from LLaMA with improved logical reasoning.&lt;/p&gt; &lt;p&gt;We are intending to open-source everything, including models, datasets, and training configs, and we would love your feedback on how it performs. Try it out, test its reasoning, and let us know what you think.&lt;/p&gt; &lt;p&gt;Check it out here: &lt;a href="https://huggingface.co/goppa-ai/Goppa-LogiLlama"&gt;Goppa-LogiLlama&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secret_Ad_6448"&gt; /u/Secret_Ad_6448 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j53cgv/introducing_logillama_a_1bparameter_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j53cgv/introducing_logillama_a_1bparameter_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j53cgv/introducing_logillama_a_1bparameter_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T19:09:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j51eya</id>
    <title>QwQ-32B is close to DeepSeek-R1 in Misguided Attention Benchmark, but there are issues with endless loops.</title>
    <updated>2025-03-06T17:50:59+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j51eya/qwq32b_is_close_to_deepseekr1_in_misguided/"&gt; &lt;img alt="QwQ-32B is close to DeepSeek-R1 in Misguided Attention Benchmark, but there are issues with endless loops." src="https://external-preview.redd.it/26jMr10jXXi4_I0JRPJzD_hB-aHllHxX-6HDNT9cHEo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0cfaa353996bc37c1479ca2028e44e2f676f0569" title="QwQ-32B is close to DeepSeek-R1 in Misguided Attention Benchmark, but there are issues with endless loops." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I evaluated QwQ-32b with 32k max tokens and the parameters provided in the QwQ model card. This was crucial to stabilize the issues a bit. I used openrouter with the paid model, which mostly defaulted to Groq.&lt;/p&gt; &lt;p&gt;Still, in several cases, QwQ entired infinite loops of &amp;quot;Hmm...&amp;quot; or simply would not stop the CoT despite the long evaluation window. In that case no result was returned and the prompt failed. I observed this behavior especially for logical problems without a solution. (Expected answer would have been to explain that the problem is unsolvable). Many other reasoning models have issues with these questions, but they usually terminate and return an answer.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4l2n02q4x3ne1.png?width=1205&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2c299937e6c3f5f4b46b41dcb007086be66e9d8"&gt;https://preview.redd.it/4l2n02q4x3ne1.png?width=1205&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2c299937e6c3f5f4b46b41dcb007086be66e9d8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Despite these issues, QwQ managed to beat o3-mini and is approaching the score of R1.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/cpldcpu/MisguidedAttention"&gt;Misguided Attention&lt;/a&gt; is a collection of prompts to challenge the reasoning abilities of large language models in presence of misguiding information. It consists of slightly modified well known logical problems and riddles. Many model are overfit to these problems and will therefore report a response to the unmodified problem.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/37j1ogpdw3ne1.png?width=2391&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c8b21a3c2d9dbb3187acbab33e17af49d2953a51"&gt;https://preview.redd.it/37j1ogpdw3ne1.png?width=2391&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c8b21a3c2d9dbb3187acbab33e17af49d2953a51&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qstmcqpew3ne1.png?width=4170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09254a0b0485b1b176a2cf08ce68e8d9524328a8"&gt;https://preview.redd.it/qstmcqpew3ne1.png?width=4170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09254a0b0485b1b176a2cf08ce68e8d9524328a8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j51eya/qwq32b_is_close_to_deepseekr1_in_misguided/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j51eya/qwq32b_is_close_to_deepseekr1_in_misguided/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j51eya/qwq32b_is_close_to_deepseekr1_in_misguided/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T17:50:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4vgk2</id>
    <title>Made a personal assistant that has acesss alot of tools, so wanted to share it (with github repo).</title>
    <updated>2025-03-06T13:29:37+00:00</updated>
    <author>
      <name>/u/FUS3N</name>
      <uri>https://old.reddit.com/user/FUS3N</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4vgk2/made_a_personal_assistant_that_has_acesss_alot_of/"&gt; &lt;img alt="Made a personal assistant that has acesss alot of tools, so wanted to share it (with github repo)." src="https://preview.redd.it/7js9zyilm2ne1.gif?width=640&amp;amp;crop=smart&amp;amp;s=0a20857aad361d43a64a623575b753839bc12b77" title="Made a personal assistant that has acesss alot of tools, so wanted to share it (with github repo)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FUS3N"&gt; /u/FUS3N &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7js9zyilm2ne1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4vgk2/made_a_personal_assistant_that_has_acesss_alot_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4vgk2/made_a_personal_assistant_that_has_acesss_alot_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T13:29:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j55tnf</id>
    <title>Anthropic warns White House about R1 and suggests "equipping the U.S. government with the capacity to rapidly evaluate whether future models—foreign or domestic—released onto the open internet internet possess security-relevant properties that merit national security attention"</title>
    <updated>2025-03-06T20:53:47+00:00</updated>
    <author>
      <name>/u/kristaller486</name>
      <uri>https://old.reddit.com/user/kristaller486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j55tnf/anthropic_warns_white_house_about_r1_and_suggests/"&gt; &lt;img alt="Anthropic warns White House about R1 and suggests &amp;quot;equipping the U.S. government with the capacity to rapidly evaluate whether future models—foreign or domestic—released onto the open internet internet possess security-relevant properties that merit national security attention&amp;quot;" src="https://external-preview.redd.it/4jL_cfkf1eOugH8Cd6gazHmY0nNfDF0kn5G0AyNVMU4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ec84648eddc4b833c73bd77f2df99dd7c97d0bf" title="Anthropic warns White House about R1 and suggests &amp;quot;equipping the U.S. government with the capacity to rapidly evaluate whether future models—foreign or domestic—released onto the open internet internet possess security-relevant properties that merit national security attention&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kristaller486"&gt; /u/kristaller486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.anthropic.com/news/anthropic-s-recommendations-ostp-u-s-ai-action-plan"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j55tnf/anthropic_warns_white_house_about_r1_and_suggests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j55tnf/anthropic_warns_white_house_about_r1_and_suggests/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T20:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4y0zy</id>
    <title>I really like the style of how QwQ represents code architecture. I haven't seen one draw it out like this.</title>
    <updated>2025-03-06T15:29:42+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4y0zy/i_really_like_the_style_of_how_qwq_represents/"&gt; &lt;img alt="I really like the style of how QwQ represents code architecture. I haven't seen one draw it out like this." src="https://preview.redd.it/66kml0c983ne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67a387d19a905d3863a6b2b7d6fb3c89f4cdaee0" title="I really like the style of how QwQ represents code architecture. I haven't seen one draw it out like this." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/66kml0c983ne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4y0zy/i_really_like_the_style_of_how_qwq_represents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4y0zy/i_really_like_the_style_of_how_qwq_represents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T15:29:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4wd9v</id>
    <title>Jamba 1.6 is out!</title>
    <updated>2025-03-06T14:14:28+00:00</updated>
    <author>
      <name>/u/inboundmage</name>
      <uri>https://old.reddit.com/user/inboundmage</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! Who is ready for another model release?&lt;/p&gt; &lt;p&gt;Let's welcome AI21 Labs Jamba 1.6 Release. Here is some information&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Beats models from Mistral, Meta &amp;amp; Cohere on quality &amp;amp; speed:&lt;/strong&gt; Jamba Large 1.6 outperforms Mistral Large 2, Llama 3.3 70B, and Command R+ on quality (Arena Hard), and Jamba Mini 1.6 outperforms Ministral 8B, Llama 3.1 8B, and Command R7.&lt;/li&gt; &lt;li&gt;Built with novel hybrid &lt;strong&gt;SSM-Transformer architecture&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long context performance:&lt;/strong&gt; With a context window of 256K, Jamba 1.6 outperforms Mistral, Llama, and Cohere on RAG and long context grounded question answering tasks (CRAG, HELMET RAG + HELMET LongQA, FinanceBench FullDoc, LongBench)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Private deployment: M&lt;/strong&gt;odel weights are available to download from &lt;a href="https://huggingface.co/ai21labs"&gt;Hugging Face&lt;/a&gt; under Jamba Open Model License to deploy privately on-prem or in-VPC&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; In addition to English, the models support Spanish, French, Portuguese, Italian, Dutch, German, Arabic and Hebrew&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Blog post: &lt;a href="https://www.ai21.com/blog/introducing-jamba-1-6/"&gt;https://www.ai21.com/blog/introducing-jamba-1-6/&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inboundmage"&gt; /u/inboundmage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4wd9v/jamba_16_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4wd9v/jamba_16_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4wd9v/jamba_16_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T14:14:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4zkiq</id>
    <title>QwQ-32B is now available on HuggingChat, unquantized and for free!</title>
    <updated>2025-03-06T16:35:11+00:00</updated>
    <author>
      <name>/u/SensitiveCranberry</name>
      <uri>https://old.reddit.com/user/SensitiveCranberry</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4zkiq/qwq32b_is_now_available_on_huggingchat/"&gt; &lt;img alt="QwQ-32B is now available on HuggingChat, unquantized and for free!" src="https://external-preview.redd.it/2kRkvRdd0EYvAJfhYjUZDeV5rTNSqMYr7S8BF5canLM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef0d71dc8e5fbfb1db3c11812ff15ced70a08c8f" title="QwQ-32B is now available on HuggingChat, unquantized and for free!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SensitiveCranberry"&gt; /u/SensitiveCranberry &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hf.co/chat/models/Qwen/QwQ-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4zkiq/qwq32b_is_now_available_on_huggingchat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4zkiq/qwq32b_is_now_available_on_huggingchat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T16:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4u57l</id>
    <title>Hunyuan Image to Video released!</title>
    <updated>2025-03-06T12:16:11+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4u57l/hunyuan_image_to_video_released/"&gt; &lt;img alt="Hunyuan Image to Video released!" src="https://external-preview.redd.it/bjcyc3R4bnc5Mm5lMSVRu4OBDxIXZycPsoc4EtwnK4B2nYL7URskxFmP5hp9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2734e75b58f76380d802fe5f62995c44dffdc8c0" title="Hunyuan Image to Video released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yck5cznw92ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4u57l/hunyuan_image_to_video_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4u57l/hunyuan_image_to_video_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T12:16:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j53w92</id>
    <title>Intro to DeepSeek's open-source week and why it's a big deal</title>
    <updated>2025-03-06T19:32:52+00:00</updated>
    <author>
      <name>/u/Brilliant-Day2748</name>
      <uri>https://old.reddit.com/user/Brilliant-Day2748</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j53w92/intro_to_deepseeks_opensource_week_and_why_its_a/"&gt; &lt;img alt="Intro to DeepSeek's open-source week and why it's a big deal" src="https://preview.redd.it/3dvsybsvf4ne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9eddab159fdcfb7e8ea6d001d8fc521b45a52638" title="Intro to DeepSeek's open-source week and why it's a big deal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brilliant-Day2748"&gt; /u/Brilliant-Day2748 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3dvsybsvf4ne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j53w92/intro_to_deepseeks_opensource_week_and_why_its_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j53w92/intro_to_deepseeks_opensource_week_and_why_its_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T19:32:52+00:00</published>
  </entry>
</feed>
