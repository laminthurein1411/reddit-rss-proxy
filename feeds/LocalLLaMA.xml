<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-01T21:23:28+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1joxzgf</id>
    <title>What is the best VLM for fine-tuning</title>
    <updated>2025-04-01T15:10:53+00:00</updated>
    <author>
      <name>/u/dethallica</name>
      <uri>https://old.reddit.com/user/dethallica</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I have a project where I have around 5000 of images of different scenarios and their explanations from industry experts with specialized jargon. I want to fine tune a VLM to (hopefully) create a generalizable solution to explain new images. &lt;/p&gt; &lt;p&gt;I want a VLM that is reasonably fast, open source (because the dataset is quite privacy sensitive) and easy to fine tune. I also really like how gemini can return bounding boxes with good quality but it's not a must for me.&lt;/p&gt; &lt;p&gt;I've seen some benchmarks such as &lt;a href="https://huggingface.co/spaces/opencompass/open_vlm_leaderboard"&gt;Open VLM Leaderboard&lt;/a&gt; but I want to know what you prefer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dethallica"&gt; /u/dethallica &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joxzgf/what_is_the_best_vlm_for_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joxzgf/what_is_the_best_vlm_for_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joxzgf/what_is_the_best_vlm_for_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T15:10:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jowhxj</id>
    <title>how many 3090 can i really connect to a Asus ProArt X670E Creator board?</title>
    <updated>2025-04-01T14:09:05+00:00</updated>
    <author>
      <name>/u/legodfader</name>
      <uri>https://old.reddit.com/user/legodfader</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, currently have 2 3090(one direct and one with pcie long cable) and a ssd on a m2 slot. using e-gpus or some other ways, what are some recommendation that i could use to add at least 1 more 3090 (or 2 if feasible)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/legodfader"&gt; /u/legodfader &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jowhxj/how_many_3090_can_i_really_connect_to_a_asus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jowhxj/how_many_3090_can_i_really_connect_to_a_asus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jowhxj/how_many_3090_can_i_really_connect_to_a_asus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T14:09:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jouhac</id>
    <title>Best current model for document analysis?</title>
    <updated>2025-04-01T12:34:56+00:00</updated>
    <author>
      <name>/u/Fant1xX</name>
      <uri>https://old.reddit.com/user/Fant1xX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We need to process sensitive documents locally and think about buying a 512GB M3 Ultra, what is the best current model to handle pdfs and images (image to text) on this kind of hardware? We could also split the text summarization and I2T into deperate models if there is no sensible multimodel.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fant1xX"&gt; /u/Fant1xX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jouhac/best_current_model_for_document_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jouhac/best_current_model_for_document_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jouhac/best_current_model_for_document_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T12:34:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jobe0u</id>
    <title>Benchmark: Dual-GPU boosts speed, despire all common internet wisdom. 2x RTX 5090 &gt; 1x H100, 2x RTX 4070 &gt; 1x RTX 4090 for QwQ-32B-AWQ. And the RTX 6000 Ada is overpriced.</title>
    <updated>2025-03-31T19:12:45+00:00</updated>
    <author>
      <name>/u/fxtentacle</name>
      <uri>https://old.reddit.com/user/fxtentacle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After yesterday's tests, I got the suggestion to test AWQ quants. And all over the internet I had repeatedly heard that dual-GPU setups won't help because they would not increase sequential speed. But the thing is: With vLLM, dual-GPU setups work anyway. I guess nobody told them ;)&lt;/p&gt; &lt;p&gt;In this benchmark set, the Time To First Token was below 0.1s in all cases, so I'm just going to ignore that. This race is all about the Output Tokens Per Second. And let's be honest, especially with a reasoning model like QwQ, those 4000 tokens of internal monologue is what we are waiting for and skipping the wait is all we care about. And, BTW, just like with my last benchmarking set, I am looking purely at 1-user setups here.&lt;/p&gt; &lt;p&gt;To nobody's surprise, the H100 80GB HBM3 again makes for great inference card with 78 OT/s. And the RTX 5090 is a beast with 65 OT/s, although it took me almost a day to get vLLM, flashInfer, and Nccl compiled just right for it to run stable enough to survive a 30 minute benchmark ... Still, the 5090 delivers 83% of a H100 at 10% the price.&lt;/p&gt; &lt;p&gt;Where things get surprising again is that 2x RTX 4070 TI SUPER actually outperform a RTX 4090 with 46 vs 43 OT/s. In line with that, 2x RTX 4080 also do well with 52 OT/s and they reach 80% of a 5090. My old RTX 3090 TI is also still very pleasant to use at 40 OT/s - which is a respectable 61% of the speed a shiny new 5090 would deliver.&lt;/p&gt; &lt;p&gt;The pricey RTX 6000 Ada completely disappoints with 42 OT/s, so it's only marginally faster than the 3090 TI and way behind a dual-4070 setup.&lt;/p&gt; &lt;p&gt;And what's truly cool is to see how well the 5090 can use additional RAM for speeding up the attention kernels. That's why 2x RTX 5090 outperforms even the mighty H100 by a small margin. That's 30,000‚Ç¨ performance for 5,718‚Ç¨.&lt;/p&gt; &lt;p&gt;Here's the new result table: &lt;a href="https://github.com/DeutscheKI/llm-performance-tests#qwq-32b-awq"&gt;https://github.com/DeutscheKI/llm-performance-tests#qwq-32b-awq&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT: I've added 4x 4090. It beats the H100 with +14% and it beats 2x 5090 with +12%.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fxtentacle"&gt; /u/fxtentacle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jobe0u/benchmark_dualgpu_boosts_speed_despire_all_common/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jobe0u/benchmark_dualgpu_boosts_speed_despire_all_common/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jobe0u/benchmark_dualgpu_boosts_speed_despire_all_common/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T19:12:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1joolgd</id>
    <title>v0.7.3 Update: Dive, An Open Source MCP Agent Desktop</title>
    <updated>2025-04-01T05:57:19+00:00</updated>
    <author>
      <name>/u/BigGo_official</name>
      <uri>https://old.reddit.com/user/BigGo_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joolgd/v073_update_dive_an_open_source_mcp_agent_desktop/"&gt; &lt;img alt="v0.7.3 Update: Dive, An Open Source MCP Agent Desktop" src="https://external-preview.redd.it/dHNhenhkNHl4NXNlMTttH9zdS9ChVet3GQR3TBIWR7IxCLqFWnntBfHnnCr8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=345f5f2c27599200502d7f8d304adbebf35fbbe2" title="v0.7.3 Update: Dive, An Open Source MCP Agent Desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is currently the easiest way to install MCP Server.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BigGo_official"&gt; /u/BigGo_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r9uvwa4yx5se1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joolgd/v073_update_dive_an_open_source_mcp_agent_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joolgd/v073_update_dive_an_open_source_mcp_agent_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T05:57:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jopcyr</id>
    <title>GPT 4o is not actually omni-modal</title>
    <updated>2025-04-01T06:51:45+00:00</updated>
    <author>
      <name>/u/kuzheren</name>
      <uri>https://old.reddit.com/user/kuzheren</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://chatgpt.com/share/67eb9fc8-458c-8007-85ad-46be9aa56519"&gt;https://chatgpt.com/share/67eb9fc8-458c-8007-85ad-46be9aa56519&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Wanted to share this here - I haven‚Äôt seen much discussion about it, and I hope it could be helpful to the LocalLLaMA community.&lt;/p&gt; &lt;p&gt;(Also, let‚Äôs define &lt;em&gt;omni-modal&lt;/em&gt; as multimodal models that support both understanding and generation across different modalities. This definition might not be perfect, but we need some way to distinguish models with multimodal decoding capabilities from those without)&lt;/p&gt; &lt;p&gt;As we know, the new GPT-4o model is highly context-aware. It can reference both images and previous user conversation. At first glance, it might seem like GPT-4o generates image tokens directly based on the full context, without relying on any external tools. But that‚Äôs not exactly how it works.&lt;/p&gt; &lt;p&gt;Image generation still relies on a new version of DALL¬∑E (at least it‚Äôs still referred to by that name), and it happens through a function call like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;image_gen.text2im { &amp;quot;prompt&amp;quot;: &amp;quot;A photorealistic owl sitting on a branch at night&amp;quot;, &amp;quot;size&amp;quot;: &amp;quot;1024x1024&amp;quot;, &amp;quot;n&amp;quot;: 1, &amp;quot;referenced_image_ids&amp;quot;: [&amp;quot;file_0000000054d45230be886096390c241a&amp;quot;], // optional &amp;quot;transparent_background&amp;quot;: false // optional } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As we can see, the process still uses an explicit API-style call. GPT writes the prompt and optionally includes image references, allowing the image generator to use much more context than DALL¬∑E 3 ever could.&lt;/p&gt; &lt;p&gt;Compare this to models like open-source OmniGen or Gemini 2.0 Flash - these do &lt;strong&gt;not&lt;/strong&gt; rely on external function calls. Instead, they generate images directly, using both text and image inputs as unified context. That‚Äôs why I‚Äôd say they‚Äôre &lt;em&gt;truly&lt;/em&gt; omni-modal.&lt;/p&gt; &lt;p&gt;One more detail: after the image is generated, GPT only sees a &lt;strong&gt;textual description&lt;/strong&gt; of the result ‚Äî not the actual image itself (unless it was user-uploaded). This means GPT-4o wasn't retrained to ‚Äúsee‚Äù its own generated images.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; GPT-4o doesn‚Äôt generate image tokens directly. It calls a separate, more advanced image model (a new DALL¬∑E version) that can handle reference images. The models are still modular, not unified.&lt;/p&gt; &lt;p&gt;Please don't k#ll me for this post. I know it might sound obvious, boring, or lame, but nobody seems to be talking about it, and many people assume the image generator is somehow merged into GPT itself - which is not the case.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kuzheren"&gt; /u/kuzheren &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jopcyr/gpt_4o_is_not_actually_omnimodal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jopcyr/gpt_4o_is_not_actually_omnimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jopcyr/gpt_4o_is_not_actually_omnimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T06:51:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp31pu</id>
    <title>I dove into MCP and how it can benefit from orchestration frameworks!</title>
    <updated>2025-04-01T18:33:09+00:00</updated>
    <author>
      <name>/u/ComfortableArm121</name>
      <uri>https://old.reddit.com/user/ComfortableArm121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spent some time writing about MCP (Model Context Protocol) and how it enables LLMs to talk to tools (like the Babel Fish in The Hitchhiker's Guide to the Galaxy).&lt;/p&gt; &lt;p&gt;Here's the synergy:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MCP:&lt;/strong&gt; Handles the &lt;em&gt;standardized communication&lt;/em&gt; with any tool.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Orchestration:&lt;/strong&gt; Manages the agent's &lt;em&gt;internal plan/logic&lt;/em&gt; ‚Äì deciding &lt;em&gt;when&lt;/em&gt; to use MCP, process data, or take other steps.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Together, you can build more complex, tool-using agents!&lt;/p&gt; &lt;p&gt;Attaching a link to the blog &lt;a href="https://theaiworld.substack.com/p/the-ai-babel-fish-mcp-pocketflow?r=65jr5&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;triedRedirect=true"&gt;here&lt;/a&gt;. Would love your thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComfortableArm121"&gt; /u/ComfortableArm121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp31pu/i_dove_into_mcp_and_how_it_can_benefit_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp31pu/i_dove_into_mcp_and_how_it_can_benefit_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp31pu/i_dove_into_mcp_and_how_it_can_benefit_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T18:33:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp6iy1</id>
    <title>Can someone ELI5 how I'd know which model is right when I've found the desired model?</title>
    <updated>2025-04-01T20:51:41+00:00</updated>
    <author>
      <name>/u/intimate_sniffer69</name>
      <uri>https://old.reddit.com/user/intimate_sniffer69</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp6iy1/can_someone_eli5_how_id_know_which_model_is_right/"&gt; &lt;img alt="Can someone ELI5 how I'd know which model is right when I've found the desired model?" src="https://preview.redd.it/iuruxe1cdase1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad37a6e66945133765d6f590a7945f097ecc6bee" title="Can someone ELI5 how I'd know which model is right when I've found the desired model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a data scientist for work, and finally getting around to experimenting with local LLMs in LM studio and Msty AI just for fun SFW purposes. However, I'm unsure which model version I need once I found one. My data science work is mostly NLP and regression, model building. I have zero experience with building out LLMs like this, but I did read a pretty thorough guide. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/intimate_sniffer69"&gt; /u/intimate_sniffer69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iuruxe1cdase1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp6iy1/can_someone_eli5_how_id_know_which_model_is_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp6iy1/can_someone_eli5_how_id_know_which_model_is_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T20:51:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp6sbn</id>
    <title>Dou (ÈÅì) updated with LM Studio (and Ollama) support</title>
    <updated>2025-04-01T21:02:13+00:00</updated>
    <author>
      <name>/u/shokuninstudio</name>
      <uri>https://old.reddit.com/user/shokuninstudio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp6sbn/dou_ÈÅì_updated_with_lm_studio_and_ollama_support/"&gt; &lt;img alt="Dou (ÈÅì) updated with LM Studio (and Ollama) support" src="https://preview.redd.it/1i6mjuscfase1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6961bce4f75454ecad32ef613c80eef4630f8da9" title="Dou (ÈÅì) updated with LM Studio (and Ollama) support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shokuninstudio"&gt; /u/shokuninstudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1i6mjuscfase1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp6sbn/dou_ÈÅì_updated_with_lm_studio_and_ollama_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp6sbn/dou_ÈÅì_updated_with_lm_studio_and_ollama_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T21:02:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp77z1</id>
    <title>Arch-Function-Chat (1B/3B/7B) - Device friendly, family of fast LLMs for function calling scenarios now trained to chat.</title>
    <updated>2025-04-01T21:20:18+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on feedback from users and the developer community that used Arch-Function (our previous gen) model, I am excited to share our latest work: &lt;a href="https://huggingface.co/katanemo/Arch-Function-Chat-3B"&gt;Arch-Function-Chat&lt;/a&gt; A collection of fast, device friendly LLMs that achieve performance on-par with GPT-4 on function calling, now trained to chat.&lt;/p&gt; &lt;p&gt;These LLMs have three additional training objectives.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Be able to refine and clarify the user request. This means to ask for required function parameters, clarify ambiguous input ( (e.g., &amp;quot;Transfer $500&amp;quot; without specifying accounts, can be ‚ÄúTransfer from‚Äù and ‚ÄúTransfer to‚Äù)&lt;/li&gt; &lt;li&gt;Accurately maintain context in two specific scenarios: &lt;ol&gt; &lt;li&gt;Progressive information disclosure such as in multi-turn conversations where information is revealed gradually (i.e., the model asks info of multiple parameters and the user only answers one or two instead of all the info)&lt;/li&gt; &lt;li&gt;Context switch where the model must infer missing parameters from context (e.g., &amp;quot;Check the weather&amp;quot; should prompt for location if not provided) and maintains context between turns (e.g., &amp;quot;What about tomorrow?&amp;quot; after a weather query but still in the middle of clarification)&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Respond to the user based on executed tools results. For common function calling scenarios where the response of the execution is all that's needed to complete the user request, Arch-Function-Chat can interpret and respond to the user via chat. Note, parallel and multiple function calling was already supported so if the model needs to respond based on multiple tools call it still can.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Of course the 3B model will now be the primary LLM used in &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;. Hope you all like the work üôè. Happy building!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp77z1/archfunctionchat_1b3b7b_device_friendly_family_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp77z1/archfunctionchat_1b3b7b_device_friendly_family_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp77z1/archfunctionchat_1b3b7b_device_friendly_family_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T21:20:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jonibh</id>
    <title>OpenWebUI Adopt OpenAPI and offer an MCP bridge</title>
    <updated>2025-04-01T04:45:39+00:00</updated>
    <author>
      <name>/u/coding_workflow</name>
      <uri>https://old.reddit.com/user/coding_workflow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open Web Ui 0.6 is adoption OpenAPI instead of MCP but offer a bridge.&lt;br /&gt; Release notes: &lt;a href="https://github.com/open-webui/open-webui/releases"&gt;https://github.com/open-webui/open-webui/releases&lt;/a&gt;&lt;br /&gt; MCO Bridge: &lt;a href="https://github.com/open-webui/mcpo"&gt;https://github.com/open-webui/mcpo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coding_workflow"&gt; /u/coding_workflow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jonibh/openwebui_adopt_openapi_and_offer_an_mcp_bridge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jonibh/openwebui_adopt_openapi_and_offer_an_mcp_bridge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jonibh/openwebui_adopt_openapi_and_offer_an_mcp_bridge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T04:45:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jobybk</id>
    <title>OpenAI is open-sourcing a model soon</title>
    <updated>2025-03-31T19:36:01+00:00</updated>
    <author>
      <name>/u/MysteriousPayment536</name>
      <uri>https://old.reddit.com/user/MysteriousPayment536</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI is taking feedback for open source model. They will probably release o3-mini based on a poll by Sam Altman in February. &lt;a href="https://x.com/sama/status/1891667332105109653"&gt;https://x.com/sama/status/1891667332105109653&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MysteriousPayment536"&gt; /u/MysteriousPayment536 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openai.com/open-model-feedback/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jobybk/openai_is_opensourcing_a_model_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jobybk/openai_is_opensourcing_a_model_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T19:36:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp1sy8</id>
    <title>Smallest model capable of detecting profane/nsfw language?</title>
    <updated>2025-04-01T17:44:27+00:00</updated>
    <author>
      <name>/u/ohcrap___fk</name>
      <uri>https://old.reddit.com/user/ohcrap___fk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I have my first ever steam game about to be released in a week which I couldn't be more excited/nervous about. It is a singleplayer game but I have a global chat that allows people to talk to other people playing. It's a space game, and space is lonely, so I thought that'd be a fun aesthetic.&lt;/p&gt; &lt;p&gt;Anyways, it is in beta-testing phase right now and I had to ban someone for the first time today because of things they were saying over chat. It was a manual process and I'd like to automate the detection/flagging of unsavory messages.&lt;/p&gt; &lt;p&gt;Are &amp;lt;1b parameter models capable of outperforming a simple keyword check? I like the idea of an LLM because it could go beyond matching strings.&lt;/p&gt; &lt;p&gt;Also, if anyone is interested in trying it out, I'm handing out keys like crazy because I'm too nervous to charge $2.99 for the game and then underdeliver. Game info &lt;a href="https://x.com/hvent90"&gt;here&lt;/a&gt;, sorry for the self-promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ohcrap___fk"&gt; /u/ohcrap___fk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp1sy8/smallest_model_capable_of_detecting_profanensfw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp1sy8/smallest_model_capable_of_detecting_profanensfw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp1sy8/smallest_model_capable_of_detecting_profanensfw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T17:44:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jojuf4</id>
    <title>Is everyone ready for all of the totally legit AI tools &amp; models being released tomorrow?</title>
    <updated>2025-04-01T01:25:55+00:00</updated>
    <author>
      <name>/u/C_Coffie</name>
      <uri>https://old.reddit.com/user/C_Coffie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I heard Llama 4 is finally coming tomorrow!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/C_Coffie"&gt; /u/C_Coffie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jojuf4/is_everyone_ready_for_all_of_the_totally_legit_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jojuf4/is_everyone_ready_for_all_of_the_totally_legit_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jojuf4/is_everyone_ready_for_all_of_the_totally_legit_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T01:25:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1joxqul</id>
    <title>Tenstorrent's Big Quiet Box of AI</title>
    <updated>2025-04-01T15:01:14+00:00</updated>
    <author>
      <name>/u/muchcharles</name>
      <uri>https://old.reddit.com/user/muchcharles</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joxqul/tenstorrents_big_quiet_box_of_ai/"&gt; &lt;img alt="Tenstorrent's Big Quiet Box of AI" src="https://external-preview.redd.it/Wq00H3F0hVMmkafgNYhCQdrVKy3_PQGmzAF1Qji8D1c.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2780c866e0ac3e9f4c44759d99f39bf0d304170d" title="Tenstorrent's Big Quiet Box of AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muchcharles"&gt; /u/muchcharles &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://m.youtube.com/watch?v=vWw-1bk7k2c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joxqul/tenstorrents_big_quiet_box_of_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joxqul/tenstorrents_big_quiet_box_of_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T15:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1jogfrz</id>
    <title>Open-source search repo beats GPT-4o Search, Perplexity Sonar Reasoning Pro on FRAMES</title>
    <updated>2025-03-31T22:42:26+00:00</updated>
    <author>
      <name>/u/jiMalinka</name>
      <uri>https://old.reddit.com/user/jiMalinka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jogfrz/opensource_search_repo_beats_gpt4o_search/"&gt; &lt;img alt="Open-source search repo beats GPT-4o Search, Perplexity Sonar Reasoning Pro on FRAMES" src="https://preview.redd.it/q2nifllfs3se1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=734a6613dfcc4ffda59b820ed615cb2ac184b109" title="Open-source search repo beats GPT-4o Search, Perplexity Sonar Reasoning Pro on FRAMES" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/sentient-agi/OpenDeepSearch"&gt;https://github.com/sentient-agi/OpenDeepSearch&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Pretty simple to plug-and-play ‚Äì nice combo of techniques (react / codeact / dynamic few-shot) integrated with search / calculator tools. I guess that‚Äôs all you need to beat SOTA billion dollar search companies :) Probably would be super interesting / useful to use with multi-agent workflows too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jiMalinka"&gt; /u/jiMalinka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q2nifllfs3se1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jogfrz/opensource_search_repo_beats_gpt4o_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jogfrz/opensource_search_repo_beats_gpt4o_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T22:42:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp5g2l</id>
    <title>Is a multimodal focused release from openai the best for us?</title>
    <updated>2025-04-01T20:08:29+00:00</updated>
    <author>
      <name>/u/AryanEmbered</name>
      <uri>https://old.reddit.com/user/AryanEmbered</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp5g2l/is_a_multimodal_focused_release_from_openai_the/"&gt; &lt;img alt="Is a multimodal focused release from openai the best for us?" src="https://preview.redd.it/w31a75fy5ase1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf8159e2e72f93f2c1e7edfd8a2bb4a73c81275c" title="Is a multimodal focused release from openai the best for us?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like with the exception of Qwen 2.5 7b(11b) audio, we have seen almost no real progress in multimodality so far in open models.&lt;/p&gt; &lt;p&gt;It seems gippty 4o mini can now do advanced voice mode as well. &lt;/p&gt; &lt;p&gt;They keep saying its a model that can run on your hardware, and 4omini is estimated to be less than a 20B model consider how badly it gets mogged by mistral smol and others. &lt;/p&gt; &lt;p&gt;It would be great if we can get a shittier 4o mini but with all the features intact like audio and image output. (A llamalover can dream)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AryanEmbered"&gt; /u/AryanEmbered &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w31a75fy5ase1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp5g2l/is_a_multimodal_focused_release_from_openai_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp5g2l/is_a_multimodal_focused_release_from_openai_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T20:08:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1joyigi</id>
    <title>GemmaCoder3-12b: Fine-Tuning Gemma 3 for Code Reasoning</title>
    <updated>2025-04-01T15:32:33+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joyigi/gemmacoder312b_finetuning_gemma_3_for_code/"&gt; &lt;img alt="GemmaCoder3-12b: Fine-Tuning Gemma 3 for Code Reasoning" src="https://external-preview.redd.it/gH2ta8Ny0Bg1Qm8qZdfZlafv4Sz_L1pzxh-y3yKJtZ8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2914dd3fff11503f8f5f868b03abfbe2d8a5ee73" title="GemmaCoder3-12b: Fine-Tuning Gemma 3 for Code Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/burtenshaw/google-gemma3-gemma-code"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joyigi/gemmacoder312b_finetuning_gemma_3_for_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joyigi/gemmacoder312b_finetuning_gemma_3_for_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T15:32:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp5y5a</id>
    <title>Different LLM models make different sounds from the GPU when doing inference</title>
    <updated>2025-04-01T20:28:29+00:00</updated>
    <author>
      <name>/u/vibjelo</name>
      <uri>https://old.reddit.com/user/vibjelo</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibjelo"&gt; /u/vibjelo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://bsky.app/profile/victor.earth/post/3llrphluwb22p"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp5y5a/different_llm_models_make_different_sounds_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp5y5a/different_llm_models_make_different_sounds_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T20:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1josy27</id>
    <title>An idea: an LLM trapped in the past</title>
    <updated>2025-04-01T11:09:36+00:00</updated>
    <author>
      <name>/u/Vehnum</name>
      <uri>https://old.reddit.com/user/Vehnum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone ever thought to make an LLM trained on data from before a certain year/time?&lt;/p&gt; &lt;p&gt;For example, an LLM trained on data only from 2010 or prior.&lt;/p&gt; &lt;p&gt;I thought it was an interesting concept but I don‚Äôt know if it had been thought of or done before. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vehnum"&gt; /u/Vehnum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1josy27/an_idea_an_llm_trapped_in_the_past/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1josy27/an_idea_an_llm_trapped_in_the_past/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1josy27/an_idea_an_llm_trapped_in_the_past/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T11:09:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1joyl9t</id>
    <title>New GGUF quants of V3-0324</title>
    <updated>2025-04-01T15:35:49+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joyl9t/new_gguf_quants_of_v30324/"&gt; &lt;img alt="New GGUF quants of V3-0324" src="https://external-preview.redd.it/VVDuLhNJdXUv9Ha7btms0J33I6ffqYD7axOIbyejSC4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb260f4a149e4b5107b97b86ee6df9cf84939894" title="New GGUF quants of V3-0324" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I cooked up these fresh new quants on &lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;ikawrakow/ik_llama.cpp&lt;/a&gt; supporting 32k+ context in under 24GB VRAM with MLA with highest quality tensors for attention/dense layers/shared experts.&lt;/p&gt; &lt;p&gt;Good both for CPU+GPU or CPU only rigs with optimized repacked quant flavours to get the most out of your RAM.&lt;/p&gt; &lt;p&gt;&lt;em&gt;NOTE&lt;/em&gt;: These quants only work with &lt;code&gt;ik_llama.cpp&lt;/code&gt; fork and won't work with mainline llama.cpp, ollama, lm studio, koboldcpp, etc.&lt;/p&gt; &lt;p&gt;Shout out to &lt;a href="https://www.youtube.com/c/level1techs"&gt;level1techs&lt;/a&gt; for supporting this research on some sweet hardware rigs!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/DeepSeek-V3-0324-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joyl9t/new_gguf_quants_of_v30324/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joyl9t/new_gguf_quants_of_v30324/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T15:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1joqnp0</id>
    <title>Top reasoning LLMs failed horribly on USA Math Olympiad (maximum 5% score)</title>
    <updated>2025-04-01T08:28:37+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joqnp0/top_reasoning_llms_failed_horribly_on_usa_math/"&gt; &lt;img alt="Top reasoning LLMs failed horribly on USA Math Olympiad (maximum 5% score)" src="https://preview.redd.it/lbaxwpako6se1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fe2ebb66027a7c9112a0c9566eaf397ca2d5a18" title="Top reasoning LLMs failed horribly on USA Math Olympiad (maximum 5% score)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need to share something that‚Äôs blown my mind today. I just came across &lt;a href="https://arxiv.org/abs/2503.21934v1"&gt;this paper &lt;/a&gt;evaluating state-of-the-art LLMs (like O3-MINI, Claude 3.7, etc.) on the 2025 USA Mathematical Olympiad (USAMO). And let me tell you‚Äîthis is &lt;em&gt;wild&lt;/em&gt; .&lt;/p&gt; &lt;h1&gt;The Results&lt;/h1&gt; &lt;p&gt;These models were tested on &lt;strong&gt;six proof-based math problems&lt;/strong&gt; from the 2025 USAMO. Each problem was scored out of 7 points, with a max total score of 42. Human experts graded their solutions rigorously.&lt;/p&gt; &lt;p&gt;The highest average score achieved by &lt;strong&gt;any model&lt;/strong&gt; ? &lt;strong&gt;Less than 5%.&lt;/strong&gt; Yes, you read that right: &lt;strong&gt;5%.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Even worse, when these models tried grading their own work (e.g., O3-MINI and Claude 3.7), they consistently &lt;strong&gt;overestimated their scores&lt;/strong&gt; , inflating them by up to &lt;strong&gt;20x&lt;/strong&gt; compared to human graders.&lt;/p&gt; &lt;h1&gt;Why This Matters&lt;/h1&gt; &lt;p&gt;These models have been trained on &lt;strong&gt;all the math data imaginable&lt;/strong&gt; ‚ÄîIMO problems, USAMO archives, textbooks, papers, etc. They‚Äôve seen it all. Yet, they struggle with tasks requiring deep logical reasoning, creativity, and rigorous proofs.&lt;/p&gt; &lt;p&gt;Here are some key issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Logical Failures&lt;/strong&gt; : Models made unjustified leaps in reasoning or labeled critical steps as &amp;quot;trivial.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lack of Creativity&lt;/strong&gt; : Most models stuck to the same flawed strategies repeatedly, failing to explore alternatives.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grading Failures&lt;/strong&gt; : Automated grading by LLMs inflated scores dramatically, showing they can't even evaluate their own work reliably.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Given that billions of dollars have been poured into investments on these models with the hope of it can &amp;quot;generalize&amp;quot; and do &amp;quot;crazy lift&amp;quot; in human knowledge, this result is shocking. Given the models here are probably trained on all Olympiad data previous (USAMO, IMO ,... anything)&lt;/p&gt; &lt;p&gt;Link to the paper: &lt;a href="https://arxiv.org/abs/2503.21934v1"&gt;https://arxiv.org/abs/2503.21934v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lbaxwpako6se1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joqnp0/top_reasoning_llms_failed_horribly_on_usa_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joqnp0/top_reasoning_llms_failed_horribly_on_usa_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T08:28:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jotzue</id>
    <title>Just upgraded my RTX 3060 with 192GB of VRAM</title>
    <updated>2025-04-01T12:09:43+00:00</updated>
    <author>
      <name>/u/Wrong_User_Logged</name>
      <uri>https://old.reddit.com/user/Wrong_User_Logged</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jotzue/just_upgraded_my_rtx_3060_with_192gb_of_vram/"&gt; &lt;img alt="Just upgraded my RTX 3060 with 192GB of VRAM" src="https://a.thumbs.redditmedia.com/0HRndElj4m4MdUTfleIWN0cAk58xxJsG5xvLYGxCDg0.jpg" title="Just upgraded my RTX 3060 with 192GB of VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Soldered in some extra memory chips I had lying around. Runs now Deepseek R1 with 1.6 bits at 8 t/s.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rzmtxp5gs7se1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=68bbae0f177ee26b9e9dd5d80ced43ca1ab364b8"&gt;https://preview.redd.it/rzmtxp5gs7se1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=68bbae0f177ee26b9e9dd5d80ced43ca1ab364b8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong_User_Logged"&gt; /u/Wrong_User_Logged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jotzue/just_upgraded_my_rtx_3060_with_192gb_of_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jotzue/just_upgraded_my_rtx_3060_with_192gb_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jotzue/just_upgraded_my_rtx_3060_with_192gb_of_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T12:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1joy1g9</id>
    <title>You can now check if your Laptop/ Rig can run a GGUF directly from Hugging Face! ü§ó</title>
    <updated>2025-04-01T15:13:10+00:00</updated>
    <author>
      <name>/u/vaibhavs10</name>
      <uri>https://old.reddit.com/user/vaibhavs10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joy1g9/you_can_now_check_if_your_laptop_rig_can_run_a/"&gt; &lt;img alt="You can now check if your Laptop/ Rig can run a GGUF directly from Hugging Face! ü§ó" src="https://external-preview.redd.it/cjl0NGVwNTJwOHNlMcYNeeStsI4th9K4vfQkpXTEQka5SvAFbcRXwVJ4maQB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d009dc08fd59bef372f2ca0785fa2ef200fe3ea8" title="You can now check if your Laptop/ Rig can run a GGUF directly from Hugging Face! ü§ó" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaibhavs10"&gt; /u/vaibhavs10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0bo4dp52p8se1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joy1g9/you_can_now_check_if_your_laptop_rig_can_run_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joy1g9/you_can_now_check_if_your_laptop_rig_can_run_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T15:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp1555</id>
    <title>DeepMind will delay sharing research to remain competitive</title>
    <updated>2025-04-01T17:17:47+00:00</updated>
    <author>
      <name>/u/mayalihamur</name>
      <uri>https://old.reddit.com/user/mayalihamur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;a href="https://archive.ph/tkuum"&gt;recent report&lt;/a&gt; in Financial Times claims that Google's DeepMind &amp;quot;has been holding back the release of its world-renowned research&amp;quot; to remain competitive. Accordingly the company will adopt a six-month embargo policy &amp;quot;before strategic papers related to generative AI are released&amp;quot;. &lt;/p&gt; &lt;p&gt;In an interesting statement, a DeepMind researcher said he could &amp;quot;not imagine us putting out the transformer papers for general use now&amp;quot;. Considering the impact of the DeepMind's transformer research on the development of LLMs, just think where we would have been now if they held back the research. The report also claims that some DeepMind staff left the company as their careers would be negatively affected if they are not allowed to publish their research. &lt;/p&gt; &lt;p&gt;I don't have any knowledge about the current impact of DeepMind's open research contributions. But just a couple of months ago we have been talking about the potential contributions the DeepSeek release will make. But as it gets competitive it looks like the big players are slowly becoming &lt;del&gt;Open&lt;/del&gt;ClosedAIs. &lt;/p&gt; &lt;p&gt;Too bad, let's hope that this won't turn into a general trend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mayalihamur"&gt; /u/mayalihamur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T17:17:47+00:00</published>
  </entry>
</feed>
