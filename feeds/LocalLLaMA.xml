<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-30T21:05:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1idrzhz</id>
    <title>"Low-Cost" 70b 8-bit inference rig.</title>
    <updated>2025-01-30T17:15:38+00:00</updated>
    <author>
      <name>/u/koalfied-coder</name>
      <uri>https://old.reddit.com/user/koalfied-coder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idrzhz/lowcost_70b_8bit_inference_rig/"&gt; &lt;img alt="&amp;quot;Low-Cost&amp;quot; 70b 8-bit inference rig." src="https://external-preview.redd.it/00dKHdq21sJRX1oSrcbJ8tTfRNH4p_MaFaor6S4Sbck.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9dd3564bdfc16966a6ab1d540c005730c11012bc" title="&amp;quot;Low-Cost&amp;quot; 70b 8-bit inference rig." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Thank you for viewing my best attempt at a reasonably priced 70b 8-bit inference rig.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I appreciate everyone's input on my sanity check post as it has yielded greatness. :)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Inspiration:&lt;/strong&gt; &lt;a href="https://towardsdatascience.com/how-to-build-a-multi-gpu-system-for-deep-learning-in-2023-e5bbb905d935"&gt;Towards Data Science Article&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Build Details and Costs:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;Low Cost&amp;quot; Necessities:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Intel Xeon W-2155 10-Core - $167.43 (used)&lt;/li&gt; &lt;li&gt;ASUS WS C422 SAGE/10G Intel C422 MOBO - $362.16 (open-box)&lt;/li&gt; &lt;li&gt;EVGA Supernova 1600 P+ - $285.36 (new)&lt;/li&gt; &lt;li&gt;(256GB) Micron (8x32GB) 2Rx4 PC4-2400T RDIMM - $227.28&lt;/li&gt; &lt;li&gt;PNY RTX A5000 GPU X4 - ~$5,596.68 (open-box)&lt;/li&gt; &lt;li&gt;Micron 7450 PRO 960 GB - ~$200 (on hand)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Personal Selections, Upgrades, and Additions:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SilverStone Technology RM44 Chassis - $319.99 (new) (Best 8 PCIE slot case IMO)&lt;/li&gt; &lt;li&gt;Noctua NH-D9DX i4 3U, Premium CPU Cooler - $59.89 (new)&lt;/li&gt; &lt;li&gt;Noctua NF-A12x25 PWM X3 - $98.76 (new)&lt;/li&gt; &lt;li&gt;Seagate Barracuda 3TB ST3000DM008 7200RPM 3.5&amp;quot; SATA Hard Drive HDD - $63.20 (new)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Total w/ GPUs:&lt;/strong&gt; ~$7,350&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Issues:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RAM issues. It seems they must be paired and it was picky needing Micron.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key Gear Reviews:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Silverstone Chassis:&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Truly a pleasure to build and work in. Cannot say enough how smart the design is. No issues.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Noctua Gear:&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;All excellent and quiet with a pleasing noise at load. I mean, it's Noctua.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Basic Benchmarks&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;~27 t/s non concurrent&lt;br /&gt; ~120 t/s concurrent&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Non-concurrent&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;**Input command:**Copy code python token_benchmark_ray.py --model &amp;quot;cortecs/Llama-3.3-70B-Instruct-FP8-Dynamic&amp;quot; --mean-input-tokens 550 --stddev-input-tokens 150 --mean-output-tokens 150 --stddev-output-tokens 10 --max-num-completed-requests 10 --timeout 600 --num-concurrent-requests 1 --results-dir &amp;quot;result_outputs&amp;quot; --llm-api openai --additional-sampling-params '{}'&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Result:&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Number Of Errored Requests: 0&lt;/li&gt; &lt;li&gt;Overall Output Throughput: 26.933382788310297&lt;/li&gt; &lt;li&gt;Number Of Completed Requests: 10&lt;/li&gt; &lt;li&gt;Completed Requests Per Minute: 9.439269668800337&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Concurrent&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;**Input command:**Copy code python token_benchmark_ray.py --model &amp;quot;cortecs/Llama-3.3-70B-Instruct-FP8-Dynamic&amp;quot; --mean-input-tokens 550 --stddev-input-tokens 150 --mean-output-tokens 150 --stddev-output-tokens 10 --max-num-completed-requests 100 --timeout 600 --num-concurrent-requests 16 --results-dir &amp;quot;result_outputs&amp;quot; --llm-api openai --additional-sampling-params '{}'&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Result:&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Number Of Errored Requests: 0&lt;/li&gt; &lt;li&gt;Overall Output Throughput: 120.43197653058412&lt;/li&gt; &lt;li&gt;Number Of Completed Requests: 100&lt;/li&gt; &lt;li&gt;Completed Requests Per Minute: 40.81286976467126&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Built a cost-effective 70b 8-bit inference rig with some open-box and used parts. Faced RAM compatibility issues but achieved satisfactory build quality and performance benchmarks. Total cost with GPUs is approximately $7,350.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8gl5mv7dz5ge1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6ce96fa726c7626d414b078c9ba4a2a2d9931ee1"&gt;https://preview.redd.it/8gl5mv7dz5ge1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6ce96fa726c7626d414b078c9ba4a2a2d9931ee1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hh8ngdwez5ge1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=171bb79ce7314c36d7649cfa4ff2b8741f48a5df"&gt;https://preview.redd.it/hh8ngdwez5ge1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=171bb79ce7314c36d7649cfa4ff2b8741f48a5df&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/oje989wez5ge1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=03288135cf349f6006ab41c5de00d4083e4b8bca"&gt;https://preview.redd.it/oje989wez5ge1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=03288135cf349f6006ab41c5de00d4083e4b8bca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uv1lfbwez5ge1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=bd8c02e8bd46fec360dbaeecf90f0566ebd51b95"&gt;https://preview.redd.it/uv1lfbwez5ge1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=bd8c02e8bd46fec360dbaeecf90f0566ebd51b95&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/koalfied-coder"&gt; /u/koalfied-coder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idrzhz/lowcost_70b_8bit_inference_rig/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idrzhz/lowcost_70b_8bit_inference_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idrzhz/lowcost_70b_8bit_inference_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T17:15:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1idny30</id>
    <title>mistralai/Mistral-Small-24B-Instruct-2501 路 Hugging Face</title>
    <updated>2025-01-30T14:17:54+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idny30/mistralaimistralsmall24binstruct2501_hugging_face/"&gt; &lt;img alt="mistralai/Mistral-Small-24B-Instruct-2501 路 Hugging Face" src="https://external-preview.redd.it/8WbpVBLCMGZToRjDI6ufWcY1nKKfpipz-TKy8aHrbsg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0fb438638f6c72b0b1b0db98ac490b6bda471f2f" title="mistralai/Mistral-Small-24B-Instruct-2501 路 Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idny30/mistralaimistralsmall24binstruct2501_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idny30/mistralaimistralsmall24binstruct2501_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:17:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1iduub3</id>
    <title>Kimi k1.5: Scaling Reinforcement Learning with LLMs --- an o1-level multi-modal model</title>
    <updated>2025-01-30T19:13:37+00:00</updated>
    <author>
      <name>/u/boxingdog</name>
      <uri>https://old.reddit.com/user/boxingdog</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iduub3/kimi_k15_scaling_reinforcement_learning_with_llms/"&gt; &lt;img alt="Kimi k1.5: Scaling Reinforcement Learning with LLMs --- an o1-level multi-modal model" src="https://external-preview.redd.it/3dvclJU73kiOVyoAbBNFISjYaAXaMZp1TTT-q-G-U14.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ab0c10e23df522cc4617f8da26234fd01e64856d" title="Kimi k1.5: Scaling Reinforcement Learning with LLMs --- an o1-level multi-modal model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boxingdog"&gt; /u/boxingdog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MoonshotAI/Kimi-k1.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iduub3/kimi_k15_scaling_reinforcement_learning_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iduub3/kimi_k15_scaling_reinforcement_learning_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T19:13:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1id7a3k</id>
    <title>I feel bad for the AI lol after seeing its chain of thought. </title>
    <updated>2025-01-29T22:58:30+00:00</updated>
    <author>
      <name>/u/Tricky_Reflection_75</name>
      <uri>https://old.reddit.com/user/Tricky_Reflection_75</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id7a3k/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt; &lt;img alt="I feel bad for the AI lol after seeing its chain of thought. " src="https://b.thumbs.redditmedia.com/Z88vfN16w6NY9yx3hsyBi8c5yYLLA29C35wbeMSZgfQ.jpg" title="I feel bad for the AI lol after seeing its chain of thought. " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/u4cm9r5oj0ge1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7cf76aa8fcfaa023df271504b53ea217f4208528"&gt;https://preview.redd.it/u4cm9r5oj0ge1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7cf76aa8fcfaa023df271504b53ea217f4208528&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tricky_Reflection_75"&gt; /u/Tricky_Reflection_75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id7a3k/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id7a3k/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1id7a3k/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T22:58:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1id2poe</id>
    <title>"DeepSeek produced a model close to the performance of US models 7-10 months older, for a good deal less cost (but NOT anywhere near the ratios people have suggested)" says Anthropic's CEO</title>
    <updated>2025-01-29T19:46:32+00:00</updated>
    <author>
      <name>/u/siegevjorn</name>
      <uri>https://old.reddit.com/user/siegevjorn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id2poe/deepseek_produced_a_model_close_to_the/"&gt; &lt;img alt="&amp;quot;DeepSeek produced a model close to the performance of US models 7-10 months older, for a good deal less cost (but NOT anywhere near the ratios people have suggested)&amp;quot; says Anthropic's CEO" src="https://external-preview.redd.it/33CmrJWIyiH-IL_JOc7gY-avdl30Pd-oQB-Pun7s774.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6512765018eb834d1f7c5898ca5a6e6f6fd0af6e" title="&amp;quot;DeepSeek produced a model close to the performance of US models 7-10 months older, for a good deal less cost (but NOT anywhere near the ratios people have suggested)&amp;quot; says Anthropic's CEO" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic's CEO has a word about DeepSeek. &lt;/p&gt; &lt;p&gt;Here are some of his statements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&amp;quot;Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;3.5 Sonnet did not involve a larger or more expensive model&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&amp;quot;Sonnet's training was conducted 9-12 months ago, while Sonnet remains notably ahead of DeepSeek in many internal and external evals. &amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;DeepSeek's cost efficiency is x8 compared to Sonnet, which is much less than the &amp;quot;original GPT-4 to Claude 3.5 Sonnet inference price differential (10x).&amp;quot; Yet 3.5 Sonnet is a better model than GPT-4, while DeepSeek is not.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;TL;DR: Although DeepSeekV3 was a real deal, but such innovation has been achieved regularly by U.S. AI companies. DeepSeek had enough resources to make it happen. /s&lt;/p&gt; &lt;p&gt;I guess an important distinction, that the Anthorpic CEO refuses to recognize, is the fact that DeepSeekV3 it open weight. In his mind, it is U.S. vs China. It appears that he doesn't give a fuck about local LLMs. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/siegevjorn"&gt; /u/siegevjorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/01/29/anthropics-ceo-says-deepseek-shows-that-u-s-export-rules-are-working-as-intended/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id2poe/deepseek_produced_a_model_close_to_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1id2poe/deepseek_produced_a_model_close_to_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T19:46:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1idtnnh</id>
    <title>Mistral Small 3 24b Q6 initial test results</title>
    <updated>2025-01-30T18:24:26+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its... kind of rough.&lt;/p&gt; &lt;p&gt;It's good. It's VERY smart, but really rough around the edges:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;It doesn't follow instructions well, basically useless for JSON formatting or anything where it has to adhere to a response style. Kind of odd as Mistral Small 2 22b &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It writes good code with random errors. If you're even a mediocre dev you'll find this fine, but it includes several random imports that don't get used and seems to randomly declare/cache things and never refer to them again&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Smart, but rough. Probably the new king of general purpose models that fit into 24gb. I still suspect that Qwen-Coder 32b will win in real world coding, and perhaps even the older Codestral 22b will be better suited in coding for now, but I haven't yet tested it on all of my repos/use cases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idtnnh/mistral_small_3_24b_q6_initial_test_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idtnnh/mistral_small_3_24b_q6_initial_test_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idtnnh/mistral_small_3_24b_q6_initial_test_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T18:24:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1idny6j</id>
    <title>mistralai/Mistral-Small-24B-Instruct-2501</title>
    <updated>2025-01-30T14:18:01+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"&gt;https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501&lt;/a&gt;&lt;/p&gt; &lt;p&gt;its show time folks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idny6j/mistralaimistralsmall24binstruct2501/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idny6j/mistralaimistralsmall24binstruct2501/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idny6j/mistralaimistralsmall24binstruct2501/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:18:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1idvuch</id>
    <title>Re-Distilling DeepSeek R1</title>
    <updated>2025-01-30T19:55:23+00:00</updated>
    <author>
      <name>/u/sightio</name>
      <uri>https://old.reddit.com/user/sightio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Weve improved DeepSeek R1 distilled models using logits distillationdelivering +4-14% gains on GSM8K while only spending $3-18 per training run.&lt;/p&gt; &lt;p&gt;Details at &lt;a href="https://mobiusml.github.io/r1_redistill_blogpost/"&gt;https://mobiusml.github.io/r1_redistill_blogpost/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Models are available on Hugging Face - run them efficiently with HQQ! &lt;a href="https://huggingface.co/collections/mobiuslabsgmbh/deepseek-r1-redistill-6793d3bea92c7fff0639ab4d"&gt;https://huggingface.co/collections/mobiuslabsgmbh/deepseek-r1-redistill-6793d3bea92c7fff0639ab4d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sightio"&gt; /u/sightio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idvuch/redistilling_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idvuch/redistilling_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idvuch/redistilling_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T19:55:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1idp6n6</id>
    <title>Deepseek is hosted on Huawei cloud</title>
    <updated>2025-01-30T15:15:46+00:00</updated>
    <author>
      <name>/u/Reasonable-Climate66</name>
      <uri>https://old.reddit.com/user/Reasonable-Climate66</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on the IP resolved in China. The chat endpoints is from Huawei DC&lt;/p&gt; &lt;p&gt;DS could be using Singapore Huawei region for WW and Shanghai region for CN users.&lt;/p&gt; &lt;p&gt;So demand for Nvidia card for training and Huawei GPU for inference is real. &lt;/p&gt; &lt;p&gt;&lt;a href="https://i.postimg.cc/0QyjxTkh/Screenshot-20250130-230756.png"&gt;https://i.postimg.cc/0QyjxTkh/Screenshot-20250130-230756.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.postimg.cc/FHknCz0B/Screenshot-20250130-230812.png"&gt;https://i.postimg.cc/FHknCz0B/Screenshot-20250130-230812.png&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable-Climate66"&gt; /u/Reasonable-Climate66 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idp6n6/deepseek_is_hosted_on_huawei_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idp6n6/deepseek_is_hosted_on_huawei_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idp6n6/deepseek_is_hosted_on_huawei_cloud/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T15:15:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ideaxu</id>
    <title>Nvidia cuts FP8 training performance in half on RTX 40 and 50 series GPUs</title>
    <updated>2025-01-30T04:22:34+00:00</updated>
    <author>
      <name>/u/Emergency-Map9861</name>
      <uri>https://old.reddit.com/user/Emergency-Map9861</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ideaxu/nvidia_cuts_fp8_training_performance_in_half_on/"&gt; &lt;img alt="Nvidia cuts FP8 training performance in half on RTX 40 and 50 series GPUs" src="https://b.thumbs.redditmedia.com/SlGpr_siDY7Rr_nl1h9FbbkgpwtHXQX47AlZAVKy8LM.jpg" title="Nvidia cuts FP8 training performance in half on RTX 40 and 50 series GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to their new RTX Blackwell GPU architecture whitepaper, Nvidia appears to have cut FP8 training performance in half on RTX 40 and 50 series GPUs after DeepSeek successfully trained their SOTA V3 and R1 models using FP8. &lt;/p&gt; &lt;p&gt;In their original Ada Lovelace whitepaper, table 2 in Appendix A shows the 4090 having &lt;strong&gt;660.6 TFlops&lt;/strong&gt; of FP8 with FP32 accumulate without sparsity, which is the same as FP8 with FP16 accumulate. The new Blackwell paper shows half the performance for the 4090 at just &lt;strong&gt;330.3 TFlops&lt;/strong&gt; of FP8 with FP32 accumulate, and the 5090 has just &lt;strong&gt;419 TFlops&lt;/strong&gt; vs &lt;strong&gt;838 TFlops&lt;/strong&gt; for FP8 with FP16 accumulate. &lt;/p&gt; &lt;p&gt;FP32 accumulate is a must when it comes to training because FP16 doesn't have the necessary precision and dynamic range required. &lt;/p&gt; &lt;p&gt;If this isn't a mistake, then it means Nvidia lobotomized their Geforce lineup to further dissuade us from using them for AI/ML training, and it could potentially be reversible for the RTX 40 series at least, as this was likely done through a driver update.&lt;/p&gt; &lt;p&gt;This is quite unfortunate but not unexpected as Nvidia has a known history of artificially limiting Geforce GPUs for AI training since the Turing architecture, while their Quadro and datacenter GPUs continue to have the full performance.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x3qfea1352ge1.jpg?width=2007&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6c20a53057eb2bf15bbf65db4900af638fef9955"&gt;https://preview.redd.it/x3qfea1352ge1.jpg?width=2007&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6c20a53057eb2bf15bbf65db4900af638fef9955&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lk3ch91352ge1.jpg?width=1934&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d267c0312fe0be00175e616512101dce69113134"&gt;https://preview.redd.it/lk3ch91352ge1.jpg?width=1934&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d267c0312fe0be00175e616512101dce69113134&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sources:&lt;/p&gt; &lt;p&gt;RTX Blackwell GPU Architecture Whitepaper:&lt;/p&gt; &lt;p&gt;&lt;a href="https://images.nvidia.com/aem-dam/Solutions/geforce/blackwell/nvidia-rtx-blackwell-gpu-architecture.pdf"&gt;https://images.nvidia.com/aem-dam/Solutions/geforce/blackwell/nvidia-rtx-blackwell-gpu-architecture.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;RTX Ada Lovelace GPU Architecture Whitepaper:&lt;/p&gt; &lt;p&gt;&lt;a href="https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf"&gt;https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency-Map9861"&gt; /u/Emergency-Map9861 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ideaxu/nvidia_cuts_fp8_training_performance_in_half_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ideaxu/nvidia_cuts_fp8_training_performance_in_half_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ideaxu/nvidia_cuts_fp8_training_performance_in_half_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T04:22:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1idro19</id>
    <title>DeepSeek R1 scores between o1 and o1-mini on NYT Connections</title>
    <updated>2025-01-30T17:02:39+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idro19/deepseek_r1_scores_between_o1_and_o1mini_on_nyt/"&gt; &lt;img alt="DeepSeek R1 scores between o1 and o1-mini on NYT Connections" src="https://preview.redd.it/e8ov1yb3x5ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=663523084a040c4c952820d45759a6a4e7a87469" title="DeepSeek R1 scores between o1 and o1-mini on NYT Connections" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e8ov1yb3x5ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idro19/deepseek_r1_scores_between_o1_and_o1mini_on_nyt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idro19/deepseek_r1_scores_between_o1_and_o1mini_on_nyt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T17:02:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1idqql6</id>
    <title>Mistral Small 3 24b's Context Window is Remarkably Efficient</title>
    <updated>2025-01-30T16:23:25+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idqql6/mistral_small_3_24bs_context_window_is_remarkably/"&gt; &lt;img alt="Mistral Small 3 24b's Context Window is Remarkably Efficient" src="https://b.thumbs.redditmedia.com/tUYsJoEn9u94ym2whVhsPOc7Lcfh9qD4M48XkP1073Y.jpg" title="Mistral Small 3 24b's Context Window is Remarkably Efficient" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using the Mistral Small 3 24b-q6k model with a full 32K context (Q8 KV cache), and I still have 1.6GB of VRAM left.&lt;br /&gt; In comparison, Qwen2.5 32b Q4 KL is roughly the same size, but I could only manage to get 24K context before getting dangerously close to running out of VRAM.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/adg5weajp5ge1.png?width=913&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bb3e935191ccc7000f402ce10e2c880ddb309730"&gt;https://preview.redd.it/adg5weajp5ge1.png?width=913&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bb3e935191ccc7000f402ce10e2c880ddb309730&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idqql6/mistral_small_3_24bs_context_window_is_remarkably/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idqql6/mistral_small_3_24bs_context_window_is_remarkably/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idqql6/mistral_small_3_24bs_context_window_is_remarkably/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T16:23:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ido2up</id>
    <title>Mistral Small</title>
    <updated>2025-01-30T14:24:15+00:00</updated>
    <author>
      <name>/u/MLTyrunt</name>
      <uri>https://old.reddit.com/user/MLTyrunt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral Small&lt;/p&gt; &lt;p&gt;Apache 2.0, 81% MMLU, 150 tokens/s&lt;/p&gt; &lt;p&gt;&lt;a href="https://mistral.ai/news/mistral-small-3/"&gt;https://mistral.ai/news/mistral-small-3/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLTyrunt"&gt; /u/MLTyrunt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ido2up/mistral_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ido2up/mistral_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ido2up/mistral_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:24:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1idva1j</id>
    <title>Welcome back, Le Mistral!</title>
    <updated>2025-01-30T19:31:40+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idva1j/welcome_back_le_mistral/"&gt; &lt;img alt="Welcome back, Le Mistral!" src="https://preview.redd.it/4td7dsrjn6ge1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bcee379bd06ff66ce0c2532f18c365ea37c8d6d1" title="Welcome back, Le Mistral!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4td7dsrjn6ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idva1j/welcome_back_le_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idva1j/welcome_back_le_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T19:31:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1idt9xz</id>
    <title>Watch this SmolAgent save me over 100 hours of work.</title>
    <updated>2025-01-30T18:08:42+00:00</updated>
    <author>
      <name>/u/Foreign-Beginning-49</name>
      <uri>https://old.reddit.com/user/Foreign-Beginning-49</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idt9xz/watch_this_smolagent_save_me_over_100_hours_of/"&gt; &lt;img alt="Watch this SmolAgent save me over 100 hours of work." src="https://external-preview.redd.it/eXpvaDN2aXY4NmdlMaIWY-pKRTEFed4oaflr_50jeaU7y6AfPZ2q49QYyqUZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ea3707bc7016b8ca7da0ee5c72fef8602edfdba" title="Watch this SmolAgent save me over 100 hours of work." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Foreign-Beginning-49"&gt; /u/Foreign-Beginning-49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/je2gcviv86ge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idt9xz/watch_this_smolagent_save_me_over_100_hours_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idt9xz/watch_this_smolagent_save_me_over_100_hours_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T18:08:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1idokcx</id>
    <title>Mistral new open models</title>
    <updated>2025-01-30T14:47:21+00:00</updated>
    <author>
      <name>/u/konilse</name>
      <uri>https://old.reddit.com/user/konilse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idokcx/mistral_new_open_models/"&gt; &lt;img alt="Mistral new open models" src="https://preview.redd.it/5nnsoy4295ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d39024b2c7d0acbb55e2f3d01eee2b120c949e0" title="Mistral new open models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral base and instruct 24B &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/konilse"&gt; /u/konilse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5nnsoy4295ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idokcx/mistral_new_open_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idokcx/mistral_new_open_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:47:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iduk3b</id>
    <title>Mistral Small 3 one-shotting Unsloth's Flappy Bird coding test in 1 min (vs 3hrs for DeepSeek R1 using NVME drive)</title>
    <updated>2025-01-30T19:02:02+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iduk3b/mistral_small_3_oneshotting_unsloths_flappy_bird/"&gt; &lt;img alt="Mistral Small 3 one-shotting Unsloth's Flappy Bird coding test in 1 min (vs 3hrs for DeepSeek R1 using NVME drive)" src="https://preview.redd.it/gazbvr6gi6ge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f6de32bcaa9f8ae8ff3f2ab317c2401bd2f5b73" title="Mistral Small 3 one-shotting Unsloth's Flappy Bird coding test in 1 min (vs 3hrs for DeepSeek R1 using NVME drive)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gazbvr6gi6ge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iduk3b/mistral_small_3_oneshotting_unsloths_flappy_bird/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iduk3b/mistral_small_3_oneshotting_unsloths_flappy_bird/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T19:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1idnyhh</id>
    <title>mistralai/Mistral-Small-24B-Base-2501 路 Hugging Face</title>
    <updated>2025-01-30T14:18:23+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idnyhh/mistralaimistralsmall24bbase2501_hugging_face/"&gt; &lt;img alt="mistralai/Mistral-Small-24B-Base-2501 路 Hugging Face" src="https://external-preview.redd.it/lDGKmq6pSZNpISh4piV15abwPTUoM5lDEjjJ9qZ_vd4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56053b8ce77cd587b1abeda9737783c65c0ebab8" title="mistralai/Mistral-Small-24B-Base-2501 路 Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idnyhh/mistralaimistralsmall24bbase2501_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idnyhh/mistralaimistralsmall24bbase2501_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:18:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1idp1z5</id>
    <title>No synthetic data?</title>
    <updated>2025-01-30T15:09:51+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idp1z5/no_synthetic_data/"&gt; &lt;img alt="No synthetic data?" src="https://preview.redd.it/98dq1wg2d5ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=448fe61c33c8db28d89becf7c1d0ccbcf95ea88a" title="No synthetic data?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That's reallllllly rare in 2025, did I understand this correctly? They didn't use any synthetic data to train this model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/98dq1wg2d5ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idp1z5/no_synthetic_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idp1z5/no_synthetic_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T15:09:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1idvlz8</id>
    <title>CLOSEDAI</title>
    <updated>2025-01-30T19:45:30+00:00</updated>
    <author>
      <name>/u/LostMyOtherAcct69</name>
      <uri>https://old.reddit.com/user/LostMyOtherAcct69</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idvlz8/closedai/"&gt; &lt;img alt="CLOSEDAI" src="https://preview.redd.it/114wlf69q6ge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e1a65442edd47a375b75cd795715035c8cc50dc" title="CLOSEDAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a pro subscriber to OpenAI, but I am changing that today. I have been using it to help teach myself how to train LLMs to create a RAG bought and overall just learned programming skills. Today they banned my account I believe because I was using it to help write code for me to train these LLMs.&lt;/p&gt; &lt;p&gt;It is frustrating how much more censored and useless the OpenAI software is compared to its competitors. I refuse to be taken advantage of anymore after this. &lt;/p&gt; &lt;p&gt;This is why open source is so important!!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostMyOtherAcct69"&gt; /u/LostMyOtherAcct69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/114wlf69q6ge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idvlz8/closedai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idvlz8/closedai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T19:45:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ido3fn</id>
    <title>Are there 陆 million people capable of running locally 685B params models?</title>
    <updated>2025-01-30T14:25:02+00:00</updated>
    <author>
      <name>/u/S1M0N38</name>
      <uri>https://old.reddit.com/user/S1M0N38</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ido3fn/are_there_陆_million_people_capable_of_running/"&gt; &lt;img alt="Are there 陆 million people capable of running locally 685B params models?" src="https://b.thumbs.redditmedia.com/nUAmR_7owY5oJQcrzV0vL3H93-ccvgV-SDlaKg3CSyw.jpg" title="Are there 陆 million people capable of running locally 685B params models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/S1M0N38"&gt; /u/S1M0N38 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ido3fn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ido3fn/are_there_陆_million_people_capable_of_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ido3fn/are_there_陆_million_people_capable_of_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:25:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1idseqb</id>
    <title>DeepSeek R1 671B over 2 tok/sec *without* GPU on local gaming rig!</title>
    <updated>2025-01-30T17:33:04+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't rush out and buy that 5090TI just yet (if you can even find one lol)!&lt;/p&gt; &lt;p&gt;I just inferenced ~2.13 tok/sec with 2k context using a dynamic quant of the full R1 671B model (not a distill) after &lt;em&gt;disabling&lt;/em&gt; my 3090TI GPU on a 96GB RAM gaming rig. The secret trick is to &lt;em&gt;not&lt;/em&gt; load anything but kv cache into RAM and let &lt;code&gt;llama.cpp&lt;/code&gt; use its default behavior to &lt;code&gt;mmap()&lt;/code&gt; the model files off of a fast NVMe SSD. The rest of your system RAM acts as disk cache for the active weights.&lt;/p&gt; &lt;p&gt;Yesterday a bunch of folks got the dynamic quant flavors of &lt;code&gt;unsloth/DeepSeek-R1-GGUF&lt;/code&gt; running on gaming rigs in another thread here. I myself got the &lt;code&gt;DeepSeek-R1-UD-Q2_K_XL&lt;/code&gt; flavor going between 1~2 toks/sec and 2k~16k context on 96GB RAM + 24GB VRAM experimenting with context length and up to 8 concurrent slots inferencing for increased aggregate throuput.&lt;/p&gt; &lt;p&gt;After experimenting with various setups, the bottle neck is clearly my Gen 5 x4 NVMe SSD card as the CPU doesn't go over ~30%, the GPU was basically idle, and the power supply fan doesn't even come on. So while slow, it isn't heating up the room.&lt;/p&gt; &lt;p&gt;So instead of a $2k GPU what about $1.5k for 4x NVMe SSDs on an expansion card for 2TB &amp;quot;VRAM&amp;quot; giving theoretical max sequential read &amp;quot;memory&amp;quot; bandwidth of ~48GB/s? This less expensive setup would likely give better price/performance for big MoEs on home rigs. If you forgo a GPU, you could have 16 lanes of PCIe 5.0 all for NVMe drives on gamer class motherboards.&lt;/p&gt; &lt;p&gt;If anyone has a fast read IOPs drive array, I'd love to hear what kind of speeds you can get. I gotta bug Wendell over at Level1Techs lol...&lt;/p&gt; &lt;p&gt;P.S. In my opinion this quantized R1 671B beats the pants off any of the distill model toys. While slow and limited in context, it is still likely the best thing available for home users for many applications.&lt;/p&gt; &lt;p&gt;Just need to figure out how to short circuit the &lt;code&gt;&amp;lt;think&amp;gt;Blah blah&amp;lt;/think&amp;gt;&lt;/code&gt; stuff by injecting a &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; into the assistant prompt to see if it gives decent results without all the yapping haha...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T17:33:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1idv7yb</id>
    <title>Marc Andreessen on Anthropic CEO's Call for Export Controls on China</title>
    <updated>2025-01-30T19:29:13+00:00</updated>
    <author>
      <name>/u/AloneCoffee4538</name>
      <uri>https://old.reddit.com/user/AloneCoffee4538</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idv7yb/marc_andreessen_on_anthropic_ceos_call_for_export/"&gt; &lt;img alt="Marc Andreessen on Anthropic CEO's Call for Export Controls on China" src="https://preview.redd.it/wlsi25dcn6ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d695bb3258d357570ad11762d15df689f13fe2a8" title="Marc Andreessen on Anthropic CEO's Call for Export Controls on China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AloneCoffee4538"&gt; /u/AloneCoffee4538 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wlsi25dcn6ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idv7yb/marc_andreessen_on_anthropic_ceos_call_for_export/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idv7yb/marc_andreessen_on_anthropic_ceos_call_for_export/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T19:29:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1idny3w</id>
    <title>Mistral Small 3</title>
    <updated>2025-01-30T14:17:56+00:00</updated>
    <author>
      <name>/u/khubebk</name>
      <uri>https://old.reddit.com/user/khubebk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idny3w/mistral_small_3/"&gt; &lt;img alt="Mistral Small 3" src="https://preview.redd.it/kj3s0jvr35ge1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0317aadc49155a8df1074618844c589ea3d2753d" title="Mistral Small 3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/khubebk"&gt; /u/khubebk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kj3s0jvr35ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idny3w/mistral_small_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idny3w/mistral_small_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:17:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1idtkll</id>
    <title>Interview with Deepseek Founder: We wont go closed-source. We believe that establishing a robust technology ecosystem matters more.</title>
    <updated>2025-01-30T18:20:59+00:00</updated>
    <author>
      <name>/u/deoxykev</name>
      <uri>https://old.reddit.com/user/deoxykev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idtkll/interview_with_deepseek_founder_we_wont_go/"&gt; &lt;img alt="Interview with Deepseek Founder: We wont go closed-source. We believe that establishing a robust technology ecosystem matters more." src="https://external-preview.redd.it/VCPkBGJsVaggWY7c9V20KQQGCJhrF411vyVYUsHeuns.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=495bbbb03e5ebeff92050c2a71f7e340cb4bbebc" title="Interview with Deepseek Founder: We wont go closed-source. We believe that establishing a robust technology ecosystem matters more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/deoxykev"&gt; /u/deoxykev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://thechinaacademy.org/interview-with-deepseek-founder-were-done-following-its-time-to-lead/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idtkll/interview_with_deepseek_founder_we_wont_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idtkll/interview_with_deepseek_founder_we_wont_go/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T18:20:59+00:00</published>
  </entry>
</feed>
