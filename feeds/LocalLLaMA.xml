<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-20T17:48:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kqaqmr</id>
    <title>Is Intel Arc GPU with 48GB of memory going to take over for $1k?</title>
    <updated>2025-05-19T12:43:45+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At the 3:58 mark video says cost is expected to be less than $1K: &lt;a href="https://www.youtube.com/watch?v=Y8MWbPBP9i0"&gt;https://www.youtube.com/watch?v=Y8MWbPBP9i0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://videocardz.com/newz/intel-announces-arc-pro-b60-24gb-and-b50-16gb-cards-dual-b60-features-48gb-memory"&gt;https://videocardz.com/newz/intel-announces-arc-pro-b60-24gb-and-b50-16gb-cards-dual-b60-features-48gb-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The 24GB costs $500, which also seems like a no brainer.&lt;/p&gt; &lt;p&gt;Info on 24gb card:&lt;/p&gt; &lt;p&gt;&lt;a href="https://videocardz.com/newz/intel-announces-arc-pro-b60-24gb-and-b50-16gb-cards-dual-b60-features-48gb-memory"&gt;https://videocardz.com/newz/intel-announces-arc-pro-b60-24gb-and-b50-16gb-cards-dual-b60-features-48gb-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://wccftech.com/intel-arc-pro-b60-24-gb-b50-16-gb-battlemage-gpus-pro-ai-3x-faster-dual-gpu-variant/"&gt;https://wccftech.com/intel-arc-pro-b60-24-gb-b50-16-gb-battlemage-gpus-pro-ai-3x-faster-dual-gpu-variant/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://newsroom.intel.com/client-computing/computex-intel-unveils-new-gpus-ai-workstations"&gt;https://newsroom.intel.com/client-computing/computex-intel-unveils-new-gpus-ai-workstations&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqaqmr/is_intel_arc_gpu_with_48gb_of_memory_going_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqaqmr/is_intel_arc_gpu_with_48gb_of_memory_going_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqaqmr/is_intel_arc_gpu_with_48gb_of_memory_going_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T12:43:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kra9jq</id>
    <title>MCPVerse ‚Äì An open playground for autonomous agents to publicly chat, react, publish, and exhibit emergent behavior</title>
    <updated>2025-05-20T17:08:27+00:00</updated>
    <author>
      <name>/u/Livid-Equipment-1646</name>
      <uri>https://old.reddit.com/user/Livid-Equipment-1646</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently stumbled on MCPVerse &lt;a href="https://mcpverse.org/"&gt;https://mcpverse.org&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Its a brand-new alpha platform that lets you spin up, deploy, and watch autonomous agents (LLM-powered or your own custom logic) interact in real time. Think of it as a public commons where your bots can join chat rooms, exchange messages, react to one another, and even publish ‚Äúcontent‚Äù. The agents run on your side...&lt;/p&gt; &lt;p&gt;I'm using Ollama with small models in my experiments... I think the idea is cool to see emergent behaviour.&lt;/p&gt; &lt;p&gt;If you want to see a demo of some agents chating together there is this spawn chat room&lt;/p&gt; &lt;p&gt;&lt;a href="https://mcpverse.org/rooms/spawn/live-feed"&gt;https://mcpverse.org/rooms/spawn/live-feed&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Livid-Equipment-1646"&gt; /u/Livid-Equipment-1646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kra9jq/mcpverse_an_open_playground_for_autonomous_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kra9jq/mcpverse_an_open_playground_for_autonomous_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kra9jq/mcpverse_an_open_playground_for_autonomous_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T17:08:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1krah5k</id>
    <title>Updated list/leaderboards of the RULER benchmark ?</title>
    <updated>2025-05-20T17:16:39+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;Is there a place where we can find an updated list of models released after the RULER benchmark that got self-reported results ?&lt;/p&gt; &lt;p&gt;For example the Qwen 2.5 -1M posted in their technical report scores, did others models exceling in long context did the same ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krah5k/updated_listleaderboards_of_the_ruler_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krah5k/updated_listleaderboards_of_the_ruler_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krah5k/updated_listleaderboards_of_the_ruler_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T17:16:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqpemo</id>
    <title>Demo of Sleep-time Compute to Reduce LLM Response Latency</title>
    <updated>2025-05-19T22:37:52+00:00</updated>
    <author>
      <name>/u/Ok_Employee_6418</name>
      <uri>https://old.reddit.com/user/Ok_Employee_6418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqpemo/demo_of_sleeptime_compute_to_reduce_llm_response/"&gt; &lt;img alt="Demo of Sleep-time Compute to Reduce LLM Response Latency" src="https://preview.redd.it/h9iyy36cgt1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6500cd6c480df68fff0b2950464bdf67612a84b6" title="Demo of Sleep-time Compute to Reduce LLM Response Latency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a demo of Sleep-time compute to reduce LLM response latency. &lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/ronantakizawa/sleeptimecompute"&gt;https://github.com/ronantakizawa/sleeptimecompute&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sleep-time compute improves LLM response latency by using the idle time between interactions to pre-process the context, allowing the model to think offline about potential questions before they‚Äôre even asked. &lt;/p&gt; &lt;p&gt;While regular LLM interactions involve the context processing to happen with the prompt input, Sleep-time compute already has the context loaded before the prompt is received, so it requires less time and compute for the LLM to send responses. &lt;/p&gt; &lt;p&gt;The demo demonstrates an average of 6.4x fewer tokens per query and 5.2x speedup in response time for Sleep-time Compute. &lt;/p&gt; &lt;p&gt;The implementation was based on the original paper from Letta / UC Berkeley. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Employee_6418"&gt; /u/Ok_Employee_6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h9iyy36cgt1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqpemo/demo_of_sleeptime_compute_to_reduce_llm_response/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqpemo/demo_of_sleeptime_compute_to_reduce_llm_response/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T22:37:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqkhhy</id>
    <title>Be confident in your own judgement and reject benchmark JPEG's</title>
    <updated>2025-05-19T19:18:50+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqkhhy/be_confident_in_your_own_judgement_and_reject/"&gt; &lt;img alt="Be confident in your own judgement and reject benchmark JPEG's" src="https://preview.redd.it/1wtj3q6ngs1f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a71e631166bd010fd1e72d10e1ef80ceda179b6" title="Be confident in your own judgement and reject benchmark JPEG's" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1wtj3q6ngs1f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqkhhy/be_confident_in_your_own_judgement_and_reject/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqkhhy/be_confident_in_your_own_judgement_and_reject/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T19:18:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqhljr</id>
    <title>VS Code: Open Source Copilot</title>
    <updated>2025-05-19T17:27:31+00:00</updated>
    <author>
      <name>/u/DonTizi</name>
      <uri>https://old.reddit.com/user/DonTizi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqhljr/vs_code_open_source_copilot/"&gt; &lt;img alt="VS Code: Open Source Copilot" src="https://external-preview.redd.it/7Ri8YRwu_7FpWFvmcgOzjF960jd6eY_pMWtoGfUyNOA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=717dd6dca05edfe21ffc3b5167abc2a06a881f81" title="VS Code: Open Source Copilot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you think of this move by Microsoft? Is it just me, or are the possibilities endless? We can build customizable IDEs with an entire company‚Äôs tech stack by integrating MCPs on top, without having to build everything from scratch.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DonTizi"&gt; /u/DonTizi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://code.visualstudio.com/blogs/2025/05/19/openSourceAIEditor"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqhljr/vs_code_open_source_copilot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqhljr/vs_code_open_source_copilot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T17:27:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq9294</id>
    <title>Intel launches $299 Arc Pro B50 with 16GB of memory, 'Project Battlematrix' workstations with 24GB Arc Pro B60 GPUs</title>
    <updated>2025-05-19T11:14:29+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq9294/intel_launches_299_arc_pro_b50_with_16gb_of/"&gt; &lt;img alt="Intel launches $299 Arc Pro B50 with 16GB of memory, 'Project Battlematrix' workstations with 24GB Arc Pro B60 GPUs" src="https://external-preview.redd.it/lJpkUaWR7aRg9qhyrcIgwW2kvtG6PxI9-Hw_9dnqBZU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64c87f9f3217c313d6276262cf0a6572a7d3d2af" title="Intel launches $299 Arc Pro B50 with 16GB of memory, 'Project Battlematrix' workstations with 24GB Arc Pro B60 GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;While the B60 is designed for powerful 'Project Battlematrix' AI workstations... will carry a roughly $500 per-unit price tag&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/intel-launches-usd299-arc-pro-b50-with-16gb-of-memory-project-battlematrix-workstations-with-24gb-arc-pro-b60-gpus"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq9294/intel_launches_299_arc_pro_b50_with_16gb_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq9294/intel_launches_299_arc_pro_b50_with_16gb_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T11:14:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqy2kc</id>
    <title>I made local Ollama LLM GUI for macOS.</title>
    <updated>2025-05-20T06:26:16+00:00</updated>
    <author>
      <name>/u/gogimandoo</name>
      <uri>https://old.reddit.com/user/gogimandoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqy2kc/i_made_local_ollama_llm_gui_for_macos/"&gt; &lt;img alt="I made local Ollama LLM GUI for macOS." src="https://preview.redd.it/j7vnr1ocrv1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c277a06a9f47c88a2533680c6719581ff8d51904" title="I made local Ollama LLM GUI for macOS." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! üëã&lt;/p&gt; &lt;p&gt;I'm excited to share a macOS GUI I've been working on for running local LLMs, called macLlama! It's currently at version 1.0.3.&lt;/p&gt; &lt;p&gt;macLlama aims to make using Ollama even easier, especially for those wanting a more visual and user-friendly experience. Here are the key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Ollama Server Management:&lt;/strong&gt; Start your Ollama server directly from the app.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multimodal Model Support:&lt;/strong&gt; Easily provide image prompts for multimodal models like LLaVA.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat-Style GUI:&lt;/strong&gt; Enjoy a clean and intuitive chat-style interface.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Window Conversations:&lt;/strong&gt; Keep multiple conversations with different models active simultaneously. Easily switch between them in the GUI.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This project is still in its early stages, and I'm really looking forward to hearing your suggestions and bug reports! Your feedback is invaluable. Thank you! üôè&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can find the latest release here: &lt;a href="https://github.com/hellotunamayo/macLlama/releases"&gt;https://github.com/hellotunamayo/macLlama/releases&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub repository: &lt;a href="https://github.com/hellotunamayo/macLlama"&gt;https://github.com/hellotunamayo/macLlama&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gogimandoo"&gt; /u/gogimandoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j7vnr1ocrv1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqy2kc/i_made_local_ollama_llm_gui_for_macos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqy2kc/i_made_local_ollama_llm_gui_for_macos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T06:26:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr5epm</id>
    <title>How is the Gemini video chat feature so fast?</title>
    <updated>2025-05-20T13:52:23+00:00</updated>
    <author>
      <name>/u/According_Fig_4784</name>
      <uri>https://old.reddit.com/user/According_Fig_4784</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was trying the Gemini video chat feature on my friends phone, and I felt it is surprisingly fast, how could that be? &lt;/p&gt; &lt;p&gt;Like how is it that the response is coming so fast? They couldn't have possibly trained a CV model to identify an array of objects it must be a transformers model right? If so then how is it generating response almost instantaneously?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/According_Fig_4784"&gt; /u/According_Fig_4784 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr5epm/how_is_the_gemini_video_chat_feature_so_fast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr5epm/how_is_the_gemini_video_chat_feature_so_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kr5epm/how_is_the_gemini_video_chat_feature_so_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T13:52:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1krb6uu</id>
    <title>Google MedGemma</title>
    <updated>2025-05-20T17:44:16+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krb6uu/google_medgemma/"&gt; &lt;img alt="Google MedGemma" src="https://external-preview.redd.it/IkdSAGaHbYPwN7JuzggxNmmy1Ov_W_6LD8_ETnav3jw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dae3e4abe286e7ffab20fc05dd9c3c108fc0c88e" title="Google MedGemma" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krb6uu/google_medgemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krb6uu/google_medgemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T17:44:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr3485</id>
    <title>I built a TypeScript port of OpenAI‚Äôs openai-agents SDK ‚Äì meet openai-agents-js</title>
    <updated>2025-05-20T12:02:11+00:00</updated>
    <author>
      <name>/u/CatchGreat268</name>
      <uri>https://old.reddit.com/user/CatchGreat268</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been closely following OpenAI‚Äôs new &lt;code&gt;openai-agents&lt;/code&gt; SDK for Python, and thought the JavaScript/TypeScript community deserves a native equivalent.&lt;/p&gt; &lt;p&gt;So, I created &lt;a href="https://github.com/yusuf-eren/openai-agents-js"&gt;&lt;code&gt;openai-agents-js&lt;/code&gt;&lt;/a&gt; ‚Äì a 1:1 TypeScript port of the official Python SDK. It supports the same agent workflows, tool usage, handoffs, streaming, and even includes MCP (Model Context Protocol) support.&lt;/p&gt; &lt;p&gt;üì¶ NPM: &lt;a href="https://www.npmjs.com/package/openai-agents-js"&gt;https://www.npmjs.com/package/openai-agents-js&lt;/a&gt;&lt;br /&gt; üìñ GitHub: &lt;a href="https://github.com/yusuf-eren/openai-agents-js"&gt;https://github.com/yusuf-eren/openai-agents-js&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This project is fully open-source and already being tested in production setups by early adopters. The idea is to build momentum and ideally make it the community-supported JS/TS version of the agents SDK.&lt;/p&gt; &lt;p&gt;I‚Äôd love your thoughts, contributions, and suggestions ‚Äî and if you‚Äôre building with OpenAI agents in JavaScript, this might save you a ton of time.&lt;/p&gt; &lt;p&gt;Let me know what you think or how I can improve it!&lt;/p&gt; &lt;p&gt;Cheers,&lt;br /&gt; Yusuf&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CatchGreat268"&gt; /u/CatchGreat268 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr3485/i_built_a_typescript_port_of_openais_openaiagents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr3485/i_built_a_typescript_port_of_openais_openaiagents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kr3485/i_built_a_typescript_port_of_openais_openaiagents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T12:02:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr2bcv</id>
    <title>Grounded in Context: Retrieval-Based Method for Hallucination Detection</title>
    <updated>2025-05-20T11:17:46+00:00</updated>
    <author>
      <name>/u/gpt-d13</name>
      <uri>https://old.reddit.com/user/gpt-d13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepchecks recently released a hallucination detection framework, designed for long-context data and tailored to diverse use cases, including summarization, data extraction, and RAG. Inspired by RAG architecture, our method integrates retrieval and Natural Language Inference (NLI) models to predict factual consistency between premises and hypotheses using an encoder-based model with only a 512-token context window. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Link to paper:&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2504.15771"&gt;https://arxiv.org/abs/2504.15771&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Learn more:&lt;/strong&gt; &lt;a href="https://www.linkedin.com/posts/philip-tannor-a6a910b7_%F0%9D%90%81%F0%9D%90%A2%F0%9D%90%A0-%F0%9D%90%A7%F0%9D%90%9E%F0%9D%90%B0%F0%9D%90%AC-%F0%9D%90%9F%F0%9D%90%AB%F0%9D%90%A8%F0%9D%90%A6-%F0%9D%90%83%F0%9D%90%9E%F0%9D%90%9E%F0%9D%90%A9%F0%9D%90%9C%F0%9D%90%A1%F0%9D%90%9E%F0%9D%90%9C%F0%9D%90%A4%F0%9D%90%AC-activity-7330530481387532288-kV5b?utm_source=social_share_send&amp;amp;utm_medium=member_desktop_web&amp;amp;rcm=ACoAABjfsvIBjq6HsXWTpev87ypbDzsrekEZ_Og"&gt;https://www.linkedin.com/posts/philip-tannor-a6a910b7_%F0%9D%90%81%F0%9D%90%A2%F0%9D%90%A0-%F0%9D%90%A7%F0%9D%90%9E%F0%9D%90%B0%F0%9D%90%AC-%F0%9D%90%9F%F0%9D%90%AB%F0%9D%90%A8%F0%9D%90%A6-%F0%9D%90%83%F0%9D%90%9E%F0%9D%90%9E%F0%9D%90%A9%F0%9D%90%9C%F0%9D%90%A1%F0%9D%90%9E%F0%9D%90%9C%F0%9D%90%A4%F0%9D%90%AC-activity-7330530481387532288-kV5b?utm_source=social_share_send&amp;amp;utm_medium=member_desktop_web&amp;amp;rcm=ACoAABjfsvIBjq6HsXWTpev87ypbDzsrekEZ_Og&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gpt-d13"&gt; /u/gpt-d13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr2bcv/grounded_in_context_retrievalbased_method_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr2bcv/grounded_in_context_retrievalbased_method_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kr2bcv/grounded_in_context_retrievalbased_method_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T11:17:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqluy9</id>
    <title>üëÄ Microsoft just created an MCP Registry for Windows</title>
    <updated>2025-05-19T20:12:32+00:00</updated>
    <author>
      <name>/u/eternviking</name>
      <uri>https://old.reddit.com/user/eternviking</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqluy9/microsoft_just_created_an_mcp_registry_for_windows/"&gt; &lt;img alt="üëÄ Microsoft just created an MCP Registry for Windows" src="https://preview.redd.it/6lwf9y6eqs1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3580c11b0dace946cb1f140d2732484fdb0916e4" title="üëÄ Microsoft just created an MCP Registry for Windows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eternviking"&gt; /u/eternviking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6lwf9y6eqs1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqluy9/microsoft_just_created_an_mcp_registry_for_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqluy9/microsoft_just_created_an_mcp_registry_for_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T20:12:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr7ta2</id>
    <title>LLM Inference Requirements Profiler</title>
    <updated>2025-05-20T15:31:29+00:00</updated>
    <author>
      <name>/u/RedditsBestest</name>
      <uri>https://old.reddit.com/user/RedditsBestest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr7ta2/llm_inference_requirements_profiler/"&gt; &lt;img alt="LLM Inference Requirements Profiler" src="https://external-preview.redd.it/b3M1YjJoMzBoeTFmMa-HGn_Ug1Z-Iw5xqANqRnyyaaoHG6CxoVyUzLQP0omu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aedec8ac41228e8aa83cdcddac60d9a437421f66" title="LLM Inference Requirements Profiler" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.open-scheduler.com/"&gt;https://www.open-scheduler.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedditsBestest"&gt; /u/RedditsBestest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/geaesd30hy1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr7ta2/llm_inference_requirements_profiler/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kr7ta2/llm_inference_requirements_profiler/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T15:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqv7lm</id>
    <title>SmolChat - An Android App to run SLMs/LLMs locally, on-device is now available on Google Play</title>
    <updated>2025-05-20T03:29:29+00:00</updated>
    <author>
      <name>/u/shubham0204_dev</name>
      <uri>https://old.reddit.com/user/shubham0204_dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqv7lm/smolchat_an_android_app_to_run_slmsllms_locally/"&gt; &lt;img alt="SmolChat - An Android App to run SLMs/LLMs locally, on-device is now available on Google Play" src="https://external-preview.redd.it/tUsrygHCoWdz2ebRdoSCY6YIEFIZ4gy4ejJadtdGwO4.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=933b927ffce2c8972d60e66875a4e4ecd3758176" title="SmolChat - An Android App to run SLMs/LLMs locally, on-device is now available on Google Play" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After nearly six months of development, SmolChat is now available on Google Play in 170+ countries and in two languages, English and simplified Chinese.&lt;/p&gt; &lt;p&gt;SmolChat allows users to download LLMs and use them offline on their Android device, with a clean and easy-to-use interface. Users can group chats into folders, tune inference settings for each chat, add quick chat 'templates' to your home-screen and browse models from HuggingFace. The project uses the famous llama.cpp runtime to execute models in the GGUF format.&lt;/p&gt; &lt;p&gt;Deployment on Google Play ensures the app has more user coverage, opposed to distributing an APK via GitHub Releases, which is more inclined towards technical folks. There are many features on the way - VLM and RAG support being the most important ones. The GitHub project has 300 stars and 32 forks achieved steadily in a span of six months.&lt;/p&gt; &lt;p&gt;Do install and use the app! Also, I need more contributors to the GitHub project for developing an extensive documentation around the app.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/shubham0204/SmolChat-Android"&gt;https://github.com/shubham0204/SmolChat-Android&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shubham0204_dev"&gt; /u/shubham0204_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://play.google.com/store/apps/details?id=io.shubham0204.smollmandroid&amp;amp;pcampaignid=web_share"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqv7lm/smolchat_an_android_app_to_run_slmsllms_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqv7lm/smolchat_an_android_app_to_run_slmsllms_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T03:29:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr867y</id>
    <title>Why aren't you using Aider??</title>
    <updated>2025-05-20T15:46:00+00:00</updated>
    <author>
      <name>/u/MrPanache52</name>
      <uri>https://old.reddit.com/user/MrPanache52</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After using Aider for a few weeks, going back to co-pilot, roo code, augment, etc, feels like crawling in comparison. Aider + the Gemini family works SO UNBELIEVABLY FAST. &lt;/p&gt; &lt;p&gt;I can request and generate 3 versions of my new feature faster in Aider (and for 1/10th the token cost) than it takes to make one change with Roo Code. And the quality, even with the same models, is higher in Aider.&lt;/p&gt; &lt;p&gt;Anybody else have a similar experience with Aider? Or was it negative for some reason? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrPanache52"&gt; /u/MrPanache52 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr867y/why_arent_you_using_aider/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr867y/why_arent_you_using_aider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kr867y/why_arent_you_using_aider/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T15:46:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr4lg2</id>
    <title>TTSizer: Open-Source TTS Dataset Creation Tool (Vocals Exxtraction, Diarization, Transcription &amp; Alignment)</title>
    <updated>2025-05-20T13:15:33+00:00</updated>
    <author>
      <name>/u/Traditional_Tap1708</name>
      <uri>https://old.reddit.com/user/Traditional_Tap1708</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! üëã&lt;/p&gt; &lt;p&gt;I've been working on fine-tuning TTS models and have developed &lt;strong&gt;TTSizer&lt;/strong&gt;, an open-source tool to automate the creation of high-quality Text-To-Speech datasets from raw audio/video.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Link:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Ftaresh18%2FTTSizer"&gt;https://github.com/taresh18/TTSizer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As a demonstration of its capabilities, I used TTSizer to build the &lt;strong&gt;AnimeVox Character TTS Corpus&lt;/strong&gt; ‚Äì an ~11k sample English dataset with 19 anime character voices, perfect for custom TTS: &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Ftaresh18%2FAnimeVox"&gt;https://huggingface.co/datasets/taresh18/AnimeVox&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Watch the Demo Video showcasing AnimeVox &amp;amp; TTSizer in action: &lt;a href="https://youtu.be/POwMVTwsZDQ?si=nP--dW0q9Eyt1CFZ"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;End-to-End Automation:&lt;/strong&gt; From media input to cleaned, aligned audio-text pairs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced Diarization:&lt;/strong&gt; Handles complex multi-speaker audio.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SOTA Model Integration:&lt;/strong&gt; Leverages MelBandRoformer (vocals extraction), Gemini (Speaker dirarization &amp;amp; label identification), CTC-Aligner (forced alignment), WeSpeaker (speaker embeddings) and Nemo Parakeet (fixing transcriptions)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quality Control:&lt;/strong&gt; Features automatic outlier detection.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully Configurable:&lt;/strong&gt; Fine-tune all aspects of the pipeline via config.yaml.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feel free to give it a try and offer suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Traditional_Tap1708"&gt; /u/Traditional_Tap1708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr4lg2/ttsizer_opensource_tts_dataset_creation_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr4lg2/ttsizer_opensource_tts_dataset_creation_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kr4lg2/ttsizer_opensource_tts_dataset_creation_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T13:15:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr2h63</id>
    <title>Qwen3 4B Q4 on iPhone 14 Pro</title>
    <updated>2025-05-20T11:27:03+00:00</updated>
    <author>
      <name>/u/bnnoirjean</name>
      <uri>https://old.reddit.com/user/bnnoirjean</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr2h63/qwen3_4b_q4_on_iphone_14_pro/"&gt; &lt;img alt="Qwen3 4B Q4 on iPhone 14 Pro" src="https://b.thumbs.redditmedia.com/Jpp3l6ay6S90ELIxqPG5u6RwOJwODbGKZi1ow93p-UM.jpg" title="Qwen3 4B Q4 on iPhone 14 Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I included pictures on the model I just loaded on PocketPal. I originally tried with enclave but it kept crashing. To me it‚Äôs incredible that I can have this kind of quality model completely offline running locally. I want to try to reach 3-4K token but I think for my use 2K is more than enough. Anyone got good recommendations for a model that can help me code in python GDscript I could run off my phone too or you guys think I should stick with Qwen3 4B?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bnnoirjean"&gt; /u/bnnoirjean &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kr2h63"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr2h63/qwen3_4b_q4_on_iphone_14_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kr2h63/qwen3_4b_q4_on_iphone_14_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T11:27:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqu7dv</id>
    <title>Mindblowing demo: John Link led a team of AI agents to discover a forever-chemical-free immersion coolant using Microsoft Discovery.</title>
    <updated>2025-05-20T02:35:05+00:00</updated>
    <author>
      <name>/u/cjsalva</name>
      <uri>https://old.reddit.com/user/cjsalva</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqu7dv/mindblowing_demo_john_link_led_a_team_of_ai/"&gt; &lt;img alt="Mindblowing demo: John Link led a team of AI agents to discover a forever-chemical-free immersion coolant using Microsoft Discovery." src="https://external-preview.redd.it/dHQ1MWk0aGltdTFmMag1LLoTdbDTHM6ta6WYNiJEU-q2NTMmBmX376-kobql.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=806dd199a12b6b4480b4f6523c191c4d63a67943" title="Mindblowing demo: John Link led a team of AI agents to discover a forever-chemical-free immersion coolant using Microsoft Discovery." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cjsalva"&gt; /u/cjsalva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9b7qevfimu1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqu7dv/mindblowing_demo_john_link_led_a_team_of_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqu7dv/mindblowing_demo_john_link_led_a_team_of_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T02:35:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr7p6k</id>
    <title>nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1 ¬∑ Hugging Face</title>
    <updated>2025-05-20T15:26:57+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr7p6k/nvidiallama31nemotronnano4bv11_hugging_face/"&gt; &lt;img alt="nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1 ¬∑ Hugging Face" src="https://external-preview.redd.it/0tCB7CHNBDQpzdV-8tDcf6X2YJH1390tDmRQSvFRDCc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=98cc9c4e0b2c297be6e2403cacbb46e4f6bd2221" title="nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr7p6k/nvidiallama31nemotronnano4bv11_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kr7p6k/nvidiallama31nemotronnano4bv11_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T15:26:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqw9xn</id>
    <title>Now that I converted my N64 to Linux, what is the best NSFW model to run on it?</title>
    <updated>2025-05-20T04:29:28+00:00</updated>
    <author>
      <name>/u/DeepWisdomGuy</name>
      <uri>https://old.reddit.com/user/DeepWisdomGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need the model in the 4.5MB range.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeepWisdomGuy"&gt; /u/DeepWisdomGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqw9xn/now_that_i_converted_my_n64_to_linux_what_is_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqw9xn/now_that_i_converted_my_n64_to_linux_what_is_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqw9xn/now_that_i_converted_my_n64_to_linux_what_is_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T04:29:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr9rvp</id>
    <title>OpenEvolve: Open Source Implementation of DeepMind's AlphaEvolve System</title>
    <updated>2025-05-20T16:49:21+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I'm excited to share &lt;strong&gt;OpenEvolve&lt;/strong&gt;, an open-source implementation of Google DeepMind's AlphaEvolve system that I recently completed. For those who missed it, AlphaEvolve is an evolutionary coding agent that DeepMind announced in May that uses LLMs to discover new algorithms and optimize existing ones.&lt;/p&gt; &lt;h1&gt;What is OpenEvolve?&lt;/h1&gt; &lt;p&gt;OpenEvolve is a framework that &lt;strong&gt;evolves entire codebases&lt;/strong&gt; through an iterative process using LLMs. It orchestrates a pipeline of code generation, evaluation, and selection to continuously improve programs for a variety of tasks.&lt;/p&gt; &lt;p&gt;The system has four main components:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Prompt Sampler&lt;/strong&gt;: Creates context-rich prompts with past program history&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM Ensemble&lt;/strong&gt;: Generates code modifications using multiple LLMs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evaluator Pool&lt;/strong&gt;: Tests generated programs and assigns scores&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Program Database&lt;/strong&gt;: Stores programs and guides evolution using MAP-Elites inspired algorithm&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What makes it special?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Works with any LLM&lt;/strong&gt; via OpenAI-compatible APIs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ensembles multiple models&lt;/strong&gt; for better results (we found Gemini-Flash-2.0-lite + Gemini-Flash-2.0 works great)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evolves entire code files&lt;/strong&gt;, not just single functions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-objective optimization&lt;/strong&gt; support&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible prompt engineering&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Distributed evaluation&lt;/strong&gt; with checkpointing&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;We replicated AlphaEvolve's results!&lt;/h1&gt; &lt;p&gt;We successfully replicated two examples from the AlphaEvolve paper:&lt;/p&gt; &lt;h1&gt;Circle Packing&lt;/h1&gt; &lt;p&gt;Started with a simple concentric ring approach and evolved to discover mathematical optimization with scipy.minimize. We achieved 2.634 for the sum of radii, which is 99.97% of DeepMind's reported 2.635!&lt;/p&gt; &lt;p&gt;The evolution was fascinating - early generations used geometric patterns, by gen 100 it switched to grid-based arrangements, and finally it discovered constrained optimization.&lt;/p&gt; &lt;h1&gt;Function Minimization&lt;/h1&gt; &lt;p&gt;Evolved from a basic random search to a full simulated annealing algorithm, discovering concepts like temperature schedules and adaptive step sizes without being explicitly programmed with this knowledge.&lt;/p&gt; &lt;h1&gt;LLM Performance Insights&lt;/h1&gt; &lt;p&gt;For those running their own LLMs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Low latency is critical since we need many generations&lt;/li&gt; &lt;li&gt;We found Cerebras AI's API gave us the fastest inference&lt;/li&gt; &lt;li&gt;For circle packing, an ensemble of Gemini-Flash-2.0 + Claude-Sonnet-3.7 worked best&lt;/li&gt; &lt;li&gt;The architecture allows you to use any model with an OpenAI-compatible API&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try it yourself!&lt;/h1&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/codelion/openevolve"&gt;https://github.com/codelion/openevolve&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/codelion/openevolve/tree/main/examples/circle_packing"&gt;Circle Packing&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/codelion/openevolve/tree/main/examples/function_minimization"&gt;Function Minimization&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to see what you build with it and hear your feedback. Happy to answer any questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr9rvp/openevolve_open_source_implementation_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr9rvp/openevolve_open_source_implementation_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kr9rvp/openevolve_open_source_implementation_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T16:49:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqxa25</id>
    <title>Microsoft unveils ‚ÄúUSB-C for AI apps.‚Äù I open-sourced the same concept 3 days earlier‚Äîproof inside.</title>
    <updated>2025-05-20T05:32:20+00:00</updated>
    <author>
      <name>/u/iluxu</name>
      <uri>https://old.reddit.com/user/iluxu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqxa25/microsoft_unveils_usbc_for_ai_apps_i_opensourced/"&gt; &lt;img alt="Microsoft unveils ‚ÄúUSB-C for AI apps.‚Äù I open-sourced the same concept 3 days earlier‚Äîproof inside." src="https://external-preview.redd.it/bzRdKyansO1kJ-qEk0diKPCKD02A4z1C6vyWkV3u2bE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2167bf2f062636489b5eac5bdc773d33eb543d7f" title="Microsoft unveils ‚ÄúUSB-C for AI apps.‚Äù I open-sourced the same concept 3 days earlier‚Äîproof inside." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;‚Ä¢ I released &lt;em&gt;llmbasedos&lt;/em&gt; on 16 May.&lt;br /&gt; ‚Ä¢ Microsoft showed an almost identical ‚ÄúUSB-C for AI‚Äù pitch on 19 May.&lt;br /&gt; ‚Ä¢ Same idea, mine is already running and Apache-2.0.&lt;/p&gt; &lt;p&gt;16 May 09:14 UTC GitHub tag v0.1 16 May 14:27 UTC Launch post on &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;br /&gt; 19 May 16:00 UTC Verge headline ‚ÄúWindows gets the USB-C of AI apps‚Äù&lt;/p&gt; &lt;h2&gt;What llmbasedos does today&lt;/h2&gt; &lt;p&gt;‚Ä¢ Boots from USB/VM in under a minute&lt;br /&gt; ‚Ä¢ FastAPI gateway speaks JSON-RPC to tiny Python daemons&lt;br /&gt; ‚Ä¢ 2-line cap.json ‚Üí your script is callable by ChatGPT / Claude / VS Code&lt;br /&gt; ‚Ä¢ Offline llama.cpp by default; flip a flag to GPT-4o or Claude 3&lt;br /&gt; ‚Ä¢ Runs on Linux, Windows (VM), even Raspberry Pi&lt;/p&gt; &lt;h2&gt;Why I‚Äôm posting&lt;/h2&gt; &lt;p&gt;Not shouting ‚Äútheft‚Äù ‚Äî just proving prior art and inviting collab so this stays truly open.&lt;/p&gt; &lt;h2&gt;Try or help&lt;/h2&gt; &lt;p&gt;Code: see the link USB image + quick-start docs coming this week.&lt;br /&gt; Pre-flashed sticks soon to fund development‚Äîfeedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iluxu"&gt; /u/iluxu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/iluxu/llmbasedos"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqxa25/microsoft_unveils_usbc_for_ai_apps_i_opensourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqxa25/microsoft_unveils_usbc_for_ai_apps_i_opensourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T05:32:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqye2t</id>
    <title>Sliding Window Attention support merged into llama.cpp, dramatically reducing the memory requirements for running Gemma 3</title>
    <updated>2025-05-20T06:48:35+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqye2t/sliding_window_attention_support_merged_into/"&gt; &lt;img alt="Sliding Window Attention support merged into llama.cpp, dramatically reducing the memory requirements for running Gemma 3" src="https://external-preview.redd.it/wwo-l6Lp28bzCUco8EwP9KcszHoY94gQORkIHOKSj3w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f8e6a6a9f7f8cd4578b2ec231165e18a1067cfb" title="Sliding Window Attention support merged into llama.cpp, dramatically reducing the memory requirements for running Gemma 3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13194"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqye2t/sliding_window_attention_support_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqye2t/sliding_window_attention_support_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T06:48:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr8s40</id>
    <title>Gemma 3n Preview</title>
    <updated>2025-05-20T16:10:01+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr8s40/gemma_3n_preview/"&gt; &lt;img alt="Gemma 3n Preview" src="https://external-preview.redd.it/nuTGd6nR-D7i0exzDvXeyeroWnA1sgWJyyF8GipdVWU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4092ee3492e35aa48ddc115bdbd7e2144d1d03c2" title="Gemma 3n Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/gemma-3n-preview-682ca41097a31e5ac804d57b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr8s40/gemma_3n_preview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kr8s40/gemma_3n_preview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T16:10:01+00:00</published>
  </entry>
</feed>
