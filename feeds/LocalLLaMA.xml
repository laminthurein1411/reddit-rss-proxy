<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-28T10:07:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kxd8cq</id>
    <title>Scores in old and new lmarena are different</title>
    <updated>2025-05-28T10:00:59+00:00</updated>
    <author>
      <name>/u/Economy_Apple_4617</name>
      <uri>https://old.reddit.com/user/Economy_Apple_4617</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have they provided any explanations on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy_Apple_4617"&gt; /u/Economy_Apple_4617 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxd8cq/scores_in_old_and_new_lmarena_are_different/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxd8cq/scores_in_old_and_new_lmarena_are_different/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxd8cq/scores_in_old_and_new_lmarena_are_different/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T10:00:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxdb1n</id>
    <title>Help: effect of Dry sampling on quality</title>
    <updated>2025-05-28T10:05:41+00:00</updated>
    <author>
      <name>/u/fakezeta</name>
      <uri>https://old.reddit.com/user/fakezeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've build a tool to create image using a gradio api, the output is a json with the url generated passed back to the model.&lt;/p&gt; &lt;p&gt;I was using Qwen 30B Moe Q4_XL from unsloth with llama.cpp as my daily driver with dry multiplier at 0.8 without any major issue but here I found that it consistently changed the url hallucinating.&lt;/p&gt; &lt;p&gt;Example with dry multiplier 0.8, suggested settings from Qwen team and presence penalty 1.5&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; given the following json write the image url: { &amp;quot;prompt&amp;quot;: &amp;quot;A cinematic view of Rome at sunset, showcasing the Colosseum and Roman Forum illuminated by warm orange and pink hues, with dramatic shadows and a vibrant sky. The scene captures the historic architecture bathed in soft, golden light, evoking a sense of timeless grandeur.&amp;quot;, &amp;quot;image_url&amp;quot;: &amp;quot;https://example.net/cache/tools_sana/20250527-224501/image.webp&amp;quot;, &amp;quot;model_used&amp;quot;: &amp;quot;Sana&amp;quot;, &amp;quot;style&amp;quot;: &amp;quot;Cinematic&amp;quot;, &amp;quot;timestamp&amp;quot;: &amp;quot;2025-05-27T22:45:01.978055&amp;quot;, &amp;quot;status&amp;quot;: &amp;quot;success&amp;quot; } /no_think &amp;lt;think&amp;gt; &amp;lt;/think&amp;gt; The image URL is: **https://example.net/cache/tools_s–∞–Ω–∞/2025052‰∏É-224501/image webp** &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;removing the dry multiplier works as expected.&lt;/p&gt; &lt;p&gt;Am I doing something wrong with sampling parameters, is it somewhat expected, any hints?&lt;/p&gt; &lt;p&gt;Thank you in advance&lt;/p&gt; &lt;p&gt;p.s. if someone is interested in the tool you can find it &lt;a href="https://openwebui.com/t/fakezeta/sana_image_generation"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fakezeta"&gt; /u/fakezeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdb1n/help_effect_of_dry_sampling_on_quality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdb1n/help_effect_of_dry_sampling_on_quality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdb1n/help_effect_of_dry_sampling_on_quality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T10:05:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxdbwv</id>
    <title>impressive streamlining in local llm deployment: Gemma 3n downloading directly to my phone without any tinkering. what a time to be alive.</title>
    <updated>2025-05-28T10:07:11+00:00</updated>
    <author>
      <name>/u/thebigvsbattlesfan</name>
      <uri>https://old.reddit.com/user/thebigvsbattlesfan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;google ai edge gallery apk: &lt;a href="https://github.com/google-ai-edge/gallery/wiki/2.-Getting-Started"&gt;https://github.com/google-ai-edge/gallery/wiki/2.-Getting-Started&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebigvsbattlesfan"&gt; /u/thebigvsbattlesfan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bsbufgpjxh3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdbwv/impressive_streamlining_in_local_llm_deployment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdbwv/impressive_streamlining_in_local_llm_deployment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T10:07:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwlxvb</id>
    <title>Run qwen 30b-a3b on Android local with Alibaba MNN Chat</title>
    <updated>2025-05-27T12:26:03+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwlxvb/run_qwen_30ba3b_on_android_local_with_alibaba_mnn/"&gt; &lt;img alt="Run qwen 30b-a3b on Android local with Alibaba MNN Chat" src="https://external-preview.redd.it/aGZnZW1ma2hpYjNmMebcV0-OYASONSRSOZTsoevngxFFIFBRatfx4SVyyBoC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5360d08748a08af161ba7604536a366b958cba1" title="Run qwen 30b-a3b on Android local with Alibaba MNN Chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/alibaba/MNN/blob/master/apps/Android/MnnLlmChat/README.md#version-050"&gt;https://github.com/alibaba/MNN/blob/master/apps/Android/MnnLlmChat/README.md#version-050&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/aafvzgkhib3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwlxvb/run_qwen_30ba3b_on_android_local_with_alibaba_mnn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwlxvb/run_qwen_30ba3b_on_android_local_with_alibaba_mnn/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T12:26:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwmlos</id>
    <title>mtmd : support Qwen 2.5 Omni (input audio+vision, no audio output) by ngxson ¬∑ Pull Request #13784 ¬∑ ggml-org/llama.cpp</title>
    <updated>2025-05-27T12:58:06+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13784"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwmlos/mtmd_support_qwen_25_omni_input_audiovision_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwmlos/mtmd_support_qwen_25_omni_input_audiovision_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T12:58:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwnv4o</id>
    <title>Switched from a PC to Mac for LLM dev - One week Later</title>
    <updated>2025-05-27T13:54:23+00:00</updated>
    <author>
      <name>/u/ETBiggs</name>
      <uri>https://old.reddit.com/user/ETBiggs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ks5sh4/broke_down_and_bought_a_mac_mini_my_processes_run/"&gt;Broke down and bought a Mac Mini - my processes run 5x faster : r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Exactly a week ago I tromped to the Apple Store and bought a Mac Mini M4 Pro with 24gb memory - the model they usually stock in store. I really *didn't* want to move from Windows because I've used Windows since 3.0 and while it has its annoyances, I know the platform and didn't want to stall my development to go down a rabbit hole of new platform hassles - and I'm not a Windows, Mac or Linux 'fan' - they're tools to me - I've used them all - but always thought the MacOS was the least enjoyable to use. &lt;/p&gt; &lt;p&gt;Despite my reservations I bought the thing - and a week later - I'm glad I did - it's a keeper. &lt;/p&gt; &lt;p&gt;It took about 2 hours to set up my simple-as-possible free stack. Anaconda, Ollama, VScode. Download models, build model files, and maybe an hour of cursing to adjust the code for the Mac and I was up and running. I have a few python libraries that complain a bit but still run fine - no issues there. &lt;/p&gt; &lt;p&gt;The unified memory is a game-changer. It's not like having a gamer box with multiple slots having Nvidia cards, but it fits my use-case perfectly - I need to be able to travel with it in a backpack. I run a 13b model 5x faster than my CPU-constrained MiniPC did with an 8b model. I do need to use a free Mac utility to speed my fans up to full blast when running so I don't melt my circuit boards and void my warranty - but this box is the sweet-spot for me. &lt;/p&gt; &lt;p&gt;Still not a big lover of the MacOS but it works - and the hardware and unified memory architecture jams a lot into a small package. &lt;/p&gt; &lt;p&gt;I was hesitant to make the switch because I thought it would be a hassle - but it wasn't all that bad. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ETBiggs"&gt; /u/ETBiggs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwnv4o/switched_from_a_pc_to_mac_for_llm_dev_one_week/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwnv4o/switched_from_a_pc_to_mac_for_llm_dev_one_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwnv4o/switched_from_a_pc_to_mac_for_llm_dev_one_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T13:54:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kx3h5w</id>
    <title>How are you using Qwen?</title>
    <updated>2025-05-28T00:29:57+00:00</updated>
    <author>
      <name>/u/xnick77x</name>
      <uri>https://old.reddit.com/user/xnick77x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm currently training speculative decoding models on Qwen, aiming for 3-4x faster inference. However, I‚Äôve noticed that Qwen‚Äôs reasoning style significantly differs from typical LLM outputs, reducing the expected performance gains. To address this, I‚Äôm looking to enhance training with additional reasoning-focused datasets aligned closely with real-world use cases.&lt;/p&gt; &lt;p&gt;I‚Äôd love your insights: ‚Ä¢ Which model are you currently using? ‚Ä¢ Do your applications primarily involve reasoning, or are they mostly direct outputs? Or a combination? ‚Ä¢ What‚Äôs your main use case for Qwen? coding, Q&amp;amp;A, or something else?&lt;/p&gt; &lt;p&gt;If you‚Äôre curious how I‚Äôm training the model, I‚Äôve open-sourced the repo and posted here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/s/2JXNhGInkx"&gt;https://www.reddit.com/r/LocalLLaMA/s/2JXNhGInkx&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xnick77x"&gt; /u/xnick77x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx3h5w/how_are_you_using_qwen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx3h5w/how_are_you_using_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kx3h5w/how_are_you_using_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T00:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwrv8g</id>
    <title>Hunyuan releases HunyuanPortrait</title>
    <updated>2025-05-27T16:34:10+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwrv8g/hunyuan_releases_hunyuanportrait/"&gt; &lt;img alt="Hunyuan releases HunyuanPortrait" src="https://preview.redd.it/66xgi7lrqc3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=457d1dce333f1637875489b18ba0f1081aa38b7a" title="Hunyuan releases HunyuanPortrait" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üéâ Introducing HunyuanPortrait: Implicit Condition Control for Enhanced Portrait Animation&lt;/p&gt; &lt;p&gt;üëâWhat's New?&lt;/p&gt; &lt;p&gt;1‚É£Turn static images into living art! üñº‚û°üé•&lt;/p&gt; &lt;p&gt;2‚É£Unparalleled realism with Implicit Control + Stable Video Diffusion&lt;/p&gt; &lt;p&gt;3‚É£SoTA temporal consistency &amp;amp; crystal-clear fidelity&lt;/p&gt; &lt;p&gt;This breakthrough method outperforms existing techniques, effectively disentangling appearance and motion under various image styles.&lt;/p&gt; &lt;p&gt;üëâWhy Matters?&lt;/p&gt; &lt;p&gt;With this method, animators can now create highly controllable and vivid animations by simply using a single portrait image and video clips as driving templates.&lt;/p&gt; &lt;p&gt;‚úÖ One-click animation üñ±: Single image + video template = hyper-realistic results! üéû&lt;/p&gt; &lt;p&gt;‚úÖ Perfectly synced facial dynamics &amp;amp; head movements&lt;/p&gt; &lt;p&gt;‚úÖ Identity consistency locked across all styles&lt;/p&gt; &lt;p&gt;üëâA Game-changer for Fields likeÔºö&lt;/p&gt; &lt;p&gt;‚ñ∂Ô∏èVirtual Reality + AR experiences üëì&lt;/p&gt; &lt;p&gt;‚ñ∂Ô∏èNext-gen gaming Characters üéÆ&lt;/p&gt; &lt;p&gt;‚ñ∂Ô∏èHuman-AI interactions ü§ñüí¨&lt;/p&gt; &lt;p&gt;üìöDive Deeper&lt;/p&gt; &lt;p&gt;Check out our paper to learn more about the magic behind HunyuanPortrait and how it‚Äôs setting a new standard for portrait animation!&lt;/p&gt; &lt;p&gt;üîó Project Page: &lt;a href="https://kkakkkka.github.io/HunyuanPortrait/"&gt;https://kkakkkka.github.io/HunyuanPortrait/&lt;/a&gt; üîó Research Paper: &lt;a href="https://arxiv.org/abs/2503.18860"&gt;https://arxiv.org/abs/2503.18860&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://x.com/tencenthunyuan/status/1912109205525528673?s=46"&gt;https://x.com/tencenthunyuan/status/1912109205525528673?s=46&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üåü Rewriting the rules of digital humans one frame at a time!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/66xgi7lrqc3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwrv8g/hunyuan_releases_hunyuanportrait/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwrv8g/hunyuan_releases_hunyuanportrait/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T16:34:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxbj8u</id>
    <title>T-MAC extends its capabilities to Snapdragon mobile NPU!</title>
    <updated>2025-05-28T08:03:21+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/microsoft/T-MAC/blob/main/t-man/README.md"&gt;https://github.com/microsoft/T-MAC/blob/main/t-man/README.md&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;50 t/s for BitNet-2B-4T on Snapdragon 8G3 NPU&lt;/li&gt; &lt;li&gt;NPU only, doesn't impact other apps&lt;/li&gt; &lt;li&gt;Prebuilt APK for SDG3 devices &lt;a href="https://github.com/microsoft/T-MAC/releases/tag/1.0.0a5"&gt;on github&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/microsoft/T-MAC/blob/main/t-man/README.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxbj8u/tmac_extends_its_capabilities_to_snapdragon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxbj8u/tmac_extends_its_capabilities_to_snapdragon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T08:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxbxmf</id>
    <title>What's possible with each currently purchasable amount of Mac Unified RAM?</title>
    <updated>2025-05-28T08:31:27+00:00</updated>
    <author>
      <name>/u/thibaut_barrere</name>
      <uri>https://old.reddit.com/user/thibaut_barrere</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a bit of an update of &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1gs7w2m/choosing_the_right_mac_for_running_large_llms/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1gs7w2m/choosing_the_right_mac_for_running_large_llms/&lt;/a&gt; more than 6 months later, with different available CPUs/GPUs.&lt;/p&gt; &lt;p&gt;I am going to renew my MacBook Air (M1) into a recent MacBook Air or Pro, and I need to decide what to pick in terms of RAM (afaik options are 24/32/48/64/128 at the moment). Budget is not an issue (business expense with good ROI).&lt;/p&gt; &lt;p&gt;While I do code &amp;amp; data engineering a lot, I'm not interested into LLM for coding (results are always under my expectations), but I'm more interested in PDF -&amp;gt; JSON transcriptions, general LLM use (brainstorming), connection to music / MIDI etc.&lt;/p&gt; &lt;p&gt;Is it worth going the 128 GB route? Or something in between? Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thibaut_barrere"&gt; /u/thibaut_barrere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxbxmf/whats_possible_with_each_currently_purchasable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxbxmf/whats_possible_with_each_currently_purchasable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxbxmf/whats_possible_with_each_currently_purchasable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T08:31:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kx82bo</id>
    <title>How much VRAM headroom for context?</title>
    <updated>2025-05-28T04:24:08+00:00</updated>
    <author>
      <name>/u/Nomski88</name>
      <uri>https://old.reddit.com/user/Nomski88</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Still new to this and couldn't find a decent answer. I've been testing various models and I'm trying to find the largest model that I can run effectively on my 5090. The calculator on HF is giving me errors regardless of which model I enter. Is there a rule of thumb that one can follow for a rough estimate? I want to try running the LIama 70B Q3_K_S model that takes up 30.9GB of VRAM which would only leave me with 1.1GB VRAM for context. Is this too low?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nomski88"&gt; /u/Nomski88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx82bo/how_much_vram_headroom_for_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx82bo/how_much_vram_headroom_for_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kx82bo/how_much_vram_headroom_for_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T04:24:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxafjv</id>
    <title>When do you think the gap between local llm and o4-mini can be closed</title>
    <updated>2025-05-28T06:49:31+00:00</updated>
    <author>
      <name>/u/GregView</name>
      <uri>https://old.reddit.com/user/GregView</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure if OpenAI recently upgraded this o4-mini free version, but I found this model really surpassed almost every local model in both correctness and consistency. I mainly tested on the coding part (not agent mode). It can understand the problem so well with minimal context (even compared to the Claude 3.7 &amp;amp; 4). I really hope one day we can get this thing running in local setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GregView"&gt; /u/GregView &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxafjv/when_do_you_think_the_gap_between_local_llm_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxafjv/when_do_you_think_the_gap_between_local_llm_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxafjv/when_do_you_think_the_gap_between_local_llm_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T06:49:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxc5vo</id>
    <title>MCP Proxy ‚Äì Use your embedded system as an agent</title>
    <updated>2025-05-28T08:47:54+00:00</updated>
    <author>
      <name>/u/arbayi</name>
      <uri>https://old.reddit.com/user/arbayi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxc5vo/mcp_proxy_use_your_embedded_system_as_an_agent/"&gt; &lt;img alt="MCP Proxy ‚Äì Use your embedded system as an agent" src="https://external-preview.redd.it/QE_AwMn8Vhy9CL-rjaMkp2CgPWYkmSjtSuxPv7QHnQs.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ecb64a2bffb05e44d274eb04fb6b8576a8f1055e" title="MCP Proxy ‚Äì Use your embedded system as an agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/x1y4mz3mkh3f1.gif"&gt;https://i.redd.it/x1y4mz3mkh3f1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video: &lt;a href="https://www.youtube.com/watch?v=foCp3ja8FRA"&gt;https://www.youtube.com/watch?v=foCp3ja8FRA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repository: &lt;a href="https://github.com/openserv-labs/mcp-proxy"&gt;https://github.com/openserv-labs/mcp-proxy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I've been playing around with agents, MCP servers and embedded systems for a while. I was trying to figure out the best way to connect my real-time devices to agents and use them in multi-agent workflows.&lt;/p&gt; &lt;p&gt;At OpenServ, we have an API to interact with agents, so at first I thought I'd just run a specialized web server to talk to the platform. But that had its own problems‚Äîmainly memory issues and needing to customize it for each device.&lt;/p&gt; &lt;p&gt;Then we thought, why not just run a regular web server and use it as an agent? The idea is simple, and the implementation is even simpler thanks to MCP. I define my server‚Äôs endpoints as tools in the MCP server, and agents (MCP clients) can call them directly.&lt;/p&gt; &lt;p&gt;Even though the initial idea was to work with embedded systems, this can work for any backend.&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts‚Äîespecially around connecting agents to real-time devices to collect sensor data or control them in mutlti-agent workflows.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arbayi"&gt; /u/arbayi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxc5vo/mcp_proxy_use_your_embedded_system_as_an_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxc5vo/mcp_proxy_use_your_embedded_system_as_an_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxc5vo/mcp_proxy_use_your_embedded_system_as_an_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T08:47:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwj2p2</id>
    <title>The Aider LLM Leaderboards were updated with benchmark results for Claude 4, revealing that Claude 4 Sonnet didn't outperform Claude 3.7 Sonnet</title>
    <updated>2025-05-27T09:37:08+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwj2p2/the_aider_llm_leaderboards_were_updated_with/"&gt; &lt;img alt="The Aider LLM Leaderboards were updated with benchmark results for Claude 4, revealing that Claude 4 Sonnet didn't outperform Claude 3.7 Sonnet" src="https://preview.redd.it/ls92grf5oa3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e89933d9870d06458186daafb142b31f9c95830f" title="The Aider LLM Leaderboards were updated with benchmark results for Claude 4, revealing that Claude 4 Sonnet didn't outperform Claude 3.7 Sonnet" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ls92grf5oa3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwj2p2/the_aider_llm_leaderboards_were_updated_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwj2p2/the_aider_llm_leaderboards_were_updated_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T09:37:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwwwil</id>
    <title>We build Curie: The Open-sourced AI Co-Scientist Making ML More Accessible for Your Research</title>
    <updated>2025-05-27T19:49:33+00:00</updated>
    <author>
      <name>/u/Pleasant-Type2044</name>
      <uri>https://old.reddit.com/user/Pleasant-Type2044</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwwwil/we_build_curie_the_opensourced_ai_coscientist/"&gt; &lt;img alt="We build Curie: The Open-sourced AI Co-Scientist Making ML More Accessible for Your Research" src="https://b.thumbs.redditmedia.com/5WB6xaADo5SQjBMeBNh63ISBduIh2t7sRigkPfA69Lo.jpg" title="We build Curie: The Open-sourced AI Co-Scientist Making ML More Accessible for Your Research" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After personally seeing many researchers in fields like biology, materials science, and chemistry struggle to &lt;strong&gt;apply machine learning&lt;/strong&gt; to &lt;strong&gt;their valuable domain datasets&lt;/strong&gt; to accelerate scientific discovery and gain deeper insights, often due to the lack of specialized ML knowledge needed to select the right algorithms, tune hyperparameters, or interpret model outputs, we knew we had to help.&lt;/p&gt; &lt;p&gt;That's why we're so excited to introduce the new AutoML feature in &lt;a href="https://github.com/Just-Curieous/Curie"&gt;Curie&lt;/a&gt; üî¨, our AI research experimentation co-scientist designed to &lt;strong&gt;make ML more accessible&lt;/strong&gt;! Our goal is to empower researchers like them to &lt;strong&gt;rapidly test hypotheses and extract deep insights from their data&lt;/strong&gt;. Curie automates the aforementioned complex ML pipeline ‚Äì taking the tedious yet critical work.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k2aq1wo6zd3f1.png?width=1455&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a8c186cf9224019b4e3964adff49d7ee612cf05"&gt;https://preview.redd.it/k2aq1wo6zd3f1.png?width=1455&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a8c186cf9224019b4e3964adff49d7ee612cf05&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For example, Curie can generate highly performant models, achieving a 0.99 AUC (top 1% performance) for a melanoma (cancer) detection task. We're passionate about open science and invite you to try Curie and even contribute to making it better for everyone!&lt;/p&gt; &lt;p&gt;Check out our post: &lt;a href="https://www.just-curieous.com/machine-learning/research/2025-05-27-automl-co-scientist.html"&gt;https://www.just-curieous.com/machine-learning/research/2025-05-27-automl-co-scientist.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pleasant-Type2044"&gt; /u/Pleasant-Type2044 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwwwil/we_build_curie_the_opensourced_ai_coscientist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwwwil/we_build_curie_the_opensourced_ai_coscientist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwwwil/we_build_curie_the_opensourced_ai_coscientist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T19:49:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kx2hcm</id>
    <title>Qwen3-14B vs Gemma3-12B</title>
    <updated>2025-05-27T23:42:05+00:00</updated>
    <author>
      <name>/u/COBECT</name>
      <uri>https://old.reddit.com/user/COBECT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you guys thinks about these models? Which one to choose?&lt;/p&gt; &lt;p&gt;I mostly ask some programming knowledge questions, primary Go and Java.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/COBECT"&gt; /u/COBECT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx2hcm/qwen314b_vs_gemma312b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx2hcm/qwen314b_vs_gemma312b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kx2hcm/qwen314b_vs_gemma312b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T23:42:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kx51dp</id>
    <title>Tip for those building agents. The CLI is king.</title>
    <updated>2025-05-28T01:46:50+00:00</updated>
    <author>
      <name>/u/LocoMod</name>
      <uri>https://old.reddit.com/user/LocoMod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx51dp/tip_for_those_building_agents_the_cli_is_king/"&gt; &lt;img alt="Tip for those building agents. The CLI is king." src="https://b.thumbs.redditmedia.com/p3QuldvOYVK8hBY2sPdoITG34rO_nYvSh-jsqyl80VA.jpg" title="Tip for those building agents. The CLI is king." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are a lot of ways of exposing tools to your agents depending on the framework or your implementation. MCP servers are making this trivial. But I am finding that exposing a simple CLI tool to your LLM/Agent with instructions on how to use common cli commands can actually work better, while reducing complexity. For example, the &lt;code&gt;wc&lt;/code&gt; command: &lt;a href="https://en.wikipedia.org/wiki/Wc_(Unix)"&gt;https://en.wikipedia.org/wiki/Wc_(Unix)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Crafting a system prompt for your agents to make use of these universal, but perhaps obscure commands for your level of experience, can greatly increase the probability of a successful task/step completion.&lt;/p&gt; &lt;p&gt;I have been experimenting with using a lot of MCP servers and exposing their tools to my agent fleet implementation (what should a group of agents be called?, a perplexity of agents? :D ), and have found that giving your agents the ability to simply issue cli commands can work a lot better.&lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LocoMod"&gt; /u/LocoMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kx51dp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx51dp/tip_for_those_building_agents_the_cli_is_king/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kx51dp/tip_for_those_building_agents_the_cli_is_king/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T01:46:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwqt64</id>
    <title>[Research] AutoThink: Adaptive reasoning technique that improves local LLM performance by 43% on GPQA-Diamond</title>
    <updated>2025-05-27T15:53:20+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I wanted to share a technique we've been working on called &lt;strong&gt;AutoThink&lt;/strong&gt; that significantly improves reasoning performance on local models through adaptive resource allocation and steering vectors.&lt;/p&gt; &lt;h1&gt;What is AutoThink?&lt;/h1&gt; &lt;p&gt;Instead of giving every query the same amount of &amp;quot;thinking time,&amp;quot; AutoThink:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Classifies query complexity&lt;/strong&gt; (HIGH/LOW) using an adaptive classifier&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamically allocates thinking tokens&lt;/strong&gt; based on complexity (70-90% for hard problems, 20-40% for simple ones)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Uses steering vectors&lt;/strong&gt; to guide reasoning patterns during generation&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Think of it as making your local model &amp;quot;think harder&amp;quot; on complex problems and &amp;quot;think faster&amp;quot; on simple ones.&lt;/p&gt; &lt;h1&gt;Performance Results&lt;/h1&gt; &lt;p&gt;Tested on &lt;strong&gt;DeepSeek-R1-Distill-Qwen-1.5B&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPQA-Diamond&lt;/strong&gt;: 31.06% vs 21.72% baseline (+9.34 points, 43% relative improvement)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MMLU-Pro&lt;/strong&gt;: 26.38% vs 25.58% baseline (+0.8 points)&lt;/li&gt; &lt;li&gt;Uses &lt;strong&gt;fewer tokens&lt;/strong&gt; than baseline approaches&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Technical Approach&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Steering Vectors&lt;/strong&gt;: We use Pivotal Token Search (PTS) - a technique from Microsoft's Phi-4 paper that we implemented and enhanced. These vectors modify activations to encourage specific reasoning patterns:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;depth_and_thoroughness&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;numerical_accuracy&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;self_correction&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;exploration&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;organization&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt;: Built on our adaptive classifier that can learn new complexity categories without retraining.&lt;/p&gt; &lt;h1&gt;Model Compatibility&lt;/h1&gt; &lt;p&gt;Works with any local reasoning model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DeepSeek-R1 variants&lt;/li&gt; &lt;li&gt;Qwen models&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How to Try It&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# Install optillm pip install optillm # Basic usage from optillm.autothink import autothink_decode response = autothink_decode( model, tokenizer, messages, { &amp;quot;steering_dataset&amp;quot;: &amp;quot;codelion/Qwen3-0.6B-pts-steering-vectors&amp;quot;, &amp;quot;target_layer&amp;quot;: 19 # adjust based on your model } ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Full examples in the repo: &lt;a href="https://github.com/codelion/optillm/tree/main/optillm/autothink"&gt;https://github.com/codelion/optillm/tree/main/optillm/autothink&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Research Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327"&gt;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AutoThink Code&lt;/strong&gt;: &lt;a href="https://github.com/codelion/optillm/tree/main/optillm/autothink"&gt;https://github.com/codelion/optillm/tree/main/optillm/autothink&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PTS Implementation&lt;/strong&gt;: &lt;a href="https://github.com/codelion/pts"&gt;https://github.com/codelion/pts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HuggingFace Blog&lt;/strong&gt;: &lt;a href="https://huggingface.co/blog/codelion/pts"&gt;https://huggingface.co/blog/codelion/pts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Adaptive Classifier&lt;/strong&gt;: &lt;a href="https://github.com/codelion/adaptive-classifier"&gt;https://github.com/codelion/adaptive-classifier&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Current Limitations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Requires models that support thinking tokens (&lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Need to tune &lt;code&gt;target_layer&lt;/code&gt; parameter for different model architectures&lt;/li&gt; &lt;li&gt;Steering vector datasets are model-specific (though we provide some pre-computed ones)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What's Next&lt;/h1&gt; &lt;p&gt;We're working on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Support for more model architectures&lt;/li&gt; &lt;li&gt;Better automatic layer detection&lt;/li&gt; &lt;li&gt;Community-driven steering vector datasets&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Discussion&lt;/h1&gt; &lt;p&gt;Has anyone tried similar approaches with local models? I'm particularly interested in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How different model families respond to steering vectors&lt;/li&gt; &lt;li&gt;Alternative ways to classify query complexity&lt;/li&gt; &lt;li&gt;Ideas for extracting better steering vectors&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear your thoughts and results if you try it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwqt64/research_autothink_adaptive_reasoning_technique/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwqt64/research_autothink_adaptive_reasoning_technique/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwqt64/research_autothink_adaptive_reasoning_technique/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T15:53:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kx077t</id>
    <title>Deepseek R2 Release?</title>
    <updated>2025-05-27T22:00:34+00:00</updated>
    <author>
      <name>/u/Old-Medicine2445</name>
      <uri>https://old.reddit.com/user/Old-Medicine2445</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Didn‚Äôt Deepseek say they were accelerating the timeline to release R2 before the original May release date shooting for April? Now that it‚Äôs almost June, have they said anything about R2 or when they will be releasing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-Medicine2445"&gt; /u/Old-Medicine2445 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx077t/deepseek_r2_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx077t/deepseek_r2_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kx077t/deepseek_r2_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T22:00:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxbmr9</id>
    <title>Another Ryzen Max+ 395 machine has been released. Are all the Chinese Max+ 395 machines the same?</title>
    <updated>2025-05-28T08:10:10+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Another AMD Ryzen Max+ 395 mini-pc has been released. The FEVM FA-EX9. For those who kept asking for it, this comes with Oculink. Here's a YT review.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-1kuUqp1X2I"&gt;https://www.youtube.com/watch?v=-1kuUqp1X2I&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I think all the Chinese Max+ mini-pcs are the same. I noticed again that this machine has &lt;em&gt;exactly&lt;/em&gt; the same port layout as the GMK X2. But how can that be if this has Oculink but the X2 doesn't? The Oculink is an addon. It takes up one of the NVME slots. It's just not the port layout, but the motherboards look exactly the same. Down to the same red color. Even the sound level is the same with the same fan configuration 2 blowers and one axial. So it's like one manufacturer is making the MB and then all the other companies are using that MB for their mini-pcs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxbmr9/another_ryzen_max_395_machine_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxbmr9/another_ryzen_max_395_machine_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxbmr9/another_ryzen_max_395_machine_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T08:10:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwk1jm</id>
    <title>Wife isn‚Äôt home, that means H200 in the living room ;D</title>
    <updated>2025-05-27T10:40:11+00:00</updated>
    <author>
      <name>/u/Flintbeker</name>
      <uri>https://old.reddit.com/user/Flintbeker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwk1jm/wife_isnt_home_that_means_h200_in_the_living_room/"&gt; &lt;img alt="Wife isn‚Äôt home, that means H200 in the living room ;D" src="https://a.thumbs.redditmedia.com/CHdnIbD-SLsvZOKpoU7Rs4hqE0GREYpW_lt-IICeGd0.jpg" title="Wife isn‚Äôt home, that means H200 in the living room ;D" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally got our H200 System, until it‚Äôs going in the datacenter next week that means localLLaMa with some extra power :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flintbeker"&gt; /u/Flintbeker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kwk1jm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwk1jm/wife_isnt_home_that_means_h200_in_the_living_room/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwk1jm/wife_isnt_home_that_means_h200_in_the_living_room/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T10:40:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwucpn</id>
    <title>üòûNo hate but claude-4 is disappointing</title>
    <updated>2025-05-27T18:10:17+00:00</updated>
    <author>
      <name>/u/Rare-Programmer-1747</name>
      <uri>https://old.reddit.com/user/Rare-Programmer-1747</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwucpn/no_hate_but_claude4_is_disappointing/"&gt; &lt;img alt="üòûNo hate but claude-4 is disappointing" src="https://preview.redd.it/9dngmfww7d3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d89328b58759f0c926b5258c859b6fbfcf5a5b32" title="üòûNo hate but claude-4 is disappointing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean how the heck literally Is Qwen-3 better than claude-4(the Claude who used to dog walk everyone). this is just disappointing ü´†&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare-Programmer-1747"&gt; /u/Rare-Programmer-1747 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9dngmfww7d3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwucpn/no_hate_but_claude4_is_disappointing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwucpn/no_hate_but_claude4_is_disappointing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T18:10:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kx9nfk</id>
    <title>Megakernel doubles Llama-1B inference speed for batch size 1</title>
    <updated>2025-05-28T05:58:06+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The authors of this &lt;a href="https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles"&gt;bloglike paper&lt;/a&gt; at Stanford found that vLLM and SGLang lose significant performance due to overhead in CUDA usage for low batch sizes - what you usually use when running locally to chat. Their improvement doubles the inference speed on a H100, which however has significantly higher memory bandwidth than a 3090 for example. It remains to be seen how this scales to user GPUs. The benefits will diminish the larger the model gets.&lt;/p&gt; &lt;p&gt;The best thing is that even with their optimizations there seems to be still some room left for further improvements - theoretically. There was also no word on llama.cpp in there. Their publication is a nice &amp;amp; easy read though.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx9nfk/megakernel_doubles_llama1b_inference_speed_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx9nfk/megakernel_doubles_llama1b_inference_speed_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kx9nfk/megakernel_doubles_llama1b_inference_speed_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T05:58:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxa788</id>
    <title>Google AI Edge Gallery</title>
    <updated>2025-05-28T06:33:50+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxa788/google_ai_edge_gallery/"&gt; &lt;img alt="Google AI Edge Gallery" src="https://preview.redd.it/s6rgmrfawg3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4720f1c95bf832e5eacd2490cf5b69783a79a11b" title="Google AI Edge Gallery" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Explore, Experience, and Evaluate the Future of On-Device Generative AI with Google AI Edge.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Google AI Edge Gallery is an experimental app that puts the power of cutting-edge Generative AI models directly into your hands, running entirely on your Android &lt;em&gt;(available now)&lt;/em&gt; and iOS &lt;em&gt;(coming soon)&lt;/em&gt; devices. Dive into a world of creative and practical AI use cases, all running locally, without needing an internet connection once the model is loaded. Experiment with different models, chat, ask questions with images, explore prompts, and more!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/google-ai-edge/gallery?tab=readme-ov-file"&gt;https://github.com/google-ai-edge/gallery?tab=readme-ov-file&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s6rgmrfawg3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxa788/google_ai_edge_gallery/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxa788/google_ai_edge_gallery/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T06:33:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxaxw9</id>
    <title>The Economist: "Companies abandon their generative AI projects"</title>
    <updated>2025-05-28T07:23:16+00:00</updated>
    <author>
      <name>/u/mayalihamur</name>
      <uri>https://old.reddit.com/user/mayalihamur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;a href="https://archive.ph/P51MQ"&gt;recent article&lt;/a&gt; in the Economist claims that &amp;quot;the share of companies abandoning most of their generative-AI pilot projects has risen to 42%, up from 17% last year.&amp;quot; Apparently companies who invested in generative AI and slashed jobs are now disappointed and they began rehiring humans for roles.&lt;/p&gt; &lt;p&gt;The hype with the generative AI increasingly looks like a &amp;quot;we have a solution, now let's find some problems&amp;quot; scenario. Apart from software developers and graphic designers, I wonder how many professionals actually feel the impact of generative AI in their workplace?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mayalihamur"&gt; /u/mayalihamur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxaxw9/the_economist_companies_abandon_their_generative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxaxw9/the_economist_companies_abandon_their_generative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxaxw9/the_economist_companies_abandon_their_generative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T07:23:16+00:00</published>
  </entry>
</feed>
