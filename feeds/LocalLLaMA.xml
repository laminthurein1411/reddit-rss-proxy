<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-08T10:35:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ikj3ck</id>
    <title>GeminiMixSuper - DeepSeek R1+Gemini 1206</title>
    <updated>2025-02-08T08:52:48+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/BunnHack/GeminiMixSuper-Eng-"&gt;https://github.com/BunnHack/GeminiMixSuper-Eng-&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikj3ck/geminimixsuper_deepseek_r1gemini_1206/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikj3ck/geminimixsuper_deepseek_r1gemini_1206/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikj3ck/geminimixsuper_deepseek_r1gemini_1206/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T08:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijsbpx</id>
    <title>If transformers were invented in a company of Anthropic/OpenAI characteristics would other labs ever reverse-engineer them?</title>
    <updated>2025-02-07T10:49:51+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm wondering how obvious would it be how our LLMs works by just observing theirs outputs? Would scientists just say from first looks, oh, attention mechanisms are in place and working wonders, let's go this route. Or quite the opposite, scratching heads for years?&lt;/p&gt; &lt;p&gt;I think, with Sonnet, we have such situation right now. It clearly have something in it that can robustly come to neat conclusions in new/broken scenarios and we scratch our heads for half a year already.&lt;/p&gt; &lt;p&gt;Closed research is disgusting and I'm glad Google published transformers and I hope more companies will follow on this ideology.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijsbpx/if_transformers_were_invented_in_a_company_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijsbpx/if_transformers_were_invented_in_a_company_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijsbpx/if_transformers_were_invented_in_a_company_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T10:49:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikhbop</id>
    <title>Can anyone tell which LLM App is this?</title>
    <updated>2025-02-08T06:49:23+00:00</updated>
    <author>
      <name>/u/73ch_nerd</name>
      <uri>https://old.reddit.com/user/73ch_nerd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikhbop/can_anyone_tell_which_llm_app_is_this/"&gt; &lt;img alt="Can anyone tell which LLM App is this?" src="https://preview.redd.it/4e4g1mmz3vhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fe3c9e4da11a00bde1e9cfb8d322e33618f8864" title="Can anyone tell which LLM App is this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First I thought it was NotebookLM but itâ€™s supporting &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/73ch_nerd"&gt; /u/73ch_nerd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4e4g1mmz3vhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikhbop/can_anyone_tell_which_llm_app_is_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikhbop/can_anyone_tell_which_llm_app_is_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T06:49:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik1pbd</id>
    <title>Could an LLM be finetuned for reverse-engineering assembly code?</title>
    <updated>2025-02-07T18:18:41+00:00</updated>
    <author>
      <name>/u/AkkerKid</name>
      <uri>https://old.reddit.com/user/AkkerKid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As I understand it, Ghidra can look at ASM and &amp;quot;decompile&amp;quot; the code into something that looks like C. It's not always able to do it and it's not perfect. Could an LLM be fine-tuned to help fill in the blanks to further make sense of assembly code?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AkkerKid"&gt; /u/AkkerKid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik1pbd/could_an_llm_be_finetuned_for_reverseengineering/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik1pbd/could_an_llm_be_finetuned_for_reverseengineering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik1pbd/could_an_llm_be_finetuned_for_reverseengineering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T18:18:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik6fy3</id>
    <title>In Feb 2025, whatâ€™s your LLM stack for productivity?</title>
    <updated>2025-02-07T21:37:13+00:00</updated>
    <author>
      <name>/u/james-jiang</name>
      <uri>https://old.reddit.com/user/james-jiang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Incredible how things have changed over the new year from 2024 to 2025.&lt;/p&gt; &lt;p&gt;We have v3 and r1 coming out for free on the app, beating o1 and even o3 in benchmarks like webdevarena.&lt;/p&gt; &lt;p&gt;These models are all open sourced and distilled and hence there are a huge variety of use cases for them depending on your level of compute.&lt;/p&gt; &lt;p&gt;On the proprietary frontier end - we have sonnet, which crushes everyone else in coding. And OpenAI, who themselves are appealing to prosumers with a 200$ per month plan.&lt;/p&gt; &lt;p&gt;I donâ€™t think weâ€™re at a point yet where one model is simply the best for all situations. Sometimes, you need fast inference on more powerful LLMs and thatâ€™s when itâ€™s hard to beat cloud.&lt;/p&gt; &lt;p&gt;Other times, a small local model is enough to do the job. And it runs decently quick enough to not wait for ages.&lt;/p&gt; &lt;p&gt;Sometimes it makes sense to have it as a mobile app (brainstorming) while in other cases having it on the desktop is critical for productivity, context, and copy pasting.&lt;/p&gt; &lt;p&gt;How are you currently using AI to enhance your productivity and how do you choose which LLM to use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/james-jiang"&gt; /u/james-jiang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik6fy3/in_feb_2025_whats_your_llm_stack_for_productivity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik6fy3/in_feb_2025_whats_your_llm_stack_for_productivity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik6fy3/in_feb_2025_whats_your_llm_stack_for_productivity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T21:37:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikk8y5</id>
    <title>vLLM serving LLAMA 3.3 70B and Langflow: how to make my functions callable as tools from agent?</title>
    <updated>2025-02-08T10:15:24+00:00</updated>
    <author>
      <name>/u/UnlikelyBite</name>
      <uri>https://old.reddit.com/user/UnlikelyBite</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;disclaimer: 15+ years of programming background but almost a noob in llm.&lt;/p&gt; &lt;p&gt;This is the command i use to start vllm and serve llama 3.3: &lt;/p&gt; &lt;p&gt;&lt;code&gt;--model meta-llama/Llama-3.3-70B-Instruct --max-model-len 8192 --port 8000 --tensor-parallel-size 2 --enable-auto-tool-choice --tool-call-parser llama3_json --chat-template examples/tool_chat_template_llama3.1_json.jinja&lt;/code&gt;&lt;/p&gt; &lt;p&gt;What i'm trying to do is to build on top of the llm, a multi agent workflow in langflow that can do for example: query on my sql database, execute python code, etc...&lt;/p&gt; &lt;p&gt;the &amp;quot;strange&amp;quot; thing is that when i use OpenAI (aka ClosedAI) the tool calling agent works without any issue and very well...when i change the llm to llama 3.3 the tools are not invoked in the right order or with the required arguments, making the response from llm unusable or completely hallucinated. &lt;/p&gt; &lt;p&gt;Iâ€™m curious if anyone has implemented a similar setup or has an alternative strategy for integrating agents with tool calls using open source models (for example llama or something like that). Is this approach valid or a complete mess? Are there improvements or pitfalls I should be aware of?&lt;/p&gt; &lt;p&gt;Thanks in advance for any feedback or shared experiences! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UnlikelyBite"&gt; /u/UnlikelyBite &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikk8y5/vllm_serving_llama_33_70b_and_langflow_how_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikk8y5/vllm_serving_llama_33_70b_and_langflow_how_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikk8y5/vllm_serving_llama_33_70b_and_langflow_how_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T10:15:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijzcn9</id>
    <title>New model for finetuners: Redemption_Wind_24B</title>
    <updated>2025-02-07T16:42:52+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Mistral&lt;/strong&gt; has blessed us with a capable new &lt;strong&gt;Apache 2.0&lt;/strong&gt; model, but not only that, we finally get a base model to play with as well. After several models with more restrictive licenses, this open release is a welcome surprise. Freedom was &lt;strong&gt;redeemed&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;With this model, I took a &lt;strong&gt;different&lt;/strong&gt; approachâ€”it's designed &lt;strong&gt;less for typical end-user&lt;/strong&gt; usage, and more for the &lt;strong&gt;fine-tuning community&lt;/strong&gt;. While it remains somewhat usable for general purposes, I wouldnâ€™t particularly recommend it for that.&lt;/p&gt; &lt;h1&gt;What is this model?&lt;/h1&gt; &lt;p&gt;This is a &lt;strong&gt;lightly fine-tuned&lt;/strong&gt; version of the Mistral 24B base model, designed as an accessible and adaptable foundation for further fine-tuning and merging fodder. Key modifications include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;ChatML-ified&lt;/strong&gt;, with no additional tokens introduced.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High quality private instruct&lt;/strong&gt;â€”not generated by ChatGPT or Claude, ensuring no slop and good markdown understanding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No refusals&lt;/strong&gt;â€”since itâ€™s a base model, refusals should be minimal to non-existent, though, in early testing, occasional warnings still appear (I assume some were baked into the pre-train).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High-quality private creative writing dataset&lt;/strong&gt; Mainly to dilute baked-in slop further, but it can actually write some stories, not bad for loss ~8.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Small, high-quality private RP dataset&lt;/strong&gt; This was done so further tuning for RP will be easier. The dataset was kept small and contains &lt;strong&gt;ZERO SLOP&lt;/strong&gt;, some entries are of &lt;strong&gt;16k token length&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Exceptional adherence to character cards&lt;/strong&gt; This was done to make it easier for further tunes intended for roleplay.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Mistral 24B &lt;strong&gt;Base&lt;/strong&gt; model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ChatML-ified&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Can &lt;strong&gt;roleplay&lt;/strong&gt; out of the box.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Exceptional&lt;/strong&gt; at following the character card.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gently tuned instruct&lt;/strong&gt;, remained at a &lt;strong&gt;high loss&lt;/strong&gt;, allows for a lot of &lt;strong&gt;further learning&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Useful for &lt;strong&gt;fine-tuners&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Very creative&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Additional thoughts about this base&lt;/h1&gt; &lt;p&gt;With how much modern models are focused on getting them benchmarks, I can definitely sense that some stuff was baked into the pretrain, as this is indeed a base model.&lt;/p&gt; &lt;p&gt;For example, in roleplay you will see stuff like &amp;quot;And he is waiting for your response...&amp;quot;, a classical sloppy phrase. This is quite interesting, as this phrase\phrasing &lt;strong&gt;does not exist&lt;/strong&gt; in any part of the data that was used to train this model. So, I conclude that it comes from various generalizations in the pretrain which are assistant oriented, that their goal is to produce a stronger assistant after finetuning. This is purely my own speculation, and I may be reading too much into it.&lt;/p&gt; &lt;p&gt;Another thing I noticed, while I tuned a few other bases, is that this one is exceptionally coherent, while the training was stopped at an extremely high loss of 8. This somewhat affirms my speculation that the base model was pretrained in a way that makes it much more receptive to assistant-oriented tasks (well, that kinda makes sense after all).&lt;/p&gt; &lt;p&gt;There's some slop in the base, whispers, shivers, all the usual offenders. We have reached the point that probably all future models will be &amp;quot;poisoned&amp;quot; by AI slop, and some will contain trillions of tokens of synthetic data, this is simply the reality of where things stand, and what the state of things continues to be. Already there are ways around it with various samplers, DPO, etc etc... It is what it is.&lt;/p&gt; &lt;h1&gt;Enjoy the model :)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Redemption_Wind_24B"&gt;https://huggingface.co/SicariusSicariiStuff/Redemption_Wind_24B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijzcn9/new_model_for_finetuners_redemption_wind_24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijzcn9/new_model_for_finetuners_redemption_wind_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijzcn9/new_model_for_finetuners_redemption_wind_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T16:42:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik38qd</id>
    <title>I Build a Deep Research with Open Source - And So Can You!</title>
    <updated>2025-02-07T19:22:06+00:00</updated>
    <author>
      <name>/u/NaturalPlastic1551</name>
      <uri>https://old.reddit.com/user/NaturalPlastic1551</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Folks, Iâ€™m a Developer Advocate at &lt;a href="https://zilliz.com/"&gt;Zilliz&lt;/a&gt;, the developers behind the open-source vector database &lt;a href="https://milvus.io/"&gt;Milvus&lt;/a&gt;. (Milvus is an open-source project in the LF AI &amp;amp; Data.)&lt;/p&gt; &lt;p&gt;I recently published a tutorial demonstrating how to easily build an agentic tool inspired by OpenAI's Deep Research - and only using open-source tools! I'll be building on this tutorial in the future to add more advanced agent concepts like conditional execution flow - I'd love to hear your feedback.&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://milvus.io/blog/i-built-a-deep-research-with-open-source-so-can-you.md"&gt;Open-Source Deep Research with Milvus, LangChain, and DeepSeek&lt;/a&gt; Colab: &lt;a href="https://colab.research.google.com/drive/1W5tW8SqWXve7ZwbSb9pVdbt5R2wq105O?usp=sharing"&gt;Baseline for an Open-Source Deep Research&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img rjeivj6aorhe1...&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NaturalPlastic1551"&gt; /u/NaturalPlastic1551 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik38qd/i_build_a_deep_research_with_open_source_and_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik38qd/i_build_a_deep_research_with_open_source_and_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik38qd/i_build_a_deep_research_with_open_source_and_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T19:22:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik9e54</id>
    <title>KokoroSharp - Local TTS in C#</title>
    <updated>2025-02-07T23:46:25+00:00</updated>
    <author>
      <name>/u/Lyrcaxis</name>
      <uri>https://old.reddit.com/user/Lyrcaxis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, to start with, I am working on a fully offline AI voice chat app, and while it's about 90% ready to release, a specific new, high-performance audio model came out (*stares at Kokoro*).&lt;/p&gt; &lt;p&gt;What did I do?&lt;/p&gt; &lt;p&gt;I dropped everything to build a local, cross-platform TTS engine! Beginner-friendly yet flexible.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;KokoroTTS tts = KokoroTTS.LoadModel(); KokoroVoice heartVoice = KokoroVoiceManager.GetVoice(&amp;quot;af_heart&amp;quot;); while (true) { tts.SpeakFast(Console.ReadLine(), heartVoice); } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It's available on &lt;a href="https://www.nuget.org/packages/KokoroSharp/"&gt;NuGet&lt;/a&gt;! Just install the package and you're ready!&lt;/p&gt; &lt;p&gt;I really hope people like it! And, of course, the source is open: &lt;a href="https://github.com/Lyrcaxis/KokoroSharp"&gt;https://github.com/Lyrcaxis/KokoroSharp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lyrcaxis"&gt; /u/Lyrcaxis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik9e54/kokorosharp_local_tts_in_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik9e54/kokorosharp_local_tts_in_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik9e54/kokorosharp_local_tts_in_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T23:46:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikbdwo</id>
    <title>Possible solution for poor token generation performance in llama.cpp on dual AMD Epyc systems</title>
    <updated>2025-02-08T01:20:28+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikbdwo/possible_solution_for_poor_token_generation/"&gt; &lt;img alt="Possible solution for poor token generation performance in llama.cpp on dual AMD Epyc systems" src="https://external-preview.redd.it/ekkK7Py0xlT-lqYgqhKU3OnrjxNRYtCzeCtltKuOicY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c6b1caf9d5319269f2c5edca00d06f1f8fb9382" title="Possible solution for poor token generation performance in llama.cpp on dual AMD Epyc systems" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggerganov/llama.cpp/issues/11744"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikbdwo/possible_solution_for_poor_token_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikbdwo/possible_solution_for_poor_token_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T01:20:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijzj3m</id>
    <title>Major stuff- I was told to post my encounter here for some intelligent eyes, yesterday I got to see o3 mini using its full reasoning</title>
    <updated>2025-02-07T16:50:24+00:00</updated>
    <author>
      <name>/u/vinigrae</name>
      <uri>https://old.reddit.com/user/vinigrae</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijzj3m/major_stuff_i_was_told_to_post_my_encounter_here/"&gt; &lt;img alt="Major stuff- I was told to post my encounter here for some intelligent eyes, yesterday I got to see o3 mini using its full reasoning" src="https://b.thumbs.redditmedia.com/IB0tegxX-WwwaD4hZ8KUXsGRpONVMLdGun8nAWMuLTA.jpg" title="Major stuff- I was told to post my encounter here for some intelligent eyes, yesterday I got to see o3 mini using its full reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had a challenging problem that all LLMs couldnâ€™t solve, even o3 had failed 6 times, but on the 7th time or so my screen looked like it had been hijacked ðŸ˜…, Iâ€™m just saying exactly how it felt to me in that moment. I copied the output as you canâ€™t quite share cursor chat. This isâ€¦real reasoning, the last line is actually the most concerning, the double confirmation. What are yâ€™allâ€™s thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vinigrae"&gt; /u/vinigrae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ijzj3m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijzj3m/major_stuff_i_was_told_to_post_my_encounter_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijzj3m/major_stuff_i_was_told_to_post_my_encounter_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T16:50:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijx1rh</id>
    <title>A script to run a full-model GRPO training of Qwen2.5 0.5B on a free Google Colab T4. +25% on gsm8k eval in just 30 minutes</title>
    <updated>2025-02-07T15:05:55+00:00</updated>
    <author>
      <name>/u/umjustpassingby</name>
      <uri>https://old.reddit.com/user/umjustpassingby</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijx1rh/a_script_to_run_a_fullmodel_grpo_training_of/"&gt; &lt;img alt="A script to run a full-model GRPO training of Qwen2.5 0.5B on a free Google Colab T4. +25% on gsm8k eval in just 30 minutes" src="https://external-preview.redd.it/DaucjXMGsNHM-CtmdilC9-Be6MC8V2z4ykjVCgOkTFc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62ca4cb88917f17e7200a6f1c665b5d959713745" title="A script to run a full-model GRPO training of Qwen2.5 0.5B on a free Google Colab T4. +25% on gsm8k eval in just 30 minutes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umjustpassingby"&gt; /u/umjustpassingby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://gist.github.com/qunash/820c86d1d267ec8051d9f68b4f4bb656"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijx1rh/a_script_to_run_a_fullmodel_grpo_training_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijx1rh/a_script_to_run_a_fullmodel_grpo_training_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T15:05:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijw4l5</id>
    <title>Stop Wasting Your Multi-GPU Setup With llama.cpp: Use vLLM or ExLlamaV2 for Tensor Parallelism</title>
    <updated>2025-02-07T14:24:24+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijw4l5/stop_wasting_your_multigpu_setup_with_llamacpp/"&gt; &lt;img alt="Stop Wasting Your Multi-GPU Setup With llama.cpp: Use vLLM or ExLlamaV2 for Tensor Parallelism" src="https://external-preview.redd.it/qPGhPtldrPs_tjiplZvAwSzWgSrwQ8e0HK8Z8gzfBS0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9bac89a4ad2d360d4f5ad9a4962d0d4f44fddb3d" title="Stop Wasting Your Multi-GPU Setup With llama.cpp: Use vLLM or ExLlamaV2 for Tensor Parallelism" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ahmadosman.com/blog/do-not-use-llama-cpp-or-ollama-on-multi-gpus-setups-use-vllm-or-exllamav2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijw4l5/stop_wasting_your_multigpu_setup_with_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijw4l5/stop_wasting_your_multigpu_setup_with_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T14:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikba3x</id>
    <title>Simple RAG pipeline: Fully dockerized, completely open source.</title>
    <updated>2025-02-08T01:15:17+00:00</updated>
    <author>
      <name>/u/NewspaperSea9851</name>
      <uri>https://old.reddit.com/user/NewspaperSea9851</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, just built out a v0 of a fairly basic RAG implementation. The goal is to have a solid starting workflow from which to branch off and customize to your specific tasks.&lt;/p&gt; &lt;p&gt;If you're looking for a starting point for a solid production-grade RAG implementation - would love for you to check out: &lt;a href="https://github.com/Emissary-Tech/legit-rag"&gt;https://github.com/Emissary-Tech/legit-rag&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NewspaperSea9851"&gt; /u/NewspaperSea9851 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikba3x/simple_rag_pipeline_fully_dockerized_completely/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikba3x/simple_rag_pipeline_fully_dockerized_completely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikba3x/simple_rag_pipeline_fully_dockerized_completely/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T01:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iji47x</id>
    <title>All DeepSeek, all the time.</title>
    <updated>2025-02-07T00:29:14+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iji47x/all_deepseek_all_the_time/"&gt; &lt;img alt="All DeepSeek, all the time." src="https://preview.redd.it/vnyyv4a93mhe1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a2c0ce4fb12db9cd74a7f55ee3931d93b15253d" title="All DeepSeek, all the time." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vnyyv4a93mhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iji47x/all_deepseek_all_the_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iji47x/all_deepseek_all_the_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T00:29:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijxefw</id>
    <title>Cerebras brings instant inference to Mistral Le Chat (Mistral Large 2 @ 1100 tokens/s)</title>
    <updated>2025-02-07T15:21:31+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijxefw/cerebras_brings_instant_inference_to_mistral_le/"&gt; &lt;img alt="Cerebras brings instant inference to Mistral Le Chat (Mistral Large 2 @ 1100 tokens/s)" src="https://external-preview.redd.it/lhNDcosywXktXr0xSigp9rZjY66RKm_rrzGjuSCPQUg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef93da1c5f0005d9bc3c8006030c6226f627ddbb" title="Cerebras brings instant inference to Mistral Le Chat (Mistral Large 2 @ 1100 tokens/s)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;The collaboration between Cerebras and Mistral has yielded a significant breakthrough in AI inference speed with the integration of Cerebras Inference into Mistral's Le Chat platform. The system achieves an unprecedented 1,100 tokens per second for text generation using the 123B parameter Mistral Large 2 model, representing a 10x performance improvement over competing AI assistants like ChatGPT 4o (115 tokens/s) and Claude Sonnet 3.5 (71 tokens/s). This exceptional speed is achieved through a combination of Cerebras's Wafer Scale Engine 3 technology, which utilizes an SRAM-based inference architecture, and speculative decoding techniques developed in partnership with Mistral researchers. The feature, branded as &amp;quot;Flash Answers,&amp;quot; is currently focused on text-based queries and is visually indicated by a lightning bolt icon in the chat interface.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cerebras.ai/blog/mistral-le-chat"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijxefw/cerebras_brings_instant_inference_to_mistral_le/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijxefw/cerebras_brings_instant_inference_to_mistral_le/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T15:21:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikjrf3</id>
    <title>OpenAI asks to Marginalise ILya as He asks investors for it's SSI &amp; also xAI | iLya &amp; elon both were OG cofounders of OpenAI</title>
    <updated>2025-02-08T09:40:24+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjrf3/openai_asks_to_marginalise_ilya_as_he_asks/"&gt; &lt;img alt="OpenAI asks to Marginalise ILya as He asks investors for it's SSI &amp;amp; also xAI | iLya &amp;amp; elon both were OG cofounders of OpenAI" src="https://preview.redd.it/wojbv3ufyvhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=672d5dd30929a3d9877d2f0489d521aeb4f36140" title="OpenAI asks to Marginalise ILya as He asks investors for it's SSI &amp;amp; also xAI | iLya &amp;amp; elon both were OG cofounders of OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wojbv3ufyvhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjrf3/openai_asks_to_marginalise_ilya_as_he_asks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjrf3/openai_asks_to_marginalise_ilya_as_he_asks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T09:40:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijxdue</id>
    <title>Kokoro WebGPU: Real-time text-to-speech running 100% locally in your browser.</title>
    <updated>2025-02-07T15:20:49+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijxdue/kokoro_webgpu_realtime_texttospeech_running_100/"&gt; &lt;img alt="Kokoro WebGPU: Real-time text-to-speech running 100% locally in your browser." src="https://external-preview.redd.it/eXpiZzdyaDVpcWhlMePeQo88FDwgFQaiUAHhHRFDa4M37cixJTBs9Mic6GzX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3934e49e8a9ed14426802e792f696c88e956d015" title="Kokoro WebGPU: Real-time text-to-speech running 100% locally in your browser." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5b2t6sh5iqhe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijxdue/kokoro_webgpu_realtime_texttospeech_running_100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijxdue/kokoro_webgpu_realtime_texttospeech_running_100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T15:20:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikek15</id>
    <title>My DeepSeek R1 671B @ Home plan: CPU+GPU hybrid, 4xGen5 NVMe offload</title>
    <updated>2025-02-08T04:03:51+00:00</updated>
    <author>
      <name>/u/bo_peng</name>
      <uri>https://old.reddit.com/user/bo_peng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"&gt; &lt;img alt="My DeepSeek R1 671B @ Home plan: CPU+GPU hybrid, 4xGen5 NVMe offload" src="https://a.thumbs.redditmedia.com/Yk1sCOddUTNzMn5fOag5QoE4yeshLWIZhFYJ14-t3u0.jpg" title="My DeepSeek R1 671B @ Home plan: CPU+GPU hybrid, 4xGen5 NVMe offload" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uggf4ioi9uhe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=527eb75829f56baaaa8c207f009cb4833e5de321"&gt;https://preview.redd.it/uggf4ioi9uhe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=527eb75829f56baaaa8c207f009cb4833e5de321&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now waiting for 4060ti 16G to arrive. Requires lots of custom code to efficiently utilize this chimera setup :) So stay tuned. I think it can reach 10+ token/s for quantized 671B after optimizations.&lt;/p&gt; &lt;p&gt;You can use &amp;quot;ASUS Hyper M.2 x16 Gen5 Card&amp;quot; to host 4 NVME. And currently you need AMD CPUs to do native x4x4x4x4 bifurcation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bo_peng"&gt; /u/bo_peng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T04:03:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik98l2</id>
    <title>I haven't seen many quad GPU setups so here is one</title>
    <updated>2025-02-07T23:39:23+00:00</updated>
    <author>
      <name>/u/dazzou5ouh</name>
      <uri>https://old.reddit.com/user/dazzou5ouh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik98l2/i_havent_seen_many_quad_gpu_setups_so_here_is_one/"&gt; &lt;img alt="I haven't seen many quad GPU setups so here is one" src="https://preview.redd.it/lav3msh9zshe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a16836ce930a2602c2271a999be6be9f7ee678b" title="I haven't seen many quad GPU setups so here is one" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dazzou5ouh"&gt; /u/dazzou5ouh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lav3msh9zshe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik98l2/i_havent_seen_many_quad_gpu_setups_so_here_is_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik98l2/i_havent_seen_many_quad_gpu_setups_so_here_is_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T23:39:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikgsl6</id>
    <title>Germany: "We released model equivalent to R1 back in November, no reason to worry"</title>
    <updated>2025-02-08T06:14:46+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikgsl6/germany_we_released_model_equivalent_to_r1_back/"&gt; &lt;img alt="Germany: &amp;quot;We released model equivalent to R1 back in November, no reason to worry&amp;quot;" src="https://b.thumbs.redditmedia.com/mRAyAl4yIV32IK8PFM79kLhWfUBplZg1xnMZQcGaLLI.jpg" title="Germany: &amp;quot;We released model equivalent to R1 back in November, no reason to worry&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ikgsl6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikgsl6/germany_we_released_model_equivalent_to_r1_back/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikgsl6/germany_we_released_model_equivalent_to_r1_back/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T06:14:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikguu9</id>
    <title>"Meta Torrented over 81 TB of Data Through Annaâ€™s Archive, Despite Few Seeders"</title>
    <updated>2025-02-08T06:18:48+00:00</updated>
    <author>
      <name>/u/remixer_dec</name>
      <uri>https://old.reddit.com/user/remixer_dec</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remixer_dec"&gt; /u/remixer_dec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://torrentfreak.com/meta-torrented-over-81-tb-of-data-through-annas-archive-despite-few-seeders-250206/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikguu9/meta_torrented_over_81_tb_of_data_through_annas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikguu9/meta_torrented_over_81_tb_of_data_through_annas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T06:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik76bj</id>
    <title>It was Ilya who "closed" OpenAI</title>
    <updated>2025-02-07T22:08:04+00:00</updated>
    <author>
      <name>/u/Singularity-42</name>
      <uri>https://old.reddit.com/user/Singularity-42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik76bj/it_was_ilya_who_closed_openai/"&gt; &lt;img alt="It was Ilya who &amp;quot;closed&amp;quot; OpenAI" src="https://preview.redd.it/juqrkouwishe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2df7513b32ac2840f23b550e538e3f7b2c6c6ca3" title="It was Ilya who &amp;quot;closed&amp;quot; OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Singularity-42"&gt; /u/Singularity-42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/juqrkouwishe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik76bj/it_was_ilya_who_closed_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik76bj/it_was_ilya_who_closed_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T22:08:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikh3vz</id>
    <title>OpenAI is hiding the actual thinking tokens in o3-mini</title>
    <updated>2025-02-08T06:35:07+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"&gt; &lt;img alt="OpenAI is hiding the actual thinking tokens in o3-mini" src="https://external-preview.redd.it/7p74lVWOq7GrqAnrRqOdU-BQFqsEnbk8XozOhsD2YQc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65833b846107b44ca5029ac644441d85516e7126" title="OpenAI is hiding the actual thinking tokens in o3-mini" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Yesterday I discovered that in fact you can not even with the current &amp;quot;thinking&amp;quot; tag on ChatGPT UI have the actual thinking tokens. In a way, the thinking of the model.&lt;/p&gt; &lt;p&gt;How did i know it? By running one single simple prompt about &amp;quot;maze puzzle&amp;quot; over the two models. The sample can be found in this dataset we built at our company. &lt;a href="https://huggingface.co/datasets/jan-hq/Maze-Reasoning"&gt;https://huggingface.co/datasets/jan-hq/Maze-Reasoning&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;The prompt is a question about solving a maze: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Deepseek-R1 got it done in 483 seconds:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2vc5mhbqxuhe1.png?width=1690&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=611419e379856111070a3094ac3fbe7bac397381"&gt;Deepseek-R1 giving the correct result&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ChatGPT 53 seconds:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yoxgrs11yuhe1.png?width=1706&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f106a5f1553b8da728d9ca05dd32ce3702ecd5f"&gt;ChatGPT result also correct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Surprisingly both results are correct but the devil is in the detail. When I inspect Deepseek-R1 thought process, it's very long, with step by step imagination and testing how it would go through the maze.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Deepseek-R1 thinking tokens:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/flt2emzlyuhe1.png?width=1610&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a16c88501392216f8998f3d97277996fac64a18"&gt;Deepseek-R1 thought process is literal pages and pages of thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ChatGPT reason mode thinking tokens:&lt;/strong&gt;&lt;br /&gt; I will put the entire thinking tokens of ChatGPT here for everyone to inspect because it's much much shorter&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Let's break down the maze format. From what I'm seeing, the maze is made up of rows and columns, ranging from 0 to 4. Each cell is represented by its coordinates in the form &amp;lt;|row-col|&amp;gt;, followed by wall tokens like &amp;lt;|up_down_left_wall|&amp;gt; and &amp;lt;|blank|&amp;gt;. The walls specify which sides of each cell are blocked, and the origin and target tokens mark the start and end points. The way these tokens are structured seems important for determining the possible movements. Letâ€™s work with this! Alright, I need to figure out where the origin and target are in the maze. I will look through the maze row by row to find them. Looking at row 1, it appears that the origin is at (1,3) and the target is at (1,2). We need to navigate from the origin (1,3) to the target (1,2). So now, it's just about predicting and outputting the individual moves one by one, separated by spaces. Let's figure out the path! I've figured out the maze's wall tokens, but before moving, I need to check if there's a direct path from the origin at (1,3) to the target at (1,2). The origin has walls on the up and left sides, according to the &amp;lt;|up_left_wall|&amp;gt; token. The target is to the left of the origin, and the cell next to the origin at (1,2) seems to have a wall on the right side, marked by &amp;lt;|down_right_wall|&amp;gt;. Iâ€™m thinking Iâ€™ll need to consider these walls carefully before predicting the next move. From what Iâ€™m seeing, the origin at (1,3) has walls on the left and up sides, so I can only move right or down. But moving left to reach the target is blocked because of the wall at (1,3). I checked the adjacency between (1,3) and (1,2), and itâ€™s blocked by the walls in both cells. So, to get from (1,3) to (1,2), I'll need to find a detour. Letâ€™s move down from (1,3) to (2,3) and explore. Okay, it looks like a direct route from (1,3) to (1,2) isnâ€™t possible due to the walls blocking access. I checked various alternatives, including moving down and exploring different connections, but many of those led to dead ends. I've landed on a slightly longer route: get from (1,3) through (2,3) to (2,4), then to (3,4), and eventually backtracking westward while checking valid moves. After verifying the path through multiple neighbor checks, this looks like the best possible route. ANSWER: &amp;lt;|down|&amp;gt; &amp;lt;|right|&amp;gt; &amp;lt;|down|&amp;gt; &amp;lt;|down|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|down|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|right|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|right|&amp;gt; &amp;lt;|right|&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Do you feel something is off about ChatGPT thought -&amp;gt; ANSWER?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It is very obvious that with the current thought process of ChatGPT reasoning model, &lt;strong&gt;it's glossing over so many details and somehow it can reach the result??&lt;/strong&gt; You basically cannot use the thinking tokens from ChatGPT to train a distilled thinking model because it is just not possible to reach the conclusion with those thinking, these are not actual thinking tokens of the models.&lt;/p&gt; &lt;p&gt;I have some hypothesis:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OpenAI is only providing &amp;quot;summarized&amp;quot; version of their thinking tokens.&lt;/li&gt; &lt;li&gt;OpenAI has a model to output bullshit thinking tokens to distract everyone from training a distill model, so they can say they provide the community with the actual data, but there is no data.&lt;/li&gt; &lt;li&gt;They don't have &amp;quot;readable&amp;quot; thinking model but it's just an approximation of the &amp;quot;latent&amp;quot; thinking tokens.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With the track record of OpenAI and ChatGPT, I am leaning towards &amp;quot;they are summarize or give bullshit thinking tokens&amp;quot; to the users more than they have more advanced model as option 3. Why? Because when I look at the UI it's obvious that the thought process is not outputting token per token but in chunk, which is either a summary, or a totally different model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does this mean?&lt;/strong&gt;&lt;br /&gt; You can't just distill openAI model anymore, so don't assume everyone is distilling their model, &lt;strong&gt;THEY ARE CLOSED AI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The full logs of both answers from ChatGPT and Deepseek-R1 can be found here:&lt;/strong&gt; &lt;a href="https://gist.github.com/tikikun/cf037180f402c5183662768045b59eed"&gt;https://gist.github.com/tikikun/cf037180f402c5183662768045b59eed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The maze dataset we build can be found here:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/datasets/jan-hq/Maze-Reasoning"&gt;https://huggingface.co/datasets/jan-hq/Maze-Reasoning&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T06:35:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik162w</id>
    <title>Trump just said â€œnoâ€ DeepSeek does not pose a national security threat at a press conference</title>
    <updated>2025-02-07T17:57:11+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik162w/trump_just_said_no_deepseek_does_not_pose_a/"&gt; &lt;img alt="Trump just said â€œnoâ€ DeepSeek does not pose a national security threat at a press conference" src="https://preview.redd.it/73sost17arhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d90041a44ec176a4b5a3630a47ce33577c957522" title="Trump just said â€œnoâ€ DeepSeek does not pose a national security threat at a press conference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/73sost17arhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik162w/trump_just_said_no_deepseek_does_not_pose_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik162w/trump_just_said_no_deepseek_does_not_pose_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T17:57:11+00:00</published>
  </entry>
</feed>
