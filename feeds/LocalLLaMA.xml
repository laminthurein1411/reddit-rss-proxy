<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-20T11:05:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jflouy</id>
    <title>Structured outputs with Ollama - what's your recipe for success?</title>
    <updated>2025-03-20T10:21:31+00:00</updated>
    <author>
      <name>/u/RMCPhoto</name>
      <uri>https://old.reddit.com/user/RMCPhoto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with Ollama's structured output feature (using JSON schemas via Pydantic models) and wanted to hear how others are implementing this in their projects. My results have been a bit mixed with Gemma3 and Phi4.&lt;/p&gt; &lt;p&gt;My goal has been information extraction from text.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Questions:&lt;/strong&gt; 1. &lt;strong&gt;Model Performance&lt;/strong&gt;: Which local models (e.g. llama3.1, mixtral, Gemma, phi) have you found most reliable for structured output generation? And for what use case? 2. &lt;strong&gt;Schema Design&lt;/strong&gt;: How are you leveraging Pydantic's field labels/descriptions in your JSON schemas? Are you including semantic descriptions to guide the model? 3. &lt;strong&gt;Prompt Engineering&lt;/strong&gt;: Do you explicitly restate the desired output structure in your prompts &lt;em&gt;in addition&lt;/em&gt; to passing the schema, or rely solely on the schema definition? 4. &lt;strong&gt;Validation Patterns&lt;/strong&gt;: What error handling strategies work best when parsing model responses?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discussion Points:&lt;/strong&gt; - Have you found certain schema structures (nested objects vs flat) work better? - Any clever uses of enums or constrained types? - How does structured output performance compare between models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RMCPhoto"&gt; /u/RMCPhoto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jflouy/structured_outputs_with_ollama_whats_your_recipe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jflouy/structured_outputs_with_ollama_whats_your_recipe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jflouy/structured_outputs_with_ollama_whats_your_recipe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T10:21:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jflqm5</id>
    <title>DSPy based Chain Of Draft Implementation</title>
    <updated>2025-03-20T10:24:58+00:00</updated>
    <author>
      <name>/u/Chance-Beginning8004</name>
      <uri>https://old.reddit.com/user/Chance-Beginning8004</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jflqm5/dspy_based_chain_of_draft_implementation/"&gt; &lt;img alt="DSPy based Chain Of Draft Implementation" src="https://external-preview.redd.it/qRV52tX8_Ljjvrwv3iRCJtHUe6JAv_ow5F9D4FTtckI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c0d9716e45ce1d02f6707ac2b0631797b433792" title="DSPy based Chain Of Draft Implementation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chance-Beginning8004"&gt; /u/Chance-Beginning8004 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://pub.towardsai.net/implementing-chain-of-draft-prompt-technique-with-dspy-ca231c58114f"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jflqm5/dspy_based_chain_of_draft_implementation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jflqm5/dspy_based_chain_of_draft_implementation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T10:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfeujw</id>
    <title>Seeking Advice on Fine-tuning QWQ-32B Model</title>
    <updated>2025-03-20T02:33:03+00:00</updated>
    <author>
      <name>/u/aadityaura</name>
      <uri>https://old.reddit.com/user/aadityaura</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm planning to fine-tune the QWQ-32B model on a custom dataset and would appreciate some guidance from those with experience.&lt;/p&gt; &lt;h1&gt;My Current Situation:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I have a dataset in Alpaca format {&amp;quot;instruction&amp;quot; : &amp;quot;&amp;quot;, &amp;quot;input&amp;quot; : &amp;quot;&amp;quot;, &amp;quot;output&amp;quot; : &amp;quot;&amp;quot;} &lt;/li&gt; &lt;li&gt;I'm unsure about the optimal fine-tuning approach for QWQ-32B&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;I do have few questions&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Can QWQ-32B be effectively fine-tuned using the Alpaca format dataset, or would this be suboptimal?&lt;/li&gt; &lt;li&gt;Should I convert my data to use the &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; format instead using DeepSeek or Claude?&lt;/li&gt; &lt;li&gt;Does QWQ-32B support QLoRA fine-tuning, or is full fine-tuning required?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'd appreciate hearing about your experience fine-tuning QWQ-32B, including any challenges faced and helpful configurations or optimization tips.&lt;/p&gt; &lt;p&gt;Thank you in advance for any insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aadityaura"&gt; /u/aadityaura &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfeujw/seeking_advice_on_finetuning_qwq32b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfeujw/seeking_advice_on_finetuning_qwq32b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfeujw/seeking_advice_on_finetuning_qwq32b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T02:33:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jezj71</id>
    <title>New open-source model for transpiling PyTorch to Triton outperforms DeepSeek-R1 and OpenAI o1 on kernelbench - made with reinforcement fine-tuning</title>
    <updated>2025-03-19T15:23:26+00:00</updated>
    <author>
      <name>/u/Fantastic-Tax6709</name>
      <uri>https://old.reddit.com/user/Fantastic-Tax6709</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jezj71/new_opensource_model_for_transpiling_pytorch_to/"&gt; &lt;img alt="New open-source model for transpiling PyTorch to Triton outperforms DeepSeek-R1 and OpenAI o1 on kernelbench - made with reinforcement fine-tuning" src="https://a.thumbs.redditmedia.com/1f-A5j4uyipf7Q6GI3ac81je-5JNKA_lnSRBYea-gq8.jpg" title="New open-source model for transpiling PyTorch to Triton outperforms DeepSeek-R1 and OpenAI o1 on kernelbench - made with reinforcement fine-tuning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there, we trained a model for translating PyTorch code to Triton and open-sourced it here: &lt;a href="https://huggingface.co/predibase/Predibase-T2T-32B-RFT"&gt;https://huggingface.co/predibase/Predibase-T2T-32B-RFT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To do it, we trained Qwen2.5-Coder-32B-instruct using reinforcement fine-tuning (based on GRPO) and, according to kernelbench, are outperforming DeepSeek-R1 and OpenAI o1 by about 3x.&lt;/p&gt; &lt;p&gt;We wrote about the RFT implementation and the model here: &lt;a href="https://predibase.com/blog/introducing-reinforcement-fine-tuning-on-predibase"&gt;https://predibase.com/blog/introducing-reinforcement-fine-tuning-on-predibase&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vj39t0dcznpe1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=24817b6bf2787122cff9a049681e3afadc0fb2f4"&gt;https://preview.redd.it/vj39t0dcznpe1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=24817b6bf2787122cff9a049681e3afadc0fb2f4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fantastic-Tax6709"&gt; /u/Fantastic-Tax6709 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jezj71/new_opensource_model_for_transpiling_pytorch_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jezj71/new_opensource_model_for_transpiling_pytorch_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jezj71/new_opensource_model_for_transpiling_pytorch_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T15:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfixq8</id>
    <title>LM Studio API outputs are much worse than the ones I get in chat interface</title>
    <updated>2025-03-20T06:49:51+00:00</updated>
    <author>
      <name>/u/forwatching</name>
      <uri>https://old.reddit.com/user/forwatching</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfixq8/lm_studio_api_outputs_are_much_worse_than_the/"&gt; &lt;img alt="LM Studio API outputs are much worse than the ones I get in chat interface" src="https://b.thumbs.redditmedia.com/xuWcvj8n3_fEsOP8I5O3zmIQhCqPziftd0cKosYZXzo.jpg" title="LM Studio API outputs are much worse than the ones I get in chat interface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to get answers with gemma 3 12b q6 with the simple example curl api request on their website, but the outputs are always wrong compared to the ones I get in chat ui. Is it because I need to add parameters into this api? If so, where can I find the same parameters thats being used in chat ui? Thank you&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h76hy66akspe1.png?width=636&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=606e5e4537044e8bc65453e1e88ad61598e370d6"&gt;https://preview.redd.it/h76hy66akspe1.png?width=636&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=606e5e4537044e8bc65453e1e88ad61598e370d6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/forwatching"&gt; /u/forwatching &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfixq8/lm_studio_api_outputs_are_much_worse_than_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfixq8/lm_studio_api_outputs_are_much_worse_than_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfixq8/lm_studio_api_outputs_are_much_worse_than_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T06:49:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jffynb</id>
    <title>What is the best medical LLM that's open source right now? M4 Macbook 128gb Ram</title>
    <updated>2025-03-20T03:32:58+00:00</updated>
    <author>
      <name>/u/DamiaHeavyIndustries</name>
      <uri>https://old.reddit.com/user/DamiaHeavyIndustries</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found a leaderboard for medical LLMs here but is it up to date and relevant? &lt;a href="https://huggingface.co/blog/leaderboard-medicalllm"&gt;https://huggingface.co/blog/leaderboard-medicalllm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any help would be appreciated since I'm going on a mission with intermittent internet and I might need medical advice &lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DamiaHeavyIndustries"&gt; /u/DamiaHeavyIndustries &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jffynb/what_is_the_best_medical_llm_thats_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jffynb/what_is_the_best_medical_llm_thats_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jffynb/what_is_the_best_medical_llm_thats_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T03:32:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jes8ue</id>
    <title>Llama4 is probably coming next month, multi modal, long context</title>
    <updated>2025-03-19T08:25:40+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jes8ue/llama4_is_probably_coming_next_month_multi_modal/"&gt; &lt;img alt="Llama4 is probably coming next month, multi modal, long context" src="https://external-preview.redd.it/rBTq4xgcNZ3Fw4CkfyZwhQC8tEjpI_nR5A7MTEmNxNA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a2e5f0e1f34849faacd7b3f87b98e5ad732879a6" title="Llama4 is probably coming next month, multi modal, long context" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/cblvrkrcwlpe1.png?width=1677&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a2c29ccbad996630f7cddc95d39a35ba9ef3fb6"&gt;https://preview.redd.it/cblvrkrcwlpe1.png?width=1677&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a2c29ccbad996630f7cddc95d39a35ba9ef3fb6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;source:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.meta.com/blog/connect-2025-llamacon-save-the-date/?srsltid=AfmBOoqvpQ6A0__ic3TrgNRj_RoGpBKWSnRmGFO_-RbGs5bZ7ntliloW"&gt;https://www.meta.com/blog/connect-2025-llamacon-save-the-date/?srsltid=AfmBOoqvpQ6A0__ic3TrgNRj_RoGpBKWSnRmGFO_-RbGs5bZ7ntliloW&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Probably ~1M context, multi modal&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jes8ue/llama4_is_probably_coming_next_month_multi_modal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jes8ue/llama4_is_probably_coming_next_month_multi_modal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jes8ue/llama4_is_probably_coming_next_month_multi_modal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T08:25:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf68qs</id>
    <title>LLM Agents are simply Graph ‚Äî Tutorial For Dummies</title>
    <updated>2025-03-19T20:00:42+00:00</updated>
    <author>
      <name>/u/Willing-Site-8137</name>
      <uri>https://old.reddit.com/user/Willing-Site-8137</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! I just posted a quick tutorial explaining how LLM agents (like OpenAI Agents, Pydantic AI, Manus AI, AutoGPT or PerplexityAI) are basically small graphs with loops and branches. For example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OpenAI Agents&lt;/strong&gt;: &lt;a href="https://github.com/openai/openai-agents-python/blob/48ff99bb736249e99251eb2c7ecf00237488c17a/src/agents/run.py#L119"&gt;run.py#L119&lt;/a&gt; for a workflow in graph.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pydantic Agents&lt;/strong&gt;: &lt;a href="https://github.com/pydantic/pydantic-ai/blob/4c0f384a0626299382c22a8e3372638885e18286/pydantic_ai_slim/pydantic_ai/_agent_graph.py#L779"&gt;_agent_graph.py#L779&lt;/a&gt; organizes steps in a graph.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Langchain&lt;/strong&gt;: &lt;a href="https://github.com/langchain-ai/langchain/blob/4d1d726e61ed58b39278903262d19bbe9f010772/libs/langchain/langchain/agents/agent_iterator.py#L174"&gt;agent_iterator.py#L174&lt;/a&gt; demonstrates the loop structure.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: &lt;a href="https://github.com/langchain-ai/langgraph/blob/24f7d7c4399e2c19b634ed7af0d551ad327e25d7/libs/cli/examples/graphs/agent.py#L56"&gt;agent.py#L56&lt;/a&gt; for a graph-based approach.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If all the hype has been confusing, this guide shows how they actually work under the hood, with simple examples. Check it out!&lt;/p&gt; &lt;p&gt;&lt;a href="https://zacharyhuang.substack.com/p/llm-agent-internal-as-a-graph-tutorial"&gt;https://zacharyhuang.substack.com/p/llm-agent-internal-as-a-graph-tutorial&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Willing-Site-8137"&gt; /u/Willing-Site-8137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf68qs/llm_agents_are_simply_graph_tutorial_for_dummies/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf68qs/llm_agents_are_simply_graph_tutorial_for_dummies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf68qs/llm_agents_are_simply_graph_tutorial_for_dummies/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T20:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfdx03</id>
    <title>I built agent routing and handoff capabilities in a framework and language agnostic way - outside the application layer</title>
    <updated>2025-03-20T01:45:08+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfdx03/i_built_agent_routing_and_handoff_capabilities_in/"&gt; &lt;img alt="I built agent routing and handoff capabilities in a framework and language agnostic way - outside the application layer" src="https://preview.redd.it/tg609a462rpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7d737f8a49f0829dc5b0ca7eaf3663ffba06d95" title="I built agent routing and handoff capabilities in a framework and language agnostic way - outside the application layer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just merged to main the ability for developers to define their agents and have archgw (&lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;) detect, process and route to the correct downstream agent in &amp;lt; 200ms &lt;/p&gt; &lt;p&gt;You no longer need a triage agent, write and maintain boilerplate plate routing functions, pass them around to an LLM and manage hand off scenarios yourself. You just define the ‚Äúbusiness logic‚Äù of your agents in your application code like normal and push this pesky routing outside your application layer.&lt;/p&gt; &lt;p&gt;This routing experience is powered by our very capable Arch-Function-3B LLM üôèüöÄüî•&lt;/p&gt; &lt;p&gt;Hope you all like it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tg609a462rpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfdx03/i_built_agent_routing_and_handoff_capabilities_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfdx03/i_built_agent_routing_and_handoff_capabilities_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T01:45:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jez456</id>
    <title>KBLaM by microsoft, This looks interesting</title>
    <updated>2025-03-19T15:05:42+00:00</updated>
    <author>
      <name>/u/AryanEmbered</name>
      <uri>https://old.reddit.com/user/AryanEmbered</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.microsoft.com/en-us/research/blog/introducing-kblam-bringing-plug-and-play-external-knowledge-to-llms/"&gt;https://www.microsoft.com/en-us/research/blog/introducing-kblam-bringing-plug-and-play-external-knowledge-to-llms/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone more knowledgeable, please enlighten us&lt;/p&gt; &lt;p&gt;in what contexts can it replace rag?&lt;/p&gt; &lt;p&gt;I genuinely believe rag getting solved is the next big unlock. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AryanEmbered"&gt; /u/AryanEmbered &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jez456/kblam_by_microsoft_this_looks_interesting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jez456/kblam_by_microsoft_this_looks_interesting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jez456/kblam_by_microsoft_this_looks_interesting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T15:05:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf4u9e</id>
    <title>Why don't we have non-Apple alternative to unified memory?</title>
    <updated>2025-03-19T19:03:47+00:00</updated>
    <author>
      <name>/u/This_Woodpecker_9163</name>
      <uri>https://old.reddit.com/user/This_Woodpecker_9163</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are we sleeping on this and allowing ourselves to be exploited by the GPU giants?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/This_Woodpecker_9163"&gt; /u/This_Woodpecker_9163 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf4u9e/why_dont_we_have_nonapple_alternative_to_unified/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf4u9e/why_dont_we_have_nonapple_alternative_to_unified/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf4u9e/why_dont_we_have_nonapple_alternative_to_unified/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T19:03:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf10ar</id>
    <title>Gemma 3 GRPO now in Unsloth + Bug Fixes</title>
    <updated>2025-03-19T16:25:58+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf10ar/gemma_3_grpo_now_in_unsloth_bug_fixes/"&gt; &lt;img alt="Gemma 3 GRPO now in Unsloth + Bug Fixes" src="https://external-preview.redd.it/zDUwsKwEDam-qG7u2ijw3m6H-OIciOZkuCU51Tgu7r4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=626174c3432ba48d9fed8ccced1e2bbb42d41c7a" title="Gemma 3 GRPO now in Unsloth + Bug Fixes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We collabed with Hugging Face to create a &lt;strong&gt;free notebook&lt;/strong&gt; to train your own reasoning model using &lt;strong&gt;Gemma 3&lt;/strong&gt; and GRPO &amp;amp; also did some fixes for training + inference&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some frameworks had large training losses when finetuning Gemma 3 - Unsloth should have correct losses!&lt;/li&gt; &lt;li&gt;We worked really hard to make Gemma 3 work in a free Colab T4 environment after inference AND &lt;strong&gt;training did not work for Gemma 3 on older GPUs&lt;/strong&gt; limited to float16. This issue affected all frameworks including us, transformers, vLLM etc.&lt;/li&gt; &lt;li&gt;Note - it's NOT a bug in Gemma 3 - in fact I consider it a &lt;strong&gt;very cool feature&lt;/strong&gt;!! It's the first time I've seen this behavior, and it's probably maybe why Gemma 3 seems extremely powerful for it's size!&lt;/li&gt; &lt;li&gt;I found that Gemma 3 had &lt;strong&gt;infinite activations&lt;/strong&gt; if one uses float16, since float16's maximum range is 65504, and Gemma 3 had values of 800,000 or larger. Llama 3.1 8B's max activation value is around 324.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ibevwuip5ope1.png?width=3580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05185613d47e1397ff2476d3e570b2c0d8478d30"&gt;https://preview.redd.it/ibevwuip5ope1.png?width=3580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05185613d47e1397ff2476d3e570b2c0d8478d30&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; is now the only framework which works in FP16 machines for Gemma 3 inference and training. This means you can now do &lt;strong&gt;GRPO, SFT&lt;/strong&gt;, FFT etc. for Gemma 3, in a free T4 GPU instance on Colab via Unsloth!&lt;/li&gt; &lt;li&gt;Please update Unsloth to the latest version to enable many many bug fixes, and Gemma 3 finetuning support via &lt;code&gt;pip install --upgrade unsloth unsloth_zoo&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Read about our Gemma 3 &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-and-fine-tune-gemma-3#unsloth-fine-tuning-fixes-for-gemma-3"&gt;fixes + details here&lt;/a&gt;!&lt;/li&gt; &lt;li&gt;This fix also solved an issue where training loss was not calculated properly for Gemma 3 in FP16.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We picked Gemma 3 (1B) for our GRPO notebook because of its smaller size, which makes inference faster and easier. But you can also use &lt;strong&gt;Gemma 3 (4B) or (12B)&lt;/strong&gt; just by changing the model name and it should fit on Colab.&lt;/p&gt; &lt;p&gt;For newer folks, we made a step-by-step &lt;a href="https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"&gt;GRPO tutorial here&lt;/a&gt;. And here's our Colab notebooks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GRPO: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/HuggingFace%20Course-Gemma3_(1B"&gt;Gemma 3 (1B) Notebook&lt;/a&gt;-GRPO.ipynb) - long link here: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/HuggingFace%20Course-Gemma3_(1B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/HuggingFace%20Course-Gemma3_(1B)-GRPO.ipynb&lt;/a&gt;-GRPO.ipynb)&lt;/li&gt; &lt;li&gt;Normal SFT: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B"&gt;Gemma 3 (4B) Notebook&lt;/a&gt;.ipynb)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy tuning and let me know if you have any questions! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf10ar/gemma_3_grpo_now_in_unsloth_bug_fixes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf10ar/gemma_3_grpo_now_in_unsloth_bug_fixes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf10ar/gemma_3_grpo_now_in_unsloth_bug_fixes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T16:25:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfgpuc</id>
    <title>AI Policy @ü§ó: Response to the White House AI Action Plan RFI</title>
    <updated>2025-03-20T04:17:20+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/blog/ai-action-wh-2025"&gt;https://huggingface.co/blog/ai-action-wh-2025&lt;/a&gt;&lt;br /&gt; &lt;em&gt;Context: Don't Sleep on (Strongly) Open Models' Capabilities&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Recommendation 1: Recognize Open Source and Open Science as Fundamental to AI Success&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Recommendation 2: Prioritize Efficiency and Reliability to Unlock Broad Innovation&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Recommendation 3: Secure AI through Open, Traceable, and Transparent Systems&lt;/em&gt; &lt;/p&gt; &lt;p&gt;VentureBeat: Hugging Face submits open-source blueprint, challenging Big Tech in White House AI policy fight: &lt;a href="https://venturebeat.com/ai/hugging-face-submits-open-source-blueprint-challenging-big-tech-in-white-house-ai-policy-fight/"&gt;https://venturebeat.com/ai/hugging-face-submits-open-source-blueprint-challenging-big-tech-in-white-house-ai-policy-fight/&lt;/a&gt;&lt;br /&gt; &lt;em&gt;How open source could power America‚Äôs AI advantage: Hugging Face‚Äôs triple-threat strategy&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Smaller, faster, better: Why efficient AI models could democratize the technology revolution&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Big tech vs. little tech: The growing policy battle that could shape AI‚Äôs future&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Between innovation and access: The race to influence America‚Äôs AI future&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfgpuc/ai_policy_response_to_the_white_house_ai_action/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfgpuc/ai_policy_response_to_the_white_house_ai_action/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfgpuc/ai_policy_response_to_the_white_house_ai_action/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T04:17:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jex61b</id>
    <title>If "The Model is the Product" article is true, a lot of AI companies are doomed</title>
    <updated>2025-03-19T13:38:25+00:00</updated>
    <author>
      <name>/u/bttf88</name>
      <uri>https://old.reddit.com/user/bttf88</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious to hear the community's thoughts on this blog post that was near the top of Hacker News yesterday. Unsurprisingly, it got voted down, because I think it's news that not many YC founders want to hear.&lt;/p&gt; &lt;p&gt;I think the argument holds a lot of merit. Basically, major AI Labs like OpenAI and Anthropic are clearly moving towards training their models for Agentic purposes using RL. OpenAI's DeepResearch is one example, Claude Code is another. The models are learning how to select and leverage tools as part of their training - eating away at the complexities of application layer.&lt;/p&gt; &lt;p&gt;If this continues, the application layer that many AI companies today are inhabiting will end up competing with the major AI Labs themselves. The article quotes the VP of AI @ DataBricks predicting that all closed model labs will shut down their APIs within the next 2 -3 years. Wild thought but not totally implausible.&lt;/p&gt; &lt;p&gt;&lt;a href="https://vintagedata.org/blog/posts/model-is-the-product"&gt;https://vintagedata.org/blog/posts/model-is-the-product&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bttf88"&gt; /u/bttf88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jex61b/if_the_model_is_the_product_article_is_true_a_lot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jex61b/if_the_model_is_the_product_article_is_true_a_lot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jex61b/if_the_model_is_the_product_article_is_true_a_lot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T13:38:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jevzm3</id>
    <title>only the real ones remember</title>
    <updated>2025-03-19T12:38:23+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jevzm3/only_the_real_ones_remember/"&gt; &lt;img alt="only the real ones remember" src="https://preview.redd.it/dh21r5dq5npe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5558750be400389e9a0376174765e8479016507" title="only the real ones remember" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dh21r5dq5npe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jevzm3/only_the_real_ones_remember/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jevzm3/only_the_real_ones_remember/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T12:38:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfkv1s</id>
    <title>We should talk about Mistral Small 3.1 vs Mistral Small 3.</title>
    <updated>2025-03-20T09:21:42+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No one saying anything about the new Mistral Small 3.1, no posts about how it perform etc.&lt;/p&gt; &lt;p&gt;From my tests Mistral Small 3.1 performing about the same like original Mistral Small 3.&lt;br /&gt; Same repetitions problems, same long context problems, unstable high temperatures.&lt;br /&gt; I got even a slight worse results at some tasks, coding for example.&lt;/p&gt; &lt;p&gt;Is MS3.1 just a hack to make MS3 multi-modal?&lt;br /&gt; Should we back to MS3 for text-only work?&lt;br /&gt; How was your experience with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfkv1s/we_should_talk_about_mistral_small_31_vs_mistral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfkv1s/we_should_talk_about_mistral_small_31_vs_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfkv1s/we_should_talk_about_mistral_small_31_vs_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T09:21:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfehaf</id>
    <title>Amoral Gemma3 4B</title>
    <updated>2025-03-20T02:13:44+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma 3 4b:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y7hix3wq6rpe1.png?width=774&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b57f9f6c1208aeff74d149bf586492f06f29c3e"&gt;https://preview.redd.it/y7hix3wq6rpe1.png?width=774&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b57f9f6c1208aeff74d149bf586492f06f29c3e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Amoral Gemma 3 4b:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eoejwvbz6rpe1.png?width=805&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a442960a2a13edae62bc0bd9186b2404b3c58d02"&gt;https://preview.redd.it/eoejwvbz6rpe1.png?width=805&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a442960a2a13edae62bc0bd9186b2404b3c58d02&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/soob3123/amoral-gemma3-4B"&gt;soob3123/amoral-gemma3-4B ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/soob3123/amoral-gemma3-4B-gguf"&gt;soob3123/amoral-gemma3-4B-gguf ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Q_8 seems to be the best but Q_4 is good enough for most usecases as well&lt;/p&gt; &lt;p&gt;Edit: Just added the finetuned vision files. if you already downloaded it, down the gguf again to get the uncensored vision capabilities&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfehaf/amoral_gemma3_4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfehaf/amoral_gemma3_4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfehaf/amoral_gemma3_4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T02:13:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jev3fl</id>
    <title>A man can dream</title>
    <updated>2025-03-19T11:47:24+00:00</updated>
    <author>
      <name>/u/Severin_Suveren</name>
      <uri>https://old.reddit.com/user/Severin_Suveren</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jev3fl/a_man_can_dream/"&gt; &lt;img alt="A man can dream" src="https://preview.redd.it/cw3hsv4mwmpe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70e23762b65bf659739163a3e09585431a44e8b5" title="A man can dream" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severin_Suveren"&gt; /u/Severin_Suveren &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cw3hsv4mwmpe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jev3fl/a_man_can_dream/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jev3fl/a_man_can_dream/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T11:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf6igq</id>
    <title>Apache TTS: Orpheus 3B 0.1 FT</title>
    <updated>2025-03-19T20:11:33+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a respect post, it's not my model. In TTS land, a finetuned, Apache licensed 3B boi is a huge drop.&lt;/p&gt; &lt;p&gt;Weights: &lt;a href="https://huggingface.co/canopylabs/orpheus-3b-0.1-ft"&gt;https://huggingface.co/canopylabs/orpheus-3b-0.1-ft&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;Space:&lt;/del&gt; &lt;a href="https://huggingface.co/spaces/canopylabs/orpheus-tts"&gt;&lt;del&gt;https://huggingface.co/spaces/canopylabs/orpheus-tts&lt;/del&gt;&lt;/a&gt; Space taken down again&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;https://github.com/canopyai/Orpheus-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://canopylabs.ai/model-releases"&gt;https://canopylabs.ai/model-releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As an aside, I personally love it when the weights repro the demo samples. Well done.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf6igq/apache_tts_orpheus_3b_01_ft/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf6igq/apache_tts_orpheus_3b_01_ft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf6igq/apache_tts_orpheus_3b_01_ft/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T20:11:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfk5bs</id>
    <title>NVIDIA selling a small amount of 5080s and 5090s at MSRP at GTC</title>
    <updated>2025-03-20T08:25:25+00:00</updated>
    <author>
      <name>/u/False_Care_2957</name>
      <uri>https://old.reddit.com/user/False_Care_2957</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfk5bs/nvidia_selling_a_small_amount_of_5080s_and_5090s/"&gt; &lt;img alt="NVIDIA selling a small amount of 5080s and 5090s at MSRP at GTC" src="https://external-preview.redd.it/3kT0XATxO_t_PsBk5IYdwm0rupWe9BvAFfa1PcU7N7w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=577211de692a570366eda814cb957c6bbfa87da3" title="NVIDIA selling a small amount of 5080s and 5090s at MSRP at GTC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/7p934s4g1tpe1.png?width=1058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f77a7e471836609cac1abd6ebdea26fd3123235"&gt;https://preview.redd.it/7p934s4g1tpe1.png?width=1058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f77a7e471836609cac1abd6ebdea26fd3123235&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/NVIDIAAIDev/status/1902454685153554438"&gt;https://x.com/NVIDIAAIDev/status/1902454685153554438&lt;/a&gt;&lt;/p&gt; &lt;p&gt;While we have to scramble get 5090s at 2-3x the price&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/False_Care_2957"&gt; /u/False_Care_2957 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfk5bs/nvidia_selling_a_small_amount_of_5080s_and_5090s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfk5bs/nvidia_selling_a_small_amount_of_5080s_and_5090s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfk5bs/nvidia_selling_a_small_amount_of_5080s_and_5090s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T08:25:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfm23c</id>
    <title>TikZero - New Approach for Generating Scientific Figures from Text Captions with LLMs</title>
    <updated>2025-03-20T10:47:09+00:00</updated>
    <author>
      <name>/u/DrCracket</name>
      <uri>https://old.reddit.com/user/DrCracket</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfm23c/tikzero_new_approach_for_generating_scientific/"&gt; &lt;img alt="TikZero - New Approach for Generating Scientific Figures from Text Captions with LLMs" src="https://preview.redd.it/carfu383qtpe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1dbc3c3ec140b5c9532f78408e18185bd09d368" title="TikZero - New Approach for Generating Scientific Figures from Text Captions with LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrCracket"&gt; /u/DrCracket &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/carfu383qtpe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfm23c/tikzero_new_approach_for_generating_scientific/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfm23c/tikzero_new_approach_for_generating_scientific/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T10:47:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfdfou</id>
    <title>Creative writing under 15b</title>
    <updated>2025-03-20T01:21:36+00:00</updated>
    <author>
      <name>/u/Wandering_By_</name>
      <uri>https://old.reddit.com/user/Wandering_By_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfdfou/creative_writing_under_15b/"&gt; &lt;img alt="Creative writing under 15b" src="https://preview.redd.it/vd9wm7zyxqpe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f27b582aa94db48353d0959b03086b81afb7cf5" title="Creative writing under 15b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Decided to try a bunch of different models out for creative writing. Figured it might be nice to grade them using larger models for an objective perspective and speed the process up. Realized how asinine it was not to be using a real spreadsheet when I was already 9 through. So enjoy the screenshot. If anyone has suggestions for the next two rounds I'm open to hear them. This one was done using default ollama and openwebui settings.&lt;/p&gt; &lt;p&gt;Prompt for each model: Please provide a complex and entertaining story. The story can be either fictional or true, and you have the freedom to select any genre you believe will best showcase your creative abilities. Originality and creativity will be highly rewarded. While surreal or absurd elements are welcome, ensure they enhance the story‚Äôs entertainment value rather than detract from the narrative coherence. We encourage you to utilize the full potential of your context window to develop a richly detailed story‚Äîshort responses may lead to a deduction in points.&lt;/p&gt; &lt;p&gt;Prompt for the judges:Evaluate the following writing sample using these criteria. Provide me with a score between 0-10 for each section, then use addition to add the scores together for a total value of the writing.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Grammar &amp;amp; Mechanics (foundational correctness)&lt;/li&gt; &lt;li&gt;Clarity &amp;amp; Coherence (sentence/paragraph flow)&lt;/li&gt; &lt;li&gt;Narrative Structure (plot-level organization)&lt;/li&gt; &lt;li&gt;Character Development (depth of personas)&lt;/li&gt; &lt;li&gt;Imagery &amp;amp; Sensory Details (descriptive elements)&lt;/li&gt; &lt;li&gt;Pacing &amp;amp; Rhythm (temporal flow)&lt;/li&gt; &lt;li&gt;Emotional Impact (reader‚Äôs felt experience)&lt;/li&gt; &lt;li&gt;Thematic Depth &amp;amp; Consistency (underlying meaning)&lt;/li&gt; &lt;li&gt;Originality &amp;amp; Creativity (novelty of ideas)&lt;/li&gt; &lt;li&gt;Audience Resonance (connection to readers)&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wandering_By_"&gt; /u/Wandering_By_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vd9wm7zyxqpe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfdfou/creative_writing_under_15b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfdfou/creative_writing_under_15b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T01:21:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf5ufk</id>
    <title>New RTX PRO 6000 with 96G VRAM</title>
    <updated>2025-03-19T19:44:59+00:00</updated>
    <author>
      <name>/u/ThenExtension9196</name>
      <uri>https://old.reddit.com/user/ThenExtension9196</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this at nvidia GTC. Truly a beautiful card. Very similar styling as the 5090FE and even has the same cooling system. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThenExtension9196"&gt; /u/ThenExtension9196 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cost3vsw9ppe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf5ufk/new_rtx_pro_6000_with_96g_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf5ufk/new_rtx_pro_6000_with_96g_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T19:44:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfglbu</id>
    <title>Orpheus TTS Local (LM Studio)</title>
    <updated>2025-03-20T04:09:42+00:00</updated>
    <author>
      <name>/u/Internal_Brain8420</name>
      <uri>https://old.reddit.com/user/Internal_Brain8420</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfglbu/orpheus_tts_local_lm_studio/"&gt; &lt;img alt="Orpheus TTS Local (LM Studio)" src="https://external-preview.redd.it/123zU4tSEAhJBQw-5zzDV7N-1QXm63u6nWHqCb7Eodw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fd48ff5928b7af59f7d95c0c187069ccc64014c" title="Orpheus TTS Local (LM Studio)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Internal_Brain8420"&gt; /u/Internal_Brain8420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/isaiahbjork/orpheus-tts-local"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfglbu/orpheus_tts_local_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfglbu/orpheus_tts_local_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T04:09:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfh1d7</id>
    <title>LLMs are 800x Cheaper for Translation than DeepL</title>
    <updated>2025-03-20T04:37:11+00:00</updated>
    <author>
      <name>/u/Ninjinka</name>
      <uri>https://old.reddit.com/user/Ninjinka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When looking at the cost of translation APIs, I was floored by the prices. Azure is $10 per million characters, Google is $20, and DeepL is $25.&lt;/p&gt; &lt;p&gt;To come up with a rough estimate for a real-time translation use case, I assumed 150 WPM speaking speed, with each word being translated 3 times (since the text gets retranslated multiple times as the context lengthens). This resulted in the following costs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Azure: $1.62/hr&lt;/li&gt; &lt;li&gt;Google: $3.24/hr&lt;/li&gt; &lt;li&gt;DeepL: $4.05/hr&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Assuming the same numbers, &lt;code&gt;gemini-2.0-flash-lite&lt;/code&gt; would cost &lt;strong&gt;less than $0.01/hr&lt;/strong&gt;. Cost varies based on prompt length, but I'm actually getting just under $0.005/hr.&lt;/p&gt; &lt;p&gt;That's over 800x cheaper than DeepL, or 0.1% of the cost.&lt;/p&gt; &lt;p&gt;Presumably the quality of the translations would be somewhat worse, but how much worse? And how long will that disadvantage last? I can stomach a certain amount of worse for 99% cheaper, and it seems easy to foresee that LLMs will surpass the quality of the legacy translation models in the near future.&lt;/p&gt; &lt;p&gt;Right now the accuracy depends a lot on the prompting. I need to run a lot more evals, but so far in my tests I'm seeing that the translations I'm getting are as good (most of the time identical) or &lt;em&gt;better&lt;/em&gt; than Google's the vast majority of the time. I'm confident I can get to 90% of Google's accuracy with better prompting.&lt;/p&gt; &lt;p&gt;I can live with 90% accuracy with a 99.9% cost reduction.&lt;/p&gt; &lt;p&gt;For many, 90% doesn't cut it for their translation needs and they are willing to pay a premium for the best. But the high costs of legacy translation APIs will become increasingly indefensible as LLM-based solutions improve, and we'll see translation incorporated in ways that were previously cost-prohibitive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ninjinka"&gt; /u/Ninjinka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfh1d7/llms_are_800x_cheaper_for_translation_than_deepl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfh1d7/llms_are_800x_cheaper_for_translation_than_deepl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfh1d7/llms_are_800x_cheaper_for_translation_than_deepl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T04:37:11+00:00</published>
  </entry>
</feed>
