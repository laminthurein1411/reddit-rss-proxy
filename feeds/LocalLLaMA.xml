<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-16T15:33:58+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iq6ngx</id>
    <title>KTransformers 2.1 and llama.cpp Comparison with DeepSeek V3</title>
    <updated>2025-02-15T17:39:26+00:00</updated>
    <author>
      <name>/u/CockBrother</name>
      <uri>https://old.reddit.com/user/CockBrother</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone Loves a Graph, Right?&lt;/p&gt; &lt;p&gt;If not, then tables are the next best thing.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Software Used&lt;/th&gt; &lt;th align="left"&gt;Virtual Memory&lt;/th&gt; &lt;th align="left"&gt;Resident Memory&lt;/th&gt; &lt;th align="left"&gt;Model Quantization&lt;/th&gt; &lt;th align="left"&gt;Prompt Eval Rate (tokens/s)&lt;/th&gt; &lt;th align="left"&gt;Eval Rate (tokens/s)&lt;/th&gt; &lt;th align="left"&gt;Relative Performance&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;KTransformers&lt;/td&gt; &lt;td align="left"&gt;714GB&lt;/td&gt; &lt;td align="left"&gt;670GB&lt;/td&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;57.41&lt;/td&gt; &lt;td align="left"&gt;5.80&lt;/td&gt; &lt;td align="left"&gt;1.946&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;KTransformers&lt;/td&gt; &lt;td align="left"&gt;426GB&lt;/td&gt; &lt;td align="left"&gt;380GB&lt;/td&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;83.02&lt;/td&gt; &lt;td align="left"&gt;8.66&lt;/td&gt; &lt;td align="left"&gt;1.986&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama.cpp&lt;/td&gt; &lt;td align="left"&gt;976GB&lt;/td&gt; &lt;td align="left"&gt;970GB&lt;/td&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;24.40&lt;/td&gt; &lt;td align="left"&gt;2.98&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama.cpp&lt;/td&gt; &lt;td align="left"&gt;716GB&lt;/td&gt; &lt;td align="left"&gt;682GB&lt;/td&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;25.58&lt;/td&gt; &lt;td align="left"&gt;4.36&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;A summary of some controlled tests and comparisons between &lt;code&gt;llama.cpp&lt;/code&gt; and &lt;code&gt;KTransformers&lt;/code&gt; for 8-bit and 4-bit quantization on DeepSeek v3. The versions tested were the latest from each project's &lt;code&gt;main&lt;/code&gt; branch as of a few hours before benchmarking.&lt;/p&gt; &lt;h1&gt;Configuration&lt;/h1&gt; &lt;p&gt;Hardware:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD EPYC 7773X CPU&lt;/li&gt; &lt;li&gt;Nvidia 3090 Ti GPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Software:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ubuntu 24.04.1&lt;/li&gt; &lt;li&gt;&lt;code&gt;llama.cpp&lt;/code&gt; build: 4722 (68ff663a)&lt;/li&gt; &lt;li&gt;&lt;code&gt;KTransformers&lt;/code&gt; main/&amp;quot;2.1&amp;quot;&lt;/li&gt; &lt;li&gt;CUDA 12.8&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Framework-Specific Settings:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;KTransformers&lt;/code&gt;: Partial GPU acceleration using a single 3090 Ti GPU. Claims &amp;quot;8K context support&amp;quot; from the 2.1 release notes.&lt;/li&gt; &lt;li&gt;&lt;code&gt;llama.cpp&lt;/code&gt;: CPU-only, 64K context.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Benchmarking Setup&lt;/h1&gt; &lt;p&gt;A significant, but not overly long, prompt of just over 500 tokens was used to ensure it fit within &lt;code&gt;KTransformers&lt;/code&gt;' processing limits. This length was sufficient to benchmark prefill performance.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The default &lt;code&gt;KTransformers&lt;/code&gt; output length of 300 tokens was used for benchmarking generation.&lt;/li&gt; &lt;li&gt;&lt;code&gt;llama.cpp&lt;/code&gt; output length was set to 300 tokens for consistency.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Tuning and Adjustments&lt;/h1&gt; &lt;p&gt;&lt;code&gt;KTransformers&lt;/code&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The model was prompted twice to &amp;quot;warm up&amp;quot; as it does not appear to lock memory to prevent CPU memory from paging out. Letting &lt;code&gt;KTransformers&lt;/code&gt; sit idle for a while caused a ~4x slowdown in prompt evaluation and a ~1.5x slowdown in token evaluation.&lt;/li&gt; &lt;li&gt;Re-prompting restored expected performance.&lt;/li&gt; &lt;li&gt;Other settings were left at their defaults.&lt;/li&gt; &lt;li&gt;The number of CPU threads was set according to the documentation recommendations, not determined by manual tuning.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt;llama.cpp&lt;/code&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Used the default &amp;quot;warm-up&amp;quot; setting before prompting.&lt;/li&gt; &lt;li&gt;Block and user block sizes were optimized at 1024 for the best balance between prefill and generation performance.&lt;/li&gt; &lt;li&gt;The number of threads was determined through experimentation and set to optimal values for the test system.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Observations&lt;/h1&gt; &lt;h1&gt;Memory Requirements and Context Handling&lt;/h1&gt; &lt;p&gt;The DeepSeek V3/R1 models are large, requiring significant memory. Even with 8-bit quantization, a 671B parameter model will not fit on systems with 512GB RAM.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;llama.cpp&lt;/code&gt; requires 300GB of RAM for 65K context, which is substantial.&lt;/li&gt; &lt;li&gt;If memory is available, &lt;code&gt;llama.cpp&lt;/code&gt; can handle contexts over 8Ã— longer than &lt;code&gt;KTransformers&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;With 4-bit quantization, &lt;code&gt;llama.cpp&lt;/code&gt; can process up to 128K context.&lt;/li&gt; &lt;li&gt;&lt;code&gt;KTransformers&lt;/code&gt;' memory scaling efficiency is unclear since it does not yet support significantly larger contexts.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Performance&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;KTransformers&lt;/code&gt; significantly outperforms &lt;code&gt;llama.cpp&lt;/code&gt; in both prefill and generation, leveraging GPU acceleration.&lt;/li&gt; &lt;li&gt;However, the observed 2Ã— performance gain is lower than expected given &lt;code&gt;KTransformers&lt;/code&gt;' claims.&lt;/li&gt; &lt;li&gt;This suggests potential over-optimization for specific hardware in &lt;code&gt;KTransformers&lt;/code&gt;, rather than broad performance improvements.&lt;/li&gt; &lt;li&gt;&lt;code&gt;llama.cpp&lt;/code&gt; is not optimized for MoE (Mixture of Experts) models, affecting its performance in this test.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;llama.cpp&lt;/code&gt; is a mature, feature-rich project with robust parameter control and a stable web API.&lt;/li&gt; &lt;li&gt;&lt;code&gt;KTransformers&lt;/code&gt; lacks many parameter controls but has unique MoE-focused features, including: &lt;ul&gt; &lt;li&gt;The ability to reduce the number of experts used in generation.&lt;/li&gt; &lt;li&gt;Detailed MoE configuration for placing different layers across CPU and GPU resources.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Usage and API Support&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Both frameworks were tested using their command-line &amp;quot;chat&amp;quot; interfaces.&lt;/li&gt; &lt;li&gt;Both provide Python APIs.&lt;/li&gt; &lt;li&gt;&lt;code&gt;llama.cpp&lt;/code&gt; has a stable, fully compatible web API.&lt;/li&gt; &lt;li&gt;&lt;code&gt;KTransformers&lt;/code&gt;' web interface is currently unavailable due to unspecified bugs.&lt;/li&gt; &lt;li&gt;Prior attempts to use &lt;code&gt;KTransformers&lt;/code&gt; with Open WebUI indicated missing API support, making it incompatible.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Final Thoughts&lt;/h1&gt; &lt;p&gt;The growing popularity of DeepSeek V3/R1 may encourage better MoE model support in &lt;code&gt;llama.cpp&lt;/code&gt;. Implementing &lt;code&gt;KTransformers&lt;/code&gt;' innovations in &lt;code&gt;llama.cpp&lt;/code&gt; could improve performance significantly.&lt;/p&gt; &lt;p&gt;However, &lt;code&gt;KTransformers&lt;/code&gt; was designed from the ground up for DeepSeek-like models, and its performance benefits reflect this. Yet, limitations in context length, stability, and configurability make it less compelling for users who need greater flexibility.&lt;/p&gt; &lt;p&gt;At present, &lt;code&gt;KTransformers&lt;/code&gt; feels more like a technology demonstrator than a full replacement for &lt;code&gt;llama.cpp&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Both projects are fast-moving, and performance and features may change dramatically in just a few months.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CockBrother"&gt; /u/CockBrother &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq6ngx/ktransformers_21_and_llamacpp_comparison_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq6ngx/ktransformers_21_and_llamacpp_comparison_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iq6ngx/ktransformers_21_and_llamacpp_comparison_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T17:39:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqtrgf</id>
    <title>Best way to handle GPU</title>
    <updated>2025-02-16T14:49:45+00:00</updated>
    <author>
      <name>/u/_A_Lost_Cat_</name>
      <uri>https://old.reddit.com/user/_A_Lost_Cat_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I want to start a small ai startup ( not a hip one) now I'm looking for different sources for GPU. I found several options, AWS Google some server less ones and... In your experience which one is the best? I already googled, I want your personal experience. Love! CatðŸ˜º&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_A_Lost_Cat_"&gt; /u/_A_Lost_Cat_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtrgf/best_way_to_handle_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtrgf/best_way_to_handle_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtrgf/best_way_to_handle_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T14:49:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqts4r</id>
    <title>GoLU - better than GELU?</title>
    <updated>2025-02-16T14:50:37+00:00</updated>
    <author>
      <name>/u/Spiritual_Piccolo793</name>
      <uri>https://old.reddit.com/user/Spiritual_Piccolo793</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Activation functions are fundamental elements of deep learning architectures as they significantly influence training dynamics. ReLU, while widely used, is prone to the dying neuron problem, which has been mitigated by variants such as LeakyReLU, PReLU, and ELU that better handle negative neuron outputs. Recently, self-gated activations like GELU and Swish have emerged as state-of-the-art alternatives, leveraging their smoothness to ensure stable gradient flow and prevent neuron inactivity. In this work, we introduce the Gompertz Linear Unit (GoLU), a novel self-gated activation function defined as GoLU(x)=xGompertz(x), where Gompertz(x)=eâˆ’eâˆ’x. The GoLU activation leverages the asymmetry in the Gompertz function to reduce variance in the latent space more effectively compared to GELU and Swish, while preserving robust gradient flow. Extensive experiments across diverse tasks, including Image Classification, Language Modeling, Semantic Segmentation, Object Detection, Instance Segmentation, and Diffusion, highlight GoLU's superior performance relative to state-of-the-art activation functions, establishing GoLU as a robust alternative to existing activation functions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiritual_Piccolo793"&gt; /u/Spiritual_Piccolo793 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqts4r/golu_better_than_gelu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqts4r/golu_better_than_gelu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqts4r/golu_better_than_gelu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T14:50:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqtyw6</id>
    <title>Langchain and Langgraph tool calling support for DeepSeek-R1</title>
    <updated>2025-02-16T14:59:34+00:00</updated>
    <author>
      <name>/u/lc19-</name>
      <uri>https://old.reddit.com/user/lc19-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While working on a side project, I needed to use tool calling with DeepSeek-R1, however LangChain and LangGraph haven't supported tool calling for DeepSeek-R1 yet. So I decided to manually write some custom code to do this.&lt;/p&gt; &lt;p&gt;Posting it here to help anyone who needs it. This package also works with any newly released model available on Langchain's ChatOpenAI library (and by extension, any newly released model available on OpenAI's library) which may not have tool calling support yet by LangChain and LangGraph. Please give my Github repo a star if you find this helpful and interesting. Thanks for your support!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/leockl/tool-ahead-of-time"&gt;https://github.com/leockl/tool-ahead-of-time&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lc19-"&gt; /u/lc19- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtyw6/langchain_and_langgraph_tool_calling_support_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtyw6/langchain_and_langgraph_tool_calling_support_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtyw6/langchain_and_langgraph_tool_calling_support_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T14:59:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqufw5</id>
    <title>All models on open llm leaderboard re-evaluated with Math-Verify</title>
    <updated>2025-02-16T15:21:27+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;We initially discovered the math evaluation issues when inspecting Qwen models, which had unusually low scores compared to the self-reported performance. After the Math-Verify introduction, the scores more than doubled for these models, showcasing previous severe underestimation of performance.&lt;/p&gt; &lt;p&gt;But Qwen models arenâ€™t alone. Another major family affected is DeepSeek. After switching to Math-Verify, DeepSeek models almost tripled their scores! This is because their answers are typically wrapped in boxed (\boxed{}) notations which the old evaluator couldnâ€™t extract.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqufw5/all_models_on_open_llm_leaderboard_reevaluated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqufw5/all_models_on_open_llm_leaderboard_reevaluated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqufw5/all_models_on_open_llm_leaderboard_reevaluated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T15:21:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1iquigb</id>
    <title>Long Context Training/Finetuning through Reinforcement-Learning Bootstrapping. A (probably stupid) Idea</title>
    <updated>2025-02-16T15:24:47+00:00</updated>
    <author>
      <name>/u/MassiveMissclicks</name>
      <uri>https://old.reddit.com/user/MassiveMissclicks</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I recently had a thought about a possible training/finetuning method to increase a models stable context size. I am afraid I probably am unaware of some technical limitation that doesn't allow this, but here I go anyway:&lt;/p&gt; &lt;p&gt;What if we could use reinforcement learning to increase a models 'stable' context length?&lt;/p&gt; &lt;p&gt;Most models with large context lengths actually have a way smaller context length where they actually are able to perform their best. I experience severe degradation of quality starting at about 8k Tokens with many, even if they claim to have 32k+ context Length.&lt;/p&gt; &lt;p&gt;Now, what if we could take a page out of Deepseeks playbook and train better performance at longer context lengths via Needle-In-A-Haystack questions and reinforcement learning?&lt;/p&gt; &lt;p&gt;Picture the following setup:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;A small, already trained model that performs at 95%+ in lower context Needle-In-A-Haystack (NIAH from now on) tasks, from what I am aware of there are several models above this threshold. You use these models as question-creators and validators.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The actual model you want to train or finetune, let's just say for this example it starts out with stable NIAH performance up to 16k Tokens. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We now take a 24k Token query and chunk it into 8k segments. We run each of the 8k segments through model 1 and let it create NIAH-Questions for their segments, optionally, we would run the segment with the generated question through model 1 two more times, one to create an answer to its own question and then to validate that answer. &lt;/p&gt; &lt;p&gt;If we assume that model one can reliably create these questions and answers, as well as validate them, which seems quite possible at this point IMO, we could then run the 24k Token Segment through Model 2 with the questions as many times as it takes for it to answer them correctly. (24k is an arbitrary number for this example, one of course would pick a number at the very edge of the current model 2's stability so there is a chance for it to get at least some of the questions right in one shot).&lt;/p&gt; &lt;p&gt;In the last step we would segment the sections back again, feeding the answers to the questions back into model 1 in the order they were given. So the first reply model 2 generates gets assigned to the first chunk, second goes second and so on.&lt;/p&gt; &lt;p&gt;Once everything clears and every chunk is validated separately, we can combine it all, 24k Token Query plus correct answers to the NIAH replies and go to a learning step, repeat this until 24k Token One-Shot stability rises above a certain threshold and expand further, maybe to 32k, continuing on and on until we reach either the models cap in a finetune situation, or as far as we would want with a newly trained model.&lt;/p&gt; &lt;p&gt;Is there something crucial I missed, or is this a theoretically valid approach? I'm assuming there is probably some hard limit to possible Tokens per Model Parameter or something, right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MassiveMissclicks"&gt; /u/MassiveMissclicks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iquigb/long_context_trainingfinetuning_through/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iquigb/long_context_trainingfinetuning_through/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iquigb/long_context_trainingfinetuning_through/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T15:24:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqehm9</id>
    <title>Multilingual creative writing ranking</title>
    <updated>2025-02-15T23:25:20+00:00</updated>
    <author>
      <name>/u/MadScientist-1214</name>
      <uri>https://old.reddit.com/user/MadScientist-1214</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tested various LLMs for their ability to generate creative writing in German. Here's how I conducted the evaluation:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Task: Each model was asked to write a 400-word story in German&lt;/li&gt; &lt;li&gt;Evaluation: Both Claude and ChatGPT assessed each story for: &lt;ul&gt; &lt;li&gt;Language quality (grammar, vocabulary, fluency)&lt;/li&gt; &lt;li&gt;Content quality (creativity, coherence, engagement)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Testing environment: &lt;ul&gt; &lt;li&gt;Some models were tested via Huggingface Spaces: &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/spaces/CohereForAI/c4ai-command"&gt;https://huggingface.co/spaces/CohereForAI/c4ai-command&lt;/a&gt;&lt;/li&gt; &lt;li&gt;huggingface.co/chat&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Others were run locally with minor parameter tuning (temperature and min_p). And some I tested twice.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Ã˜ Language&lt;/th&gt; &lt;th&gt;Ã˜ Content&lt;/th&gt; &lt;th&gt;Average Ã˜&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;nvidia/Llama-3.1-Nemotron-70B-Instruct-HF&lt;/td&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;4.5&lt;/td&gt; &lt;td&gt;4.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;meta-llama/Llama-3.3-70B-Instruct&lt;/td&gt; &lt;td&gt;4.5&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;4.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;arcee-ai/SuperNova-Medius&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;4.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gghfez/Writer-Large-2411-v2.1-AWQ&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;stelterlab/Mistral-Small-24B-Instruct-2501-AWQ&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;google/gemma-2-27b-it&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NousResearch/Hermes-3-Llama-3.1-8B&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;CohereForAI/c4ai-command-r-plus-08-2024&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;3.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Command R 08-2024&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;3.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;aya-expanse-32B&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;3.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mistralai/Mistral-Nemo-Instruct-2407&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen/Qwen2.5-72B-Instruct&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen/Qwen2.5-72B-Instruct-AWQ&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;c4ai-command-r-08-2024-awq&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;3.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;solidrust/Gemma-2-Ataraxy-9B-AWQ&lt;/td&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;solidrust/gemma-2-9b-it-AWQ&lt;/td&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;modelscope/Yi-1.5-34B-Chat-AWQ&lt;/td&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;modelscope/Yi-1.5-34B-Chat-AWQ&lt;/td&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Command R7B 12-2024&lt;/td&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Finally, I took a closer look at nvidia/Llama-3.1-Nemotron-70B-Instruct-HF, which got a perfect grammar score. While its German skills are pretty impressive, I wouldnâ€™t quite agree with the perfect score. The model usually gets German right, but there are a couple of spots where the phrasing feels a bit off (maybe 2-3 instances in every 400 words).&lt;/p&gt; &lt;p&gt;I hope this helps anyone. If you have any other model suggestions, feel free to share them. Iâ€™d also be interested in seeing results in other languages from native speakers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MadScientist-1214"&gt; /u/MadScientist-1214 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqehm9/multilingual_creative_writing_ranking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqehm9/multilingual_creative_writing_ranking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqehm9/multilingual_creative_writing_ranking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T23:25:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq54yg</id>
    <title>Why LLMs are always so confident?</title>
    <updated>2025-02-15T16:32:29+00:00</updated>
    <author>
      <name>/u/Consistent_Equal5327</name>
      <uri>https://old.reddit.com/user/Consistent_Equal5327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They're almost never like &amp;quot;I really don't know what to do here&amp;quot;. Sure sometimes they spit out boilerplate like my training data cuts of at blah blah. But given the huge amount of training data, there must be a lot of incidents where data was like &amp;quot;I don't know&amp;quot;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Consistent_Equal5327"&gt; /u/Consistent_Equal5327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq54yg/why_llms_are_always_so_confident/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq54yg/why_llms_are_always_so_confident/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iq54yg/why_llms_are_always_so_confident/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T16:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqo4kt</id>
    <title>Best local vision model for technical drawings?</title>
    <updated>2025-02-16T08:50:12+00:00</updated>
    <author>
      <name>/u/Mundane_Maximum5795</name>
      <uri>https://old.reddit.com/user/Mundane_Maximum5795</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I think the title says it all, but maybe some context. I work for a small industrial company and we deal with technical drawings on a daily basis. One of our problems is that due to our small size we often lack the time to do some checks on customer and internal drawings before they go in production. I have played with Chatgpt and reading technical drawings and have been blown away with the quality of the analysis, but these were for completely fake drawings to ensure privacy. I have looked at different local llms to replace this, but none come even remotely close to what I need, frequently hallucinating answers. Anybody have a great model/prompt combo that works? Needs to be completely local for infosec reasons...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mundane_Maximum5795"&gt; /u/Mundane_Maximum5795 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqo4kt/best_local_vision_model_for_technical_drawings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqo4kt/best_local_vision_model_for_technical_drawings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqo4kt/best_local_vision_model_for_technical_drawings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T08:50:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipvp2h</id>
    <title>LLMs make flying 1000x better</title>
    <updated>2025-02-15T06:45:17+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Normally I hate flying, internet is flaky and it's hard to get things done. I've found that i can get a lot of what I want the internet for on a local model and with the internet gone I don't get pinged and I can actually head down and focus. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipvp2h/llms_make_flying_1000x_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipvp2h/llms_make_flying_1000x_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ipvp2h/llms_make_flying_1000x_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T06:45:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqe6wv</id>
    <title>Have you guys tried DeepSeek-R1-Zero?</title>
    <updated>2025-02-15T23:10:59+00:00</updated>
    <author>
      <name>/u/CodeMurmurer</name>
      <uri>https://old.reddit.com/user/CodeMurmurer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was reading R1 paper and their pure RL model DeepSeek-R1-Zero got 86.7% on AIME 2024. I wasn't able to find any service hosting the model. Deepseek-R1 got 79.8 on AIME 2024. So I was just wondering if some people here ran it locally or have found a service hosting it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CodeMurmurer"&gt; /u/CodeMurmurer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqe6wv/have_you_guys_tried_deepseekr1zero/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqe6wv/have_you_guys_tried_deepseekr1zero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqe6wv/have_you_guys_tried_deepseekr1zero/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T23:10:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq7yea</id>
    <title>Since it's so hard to find max context windows all in one place, I started a table - contributions welcome!</title>
    <updated>2025-02-15T18:36:22+00:00</updated>
    <author>
      <name>/u/taylorwilsdon</name>
      <uri>https://old.reddit.com/user/taylorwilsdon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq7yea/since_its_so_hard_to_find_max_context_windows_all/"&gt; &lt;img alt="Since it's so hard to find max context windows all in one place, I started a table - contributions welcome!" src="https://external-preview.redd.it/yabl__4Ab0fX56Bb4wbP-vpdHxIRx-xXjgB7Jvk4-sU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d1e1f2fe159585b0d0e6b897ae3b1557ad44856" title="Since it's so hard to find max context windows all in one place, I started a table - contributions welcome!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taylorwilsdon"&gt; /u/taylorwilsdon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/taylorwilsdon/llm-context-limits"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq7yea/since_its_so_hard_to_find_max_context_windows_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iq7yea/since_its_so_hard_to_find_max_context_windows_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T18:36:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipy2fg</id>
    <title>Microsoft drops OmniParser V2 - Agent that controls Windows and Browser</title>
    <updated>2025-02-15T09:45:40+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft just released an open source tool that acts as an Agent that controls Windows and Browser to complete tasks given through prompts.&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://www.microsoft.com/en-us/research/articles/omniparser-v2-turning-any-llm-into-a-computer-use-agent/"&gt;https://www.microsoft.com/en-us/research/articles/omniparser-v2-turning-any-llm-into-a-computer-use-agent/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/microsoft/OmniParser-v2.0"&gt;https://huggingface.co/microsoft/OmniParser-v2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/microsoft/OmniParser/tree/master/omnitool"&gt;https://github.com/microsoft/OmniParser/tree/master/omnitool&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/OmniParser-v2.0og"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipy2fg/microsoft_drops_omniparser_v2_agent_that_controls/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ipy2fg/microsoft_drops_omniparser_v2_agent_that_controls/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T09:45:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqtd15</id>
    <title>How to use Deepkseek r1 locally but with Internet for it to use?</title>
    <updated>2025-02-16T14:29:58+00:00</updated>
    <author>
      <name>/u/AssistantVisible3889</name>
      <uri>https://old.reddit.com/user/AssistantVisible3889</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm writing a script for a video and i use web version of deepthink r1 to get links to facts and research data &lt;/p&gt; &lt;p&gt;I'm installing deepseek r1 using LMstudio following youtube video for a guide &lt;/p&gt; &lt;p&gt;But i failed to find any guide or video to know how to connect it to the internet &lt;/p&gt; &lt;p&gt;I have no background in coding python anything I'm just downloading it locally bcoz website takes lot of time &lt;/p&gt; &lt;p&gt;Can you please suggest me a way to connect it to internet? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AssistantVisible3889"&gt; /u/AssistantVisible3889 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtd15/how_to_use_deepkseek_r1_locally_but_with_internet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtd15/how_to_use_deepkseek_r1_locally_but_with_internet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtd15/how_to_use_deepkseek_r1_locally_but_with_internet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T14:29:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqugti</id>
    <title>Kernel refinement via LLM</title>
    <updated>2025-02-16T15:22:39+00:00</updated>
    <author>
      <name>/u/BreakIt-Boris</name>
      <uri>https://old.reddit.com/user/BreakIt-Boris</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqugti/kernel_refinement_via_llm/"&gt; &lt;img alt="Kernel refinement via LLM" src="https://external-preview.redd.it/Ud40tCnkvgrTgsbDjMTILGOg7G9SqNJ-hrdg_SDxvWo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b1994afccbdb6a19230f6779589edce8ca823a5" title="Kernel refinement via LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I honestly haven't seen this mentioned as of yet. Has left me quite speechless tbh. &lt;/p&gt; &lt;p&gt;&amp;quot;This closed-loop approach makes the code generation process better by guiding it in a different way each time. The team found that by letting this process continue for 15 minutes resulted in an improved attention kernel. &lt;/p&gt; &lt;p&gt;A bar chart showing averaged attention kernel speedup on Hopper GPU, compares the speedup of different attention kernel types between two approaches: 'PyTorch API (Flex Attention)' in orange and 'NVIDIA Workflow with DeepSeek-R1' in green. The PyTorch API maintains a baseline of 1x for all kernels, while the NVIDIA Workflow with DeepSeek-R1 achieves speedups of 1.1x for Causal Mask and Document Mask, 1.5x for Relative Positional, 1.6x for Alibi Bias and Full Mask, and 2.1x for Softcap. Figure 3. Performance of automatically generated optimized attention kernels with flex attention This workflow produced numerically correct kernels for 100% of Level-1 problems and 96% of Level-2 problems, as tested by Stanfordâ€™s KernelBench benchmark. â€Œ&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakIt-Boris"&gt; /u/BreakIt-Boris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developer.nvidia.com/blog/automating-gpu-kernel-generation-with-deepseek-r1-and-inference-time-scaling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqugti/kernel_refinement_via_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqugti/kernel_refinement_via_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T15:22:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipz13t</id>
    <title>Deepseek R1 just became the most liked model ever on Hugging Face just a few weeks after release - with thousands of variants downloaded over 10 million times now</title>
    <updated>2025-02-15T10:58:27+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipz13t/deepseek_r1_just_became_the_most_liked_model_ever/"&gt; &lt;img alt="Deepseek R1 just became the most liked model ever on Hugging Face just a few weeks after release - with thousands of variants downloaded over 10 million times now" src="https://preview.redd.it/lz0e93q9aaje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aaf4142f69cd28ee8e23da316f638a807cbb3526" title="Deepseek R1 just became the most liked model ever on Hugging Face just a few weeks after release - with thousands of variants downloaded over 10 million times now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lz0e93q9aaje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipz13t/deepseek_r1_just_became_the_most_liked_model_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ipz13t/deepseek_r1_just_became_the_most_liked_model_ever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T10:58:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq6ite</id>
    <title>GPT-4o reportedly just dropped on lmarena</title>
    <updated>2025-02-15T17:33:40+00:00</updated>
    <author>
      <name>/u/Worldly_Expression43</name>
      <uri>https://old.reddit.com/user/Worldly_Expression43</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq6ite/gpt4o_reportedly_just_dropped_on_lmarena/"&gt; &lt;img alt="GPT-4o reportedly just dropped on lmarena" src="https://preview.redd.it/cjz352y89cje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f9b527fb493206e2b7fe73cec4a70245655f39c" title="GPT-4o reportedly just dropped on lmarena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worldly_Expression43"&gt; /u/Worldly_Expression43 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cjz352y89cje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq6ite/gpt4o_reportedly_just_dropped_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iq6ite/gpt4o_reportedly_just_dropped_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T17:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqf4v2</id>
    <title>Created a gui for llama.cpp and other apis - all contained in a single html</title>
    <updated>2025-02-15T23:56:24+00:00</updated>
    <author>
      <name>/u/tar_alex</name>
      <uri>https://old.reddit.com/user/tar_alex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqf4v2/created_a_gui_for_llamacpp_and_other_apis_all/"&gt; &lt;img alt="Created a gui for llama.cpp and other apis - all contained in a single html" src="https://external-preview.redd.it/Nnk1dGhwajI1ZWplMX73soshjT9KG-paaQOq0mm21JJPvLVNQkyV4_Cd4e00.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d972894ad4c00fad5f2bde2993e3727538a2c84d" title="Created a gui for llama.cpp and other apis - all contained in a single html" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tar_alex"&gt; /u/tar_alex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8g1dqnj25eje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqf4v2/created_a_gui_for_llamacpp_and_other_apis_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqf4v2/created_a_gui_for_llamacpp_and_other_apis_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T23:56:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipxszq</id>
    <title>Ridiculous</title>
    <updated>2025-02-15T09:25:02+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipxszq/ridiculous/"&gt; &lt;img alt="Ridiculous" src="https://preview.redd.it/95cr17p3u9je1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6000872e6551351c948ff99297bb4130600cc27d" title="Ridiculous" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/95cr17p3u9je1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipxszq/ridiculous/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ipxszq/ridiculous/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T09:25:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqmwsl</id>
    <title>I pay for chatGPT (20 USD), I specifically use the 4o model as a writing editor. For this kind of task, am I better off using a local model instead?</title>
    <updated>2025-02-16T07:21:41+00:00</updated>
    <author>
      <name>/u/MisPreguntas</name>
      <uri>https://old.reddit.com/user/MisPreguntas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't use chatGPT for anything else beyond editing my stories, as mentioned in the title, I only use the 4o model, and I tell it to edit my writing (stories) for grammar, and help me figure out better pacing, better approaches to explain a scene. It's like having a personal editor 24/7.&lt;/p&gt; &lt;p&gt;Am I better off using a local model for this kind of task? If so which one? I've got a 8GB RTX 3070 and 32 GB of RAM.&lt;/p&gt; &lt;p&gt;I'm asking since I don't use chatGPT for anything else. I used to use it for coding and used a better model, but I recently quit programming and only need a writer editor :) &lt;/p&gt; &lt;p&gt;Any model suggestions or system prompts are more than welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MisPreguntas"&gt; /u/MisPreguntas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqmwsl/i_pay_for_chatgpt_20_usd_i_specifically_use_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqmwsl/i_pay_for_chatgpt_20_usd_i_specifically_use_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqmwsl/i_pay_for_chatgpt_20_usd_i_specifically_use_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T07:21:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqh3k1</id>
    <title>Meta's Brain-to-Text AI</title>
    <updated>2025-02-16T01:35:00+00:00</updated>
    <author>
      <name>/u/Particular-Sea2005</name>
      <uri>https://old.reddit.com/user/Particular-Sea2005</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta's groundbreaking research, conducted in collaboration with the Basque Center on Cognition, Brain and Language, marks a significant advancement in non-invasive brain-to-text communication. The study involved 35 healthy volunteers at BCBL, using both magnetoencephalography (MEG) and electroencephalography (EEG) to record brain activity while participants typed sentences[1][2]. Researchers then trained an AI model to reconstruct these sentences solely from the recorded brain signals, achieving up to 80% accuracy in decoding characters from MEG recordings - at least twice the performance of traditional EEG systems[2].&lt;/p&gt; &lt;p&gt;This research builds upon Meta's previous work in decoding image and speech perception from brain activity, now extending to sentence production[1]. The study's success opens new possibilities for non-invasive brain-computer interfaces, potentially aiding in restoring communication for individuals who have lost the ability to speak[2]. However, challenges remain, including the need for further improvements in decoding performance and addressing the practical limitations of MEG technology, which requires subjects to remain still in a magnetically shielded room[1].&lt;/p&gt; &lt;p&gt;Sources [1] Meta announces technology that uses AI and non-invasive magnetic ... &lt;a href="https://gigazine.net/gsc_news/en/20250210-ai-decode-language-from-brain/"&gt;https://gigazine.net/gsc_news/en/20250210-ai-decode-language-from-brain/&lt;/a&gt; [2] Using AI to decode language from the brain and advance our ... &lt;a href="https://ai.meta.com/blog/brain-ai-research-human-communication/"&gt;https://ai.meta.com/blog/brain-ai-research-human-communication/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Particular-Sea2005"&gt; /u/Particular-Sea2005 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqh3k1/metas_braintotext_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqh3k1/metas_braintotext_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqh3k1/metas_braintotext_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T01:35:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqp1gh</id>
    <title>Why we don't use RXs 7600 XT?</title>
    <updated>2025-02-16T09:57:52+00:00</updated>
    <author>
      <name>/u/Anyusername7294</name>
      <uri>https://old.reddit.com/user/Anyusername7294</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This GPU has probably cheapest VRAM out there. $330 for 16gb is crazy value, but most people use RTXs 3090 which cost ~$700 on a used market and draw significantly more power. I know that RTXs are better for other tasks, but as far as I know, only important thing in running LLMs is VRAM, especially capacity. Or there's something I don't know&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anyusername7294"&gt; /u/Anyusername7294 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp1gh/why_we_dont_use_rxs_7600_xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp1gh/why_we_dont_use_rxs_7600_xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp1gh/why_we_dont_use_rxs_7600_xt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T09:57:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqp2dd</id>
    <title>I made a UI Reasoning model with 7b parameters with only 450 lines of data. UIGEN-T1-7B</title>
    <updated>2025-02-16T09:59:45+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp2dd/i_made_a_ui_reasoning_model_with_7b_parameters/"&gt; &lt;img alt="I made a UI Reasoning model with 7b parameters with only 450 lines of data. UIGEN-T1-7B" src="https://external-preview.redd.it/dXNvamJ5ZjQ0aGplMYWu7AlJhgav8Lym1d_sC2ZIykYlrs6Ptnxf7yQvwsov.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d8bdc25d398763f3bc1abe33281d773d4310de4" title="I made a UI Reasoning model with 7b parameters with only 450 lines of data. UIGEN-T1-7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ro798yf44hje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp2dd/i_made_a_ui_reasoning_model_with_7b_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp2dd/i_made_a_ui_reasoning_model_with_7b_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T09:59:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqtfy9</id>
    <title>Just a bunch of H100s required</title>
    <updated>2025-02-16T14:33:57+00:00</updated>
    <author>
      <name>/u/Hv_V</name>
      <uri>https://old.reddit.com/user/Hv_V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"&gt; &lt;img alt="Just a bunch of H100s required" src="https://b.thumbs.redditmedia.com/CMhWZRD3a6zl90Wagyddf6mqUWPT3h6kxFjzeLVrOCc.jpg" title="Just a bunch of H100s required" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d4tu6z13iije1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f1d71756751de2f920195fcdaebe80c93b793c6"&gt;https://preview.redd.it/d4tu6z13iije1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f1d71756751de2f920195fcdaebe80c93b793c6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hv_V"&gt; /u/Hv_V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T14:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqpzpk</id>
    <title>8x RTX 3090 open rig</title>
    <updated>2025-02-16T11:04:58+00:00</updated>
    <author>
      <name>/u/Armym</name>
      <uri>https://old.reddit.com/user/Armym</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqpzpk/8x_rtx_3090_open_rig/"&gt; &lt;img alt="8x RTX 3090 open rig" src="https://preview.redd.it/sx3t2omvghje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8156846c180a3c1bdf1f4c1dceba69bdbf7a6a6" title="8x RTX 3090 open rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The whole length is about 65 cm. Two PSUs 1600W and 2000W 8x RTX 3090, all repasted with copper pads Amd epyc 7th gen 512 gb ram Supermicro mobo&lt;/p&gt; &lt;p&gt;Had to design and 3D print a few things. To raise the GPUs so they wouldn't touch the heatsink of the cpu or PSU. It's not a bug, it's a feature, the airflow is better! Temperatures are maximum at 80C when full load and the fans don't even run full speed.&lt;/p&gt; &lt;p&gt;4 cards connected with risers and 4 with oculink. So far the oculink connection is better, but I am not sure if it's optimal. Only pcie 4x connection to each. &lt;/p&gt; &lt;p&gt;Maybe SlimSAS for all of them would be better? &lt;/p&gt; &lt;p&gt;It runs 70B models very fast. Training is very slow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Armym"&gt; /u/Armym &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sx3t2omvghje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqpzpk/8x_rtx_3090_open_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqpzpk/8x_rtx_3090_open_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T11:04:58+00:00</published>
  </entry>
</feed>
