<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-20T14:24:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jfixq8</id>
    <title>LM Studio API outputs are much worse than the ones I get in chat interface</title>
    <updated>2025-03-20T06:49:51+00:00</updated>
    <author>
      <name>/u/forwatching</name>
      <uri>https://old.reddit.com/user/forwatching</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfixq8/lm_studio_api_outputs_are_much_worse_than_the/"&gt; &lt;img alt="LM Studio API outputs are much worse than the ones I get in chat interface" src="https://b.thumbs.redditmedia.com/xuWcvj8n3_fEsOP8I5O3zmIQhCqPziftd0cKosYZXzo.jpg" title="LM Studio API outputs are much worse than the ones I get in chat interface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to get answers with gemma 3 12b q6 with the simple example curl api request on their website, but the outputs are always wrong compared to the ones I get in chat ui. Is it because I need to add parameters into this api? If so, where can I find the same parameters thats being used in chat ui? Thank you&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h76hy66akspe1.png?width=636&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=606e5e4537044e8bc65453e1e88ad61598e370d6"&gt;https://preview.redd.it/h76hy66akspe1.png?width=636&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=606e5e4537044e8bc65453e1e88ad61598e370d6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/forwatching"&gt; /u/forwatching &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfixq8/lm_studio_api_outputs_are_much_worse_than_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfixq8/lm_studio_api_outputs_are_much_worse_than_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfixq8/lm_studio_api_outputs_are_much_worse_than_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T06:49:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfntc1</id>
    <title>A Primer on Orpheus, Sesame‚Äôs CSM-1B and Kyutai‚Äôs Moshi</title>
    <updated>2025-03-20T12:31:49+00:00</updated>
    <author>
      <name>/u/TrelisResearch</name>
      <uri>https://old.reddit.com/user/TrelisResearch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;*What is CSM-1B?*&lt;/p&gt; &lt;p&gt;CSM-1B is a a small transformer model that allows for text to be converted to speech. Uniquely it is context-aware in the sense that it can take in previous sound waves from the conversation history to inform the style of audio that is generated. It is also heavily trained on multi-turn audio conversational data (which is different than written conversations! And results in much better results for voice assistants.&lt;/p&gt; &lt;p&gt;*What is Orpheus*&lt;/p&gt; &lt;p&gt;Orpheus, like CSM-1B is transformer based TTS model. It is based on a 3B Llama model, rather than 1B for CSM-1B. Unlike CSM, the base and fine-tuned Orpheus models do not encode a speaker number (e.g. speaker 0 or 1) - although this would be possible via fine-tuning. Orpheus DOES use special tokens like &amp;lt;laugh&amp;gt; in order to get the model to make non-word sounds. This kind of fine-tuning would be possible with other models too, but not available out of the box (afaik).&lt;/p&gt; &lt;p&gt;*What is Moshi?*&lt;/p&gt; &lt;p&gt;Moshi is a transformer-based model that can take in speech and respond with speech in real time. It is capable of detecting emotion and also allowing for overlapping speakers ‚Äì in principle. Moshi is primarily based on a 7B parameter model called Helium that was trained from scratch.&lt;/p&gt; &lt;p&gt;*How are these models similar?*&lt;/p&gt; &lt;p&gt;All three models handle sound as tokens. Moshi and CSM-1B make use of a converter called Mimi (developed as part of Moshi) that allows audio to be converted into tokens or tokens to be converted into audio. Orpheus makes use of the SNAC tokeniser which represents sound in a hierarchical way - essentially there are tokens providing a coarse representation and tokens providing a fine representation.&lt;/p&gt; &lt;p&gt;While Moshi is predominantly known as a model that can take in audio and provide responses as audio, in principle it is capable of doing any combinations of speech or text input and speech or text output. In other words, it can be fine tuned to operate as a text to speech model or a speech to text model or a speech to speech model.&lt;/p&gt; &lt;p&gt;CSM-1B on the other hand is uniquely designed for taking in an audio and text history along with a new portion of text that is then converted into an audio output that is consistent with the styles of speakers in the prior history. For example, if you input audio between a man and then a woman, and you then ask for the speech corresponding to new text it will be generated in the voice of a man ‚Äì in line with what one would expect from the prior order of turns.&lt;/p&gt; &lt;p&gt;Orpheus can also take in a text and audio history, to allow for voice cloning, but is not specifically fine-tuned for taking in a conversation history with alternating turns.&lt;/p&gt; &lt;p&gt;*Isn't sound continuous? How do you represent it as tokens?*&lt;/p&gt; &lt;p&gt;By its nature, text is discrete rather than continuous because it consists of letters. By contrast, sound is continuous in nature. It is nonetheless possible to represent a sound wave as a series of tokens, provided one defines the sound with a stream of tokens at sufficiently high frequency ‚Äì 12.5 Hz in the case of Mimi ‚Äì and provided one uses a sufficient number of tokens to represent the sound at each time stamp.&lt;/p&gt; &lt;p&gt;Sound is best represented by a hierarchy of different sets of tokens. Very loosely, you can think of a sound being described like searching in a library‚Ä¶ first, you find the right shelf, then you go to the shelf and you find the closest book, then you find the closest page.&lt;/p&gt; &lt;p&gt;Moshi uses a Mimi-type encoder-decoder with eight levels of hierarchy at a given timestamp, with one for semantic information and seven to represent acoustic information. CSM-1B uses Mimi too, but with 32 levels of hierarchy, which cover semantics and acoustics (there is no separation). Orpheus uses SNAC, which creates tokens at four levels of hierarchy (the initial sound is downsampled to give coarse tokens, then downsampled again to give finer tokens, then again, then again). (I‚Äôm being loose here in describing Mimi versus SNAC. Mimi uses multiple codebooks (think different tokenisers for each level of hierarchy), while SNAC uses one codebook but tokens are created for each level of downsampling.)&lt;/p&gt; &lt;p&gt;*Why tokens?*&lt;/p&gt; &lt;p&gt;If you can treat sound as tokens, then you can use transformers to auto-regressively produce sound. And we know transformers work well for LLMs. And if we can use transformers, then we can stream sound continuously (rather than having to wait for chunks).&lt;/p&gt; &lt;p&gt;*What‚Äôs the problem with using tokens for sound?*&lt;/p&gt; &lt;p&gt;In a hierarchical approach to tokenising (needed for good quality), you have multiple tokens per timestamp. If you sample at 12.5 Hz and have eight layers of hierarchy (8 codebooks), then you need to generate 100 tokens per second. That means you need to generate tokens very fast to keep up with voice!&lt;/p&gt; &lt;p&gt;There are a few ways around this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Use smaller levels of hierarchy and a fast model, e.g. Orpheus with 4 hierarchy layers (from SNAC) and a 3B model OR CSM-1B with 32 codebooks but a 1B backbone transformer.&lt;/li&gt; &lt;li&gt;Use hierarchical transformers (yes, an additional/different form of hierarchy) whereby you use a main transformer to decode a first coarse token, and then a smaller transformer (100M params) to decode the other tokens at that time step (i.e. the other 31 tokens in the case of CSM-1B). Moshi does a variant of this whereby the main transformer decodes one big vector for that timestep, and the tokens are then decoded from another transformer that takes that vector/embedding as an input.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Side-note: It‚Äôs interesting that Kyutai trained Helium 7B from scratch rather than start with an off-the-shelf model. LLMs have gotten better since Helium‚Äôs training was started, which has made it possible to use 1B and 3B models as backbones, like CSM and Orpheus have done. Actually Kyutai have released a 2B version of Helium, supporting this line of argument.&lt;/p&gt; &lt;p&gt;*How are these voice models different from approaches like Style TTS2*&lt;/p&gt; &lt;p&gt;Another way to create sound from text is to use diffusion (e.g. what stable diffusion does for images, same as what DALL-E does). This is how StyleTTS2 works, and it works well, although it is not auto-regressive, I.e. it generates whole phrases rather than autoregressively generating the next part of the phrase. This makes it less adaptive to interruptions or changes in speech that need to happen in response at short notice.&lt;/p&gt; &lt;p&gt;*How is this different from adapter approaches like Llama 3.2 audio (not released) or Qwen Audio*&lt;/p&gt; &lt;p&gt;These two models allow for audio and text input, but they do so by converting audio into an embedding vector that is then adapted (via MLP layers) to be compatible with the input of an LLM (like Llama 3.1 8B). The sound is not (explicitly) encoded hierarchically and the sound is not tokenized. However, passing in an embedded representation does work well as an input BUT there is no easy symmetric way to output sound. By contrast, if one works with sound as tokens, it is possible to input sound (and text) tokens, and output sound (and text) tokens.&lt;/p&gt; &lt;p&gt;*Where from here?*&lt;/p&gt; &lt;p&gt;Right now we have these small (and fast) speech models that - with greater amounts of data - should be able to provide more natural conversations than is possible by cobbling together a transcription model with a text model and then a text to speech model.&lt;/p&gt; &lt;p&gt;However, these models will still lag in terms of reasoning, simply because their transformers are not large enough - and it still appears that models of at least 27B (like Gemma 3) or 24B (like Mistral Small) are needed to get strong reasoning (and even bigger for the best reasoning). Those model sizes would result in generation speeds that are too slow for real time voice. This is why many current applications of voice use the cobbled-together approach of putting multiple models together (TTS, LLM, STT) - even if this means you need to manage how these models AND voice activation and turn detection all mesh together. To be clear, with a unified model like Moshi, there is no need to separately handle voice detection or turn detection - everything is handled by the unified model, including noise cancellation!&lt;/p&gt; &lt;p&gt;In one sense, what has enabled Moshi and CSM-1B and Orpheus, is that tiny models have gotten really strong (like llama 1b) so you can have a good backbone that is still fast. Possibly, if you take the tricks from CSM and from Orpheus and from Moshi, combined - you can maybe move towards a 7B model, or maybe larger, that still is fast enough.&lt;/p&gt; &lt;p&gt;But for now, until new tricks are found (which they will) the unified models are weaker than pure text models on reasoning. The holy grail might be to have a model that uses tokens for text, sound and for images - then you can train end-to-end on all of those forms of data, and potentially get the strongest possible model.&lt;/p&gt; &lt;p&gt;‚Äî THE END. I‚Äôll also put out a video soon (Trelis Research on YouTube and Substack) on these models, including cloning and fine-tuning. --&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TrelisResearch"&gt; /u/TrelisResearch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfntc1/a_primer_on_orpheus_sesames_csm1b_and_kyutais/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfntc1/a_primer_on_orpheus_sesames_csm1b_and_kyutais/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfntc1/a_primer_on_orpheus_sesames_csm1b_and_kyutais/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T12:31:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfnwl1</id>
    <title>Nvidia MPS - run multiple models on one GPU fast</title>
    <updated>2025-03-20T12:36:48+00:00</updated>
    <author>
      <name>/u/Armym</name>
      <uri>https://old.reddit.com/user/Armym</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is an nvidia feature that upped my inference throughput for multiple models by about 43% on one L40S GPU: &lt;a href="https://docs.nvidia.com/deploy/mps/index.html"&gt;https://docs.nvidia.com/deploy/mps/index.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Armym"&gt; /u/Armym &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfnwl1/nvidia_mps_run_multiple_models_on_one_gpu_fast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfnwl1/nvidia_mps_run_multiple_models_on_one_gpu_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfnwl1/nvidia_mps_run_multiple_models_on_one_gpu_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T12:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfdx03</id>
    <title>I built agent routing and handoff capabilities in a framework and language agnostic way - outside the application layer</title>
    <updated>2025-03-20T01:45:08+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfdx03/i_built_agent_routing_and_handoff_capabilities_in/"&gt; &lt;img alt="I built agent routing and handoff capabilities in a framework and language agnostic way - outside the application layer" src="https://preview.redd.it/tg609a462rpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7d737f8a49f0829dc5b0ca7eaf3663ffba06d95" title="I built agent routing and handoff capabilities in a framework and language agnostic way - outside the application layer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just merged to main the ability for developers to define their agents and have archgw (&lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;) detect, process and route to the correct downstream agent in &amp;lt; 200ms &lt;/p&gt; &lt;p&gt;You no longer need a triage agent, write and maintain boilerplate plate routing functions, pass them around to an LLM and manage hand off scenarios yourself. You just define the ‚Äúbusiness logic‚Äù of your agents in your application code like normal and push this pesky routing outside your application layer.&lt;/p&gt; &lt;p&gt;This routing experience is powered by our very capable Arch-Function-3B LLM üôèüöÄüî•&lt;/p&gt; &lt;p&gt;Hope you all like it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tg609a462rpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfdx03/i_built_agent_routing_and_handoff_capabilities_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfdx03/i_built_agent_routing_and_handoff_capabilities_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T01:45:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jez456</id>
    <title>KBLaM by microsoft, This looks interesting</title>
    <updated>2025-03-19T15:05:42+00:00</updated>
    <author>
      <name>/u/AryanEmbered</name>
      <uri>https://old.reddit.com/user/AryanEmbered</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.microsoft.com/en-us/research/blog/introducing-kblam-bringing-plug-and-play-external-knowledge-to-llms/"&gt;https://www.microsoft.com/en-us/research/blog/introducing-kblam-bringing-plug-and-play-external-knowledge-to-llms/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone more knowledgeable, please enlighten us&lt;/p&gt; &lt;p&gt;in what contexts can it replace rag?&lt;/p&gt; &lt;p&gt;I genuinely believe rag getting solved is the next big unlock. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AryanEmbered"&gt; /u/AryanEmbered &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jez456/kblam_by_microsoft_this_looks_interesting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jez456/kblam_by_microsoft_this_looks_interesting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jez456/kblam_by_microsoft_this_looks_interesting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T15:05:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf4u9e</id>
    <title>Why don't we have non-Apple alternative to unified memory?</title>
    <updated>2025-03-19T19:03:47+00:00</updated>
    <author>
      <name>/u/This_Woodpecker_9163</name>
      <uri>https://old.reddit.com/user/This_Woodpecker_9163</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are we sleeping on this and allowing ourselves to be exploited by the GPU giants?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/This_Woodpecker_9163"&gt; /u/This_Woodpecker_9163 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf4u9e/why_dont_we_have_nonapple_alternative_to_unified/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf4u9e/why_dont_we_have_nonapple_alternative_to_unified/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf4u9e/why_dont_we_have_nonapple_alternative_to_unified/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T19:03:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf10ar</id>
    <title>Gemma 3 GRPO now in Unsloth + Bug Fixes</title>
    <updated>2025-03-19T16:25:58+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf10ar/gemma_3_grpo_now_in_unsloth_bug_fixes/"&gt; &lt;img alt="Gemma 3 GRPO now in Unsloth + Bug Fixes" src="https://external-preview.redd.it/zDUwsKwEDam-qG7u2ijw3m6H-OIciOZkuCU51Tgu7r4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=626174c3432ba48d9fed8ccced1e2bbb42d41c7a" title="Gemma 3 GRPO now in Unsloth + Bug Fixes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We collabed with Hugging Face to create a &lt;strong&gt;free notebook&lt;/strong&gt; to train your own reasoning model using &lt;strong&gt;Gemma 3&lt;/strong&gt; and GRPO &amp;amp; also did some fixes for training + inference&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some frameworks had large training losses when finetuning Gemma 3 - Unsloth should have correct losses!&lt;/li&gt; &lt;li&gt;We worked really hard to make Gemma 3 work in a free Colab T4 environment after inference AND &lt;strong&gt;training did not work for Gemma 3 on older GPUs&lt;/strong&gt; limited to float16. This issue affected all frameworks including us, transformers, vLLM etc.&lt;/li&gt; &lt;li&gt;Note - it's NOT a bug in Gemma 3 - in fact I consider it a &lt;strong&gt;very cool feature&lt;/strong&gt;!! It's the first time I've seen this behavior, and it's probably maybe why Gemma 3 seems extremely powerful for it's size!&lt;/li&gt; &lt;li&gt;I found that Gemma 3 had &lt;strong&gt;infinite activations&lt;/strong&gt; if one uses float16, since float16's maximum range is 65504, and Gemma 3 had values of 800,000 or larger. Llama 3.1 8B's max activation value is around 324.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ibevwuip5ope1.png?width=3580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05185613d47e1397ff2476d3e570b2c0d8478d30"&gt;https://preview.redd.it/ibevwuip5ope1.png?width=3580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05185613d47e1397ff2476d3e570b2c0d8478d30&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; is now the only framework which works in FP16 machines for Gemma 3 inference and training. This means you can now do &lt;strong&gt;GRPO, SFT&lt;/strong&gt;, FFT etc. for Gemma 3, in a free T4 GPU instance on Colab via Unsloth!&lt;/li&gt; &lt;li&gt;Please update Unsloth to the latest version to enable many many bug fixes, and Gemma 3 finetuning support via &lt;code&gt;pip install --upgrade unsloth unsloth_zoo&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Read about our Gemma 3 &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-and-fine-tune-gemma-3#unsloth-fine-tuning-fixes-for-gemma-3"&gt;fixes + details here&lt;/a&gt;!&lt;/li&gt; &lt;li&gt;This fix also solved an issue where training loss was not calculated properly for Gemma 3 in FP16.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We picked Gemma 3 (1B) for our GRPO notebook because of its smaller size, which makes inference faster and easier. But you can also use &lt;strong&gt;Gemma 3 (4B) or (12B)&lt;/strong&gt; just by changing the model name and it should fit on Colab.&lt;/p&gt; &lt;p&gt;For newer folks, we made a step-by-step &lt;a href="https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"&gt;GRPO tutorial here&lt;/a&gt;. And here's our Colab notebooks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GRPO: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/HuggingFace%20Course-Gemma3_(1B"&gt;Gemma 3 (1B) Notebook&lt;/a&gt;-GRPO.ipynb) - long link here: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/HuggingFace%20Course-Gemma3_(1B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/HuggingFace%20Course-Gemma3_(1B)-GRPO.ipynb&lt;/a&gt;-GRPO.ipynb)&lt;/li&gt; &lt;li&gt;Normal SFT: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B"&gt;Gemma 3 (4B) Notebook&lt;/a&gt;.ipynb)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy tuning and let me know if you have any questions! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf10ar/gemma_3_grpo_now_in_unsloth_bug_fixes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf10ar/gemma_3_grpo_now_in_unsloth_bug_fixes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf10ar/gemma_3_grpo_now_in_unsloth_bug_fixes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T16:25:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfn8ch</id>
    <title>How to get optimal performance with A770?</title>
    <updated>2025-03-20T12:00:35+00:00</updated>
    <author>
      <name>/u/_toojays</name>
      <uri>https://old.reddit.com/user/_toojays</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can someone who is happy with their A770 performance help me figure out if I can get more out of mine?&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md"&gt;Llama.cpp's SYCL backend doc&lt;/a&gt; claims 55 TPS on the A770. I presume that refers to the &amp;quot;eval time&amp;quot; TPS running &lt;code&gt;examples/sycl/run-llama2.sh&lt;/code&gt;. So we're talking the 3.6GB llama-2-7b.Q4_0.gguf.&lt;/p&gt; &lt;p&gt;With current llama.cpp running against the current libze-intel-gpu1 package (llama-ls-sycl-device reports driver version 1.6.31294+21) I am only getting 23 TPS.&lt;/p&gt; &lt;p&gt;If I downgrade from libze-intel-gpu1 to intel-level-zero-gpu (llama-ls-sycl-device reports driver version 1.3.30049+10) I get 35 TPS.&lt;/p&gt; &lt;p&gt;With the downgraded driver, if I use the &lt;a href="https://github.com/intel/ipex-llm/releases/tag/v2.2.0-nightly"&gt;latest nightly IPEX build of llama.cpp&lt;/a&gt; I get 70 TPS.&lt;/p&gt; &lt;p&gt;Does anyone have comparable A770 SYCL results from that llama2 example script they could share? Does anyone know the secret to 55 TPS? Obviously the IPEX result is better, but the IPEX version of llama.cpp has other idiosynchracies (it gives me OOM with gemma, illegal instruction with QwQ) so it's nice to be able to use the mainline llama.cpp.&lt;/p&gt; &lt;p&gt;I'm running in a B450M motherboard (so PCI 3.0) with a Ryzen 5 3600. My host system is Debian with a 6.12.12+bpo-amd64 kernel, then I'm building and running llama.cpp inside the intel/oneapi-basekit docker image.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_toojays"&gt; /u/_toojays &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfn8ch/how_to_get_optimal_performance_with_a770/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfn8ch/how_to_get_optimal_performance_with_a770/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfn8ch/how_to_get_optimal_performance_with_a770/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T12:00:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfobyf</id>
    <title>Small Models With Good Data &gt; API Giants: ModernBERT Destroys Claude Haiku</title>
    <updated>2025-03-20T12:59:37+00:00</updated>
    <author>
      <name>/u/wanderingtraveller</name>
      <uri>https://old.reddit.com/user/wanderingtraveller</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nice little project from Marwan Zaarab where he pits a fine-tuned ModernBERT against Claude Haiku for classifying LLMOps case studies. The results are eye-opening for anyone sick of paying for API calls.&lt;/p&gt; &lt;p&gt;(Note: this is just for the specific classification task. It's not that ModernBERT replaces the generalisation of Haiku ;) )&lt;/p&gt; &lt;h1&gt;The Setup üß©&lt;/h1&gt; &lt;p&gt;He needed to automatically sort articles - is this a real production LLM system mentioned or just theoretical BS?&lt;/p&gt; &lt;h1&gt;What He Did üìä&lt;/h1&gt; &lt;p&gt;Started with prompt engineering (which sucked for consistency), then went to fine-tuning ModernBERT on ~850 examples.&lt;/p&gt; &lt;h1&gt;The Beatdown üöÄ&lt;/h1&gt; &lt;p&gt;ModernBERT absolutely wrecked Claude Haiku:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;31% better accuracy (96.7% vs 65.7%)&lt;/li&gt; &lt;li&gt;69√ó faster (0.093s vs 6.45s)&lt;/li&gt; &lt;li&gt;225√ó cheaper ($1.11 vs $249.51 per 1000 samples)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The wildest part? Their memory-optimized version used 81% less memory while only dropping 3% in F1 score.&lt;/p&gt; &lt;h1&gt;Why I'm Posting This Here üíª&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Runs great on M-series Macs&lt;/li&gt; &lt;li&gt;No more API anxiety or rate limit bs&lt;/li&gt; &lt;li&gt;Works with modest hardware&lt;/li&gt; &lt;li&gt;Proves you don't need giant models for specific tasks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Yet another example of how understanding your problem domain + smaller fine-tuned model &amp;gt; throwing money at API providers for giant models.&lt;/p&gt; &lt;p&gt;üìö Blog: &lt;a href="https://www.zenml.io/blog/building-a-pipeline-for-automating-case-study-classification"&gt;https://www.zenml.io/blog/building-a-pipeline-for-automating-case-study-classification&lt;/a&gt;&lt;br /&gt; üíª Code: &lt;a href="https://github.com/zenml-io/zenml-projects/tree/main/research-radar"&gt;https://github.com/zenml-io/zenml-projects/tree/main/research-radar&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wanderingtraveller"&gt; /u/wanderingtraveller &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfobyf/small_models_with_good_data_api_giants_modernbert/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfobyf/small_models_with_good_data_api_giants_modernbert/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfobyf/small_models_with_good_data_api_giants_modernbert/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T12:59:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jex61b</id>
    <title>If "The Model is the Product" article is true, a lot of AI companies are doomed</title>
    <updated>2025-03-19T13:38:25+00:00</updated>
    <author>
      <name>/u/bttf88</name>
      <uri>https://old.reddit.com/user/bttf88</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious to hear the community's thoughts on this blog post that was near the top of Hacker News yesterday. Unsurprisingly, it got voted down, because I think it's news that not many YC founders want to hear.&lt;/p&gt; &lt;p&gt;I think the argument holds a lot of merit. Basically, major AI Labs like OpenAI and Anthropic are clearly moving towards training their models for Agentic purposes using RL. OpenAI's DeepResearch is one example, Claude Code is another. The models are learning how to select and leverage tools as part of their training - eating away at the complexities of application layer.&lt;/p&gt; &lt;p&gt;If this continues, the application layer that many AI companies today are inhabiting will end up competing with the major AI Labs themselves. The article quotes the VP of AI @ DataBricks predicting that all closed model labs will shut down their APIs within the next 2 -3 years. Wild thought but not totally implausible.&lt;/p&gt; &lt;p&gt;&lt;a href="https://vintagedata.org/blog/posts/model-is-the-product"&gt;https://vintagedata.org/blog/posts/model-is-the-product&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bttf88"&gt; /u/bttf88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jex61b/if_the_model_is_the_product_article_is_true_a_lot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jex61b/if_the_model_is_the_product_article_is_true_a_lot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jex61b/if_the_model_is_the_product_article_is_true_a_lot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T13:38:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jevzm3</id>
    <title>only the real ones remember</title>
    <updated>2025-03-19T12:38:23+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jevzm3/only_the_real_ones_remember/"&gt; &lt;img alt="only the real ones remember" src="https://preview.redd.it/dh21r5dq5npe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5558750be400389e9a0376174765e8479016507" title="only the real ones remember" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dh21r5dq5npe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jevzm3/only_the_real_ones_remember/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jevzm3/only_the_real_ones_remember/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T12:38:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfntk7</id>
    <title>Anything better then google's Gemma 9b for its size of parameters?</title>
    <updated>2025-03-20T12:32:10+00:00</updated>
    <author>
      <name>/u/Crockiestar</name>
      <uri>https://old.reddit.com/user/Crockiestar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im still using google's Gemma 9B. Wondering if a new model has been released open source thats better than it around that mark for function calling. Needs to be quick so i don't think deepseek would work well for my usecase. I only have 6 GB VRAM and need something that runs entirely within it no cpu offload. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Crockiestar"&gt; /u/Crockiestar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfntk7/anything_better_then_googles_gemma_9b_for_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfntk7/anything_better_then_googles_gemma_9b_for_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfntk7/anything_better_then_googles_gemma_9b_for_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T12:32:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfpnrz</id>
    <title>Moores law for AI agents</title>
    <updated>2025-03-20T14:04:36+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfpnrz/moores_law_for_ai_agents/"&gt; &lt;img alt="Moores law for AI agents" src="https://preview.redd.it/2zht3052qupe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d0afdbeec782b10a101dc05f16c39e266a2ce1f" title="Moores law for AI agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2zht3052qupe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfpnrz/moores_law_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfpnrz/moores_law_for_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T14:04:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jev3fl</id>
    <title>A man can dream</title>
    <updated>2025-03-19T11:47:24+00:00</updated>
    <author>
      <name>/u/Severin_Suveren</name>
      <uri>https://old.reddit.com/user/Severin_Suveren</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jev3fl/a_man_can_dream/"&gt; &lt;img alt="A man can dream" src="https://preview.redd.it/cw3hsv4mwmpe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70e23762b65bf659739163a3e09585431a44e8b5" title="A man can dream" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severin_Suveren"&gt; /u/Severin_Suveren &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cw3hsv4mwmpe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jev3fl/a_man_can_dream/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jev3fl/a_man_can_dream/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T11:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfehaf</id>
    <title>Amoral Gemma3 4B</title>
    <updated>2025-03-20T02:13:44+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma 3 4b:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y7hix3wq6rpe1.png?width=774&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b57f9f6c1208aeff74d149bf586492f06f29c3e"&gt;https://preview.redd.it/y7hix3wq6rpe1.png?width=774&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b57f9f6c1208aeff74d149bf586492f06f29c3e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Amoral Gemma 3 4b:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eoejwvbz6rpe1.png?width=805&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a442960a2a13edae62bc0bd9186b2404b3c58d02"&gt;https://preview.redd.it/eoejwvbz6rpe1.png?width=805&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a442960a2a13edae62bc0bd9186b2404b3c58d02&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/soob3123/amoral-gemma3-4B"&gt;soob3123/amoral-gemma3-4B ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/soob3123/amoral-gemma3-4B-gguf"&gt;soob3123/amoral-gemma3-4B-gguf ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Q_8 seems to be the best but Q_4 is good enough for most usecases as well&lt;/p&gt; &lt;p&gt;Edit: Just added the finetuned vision files. if you already downloaded it, down the gguf again to get the uncensored vision capabilities&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfehaf/amoral_gemma3_4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfehaf/amoral_gemma3_4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfehaf/amoral_gemma3_4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T02:13:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf6igq</id>
    <title>Apache TTS: Orpheus 3B 0.1 FT</title>
    <updated>2025-03-19T20:11:33+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a respect post, it's not my model. In TTS land, a finetuned, Apache licensed 3B boi is a huge drop.&lt;/p&gt; &lt;p&gt;Weights: &lt;a href="https://huggingface.co/canopylabs/orpheus-3b-0.1-ft"&gt;https://huggingface.co/canopylabs/orpheus-3b-0.1-ft&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;Space:&lt;/del&gt; &lt;a href="https://huggingface.co/spaces/canopylabs/orpheus-tts"&gt;&lt;del&gt;https://huggingface.co/spaces/canopylabs/orpheus-tts&lt;/del&gt;&lt;/a&gt; Space taken down again&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;https://github.com/canopyai/Orpheus-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://canopylabs.ai/model-releases"&gt;https://canopylabs.ai/model-releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As an aside, I personally love it when the weights repro the demo samples. Well done.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf6igq/apache_tts_orpheus_3b_01_ft/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf6igq/apache_tts_orpheus_3b_01_ft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf6igq/apache_tts_orpheus_3b_01_ft/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T20:11:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfk5bs</id>
    <title>NVIDIA selling a small amount of 5080s and 5090s at MSRP at GTC</title>
    <updated>2025-03-20T08:25:25+00:00</updated>
    <author>
      <name>/u/False_Care_2957</name>
      <uri>https://old.reddit.com/user/False_Care_2957</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfk5bs/nvidia_selling_a_small_amount_of_5080s_and_5090s/"&gt; &lt;img alt="NVIDIA selling a small amount of 5080s and 5090s at MSRP at GTC" src="https://external-preview.redd.it/3kT0XATxO_t_PsBk5IYdwm0rupWe9BvAFfa1PcU7N7w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=577211de692a570366eda814cb957c6bbfa87da3" title="NVIDIA selling a small amount of 5080s and 5090s at MSRP at GTC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/7p934s4g1tpe1.png?width=1058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f77a7e471836609cac1abd6ebdea26fd3123235"&gt;https://preview.redd.it/7p934s4g1tpe1.png?width=1058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f77a7e471836609cac1abd6ebdea26fd3123235&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/NVIDIAAIDev/status/1902454685153554438"&gt;https://x.com/NVIDIAAIDev/status/1902454685153554438&lt;/a&gt;&lt;/p&gt; &lt;p&gt;While we have to scramble get 5090s at 2-3x the price&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/False_Care_2957"&gt; /u/False_Care_2957 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfk5bs/nvidia_selling_a_small_amount_of_5080s_and_5090s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfk5bs/nvidia_selling_a_small_amount_of_5080s_and_5090s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfk5bs/nvidia_selling_a_small_amount_of_5080s_and_5090s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T08:25:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfdfou</id>
    <title>Creative writing under 15b</title>
    <updated>2025-03-20T01:21:36+00:00</updated>
    <author>
      <name>/u/Wandering_By_</name>
      <uri>https://old.reddit.com/user/Wandering_By_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfdfou/creative_writing_under_15b/"&gt; &lt;img alt="Creative writing under 15b" src="https://preview.redd.it/vd9wm7zyxqpe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f27b582aa94db48353d0959b03086b81afb7cf5" title="Creative writing under 15b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Decided to try a bunch of different models out for creative writing. Figured it might be nice to grade them using larger models for an objective perspective and speed the process up. Realized how asinine it was not to be using a real spreadsheet when I was already 9 through. So enjoy the screenshot. If anyone has suggestions for the next two rounds I'm open to hear them. This one was done using default ollama and openwebui settings.&lt;/p&gt; &lt;p&gt;Prompt for each model: Please provide a complex and entertaining story. The story can be either fictional or true, and you have the freedom to select any genre you believe will best showcase your creative abilities. Originality and creativity will be highly rewarded. While surreal or absurd elements are welcome, ensure they enhance the story‚Äôs entertainment value rather than detract from the narrative coherence. We encourage you to utilize the full potential of your context window to develop a richly detailed story‚Äîshort responses may lead to a deduction in points.&lt;/p&gt; &lt;p&gt;Prompt for the judges:Evaluate the following writing sample using these criteria. Provide me with a score between 0-10 for each section, then use addition to add the scores together for a total value of the writing.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Grammar &amp;amp; Mechanics (foundational correctness)&lt;/li&gt; &lt;li&gt;Clarity &amp;amp; Coherence (sentence/paragraph flow)&lt;/li&gt; &lt;li&gt;Narrative Structure (plot-level organization)&lt;/li&gt; &lt;li&gt;Character Development (depth of personas)&lt;/li&gt; &lt;li&gt;Imagery &amp;amp; Sensory Details (descriptive elements)&lt;/li&gt; &lt;li&gt;Pacing &amp;amp; Rhythm (temporal flow)&lt;/li&gt; &lt;li&gt;Emotional Impact (reader‚Äôs felt experience)&lt;/li&gt; &lt;li&gt;Thematic Depth &amp;amp; Consistency (underlying meaning)&lt;/li&gt; &lt;li&gt;Originality &amp;amp; Creativity (novelty of ideas)&lt;/li&gt; &lt;li&gt;Audience Resonance (connection to readers)&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wandering_By_"&gt; /u/Wandering_By_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vd9wm7zyxqpe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfdfou/creative_writing_under_15b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfdfou/creative_writing_under_15b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T01:21:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfmpas</id>
    <title>Why whisper v3 turbo has not been replaced?</title>
    <updated>2025-03-20T11:28:45+00:00</updated>
    <author>
      <name>/u/Bakedsoda</name>
      <uri>https://old.reddit.com/user/Bakedsoda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the absolute frenzy in the TTS open source release from Kokoro , Zonos and now Oprheus. &lt;/p&gt; &lt;p&gt;I assume we should be getting some next gen STT open source models soon.&lt;/p&gt; &lt;p&gt;Even at v3 turbo quality but smaller size that can run on edge in real time would be amazing!!!&lt;/p&gt; &lt;p&gt;Anyone working on anything like that ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bakedsoda"&gt; /u/Bakedsoda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfmpas/why_whisper_v3_turbo_has_not_been_replaced/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfmpas/why_whisper_v3_turbo_has_not_been_replaced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfmpas/why_whisper_v3_turbo_has_not_been_replaced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T11:28:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfkv1s</id>
    <title>We should talk about Mistral Small 3.1 vs Mistral Small 3.</title>
    <updated>2025-03-20T09:21:42+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No one saying anything about the new Mistral Small 3.1, no posts about how it perform etc.&lt;/p&gt; &lt;p&gt;From my tests Mistral Small 3.1 performing about the same like original Mistral Small 3.&lt;br /&gt; Same repetitions problems, same long context problems, unstable high temperatures.&lt;br /&gt; I got even a slight worse results at some tasks, coding for example.&lt;/p&gt; &lt;p&gt;Is MS3.1 just a hack to make MS3 multi-modal?&lt;br /&gt; Should we back to MS3 for text-only work?&lt;br /&gt; How was your experience with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfkv1s/we_should_talk_about_mistral_small_31_vs_mistral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfkv1s/we_should_talk_about_mistral_small_31_vs_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfkv1s/we_should_talk_about_mistral_small_31_vs_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T09:21:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf5ufk</id>
    <title>New RTX PRO 6000 with 96G VRAM</title>
    <updated>2025-03-19T19:44:59+00:00</updated>
    <author>
      <name>/u/ThenExtension9196</name>
      <uri>https://old.reddit.com/user/ThenExtension9196</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this at nvidia GTC. Truly a beautiful card. Very similar styling as the 5090FE and even has the same cooling system. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThenExtension9196"&gt; /u/ThenExtension9196 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cost3vsw9ppe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf5ufk/new_rtx_pro_6000_with_96g_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf5ufk/new_rtx_pro_6000_with_96g_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T19:44:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfglbu</id>
    <title>Orpheus TTS Local (LM Studio)</title>
    <updated>2025-03-20T04:09:42+00:00</updated>
    <author>
      <name>/u/Internal_Brain8420</name>
      <uri>https://old.reddit.com/user/Internal_Brain8420</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfglbu/orpheus_tts_local_lm_studio/"&gt; &lt;img alt="Orpheus TTS Local (LM Studio)" src="https://external-preview.redd.it/123zU4tSEAhJBQw-5zzDV7N-1QXm63u6nWHqCb7Eodw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fd48ff5928b7af59f7d95c0c187069ccc64014c" title="Orpheus TTS Local (LM Studio)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Internal_Brain8420"&gt; /u/Internal_Brain8420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/isaiahbjork/orpheus-tts-local"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfglbu/orpheus_tts_local_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfglbu/orpheus_tts_local_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T04:09:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfm23c</id>
    <title>TikZero - New Approach for Generating Scientific Figures from Text Captions with LLMs</title>
    <updated>2025-03-20T10:47:09+00:00</updated>
    <author>
      <name>/u/DrCracket</name>
      <uri>https://old.reddit.com/user/DrCracket</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfm23c/tikzero_new_approach_for_generating_scientific/"&gt; &lt;img alt="TikZero - New Approach for Generating Scientific Figures from Text Captions with LLMs" src="https://preview.redd.it/carfu383qtpe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1dbc3c3ec140b5c9532f78408e18185bd09d368" title="TikZero - New Approach for Generating Scientific Figures from Text Captions with LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrCracket"&gt; /u/DrCracket &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/carfu383qtpe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfm23c/tikzero_new_approach_for_generating_scientific/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfm23c/tikzero_new_approach_for_generating_scientific/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T10:47:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfh1d7</id>
    <title>LLMs are 800x Cheaper for Translation than DeepL</title>
    <updated>2025-03-20T04:37:11+00:00</updated>
    <author>
      <name>/u/Ninjinka</name>
      <uri>https://old.reddit.com/user/Ninjinka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When looking at the cost of translation APIs, I was floored by the prices. Azure is $10 per million characters, Google is $20, and DeepL is $25.&lt;/p&gt; &lt;p&gt;To come up with a rough estimate for a real-time translation use case, I assumed 150 WPM speaking speed, with each word being translated 3 times (since the text gets retranslated multiple times as the context lengthens). This resulted in the following costs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Azure: $1.62/hr&lt;/li&gt; &lt;li&gt;Google: $3.24/hr&lt;/li&gt; &lt;li&gt;DeepL: $4.05/hr&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Assuming the same numbers, &lt;code&gt;gemini-2.0-flash-lite&lt;/code&gt; would cost &lt;strong&gt;less than $0.01/hr&lt;/strong&gt;. Cost varies based on prompt length, but I'm actually getting just under $0.005/hr.&lt;/p&gt; &lt;p&gt;That's over 800x cheaper than DeepL, or 0.1% of the cost.&lt;/p&gt; &lt;p&gt;Presumably the quality of the translations would be somewhat worse, but how much worse? And how long will that disadvantage last? I can stomach a certain amount of worse for 99% cheaper, and it seems easy to foresee that LLMs will surpass the quality of the legacy translation models in the near future.&lt;/p&gt; &lt;p&gt;Right now the accuracy depends a lot on the prompting. I need to run a lot more evals, but so far in my tests I'm seeing that the translations I'm getting are as good (most of the time identical) or &lt;em&gt;better&lt;/em&gt; than Google's the vast majority of the time. I'm confident I can get to 90% of Google's accuracy with better prompting.&lt;/p&gt; &lt;p&gt;I can live with 90% accuracy with a 99.9% cost reduction.&lt;/p&gt; &lt;p&gt;For many, 90% doesn't cut it for their translation needs and they are willing to pay a premium for the best. But the high costs of legacy translation APIs will become increasingly indefensible as LLM-based solutions improve, and we'll see translation incorporated in ways that were previously cost-prohibitive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ninjinka"&gt; /u/Ninjinka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfh1d7/llms_are_800x_cheaper_for_translation_than_deepl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfh1d7/llms_are_800x_cheaper_for_translation_than_deepl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfh1d7/llms_are_800x_cheaper_for_translation_than_deepl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T04:37:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfnw9x</id>
    <title>Sharing my build: Budget 64 GB VRAM GPU Server under $700 USD</title>
    <updated>2025-03-20T12:36:16+00:00</updated>
    <author>
      <name>/u/Hyungsun</name>
      <uri>https://old.reddit.com/user/Hyungsun</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfnw9x/sharing_my_build_budget_64_gb_vram_gpu_server/"&gt; &lt;img alt="Sharing my build: Budget 64 GB VRAM GPU Server under $700 USD" src="https://b.thumbs.redditmedia.com/weO7zUFK46BYVtLrB9NR6mPsIETCehibf3c566iKGHI.jpg" title="Sharing my build: Budget 64 GB VRAM GPU Server under $700 USD" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hyungsun"&gt; /u/Hyungsun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jfnw9x"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfnw9x/sharing_my_build_budget_64_gb_vram_gpu_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfnw9x/sharing_my_build_budget_64_gb_vram_gpu_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T12:36:16+00:00</published>
  </entry>
</feed>
