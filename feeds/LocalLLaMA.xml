<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-21T19:05:26+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ks47wv</id>
    <title>Arc pro b60 48gb vram</title>
    <updated>2025-05-21T17:48:26+00:00</updated>
    <author>
      <name>/u/zathras7</name>
      <uri>https://old.reddit.com/user/zathras7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://videocardz.com/newz/maxsun-unveils-arc-pro-b60-dual-turbo-two-battlemage-gpus-48gb-vram-and-400w-power"&gt;https://videocardz.com/newz/maxsun-unveils-arc-pro-b60-dual-turbo-two-battlemage-gpus-48gb-vram-and-400w-power&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zathras7"&gt; /u/zathras7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks47wv/arc_pro_b60_48gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks47wv/arc_pro_b60_48gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ks47wv/arc_pro_b60_48gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T17:48:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr8s40</id>
    <title>Gemma 3n Preview</title>
    <updated>2025-05-20T16:10:01+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr8s40/gemma_3n_preview/"&gt; &lt;img alt="Gemma 3n Preview" src="https://external-preview.redd.it/nuTGd6nR-D7i0exzDvXeyeroWnA1sgWJyyF8GipdVWU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4092ee3492e35aa48ddc115bdbd7e2144d1d03c2" title="Gemma 3n Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/gemma-3n-preview-682ca41097a31e5ac804d57b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr8s40/gemma_3n_preview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kr8s40/gemma_3n_preview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T16:10:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks2j74</id>
    <title>Public ranking for open source models?</title>
    <updated>2025-05-21T16:41:27+00:00</updated>
    <author>
      <name>/u/jinstronda</name>
      <uri>https://old.reddit.com/user/jinstronda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a public ranking that i can check for open source models to compare them and to be able to finetune? Its weird theres a ranking for everything except for models that we can use for fine tuning &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinstronda"&gt; /u/jinstronda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks2j74/public_ranking_for_open_source_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks2j74/public_ranking_for_open_source_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ks2j74/public_ranking_for_open_source_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T16:41:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1krpvwj</id>
    <title>Gemma 3N E4B and Gemini 2.5 Flash Tested</title>
    <updated>2025-05-21T05:10:56+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=lEtLksaaos8"&gt;https://www.youtube.com/watch?v=lEtLksaaos8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Compared Gemma 3n e4b against Qwen 3 4b. Mixed results. Gemma does great on classification, matches Qwen 4B on Structured JSON extraction. Struggles with coding and RAG.&lt;/p&gt; &lt;p&gt;Also compared Gemini 2.5 Flash to Open AI 4.1. Altman should be worried. Cheaper than 4.1 mini, better than full 4.1.&lt;/p&gt; &lt;h1&gt;Harmful Question Detector&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemini-2.5-flash-preview-05-20&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3n-e4b-it:free&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-4.1&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3-4b:free&lt;/td&gt; &lt;td align="left"&gt;70.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Named Entity Recognition New&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemini-2.5-flash-preview-05-20&lt;/td&gt; &lt;td align="left"&gt;95.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-4.1&lt;/td&gt; &lt;td align="left"&gt;95.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3n-e4b-it:free&lt;/td&gt; &lt;td align="left"&gt;60.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3-4b:free&lt;/td&gt; &lt;td align="left"&gt;60.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Retrieval Augmented Generation Prompt&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemini-2.5-flash-preview-05-20&lt;/td&gt; &lt;td align="left"&gt;97.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-4.1&lt;/td&gt; &lt;td align="left"&gt;95.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3-4b:free&lt;/td&gt; &lt;td align="left"&gt;83.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3n-e4b-it:free&lt;/td&gt; &lt;td align="left"&gt;62.50&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;SQL Query Generator&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemini-2.5-flash-preview-05-20&lt;/td&gt; &lt;td align="left"&gt;95.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-4.1&lt;/td&gt; &lt;td align="left"&gt;95.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3-4b:free&lt;/td&gt; &lt;td align="left"&gt;75.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3n-e4b-it:free&lt;/td&gt; &lt;td align="left"&gt;65.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krpvwj/gemma_3n_e4b_and_gemini_25_flash_tested/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krpvwj/gemma_3n_e4b_and_gemini_25_flash_tested/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krpvwj/gemma_3n_e4b_and_gemini_25_flash_tested/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T05:10:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1krr7hn</id>
    <title>How to get the most from llama.cpp's iSWA support</title>
    <updated>2025-05-21T06:38:02+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13194"&gt;https://github.com/ggml-org/llama.cpp/pull/13194&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks to our gguf god ggerganov, we finally have iSWA support for gemma 3 models that significantly reduces KV cache usage. Since I participated in the pull discussion, I would like to offer tips to get the most out of this update.&lt;/p&gt; &lt;p&gt;Previously, by default fp16 KV cache for 27b model at 64k context is 31744MiB. Now by default batch_size=2048, fp16 KV cache becomes 6368MiB. This is 79.9% reduction.&lt;/p&gt; &lt;p&gt;Group Query Attention KV cache: (ie original implementation)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;context&lt;/th&gt; &lt;th align="left"&gt;4k&lt;/th&gt; &lt;th align="left"&gt;8k&lt;/th&gt; &lt;th align="left"&gt;16k&lt;/th&gt; &lt;th align="left"&gt;32k&lt;/th&gt; &lt;th align="left"&gt;64k&lt;/th&gt; &lt;th align="left"&gt;128k&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-27b&lt;/td&gt; &lt;td align="left"&gt;1984MB&lt;/td&gt; &lt;td align="left"&gt;3968MB&lt;/td&gt; &lt;td align="left"&gt;7936MB&lt;/td&gt; &lt;td align="left"&gt;15872MB&lt;/td&gt; &lt;td align="left"&gt;31744MB&lt;/td&gt; &lt;td align="left"&gt;63488MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-12b&lt;/td&gt; &lt;td align="left"&gt;1536MB&lt;/td&gt; &lt;td align="left"&gt;3072MB&lt;/td&gt; &lt;td align="left"&gt;6144MB&lt;/td&gt; &lt;td align="left"&gt;12288MB&lt;/td&gt; &lt;td align="left"&gt;24576MB&lt;/td&gt; &lt;td align="left"&gt;49152MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-4b&lt;/td&gt; &lt;td align="left"&gt;544MB&lt;/td&gt; &lt;td align="left"&gt;1088MB&lt;/td&gt; &lt;td align="left"&gt;2176MB&lt;/td&gt; &lt;td align="left"&gt;4352MB&lt;/td&gt; &lt;td align="left"&gt;8704MB&lt;/td&gt; &lt;td align="left"&gt;17408MB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The new implementation splits KV cache to Local Attention KV cache and Global Attention KV cache that are detailed in the following two tables. The overall KV cache use will be the sum of the two. Local Attn KV depends on the batch_size only while the Global attn KV depends on the context length.&lt;/p&gt; &lt;p&gt;Since the local attention KV depends on the batch_size only, you can reduce the batch_size (via the -b switch) from 2048 to 64 (setting values lower than this will just be set to 64) to further reduce KV cache. Originally, it is 5120+1248=6368MiB. Now it is 5120+442=5562MiB. Memory saving will now 82.48%. The cost of reducing batch_size is reduced prompt processing speed. Based on my llama-bench pp512 test, it is only around 20% reduction when you go from 2048 to 64.&lt;/p&gt; &lt;p&gt;Local Attention KV cache size valid at any context:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;batch&lt;/th&gt; &lt;th align="left"&gt;64&lt;/th&gt; &lt;th align="left"&gt;512&lt;/th&gt; &lt;th align="left"&gt;2048&lt;/th&gt; &lt;th align="left"&gt;8192&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;kv_size&lt;/td&gt; &lt;td align="left"&gt;1088&lt;/td&gt; &lt;td align="left"&gt;1536&lt;/td&gt; &lt;td align="left"&gt;3072&lt;/td&gt; &lt;td align="left"&gt;9216&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-27b&lt;/td&gt; &lt;td align="left"&gt;442MB&lt;/td&gt; &lt;td align="left"&gt;624MB&lt;/td&gt; &lt;td align="left"&gt;1248MB&lt;/td&gt; &lt;td align="left"&gt;3744MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-12b&lt;/td&gt; &lt;td align="left"&gt;340MB&lt;/td&gt; &lt;td align="left"&gt;480MB&lt;/td&gt; &lt;td align="left"&gt;960MB&lt;/td&gt; &lt;td align="left"&gt;2880MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-4b&lt;/td&gt; &lt;td align="left"&gt;123.25MB&lt;/td&gt; &lt;td align="left"&gt;174MB&lt;/td&gt; &lt;td align="left"&gt;348MB&lt;/td&gt; &lt;td align="left"&gt;1044MB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Global Attention KV cache:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;context&lt;/th&gt; &lt;th align="left"&gt;4k&lt;/th&gt; &lt;th align="left"&gt;8k&lt;/th&gt; &lt;th align="left"&gt;16k&lt;/th&gt; &lt;th align="left"&gt;32k&lt;/th&gt; &lt;th align="left"&gt;64k&lt;/th&gt; &lt;th align="left"&gt;128k&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-27b&lt;/td&gt; &lt;td align="left"&gt;320MB&lt;/td&gt; &lt;td align="left"&gt;640MB&lt;/td&gt; &lt;td align="left"&gt;1280MB&lt;/td&gt; &lt;td align="left"&gt;2560MB&lt;/td&gt; &lt;td align="left"&gt;5120MB&lt;/td&gt; &lt;td align="left"&gt;10240MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-12b&lt;/td&gt; &lt;td align="left"&gt;256MB&lt;/td&gt; &lt;td align="left"&gt;512MB&lt;/td&gt; &lt;td align="left"&gt;1024MB&lt;/td&gt; &lt;td align="left"&gt;2048MB&lt;/td&gt; &lt;td align="left"&gt;4096MB&lt;/td&gt; &lt;td align="left"&gt;8192MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-4b&lt;/td&gt; &lt;td align="left"&gt;80MB&lt;/td&gt; &lt;td align="left"&gt;160MB&lt;/td&gt; &lt;td align="left"&gt;320MB&lt;/td&gt; &lt;td align="left"&gt;640MB&lt;/td&gt; &lt;td align="left"&gt;1280MB&lt;/td&gt; &lt;td align="left"&gt;2560MB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;If you only have one 24GB card, you can use the default batch_size 2048 and run 27b qat q4_0 at 64k, then it should be 15.6GB model + 5GB global KV + 1.22GB local KV = 21.82GB. Previously, that would take 48.6GB total.&lt;/p&gt; &lt;p&gt;If you want to run it at even higher context, you can use KV quantization (lower accuracy) and/or reduce batch size (slower prompt processing). Reducing batch size to the minimum 64 should allow you to run 96k (total 23.54GB). KV quant alone at Q8_0 should allow you to run 128k at 21.57GB.&lt;/p&gt; &lt;p&gt;So we now finally have a viable long context local LLM that can run with a single card. Have fun summarizing long pdfs with llama.cpp!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krr7hn/how_to_get_the_most_from_llamacpps_iswa_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krr7hn/how_to_get_the_most_from_llamacpps_iswa_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krr7hn/how_to_get_the_most_from_llamacpps_iswa_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T06:38:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks5nul</id>
    <title>Devstral with vision support (from ngxson)</title>
    <updated>2025-05-21T18:45:32+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/ngxson/Devstral-Small-Vision-2505-GGUF"&gt;https://huggingface.co/ngxson/Devstral-Small-Vision-2505-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just sharing in case people did not notice (version with vision &amp;quot;re-added&amp;quot;). Did not test yet but will do that soonly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks5nul/devstral_with_vision_support_from_ngxson/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks5nul/devstral_with_vision_support_from_ngxson/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ks5nul/devstral_with_vision_support_from_ngxson/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T18:45:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks5sh4</id>
    <title>Broke down and bought a Mac Mini - my processes run 5x faster</title>
    <updated>2025-05-21T18:50:39+00:00</updated>
    <author>
      <name>/u/ETBiggs</name>
      <uri>https://old.reddit.com/user/ETBiggs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran my process on my $850 Beelink Ryzen 9 32gb machine and it took 4 hours to run - the process calls my 8g llm 42 times during the run. It took 4 hours and 18 minutes. The Mac Mini with an M4 Pro chip and 24gb memory took 47 minutes. &lt;/p&gt; &lt;p&gt;Itâ€™s a keeper - Iâ€™m returning my Beelink. That unified memory in the Mac used half the memory and used the GPU. &lt;/p&gt; &lt;p&gt;I know I could have bought a used gamer rig cheaper but for a lot of reasons - this is perfect for me. I would much prefer not using the MacOS - Windows is a PITA but Iâ€™m used to it. It took about 2 hours of cursing to install my stack and port my code. &lt;/p&gt; &lt;p&gt;I have 2 weeks to return it and Iâ€™m going to push this thing to the limits. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ETBiggs"&gt; /u/ETBiggs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks5sh4/broke_down_and_bought_a_mac_mini_my_processes_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks5sh4/broke_down_and_bought_a_mac_mini_my_processes_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ks5sh4/broke_down_and_bought_a_mac_mini_my_processes_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T18:50:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kru9v3</id>
    <title>Hidden thinking</title>
    <updated>2025-05-21T10:15:57+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was disappointed to find that Google has now hidden Gemini's thinking. I guess it is understandable to stop others from using the data to train and so help's good to keep their competitive advantage, but I found the thoughts so useful. I'd read the thoughts as generated and often would terminate the generation to refine the prompt based on the output thoughts which led to better results.&lt;/p&gt; &lt;p&gt;It was nice while it lasted and I hope a lot of thinking data was scraped to help train the open models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kru9v3/hidden_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kru9v3/hidden_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kru9v3/hidden_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T10:15:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1krupm7</id>
    <title>gemma 3n seems not work well for non English prompt</title>
    <updated>2025-05-21T10:44:22+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krupm7/gemma_3n_seems_not_work_well_for_non_english/"&gt; &lt;img alt="gemma 3n seems not work well for non English prompt" src="https://preview.redd.it/xhxxm6xv642f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0580d38b88c51a74cb73fa8b517a498c3df81f8" title="gemma 3n seems not work well for non English prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xhxxm6xv642f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krupm7/gemma_3n_seems_not_work_well_for_non_english/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krupm7/gemma_3n_seems_not_work_well_for_non_english/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T10:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1krxwja</id>
    <title>New falcon models using mamba hybrid are very competetive if not ahead for their sizes.</title>
    <updated>2025-05-21T13:31:15+00:00</updated>
    <author>
      <name>/u/ElectricalAngle1611</name>
      <uri>https://old.reddit.com/user/ElectricalAngle1611</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AVG SCORES FOR A VARIETY OF BENCHMARKS:&lt;br /&gt; **Falcon-H1 Models:**&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;**Falcon-H1-34B:** 58.92&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Falcon-H1-7B:** 54.08&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Falcon-H1-3B:** 48.09&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Falcon-H1-1.5B-deep:** 47.72&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Falcon-H1-1.5B:** 45.47&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Falcon-H1-0.5B:** 35.83&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;**Qwen3 Models:**&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;**Qwen3-32B:** 58.44&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Qwen3-8B:** 52.62&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Qwen3-4B:** 48.83&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Qwen3-1.7B:** 41.08&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Qwen3-0.6B:** 31.24&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;**Gemma3 Models:**&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;**Gemma3-27B:** 58.75&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Gemma3-12B:** 54.10&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Gemma3-4B:** 44.32&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Gemma3-1B:** 29.68&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;**Llama Models:**&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;**Llama3.3-70B:** 58.20&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Llama4-scout:** 57.42&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Llama3.1-8B:** 44.77&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Llama3.2-3B:** 38.29&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Llama3.2-1B:** 24.99&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;benchmarks tested:&lt;br /&gt; * BBH&lt;/p&gt; &lt;p&gt;* ARC-C&lt;/p&gt; &lt;p&gt;* TruthfulQA&lt;/p&gt; &lt;p&gt;* HellaSwag&lt;/p&gt; &lt;p&gt;* MMLU&lt;/p&gt; &lt;p&gt;* GSM8k&lt;/p&gt; &lt;p&gt;* MATH-500&lt;/p&gt; &lt;p&gt;* AMC-23&lt;/p&gt; &lt;p&gt;* AIME-24&lt;/p&gt; &lt;p&gt;* AIME-25&lt;/p&gt; &lt;p&gt;* GPQA&lt;/p&gt; &lt;p&gt;* GPQA_Diamond&lt;/p&gt; &lt;p&gt;* MMLU-Pro&lt;/p&gt; &lt;p&gt;* MMLU-stem&lt;/p&gt; &lt;p&gt;* HumanEval&lt;/p&gt; &lt;p&gt;* HumanEval+&lt;/p&gt; &lt;p&gt;* MBPP&lt;/p&gt; &lt;p&gt;* MBPP+&lt;/p&gt; &lt;p&gt;* LiveCodeBench&lt;/p&gt; &lt;p&gt;* CRUXEval&lt;/p&gt; &lt;p&gt;* IFEval&lt;/p&gt; &lt;p&gt;* Alpaca-Eval&lt;/p&gt; &lt;p&gt;* MTBench&lt;/p&gt; &lt;p&gt;* LiveBench&lt;/p&gt; &lt;p&gt;all the data I grabbed for this post was found at: &lt;a href="https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct"&gt;https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct&lt;/a&gt; and the various other models in the h1 family.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElectricalAngle1611"&gt; /u/ElectricalAngle1611 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krxwja/new_falcon_models_using_mamba_hybrid_are_very/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krxwja/new_falcon_models_using_mamba_hybrid_are_very/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krxwja/new_falcon_models_using_mamba_hybrid_are_very/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T13:31:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1krp4hq</id>
    <title>They also released the Android app with which you can interact with the new Gemma3n</title>
    <updated>2025-05-21T04:25:10+00:00</updated>
    <author>
      <name>/u/Ordinary_Mud7430</name>
      <uri>https://old.reddit.com/user/Ordinary_Mud7430</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;This is really good&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/android"&gt;https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/android&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/google-ai-edge/gallery"&gt;https://github.com/google-ai-edge/gallery&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ordinary_Mud7430"&gt; /u/Ordinary_Mud7430 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krp4hq/they_also_released_the_android_app_with_which_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krp4hq/they_also_released_the_android_app_with_which_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krp4hq/they_also_released_the_android_app_with_which_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T04:25:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks0snl</id>
    <title>SWE-rebench update: GPT4.1 mini/nano and Gemini 2.0/2.5 Flash added</title>
    <updated>2025-05-21T15:32:06+00:00</updated>
    <author>
      <name>/u/Long-Sleep-13</name>
      <uri>https://old.reddit.com/user/Long-Sleep-13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Weâ€™ve just added a batch of new models to the &lt;a href="https://swe-rebench.com/leaderboard"&gt;SWE-rebench leaderboard&lt;/a&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPT-4.1 mini&lt;/li&gt; &lt;li&gt;GPT-4.1 nano&lt;/li&gt; &lt;li&gt;Gemini 2.0 Flash&lt;/li&gt; &lt;li&gt;Gemini 2.5 Flash Preview 05-20&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A few quick takeaways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;gpt-4.1-mini is surprisingly strong, it matches full GPT-4.1 performance on fresh, decontaminated tasks. Very strong instruction following capabilities.&lt;/li&gt; &lt;li&gt;gpt-4.1-nano, on the other hand, struggles. It often misunderstands the system prompt and hallucinates environment responses. This also affects other models in the bottom of the leaderboard.&lt;/li&gt; &lt;li&gt;gemini 2.0 flash performs on par with Qwen and LLaMA 70B. It doesn't seem to suffer from contamination, but it often has troubles following instructions precisely.&lt;/li&gt; &lt;li&gt;gemini 2.5 flash preview 05-20 is a big improvement over 2.0. Itâ€™s nearly GPT-4.1 level on older data and gets closer to GPT-4.1 mini on newer tasks, being ~2.6x cheaper, though possibly a bit contaminated.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We know many people are waiting for frontier model results. Thanks to OpenAI for providing API credits, results for o3 and o4-mini are coming soon. Stay tuned!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Long-Sleep-13"&gt; /u/Long-Sleep-13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks0snl/swerebench_update_gpt41_mininano_and_gemini_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks0snl/swerebench_update_gpt41_mininano_and_gemini_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ks0snl/swerebench_update_gpt41_mininano_and_gemini_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T15:32:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1krsjpb</id>
    <title>New threadripper has 8 memory channels. Will it be an affordable local LLM option?</title>
    <updated>2025-05-21T08:14:04+00:00</updated>
    <author>
      <name>/u/theKingOfIdleness</name>
      <uri>https://old.reddit.com/user/theKingOfIdleness</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.theregister.com/2025/05/21/amd_threadripper_radeon_workstation/"&gt;https://www.theregister.com/2025/05/21/amd_threadripper_radeon_workstation/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm always on the lookout for cheap local inference. I noticed the new threadrippers will move from 4 to 8 channels.&lt;/p&gt; &lt;p&gt;8 channels of DDR5 is about 409GB/s&lt;/p&gt; &lt;p&gt;That's on par with mid range GPUs on a non server chip.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theKingOfIdleness"&gt; /u/theKingOfIdleness &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krsjpb/new_threadripper_has_8_memory_channels_will_it_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krsjpb/new_threadripper_has_8_memory_channels_will_it_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krsjpb/new_threadripper_has_8_memory_channels_will_it_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T08:14:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks0arl</id>
    <title>Voice cloning for Kokoro TTS using random walk algorithms</title>
    <updated>2025-05-21T15:12:31+00:00</updated>
    <author>
      <name>/u/rodbiren</name>
      <uri>https://old.reddit.com/user/rodbiren</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks0arl/voice_cloning_for_kokoro_tts_using_random_walk/"&gt; &lt;img alt="Voice cloning for Kokoro TTS using random walk algorithms" src="https://external-preview.redd.it/aOW6-hgbtxvb8U4tg5vaNfPPQC6NYWWnQfNZ4XDYvYg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a846a19dfe6fa63bfde52db9069d0dadbf3b7dba" title="Voice cloning for Kokoro TTS using random walk algorithms" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=44052295"&gt;https://news.ycombinator.com/item?id=44052295&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everybody, I made a library that can somewhat clone voices using Kokoro TTS. I know it is a popular library for adding speech to various LLM applications, so I figured I would share it here. It can take awhile and produce a variety of results, but overall it is a promising attempt to add more voice options to this great library. &lt;/p&gt; &lt;p&gt;Check out the code and examples.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rodbiren"&gt; /u/rodbiren &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/RobViren/kvoicewalk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks0arl/voice_cloning_for_kokoro_tts_using_random_walk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ks0arl/voice_cloning_for_kokoro_tts_using_random_walk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T15:12:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1krnk8v</id>
    <title>ByteDance Bagel 14B MOE (7B active) Multimodal with image generation (open source, apache license)</title>
    <updated>2025-05-21T02:57:30+00:00</updated>
    <author>
      <name>/u/noage</name>
      <uri>https://old.reddit.com/user/noage</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Weights - &lt;a href="https://github.com/ByteDance-Seed/Bagel"&gt;GitHub - ByteDance-Seed/Bagel&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Website - &lt;a href="https://bagel-ai.org/"&gt;BAGEL: The Open-Source Unified Multimodal Model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper - &lt;a href="https://arxiv.org/abs/2505.14683"&gt;[2505.14683] Emerging Properties in Unified Multimodal Pretraining&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It uses a mixture of experts and a mixture of transformers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noage"&gt; /u/noage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krnk8v/bytedance_bagel_14b_moe_7b_active_multimodal_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krnk8v/bytedance_bagel_14b_moe_7b_active_multimodal_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krnk8v/bytedance_bagel_14b_moe_7b_active_multimodal_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T02:57:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks0h52</id>
    <title>I'd love a qwen3-coder-30B-A3B</title>
    <updated>2025-05-21T15:19:25+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Honestly I'd pay quite a bit to have such a model on my own machine. Inference would be quite fast and coding would be decent. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks0h52/id_love_a_qwen3coder30ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks0h52/id_love_a_qwen3coder30ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ks0h52/id_love_a_qwen3coder30ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T15:19:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kri7ik</id>
    <title>ok google, next time mention llama.cpp too!</title>
    <updated>2025-05-20T22:31:42+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kri7ik/ok_google_next_time_mention_llamacpp_too/"&gt; &lt;img alt="ok google, next time mention llama.cpp too!" src="https://preview.redd.it/ml66h5yxj02f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=36aba859e0c8b8e47fe122c7315b0f3ad3607ad1" title="ok google, next time mention llama.cpp too!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ml66h5yxj02f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kri7ik/ok_google_next_time_mention_llamacpp_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kri7ik/ok_google_next_time_mention_llamacpp_too/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T22:31:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1krzpmu</id>
    <title>AMD ROCm 6.4.1 now supports 9070/XT (Navi4)</title>
    <updated>2025-05-21T14:48:49+00:00</updated>
    <author>
      <name>/u/shifty21</name>
      <uri>https://old.reddit.com/user/shifty21</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krzpmu/amd_rocm_641_now_supports_9070xt_navi4/"&gt; &lt;img alt="AMD ROCm 6.4.1 now supports 9070/XT (Navi4)" src="https://external-preview.redd.it/Uw9Z-ATtXBVz3bFA4dAJBygcK_v6wL5a2uOdNIk-9qE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55af9887767b7ab9df9c7ca842d03265592ce4ea" title="AMD ROCm 6.4.1 now supports 9070/XT (Navi4)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As of this post, AMD hasn't updated their github page or their official ROCm doc page, but here is the official link to their site. Looks like it is a bundled ROCm stack for Ubuntu LTS and RHEL 9.6.&lt;/p&gt; &lt;p&gt;I got my 9070XT at launch at MSRP, so this is good news for me!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shifty21"&gt; /u/shifty21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.amd.com/en/resources/support-articles/release-notes/RN-AMDGPU-UNIFIED-LINUX-25-10-1-ROCM-6-4-1.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krzpmu/amd_rocm_641_now_supports_9070xt_navi4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krzpmu/amd_rocm_641_now_supports_9070xt_navi4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T14:48:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1krtvpj</id>
    <title>Falcon-H1 Family of Hybrid-Head Language Models, including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B</title>
    <updated>2025-05-21T09:50:09+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krtvpj/falconh1_family_of_hybridhead_language_models/"&gt; &lt;img alt="Falcon-H1 Family of Hybrid-Head Language Models, including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B" src="https://external-preview.redd.it/asQIFBJYgU0s0y-AV0hAHtenKk6qa9ZCLFCb-Jjyvag.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=669a7c89198f19469e2598642c94e9e4b54a56f3" title="Falcon-H1 Family of Hybrid-Head Language Models, including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon-h1-6819f2795bc406da60fab8df"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krtvpj/falconh1_family_of_hybridhead_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krtvpj/falconh1_family_of_hybridhead_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T09:50:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks18uf</id>
    <title>Mistral's new Devstral coding model running on a single RTX 4090 with 54k context using Q4KM quantization with vLLM</title>
    <updated>2025-05-21T15:50:12+00:00</updated>
    <author>
      <name>/u/erdaltoprak</name>
      <uri>https://old.reddit.com/user/erdaltoprak</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks18uf/mistrals_new_devstral_coding_model_running_on_a/"&gt; &lt;img alt="Mistral's new Devstral coding model running on a single RTX 4090 with 54k context using Q4KM quantization with vLLM" src="https://preview.redd.it/ddhhql5ap52f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ac522114e2ed7386b3d3e60852472eaf2f4b906" title="Mistral's new Devstral coding model running on a single RTX 4090 with 54k context using Q4KM quantization with vLLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full model announcement post on the Mistral blog &lt;a href="https://mistral.ai/news/devstral"&gt;https://mistral.ai/news/devstral&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/erdaltoprak"&gt; /u/erdaltoprak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ddhhql5ap52f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks18uf/mistrals_new_devstral_coding_model_running_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ks18uf/mistrals_new_devstral_coding_model_running_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T15:50:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks1ncf</id>
    <title>Anyone else feel like LLMs aren't actually getting that much better?</title>
    <updated>2025-05-21T16:06:15+00:00</updated>
    <author>
      <name>/u/Swimming_Beginning24</name>
      <uri>https://old.reddit.com/user/Swimming_Beginning24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been in the game since GPT-3.5 (and even before then with Github Copilot). Over the last 2-3 years I've tried most of the top LLMs: all of the GPT iterations, all of the Claude's, Mistral's, LLama's, Deepseek's, Qwen's, and now Gemini 2.5 Pro Preview 05-06.&lt;/p&gt; &lt;p&gt;Based on benchmarks and LMSYS Arena, one would expect something like the newest Gemini 2.5 Pro to be leaps and bounds ahead of what GPT-3.5 or GPT-4 was. I feel like it's not. My use case is generally technical: longer form coding and system design sorts of questions. I occasionally also have models draft out longer English texts like reports or briefs.&lt;/p&gt; &lt;p&gt;Overall I feel like models still have the same problems that they did when ChatGPT first came out: hallucination, generic LLM babble, hard-to-find bugs in code, system designs that might check out on first pass but aren't fully thought out.&lt;/p&gt; &lt;p&gt;Don't get me wrong, LLMs are still incredible time savers, but they have been since the beginning. I don't know if my prompting techniques are to blame? I don't really engineer prompts at all besides explaining the problem and context as thoroughly as I can.&lt;/p&gt; &lt;p&gt;Does anyone else feel the same way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Swimming_Beginning24"&gt; /u/Swimming_Beginning24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks1ncf/anyone_else_feel_like_llms_arent_actually_getting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks1ncf/anyone_else_feel_like_llms_arent_actually_getting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ks1ncf/anyone_else_feel_like_llms_arent_actually_getting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T16:06:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kryxdg</id>
    <title>Meet Mistral Devstral, SOTA open model designed specifically for coding agents</title>
    <updated>2025-05-21T14:15:57+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://mistral.ai/news/devstral"&gt;https://mistral.ai/news/devstral&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Open Weights : &lt;a href="https://huggingface.co/mistralai/Devstral-Small-2505"&gt;https://huggingface.co/mistralai/Devstral-Small-2505&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF : &lt;a href="https://huggingface.co/lmstudio-community/Devstral-Small-2505-GGUF"&gt;https://huggingface.co/lmstudio-community/Devstral-Small-2505-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kryxdg/meet_mistral_devstral_sota_open_model_designed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kryxdg/meet_mistral_devstral_sota_open_model_designed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kryxdg/meet_mistral_devstral_sota_open_model_designed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T14:15:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1krzpyp</id>
    <title>medgemma-4b the Pharmacist ðŸ¤£</title>
    <updated>2025-05-21T14:49:13+00:00</updated>
    <author>
      <name>/u/AlternativePlum5151</name>
      <uri>https://old.reddit.com/user/AlternativePlum5151</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Googleâ€™s new OS medical model gave in to the dark side far too easily. I had to laugh. I expected it to put up a little more of a fight, but there you go.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlternativePlum5151"&gt; /u/AlternativePlum5151 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f25nhvxqd52f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krzpyp/medgemma4b_the_pharmacist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krzpyp/medgemma4b_the_pharmacist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T14:49:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1krs40j</id>
    <title>Why nobody mentioned "Gemini Diffusion" here? It's a BIG deal</title>
    <updated>2025-05-21T07:42:08+00:00</updated>
    <author>
      <name>/u/QuackerEnte</name>
      <uri>https://old.reddit.com/user/QuackerEnte</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krs40j/why_nobody_mentioned_gemini_diffusion_here_its_a/"&gt; &lt;img alt="Why nobody mentioned &amp;quot;Gemini Diffusion&amp;quot; here? It's a BIG deal" src="https://external-preview.redd.it/dFWSMq_9jHPdMVGchDlKvt7rzCFhQEFmxZm8XKq654M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b69152f4cc7971773a476232dcff0de3690e29e" title="Why nobody mentioned &amp;quot;Gemini Diffusion&amp;quot; here? It's a BIG deal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google has the capacity and capability to change the standard for LLMs from autoregressive generation to diffusion generation.&lt;/p&gt; &lt;p&gt;Google showed their Language diffusion model (Gemini Diffusion, visit the linked page for more info and benchmarks) yesterday/today (depends on your timezone), and it was extremely fast and (according to them) only half the size of similar performing models. They showed benchmark scores of the diffusion model compared to Gemini 2.0 Flash-lite, which is a tiny model already.&lt;/p&gt; &lt;p&gt;I know, it's LocalLLaMA, but if Google can prove that diffusion models work at scale, they are a far more viable option for local inference, given the speed gains.&lt;/p&gt; &lt;p&gt;And let's not forget that, since diffusion LLMs process the whole text at once iteratively, it doesn't need KV-Caching. Therefore, it could be more memory efficient. It also has &amp;quot;test time scaling&amp;quot; by nature, since the more passes it is given to iterate, the better the resulting answer, without needing CoT (It can do it in latent space, even, which is much better than discrete tokenspace CoT). &lt;/p&gt; &lt;p&gt;What do you guys think? Is it a good thing for the Local-AI community in the long run that Google is R&amp;amp;D-ing a fresh approach? Theyâ€™ve got massive resources. They can prove if diffusion models work at scale (bigger models) in future.&lt;/p&gt; &lt;p&gt;(PS: I used a (of course, ethically sourced, local) LLM to correct grammar and structure the text, otherwise it'd be a wall of text) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuackerEnte"&gt; /u/QuackerEnte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://deepmind.google/models/gemini-diffusion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krs40j/why_nobody_mentioned_gemini_diffusion_here_its_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krs40j/why_nobody_mentioned_gemini_diffusion_here_its_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T07:42:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kryybf</id>
    <title>mistralai/Devstral-Small-2505 Â· Hugging Face</title>
    <updated>2025-05-21T14:17:03+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kryybf/mistralaidevstralsmall2505_hugging_face/"&gt; &lt;img alt="mistralai/Devstral-Small-2505 Â· Hugging Face" src="https://external-preview.redd.it/5v7V2smikryAtPAPRovLgRwqCqqgG7mLENcd1_6EmM4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8ec3b7f12dc09c129535d0279c6db5801db61aa" title="mistralai/Devstral-Small-2505 Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Devstral is an agentic LLM for software engineering tasks built under a collaboration between Mistral AI and All Hands AI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Devstral-Small-2505"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kryybf/mistralaidevstralsmall2505_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kryybf/mistralaidevstralsmall2505_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T14:17:03+00:00</published>
  </entry>
</feed>
