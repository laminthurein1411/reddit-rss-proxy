<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-04T20:06:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1keqw5a</id>
    <title>Infrence on the cloud</title>
    <updated>2025-05-04T18:34:38+00:00</updated>
    <author>
      <name>/u/Sad_Bodybuilder8649</name>
      <uri>https://old.reddit.com/user/Sad_Bodybuilder8649</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm starting a newLLM inference project. How is it possible to do inference on the cloud in the most efficient way? Any experience is appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad_Bodybuilder8649"&gt; /u/Sad_Bodybuilder8649 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keqw5a/infrence_on_the_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keqw5a/infrence_on_the_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1keqw5a/infrence_on_the_cloud/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T18:34:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kedbv7</id>
    <title>Ryzen AI Max+ 395 + a gpu?</title>
    <updated>2025-05-04T06:31:04+00:00</updated>
    <author>
      <name>/u/Alarming-Ad8154</name>
      <uri>https://old.reddit.com/user/Alarming-Ad8154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see the Ryzen 395 Max+ spec sheet lists 16 PCIe 4.0 lanes. It‚Äôs also been use in some desktops. Is there any way to combine a max+ with a cheap 24gb GPU? Like an AMD 7900xtx or a 3090? I feel if you could put shared experts (llama 4) or most frequently used experts (qwen3) on the GPU the 395 max+ would be an absolute beast‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarming-Ad8154"&gt; /u/Alarming-Ad8154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kedbv7/ryzen_ai_max_395_a_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kedbv7/ryzen_ai_max_395_a_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kedbv7/ryzen_ai_max_395_a_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T06:31:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ke3heg</id>
    <title>How is your experience with Qwen3 so far?</title>
    <updated>2025-05-03T21:20:12+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do they prove their worth? Are the benchmark scores representative to their real world performance?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ke3heg/how_is_your_experience_with_qwen3_so_far/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ke3heg/how_is_your_experience_with_qwen3_so_far/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ke3heg/how_is_your_experience_with_qwen3_so_far/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-03T21:20:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdyasr</id>
    <title>Hey step-bro, that's HF forum, not the AI chat...</title>
    <updated>2025-05-03T17:28:57+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kdyasr/hey_stepbro_thats_hf_forum_not_the_ai_chat/"&gt; &lt;img alt="Hey step-bro, that's HF forum, not the AI chat..." src="https://preview.redd.it/3a4xy047qlye1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f0827b40d5dad9a38133015ced4dd0b976f205e" title="Hey step-bro, that's HF forum, not the AI chat..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3a4xy047qlye1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kdyasr/hey_stepbro_thats_hf_forum_not_the_ai_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kdyasr/hey_stepbro_thats_hf_forum_not_the_ai_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-03T17:28:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1keibf8</id>
    <title>Does your AI need help writing unified diffs?</title>
    <updated>2025-05-04T12:09:13+00:00</updated>
    <author>
      <name>/u/createthiscom</name>
      <uri>https://old.reddit.com/user/createthiscom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keibf8/does_your_ai_need_help_writing_unified_diffs/"&gt; &lt;img alt="Does your AI need help writing unified diffs?" src="https://external-preview.redd.it/vw1iWbdmvJgleS6hQIxSMNVxDXBOF3RAH6ljJQEZy2Y.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=765c0aab582be0348684b67a06ea6240d3c572f0" title="Does your AI need help writing unified diffs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use Deepseek-V3-0324 a lot for work in an agentic coding capacity with Open Hands AI. I found the existing tools lacking when editing large files. I got a lot of errors due to lines not being unique and such. I really want the AI to just use UNIX diff and patch, but it had a lot of trouble generating valid unified diffs. So I made a tool AIs can use as a crutch to help them fix their diffs: &lt;a href="https://github.com/createthis/diffcalculia"&gt;https://github.com/createthis/diffcalculia&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm pretty happy with the result, so I thought I'd share it. Maybe someone else finds it helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/createthiscom"&gt; /u/createthiscom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/createthis/diffcalculia"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keibf8/does_your_ai_need_help_writing_unified_diffs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1keibf8/does_your_ai_need_help_writing_unified_diffs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T12:09:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kefods</id>
    <title>Serving Qwen3-235B-A22B with 4-bit quantization and 32k context from a 128GB Mac</title>
    <updated>2025-05-04T09:16:55+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have tested this on Mac Studio M1 Ultra with 128GB running Sequoia 15.0.1, but this might work on macbooks that have the same amount of RAM if you are willing to set it up it as a LAN headless server. I suggest running some of the steps in &lt;a href="https://github.com/anurmatov/mac-studio-server/blob/main/scripts/optimize-mac-server.sh"&gt;https://github.com/anurmatov/mac-studio-server/blob/main/scripts/optimize-mac-server.sh&lt;/a&gt; to optimize resource usage.&lt;/p&gt; &lt;p&gt;The trick is to select the IQ4_XS quantization which uses less memory than Q4_K_M. In my tests there's no noticeable difference between the two other than IQ4_XS having lower TPS. In my setup I get ~18 TPS in the initial questions but it slows down to ~8 TPS when context is close to 32k tokens.&lt;/p&gt; &lt;p&gt;This is a very tight fit and you cannot be running anything else other than open webui (bare install without docker, as it would require more memory). That means llama-server will be used (can be downloaded by selecting the mac/arm64 zip here: &lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;https://github.com/ggml-org/llama.cpp/releases&lt;/a&gt;). Alternatively a smaller context window can be used to reduce memory usage.&lt;/p&gt; &lt;p&gt;Open Webui is optional and you can be running it in a different machine in the same LAN, just make sure to point to the correct llama-server address (admin panel -&amp;gt; settings -&amp;gt; connections -&amp;gt; Manage OpenAI API Connections). Any UI that can connect to OpenAI compatible endpoints should work. If you just want to code with aider-like tools, then UIs are not necessary.&lt;/p&gt; &lt;p&gt;The main steps to get this working are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Increase maximum VRAM allocation to 125GB by setting &lt;code&gt;iogpu.wired_limit_mb=128000&lt;/code&gt; in &lt;code&gt;/etc/sysctl.conf&lt;/code&gt; (need to reboot for this to take effect)&lt;/li&gt; &lt;li&gt;download all IQ4_XS weight parts from &lt;a href="https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/tree/main/IQ4_XS"&gt;https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/tree/main/IQ4_XS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;from the directory where the weights are downloaded to, run llama-server with &lt;/p&gt; &lt;p&gt;llama-server -fa -ctk q8_0 -ctv q8_0 --model Qwen3-235B-A22B-IQ4_XS-00001-of-00003.gguf --ctx-size 32768 --min-p 0.0 --top-k 20 --top-p 0.8 --temp 0.7 --slot-save-path kv-cache --port 8000&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These temp/top-p settings are the recommended for non-thinking mode, so make sure to add &lt;code&gt;/nothink&lt;/code&gt; to the system prompt!&lt;/p&gt; &lt;p&gt;An OpenAI compatible API endpoint should now be running on &lt;code&gt;http://127.0.0.1:8000&lt;/code&gt; (adjust &lt;code&gt;--host&lt;/code&gt; / &lt;code&gt;--port&lt;/code&gt; to your needs).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kefods/serving_qwen3235ba22b_with_4bit_quantization_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kefods/serving_qwen3235ba22b_with_4bit_quantization_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kefods/serving_qwen3235ba22b_with_4bit_quantization_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T09:16:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kegr4q</id>
    <title>What are your must have MCPs?</title>
    <updated>2025-05-04T10:31:22+00:00</updated>
    <author>
      <name>/u/m_abdelfattah</name>
      <uri>https://old.reddit.com/user/m_abdelfattah</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As LLMs are accessible now and MCPs are relatively mature, what are your must have ones?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m_abdelfattah"&gt; /u/m_abdelfattah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kegr4q/what_are_your_must_have_mcps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kegr4q/what_are_your_must_have_mcps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kegr4q/what_are_your_must_have_mcps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T10:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ked5iy</id>
    <title>Qwen 3 32b vs QwQ 32b</title>
    <updated>2025-05-04T06:18:30+00:00</updated>
    <author>
      <name>/u/nore_se_kra</name>
      <uri>https://old.reddit.com/user/nore_se_kra</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a comparison I barely see and its slightly confusing too as QwQ is kinda a pure reasoning model while Qwen 3 is using reasoning by default but it can be deactivated. In some benchmarks QwQ is even better - so the only advantage of Qwen seems to be that you can use it without reasoning. I assume most benchmarks were done with the default so how good is it without reasoning? Any experience? Other advantages? Or does someone know benchmarks that explicitly test Qwen without reasoning?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nore_se_kra"&gt; /u/nore_se_kra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ked5iy/qwen_3_32b_vs_qwq_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ked5iy/qwen_3_32b_vs_qwq_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ked5iy/qwen_3_32b_vs_qwq_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T06:18:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kegoi2</id>
    <title>Which is better for coding in 16GB (V)RAM at q4: Qwen3.0-30B-A3B, Qwen3.0-14B, Qwen2.5-Coding-14B, Phi4-14B, Mistral Small 3.0/3.1 24B?</title>
    <updated>2025-05-04T10:26:32+00:00</updated>
    <author>
      <name>/u/ethereel1</name>
      <uri>https://old.reddit.com/user/ethereel1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now that the dust has settled regarding Qwen3.0 quants, I feel it's finally safe to ask this question. My hunch is that Qwen2.5-Coding-14B is still the best in this range, but I want to check with those of you who've tested the latest corrected quants of Qwen3.0-30B-A3B and Qwen3.0-14B. Throwing in Phi and Mistral just in case as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ethereel1"&gt; /u/ethereel1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kegoi2/which_is_better_for_coding_in_16gb_vram_at_q4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kegoi2/which_is_better_for_coding_in_16gb_vram_at_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kegoi2/which_is_better_for_coding_in_16gb_vram_at_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T10:26:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1keblo7</id>
    <title>Quick shout-out to Qwen3-30b-a3b as a study tool for Calc2/3</title>
    <updated>2025-05-04T04:35:32+00:00</updated>
    <author>
      <name>/u/Skkeep</name>
      <uri>https://old.reddit.com/user/Skkeep</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, &lt;/p&gt; &lt;p&gt;I know the recent Qwen launch has been glazed to death already, but I want to give extra praise and acclaim to this model when it comes to studying. Extremely fast responses of broad, complex topics which are otherwise explained by AWFUL lecturers with terrible speaking skills. Yes, it isnt as smart as the 32b alternative, but for explanations of concepts or integrations/derivations, it is more than enough AND 3x the speed. &lt;/p&gt; &lt;p&gt;Thank you Alibaba, &lt;/p&gt; &lt;p&gt;EEE student. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Skkeep"&gt; /u/Skkeep &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keblo7/quick_shoutout_to_qwen330ba3b_as_a_study_tool_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keblo7/quick_shoutout_to_qwen330ba3b_as_a_study_tool_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1keblo7/quick_shoutout_to_qwen330ba3b_as_a_study_tool_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T04:35:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1keqrzb</id>
    <title>Super simple RAG?</title>
    <updated>2025-05-04T18:29:47+00:00</updated>
    <author>
      <name>/u/9acca9</name>
      <uri>https://old.reddit.com/user/9acca9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use LM-Studio, and I wanted to know if it's useful to use an install-and-use RAG to ask questions about a set of books (text). Or is it the same as adding the book(s) to the LM-Studio chat (which, from what I noticed, also creates a RAG when you query (I saw it says something about &amp;quot;retrieval&amp;quot; and sending parts of the book)).&lt;/p&gt; &lt;p&gt;In that case, it might be useful. Which one do you recommend? (Or should I stick with what LM-Studio does?)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/9acca9"&gt; /u/9acca9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keqrzb/super_simple_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keqrzb/super_simple_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1keqrzb/super_simple_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T18:29:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ke3nw3</id>
    <title>Qwen 3 235b beats sonnet 3.7 in aider polyglot</title>
    <updated>2025-05-03T21:28:35+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ke3nw3/qwen_3_235b_beats_sonnet_37_in_aider_polyglot/"&gt; &lt;img alt="Qwen 3 235b beats sonnet 3.7 in aider polyglot" src="https://preview.redd.it/sjw1h2yexmye1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3228950c23440471a53b0440043d2bbfaeaa2b03" title="Qwen 3 235b beats sonnet 3.7 in aider polyglot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Win for open source &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sjw1h2yexmye1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ke3nw3/qwen_3_235b_beats_sonnet_37_in_aider_polyglot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ke3nw3/qwen_3_235b_beats_sonnet_37_in_aider_polyglot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-03T21:28:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kenw3u</id>
    <title>Run AI Agents with Near-Native Speed on macOS‚ÄîIntroducing C/ua.</title>
    <updated>2025-05-04T16:28:29+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share an exciting open-source framework called C/ua, specifically optimized for Apple Silicon Macs. C/ua allows AI agents to seamlessly control entire operating systems running inside high-performance, lightweight virtual containers.&lt;/p&gt; &lt;p&gt;Key Highlights:&lt;/p&gt; &lt;p&gt;Performance: Achieves up to 97% of native CPU speed on Apple Silicon. Compatibility: Works smoothly with any AI language model. Open Source: Fully available on GitHub for customization and community contributions.&lt;/p&gt; &lt;p&gt;Whether you're into automation, AI experimentation, or just curious about pushing your Mac's capabilities, check it out here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts and see what innovative use cases the macOS community can come up with!&lt;/p&gt; &lt;p&gt;Happy hacking!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kenw3u/run_ai_agents_with_nearnative_speed_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kenw3u/run_ai_agents_with_nearnative_speed_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kenw3u/run_ai_agents_with_nearnative_speed_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T16:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1keh542</id>
    <title>Qwen3 on Dubesor Benchmark</title>
    <updated>2025-05-04T10:57:18+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/"&gt; &lt;img alt="Qwen3 on Dubesor Benchmark" src="https://b.thumbs.redditmedia.com/SK05go83T5ZWwsJO6IMatx5Z4uUx4gveCuwSgrYHpAg.jpg" title="Qwen3 on Dubesor Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://dubesor.de/benchtable.html"&gt;https://dubesor.de/benchtable.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One of the few benchmarks that tested both thinking on/off of qwen3 &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eim5m35nxqye1.png?width=1265&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cd814d571735444429331c73b4cd17a066497907"&gt;https://preview.redd.it/eim5m35nxqye1.png?width=1265&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cd814d571735444429331c73b4cd17a066497907&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Small-scale manual performance comparison benchmark I made for myself. This table showcases the results I recorded of various AI models across different personal tasks I encountered over time (currently 83). I use a &lt;strong&gt;weighted rating system&lt;/strong&gt; and calculate the difficulty for each tasks by incorporating the results of all models. This is particularly relevant in scoring when failing easy questions or passing hard ones.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE, THAT THIS JUST ME SHARING THE RESULTS FROM MY OWN SMALL-SCALE PERSONAL TESTING. YMMV! OBVIOUSLY THE SCORES ARE JUST THAT AND MIGHT NOT REFLECT YOUR OWN PERSONAL EXPERIENCES OR OTHER WELL-KNOWN BENCHMARKS.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T10:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1kegrce</id>
    <title>Qwen3 no reasoning vs Qwen2.5</title>
    <updated>2025-05-04T10:31:46+00:00</updated>
    <author>
      <name>/u/No-Bicycle-132</name>
      <uri>https://old.reddit.com/user/No-Bicycle-132</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems evident that Qwen3 with reasoning beats Qwen2.5. But I wonder if the Qwen3 dense models with reasoning turned off also outperforms Qwen2.5. Essentially what I am wondering is if the improvements mostly come from the reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Bicycle-132"&gt; /u/No-Bicycle-132 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T10:31:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kedu0d</id>
    <title>IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models</title>
    <updated>2025-05-04T07:05:53+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/"&gt; &lt;img alt="IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models" src="https://external-preview.redd.it/yxzFCzIK4WeSo6gF_lu0-lKTBRUJ2trx5h9oRthAsG8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11a2564e23d1a12b0fe7b5c447c7819e46dd629d" title="IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T07:05:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1keh382</id>
    <title>Local Deep Research v0.3.1: We need your help for improving the tool</title>
    <updated>2025-05-04T10:53:51+00:00</updated>
    <author>
      <name>/u/ComplexIt</name>
      <uri>https://old.reddit.com/user/ComplexIt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, we are trying to improve LDR. &lt;/p&gt; &lt;p&gt;What areas do need attention in your opinion? - What features do you need? - What types of research you need? - How to improve the UI?&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/LearningCircuit/local-deep-research"&gt;https://github.com/LearningCircuit/local-deep-research&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Quick install:&lt;/h3&gt; &lt;p&gt;```bash pip install local-deep-research python -m local_deep_research.web.app&lt;/p&gt; &lt;h1&gt;For SearXNG (highly recommended):&lt;/h1&gt; &lt;p&gt;docker pull searxng/searxng docker run -d -p 8080:8080 --name searxng searxng/searxng&lt;/p&gt; &lt;h1&gt;Start SearXNG (Required after system restart)&lt;/h1&gt; &lt;p&gt;docker start searxng ```&lt;/p&gt; &lt;p&gt;(Use Direct SearXNG for maximum speed instead of &amp;quot;auto&amp;quot; - this bypasses the LLM calls needed for engine selection in auto mode)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexIt"&gt; /u/ComplexIt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keh382/local_deep_research_v031_we_need_your_help_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keh382/local_deep_research_v031_we_need_your_help_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1keh382/local_deep_research_v031_we_need_your_help_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T10:53:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kemt2m</id>
    <title>Which coding model is best for 48GB VRAM</title>
    <updated>2025-05-04T15:42:29+00:00</updated>
    <author>
      <name>/u/Su1tz</name>
      <uri>https://old.reddit.com/user/Su1tz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is for data science, mostly excel data manipulation in python. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Su1tz"&gt; /u/Su1tz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T15:42:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1keoint</id>
    <title>LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!</title>
    <updated>2025-05-04T16:55:10+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/"&gt; &lt;img alt="LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!" src="https://a.thumbs.redditmedia.com/bM9LC8PSLdBmtmFQOjBwlZthNsiYL5J4IXaOzEPqwY4.jpg" title="LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/3bwwfd4epsye1.png?width=3404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=adbb0bce2c13bc560499b0d3459329d16d0a3291"&gt;You can't go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B and 30B)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m4x5z2sposye1.png?width=3404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=26b2ff50d960dd957e86feb04a8c21030ef0195c"&gt;mainline llama.cpp just got a boost for fully offloaded Qwen3 MoE (single expert)&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;tl;dr;&lt;/h1&gt; &lt;p&gt;I highly recommend doing a &lt;code&gt;git pull&lt;/code&gt; and re-building your &lt;code&gt;ik_llama.cpp&lt;/code&gt; or &lt;code&gt;llama.cpp&lt;/code&gt; repo to take advantage of recent major performance improvements just released.&lt;/p&gt; &lt;p&gt;The friendly competition between these amazing projects is producing delicious fruit for the whole GGUF loving &lt;code&gt;r/LocalLLaMA&lt;/code&gt; community!&lt;/p&gt; &lt;p&gt;If you have enough VRAM to fully offload and already have an existing &amp;quot;normal&amp;quot; quant of Qwen3 MoE then you'll get a little more speed out of mainline llama.cpp. If you are doing hybrid CPU+GPU offload or want to take advantage of the new SotA iqN_k quants, then check out ik_llama.cpp fork!&lt;/p&gt; &lt;h1&gt;Details&lt;/h1&gt; &lt;p&gt;I spent yesterday compiling and running benhmarks on the newest versions of both &lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;ik_llama.cpp&lt;/a&gt; and mainline &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For those that don't know, ikawrakow was an early contributor to mainline llama.cpp working on important features that have since trickled down into ollama, lmstudio, koboldcpp etc. At some point (presumably for reasons beyond my understanding) the &lt;code&gt;ik_llama.cpp&lt;/code&gt; fork was built and has a number of interesting features including SotA &lt;code&gt;iqN_k&lt;/code&gt; quantizations that pack in a lot of quality for the size while retaining good speed performance. (These new quants are &lt;em&gt;not&lt;/em&gt; available in ollma, lmstudio, koboldcpp, etc.)&lt;/p&gt; &lt;p&gt;A few recent PRs made by ikawrakow to &lt;code&gt;ik_llama.cpp&lt;/code&gt; and by JohannesGaessler to mainline have &lt;em&gt;boosted performance across the board&lt;/em&gt; and especially on CUDA with Flash Attention implementations for Grouped Query Attention (GQA) models and also Mixutre of Experts (MoEs) like the recent and amazing Qwen3 235B and 30B releases!&lt;/p&gt; &lt;h1&gt;References&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp/pull/370"&gt;ikawrakow/ik_llama.cpp/pull/370&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T16:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ken4uk</id>
    <title>I made a fake phone to text fake people with llamacpp</title>
    <updated>2025-05-04T15:56:28+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's useless and stupid, but also kinda fun. You create and add characters to a pretend phone, and then message them.&lt;/p&gt; &lt;p&gt;Does not work with &amp;quot;thinking&amp;quot; models as it isn't set to parse out the thinking tags.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/openconstruct/llamaphone"&gt;LLamaPhone&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ken4uk/i_made_a_fake_phone_to_text_fake_people_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ken4uk/i_made_a_fake_phone_to_text_fake_people_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ken4uk/i_made_a_fake_phone_to_text_fake_people_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T15:56:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kebauw</id>
    <title>Apparently shipping AI platforms is a thing now as per this post from the Qwen X account</title>
    <updated>2025-05-04T04:16:37+00:00</updated>
    <author>
      <name>/u/MushroomGecko</name>
      <uri>https://old.reddit.com/user/MushroomGecko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kebauw/apparently_shipping_ai_platforms_is_a_thing_now/"&gt; &lt;img alt="Apparently shipping AI platforms is a thing now as per this post from the Qwen X account" src="https://preview.redd.it/fjze9by1yoye1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e0eb0e11cb1eca743ebbe8cd75e69b96fffd960" title="Apparently shipping AI platforms is a thing now as per this post from the Qwen X account" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MushroomGecko"&gt; /u/MushroomGecko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fjze9by1yoye1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kebauw/apparently_shipping_ai_platforms_is_a_thing_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kebauw/apparently_shipping_ai_platforms_is_a_thing_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T04:16:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kenk4f</id>
    <title>QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison.</title>
    <updated>2025-05-04T16:14:12+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kenk4f/qwq_32b_vs_qwen_3_32b_vs_glm432b_html_coding_only/"&gt; &lt;img alt="QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison." src="https://external-preview.redd.it/rvBe2sSMWUb2BiTsxft299oO0IRh9G0lMoWcfjP8v_w.png?width=140&amp;amp;height=70&amp;amp;crop=140:70,smart&amp;amp;auto=webp&amp;amp;s=725aced25c191e436bfd44f4619541673facf020" title="QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All models are from Bartowski - q4km version&lt;/p&gt; &lt;p&gt;Test only HTML frontend.&lt;/p&gt; &lt;p&gt;My assessment lauout quality from 0 to 10&lt;/p&gt; &lt;p&gt;Prompt&lt;/p&gt; &lt;p&gt;&amp;quot;Generate a beautiful website for Steve's pc repair using a single html script.&amp;quot;&lt;/p&gt; &lt;p&gt;QwQ 32b - 3/10&lt;/p&gt; &lt;p&gt;- poor layout but ..works , very basic&lt;/p&gt; &lt;p&gt;- 250 line of code&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6rol9pc6hsye1.png?width=2461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b"&gt;https://preview.redd.it/6rol9pc6hsye1.png?width=2461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen 3 32b - 6/10&lt;/p&gt; &lt;p&gt;- much better looks but still not too complex layout&lt;/p&gt; &lt;p&gt;- 310 lines of the code&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z9qixbh8hsye1.png?width=2461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29e6bb4b272399ba8140785feb429a196ecc5173"&gt;https://preview.redd.it/z9qixbh8hsye1.png?width=2461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29e6bb4b272399ba8140785feb429a196ecc5173&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM-4-32b 9/10&lt;/p&gt; &lt;p&gt;- looks insanely good , quality layout like sonnet 3.7 easily&lt;/p&gt; &lt;p&gt;- 1500+ code lines&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3zj2lr2ahsye1.png?width=2469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964"&gt;https://preview.redd.it/3zj2lr2ahsye1.png?width=2469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM-4-32b is insanely good for html code frontend.&lt;/p&gt; &lt;p&gt;I say that model is VERY GOOD ONLY IN THIS FIELD and JavaScript at most.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Other coding language like python , c , c++ or any other quality of the code will be on the level of qwen 2.5 32b coder, reasoning and math also is on the seme level but for html and JavaScript ... is GREAT.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kenk4f/qwq_32b_vs_qwen_3_32b_vs_glm432b_html_coding_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kenk4f/qwq_32b_vs_qwen_3_32b_vs_glm432b_html_coding_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kenk4f/qwq_32b_vs_qwen_3_32b_vs_glm432b_html_coding_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T16:14:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kepuli</id>
    <title>Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)</title>
    <updated>2025-05-04T17:51:10+00:00</updated>
    <author>
      <name>/u/intofuture</name>
      <uri>https://old.reddit.com/user/intofuture</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/"&gt; &lt;img alt="Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)" src="https://external-preview.redd.it/MEsW6HAKwMTePsFq8QDbEDhgynEAo3tDzKfDXY_7QMw.gif?width=640&amp;amp;crop=smart&amp;amp;s=15d5869bcc71cdb59dfcd1c7aad46834fe7e81cd" title="Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLlama!&lt;/p&gt; &lt;p&gt;We've started publishing open-source model performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS, Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+ soon.&lt;/p&gt; &lt;p&gt;We‚Äôre doing this because perf metrics determine the viability of shipping models in apps to users (no end-user wants crashing/slow AI features that hog up their specific device).&lt;/p&gt; &lt;p&gt;Although benchmarks get posted in threads here and there, we feel like a more consolidated and standardized hub should probably exist.&lt;/p&gt; &lt;p&gt;We figured we'd kickstart this since we already maintain this benchmarking infra/tooling at RunLocal for our enterprise customers. Note: We‚Äôve mostly focused on supporting model formats like Core ML, ONNX and TFLite to date, so a few things are still WIP for GGUF support. &lt;/p&gt; &lt;p&gt;Thought it would be cool to start with benchmarks for Qwen3 (Num Prefill Tokens=512, Num Generation Tokens=128). &lt;a href="https://huggingface.co/collections/unsloth/qwen3-680edabfb790c8c34a242f95"&gt;GGUFs are from Unsloth&lt;/a&gt; üêê&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l59qu1gxysye1.png?width=961&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=381abad7b25e1d719265826441b51aa50177d143"&gt;Qwen3 GGUF benchmarks on laptops&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z5qxhpc1zsye1.png?width=913&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c48aa6c5c753c7dc74c4397aac34f92383d17afe"&gt;Qwen3 GGUF benchmarks on phones&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can see more of the benchmark data for Qwen3 &lt;a href="https://edgemeter.runlocal.ai/public/pipelines/a240f768-2847-4e06-8df9-156ea3c2c321"&gt;here&lt;/a&gt;. We realize there are so many variables (devices, backends, etc.) that interpreting the data is currently harder than it should be. We'll work on that!&lt;/p&gt; &lt;p&gt;You can also see benchmarks for a few other models &lt;a href="https://edgemeter.runlocal.ai/public/pipelines"&gt;here&lt;/a&gt;. If you want to see benchmarks for any others, feel free to request them and we‚Äôll try to publish ASAP!&lt;/p&gt; &lt;p&gt;Lastly, you can run your own benchmarks on our devices for free (limited to some degree to avoid our devices melting!).&lt;/p&gt; &lt;p&gt;This free/public version is a bit of a frankenstein fork of our enterprise product, so any benchmarks you run would be private to your account. But if there's interest, we can add a way for you to also publish them so that the public benchmarks aren‚Äôt bottlenecked by us. &lt;/p&gt; &lt;p&gt;It‚Äôs still very early days for us with this, so please let us know what would make it better/cooler for the community!&lt;/p&gt; &lt;p&gt;To more on-device AI in production! üí™&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/aev5rgjazsye1.gif"&gt;https://i.redd.it/aev5rgjazsye1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/intofuture"&gt; /u/intofuture &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T17:51:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1keo3te</id>
    <title>UI-Tars-1.5 reasoning never fails to entertain me.</title>
    <updated>2025-05-04T16:37:31+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keo3te/uitars15_reasoning_never_fails_to_entertain_me/"&gt; &lt;img alt="UI-Tars-1.5 reasoning never fails to entertain me." src="https://preview.redd.it/627wnr5emsye1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b896f5165e878160c1e104137518ab1d80b3addc" title="UI-Tars-1.5 reasoning never fails to entertain me." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;7B parameter computer use agent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/627wnr5emsye1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keo3te/uitars15_reasoning_never_fails_to_entertain_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1keo3te/uitars15_reasoning_never_fails_to_entertain_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T16:37:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1keolh9</id>
    <title>Visa is looking for vibe coders - thoughts?</title>
    <updated>2025-05-04T16:58:36+00:00</updated>
    <author>
      <name>/u/eastwindtoday</name>
      <uri>https://old.reddit.com/user/eastwindtoday</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keolh9/visa_is_looking_for_vibe_coders_thoughts/"&gt; &lt;img alt="Visa is looking for vibe coders - thoughts?" src="https://preview.redd.it/gefvhv84qsye1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=235b3e1de1b7df4bd1bc1f7519f84b5259303d05" title="Visa is looking for vibe coders - thoughts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eastwindtoday"&gt; /u/eastwindtoday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gefvhv84qsye1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keolh9/visa_is_looking_for_vibe_coders_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1keolh9/visa_is_looking_for_vibe_coders_thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T16:58:36+00:00</published>
  </entry>
</feed>
