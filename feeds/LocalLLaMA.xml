<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-10T19:48:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j81d1p</id>
    <title>Can an LLM Learn to See? Fine Tuning Qwen 0.5B for Vision Tasks with SFT + GRPO</title>
    <updated>2025-03-10T15:32:57+00:00</updated>
    <author>
      <name>/u/JacksonCakess</name>
      <uri>https://old.reddit.com/user/JacksonCakess</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j81d1p/can_an_llm_learn_to_see_fine_tuning_qwen_05b_for/"&gt; &lt;img alt="Can an LLM Learn to See? Fine Tuning Qwen 0.5B for Vision Tasks with SFT + GRPO" src="https://b.thumbs.redditmedia.com/gsc7NXCXOZm-2WKKXpiQ-5Lp8WMwgU71T_1ywxzIpBc.jpg" title="Can an LLM Learn to See? Fine Tuning Qwen 0.5B for Vision Tasks with SFT + GRPO" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I just published a blog breaking down the &lt;strong&gt;math behind&lt;/strong&gt; &lt;em&gt;Group Relative Policy Optimization&lt;/em&gt; &lt;strong&gt;GRPO, the RL method behind DeepSeek R1&lt;/strong&gt; and walking through its &lt;strong&gt;implementation in&lt;/strong&gt; &lt;code&gt;trl&lt;/code&gt;‚Äîstep by step!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y7e4j2jksvne1.png?width=927&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38e2af3a04370a8edfb55e889b5f2e94e724ebd4"&gt;https://preview.redd.it/y7e4j2jksvne1.png?width=927&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38e2af3a04370a8edfb55e889b5f2e94e724ebd4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fun experiment included&lt;/strong&gt;:&lt;br /&gt; I fine-tuned &lt;strong&gt;Qwen 2.5 0.5B&lt;/strong&gt;, a &lt;strong&gt;language-only&lt;/strong&gt; model without prior visual training, using &lt;strong&gt;SFT + GRPO&lt;/strong&gt; and got &lt;strong&gt;~73% accuracy&lt;/strong&gt; on a &lt;strong&gt;visual counting task&lt;/strong&gt;! &lt;/p&gt; &lt;p&gt;&lt;a href="https://jacksoncakes.com/2025/03/10/can-an-llm-learn-to-see-fine-tuning-qwen-05b-for-vision-tasks-with-sft-grpo/"&gt;Full blog&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/JacksonCakes/vision-r1"&gt;Github&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JacksonCakess"&gt; /u/JacksonCakess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j81d1p/can_an_llm_learn_to_see_fine_tuning_qwen_05b_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j81d1p/can_an_llm_learn_to_see_fine_tuning_qwen_05b_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j81d1p/can_an_llm_learn_to_see_fine_tuning_qwen_05b_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T15:32:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7wymw</id>
    <title>Expert opinion requested: Am I reaching the limits of &lt;10GB models?</title>
    <updated>2025-03-10T12:00:58+00:00</updated>
    <author>
      <name>/u/yo252yo</name>
      <uri>https://old.reddit.com/user/yo252yo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I've been trying to make a conversation agent for a few weeks now and I'm not very happy with what I'm getting.&lt;/p&gt; &lt;p&gt;I'm working on an RTX 4070 and I've found that it allows me to run perfectly smoothly models around 7/8B params, essentially everything that takes &lt;strong&gt;&amp;lt;8GB VRAM&lt;/strong&gt; comfortably.&lt;/p&gt; &lt;p&gt;I'm honestly really impressed by the quality of the output for such small models, but I'm &lt;strong&gt;struggling with them understanding instructions&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Since these models are pretty small, I'm trying to avoid too-long system prompts and have been keeping mine around 400 words.&lt;/p&gt; &lt;p&gt;I've tried shorter and longer, I've tried various models but they all tend to gravitate towards &lt;strong&gt;common pitfalls&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;they produce big responses, ignoring my instructions to keep things to 1/2 sentences&lt;/li&gt; &lt;li&gt;they produce the stereotypical LLM responses with open ended questions at the end of every response &amp;quot;What do you think about X?&amp;quot;&lt;/li&gt; &lt;li&gt;they sometimes get weirdly stuck talking repetitively in broad terms about a generic topic instead of following the flow of the conversation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These problems are quite abstract and hard to investigate. The biggest pain point though is that whatever I seem to do in prompt to mitigate seems mostly ignored.&lt;/p&gt; &lt;p&gt;It's my understanding that those are common pitfalls of small or old models. I have &lt;strong&gt;ideas for further exploration&lt;/strong&gt; such as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;maybe try to write a really long prompt explaining exactly what is a conversation&lt;/li&gt; &lt;li&gt;maybe i should try to feed it more examples, for instance hardcode a beginning of a conversation&lt;/li&gt; &lt;li&gt;maybe i should try my own fine-tuning (though I'm not super good at complex tech stuff). In particular I'm thinking maybe all models are either tuned for ERP or chatbot query/answer and I might not have found a model that does good friendly SFW conversation&lt;/li&gt; &lt;li&gt;i'm also experimenting with how much metadata I feed into the system and how I feed it (i.e. the conversation topic is X, the conversation so far has been XYZ...). I was inserting this as SystemMessage in the conversation feed to complete but maybe that's not a good thing, idk. I wonder if that stuff is best in the system prompt or in the discussion thread...&lt;/li&gt; &lt;li&gt;maybe i can have another round-trip of a tiny model taking the output of my model and shortening it to make it fit in a conversation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But before I continue investing so much time in all of this, I wanted to gather feedback from people who might know more, because maybe I'm just hitting a wall and nothing I do will help short of investing in better hardware. That being said I'll lose it if I spend so much money on a bit more VRAM and the 13b or more models still cant follow simple instructions.&lt;/p&gt; &lt;p&gt;What do you guys think? I've read everything I could find about small model pitfalls, but I haven't found an answer to questions like: Does anyone have an understanding on how long can I afford to make a system prompt for a 7B model? Do any of my mitigation plans seem more promising than the others? Is there any trick to conversational AI that I missed?&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;p&gt;&lt;em&gt;PS: my best results have been with neuraldaredevil-8b-abliterated:q8_0, l3-8b-stheno-v3.2 or mn-12b-mag-mell-r1:latest, deepseek-r1:8b is nice but i cant get it to make short answers.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yo252yo"&gt; /u/yo252yo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7wymw/expert_opinion_requested_am_i_reaching_the_limits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7wymw/expert_opinion_requested_am_i_reaching_the_limits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7wymw/expert_opinion_requested_am_i_reaching_the_limits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T12:00:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7ytq4</id>
    <title>Enjoying local LLM so much! My ultimate wish: A bag of compact, domain-focused "expert" models.</title>
    <updated>2025-03-10T13:39:29+00:00</updated>
    <author>
      <name>/u/partysnatcher</name>
      <uri>https://old.reddit.com/user/partysnatcher</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like the title says, I'm really hooked on the &amp;quot;local LLM movement&amp;quot;. I'm very much enjoying and making use of for instance DeepSeek-R1:14b locally - with plenty of use for it (for instance, Im batch scripting to create a mini trainingset Im playing with).&lt;/p&gt; &lt;p&gt;However, 14B quantized (Qwen-2.5 based one), while extremely impressive for what it can do, is definitely limited by parameter size (in terms of precision, hallucination etc).&lt;/p&gt; &lt;p&gt;Despite that, I do not want to buy 64x 3090s to create some AI god that thinks for me and does everything for me.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I want to manually choose an expert (or a mix of experts) per task.&lt;/strong&gt; Not only is that less troublesome, but I think it offers more control and is more involving and fun.&lt;/p&gt; &lt;p&gt;I also think that focused &amp;quot;verifier models&amp;quot; that are solely based on breaking down and criticizing text, are very useful, not only for the individual user tasks, but also, when an expert and a verifier are running serially and bouncing back and forth, they can create a stronger and more tightly wound form of the same back-and-forth that reasoning models do.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;Topic: what is the next breakthrough in physics?&lt;/code&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Physics expert&amp;quot;:&lt;/strong&gt; &lt;code&gt;Deeper understanding of engineering quantum mechanics .. quantum computing .. blabla. &amp;lt;VERIFICATION REQUESTED&amp;gt;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Physics tester/verifier&amp;quot;:&lt;/strong&gt; &lt;code&gt;Interesting thoughts, but paragraph 1 breaks with the principle in the standard model of .. &amp;lt;REITERATION REQUESTED&amp;gt;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Physics expert&amp;quot;:&lt;/strong&gt; &lt;code&gt;I have modified paragraph 1 for better coherence with the standard model. This changes some of the premises in paragraph 2. &amp;lt;VERIFICATION REQUESTED&amp;gt;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Physics tester/verifier&amp;quot;:&lt;/strong&gt; &lt;code&gt;That looks good..&amp;lt;ITERATION END&amp;gt;&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Here is an example list of focused experts (with verifiers / testers) that I want to pull from ollama some day:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Task planning and project management agent (with strong interdisciplinary overview) &lt;ul&gt; &lt;li&gt;TESTER/VERIFIER: Local user (me)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Coding (on the architectural level)&lt;/li&gt; &lt;li&gt;Coding (on the function level) &lt;ul&gt; &lt;li&gt;TESTER/VERIFIER: Coding (on the testing level)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Multilinguality and single-direction translation &lt;ul&gt; &lt;li&gt;TESTER/VERIFIER: Other-direction translation&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Expert understanding of &amp;lt;field&amp;gt; (physics, biology, medicine, economy, local accounting) &lt;ul&gt; &lt;li&gt;TESTER/VERIFIER: Dictionary-style knowledge LLM (&amp;quot;wiki mixer&amp;quot;)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Good text creation &lt;ul&gt; &lt;li&gt;TESTER/VERIFIER: Good text analysis and understanding; coherence, purpose-oriented, reader experience focused LLM&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Image generation - diffusion &lt;ul&gt; &lt;li&gt;TESTER/VERIFIER: Image testing - interpretation and analysis model&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Mainly, I would love to run these independently, but of course, each of these can recursively &amp;quot;script each other up&amp;quot;, and run serially, either in agentic setup or in a inter-model reasoning design.&lt;/p&gt; &lt;p&gt;In short, I don't really anymore believe in this vision of a singular intelligent entity hosted in Silicon Valley that knows anything and everything. To me, all arrows point in the direction of focused dense models, and I want as many compact dense expert models as I can get my hands on.&lt;/p&gt; &lt;p&gt;What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/partysnatcher"&gt; /u/partysnatcher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7ytq4/enjoying_local_llm_so_much_my_ultimate_wish_a_bag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7ytq4/enjoying_local_llm_so_much_my_ultimate_wish_a_bag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7ytq4/enjoying_local_llm_so_much_my_ultimate_wish_a_bag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T13:39:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8135u</id>
    <title>What are some good LLMs for NSWF stories and dialogues? (RTX 3090, 64GB RAM, Win10)</title>
    <updated>2025-03-10T15:21:16+00:00</updated>
    <author>
      <name>/u/rookan</name>
      <uri>https://old.reddit.com/user/rookan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need to create scripts with lewd stories and dialogues for NSFW animations with popular game franchises characters (Final Fantasy, Mass Effect). Are there LLMs that could mimic a style a specific character talks? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rookan"&gt; /u/rookan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8135u/what_are_some_good_llms_for_nswf_stories_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8135u/what_are_some_good_llms_for_nswf_stories_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8135u/what_are_some_good_llms_for_nswf_stories_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T15:21:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j87eum</id>
    <title>QwQ 32B can do it if you coach it 2 times</title>
    <updated>2025-03-10T19:41:49+00:00</updated>
    <author>
      <name>/u/DrVonSinistro</name>
      <uri>https://old.reddit.com/user/DrVonSinistro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j87eum/qwq_32b_can_do_it_if_you_coach_it_2_times/"&gt; &lt;img alt="QwQ 32B can do it if you coach it 2 times" src="https://external-preview.redd.it/bGFmOXk2NDIxeG5lMalrzKbbY1wxsyua5vTpp1g3RTatq_ecPpvEXRJ-_J8E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3bc2c544369e356552a9d78fa1f23bdc00fdf6c3" title="QwQ 32B can do it if you coach it 2 times" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrVonSinistro"&gt; /u/DrVonSinistro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6wn0l7421xne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j87eum/qwq_32b_can_do_it_if_you_coach_it_2_times/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j87eum/qwq_32b_can_do_it_if_you_coach_it_2_times/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T19:41:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j85q5m</id>
    <title>every LLM metric you need to know</title>
    <updated>2025-03-10T18:31:58+00:00</updated>
    <author>
      <name>/u/FlimsyProperty8544</name>
      <uri>https://old.reddit.com/user/FlimsyProperty8544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The best way to improve LLM performance is to consistently benchmark your model using a well-defined set of metrics throughout development, rather than relying on ‚Äúvibe check‚Äù coding‚Äîthis approach helps ensure that any modifications don‚Äôt inadvertently cause regressions.&lt;/p&gt; &lt;p&gt;I‚Äôve listed below some essential LLM metrics to know before you begin benchmarking your LLM. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;A Note about Statistical Metrics:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Traditional NLP evaluation methods like BERT and ROUGE are fast, affordable, and reliable. However, their reliance on reference texts and inability to capture the nuanced semantics of open-ended, often complexly formatted LLM outputs make them less suitable for production-level evaluations. &lt;/p&gt; &lt;p&gt;LLM judges are much more effective if you care about evaluation accuracy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RAG metrics&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-answer-relevancy"&gt;Answer Relevancy:&lt;/a&gt; measures the quality of your RAG pipeline's generator by evaluating how relevant the actual output of your LLM application is compared to the provided input&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-faithfulness"&gt;Faithfulness:&lt;/a&gt; measures the quality of your RAG pipeline's generator by evaluating whether the actual output factually aligns with the contents of your retrieval context&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-contextual-precision"&gt;Contextual Precision:&lt;/a&gt; measures your RAG pipeline's retriever by evaluating whether nodes in your retrieval context that are relevant to the given input are ranked higher than irrelevant ones.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-contextual-recall"&gt;Contextual Recall:&lt;/a&gt; measures the quality of your RAG pipeline's retriever by evaluating the extent of which the retrieval context aligns with the expected output&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-contextual-relevancy"&gt;Contextual Relevancy:&lt;/a&gt; measures the quality of your RAG pipeline's retriever by evaluating the overall relevance of the information presented in your retrieval context for a given input&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Agentic metrics&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-tool-correctness"&gt;Tool Correctness:&lt;/a&gt; assesses your LLM agent's function/tool calling ability. It is calculated by comparing whether every tool that is expected to be used was indeed called.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-task-completion"&gt;Task Completion:&lt;/a&gt; evaluates how effectively an LLM agent accomplishes a task as outlined in the input, based on tools called and the actual output of the agent.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Conversational metrics&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-role-adherence"&gt;Role Adherence:&lt;/a&gt; determines whether your LLM chatbot is able to adhere to its given role throughout a conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-knowledge-retention"&gt;Knowledge Retention:&lt;/a&gt; determines whether your LLM chatbot is able to retain factual information presented throughout a conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-conversation-completeness"&gt;Conversational Completeness:&lt;/a&gt; determines whether your LLM chatbot is able to complete an end-to-end conversation by satisfying user needs throughout a conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-conversation-relevancy"&gt;Conversational Relevancy:&lt;/a&gt; determines whether your LLM chatbot is able to consistently generate relevant responses throughout a conversation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Robustness&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-prompt-alignment"&gt;Prompt Alignment:&lt;/a&gt; measures whether your LLM application is able to generate outputs that aligns with any instructions specified in your prompt template.&lt;/li&gt; &lt;li&gt;Output Consistency: measures the consistency of your LLM output given the same input.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Custom metrics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Custom metrics are particularly effective when you have a specialized use case, such as in medicine or healthcare, where it is necessary to define your own criteria.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-llm-evals"&gt;GEval:&lt;/a&gt; a framework that uses LLMs with chain-of-thoughts (CoT) to evaluate LLM outputs based on ANY custom criteria.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-dag"&gt;DAG (Directed Acyclic Graphs):&lt;/a&gt; the most versatile custom metric for you to easily build deterministic decision trees for evaluation with the help of using LLM-as-a-judge&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Red-teaming metrics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There are hundreds of red-teaming metrics available, but bias, toxicity, and hallucination are among the most common. These metrics are particularly valuable for detecting harmful outputs and ensuring that the model maintains high standards of safety and reliability.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-bias"&gt;Bias&lt;/a&gt;: determines whether your LLM output contains gender, racial, or political bias.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-toxicity"&gt;Toxicity&lt;/a&gt;: evaluates toxicity in your LLM outputs.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-hallucination"&gt;Hallucination&lt;/a&gt;: determines whether your LLM generates factually correct information by comparing the output to the provided context&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Although this is quite lengthy, and a good starting place, it is by no means comprehensive. Besides this there are other categories of metrics like multimodal metrics, which can range from image quality metrics like image coherence to multimodal RAG metrics like multimodal contextual precision or recall. &lt;/p&gt; &lt;p&gt;For a more comprehensive list + calculations, you might want to visit &lt;a href="https://docs.confident-ai.com/"&gt;deepeval docs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/confident-ai/deepeval"&gt;Github Repo&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FlimsyProperty8544"&gt; /u/FlimsyProperty8544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j85q5m/every_llm_metric_you_need_to_know/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j85q5m/every_llm_metric_you_need_to_know/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j85q5m/every_llm_metric_you_need_to_know/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T18:31:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j82o15</id>
    <title>Fixed Ollama template for Mistral Small 3</title>
    <updated>2025-03-10T16:26:52+00:00</updated>
    <author>
      <name>/u/logkn</name>
      <uri>https://old.reddit.com/user/logkn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was finding that Mistral Small 3 on Ollama (mistral-small:24b) had some trouble calling tools -- mainly, adding or dropping tokens that rendered the tool call as message content rather than an actual tool call.&lt;br /&gt; The chat template on the model's Huggingface page was actually not very helpful because it doesn't even include tool calling. I dug around a bit to find the Tekken V7 tokenizer, and sure enough the chat template for providing and calling tools didn't match up with Ollama's.&lt;/p&gt; &lt;p&gt;Here's a fixed version, and it's MUCH more consistent with tool calling:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{{- range $index, $_ := .Messages }} {{- if eq .Role &amp;quot;system&amp;quot; }}[SYSTEM_PROMPT]{{ .Content }}[/SYSTEM_PROMPT] {{- else if eq .Role &amp;quot;user&amp;quot; }} {{- if and (le (len (slice $.Messages $index)) 2) $.Tools }}[AVAILABLE_TOOLS]{{ $.Tools }}[/AVAILABLE_TOOLS] {{- end }}[INST]{{ .Content }}[/INST] {{- else if eq .Role &amp;quot;assistant&amp;quot; }} {{- if .Content }}{{ .Content }} {{- if not (eq (len (slice $.Messages $index)) 1) }}&amp;lt;/s&amp;gt; {{- end }} {{- else if .ToolCalls }}[TOOL_CALLS] [ {{- range .ToolCalls }}{&amp;quot;name&amp;quot;: &amp;quot;{{ .Function.Name }}&amp;quot;, &amp;quot;arguments&amp;quot;: {{ .Function.Arguments }}} {{- end }}]&amp;lt;/s&amp;gt; {{- end }} {{- else if eq .Role &amp;quot;tool&amp;quot; }}[TOOL_RESULTS] [TOOL_CONTENT] {{ .Content }}[/TOOL_RESULTS] {{- end }} {{- end }} &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/logkn"&gt; /u/logkn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j82o15/fixed_ollama_template_for_mistral_small_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j82o15/fixed_ollama_template_for_mistral_small_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j82o15/fixed_ollama_template_for_mistral_small_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T16:26:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7vwcr</id>
    <title>Deepseek coder v2</title>
    <updated>2025-03-10T10:53:20+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got this model last night, for a 7B it is soooo good at web coding!!!&lt;/p&gt; &lt;p&gt;I have made a working calculator, pong, and flappy bird.&lt;/p&gt; &lt;p&gt;I'm using the lite model by lmstudio. best of all I'm getting 16 tps on my ryzen!!!&lt;/p&gt; &lt;p&gt;using this model in particular &lt;a href="https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF"&gt;https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7vwcr/deepseek_coder_v2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7vwcr/deepseek_coder_v2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7vwcr/deepseek_coder_v2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T10:53:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1j83md3</id>
    <title>Could GEMMA-3 Be Unveiled at GDC 2025 (March 18)?</title>
    <updated>2025-03-10T17:05:48+00:00</updated>
    <author>
      <name>/u/hCKstp4BtL</name>
      <uri>https://old.reddit.com/user/hCKstp4BtL</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://schedule.gdconf.com/session/beyond-the-hype-real-world-applications-of-google-ai-in-gaming-presented-by-google-play/911129"&gt;https://schedule.gdconf.com/session/beyond-the-hype-real-world-applications-of-google-ai-in-gaming-presented-by-google-play/911129&lt;/a&gt;&lt;/p&gt; &lt;p&gt;in this session description, we can read that they will talk about &amp;quot;Gemma models&amp;quot; (among other things). I think everyone knows about &amp;quot;Gemma 2&amp;quot; and there is no need to mention it because everyone knows how it works, right? Bigger chance is that they will show &amp;quot;Gemma 3&amp;quot; and they will release it shorly? because it seems to me that the deadline of May 20-21 (Google I/O) is a bit too late.&lt;/p&gt; &lt;p&gt;It looks like Google wants to focus the eyes of game developers on Gemma, so that they can combine the models with their games to create: ‚Äúnew AI-based game features and mechanics.‚Äù&lt;/p&gt; &lt;p&gt;... and to make it work, I think such a &amp;quot;Gemma 3&amp;quot; model should be prioritize with &amp;quot;perfect JSON generation&amp;quot; for the interface model&amp;lt;-&amp;gt;game and also improved instruction following.&lt;/p&gt; &lt;p&gt;I waiting for a small model (7b-9b) to be good enough to make a game with llm controlling npc (not only talk).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hCKstp4BtL"&gt; /u/hCKstp4BtL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83md3/could_gemma3_be_unveiled_at_gdc_2025_march_18/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83md3/could_gemma3_be_unveiled_at_gdc_2025_march_18/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j83md3/could_gemma3_be_unveiled_at_gdc_2025_march_18/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T17:05:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7s1ef</id>
    <title>Why Isn't There a Real-Time AI Translation App for Smartphones Yet?</title>
    <updated>2025-03-10T06:03:28+00:00</updated>
    <author>
      <name>/u/spbxspb</name>
      <uri>https://old.reddit.com/user/spbxspb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With all the advancements in AI, especially in language models and real-time processing, why don‚Äôt we have a truly seamless AI-powered translation app for smartphones? Something that works offline, translates speech in real-time with minimal delay, and supports multiple languages fluently.&lt;/p&gt; &lt;p&gt;Most current apps either require an internet connection, have significant lag, or struggle with natural-sounding translations. Given how powerful AI has become, it feels like we should already have a Star Trek-style universal translator by now.&lt;/p&gt; &lt;p&gt;Is it a technical limitation, a business decision, or something else?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spbxspb"&gt; /u/spbxspb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7s1ef/why_isnt_there_a_realtime_ai_translation_app_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7s1ef/why_isnt_there_a_realtime_ai_translation_app_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7s1ef/why_isnt_there_a_realtime_ai_translation_app_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T06:03:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7j6cg</id>
    <title>&lt;70B models aren't ready to solo codebases yet, but we're gaining momentum and fast</title>
    <updated>2025-03-09T22:11:41+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j6cg/70b_models_arent_ready_to_solo_codebases_yet_but/"&gt; &lt;img alt="&amp;lt;70B models aren't ready to solo codebases yet, but we're gaining momentum and fast" src="https://external-preview.redd.it/ZDFiNmN0NHptcW5lMdBuqabr-hQLmYC8Qi5X9EdtbTx_2YZ-hAZhcsR_hrB1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2014a6f010867f98da226a97f756cd7d035b3cb" title="&amp;lt;70B models aren't ready to solo codebases yet, but we're gaining momentum and fast" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2wo0b8lqmqne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j6cg/70b_models_arent_ready_to_solo_codebases_yet_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j6cg/70b_models_arent_ready_to_solo_codebases_yet_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T22:11:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7uqv4</id>
    <title>v0.6.0 Update: Dive - An Open Source MCP Agent Desktop</title>
    <updated>2025-03-10T09:29:10+00:00</updated>
    <author>
      <name>/u/BigGo_official</name>
      <uri>https://old.reddit.com/user/BigGo_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7uqv4/v060_update_dive_an_open_source_mcp_agent_desktop/"&gt; &lt;img alt="v0.6.0 Update: Dive - An Open Source MCP Agent Desktop" src="https://external-preview.redd.it/emVhYWcxajZ6dG5lMatJSspljsEqCKIUqwl7TvTa14fhRRDdCNE6VsWU_1B_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d328715d2f1b9c124872513a6e543c4697240cf6" title="v0.6.0 Update: Dive - An Open Source MCP Agent Desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BigGo_official"&gt; /u/BigGo_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/e11wp2j6ztne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7uqv4/v060_update_dive_an_open_source_mcp_agent_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7uqv4/v060_update_dive_an_open_source_mcp_agent_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T09:29:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7vx6z</id>
    <title>Open manus</title>
    <updated>2025-03-10T10:55:02+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/mannaandpoem/OpenManus"&gt;https://github.com/mannaandpoem/OpenManus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone got any views on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7vx6z/open_manus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7vx6z/open_manus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7vx6z/open_manus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T10:55:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7n2s5</id>
    <title>Manus turns out to be just Claude Sonnet + 29 other tools, Reflection 70B vibes ngl</title>
    <updated>2025-03-10T01:18:27+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"&gt; &lt;img alt="Manus turns out to be just Claude Sonnet + 29 other tools, Reflection 70B vibes ngl" src="https://b.thumbs.redditmedia.com/aZROr3LtwGC89EWOHjMHoIbCvPQTB8Fs1jwIfYjEb8U.jpg" title="Manus turns out to be just Claude Sonnet + 29 other tools, Reflection 70B vibes ngl" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yi72dtz4krne1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f8e56eaed3a6e5eee634567caa3f64f1fbcc2f1"&gt;https://preview.redd.it/yi72dtz4krne1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f8e56eaed3a6e5eee634567caa3f64f1fbcc2f1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/Dorialexander/status/1898719861284454718"&gt;https://x.com/Dorialexander/status/1898719861284454718&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6b84agl7krne1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ce944225e1c92f7d0d466c896fdf2a80c667837"&gt;https://preview.redd.it/6b84agl7krne1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ce944225e1c92f7d0d466c896fdf2a80c667837&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/jianxliao/status/1898861051183349870"&gt;https://x.com/jianxliao/status/1898861051183349870&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T01:18:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7zzdt</id>
    <title>All about LLMs</title>
    <updated>2025-03-10T14:32:28+00:00</updated>
    <author>
      <name>/u/meme_watcher69420</name>
      <uri>https://old.reddit.com/user/meme_watcher69420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was given an offer to join this startup. They were impressed with my &amp;quot;knowledge&amp;quot; about AI and LLMs. But in reality, all my projects are made by pasting stuff from Claude, stackoverflow and improved with reading a few documents.&lt;/p&gt; &lt;p&gt;How do I get to know everything about setting up LLMs, integrating them into an application and deploying them? Is there a guide or a roadmap to it? I'll join this startup in a month so I got a bit of time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/meme_watcher69420"&gt; /u/meme_watcher69420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7zzdt/all_about_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7zzdt/all_about_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7zzdt/all_about_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T14:32:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j85snw</id>
    <title>[Experimental] Control the 'Thinking Effort' of QwQ &amp; R1 Models with a Custom Logits Processor</title>
    <updated>2025-03-10T18:34:42+00:00</updated>
    <author>
      <name>/u/ASL_Dev</name>
      <uri>https://old.reddit.com/user/ASL_Dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed several posts lately discussing how the QwQ model tends to produce an excessive amount of tokens, often leading it to &amp;quot;overthink&amp;quot; unnecessarily. I've also seen some creative attempts to control this behavior using carefully crafted system prompts.&lt;/p&gt; &lt;p&gt;To help address this issue more systematically, I've put together a small and simple solution using a custom &lt;strong&gt;logits processor&lt;/strong&gt;. This approach dynamically adjusts the likelihood of the end-of-thinking token (&lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt;) appearing during generation.&lt;/p&gt; &lt;p&gt;The basic idea:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can set a &amp;quot;thinking effort&amp;quot; parameter (&lt;code&gt;0.0&lt;/code&gt; = minimal thinking, token &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; quickly appears; &lt;code&gt;1.0&lt;/code&gt; = normal behavior, &amp;gt;&lt;code&gt;1.0&lt;/code&gt; = it takes longer to output the &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; token).&lt;/li&gt; &lt;li&gt;The logic is straightforward: once the &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; token has been generated, the processor stops adjusting logits for that sequence.&lt;/li&gt; &lt;li&gt;This allows controlling how much the model thinks (or ‚Äúoverthinks‚Äù) without complicated prompt engineering.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've seen good results in reducing unnecessary thinking tokens in simple tasks, but I haven't yet extensively tested how this might influence longer chain-of-thought (CoT) reasoning.&lt;/p&gt; &lt;p&gt;I'd love for others to try it out and share your experiences or thoughts!&lt;/p&gt; &lt;p&gt;Here‚Äôs the repo with code and examples for both llama-cpp-python (gguf models) and Hugging Face Transformers (&lt;strong&gt;Note:&lt;/strong&gt; The code is still very raw, not optimized, and not organized lol... ‚Äîthis is just to share the basic idea quickly with the community!):&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/and270/thinking_effort_processor"&gt;https://github.com/and270/thinking_effort_processor&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Quick Example (Qwen-1.5B R1-Distill)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; What is the capital of France?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Regular Inference:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Okay, so I need to figure out what the capital of France is. I've heard a few things before, but I'm not entirely sure. Let me start by recalling what I know about France. France is a country in Europe, known for its diverse landscapes and vibrant culture. The name &amp;quot;France&amp;quot; itself comes from the French word &amp;quot;fran√ßais,&amp;quot; which means &amp;quot;french&amp;quot; or &amp;quot;colorful.&amp;quot; I think the capital is a significant city, maybe something like Paris or maybe another city...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;(The model generates a lengthy reasoning sequence before concluding)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;...To summarize, I believe the capital of France is Paris.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Thinking Effort Inference (0.1):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;The capital of France is Paris.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Any feedback or tests are very welcome!&lt;/p&gt; &lt;p&gt;Let me know your thoughts or experiences‚ÄîI'm especially curious how this affects your use-cases with the QwQ or similar models. üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ASL_Dev"&gt; /u/ASL_Dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j85snw/experimental_control_the_thinking_effort_of_qwq/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j85snw/experimental_control_the_thinking_effort_of_qwq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j85snw/experimental_control_the_thinking_effort_of_qwq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T18:34:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7usrm</id>
    <title>EuroBERT: A High-Performance Multilingual Encoder Model</title>
    <updated>2025-03-10T09:33:09+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7usrm/eurobert_a_highperformance_multilingual_encoder/"&gt; &lt;img alt="EuroBERT: A High-Performance Multilingual Encoder Model" src="https://external-preview.redd.it/CnsI7xgYaguHri1_uGabuT9boB9PVspWCoeHvZsE1IM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f747438c0575ca5fca91283bb815527cfb8627a" title="EuroBERT: A High-Performance Multilingual Encoder Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/EuroBERT/release"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7usrm/eurobert_a_highperformance_multilingual_encoder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7usrm/eurobert_a_highperformance_multilingual_encoder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T09:33:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j83zkt</id>
    <title>Don't underestimate the power of RAG</title>
    <updated>2025-03-10T17:21:09+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83zkt/dont_underestimate_the_power_of_rag/"&gt; &lt;img alt="Don't underestimate the power of RAG" src="https://preview.redd.it/moz1h1pzbwne1.gif?width=640&amp;amp;crop=smart&amp;amp;s=adc13823a2909ba4af349f77c5405bf1ce990c2e" title="Don't underestimate the power of RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/moz1h1pzbwne1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83zkt/dont_underestimate_the_power_of_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j83zkt/dont_underestimate_the_power_of_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T17:21:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7t18m</id>
    <title>Framework and DIGITS suddenly seem underwhelming compared to the 512GB Unified Memory on the new Mac.</title>
    <updated>2025-03-10T07:15:44+00:00</updated>
    <author>
      <name>/u/Common_Ad6166</name>
      <uri>https://old.reddit.com/user/Common_Ad6166</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was holding out on purchasing a FrameWork desktop until we could see what kind of performance the DIGITS would get when it comes out in May. But now that Apple has announced the new M4 Max/ M3 Ultra Mac's with 512 GB Unified memory, the 128 GB options on the other two seem paltry in comparison. &lt;/p&gt; &lt;p&gt;Are we actually going to be locked into the Apple ecosystem for another decade? This can't be true!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Common_Ad6166"&gt; /u/Common_Ad6166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7t18m/framework_and_digits_suddenly_seem_underwhelming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7t18m/framework_and_digits_suddenly_seem_underwhelming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7t18m/framework_and_digits_suddenly_seem_underwhelming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T07:15:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j80hbo</id>
    <title>Hunyuan-TurboS.</title>
    <updated>2025-03-10T14:54:37+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://twitter.com/TXhunyuan/status/1899105803073958010"&gt;https://twitter.com/TXhunyuan/status/1899105803073958010&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j80hbo/hunyuanturbos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j80hbo/hunyuanturbos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j80hbo/hunyuanturbos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T14:54:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8554a</id>
    <title>Qwen QwQ-32B joins DeepSeek R1 and Claude Sonnets at the top of the Creative Story-Writing Benchmark</title>
    <updated>2025-03-10T18:08:27+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8554a/qwen_qwq32b_joins_deepseek_r1_and_claude_sonnets/"&gt; &lt;img alt="Qwen QwQ-32B joins DeepSeek R1 and Claude Sonnets at the top of the Creative Story-Writing Benchmark" src="https://b.thumbs.redditmedia.com/Yaa0ATPdfMfRcdIPRl3zAR-18YhojdxTeqLYJXaDdUk.jpg" title="Qwen QwQ-32B joins DeepSeek R1 and Claude Sonnets at the top of the Creative Story-Writing Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j8554a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8554a/qwen_qwq32b_joins_deepseek_r1_and_claude_sonnets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8554a/qwen_qwq32b_joins_deepseek_r1_and_claude_sonnets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T18:08:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8766b</id>
    <title>New rig who dis</title>
    <updated>2025-03-10T19:31:29+00:00</updated>
    <author>
      <name>/u/MotorcyclesAndBizniz</name>
      <uri>https://old.reddit.com/user/MotorcyclesAndBizniz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8766b/new_rig_who_dis/"&gt; &lt;img alt="New rig who dis" src="https://b.thumbs.redditmedia.com/0XSP2n-GAI5n3Op8qnPsulZZgY7u_Dk_E6IZd3L-Ixg.jpg" title="New rig who dis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPU: 6x 3090 FE via 6x PCIe 4.0 x4 Oculink&lt;br /&gt; CPU: AMD 7950x3D&lt;br /&gt; MoBo: B650M WiFi&lt;br /&gt; RAM: 192GB DDR5 @ 4800MHz&lt;br /&gt; NIC: 10Gbe&lt;br /&gt; NVMe: Samsung 980 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MotorcyclesAndBizniz"&gt; /u/MotorcyclesAndBizniz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j8766b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8766b/new_rig_who_dis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8766b/new_rig_who_dis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T19:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j84c79</id>
    <title>Qwen QwQ-32B is the LLM most frequently voted out first by its peers in the Elimination Game Benchmark, resulting in poor overall performance</title>
    <updated>2025-03-10T17:35:19+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j84c79/qwen_qwq32b_is_the_llm_most_frequently_voted_out/"&gt; &lt;img alt="Qwen QwQ-32B is the LLM most frequently voted out first by its peers in the Elimination Game Benchmark, resulting in poor overall performance" src="https://b.thumbs.redditmedia.com/GITV-BcVRUV84azdQvU9AjF2LmByzEd0hc-J34tPTRc.jpg" title="Qwen QwQ-32B is the LLM most frequently voted out first by its peers in the Elimination Game Benchmark, resulting in poor overall performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j84c79"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j84c79/qwen_qwq32b_is_the_llm_most_frequently_voted_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j84c79/qwen_qwq32b_is_the_llm_most_frequently_voted_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T17:35:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j83imv</id>
    <title>We tested open and closed models for embodied decision alignment, and we found Qwen 2.5 VL is surprisingly stronger than most closed frontier models.</title>
    <updated>2025-03-10T17:01:38+00:00</updated>
    <author>
      <name>/u/ProKil_Chu</name>
      <uri>https://old.reddit.com/user/ProKil_Chu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"&gt; &lt;img alt="We tested open and closed models for embodied decision alignment, and we found Qwen 2.5 VL is surprisingly stronger than most closed frontier models." src="https://external-preview.redd.it/j6U6CbNbZYwJeWKzZ2YpYx2JnlAn2s-v4-SyaPJ7zcM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7b71aa459f95dab63628bf7dd6ae70b7382ecb25" title="We tested open and closed models for embodied decision alignment, and we found Qwen 2.5 VL is surprisingly stronger than most closed frontier models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1j83imv/video/t190t6fsewne1/player"&gt;https://reddit.com/link/1j83imv/video/t190t6fsewne1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One thing that surprised us during benchmarking with EgoNormia is that Qwen 2.5 VL is indeed a very strong model for vision which rivals Gemini 1.5/2.0, better than GPT-4o and Claude 3.5 Sonnet.&lt;/p&gt; &lt;p&gt;Tweet: &lt;a href="https://x.com/_Hao_Zhu/status/1899151181534134648"&gt;https://x.com/_Hao_Zhu/status/1899151181534134648&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Leaderboard: &lt;a href="https://egonormia.org"&gt;https://egonormia.org&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Eval code: &lt;a href="https://github.com/Open-Social-World/EgoNormia"&gt;https://github.com/Open-Social-World/EgoNormia&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ProKil_Chu"&gt; /u/ProKil_Chu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T17:01:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7r47l</id>
    <title>I just made an animation of a ball bouncing inside a spinning hexagon</title>
    <updated>2025-03-10T05:01:09+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/"&gt; &lt;img alt="I just made an animation of a ball bouncing inside a spinning hexagon" src="https://external-preview.redd.it/aHcybDc4eW5tc25lMWpXkBeJA0bkbXxKyNPWYhDqX6Z4Wwq4cQiczMXRiEBU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1910662e66472f313e9a9c19401be8a1be2f181a" title="I just made an animation of a ball bouncing inside a spinning hexagon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cy79860omsne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T05:01:09+00:00</published>
  </entry>
</feed>
