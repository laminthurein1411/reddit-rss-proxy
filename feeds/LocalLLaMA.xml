<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-11T14:06:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jwq6k2</id>
    <title>Newbie question: can there be loras for tts?</title>
    <updated>2025-04-11T13:42:18+00:00</updated>
    <author>
      <name>/u/Silver-Champion-4846</name>
      <uri>https://old.reddit.com/user/Silver-Champion-4846</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all. I'm not a coder yet nor do I know the vagueries of how lora works, i.e: if it can only apply to language models or transformers in general. Can you help answer this question? If I hypothetically have the knowledge, can I make a lora for a specific voice or language? Or is that not how it works and I'm just doing the equivalent of saying can I eat fire? Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silver-Champion-4846"&gt; /u/Silver-Champion-4846 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwq6k2/newbie_question_can_there_be_loras_for_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwq6k2/newbie_question_can_there_be_loras_for_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jwq6k2/newbie_question_can_there_be_loras_for_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-11T13:42:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw1n27</id>
    <title>Introducing ZR1-1.5B, a small but powerful reasoning model for math and code</title>
    <updated>2025-04-10T16:08:44+00:00</updated>
    <author>
      <name>/u/retrolione</name>
      <uri>https://old.reddit.com/user/retrolione</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/retrolione"&gt; /u/retrolione &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.zyphra.com/post/introducing-zr1-1-5b-a-small-but-powerful-math-code-reasoning-model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1n27/introducing_zr115b_a_small_but_powerful_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1n27/introducing_zr115b_a_small_but_powerful_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T16:08:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jwkc8v</id>
    <title>Continual Knowledge Circuits</title>
    <updated>2025-04-11T07:41:26+00:00</updated>
    <author>
      <name>/u/itchykittehs</name>
      <uri>https://old.reddit.com/user/itchykittehs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/zjunlp/dynamicknowledgecircuits"&gt;https://github.com/zjunlp/dynamicknowledgecircuits&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone played with Knowledge Circuits? This one seems crazy, am I right in understanding that it is continually training the model as it consume knowledge?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/itchykittehs"&gt; /u/itchykittehs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwkc8v/continual_knowledge_circuits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwkc8v/continual_knowledge_circuits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jwkc8v/continual_knowledge_circuits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-11T07:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw9cn3</id>
    <title>Fiction.liveBench: new Grok 3 scores are solid, llama 4 scores improved after vllm fixes</title>
    <updated>2025-04-10T21:29:24+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw9cn3/fictionlivebench_new_grok_3_scores_are_solid/"&gt; &lt;img alt="Fiction.liveBench: new Grok 3 scores are solid, llama 4 scores improved after vllm fixes" src="https://preview.redd.it/6tbbef5js2ue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=660231214b036de1e98f805d80a58cd827280c9f" title="Fiction.liveBench: new Grok 3 scores are solid, llama 4 scores improved after vllm fixes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6tbbef5js2ue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw9cn3/fictionlivebench_new_grok_3_scores_are_solid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw9cn3/fictionlivebench_new_grok_3_scores_are_solid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T21:29:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvs66w</id>
    <title>Qwen Dev: Qwen3 not gonna release "in hours", still need more time</title>
    <updated>2025-04-10T07:29:02+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvs66w/qwen_dev_qwen3_not_gonna_release_in_hours_still/"&gt; &lt;img alt="Qwen Dev: Qwen3 not gonna release &amp;quot;in hours&amp;quot;, still need more time" src="https://preview.redd.it/3kcfx9xnmyte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acd7d81fbe37029fcaaed94d7eab854f4dea3442" title="Qwen Dev: Qwen3 not gonna release &amp;quot;in hours&amp;quot;, still need more time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3kcfx9xnmyte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvs66w/qwen_dev_qwen3_not_gonna_release_in_hours_still/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvs66w/qwen_dev_qwen3_not_gonna_release_in_hours_still/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T07:29:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw0c2i</id>
    <title>Llama 4 Maverick scores on seven independent benchmarks</title>
    <updated>2025-04-10T15:13:37+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw0c2i/llama_4_maverick_scores_on_seven_independent/"&gt; &lt;img alt="Llama 4 Maverick scores on seven independent benchmarks" src="https://b.thumbs.redditmedia.com/_L8mk4hZtxyzWyaFtsc3lifBL2BFQ1j3lLTdiCYP1ho.jpg" title="Llama 4 Maverick scores on seven independent benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/lechmazur/nyt-connections/"&gt;Extended NYT Connections&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/writing/"&gt;Creative Short Story Writing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/confabulations/"&gt;Confabulations/Hallucinations&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/generalization/"&gt;Thematic Generalization&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/elimination_game/"&gt;Elimination Game&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/step_game/"&gt;Step Race Benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/pgg_bench/"&gt;Public Goods Game&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jw0c2i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw0c2i/llama_4_maverick_scores_on_seven_independent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw0c2i/llama_4_maverick_scores_on_seven_independent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T15:13:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw6cdk</id>
    <title>Openai New Memory feature is just Vector Search?</title>
    <updated>2025-04-10T19:23:39+00:00</updated>
    <author>
      <name>/u/AryanEmbered</name>
      <uri>https://old.reddit.com/user/AryanEmbered</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't get what's the big deal about this?&lt;/p&gt; &lt;p&gt;they are simply creating the embeddings for past chats and doing a vector search and adding chunks to context for every prompt right?&lt;/p&gt; &lt;p&gt;I've (we've) made this stuff 3 years ago, I don't get it, what am I missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AryanEmbered"&gt; /u/AryanEmbered &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw6cdk/openai_new_memory_feature_is_just_vector_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw6cdk/openai_new_memory_feature_is_just_vector_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw6cdk/openai_new_memory_feature_is_just_vector_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T19:23:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw91nh</id>
    <title>Orpheus TTS released multilingual support</title>
    <updated>2025-04-10T21:16:12+00:00</updated>
    <author>
      <name>/u/YearnMar10</name>
      <uri>https://old.reddit.com/user/YearnMar10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I couldn‚Äôt find a thread on this here so far.&lt;/p&gt; &lt;p&gt;CanopyAI released new models for their Orpheus TTS model for different languages.&lt;/p&gt; &lt;p&gt;LANGUAGE(S) - French - German - Mandarin - Korean - Hindi - Spanish + Italian&lt;/p&gt; &lt;p&gt;More info here: &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;https://github.com/canopyai/Orpheus-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here: &lt;a href="https://huggingface.co/collections/canopylabs/orpheus-multilingual-research-release-67f5894cd16794db163786ba"&gt;https://huggingface.co/collections/canopylabs/orpheus-multilingual-research-release-67f5894cd16794db163786ba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here: &lt;a href="https://canopylabs.ai/releases/orpheus_can_speak_any_language"&gt;https://canopylabs.ai/releases/orpheus_can_speak_any_language&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They also released a training guide, and there are already some finetunes floating around on HF and the first gguf versions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YearnMar10"&gt; /u/YearnMar10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw91nh/orpheus_tts_released_multilingual_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw91nh/orpheus_tts_released_multilingual_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw91nh/orpheus_tts_released_multilingual_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T21:16:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw2tbk</id>
    <title>So, Quasar Alpha might actually be OpenAI's model</title>
    <updated>2025-04-10T16:58:12+00:00</updated>
    <author>
      <name>/u/-Cacique</name>
      <uri>https://old.reddit.com/user/-Cacique</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw2tbk/so_quasar_alpha_might_actually_be_openais_model/"&gt; &lt;img alt="So, Quasar Alpha might actually be OpenAI's model" src="https://preview.redd.it/3xztmaoxf1ue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0979fac46ea6a31825df6db93efc19bf9cce1a9b" title="So, Quasar Alpha might actually be OpenAI's model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cacique"&gt; /u/-Cacique &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3xztmaoxf1ue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw2tbk/so_quasar_alpha_might_actually_be_openais_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw2tbk/so_quasar_alpha_might_actually_be_openais_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T16:58:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw1l9n</id>
    <title>ByteDance just released the technical report for Seed-Thinking-v1.5</title>
    <updated>2025-04-10T16:06:36+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1l9n/bytedance_just_released_the_technical_report_for/"&gt; &lt;img alt="ByteDance just released the technical report for Seed-Thinking-v1.5" src="https://preview.redd.it/zok3h2gu61ue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82891e270a5455a5741a8218234c99a25e56cc02" title="ByteDance just released the technical report for Seed-Thinking-v1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ByteDance just released the technical report for Seed-Thinking-v1.5, which is also an inference model trained using reinforcement learning. Based on the scores, it outperforms DeepSeek-R1 and is at a level close to Gemini-2.5-Pro and O3-mini-high. &lt;/p&gt; &lt;p&gt;However, I've searched everywhere and haven't found where the model is. I'm uncertain if they will release the weights. Once it's released, I will test it immediately.&lt;/p&gt; &lt;p&gt;Technical report link: &lt;a href="https://github.com/ByteDance-Seed/Seed-Thinking-v1.5"&gt;https://github.com/ByteDance-Seed/Seed-Thinking-v1.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zok3h2gu61ue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1l9n/bytedance_just_released_the_technical_report_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1l9n/bytedance_just_released_the_technical_report_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T16:06:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw1e6b</id>
    <title>Can we all agree that Qwen has the best LLM mascot? (not at all trying to suck up so they‚Äôll drop Qwen3 today)</title>
    <updated>2025-04-10T15:58:22+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1e6b/can_we_all_agree_that_qwen_has_the_best_llm/"&gt; &lt;img alt="Can we all agree that Qwen has the best LLM mascot? (not at all trying to suck up so they‚Äôll drop Qwen3 today)" src="https://b.thumbs.redditmedia.com/ZuvbVVx37QGz7TIEX4U5keZjwMgFyohptWI2tF28Stc.jpg" title="Can we all agree that Qwen has the best LLM mascot? (not at all trying to suck up so they‚Äôll drop Qwen3 today)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jw1e6b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1e6b/can_we_all_agree_that_qwen_has_the_best_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1e6b/can_we_all_agree_that_qwen_has_the_best_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T15:58:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jwllvz</id>
    <title>Open LLM leaderboard is archived, what are the alternatives?</title>
    <updated>2025-04-11T09:18:18+00:00</updated>
    <author>
      <name>/u/Initial_Track6190</name>
      <uri>https://old.reddit.com/user/Initial_Track6190</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want a leaderboard for open-source models; the last one, Open LLM Leaderboard, is now archived. What do you use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Initial_Track6190"&gt; /u/Initial_Track6190 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwllvz/open_llm_leaderboard_is_archived_what_are_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwllvz/open_llm_leaderboard_is_archived_what_are_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jwllvz/open_llm_leaderboard_is_archived_what_are_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-11T09:18:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jwhvnv</id>
    <title>Arch-Function-Chat Trending #1 on HuggingFace!</title>
    <updated>2025-04-11T04:49:20+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwhvnv/archfunctionchat_trending_1_on_huggingface/"&gt; &lt;img alt="Arch-Function-Chat Trending #1 on HuggingFace!" src="https://preview.redd.it/aps11mcty4ue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a57faea18fe56dbe618d675c781667e6d76fcf25" title="Arch-Function-Chat Trending #1 on HuggingFace!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So thrilled to share that the work we build with the community here has such a large impact. Just wanted to say thanks. And I'll leave the links in the comments if someone wants to explore further. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aps11mcty4ue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwhvnv/archfunctionchat_trending_1_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jwhvnv/archfunctionchat_trending_1_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-11T04:49:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw9fba</id>
    <title>Macbook Pro M4 Max inference speeds</title>
    <updated>2025-04-10T21:32:31+00:00</updated>
    <author>
      <name>/u/SufficientRadio</name>
      <uri>https://old.reddit.com/user/SufficientRadio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw9fba/macbook_pro_m4_max_inference_speeds/"&gt; &lt;img alt="Macbook Pro M4 Max inference speeds" src="https://preview.redd.it/bms6abl5s2ue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fa3ff737f60bcdc49e3d199033b76539d0bc294d" title="Macbook Pro M4 Max inference speeds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had trouble finding this kind of information when I was deciding on what Macbook to buy so putting this out there to help future purchase decisions:&lt;/p&gt; &lt;p&gt;Macbook Pro 16&amp;quot; M4 Max 36gb 14‚Äëcore CPU, 32‚Äëcore GPU, 16‚Äëcore Neural&lt;/p&gt; &lt;p&gt;During inference, cpu/gpu temps get up to 103C and power draw is about 130W.&lt;/p&gt; &lt;p&gt;36gb ram allows me to comfortably load these models and still use my computer as usual (browsers, etc) without having to close every window. However, I do no need to close programs like Lightroom and Photoshop to make room.&lt;/p&gt; &lt;p&gt;Finally, the nano texture glass is worth it...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SufficientRadio"&gt; /u/SufficientRadio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bms6abl5s2ue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw9fba/macbook_pro_m4_max_inference_speeds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw9fba/macbook_pro_m4_max_inference_speeds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T21:32:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jwormp</id>
    <title>Deconstructing agentic AI prompts: some patterns I noticed</title>
    <updated>2025-04-11T12:34:54+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwormp/deconstructing_agentic_ai_prompts_some_patterns_i/"&gt; &lt;img alt="Deconstructing agentic AI prompts: some patterns I noticed" src="https://external-preview.redd.it/azdsOGd4aXU5N3VlMS-DOok8VecI4VBh-SaZNHm4Aspcxmsyk9I5WC2oHNIS.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75b1e5a7509a453b428b11dc761e3685731e5d7a" title="Deconstructing agentic AI prompts: some patterns I noticed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spending some time digging into the system prompts behind agents like v0, Manus, ChatGPT 4o, (...).&lt;/p&gt; &lt;p&gt;It's pretty interesting seeing the common threads emerge ‚Äì how they define the agent's role, structure complex instructions, handle tool use (often very explicitly), encourage step-by-step planning, and bake in safety rules. Seems like a kind of 'convergent evolution' in prompt design for getting these things to actually work reliably.&lt;/p&gt; &lt;p&gt;Wrote up a more detailed breakdown with examples from the repo if anyone's interested in this stuff:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Fdontriskit%2Fawesome-ai-system-prompts"&gt;awesome-ai-system-prompts&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Might be useful if you're building agents or just curious about the 'ghost in the machine'. Curious what patterns others are finding indispensable?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5g15kxiu97ue1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwormp/deconstructing_agentic_ai_prompts_some_patterns_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jwormp/deconstructing_agentic_ai_prompts_some_patterns_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-11T12:34:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jwcbfm</id>
    <title>Mistral hasn't released a big model in ages.</title>
    <updated>2025-04-10T23:46:41+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How about a new version of MoE that can put the LLama4 to shame? Hopefully something with less than 120B params total.&lt;/p&gt; &lt;p&gt;Or a new version of Mistral large. Or a Mistral Medium (30-40B range)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwcbfm/mistral_hasnt_released_a_big_model_in_ages/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwcbfm/mistral_hasnt_released_a_big_model_in_ages/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jwcbfm/mistral_hasnt_released_a_big_model_in_ages/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T23:46:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jwlvjs</id>
    <title>Paper page - OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens</title>
    <updated>2025-04-11T09:38:02+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwlvjs/paper_page_olmotrace_tracing_language_model/"&gt; &lt;img alt="Paper page - OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens" src="https://external-preview.redd.it/VVxJB7KWWo4CLRWMs0X6vQWrqVzjSQnYrxGfyVikjbM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d8585c02115148b50c8aa1af8e6bbf364cb541b1" title="Paper page - OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2504.07096"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwlvjs/paper_page_olmotrace_tracing_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jwlvjs/paper_page_olmotrace_tracing_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-11T09:38:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jwhp26</id>
    <title>DeepCoder 14B vs Qwen2.5 Coder 32B vs QwQ 32B</title>
    <updated>2025-04-11T04:37:27+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I ran a quick test to compare the coding ability between the 3 models that was known for good coding performance:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;DeepCoder 14B&lt;/li&gt; &lt;li&gt;Qwen2.5 Coder 32B&lt;/li&gt; &lt;li&gt;QwQ 32B&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here's the prompt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;use HTML5 canvas, create a bouncing ball in a hexagon demo, there‚Äôs a hexagon shape, and a ball inside it, the hexagon will slowly rotate clockwise, under the physic effect, the ball will fall down and bounce when it hit the edge of the hexagon. also, add a button to reset the game as well. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;All models are given just one shot to try, no follow up asking. And in the end, I also test with o3-mini to see which one has a closer result.&lt;/p&gt; &lt;p&gt;First, this is what o3-mini implemented:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1jwhp26/video/lvi4eug9o4ue1/player"&gt;https://reddit.com/link/1jwhp26/video/lvi4eug9o4ue1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is how DeepCoder 14B do it, pretty close, but it's not working, it also implemented the Reset button wrong (click on it will make the hexagon rotate faster üòí, not reset the game).&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1jwhp26/video/2efz73ztp4ue1/player"&gt;https://reddit.com/link/1jwhp26/video/2efz73ztp4ue1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen2.5 Coder 32B was able to implement the Reset button right, and the ball are moving, but not bouncing.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1jwhp26/video/jiai2kgjs4ue1/player"&gt;https://reddit.com/link/1jwhp26/video/jiai2kgjs4ue1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;QwQ 32B thought for 17 minutes, and then flop üòÜ&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1jwhp26/video/s0vsid57v4ue1/player"&gt;https://reddit.com/link/1jwhp26/video/s0vsid57v4ue1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Conclusion:&lt;/p&gt; &lt;p&gt;Qwen2.5 Coder 32B is still a better choice for coding, and it's not prime time for a 14B model yet. &lt;/p&gt; &lt;p&gt;Also, I know it's a bit unfair to compare a 32B model with a 14B one, but DeepCoder ranked among o3-mini, so why not? I also tried comparing it with Qwen2.5 Coder 14B, but it generated invalid code. To be fair, Qwen didn't even focus on styling, and it's true that DeepCoder got the style closer to o3-mini, but not the functionality :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwhp26/deepcoder_14b_vs_qwen25_coder_32b_vs_qwq_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwhp26/deepcoder_14b_vs_qwen25_coder_32b_vs_qwq_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jwhp26/deepcoder_14b_vs_qwen25_coder_32b_vs_qwq_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-11T04:37:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jwhdkx</id>
    <title>I fine-tuned CSM to make it always speak in whisper.</title>
    <updated>2025-04-11T04:18:05+00:00</updated>
    <author>
      <name>/u/PresentationSame1738</name>
      <uri>https://old.reddit.com/user/PresentationSame1738</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwhdkx/i_finetuned_csm_to_make_it_always_speak_in_whisper/"&gt; &lt;img alt="I fine-tuned CSM to make it always speak in whisper." src="https://external-preview.redd.it/0LClg4PDbRTHdYfFBDvXnXLwRGccbfNGftgSZaaDPzs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=873cf686579af675426ef66ed8d2c3b12ce13cf2" title="I fine-tuned CSM to make it always speak in whisper." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, LocalLLaMA!&lt;/p&gt; &lt;p&gt;Recently, I've been looking closely at the Sesame's CSM-1b model. Although there were a lot of controversies around it, I believe it's one of the strongest TTS-like models open-source has along with Orpheus, especially with context awareness!&lt;/p&gt; &lt;p&gt;With &lt;a href="https://github.com/senstella/csm-mlx/pull/10"&gt;an amazing PR&lt;/a&gt; to my CSM repository, contributors and I made CSM SFT fine-tunable on Mac, and ran a short fine-tune with my MacBook Air M2! (Around 40 samples) The result is pretty good - it generates a consistent whisper voice quite nicely.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/senstella/csm-expressiva-1b/resolve/main/assets/demo.wav"&gt;Here's a quick sample.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/senstella/csm-expressiva-1b"&gt;Model Page&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There's a lot of room for improvement though. First of all, it just goes through SFT-phase, not RL-phase. I plan to quickly implement KTO and giving another shot on top of this model to further improve the stability of the model.&lt;/p&gt; &lt;p&gt;Hope you like it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PresentationSame1738"&gt; /u/PresentationSame1738 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/senstella/csm-expressiva-1b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwhdkx/i_finetuned_csm_to_make_it_always_speak_in_whisper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jwhdkx/i_finetuned_csm_to_make_it_always_speak_in_whisper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-11T04:18:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jwm63r</id>
    <title>DeepCoder-14B: Superior Open-Source LLM</title>
    <updated>2025-04-11T09:59:11+00:00</updated>
    <author>
      <name>/u/sonichigo-1219</name>
      <uri>https://old.reddit.com/user/sonichigo-1219</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwm63r/deepcoder14b_superior_opensource_llm/"&gt; &lt;img alt="DeepCoder-14B: Superior Open-Source LLM" src="https://external-preview.redd.it/gpYKlsAsXO_PBVDFzTm7rzzEIh9LVPBdCM3aQoN3w7Y.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce17c2dedfda380f36b49232b8ad2df92b800ecb" title="DeepCoder-14B: Superior Open-Source LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sonichigo-1219"&gt; /u/sonichigo-1219 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.sonichigo.com/deepcoder-14b-open-source-llm-that-beats-giants"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwm63r/deepcoder14b_superior_opensource_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jwm63r/deepcoder14b_superior_opensource_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-11T09:59:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw9upz</id>
    <title>Facebook Pushes Its Llama 4 AI Model to the Right, Wants to Present ‚ÄúBoth Sides‚Äù</title>
    <updated>2025-04-10T21:51:41+00:00</updated>
    <author>
      <name>/u/WanderingStranger0</name>
      <uri>https://old.reddit.com/user/WanderingStranger0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw9upz/facebook_pushes_its_llama_4_ai_model_to_the_right/"&gt; &lt;img alt="Facebook Pushes Its Llama 4 AI Model to the Right, Wants to Present ‚ÄúBoth Sides‚Äù" src="https://external-preview.redd.it/03uwcLJN0ZpAJgNqUGMgr-4V8aM1Tl_IIQXCSo0btJA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1a58da6c992c3251df315afbe32e193f0741c63" title="Facebook Pushes Its Llama 4 AI Model to the Right, Wants to Present ‚ÄúBoth Sides‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WanderingStranger0"&gt; /u/WanderingStranger0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.404media.co/facebook-pushes-its-llama-4-ai-model-to-the-right-wants-to-present-both-sides/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw9upz/facebook_pushes_its_llama_4_ai_model_to_the_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw9upz/facebook_pushes_its_llama_4_ai_model_to_the_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T21:51:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jwiye4</id>
    <title>Lmarena.ai boots off llama4 from leaderboard</title>
    <updated>2025-04-11T06:01:09+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://lmarena.ai/?leaderboard"&gt;https://lmarena.ai/?leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Related discussion: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ju5aux/lmarenaai_confirms_that_meta_cheated/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ju5aux/lmarenaai_confirms_that_meta_cheated/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Correction: the non human preference version, is at rank 32. Thanks DFruct and OneHalf for the correction.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwiye4/lmarenaai_boots_off_llama4_from_leaderboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwiye4/lmarenaai_boots_off_llama4_from_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jwiye4/lmarenaai_boots_off_llama4_from_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-11T06:01:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jwlcar</id>
    <title>Wouldn't it make sense to use torrent?</title>
    <updated>2025-04-11T08:59:02+00:00</updated>
    <author>
      <name>/u/Nightslide1</name>
      <uri>https://old.reddit.com/user/Nightslide1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It just came to my mind that Huggingface is basically a central point for LLM downloads and hosting. What if we just used torrent to download and &amp;quot;host&amp;quot; LLM files?&lt;/p&gt; &lt;p&gt;This would mean faster downloads and less reliance on one singular organization. Also Huggingface wouldn't need a tremendous amount of bandwidth which probably costs quite a lot. And the best part: Everyone with a home server and some spare bandwidth could contribute and help to keep the system stable.&lt;/p&gt; &lt;p&gt;I'd just like to open a discussion about this topic since I think this might be kind of helpful for both LLM hosters and end consumers.&lt;/p&gt; &lt;p&gt;So, what do you think, does this make sense?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nightslide1"&gt; /u/Nightslide1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwlcar/wouldnt_it_make_sense_to_use_torrent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwlcar/wouldnt_it_make_sense_to_use_torrent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jwlcar/wouldnt_it_make_sense_to_use_torrent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-11T08:59:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jwe7pb</id>
    <title>Open source, when?</title>
    <updated>2025-04-11T01:24:41+00:00</updated>
    <author>
      <name>/u/Specter_Origin</name>
      <uri>https://old.reddit.com/user/Specter_Origin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwe7pb/open_source_when/"&gt; &lt;img alt="Open source, when?" src="https://preview.redd.it/qg5a1njiy3ue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9fad36429a8d9f30a62e3e07da681ffb9be6ef5" title="Open source, when?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specter_Origin"&gt; /u/Specter_Origin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qg5a1njiy3ue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwe7pb/open_source_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jwe7pb/open_source_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-11T01:24:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jwlxlt</id>
    <title>Meta‚Äôs AI research lab is ‚Äòdying a slow death,‚Äô some insiders say‚Äîbut‚Ä¶</title>
    <updated>2025-04-11T09:42:16+00:00</updated>
    <author>
      <name>/u/UnforgottenPassword</name>
      <uri>https://old.reddit.com/user/UnforgottenPassword</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwlxlt/metas_ai_research_lab_is_dying_a_slow_death_some/"&gt; &lt;img alt="Meta‚Äôs AI research lab is ‚Äòdying a slow death,‚Äô some insiders say‚Äîbut‚Ä¶" src="https://external-preview.redd.it/2o1G5emSxIhWAEIHS9O-76Nrl3QaDkBsS0bYLzwXgQI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7c50b1f44aaddd11771e00fe683ac087a57f799" title="Meta‚Äôs AI research lab is ‚Äòdying a slow death,‚Äô some insiders say‚Äîbut‚Ä¶" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Original paywalled link:&lt;/p&gt; &lt;p&gt;&lt;a href="https://fortune.com/2025/04/10/meta-ai-research-lab-fair-questions-departures-future-yann-lecun-new-beginning"&gt;https://fortune.com/2025/04/10/meta-ai-research-lab-fair-questions-departures-future-yann-lecun-new-beginning&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UnforgottenPassword"&gt; /u/UnforgottenPassword &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://archive.ph/fY2ND"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jwlxlt/metas_ai_research_lab_is_dying_a_slow_death_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jwlxlt/metas_ai_research_lab_is_dying_a_slow_death_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-11T09:42:16+00:00</published>
  </entry>
</feed>
