<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-15T13:35:06+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i1knlj</id>
    <title>2025 will be the year of small omni models?</title>
    <updated>2025-01-15T00:13:27+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I believe 2025 will be the year of small omni models.&lt;/p&gt; &lt;p&gt;What we already have:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Infinigence/Megrez-3B-Omni"&gt;Megrez-3B-Omni&lt;/a&gt; (released at the end of 2024)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;MiniCPM-o&lt;/a&gt; built on top of SigLip-400M, Whisper-medium-300M, ChatTTS-200M, and Qwen2.5-7B.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What's your opinion?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1knlj/2025_will_be_the_year_of_small_omni_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1knlj/2025_will_be_the_year_of_small_omni_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1knlj/2025_will_be_the_year_of_small_omni_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T00:13:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1vgez</id>
    <title>Chunking and resubmission a viable strategy to work around the context window limit?</title>
    <updated>2025-01-15T11:32:51+00:00</updated>
    <author>
      <name>/u/brian-the-porpoise</name>
      <uri>https://old.reddit.com/user/brian-the-porpoise</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all&lt;/p&gt; &lt;p&gt;So I am new to working with LLMs (web dev by day, so not new to tech in general) and have a use case to summarize larger texts. Reading through the forum, this seems to be a known issue with LLMs and their context window. &lt;/p&gt; &lt;p&gt;(I am working with Llama3 via GPT4All locally in python via llm.datasette). &lt;/p&gt; &lt;p&gt;So one way I am currently attempting to get around that is by chunking the text to about 30% below the context window, summarizing the chunk, and then re-adding the summary to the next raw chunk to be summarized. &lt;/p&gt; &lt;p&gt;Are there any concerns with this approach? The results look okay so far, but since I have very little knowledge of whats under the hood, I am wondering if there is an inherent flaw in this. &lt;/p&gt; &lt;p&gt;(The texts to be summarized are not ultra crucial. A good enough summary will do and does not need to be super detailed either)- &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brian-the-porpoise"&gt; /u/brian-the-porpoise &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1vgez/chunking_and_resubmission_a_viable_strategy_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1vgez/chunking_and_resubmission_a_viable_strategy_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1vgez/chunking_and_resubmission_a_viable_strategy_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T11:32:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i192xf</id>
    <title>DDR6 RAM and a reasonable GPU should be able to run 70b models with good speed</title>
    <updated>2025-01-14T15:51:58+00:00</updated>
    <author>
      <name>/u/itsnottme</name>
      <uri>https://old.reddit.com/user/itsnottme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Right now low VRAM GPUs are the bottleneck in running bigger models, but DDR6 ram should somewhat fix this issue. The ram can supplement GPUs to run LLMs at pretty good speed. &lt;/p&gt; &lt;p&gt;Running bigger models on CPU alone is not ideal, a reasonable speed GPU will still be needed to calculate the context. Let's use a RTX 4080 for example but a slower one is fine as well.&lt;/p&gt; &lt;p&gt;A 70b Q4 KM model is ~40 GB&lt;/p&gt; &lt;p&gt;8192 context is around 3.55 GB&lt;/p&gt; &lt;p&gt;RTX 4080 can hold around 12 GB of the model + 3.55 GB context + leaving 0.45 GB for system memory.&lt;/p&gt; &lt;p&gt;RTX 4080 Memory Bandwidth is 716.8 GB/s x 0.7 for efficiency = ~502 GB/s&lt;/p&gt; &lt;p&gt;For DDR6 ram, it's hard to say for sure but should be around twice the speed of DDR5 and supports Quad Channel so should be close to 360 GB/s * 0.7 = 252 GB/s&lt;/p&gt; &lt;p&gt;(0.3√ó502) + (0.7√ó252) = 327 GB/s&lt;/p&gt; &lt;p&gt;So the model should run at around 8.2 tokens/s&lt;/p&gt; &lt;p&gt;It should be a pretty reasonable speed for the average user. Even a slower GPU should be fine as well.&lt;/p&gt; &lt;p&gt;If I made a mistake in the calculation, feel free to let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/itsnottme"&gt; /u/itsnottme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i192xf/ddr6_ram_and_a_reasonable_gpu_should_be_able_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i192xf/ddr6_ram_and_a_reasonable_gpu_should_be_able_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i192xf/ddr6_ram_and_a_reasonable_gpu_should_be_able_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T15:51:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1jpvi</id>
    <title>I built a fast "agentic" insurance app with FastAPIs using small function calling LLMs</title>
    <updated>2025-01-14T23:29:23+00:00</updated>
    <author>
      <name>/u/Terrible_Attention83</name>
      <uri>https://old.reddit.com/user/Terrible_Attention83</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1jpvi/i_built_a_fast_agentic_insurance_app_with/"&gt; &lt;img alt="I built a fast &amp;quot;agentic&amp;quot; insurance app with FastAPIs using small function calling LLMs" src="https://preview.redd.it/gf7clczln1de1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1d16508d11da2cd20e0adecb6812fc481fae7d0" title="I built a fast &amp;quot;agentic&amp;quot; insurance app with FastAPIs using small function calling LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently came across this post on small function-calling LLMs &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hr9ll1/i_built_a_small_function_calling_llm_that_packs_a/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1hr9ll1/i_built_a_small_function_calling_llm_that_packs_a/&lt;/a&gt; and decided to give the project a whirl. My use case was to build an agentic workflow for insurance claims (being able to process them, show updates, add documents, etc)&lt;/p&gt; &lt;p&gt;Here is what I liked: I was able to build an agentic solution with just APIs (for the most part) - and it was fast as advertised. The Arch-Function LLMs did generalize well and I wrote mostly business logic. The thing that I found interesting was its prompt_target feature which helped me build task routing and extracted keywords/information from a user query so that I can improve accuracy of tasks and trigger downstream agents when/if needed.&lt;/p&gt; &lt;p&gt;Here is what I did not like: There seems to be a close integration with Gradio at the moment. The gateway enriches conversational state with meta-data, which seems to improve function calling performance. But i suspect they might improve that over time. Also descriptions of prompt_targets/function calling need to be simple and terse. There is some work to make sure the parameters and descriptions aren't too obtuse. I think OpenAI offers similar guidance, but it needs simple and concise descriptions of downstream tasks and parameters. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terrible_Attention83"&gt; /u/Terrible_Attention83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gf7clczln1de1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1jpvi/i_built_a_fast_agentic_insurance_app_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1jpvi/i_built_a_fast_agentic_insurance_app_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T23:29:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i148es</id>
    <title>Today I start my very own org 100% devoted to open-source - and it's all thanks to LLMs</title>
    <updated>2025-01-14T11:43:10+00:00</updated>
    <author>
      <name>/u/mark-lord</name>
      <uri>https://old.reddit.com/user/mark-lord</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; Big thank you to every single one of you here!! My background is in biology - not software dev. This huge milestone in my life could never have happened if it wasn't for LLMs, the fantastic open source ecosystem around them, and of course all the awesome folks here in r /LocalLlama!&lt;/p&gt; &lt;p&gt;Also this post was originally a lot longer but I keep getting autofiltered lol - will put the rest in comments üòÑ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mark-lord"&gt; /u/mark-lord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i148es/today_i_start_my_very_own_org_100_devoted_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i148es/today_i_start_my_very_own_org_100_devoted_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i148es/today_i_start_my_very_own_org_100_devoted_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T11:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i11hre</id>
    <title>Why are they releasing open source models for free?</title>
    <updated>2025-01-14T08:21:50+00:00</updated>
    <author>
      <name>/u/wochiramen</name>
      <uri>https://old.reddit.com/user/wochiramen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are getting several quite good AI models. It takes money to train them, yet they are being released for free.&lt;/p&gt; &lt;p&gt;Why? What‚Äôs the incentive to release a model for free?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wochiramen"&gt; /u/wochiramen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i11hre/why_are_they_releasing_open_source_models_for_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i11hre/why_are_they_releasing_open_source_models_for_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i11hre/why_are_they_releasing_open_source_models_for_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T08:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1s6n5</id>
    <title>üçí Cherry Studio: A Desktop Client Supporting Multi-Model Services, Designed for Professionals</title>
    <updated>2025-01-15T07:22:14+00:00</updated>
    <author>
      <name>/u/XinmingWong</name>
      <uri>https://old.reddit.com/user/XinmingWong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1s6n5/cherry_studio_a_desktop_client_supporting/"&gt; &lt;img alt="üçí Cherry Studio: A Desktop Client Supporting Multi-Model Services, Designed for Professionals " src="https://external-preview.redd.it/r1Yo3XfvNwik4F4JitQEcSvauLBFZhasaAJAW6INyI0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fbe1a18a6531f32c71769138c9ec787488988aa" title="üçí Cherry Studio: A Desktop Client Supporting Multi-Model Services, Designed for Professionals " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üçí &lt;strong&gt;Cherry Studio: A Desktop Client Supporting Multi-Model Services, Designed for Professionals&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Cherry Studio is a powerful desktop client built for professionals, featuring over 30 industry-specific intelligent assistants to help users enhance productivity across a variety of scenarios.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Aggregated Model Services&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Cherry Studio integrates numerous service providers, offering access to over 300 large language models. You can seamlessly switch between models during usage, leveraging the strengths of each model to solve problems efficiently. For details on the integrated providers, refer to the configuration page.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z0ocnljrz3de1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb732d73493e09a1f2b3c62c52a6e315681f7801"&gt;https://preview.redd.it/z0ocnljrz3de1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb732d73493e09a1f2b3c62c52a6e315681f7801&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Cross-Platform Compatibility for a Seamless Experience&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Cherry Studio supports both Windows and macOS operating systems, with plans to expand to mobile platforms in the future. This means no matter what device you use, you can enjoy the convenience Cherry Studio brings. Say goodbye to platform restrictions and fully explore the potential of GPT technology!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/or6yogatz3de1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa7e6b69d1264996f551754aeb6e06ee1940a893"&gt;https://preview.redd.it/or6yogatz3de1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa7e6b69d1264996f551754aeb6e06ee1940a893&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tailored for Diverse Professionals&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Cherry Studio is designed to meet the needs of various industries utilizing GPT technology. Whether you are a developer coding away, a designer seeking inspiration, or a writer crafting stories, Cherry Studio can be your reliable assistant. With advanced natural language processing, it helps you tackle challenges like data analysis, text generation, and code writing effortlessly.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/khzjl29uz3de1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6cbe6c70431ece12dc388357c97b59357d6905c7"&gt;https://preview.redd.it/khzjl29uz3de1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6cbe6c70431ece12dc388357c97b59357d6905c7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rich Application Scenarios to Inspire Creativity&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Developer‚Äôs Coding Partner:&lt;/strong&gt; Generate and debug code efficiently with Cherry Studio.&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Designer‚Äôs Creative Tool:&lt;/strong&gt; Produce creative text and design descriptions to spark ideas.&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Writer‚Äôs Trusted Assistant:&lt;/strong&gt; Assist with drafting and editing articles for a smoother writing process.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Built-in Translation Assistant:&lt;/strong&gt; Break language barriers with ease.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y250tqtvz3de1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a214a85bd0447096f38cf31e1a9a316653070b7"&gt;https://preview.redd.it/y250tqtvz3de1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a214a85bd0447096f38cf31e1a9a316653070b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Standout Features Driving Innovation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Open-Source Spirit:&lt;/strong&gt; Cherry Studio offers open-source code, encouraging users to customize and expand their personalized GPT assistant.&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Continuous Updates:&lt;/strong&gt; The latest version, v0.4.4, is now available, with developers committed to enhancing functionality and user experience.&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Minimalist Design:&lt;/strong&gt; An intuitive interface ensures you can focus on your creations.&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Efficient Workflow:&lt;/strong&gt; Quickly switch between models to find the best solutions.&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Smart Conversations:&lt;/strong&gt; AI-powered session naming keeps your chat history organized for easy review.&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Drag-and-Drop Sorting:&lt;/strong&gt; Sort agents, conversations, or settings effortlessly for better organization.&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Worry-Free Translation:&lt;/strong&gt; Built-in intelligent translation covers major languages for accurate cross-language communication.&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Multi-Language Support:&lt;/strong&gt; Designed for global users, breaking language barriers with GPT technology.&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Theme Switching:&lt;/strong&gt; Day and night modes ensure an enjoyable visual experience at any time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Getting Started with Cherry Studio&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Using Cherry Studio is simple. Follow these steps to embark on your GPT journey:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Download the version for your system.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Install and launch the client.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Follow the on-screen instructions.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Explore powerful features.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Adjust settings as needed.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Join the community to share experiences with other users.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cherry Studio is not just software‚Äîit‚Äôs your gateway to the boundless possibilities of GPT technology. By simplifying complex technology into user-friendly tools, it empowers everyone to harness the power of GPT with ease. Whether you are a tech expert or a casual user, Cherry Studio will bring unparalleled convenience to your work and life.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Download Cherry Studio now and begin your intelligent journey!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/CherryHQ/cherry-studio"&gt;https://github.com/CherryHQ/cherry-studio&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XinmingWong"&gt; /u/XinmingWong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1s6n5/cherry_studio_a_desktop_client_supporting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1s6n5/cherry_studio_a_desktop_client_supporting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1s6n5/cherry_studio_a_desktop_client_supporting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T07:22:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1palb</id>
    <title>Just added support for Phi-4 to MLX Model Manager so you can use it in your Swift applications with just a couple of lines of code.</title>
    <updated>2025-01-15T04:13:12+00:00</updated>
    <author>
      <name>/u/Onboto</name>
      <uri>https://old.reddit.com/user/Onboto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1palb/just_added_support_for_phi4_to_mlx_model_manager/"&gt; &lt;img alt="Just added support for Phi-4 to MLX Model Manager so you can use it in your Swift applications with just a couple of lines of code. " src="https://external-preview.redd.it/dHdqaG90dDUyM2RlMaMEkI1tj4yerNzAEj0qZXlRscYhMqjgZV-XtG9Ya7Pu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ffa106752dc1802dccebf3bd97a07f62039b7d2" title="Just added support for Phi-4 to MLX Model Manager so you can use it in your Swift applications with just a couple of lines of code. " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Onboto"&gt; /u/Onboto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ay8wkst523de1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1palb/just_added_support_for_phi4_to_mlx_model_manager/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1palb/just_added_support_for_phi4_to_mlx_model_manager/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T04:13:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1vpaz</id>
    <title>Performance of 64GB DDR4 for model + 6gb vram flash-attention for context?</title>
    <updated>2025-01-15T11:49:40+00:00</updated>
    <author>
      <name>/u/Imjustmisunderstood</name>
      <uri>https://old.reddit.com/user/Imjustmisunderstood</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My idea is to feed ~3000 tokens of documents into context to improve output quality. I dont mind slow token/s inference, but I do very much mind the time for prompt eval given these large contexts. &lt;/p&gt; &lt;p&gt;Is it possible to load all layers of a model into memory and use VRAM exclusively for context? (Speeding up eval with flash-attention)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Imjustmisunderstood"&gt; /u/Imjustmisunderstood &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1vpaz/performance_of_64gb_ddr4_for_model_6gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1vpaz/performance_of_64gb_ddr4_for_model_6gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1vpaz/performance_of_64gb_ddr4_for_model_6gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T11:49:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1eyl5</id>
    <title>2025 and the future of Local AI</title>
    <updated>2025-01-14T19:59:17+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2024 was an amazing year for Local AI. We had great free models Llama 3.x, Qwen2.5 Deepseek v3 and much more.&lt;/p&gt; &lt;p&gt;However, we also see some counter-trends such as Mistral previously released very liberal licenses, but started moving towards Research licenses. We see some AI shops closing down.&lt;/p&gt; &lt;p&gt;I wonder if we are getting close to Peak 'free' AI as competition heats up and competitors drop out leaving remaining competitors forced to monetize.&lt;/p&gt; &lt;p&gt;We still have LLama, Qwen and Deepseek providing open models - but even here, there are questions on whether we can really deploy these easily (esp. with monstrous 405B Llama and DS v3).&lt;/p&gt; &lt;p&gt;Let's also think about economics. Imagine a world where OpenAI does make a leap ahead. They release an AI which they sell to corporations for $1,000 a month subject to a limited duty cycle. Let's say this is powerful enough and priced right to wipe out 30% of office jobs. What will this do to society and the economy? What happens when this 30% ticks upwards to 50%, 70%?&lt;/p&gt; &lt;p&gt;Currently, we have software companies like Google which have huge scale, servicing the world with a relatively small team. What if most companies are like this? A core team of execs with the work done mainly through AI systems. What happens when this comes to manual jobs through AI robots?&lt;/p&gt; &lt;p&gt;What would the average person do? How can such an economy function?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1eyl5/2025_and_the_future_of_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1eyl5/2025_and_the_future_of_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1eyl5/2025_and_the_future_of_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T19:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1b2xq</id>
    <title>Transformer^2: Self-adaptive LLMs</title>
    <updated>2025-01-14T17:16:19+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2501.06252"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1b2xq/transformer2_selfadaptive_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1b2xq/transformer2_selfadaptive_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T17:16:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1i19e8u</id>
    <title>Agentic setups beat vanilla LLMs by a huge margin üìà</title>
    <updated>2025-01-14T16:05:40+00:00</updated>
    <author>
      <name>/u/unofficialmerve</name>
      <uri>https://old.reddit.com/user/unofficialmerve</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i19e8u/agentic_setups_beat_vanilla_llms_by_a_huge_margin/"&gt; &lt;img alt="Agentic setups beat vanilla LLMs by a huge margin üìà" src="https://b.thumbs.redditmedia.com/VaHY2thgF4XeViAsdm5g4KaSQqjvdrVHVe17FR8kgTs.jpg" title="Agentic setups beat vanilla LLMs by a huge margin üìà" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello folks üëãüèª I'm Merve, I work on Hugging Face's new agents library smolagents. &lt;/p&gt; &lt;p&gt;We recently observed that many people are sceptic of agentic systems, so we benchmarked our CodeAgents (agents that write their actions/tool calls in python blobs) against vanilla LLM calls.&lt;/p&gt; &lt;p&gt;Plot twist: agentic setups easily bring 40 percentage point improvements compared to vanilla LLMs This crazy score increase makes sense, let's take this SimpleQA question:&lt;br /&gt; &amp;quot;Which Dutch player scored an open-play goal in the 2022 Netherlands vs Argentina game in the men‚Äôs FIFA World Cup?&amp;quot;&lt;/p&gt; &lt;p&gt;If I had to answer that myself, I certainly would do better with access to a web search tool than with my vanilla knowledge. (argument put forward by Andrew Ng in a great talk at Sequoia)&lt;br /&gt; Here each benchmark is a subsample of ~50 questions from the original benchmarks. Find the whole benchmark here: &lt;a href="https://github.com/huggingface/smolagents/blob/main/examples/benchmark.ipynb"&gt;https://github.com/huggingface/smolagents/blob/main/examples/benchmark.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7p6lbz7fgzce1.png?width=1467&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30d91e22b32e572e8824b08b4d95a52aeb82c5d5"&gt;https://preview.redd.it/7p6lbz7fgzce1.png?width=1467&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30d91e22b32e572e8824b08b4d95a52aeb82c5d5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialmerve"&gt; /u/unofficialmerve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i19e8u/agentic_setups_beat_vanilla_llms_by_a_huge_margin/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i19e8u/agentic_setups_beat_vanilla_llms_by_a_huge_margin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i19e8u/agentic_setups_beat_vanilla_llms_by_a_huge_margin/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T16:05:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1xbv1</id>
    <title>OuteTTS 0.3: New 1B &amp; 500M Models</title>
    <updated>2025-01-15T13:26:15+00:00</updated>
    <author>
      <name>/u/OuteAI</name>
      <uri>https://old.reddit.com/user/OuteAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1xbv1/outetts_03_new_1b_500m_models/"&gt; &lt;img alt="OuteTTS 0.3: New 1B &amp;amp; 500M Models" src="https://external-preview.redd.it/MnR2a241bWpzNWRlMS8HG7_sP5Xscyq5qRLwQkOnJIWAwD3-JkIhoicGw7Ke.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=30902d1d9fa313454bf58da04ab6d0ae30505a12" title="OuteTTS 0.3: New 1B &amp;amp; 500M Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OuteAI"&gt; /u/OuteAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rb1px5mjs5de1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1xbv1/outetts_03_new_1b_500m_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1xbv1/outetts_03_new_1b_500m_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T13:26:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1tosi</id>
    <title>Company has plans to add external gpu memory</title>
    <updated>2025-01-15T09:21:01+00:00</updated>
    <author>
      <name>/u/mindwip</name>
      <uri>https://old.reddit.com/user/mindwip</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://blocksandfiles.com/2025/01/13/panmnesia-gpu-cxl-memory-expansion/"&gt;https://blocksandfiles.com/2025/01/13/panmnesia-gpu-cxl-memory-expansion/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.archyde.com/panmnesia-wins-ces-award-for-gpu-cxl-memory-expansion-technology-blocks-and-files/"&gt;https://www.archyde.com/panmnesia-wins-ces-award-for-gpu-cxl-memory-expansion-technology-blocks-and-files/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This looks pretty cool while not yet meant for home use as I think they targeting server stacks first. I hope we get a retail version of this! Sounds like they at the proof of concept stage. So maybe 2026 will be interesting. If more companys can train much cheaper we might get way more open source models. &lt;/p&gt; &lt;p&gt;A lot of it over my head, but sounds like they are essentially just connecting ssds and ddr to gpus creating a unified memory space that the gpu sees. Whish the articals had more memory bandwidth and sizing specs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mindwip"&gt; /u/mindwip &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1tosi/company_has_plans_to_add_external_gpu_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1tosi/company_has_plans_to_add_external_gpu_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1tosi/company_has_plans_to_add_external_gpu_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T09:21:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1ntmb</id>
    <title>[2501.08313] MiniMax-01: Scaling Foundation Models with Lightning Attention</title>
    <updated>2025-01-15T02:52:54+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2501.08313"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ntmb/250108313_minimax01_scaling_foundation_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ntmb/250108313_minimax01_scaling_foundation_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T02:52:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1x1mm</id>
    <title>Flow charts, flow charts everywhere</title>
    <updated>2025-01-15T13:10:56+00:00</updated>
    <author>
      <name>/u/AnotherSoftEng</name>
      <uri>https://old.reddit.com/user/AnotherSoftEng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1x1mm/flow_charts_flow_charts_everywhere/"&gt; &lt;img alt="Flow charts, flow charts everywhere" src="https://preview.redd.it/su32pem6q5de1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48d3c26cb427054e3de9f38bb7fa6f4ea73f3686" title="Flow charts, flow charts everywhere" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnotherSoftEng"&gt; /u/AnotherSoftEng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/su32pem6q5de1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1x1mm/flow_charts_flow_charts_everywhere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1x1mm/flow_charts_flow_charts_everywhere/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T13:10:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1a88y</id>
    <title>MiniMax-Text-01 - A powerful new MoE language model with 456B total parameters (45.9 billion activated)</title>
    <updated>2025-01-14T16:41:20+00:00</updated>
    <author>
      <name>/u/Many_SuchCases</name>
      <uri>https://old.reddit.com/user/Many_SuchCases</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1a88y/minimaxtext01_a_powerful_new_moe_language_model/"&gt; &lt;img alt="MiniMax-Text-01 - A powerful new MoE language model with 456B total parameters (45.9 billion activated)" src="https://external-preview.redd.it/HbANHzNsjzvIfVaIHJm0DQnyVjkhdwH7FoXz3GLoR3k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ceab60c72e05525604b9367fa7915922146839a5" title="MiniMax-Text-01 - A powerful new MoE language model with 456B total parameters (45.9 billion activated)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-Text-01"&gt;https://huggingface.co/MiniMaxAI/MiniMax-Text-01&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8os84sl2mzce1.png?width=3320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4f6f93b8a0965d65139ba727de29c55880f1b91"&gt;https://preview.redd.it/8os84sl2mzce1.png?width=3320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4f6f93b8a0965d65139ba727de29c55880f1b91&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Description:&lt;/strong&gt; MiniMax-Text-01 is a powerful language model with 456 billion total parameters, of which 45.9 billion are activated per token. To better unlock the long context capabilities of the model, MiniMax-Text-01 adopts a hybrid architecture that combines Lightning Attention, Softmax Attention and Mixture-of-Experts (MoE). Leveraging advanced parallel strategies and innovative compute-communication overlap methods‚Äîsuch as Linear Attention Sequence Parallelism Plus (LASP+), varlen ring attention, Expert Tensor Parallel (ETP), etc., MiniMax-Text-01's training context length is extended to 1 million tokens, and it can handle a context of up to 4 million tokens during the inference. On various academic benchmarks, MiniMax-Text-01 also demonstrates the performance of a top-tier model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Architecture:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Total Parameters: 456B&lt;/li&gt; &lt;li&gt;Activated Parameters per Token: 45.9B&lt;/li&gt; &lt;li&gt;Number Layers: 80&lt;/li&gt; &lt;li&gt;Hybrid Attention: a softmax attention is positioned after every 7 lightning attention. &lt;ul&gt; &lt;li&gt;Number of attention heads: 64&lt;/li&gt; &lt;li&gt;Attention head dimension: 128&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Mixture of Experts: &lt;ul&gt; &lt;li&gt;Number of experts: 32&lt;/li&gt; &lt;li&gt;Expert hidden dimension: 9216&lt;/li&gt; &lt;li&gt;Top-2 routing strategy&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Positional Encoding: Rotary Position Embedding (RoPE) applied to half of the attention head dimension with a base frequency of 10,000,000&lt;/li&gt; &lt;li&gt;Hidden Size: 6144&lt;/li&gt; &lt;li&gt;Vocab Size: 200,064&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Blog post:&lt;/strong&gt; &lt;a href="https://www.minimaxi.com/en/news/minimax-01-series-2"&gt;https://www.minimaxi.com/en/news/minimax-01-series-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HuggingFace:&lt;/strong&gt; &lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-Text-01"&gt;https://huggingface.co/MiniMaxAI/MiniMax-Text-01&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try online:&lt;/strong&gt; &lt;a href="https://www.hailuo.ai/"&gt;https://www.hailuo.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href="https://github.com/MiniMax-AI/MiniMax-01"&gt;https://github.com/MiniMax-AI/MiniMax-01&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Homepage:&lt;/strong&gt; &lt;a href="https://www.minimaxi.com/en"&gt;https://www.minimaxi.com/en&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PDF paper:&lt;/strong&gt; &lt;a href="https://filecdn.minimax.chat/_Arxiv_MiniMax_01_Report.pdf"&gt;https://filecdn.minimax.chat/_Arxiv_MiniMax_01_Report.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: I am not affiliated&lt;/p&gt; &lt;p&gt;GGUF quants might take a while because the architecture is new (MiniMaxText01ForCausalLM)&lt;/p&gt; &lt;p&gt;A Vision model was also released: &lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-VL-01"&gt;https://huggingface.co/MiniMaxAI/MiniMax-VL-01&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Many_SuchCases"&gt; /u/Many_SuchCases &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1a88y/minimaxtext01_a_powerful_new_moe_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1a88y/minimaxtext01_a_powerful_new_moe_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1a88y/minimaxtext01_a_powerful_new_moe_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T16:41:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i17k5e</id>
    <title>OASIS: Open social media stimulator that uses up to 1 million agents.</title>
    <updated>2025-01-14T14:43:05+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i17k5e/oasis_open_social_media_stimulator_that_uses_up/"&gt; &lt;img alt="OASIS: Open social media stimulator that uses up to 1 million agents." src="https://preview.redd.it/rgfjjzbf1zce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad7c4c95213e6848f1fad91fc11eacb2cb18e3b8" title="OASIS: Open social media stimulator that uses up to 1 million agents." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rgfjjzbf1zce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i17k5e/oasis_open_social_media_stimulator_that_uses_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i17k5e/oasis_open_social_media_stimulator_that_uses_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T14:43:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1kz1c</id>
    <title>Sharing my unorthodox home setup, and how I use local LLMs</title>
    <updated>2025-01-15T00:28:26+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So for the past year and a half+ I've been tinkering with, planning out and updating my home setup, and figured that with 2025 here, I'd join in on sharing where it's at. It's an expensive little home lab, though nothing nearly as fancy or cool as what other folks have.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;tl;dr&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;- I have 2 &amp;quot;assistants&amp;quot; (1 large and 1 small, with each assistant made up of between 4-7 models working together), and a development machine/assistant. The dev box simulates the smaller assistant for dev purposes. Each assistant has offline wiki access, vision capability, and I use them for all my hobby work/random stuff.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;The Hardware&lt;/h1&gt; &lt;p&gt;The hardware is a mix of stuff I already had, or stuff I bought for LLM tinkering. I'm a software dev and tinkering with stuff is one of my main hobbies, so I threw a fair bit of money at it. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Refurb M2 Ultra Mac Studio w/1 TB internal drive + USB C 2TB drive&lt;/li&gt; &lt;li&gt;Refurb M2 Max Macbook Pro 96GB&lt;/li&gt; &lt;li&gt;Refurb M2 Mac Mini base model&lt;/li&gt; &lt;li&gt;Windows 10 Desktop w/ RTX 4090&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Total Hardware Pricing: ~$5,500 for studio refurbished + ~$3000 for Macbook Pro refurbished + ~$500 Mac Mini refurbished (&lt;em&gt;already owned&lt;/em&gt;) + ~$2000 Windows desktop (&lt;em&gt;already owned&lt;/em&gt;) == &lt;strong&gt;$10,500 in total hardware&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;The Software&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I do most of my inference using KoboldCPP&lt;/li&gt; &lt;li&gt;I do vision inference through Ollama and my dev box uses Ollama&lt;/li&gt; &lt;li&gt;I run all inference through WilmerAI, which handles all the workflows and domain routing. This lets me use as many models as I want to power the assistants, and also setup workflows for coding windows, use the offline wiki api, etc.&lt;/li&gt; &lt;li&gt;For zero-shots, simple dev questions and other quick hits, I use Open WebUI as my front end. Otherwise I use SillyTavern for more involved programming tasks and for my assistants. &lt;ul&gt; &lt;li&gt;All of the gaming quality of life features in ST double over very nicely for assistant work and programming lol&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Setup&lt;/h1&gt; &lt;p&gt;The Mac Mini acts as one of three WilmerAI &amp;quot;cores&amp;quot;; the mini is the Wilmer home core, and also acts as the web server for all of my instances of ST and Open WebUI. There are 6 instances of Wilmer on this machine, each with its own purpose. The Macbook Pro is the Wilmer portable core (3 instances of Wilmer), and the Windows Desktop is the Wilmer dev core (2 instances of Wilmer).&lt;/p&gt; &lt;p&gt;All of the models for the Wilmer home core are on the Mac Studio, and I hope to eventually add another box to expand the home core.&lt;/p&gt; &lt;p&gt;Each core acts independently from the others, meaning doing things like removing the macbook from the network won't hurt the home core. Each core has its own text models, offline wiki api, and vision model.&lt;/p&gt; &lt;p&gt;I have 2 &amp;quot;assistants&amp;quot; set up, with the intention to later add a third. Each assistant is essentially built to be an advanced &amp;quot;rubber duck&amp;quot; (&lt;em&gt;as in the rubber duck programming method where you talk through a problem to an inanimate object and it helps you solve this problem&lt;/em&gt;). Each assistant is built entirely to talk through problems with me, of any kind, and help me solve them by challenging me, answering my questions, or using a specific set of instructions on how to think through issues in unique ways. Each assistant is built to be different, and thus solve things differently.&lt;/p&gt; &lt;p&gt;Each assistant is made up of multiple LLMs. Some examples would be: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;A responder model, which does the talking&lt;/li&gt; &lt;li&gt;A RAG model, which I use for pulling data from the offline wikipedia api for factual questions&lt;/li&gt; &lt;li&gt;A reasoning model, for thinking through a response before the responder answers&lt;/li&gt; &lt;li&gt;A coding model, for handle code issues and math issues.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The two assistants are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;RolandAI&lt;/strong&gt;- powered by the home core. All of Roland's models are generally running on the Mac Studio, and is by far the more powerful of the two. Its got conversation memories going back to early 2024, and I primarily use it. At this point I have to prune the memories regularly lol. I'm saving the pruned memories for when I get a secondary memory system into Wilmer that I can backload them into.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SomeOddCodeBot&lt;/strong&gt;- powered by the portable core. All these models run on the Macbook. This is my &amp;quot;second opinion&amp;quot; bot, and also my portable bot for when I'm on the road. It's setup is specifically different from Roland, beyond just being smaller, so that they will &amp;quot;think&amp;quot; differently about problems.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Each assistant's persona and problem solving instructions exist only within the workflows of Wilmer, meaning that front ends like SillyTavern have no information in a character card for it, Open WebUI has no prompt for it, etc. Roland, as an entity, is a specific series of workflow nodes that are designed to act, speak and process problems/prompts in a very specific way. &lt;/p&gt; &lt;p&gt;I generally have a total of about 8 front end SillyTavern/Open WebUI windows open. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Four ST windows. Two are for the two assistants individually, and one is a group chat that have both in case I want the two assistants to process a longer/more complex concept together. This replaced my old &amp;quot;development group&amp;quot;.&lt;/li&gt; &lt;li&gt;I have a fourth ST window for my home core &amp;quot;Coding&amp;quot; Wilmer instance, which is a workflow that is just for coding questions (for example, one iteration of this was using QwQ + Qwen2.5 32b coder, which the response quality landed somewhere between ChatGPT 4o and o1. Tis slow though). &lt;/li&gt; &lt;li&gt;After that, I have 4 Open WebUI windows for coding workflows, reasoning workflows and a encyclopedic questions using the offline wiki api.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How I Use Them&lt;/h1&gt; &lt;p&gt;Roland is obviously going to be the more powerful of the two assistants; I have 180GB, give or take, of VRAM to build out its model structure with. SomeOddCodeBot has about 76GB of VRAM, but has a similar structure just using smaller models.&lt;/p&gt; &lt;p&gt;I use these assistants for any personal projects that I have; I can't use them for anything work related, but I do a &lt;em&gt;lot&lt;/em&gt; of personal dev and tinkering. Whenever I have an idea, whenever I'm checking something, etc I usually bounce the ideas off of one or both assistants. If I'm trying to think through a problem I might do similarly.&lt;/p&gt; &lt;p&gt;Another example is code reviews: I often pass in the before/after code to both bots, and ask for a general analysis of what's what. I'm reviewing it myself as well, but the bots help me find little things I might have missed, and generally make me feel better that I didn't miss anything. &lt;/p&gt; &lt;p&gt;The code reviews will often be for my own work, as well as anyone committing to my personal projects.&lt;/p&gt; &lt;p&gt;For the dev core, I use Ollama as the main inference because I can do a neat trick with Wilmer on it. As long as each individual model fits on 20GB of VRAM, I can use as many models as I want in the workflow. Ollama API calls let you pass the model name in, and it unloads the current model and loads the new model instead, so I can have each Wilmer node just pass in a different model name. This lets me simulate the 76GB portable core with only 20GB, since I only use smaller models on the portable core, so I can have a dev assistant to break and mess with while I'm updating Wilmer code.&lt;/p&gt; &lt;h1&gt;2025 Plans&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I plan to convert the dev core into a coding agent box and build a Wilmer agent jobs system; think of like an agent wrapping an agent lol. I want something like Aider running as the worker agent, that is controlled by a wrapping agent that calls a Roland Wilmer instance to manage the coder. ie- Roland is in charge of the agent doing the coding. &lt;ul&gt; &lt;li&gt;I've been using Roland to code review me, help me come up with architectures for things, etc for a while. The goal of that is to tune the workflows so that I can eventually just put Roland in charge of a coding agent running on the Windows box. Write down what I want, get back a higher quality version than if I just left the normal agent to its devices; something QAed by a workflow thinking in a specific way that I want it to think. If that works well, I'd try to expand that out to have N number of agents running off of runpod boxes for larger dev work.&lt;/li&gt; &lt;li&gt;All of this is just a really high level plan atm, but I became more interested in it after finding out about that $1m competition =D What was a &amp;quot;that's a neat idea&amp;quot; became a &amp;quot;I really want to try this&amp;quot;. So this whole plan may fail miserably, but I do have some hope based on how I'm already using Wilmer today.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;I want to add Home Assistant integration in and start making home automation workflows in Wilmer. Once I've got some going, I'll add a new Wilmer core to the house, as well as a third assistant, to manage it.&lt;/li&gt; &lt;li&gt;I've got my eye on an NVidia digits... might get it to expand Roland a bit.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Anyhow, that's pretty much it. It's an odd setup, but I thought some of you might get a kick out of it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1kz1c/sharing_my_unorthodox_home_setup_and_how_i_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1kz1c/sharing_my_unorthodox_home_setup_and_how_i_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1kz1c/sharing_my_unorthodox_home_setup_and_how_i_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T00:28:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1p0yb</id>
    <title>Running Deepseek V3 with a box of scraps (but not in a cave)</title>
    <updated>2025-01-15T03:58:00+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got Deepseek running on a bunch of old 10GB Nvidia P102-100's on PCIE 1.0 x1 risers. (GPU's built for mining)&lt;br /&gt; Spread across 3 machines, connected via 1gb lan and through a firewall! &lt;/p&gt; &lt;p&gt;Bought these GPU's for $30 each, (not for this purpose lol)&lt;/p&gt; &lt;p&gt;Funnily enough the hardest part is that Llama.cpp wanted enough cpu ram to load the model before moving it to VRAM. Had to run it at Q2 because of this.&lt;br /&gt; Will try again at Q4 when I get some more.&lt;/p&gt; &lt;p&gt;Speed, a whopping &lt;strong&gt;3.6 T/s.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Considering this setup has literally everything going against it, not half bad really.&lt;/p&gt; &lt;p&gt;If you are curious, without the GPUs, the CPU server alone starts around 2.4T/s but even after 1k tokens it was down to 1.8T/s&lt;/p&gt; &lt;p&gt;Was only seeing like 30MB/s on the network, but might try upgrading everything to 10G lan just to see if it matters.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1p0yb/running_deepseek_v3_with_a_box_of_scraps_but_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1p0yb/running_deepseek_v3_with_a_box_of_scraps_but_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1p0yb/running_deepseek_v3_with_a_box_of_scraps_but_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T03:58:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1k8yq</id>
    <title>Audiblez: Generate audiobooks from e-books with Kokoro-82M</title>
    <updated>2025-01-14T23:54:42+00:00</updated>
    <author>
      <name>/u/inkompatible</name>
      <uri>https://old.reddit.com/user/inkompatible</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkompatible"&gt; /u/inkompatible &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://claudio.uk/posts/epub-to-audiobook.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1k8yq/audiblez_generate_audiobooks_from_ebooks_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1k8yq/audiblez_generate_audiobooks_from_ebooks_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T23:54:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1owp1</id>
    <title>OpenRouter Users: What feature are you missing?</title>
    <updated>2025-01-15T03:51:27+00:00</updated>
    <author>
      <name>/u/punkpeye</name>
      <uri>https://old.reddit.com/user/punkpeye</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I accidentally built an &lt;a href="https://glama.ai/gateway"&gt;OpenRouter alternative&lt;/a&gt;. I say accidentally because that wasn‚Äôt the goal of my project, but as people and companies adopted it, they requested similar features. Over time, I ended up with something that feels like an alternative.&lt;/p&gt; &lt;p&gt;The main benefit of both services is elevated rate limits without subscription, and the ability to easily switch models using OpenAI-compatible API. That's not different.&lt;/p&gt; &lt;p&gt;The unique benefits to my gateway include integration with the Chat and &lt;a href="https://glama.ai/mcp/servers"&gt;MCP ecosystem&lt;/a&gt;, more advanced analytics/logging, and reportedly lower latency and greater stability than OpenRouter. Pricing is similar, and we process several billion tokens daily. Having addressed feedback from current users, I‚Äôm now looking to the broader community for ideas on where to take the project next.&lt;/p&gt; &lt;p&gt;What are your painpoints with OpenRouter?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/punkpeye"&gt; /u/punkpeye &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1owp1/openrouter_users_what_feature_are_you_missing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1owp1/openrouter_users_what_feature_are_you_missing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1owp1/openrouter_users_what_feature_are_you_missing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T03:51:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1ty0e</id>
    <title>405B MiniMax MoE technical deepdive</title>
    <updated>2025-01-15T09:41:33+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tl;dr very (very) nice paper/model, lot of details and experiment details, hybrid with 7/8 Lightning attn, different MoE strategy than deepseek, deepnorm, WSD schedule, ~2000 H800 for training, ~12T token.&lt;br /&gt; blog: &lt;a href="https://huggingface.co/blog/eliebak/minimax01-deepdive"&gt;https://huggingface.co/blog/eliebak/minimax01-deepdive&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ty0e/405b_minimax_moe_technical_deepdive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ty0e/405b_minimax_moe_technical_deepdive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ty0e/405b_minimax_moe_technical_deepdive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T09:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1ffid</id>
    <title>I accidentally built an open alternative to Google AI Studio</title>
    <updated>2025-01-14T20:18:39+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday, I had a mini heart attack when I discovered Google AI Studio, a product that looked (at first glance) just like the tool I've been building for 5 months. However, I dove in and was super relieved once I got into the details. There were a bunch of differences, which I've detailed below.&lt;/p&gt; &lt;p&gt;I thought I‚Äôd share what I have, in case anyone has been using G AI Sudio, and might want to check out my &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;rapid prototyping tool on Github, called Kiln&lt;/a&gt;. There are some similarities, but there are also some big differences when it comes to privacy, collaboration, model support, fine-tuning, and ML techniques. I built Kiln because I've been building AI products for ~10 years (most recently at Apple, and my own startup &amp;amp; MSFT before that), and I wanted to build an easy to use, privacy focused, open source AI tooling.&lt;/p&gt; &lt;p&gt;Differences:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model Support: Kiln allows any LLM (including Gemini/Gemma) through a ton of hosts: Ollama, OpenRouter, OpenAI, etc. Google supports only Gemini &amp;amp; Gemma via Google Cloud.&lt;/li&gt; &lt;li&gt;Fine Tuning: Google lets you fine tune only Gemini, with at most 500 samples. Kiln has no limits on data size, 9 models you can tune in a few clicks (no code), and support for tuning any open model via Unsloth.&lt;/li&gt; &lt;li&gt;Data Privacy: Kiln can't access your data (it runs locally, data stays local); Google stores everything. Kiln can run/train local models (Ollama/Unsloth/LiteLLM); Google always uses their cloud.&lt;/li&gt; &lt;li&gt;Collaboration: Google is single user, while Kiln allows unlimited users/collaboration.&lt;/li&gt; &lt;li&gt;ML Techniques: Google has standard prompting. Kiln has standard prompts, chain-of-thought/reasoning, and auto-prompts (using your dataset for multi-shot).&lt;/li&gt; &lt;li&gt;Dataset management: Google has a table with max 500 rows. Kiln has powerful dataset management for teams with Git sync, tags, unlimited rows, human ratings, and more.&lt;/li&gt; &lt;li&gt;Python Library: Google is UI only. Kiln has a python library for extending it for when you need more than the UI can offer.&lt;/li&gt; &lt;li&gt;Open Source: Google‚Äôs is completely proprietary and private source. Kiln‚Äôs library is MIT open source; the UI isn‚Äôt MIT, but it is 100% source-available, on Github, and free.&lt;/li&gt; &lt;li&gt;Similarities: Both handle structured data well, both have a prompt library, both have similar ‚ÄúRun‚Äù UX, both had user friendly UIs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If anyone wants to check Kiln out, &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;here's the GitHub repository&lt;/a&gt; and &lt;a href="https://docs.getkiln.ai"&gt;docs are here&lt;/a&gt;. Getting started is super easy - it's a one-click install to get setup and running.&lt;/p&gt; &lt;p&gt;I‚Äôm very interested in any feedback or feature requests (model requests, integrations with other tools, etc.) I'm currently working on comprehensive evals, so feedback on what you'd like to see in that area would be super helpful. My hope is to make something as easy to use as G AI Studio, as powerful as Vertex AI, all while open and private.&lt;/p&gt; &lt;p&gt;Thanks in advance! I‚Äôm happy to answer any questions.&lt;/p&gt; &lt;p&gt;Side note: I‚Äôm usually pretty good at competitive research before starting a project. I had looked up Google's &amp;quot;AI Studio&amp;quot; before I started. However, I found and looked at &amp;quot;Vertex AI Studio&amp;quot;, which is a completely different type of product. How one company can have 2 products with almost identical names is beyond me...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ffid/i_accidentally_built_an_open_alternative_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ffid/i_accidentally_built_an_open_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ffid/i_accidentally_built_an_open_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T20:18:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1rgn9</id>
    <title>New model....</title>
    <updated>2025-01-15T06:29:44+00:00</updated>
    <author>
      <name>/u/Many_SuchCases</name>
      <uri>https://old.reddit.com/user/Many_SuchCases</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1rgn9/new_model/"&gt; &lt;img alt="New model...." src="https://preview.redd.it/curwy8vkq3de1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a262daf4d3df2ddd46d444593d7171ed6352a6c5" title="New model...." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Many_SuchCases"&gt; /u/Many_SuchCases &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/curwy8vkq3de1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1rgn9/new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1rgn9/new_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T06:29:44+00:00</published>
  </entry>
</feed>
