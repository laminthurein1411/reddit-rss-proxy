<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-16T22:48:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k0s2cx</id>
    <title>What are some Local search offerings that are competitive with OpenAI/Google, if such a thing can exist?</title>
    <updated>2025-04-16T18:45:08+00:00</updated>
    <author>
      <name>/u/m1tm0</name>
      <uri>https://old.reddit.com/user/m1tm0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0s2cx/what_are_some_local_search_offerings_that_are/"&gt; &lt;img alt="What are some Local search offerings that are competitive with OpenAI/Google, if such a thing can exist?" src="https://a.thumbs.redditmedia.com/VpsLH3HCgZQKXGAOfWOPI0jNF08rLfEmd7yKlAwjEM0.jpg" title="What are some Local search offerings that are competitive with OpenAI/Google, if such a thing can exist?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/8jfl3fj9q8ve1.png?width=2466&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f48aab9b34dd199482fc0248d8f1c320a21f8331"&gt;I was excited to ask about the new models, but only one of those citations were related to my query (pure hallucination otherwise). Also 1 minute for a simple question is totally unacceptable.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kebxiirtr8ve1.jpg?width=1170&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=64e7da8830468d9483cf9f12a246e7f0f7224580"&gt;I asked the same thing to 4o on a different account, with search enabled&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/el0js5oor8ve1.png?width=1350&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=733cb01461f82051e39dd9f821773578bb477064"&gt;~~The right answer was on OpenAI's blog~~&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://openai.com/index/introducing-o3-and-o4-mini/"&gt;https://openai.com/index/introducing-o3-and-o4-mini/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Google was fast and didn't give me any relevant results at all, ChatGPT can't even answer questions about itself, where do I go for information?&lt;/p&gt; &lt;p&gt;EDIT: The right answer was not cited in any of my queries at all:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/s/YH5L1ztLOs"&gt;https://www.reddit.com/r/LocalLLaMA/s/YH5L1ztLOs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you for the answer &lt;a href="/r/LocalLLaMa"&gt;r/LocalLLaMa&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m1tm0"&gt; /u/m1tm0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0s2cx/what_are_some_local_search_offerings_that_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0s2cx/what_are_some_local_search_offerings_that_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0s2cx/what_are_some_local_search_offerings_that_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T18:45:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0mrrt</id>
    <title>Setting Power Limit on RTX 3090 – LLM Test</title>
    <updated>2025-04-16T15:09:42+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0mrrt/setting_power_limit_on_rtx_3090_llm_test/"&gt; &lt;img alt="Setting Power Limit on RTX 3090 – LLM Test" src="https://external-preview.redd.it/Y9Fmw3qDWhZLIrYjGmc0j5sWzi_xA95bLGoO96c8w0g.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=394bf8a00fb680ac3ffcf8e9f923c9c01b9df50e" title="Setting Power Limit on RTX 3090 – LLM Test" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/4KzetHrFHAE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0mrrt/setting_power_limit_on_rtx_3090_llm_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0mrrt/setting_power_limit_on_rtx_3090_llm_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T15:09:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0w7f9</id>
    <title>What is the best option for running eight GPUs in a single motherboard?</title>
    <updated>2025-04-16T21:37:38+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: Can I run 8 GPUs with two 1 to 4 PCIE splitter with bifurcation on my ASUS ROG CROSSHAIR VIII DARK HERO and AMD 5950x? or I need to purchase another motherboard?&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I recently bought eight AMD MI50 32GB GPUs (total of 256 GB VRAM) for experimenting with 100B+ LLMs. However, I am not sure if my motherboard supports 8 GPUs. My motherboard is ASUS ROG CROSSHAIR VIII DARK HERO. It has three PCIE 4.0 x16 slots, one PCIE4.0 x1, and two M.2 PCIE4.0 x4 slots. The CPU is AMD 5950x which has 24 lanes on the CPU. I have 96GB of RAM.&lt;/p&gt; &lt;p&gt;Currently, both M.2 slots are occupied with NVME storage. I also installed three GPUs on all available three PCIE 4.0 x16 slots. Now, my motherboard BIOS shows each GPU is running at x8, x8 (Both MI50 cards) and x4 (RTX 3090).&lt;/p&gt; &lt;p&gt;My question is does this motherboard support 8 GPUs at once if I use PCIE splitter (e.g. 1 PCIE slot to 4 PCIE slots)? I see the user manual says the first PCIE 4.0 x16 slot supports PCIE bifurcation with x4+x4+x4+x4 for M.2 cards. But let's say I install 1 to 4 PCIE splitter on the first and second slot both running at x8. Can I install eight GPUs and run each of them at PCIE4.0 x2 with bifurcation (not sure if I need to purchase some other part other than 1 to 4 splitter for this)?&lt;/p&gt; &lt;p&gt;If not, what is the alternative? I do not want to buy a server for $1000.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T21:37:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0fjny</id>
    <title>InternVL3: Advanced MLLM series just got a major update – InternVL3-14B seems to match the older InternVL2.5-78B in performance</title>
    <updated>2025-04-16T08:37:27+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0fjny/internvl3_advanced_mllm_series_just_got_a_major/"&gt; &lt;img alt="InternVL3: Advanced MLLM series just got a major update – InternVL3-14B seems to match the older InternVL2.5-78B in performance" src="https://b.thumbs.redditmedia.com/0V2Wv1PLXN7erQ2W_9E8spsKPgkJtdm-Gdkx4LeZRCQ.jpg" title="InternVL3: Advanced MLLM series just got a major update – InternVL3-14B seems to match the older InternVL2.5-78B in performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenGVLab released &lt;a href="https://huggingface.co/collections/OpenGVLab/internvl3-67f7f690be79c2fe9d74fe9d"&gt;InternVL3&lt;/a&gt; (HF link) today with a wide range of models, covering a wide parameter count spectrum with a 1B, 2B, 8B, 9B, 14B, 38B and 78B model along with VisualPRM models. These PRM models are &amp;quot;advanced multimodal Process Reward Models&amp;quot; which enhance MLLMs by selecting the best reasoning outputs during a Best-of-N (BoN) evaluation strategy, leading to improved performance across various multimodal reasoning benchmarks.&lt;/p&gt; &lt;p&gt;The scores achieved on OpenCompass suggest that InternVL3-14B is very close in performance to the previous flagship model InternVL2.5-78B while the new InternVL3-78B comes close to Gemini-2.5-Pro. It is to be noted that OpenCompass is a benchmark with a Chinese dataset, so performance in other languages needs to be evaluated separately. Open source is really doing a great job in keeping up with closed source. Thank you OpenGVLab for this release! &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/66ifgifkr5ve1.png?width=2756&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=77650cfe31229f9bde35da3e569cef3d5caa885f"&gt;https://preview.redd.it/66ifgifkr5ve1.png?width=2756&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=77650cfe31229f9bde35da3e569cef3d5caa885f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0fjny/internvl3_advanced_mllm_series_just_got_a_major/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0fjny/internvl3_advanced_mllm_series_just_got_a_major/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0fjny/internvl3_advanced_mllm_series_just_got_a_major/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T08:37:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1k05wpt</id>
    <title>ByteDance releases Liquid model family of multimodal auto-regressive models (like GTP-4o)</title>
    <updated>2025-04-15T23:11:34+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k05wpt/bytedance_releases_liquid_model_family_of/"&gt; &lt;img alt="ByteDance releases Liquid model family of multimodal auto-regressive models (like GTP-4o)" src="https://preview.redd.it/393vjiodz2ve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afb315c5ae73bc479aead0533e99e06cf2db069a" title="ByteDance releases Liquid model family of multimodal auto-regressive models (like GTP-4o)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Architecture Liquid is an auto-regressive model extending from existing LLMs that uses an transformer architecture (similar to GPT-4o imagegen).&lt;/p&gt; &lt;p&gt;Input: text and image. Output: generate text or generated image.&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/Junfeng5/Liquid_V1_7B"&gt;https://huggingface.co/Junfeng5/Liquid_V1_7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;App demo: &lt;a href="https://huggingface.co/spaces/Junfeng5/Liquid_demo"&gt;https://huggingface.co/spaces/Junfeng5/Liquid_demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Personal review: the quality of the image generation is definitely not as good as gpt-4o imagegen. However it’s important as a release due to using an auto-regressive generation paradigm using a single LLM, unlike previous multimodal large language model (MLLM) which used external pretrained visual embeddings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/393vjiodz2ve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k05wpt/bytedance_releases_liquid_model_family_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k05wpt/bytedance_releases_liquid_model_family_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T23:11:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0qw6k</id>
    <title>Open Source tool from OpenAI for Coding Agent in terminal</title>
    <updated>2025-04-16T17:57:46+00:00</updated>
    <author>
      <name>/u/_anotherRandomGuy</name>
      <uri>https://old.reddit.com/user/_anotherRandomGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;repo: &lt;a href="https://github.com/openai/codex"&gt;https://github.com/openai/codex&lt;/a&gt;&lt;br /&gt; Real question is, can we use it with local reasoning models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_anotherRandomGuy"&gt; /u/_anotherRandomGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0qw6k/open_source_tool_from_openai_for_coding_agent_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0qw6k/open_source_tool_from_openai_for_coding_agent_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0qw6k/open_source_tool_from_openai_for_coding_agent_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T17:57:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0xc46</id>
    <title>A fast, native desktop UI for transcribing audio and video using Whisper</title>
    <updated>2025-04-16T22:27:11+00:00</updated>
    <author>
      <name>/u/mehtabmahir</name>
      <uri>https://old.reddit.com/user/mehtabmahir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since my last post, I've added several new features such as batch processing (multiple files at once) and more.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A fast, native desktop UI for transcribing audio and video using Whisper — built entirely in modern C++ and Qt. I’ll be regularly updating it with more features.&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://github.com/mehtabmahir/easy-whisper-ui"&gt;https://github.com/mehtabmahir/easy-whisper-ui&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Batch processing&lt;/strong&gt; — drag in multiple files, select several at once, or use &amp;quot;Open With&amp;quot; on multiple items; they'll run one-by-one automatically.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Installer handles everything&lt;/strong&gt; — downloads dependencies, compiles and optimizes Whisper for your system.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully C++ implementation&lt;/strong&gt; — no Python, no scripts, no CLI fuss.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU acceleration via Vulkan&lt;/strong&gt; — runs fast on AMD, Intel, or NVIDIA.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Drag &amp;amp; drop&lt;/strong&gt;, &lt;strong&gt;Open With&lt;/strong&gt;, or click &amp;quot;Open File&amp;quot; — multiple ways to load media.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Auto-converts&lt;/strong&gt; to &lt;code&gt;.mp3&lt;/code&gt; if needed using FFmpeg.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart conversion&lt;/strong&gt; — skips mp3 conversion if it's already there.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dropdown menus&lt;/strong&gt; to pick model (e.g. &lt;code&gt;tiny&lt;/code&gt;, &lt;code&gt;medium-en&lt;/code&gt;, &lt;code&gt;large-v3&lt;/code&gt;) and language (e.g. &lt;code&gt;en&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Textbox for extra Whisper arguments&lt;/strong&gt; if you want advanced control.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Auto-downloads missing models&lt;/strong&gt; from Hugging Face.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time console output&lt;/strong&gt; while transcription is running.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transcript opens in Notepad&lt;/strong&gt; when finished.&lt;/li&gt; &lt;li&gt;Choose between &lt;code&gt;.txt&lt;/code&gt; &lt;strong&gt;and/or&lt;/strong&gt; &lt;code&gt;.srt&lt;/code&gt; &lt;strong&gt;output&lt;/strong&gt; (with timestamps!).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Requirements&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Windows 10 or later&lt;/li&gt; &lt;li&gt;AMD, Intel, or NVIDIA Graphics Card with Vulkan support (almost all modern GPUs)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Setup&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Download the latest installer from the Releases page.&lt;/li&gt; &lt;li&gt;Run the app — that’s it.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Credits&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;whisper.cpp&lt;/code&gt; by Georgi Gerganov&lt;/li&gt; &lt;li&gt;FFmpeg builds by &lt;a href="http://Gyan.dev"&gt;Gyan.dev&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Built with Qt&lt;/li&gt; &lt;li&gt;Installer created with Inno Setup&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you’ve ever wanted a &lt;strong&gt;simple, native app&lt;/strong&gt; for Whisper that runs fast and handles everything for you — give this a try.&lt;/p&gt; &lt;p&gt;Let me know what you think, I’m actively improving it!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mehtabmahir/easy-whisper-ui/blob/main/preview.png"&gt;preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehtabmahir"&gt; /u/mehtabmahir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0xc46/a_fast_native_desktop_ui_for_transcribing_audio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0xc46/a_fast_native_desktop_ui_for_transcribing_audio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0xc46/a_fast_native_desktop_ui_for_transcribing_audio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T22:27:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0nxlb</id>
    <title>It is almost May of 2025. What do you consider to be the best coding tools?</title>
    <updated>2025-04-16T15:58:33+00:00</updated>
    <author>
      <name>/u/Material_Key7014</name>
      <uri>https://old.reddit.com/user/Material_Key7014</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is almost May of 2025. What do you consider to be the best coding tools? &lt;/p&gt; &lt;p&gt;I would like to get an organic assessment of the community’s choice of IDE and AI tools that successfully helps them in their programming projects. &lt;/p&gt; &lt;p&gt;I’m wondering how many people still use cursor, windsurf especially with the improvements of models vs cost progression over the past few months. &lt;/p&gt; &lt;p&gt;For the people that are into game development, what IDE helps your most for your game projects made in Unity/Godot etc. &lt;/p&gt; &lt;p&gt;Would love to hear everyone’s input. &lt;/p&gt; &lt;p&gt;As for me,&lt;/p&gt; &lt;p&gt;I’m currently find very consistent results in creating a vieriety of small programs with Python using cursor and Gemini 2.5. Before Gemini 2.5 came out, I was using 3.7 Claude, but was really debating with myself on if 3.7 was better than 3.5 as I was getting mixed results. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Material_Key7014"&gt; /u/Material_Key7014 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0nxlb/it_is_almost_may_of_2025_what_do_you_consider_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0nxlb/it_is_almost_may_of_2025_what_do_you_consider_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0nxlb/it_is_almost_may_of_2025_what_do_you_consider_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T15:58:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0haqw</id>
    <title>LocalAI v2.28.0 + Announcing LocalAGI: Build &amp; Run AI Agents Locally Using Your Favorite LLMs</title>
    <updated>2025-04-16T10:41:06+00:00</updated>
    <author>
      <name>/u/mudler_it</name>
      <uri>https://old.reddit.com/user/mudler_it</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/"&gt; &lt;img alt="LocalAI v2.28.0 + Announcing LocalAGI: Build &amp;amp; Run AI Agents Locally Using Your Favorite LLMs" src="https://external-preview.redd.it/kcMkQKbjZGZ_QZiKW5RCtauHq1PUD9qoQ4d1zIJbWdg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc273ae475c76068af1debbaf450c1139cfecac4" title="LocalAI v2.28.0 + Announcing LocalAGI: Build &amp;amp; Run AI Agents Locally Using Your Favorite LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; fam!&lt;/p&gt; &lt;p&gt;Got an update and a pretty exciting announcement relevant to running and &lt;em&gt;using&lt;/em&gt; your local LLMs in more advanced ways. We've just shipped &lt;strong&gt;LocalAI v2.28.0&lt;/strong&gt;, but the bigger news is the launch of &lt;strong&gt;LocalAGI&lt;/strong&gt;, a new platform for building AI agent workflows that leverages your local models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LocalAI (v2.28.0):&lt;/strong&gt; Our open-source inference server (acting as an OpenAI API for backends like llama.cpp, Transformers, etc.) gets updates. Link:&lt;a href="https://github.com/mudler/LocalAI"&gt;https://github.com/mudler/LocalAI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LocalAGI (New!):&lt;/strong&gt; A self-hosted &lt;strong&gt;AI Agent Orchestration platform&lt;/strong&gt; (rewritten in Go) with a WebUI. Lets you build complex agent tasks (think AutoGPT-style) that are powered by &lt;strong&gt;your local LLMs&lt;/strong&gt; via an OpenAI-compatible API. Link:&lt;a href="https://github.com/mudler/LocalAGI"&gt;https://github.com/mudler/LocalAGI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LocalRecall (New-ish):&lt;/strong&gt; A companion local REST API for agent memory. Link:&lt;a href="https://github.com/mudler/LocalRecall"&gt;https://github.com/mudler/LocalRecall&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Key Idea:&lt;/strong&gt; Use your preferred local models (served via LocalAI or another compatible API) as the &amp;quot;brains&amp;quot; for autonomous agents running complex tasks, all locally.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Quick Context: LocalAI as your Local Inference Server&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Many of you know LocalAI as a way to slap an OpenAI-compatible API onto various model backends. You can point it at your GGUF files (using its built-in llama.cpp backend), Hugging Face models, Diffusers for image gen, etc., and interact with them via a standard API, all locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Introducing LocalAGI: Using Your Local LLMs for Agentic Tasks&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is where it gets really interesting for this community. LocalAGI is designed to let you build workflows where AI agents collaborate, use tools, and perform multi-step tasks. It works better with LocalAI as it leverages internal capabilities for structured output, but should work as well with other providers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How does it use&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;your&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;local LLMs?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LocalAGI connects to any &lt;strong&gt;OpenAI-compatible API endpoint&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;You can simply point LocalAGI to your running &lt;strong&gt;LocalAI instance&lt;/strong&gt; (which is serving your Llama 3, Mistral, Mixtral, Phi, or whatever GGUF/HF model you prefer).&lt;/li&gt; &lt;li&gt;Alternatively, if you're using another OpenAI-compatible server (like &lt;code&gt;llama-cpp-python&lt;/code&gt;'s server mode, vLLM's API, etc.), you can likely point LocalAGI to that too.&lt;/li&gt; &lt;li&gt;Your local LLM then becomes the decision-making engine for the agents within LocalAGI.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key Features of LocalAGI:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Runs Locally:&lt;/strong&gt; Like LocalAI, it's designed to run entirely on your hardware. No data leaves your machine.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;WebUI for Management:&lt;/strong&gt; Configure agent roles, prompts, models, tool access, and multi-agent &amp;quot;groups&amp;quot; visually. No drag and drop stuff.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool Usage:&lt;/strong&gt; Allow agents to interact with external tools or APIs (potentially custom local tools too).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Connectors:&lt;/strong&gt; Ready-to-go connectors for Telegram, Discord, Slack, IRC, and more to come.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Persistent Memory:&lt;/strong&gt; Integrates with LocalRecall (also local) for long-term memory capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;API:&lt;/strong&gt; Agents can be created programmatically via API, and every agent can be used via REST-API, providing drop-in replacement for OpenAI's Responses APIs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Go Backend:&lt;/strong&gt; Rewritten in Go for efficiency.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Source (MIT).&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out the UI for configuring agents:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x3ud7tfxd6ve1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=456a83ed666dcf55ba70904126a3df43a7673661"&gt;https://preview.redd.it/x3ud7tfxd6ve1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=456a83ed666dcf55ba70904126a3df43a7673661&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dtjimnfxd6ve1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a06b61976c3e78d3f78bc64e4e9b1eda3864fc38"&gt;https://preview.redd.it/dtjimnfxd6ve1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a06b61976c3e78d3f78bc64e4e9b1eda3864fc38&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/403v7ofxd6ve1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=99880111207fae2064732ed91f6e6c3ca9db9736"&gt;https://preview.redd.it/403v7ofxd6ve1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=99880111207fae2064732ed91f6e6c3ca9db9736&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LocalAI v2.28.0 Updates&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The underlying LocalAI inference server also got some updates:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SYCL support via &lt;code&gt;stablediffusion.cpp&lt;/code&gt; (relevant for some Intel GPUs).&lt;/li&gt; &lt;li&gt;Support for the Lumina Text-to-Image models.&lt;/li&gt; &lt;li&gt;Various backend improvements and bug fixes.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why is this Interesting for&lt;/strong&gt; &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;strong&gt;?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This stack (LocalAI + LocalAGI) provides a way to leverage the powerful local models we all spend time setting up and tuning for more than just chat or single-prompt tasks. You can start building:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Autonomous research agents.&lt;/li&gt; &lt;li&gt;Code generation/debugging workflows.&lt;/li&gt; &lt;li&gt;Content summarization/analysis pipelines.&lt;/li&gt; &lt;li&gt;RAG setups with agentic interaction.&lt;/li&gt; &lt;li&gt;Anything where multiple steps or &amp;quot;thinking&amp;quot; loops powered by your local LLM would be beneficial.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Docker is probably the easiest way to get both LocalAI and LocalAGI running. Check the READMEs in the repos for setup instructions and docker-compose examples. You'll configure LocalAGI with the API endpoint address of your LocalAI (or other compatible) server or just run the complete stack from the docker-compose files.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LocalAI (Inference Server):&lt;/strong&gt;&lt;a href="https://github.com/mudler/LocalAI"&gt;https://github.com/mudler/LocalAI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LocalAGI (Agent Platform):&lt;/strong&gt;&lt;a href="https://github.com/mudler/LocalAGI"&gt;https://github.com/mudler/LocalAGI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LocalRecall (Memory):&lt;/strong&gt;&lt;a href="https://github.com/mudler/LocalRecall"&gt;https://github.com/mudler/LocalRecall&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Release notes:&lt;/strong&gt; &lt;a href="https://github.com/mudler/LocalAI/releases/tag/v2.28.0"&gt;&lt;strong&gt;https://github.com/mudler/LocalAI/releases/tag/v2.28.0&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We believe this combo opens up many possibilities for local LLMs. We're keen to hear your thoughts! Would you try running agents with your local models? What kind of workflows would you build? Any feedback on connecting LocalAGI to different local API servers would also be great.&lt;/p&gt; &lt;p&gt;Let us know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mudler_it"&gt; /u/mudler_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T10:41:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0q0bc</id>
    <title>Hugging Face has launched a reasoning datasets competition with Bespoke Labs and Together AI</title>
    <updated>2025-04-16T17:22:21+00:00</updated>
    <author>
      <name>/u/dvanstrien</name>
      <uri>https://old.reddit.com/user/dvanstrien</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Reasoning datasets currently dominate Hugging Face's trending datasets, but they mostly focus on code and maths. Along with Bespoke Labs and Together AI, we've launched a competition to try and diversify this landscape by encouraging new reasoning datasets focusing on underexplored domains or tasks. &lt;/p&gt; &lt;p&gt;Key details:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create a proof-of-concept dataset (minimum 100 examples)&lt;/li&gt; &lt;li&gt;Upload to Hugging Face Hub with tag &amp;quot;reasoning-datasets-competition&amp;quot;&lt;/li&gt; &lt;li&gt;Deadline: May 1, 2025&lt;/li&gt; &lt;li&gt;Prizes: $3,000+ in cash/credits&lt;/li&gt; &lt;li&gt;All participants get $50 in &lt;a href="http://Together.ai"&gt;Together.ai&lt;/a&gt; API credits&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We welcome datasets in various domains (e.g., legal, financial, literary, ethics) and novel tasks (e.g., structured data extraction, zero-shot classification). We're also interested in datasets supporting the broader &amp;quot;reasoning ecosystem.&amp;quot;&lt;/p&gt; &lt;p&gt;For inspiration, I made my own proof of concept dataset &lt;a href="https://huggingface.co/datasets/davanstrien/fine-reasoning-questions"&gt;davanstrien/fine-reasoning-questions&lt;/a&gt;, which generates reasoning questions from web text using a pipeline approach. First, I trained a smaller ModernBERT-based classifier to identify texts that require complex reasoning, then filtered FineWeb-Edu content based on reasoning scores, classified topics, and finally used Qwen/QWQ-32B to generate the reasoning questions. I hope this approach demonstrates how you can create domain-focused reasoning datasets without starting from scratch/needing a ton of GPUs. &lt;/p&gt; &lt;p&gt;Full details: &lt;a href="https://huggingface.co/blog/bespokelabs/reasoning-datasets-competition"&gt;https://huggingface.co/blog/bespokelabs/reasoning-datasets-competition&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dvanstrien"&gt; /u/dvanstrien &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0q0bc/hugging_face_has_launched_a_reasoning_datasets/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0q0bc/hugging_face_has_launched_a_reasoning_datasets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0q0bc/hugging_face_has_launched_a_reasoning_datasets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T17:22:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0b8wx</id>
    <title>Yes, you could have 160gb of vram for just about $1000.</title>
    <updated>2025-04-16T03:46:47+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please see my original post that posted about this journey - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This will be up to par to readily beat DIGITs and the AMD MAX AI integrated 128gb systems.... &lt;/p&gt; &lt;p&gt;Sorry, I'm going to dump this before I get busy for anyone that might find it useful. So I bought 10 MI50 gpus for $90 each $900. Octominer case for $100. But I did pay $150 for the shipping and $6 tax for the case. So there you go $1156. I also bought a PCIe ethernet card for 99cents. $1157.&lt;/p&gt; &lt;p&gt;Octominer XULTRA 12 has 12 PCIe slots, it's designed for mining, it has weak celeron CPU, the one I got has only 4gb of ram. But it works and is a great system for low budget GPU inference workload.&lt;/p&gt; &lt;p&gt;I took out the SSD drive and threw an old 250gb I had lying around and installed Ubuntu. Got the cards working, went with rocm. vulkan was surprising a bit problematic, and rocm was easy once I figured out. Blew up the system the first attempt and had to reinstall for anyone curious, I installed 24.04 ubuntu, MI50 is no longer supported on the latest roc 6.4.0, but you can install 6.3.0 so I did that. Built llama.cpp from source, and tried a few models. I'll post data later.&lt;/p&gt; &lt;p&gt;Since the card has 12 slots, it has 1 8 pin for each slot, for a total of 12 cables. The cards have 2 8 pin each, so I had a choice, use an 8 pin to dual 8 pin cable or 2 to 1. To play it safe for starters, I did 2 to 1. For a total of 6 cards installed. The cards also supposedly have a peak of 300watts, so 10 cards would be 3000 watts. I have 3 power supplies of 750watts for a total of 2250watts. The cool thing about the power supply is that it's hot swappable, I can plug in and take out while it's running. You don't need all 3 to run, only 1. The good news is that this thing doesn't draw power! The cards are a bit high idle at about 20watts, so 6 cards 120watts, system idles really at &amp;lt; 130 watts. I'm measuring at the outlet with an electrical measurement meter. During inference across the cards, peak was about 340watt. I'm using llama.cpp so inference is serial and not parallel. You can see the load move from one card to the other. This as you can guess is &amp;quot;inefficient&amp;quot; so llama.cpp is not as far as say using vLLM with tensor parallel. But it does support multi users, so you can push it by running parallel requests if you are sharing the rig with others, running agents or custom code. In such a situation, you can have the cards all max out. I didn't power limit the cards, system reports them at 250watts, I saw about 230watt max while inferring.&lt;/p&gt; &lt;p&gt;The case fan at 100% sounds like a jet engine, but the great thing is they are easy to control and at 10% you can't hear it. The cards run cooler than my Nvidia cards that are on an open rig, my Nvidia cards idle at 30-40C, these cards idle in the 20C range with 5% fan. I can't hear the fan until about 25% and it's very quiet and blends in. It takes about 50-60% before anyone that walks into the room will notice.&lt;/p&gt; &lt;p&gt;I just cut and paste and took some rough notes, I don't have any blogs or anything to sell, just sharing for those that might be interested. One of the cards seems to have issue. llama.cpp crashes when I try to use it both local and via RPC. I'll swap and move it around to see if it makes a difference. I have 2 other rigs, llama.cpp won't let me infer across more than 16 cards.&lt;/p&gt; &lt;p&gt;I'm spending time trying to figure it out, updated the *_MAX_DEVICES and MAX_BACKENDS, MAX_SERVERS in code from 16 to 32, it sometimes works. I did build with -DGGML_SCHED_MAX_BACKENDS=48 makes no difference. So if you have any idea, let me know. :)&lt;/p&gt; &lt;p&gt;Now on power and electricity. Save it, don't care. With that said, the box idles at about 120watts, my other rigs probably idle more. Between the 3 rigs, maybe idle of 600watts. I have experimented with &amp;quot;wake on lan&amp;quot; That means I can suspend the machines and then wake them up remotely. One of my weekend plans is to put a daemon that will monitor the GPUs and system, if idle and nothing going on for 30 minutes. Hibernate the system, when I'm ready to use them wake them up remotely. Do this for all rig and don't keep them running. I don't know how loaded models will behave, my guess is that it would need to be reloaded, it's &amp;quot;vram&amp;quot; aka &amp;quot;RAM&amp;quot; after all, and unlike system ram that gets saved to disk, GPU doesn't. I'm still shocked at the low power use.&lt;/p&gt; &lt;p&gt;So on PCIe electrical x1 speed. I read it was 1GBps, but hey, there's a difference from 1Gbps and that. So PCie3x1 is capable of 985 MB/s. My network cards are 1Gbps which are more around 125 MB/s. So upgrading to a 10Gbps network should theoretically allow for much faster load. 7x. In practice, I think it would be less. llama.cpp hackers are just programmers getting it done by any means necessary, the goal is to infer models not the best program, from my wandering around the rpc code today and observed behavior it's not that performant. So if you're into unix network programming and wanna contribute, that would be a great area. ;-)&lt;/p&gt; &lt;p&gt;With all this said, yes, for a just about $1000, 160gb of vram is sort of possible. There was a lot of MI50 on ebay and I suppose some other hawks saw them as well and took their chance so it's sold out. Keep your eyes out for deals. I even heard I didn't get the best deal, some lucky sonomabbb got the MI50's that were 32gb. It might just be that companies might start replacing more of their old cards and we will see more of these or even better ones. Don't be scared, don't worry about that mess of you need a power plant and it's no longer supported. Most of the things folks argued about on here are flat out wrong from my practical experience, so risk it all.&lt;/p&gt; &lt;p&gt;Oh yeah, largest model I did run was llama405b, and had it write code and was getting about 2tk/s. Yes it's a large dense model. It would perform the worse, MoE like deepseekv3, llama4 are going to fly. I'll get some numbers up on those if I remember to.&lt;/p&gt; &lt;p&gt;Future stuff.&lt;br /&gt; Decide if I'm going to pack all the GPUs in one server or another server. From the load observed today, one server will handle it fine. Unlike newer Nvidia GPUs with cable going in from the top, this one has the cables going in from the back and it's quite a tight fit to get in. PCI standards from what I understand expect cards to pull a max of 75w and an 8pin cable can supply 150w, for a max of 225w. So I could power them with a single cable, figure out how to limit power to 200w and be good to go. As a matter of fact, some of the cables had those adapter and I took them out. I saw a video of a crypto bro running an Octominer with 3080s and those have more power demand than MI50s.&lt;/p&gt; &lt;p&gt;Here goes data from my notes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;llama3.1-8b-instruct-q8&lt;/strong&gt; inference, same prompt, same seed&lt;/p&gt; &lt;pre&gt;&lt;code&gt;MI50 local &amp;gt; llama_perf_sampler_print: sampling time = 141.03 ms / 543 runs ( 0.26 ms per token, 3850.22 tokens per second) llama_perf_context_print: load time = 164330.99 ms *** SSD through PCIe3x1 slot*** llama_perf_context_print: prompt eval time = 217.66 ms / 42 tokens ( 5.18 ms per token, 192.97 tokens per second) llama_perf_context_print: eval time = 12046.14 ms / 500 runs ( 24.09 ms per token, 41.51 tokens per second) llama_perf_context_print: total time = 18773.63 ms / 542 tokens 3090 local &amp;gt; llama_perf_context_print: load time = 3088.11 ms *** NVME through PCIex16 *** llama_perf_context_print: prompt eval time = 27.76 ms / 42 tokens ( 0.66 ms per token, 1512.91 tokens per second) llama_perf_context_print: eval time = 6472.99 ms / 510 runs ( 12.69 ms per token, 78.79 tokens per second) 3080ti local &amp;gt; llama_perf_context_print: prompt eval time = 41.82 ms / 42 tokens ( 1.00 ms per token, 1004.26 tokens per second) llama_perf_context_print: eval time = 5976.19 ms / 454 runs ( 13.16 ms per token, 75.97 tokens per second) 3060 local &amp;gt; llama_perf_sampler_print: sampling time = 392.98 ms / 483 runs ( 0.81 ms per token, 1229.09 tokens per second) llama_perf_context_print: eval time = 12351.84 ms / 440 runs ( 28.07 ms per token, 35.62 tokens per second) p40 local &amp;gt; llama_perf_context_print: prompt eval time = 95.65 ms / 42 tokens ( 2.28 ms per token, 439.12 tokens per second) llama_perf_context_print: eval time = 12083.73 ms / 376 runs ( 32.14 ms per token, 31.12 tokens per second) MI50B local *** different GPU from above, consistent *** llama_perf_context_print: prompt eval time = 229.34 ms / 42 tokens ( 5.46 ms per token, 183.14 tokens per second) llama_perf_context_print: eval time = 12186.78 ms / 500 runs ( 24.37 ms per token, 41.03 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you are paying attention MI50s are not great at prompt processing.&lt;/p&gt; &lt;p&gt;a little bit larger context, demonstrates that MI50 sucks at prompt processing... and demonstrating performance over RPC. I got these to see if I could use them via RPC for very huge models.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;p40 local llama_perf_context_print: prompt eval time = 512.56 ms / 416 tokens ( 1.23 ms per token, 811.61 tokens per second) llama_perf_context_print: eval time = 12582.57 ms / 370 runs ( 34.01 ms per token, 29.41 tokens per second) 3060 local llama_perf_context_print: prompt eval time = 307.63 ms / 416 tokens ( 0.74 ms per token, 1352.27 tokens per second) llama_perf_context_print: eval time = 10149.66 ms / 357 runs ( 28.43 ms per token, 35.17 tokens per second) 3080ti local llama_perf_context_print: prompt eval time = 141.43 ms / 416 tokens ( 0.34 ms per token, 2941.45 tokens per second) llama_perf_context_print: eval time = 6079.14 ms / 451 runs ( 13.48 ms per token, 74.19 tokens per second) 3090 local llama_perf_context_print: prompt eval time = 140.91 ms / 416 tokens ( 0.34 ms per token, 2952.30 tokens per second) llama_perf_context_print: eval time = 4170.36 ms / 314 runs ( 13.28 ms per token, 75.29 tokens per second MI50 local llama_perf_context_print: prompt eval time = 1391.44 ms / 416 tokens ( 3.34 ms per token, 298.97 tokens per second) llama_perf_context_print: eval time = 8497.04 ms / 340 runs ( 24.99 ms per token, 40.01 tokens per second) MI50 over RPC (1GPU) llama_perf_context_print: prompt eval time = 1177.23 ms / 416 tokens ( 2.83 ms per token, 353.37 tokens per second) llama_perf_context_print: eval time = 16800.55 ms / 340 runs ( 49.41 ms per token, 20.24 tokens per second) MI50 over RPC (2xGPU) llama_perf_context_print: prompt eval time = 1400.72 ms / 416 tokens ( 3.37 ms per token, 296.99 tokens per second) llama_perf_context_print: eval time = 17539.33 ms / 340 runs ( 51.59 ms per token, 19.39 tokens per second) MI50 over RPC (3xGPU) llama_perf_context_print: prompt eval time = 1562.64 ms / 416 tokens ( 3.76 ms per token, 266.22 tokens per second) llama_perf_context_print: eval time = 18325.72 ms / 340 runs ( 53.90 ms per token, 18.55 tokens per second) p40 over RPC (3xGPU) llama_perf_context_print: prompt eval time = 968.91 ms / 416 tokens ( 2.33 ms per token, 429.35 tokens per second) llama_perf_context_print: eval time = 22888.16 ms / 370 runs ( 61.86 ms per token, 16.17 tokens per second) MI50 over RPC (5xGPU) (1 token a second loss for every RPC?) llama_perf_context_print: prompt eval time = 1955.87 ms / 416 tokens ( 4.70 ms per token, 212.69 tokens per second) llama_perf_context_print: eval time = 22217.03 ms / 340 runs ( 65.34 ms per token, 15.30 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;max inference over RPC observed with rocm-smi was 100w, lower than when running locally, saw 240w&lt;/p&gt; &lt;p&gt;max watt observed at outlet before RPC was 361w, max watt after 361w&lt;/p&gt; &lt;p&gt;&lt;strong&gt;llama-70b-q8&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;if you want to approximate how fast it will run in q4, just multiple by 2. This was done with llama.cpp, yes vLLM is faster, someone already did q4 llama8 with vLLM and tensor parallel for 25tk/s&lt;/p&gt; &lt;pre&gt;&lt;code&gt;3090 5xGPU llama-70b llama_perf_context_print: prompt eval time = 785.20 ms / 416 tokens ( 1.89 ms per token, 529.80 tokens per second) llama_perf_context_print: eval time = 26483.01 ms / 281 runs ( 94.25 ms per token, 10.61 tokens per second) llama_perf_context_print: total time = 133787.93 ms / 756 tokens MI50 over RPC (5xGPU) llama-70b llama_perf_context_print: prompt eval time = 11841.23 ms / 416 tokens ( 28.46 ms per token, 35.13 tokens per second) llama_perf_context_print: eval time = 84088.80 ms / 415 runs ( 202.62 ms per token, 4.94 tokens per second) llama_perf_context_print: total time = 101548.44 ms / 831 tokens RPC across 17GPUs, 6 main 3090l and 11 remote GPUs (3090, 3080ti,3060, 3xP40, 5xMI50) true latency test llama_perf_context_print: prompt eval time = 8172.69 ms / 416 tokens ( 19.65 ms per token, 50.90 tokens per second) llama_perf_context_print: eval time = 74990.44 ms / 345 runs ( 217.36 ms per token, 4.60 tokens per second) llama_perf_context_print: total time = 556723.90 ms / 761 tokens Misc notes idle watt at outlet = 126watts temp about 25-27C across GPUs idle power across individual 21-26watts powercap - 250watts inference across 3GPUs at outlet - 262watts highest power on one GPU = 223W at 10% speed, fan got to 60C, at 20% speed highest is 53C while GPU is active. turned up to 100% it brought the GPUs down to high 20's in under 2 minutes &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T03:46:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0kzgn</id>
    <title>the budget rig goes bigger, 5060tis bought! test results incoming tonight</title>
    <updated>2025-04-16T13:53:42+00:00</updated>
    <author>
      <name>/u/gaspoweredcat</name>
      <uri>https://old.reddit.com/user/gaspoweredcat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;well after my experiments with mining GPUs i was planning to build out my rig with some chinese modded 3080ti mobile cards with 16gb which came in at like £330 which at the time seemed a bargain. but then today i noticed the 5060i dropped at only £400 for 16gb! i was fully expecting to see them be £500 a card. luckily im very close to a major computer retailer so im heading to collect a pair of them this afternoon!&lt;/p&gt; &lt;p&gt;come back to this thread later for some info on how these things perform with LLMs. they could/should be an absolute bargain for local rigs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gaspoweredcat"&gt; /u/gaspoweredcat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0kzgn/the_budget_rig_goes_bigger_5060tis_bought_test/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0kzgn/the_budget_rig_goes_bigger_5060tis_bought_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0kzgn/the_budget_rig_goes_bigger_5060tis_bought_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T13:53:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0r9pi</id>
    <title>Llama.cpp has much higher generation quality for Gemma 3 27B on M4 Max</title>
    <updated>2025-04-16T18:12:44+00:00</updated>
    <author>
      <name>/u/IonizedRay</name>
      <uri>https://old.reddit.com/user/IonizedRay</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When running the llama.cpp WebUI with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server -m Gemma-3-27B-Instruct-Q6_K.gguf \ --seed 42 \ --mlock \ --n-gpu-layers -1 \ --ctx-size 8096 \ --port 10000 \ --temp 1.0 \ --top-k 64 \ --top-p 0.95 \ --min-p 0.0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And running Ollama trough OpenWebUI using the same temp, top-p, top-k, min-p, i get incredibly worse quality.&lt;/p&gt; &lt;p&gt;For example when i ask to add a feature to a python script, llama.cpp correctly adds the piece of code needed without any unnecessary edit, while Ollama completely rewrites the script, making a lot of stupid syntax mistakes that are so bad that the linter catches tons of them even before running it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IonizedRay"&gt; /u/IonizedRay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T18:12:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0odhq</id>
    <title>KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...</title>
    <updated>2025-04-16T16:16:39+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/"&gt; &lt;img alt="KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say..." src="https://external-preview.redd.it/E0QrtLGdAenlhx0dgrRxQhYXEHxRQVilnk0OkkkKL-M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7a4d66aac5a95d9f7e7eefde94e0ec3332c0946" title="KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/py5Tvae.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T16:16:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0c40c</id>
    <title>We GRPO-ed a Model to Keep Retrying 'Search' Until It Found What It Needed</title>
    <updated>2025-04-16T04:38:13+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0c40c/we_grpoed_a_model_to_keep_retrying_search_until/"&gt; &lt;img alt="We GRPO-ed a Model to Keep Retrying 'Search' Until It Found What It Needed" src="https://external-preview.redd.it/OTVoem9nbmRsNHZlMRZyoyYKNpzPJZZUnGrUtyeCYi3ToyFLi7JPjGL-ftCw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b12cd479c2024bd0aed4acb204f01a7a4780624" title="We GRPO-ed a Model to Keep Retrying 'Search' Until It Found What It Needed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, it's Menlo Research again, and today we’d like to introduce a new paper from our team related to search.&lt;/p&gt; &lt;p&gt;Have you ever felt that when searching on Google, &lt;strong&gt;you know for sure there’s no way you’ll get the result you want on the first try&lt;/strong&gt; (you’re already mentally prepared for 3-4 attempts)? ReZero, which we just trained, is based on this very idea.&lt;/p&gt; &lt;p&gt;We used GRPO and tool-calling to train a model with a retry_reward and tested whether, if we made the model &amp;quot;work harder&amp;quot; and be more diligent, it could actually perform better.&lt;/p&gt; &lt;p&gt;Normally when training LLMs, repetitive actions are something people want to avoid, because they’re thought to cause hallucinations - maybe. But the results from ReZero are pretty interesting. We got a performance score of &lt;strong&gt;46%&lt;/strong&gt;, compared to just &lt;strong&gt;20%&lt;/strong&gt; from a baseline model trained the same way. So that gives us some evidence that &lt;strong&gt;Repetition is not hallucination.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There are a few ideas for application. The model could act as an abstraction layer over the main LLM loop, so that the main LLM can search better. Or simply an abstraction layer on top of current search engines to help you generate more relevant queries - a query generator - perfect for research use cases.&lt;/p&gt; &lt;p&gt;Attached a demo in the clip.&lt;/p&gt; &lt;p&gt;(The beginning has a little meme to bring you some laughs 😄 - Trust me ReZero is Retry and Zero from Deepseek-zero)&lt;/p&gt; &lt;p&gt;Links to the paper/data below:&lt;/p&gt; &lt;p&gt;paper: &lt;a href="https://arxiv.org/abs/2504.11001"&gt;https://arxiv.org/abs/2504.11001&lt;/a&gt;&lt;br /&gt; huggingface: &lt;a href="https://huggingface.co/Menlo/ReZero-v0.1-llama-3.2-3b-it-grpo-250404"&gt;https://huggingface.co/Menlo/ReZero-v0.1-llama-3.2-3b-it-grpo-250404&lt;/a&gt;&lt;br /&gt; github: &lt;a href="https://github.com/menloresearch/ReZero"&gt;https://github.com/menloresearch/ReZero&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; As much as we want to make this model perfect, we are well aware of its limitations, specifically about training set and a bit poor design choice of reward functions. However we decided to release the model anyway, because it's better for the community to have access and play with it (also our time budget for this research is already up).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x9c46kt8l4ve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0c40c/we_grpoed_a_model_to_keep_retrying_search_until/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0c40c/we_grpoed_a_model_to_keep_retrying_search_until/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T04:38:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0iu5z</id>
    <title>Announcing RealHarm: A Collection of Real-World Language Model Application Failure</title>
    <updated>2025-04-16T12:10:26+00:00</updated>
    <author>
      <name>/u/chef1957</name>
      <uri>https://old.reddit.com/user/chef1957</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm David from&lt;a href="https://giskard.ai"&gt; Giskard&lt;/a&gt;, and we work on securing Agents.&lt;/p&gt; &lt;p&gt;Today, we are announcing &lt;strong&gt;RealHarm&lt;/strong&gt;: a dataset of &lt;em&gt;real-world&lt;/em&gt; problematic interactions with &lt;strong&gt;AI agents&lt;/strong&gt;, drawn from publicly reported incidents.&lt;/p&gt; &lt;p&gt;Most of the research on AI harms is focused on theoretical risks or regulatory guidelines. But the real-world failure modes are often different—and much messier.&lt;/p&gt; &lt;p&gt;With RealHarm, we collected and annotated hundreds of incidents involving deployed language models, using an evidence-based taxonomy for understanding and addressing the AI risks. We did so by analyzing the cases through the lens of &lt;em&gt;deployers&lt;/em&gt;—the companies or teams actually shipping LLMs—and we found some surprising results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Reputational damage&lt;/strong&gt; was the most common organizational harm.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Misinformation and hallucination&lt;/strong&gt; were the most frequent hazards&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;State-of-the-art guardrails have failed&lt;/strong&gt; to catch many of the incidents. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We hope this dataset can help researchers, developers, and product teams better understand, test, and prevent real-world harms.&lt;/p&gt; &lt;p&gt;The paper and dataset: &lt;a href="https://realharm.giskard.ai/"&gt;https://realharm.giskard.ai/&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We'd love feedback, questions, or suggestions—especially if you're deploying LLMs and have real harmful scenarios.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chef1957"&gt; /u/chef1957 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0iu5z/announcing_realharm_a_collection_of_realworld/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0iu5z/announcing_realharm_a_collection_of_realworld/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0iu5z/announcing_realharm_a_collection_of_realworld/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T12:10:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0qbme</id>
    <title>o4-mini is 186ᵗʰ best coder, sleep well platter! Enjoy retirement!</title>
    <updated>2025-04-16T17:34:46+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0qbme/o4mini_is_186ᵗʰ_best_coder_sleep_well_platter/"&gt; &lt;img alt="o4-mini is 186ᵗʰ best coder, sleep well platter! Enjoy retirement!" src="https://preview.redd.it/0p5ymcc7g8ve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d96f80ad111301c2dfe2b713b55f0121905d377" title="o4-mini is 186ᵗʰ best coder, sleep well platter! Enjoy retirement!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0p5ymcc7g8ve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0qbme/o4mini_is_186ᵗʰ_best_coder_sleep_well_platter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0qbme/o4mini_is_186ᵗʰ_best_coder_sleep_well_platter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T17:34:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0qisr</id>
    <title>OpenAI introduces codex: a lightweight coding agent that runs in your terminal</title>
    <updated>2025-04-16T17:42:48+00:00</updated>
    <author>
      <name>/u/MorroWtje</name>
      <uri>https://old.reddit.com/user/MorroWtje</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0qisr/openai_introduces_codex_a_lightweight_coding/"&gt; &lt;img alt="OpenAI introduces codex: a lightweight coding agent that runs in your terminal" src="https://external-preview.redd.it/L2s8FUcxTxmbnY9A3xFNDqLsOD-NZikx1UTncO36YW4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9814fd7577317ca58f6bc696ee800e0ebe489eab" title="OpenAI introduces codex: a lightweight coding agent that runs in your terminal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MorroWtje"&gt; /u/MorroWtje &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/openai/codex"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0qisr/openai_introduces_codex_a_lightweight_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0qisr/openai_introduces_codex_a_lightweight_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T17:42:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0p3h0</id>
    <title>Results of Ollama Leakage</title>
    <updated>2025-04-16T16:46:35+00:00</updated>
    <author>
      <name>/u/zxbsmk</name>
      <uri>https://old.reddit.com/user/zxbsmk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0p3h0/results_of_ollama_leakage/"&gt; &lt;img alt="Results of Ollama Leakage" src="https://preview.redd.it/kl4bv7ne78ve1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52549e31655556f832850c261393e3623b27e4f3" title="Results of Ollama Leakage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many servers still seem to be missing basic security.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.freeollama.com/"&gt;https://www.freeollama.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zxbsmk"&gt; /u/zxbsmk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kl4bv7ne78ve1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0p3h0/results_of_ollama_leakage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0p3h0/results_of_ollama_leakage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T16:46:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0kape</id>
    <title>Price vs LiveBench Performance of non-reasoning LLMs</title>
    <updated>2025-04-16T13:22:09+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0kape/price_vs_livebench_performance_of_nonreasoning/"&gt; &lt;img alt="Price vs LiveBench Performance of non-reasoning LLMs" src="https://preview.redd.it/eiojps9w67ve1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d129a188635d4a6845ab6a526591d280f4cd4c30" title="Price vs LiveBench Performance of non-reasoning LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eiojps9w67ve1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0kape/price_vs_livebench_performance_of_nonreasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0kape/price_vs_livebench_performance_of_nonreasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T13:22:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0h641</id>
    <title>Droidrun is now Open Source</title>
    <updated>2025-04-16T10:32:33+00:00</updated>
    <author>
      <name>/u/Sleyn7</name>
      <uri>https://old.reddit.com/user/Sleyn7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0h641/droidrun_is_now_open_source/"&gt; &lt;img alt="Droidrun is now Open Source" src="https://preview.redd.it/9zbo1emvc6ve1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9650416ab69f13bb7190fc4810e3ec5984d6be6d" title="Droidrun is now Open Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, Wow! Just a couple of days ago, I posted here about Droidrun and the response was incredible – we had over 900 people sign up for the waitlist! Thank you all so much for the interest and feedback.&lt;/p&gt; &lt;p&gt;Well, the wait is over! We're thrilled to announce that the Droidrun framework is now public and open-source on GitHub!&lt;/p&gt; &lt;p&gt;GitHub Repo: &lt;a href="https://github.com/droidrun/droidrun"&gt;https://github.com/droidrun/droidrun&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks again for your support. Let's keep on running&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sleyn7"&gt; /u/Sleyn7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9zbo1emvc6ve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0h641/droidrun_is_now_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0h641/droidrun_is_now_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T10:32:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0pnvl</id>
    <title>OpenAI Introducing OpenAI o3 and o4-mini</title>
    <updated>2025-04-16T17:08:52+00:00</updated>
    <author>
      <name>/u/stocksavvy_ai</name>
      <uri>https://old.reddit.com/user/stocksavvy_ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, OpenAI releasing OpenAI &lt;strong&gt;o3&lt;/strong&gt; and &lt;strong&gt;o4-mini,&lt;/strong&gt; the latest o-series of models trained to think for longer before responding. These are the smartest models they've released to date, representing a step change in ChatGPT's capabilities for everyone from curious users to advanced researchers. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stocksavvy_ai"&gt; /u/stocksavvy_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openai.com/index/introducing-o3-and-o4-mini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0pnvl/openai_introducing_openai_o3_and_o4mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0pnvl/openai_introducing_openai_o3_and_o4mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T17:08:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0tkca</id>
    <title>Massive 5000 tokens per second on 2x3090</title>
    <updated>2025-04-16T19:47:07+00:00</updated>
    <author>
      <name>/u/woozzz123</name>
      <uri>https://old.reddit.com/user/woozzz123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0tkca/massive_5000_tokens_per_second_on_2x3090/"&gt; &lt;img alt="Massive 5000 tokens per second on 2x3090" src="https://b.thumbs.redditmedia.com/Kqc4r4j1pvS-lOt8Ugi0fd-cS_ZlQgpSRkB-O5FUESc.jpg" title="Massive 5000 tokens per second on 2x3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For research purposes I need to process huge amounts of data as quickly as possible.&lt;/p&gt; &lt;h1&gt;The model&lt;/h1&gt; &lt;p&gt;Did testing across models, and it came to be that Qwen2.5-7B is &amp;quot;just good enough&amp;quot;. Bigger ones are better but slower. The two tests which were indicative were MMLU-pro (language understanding) and BBH (a bunch of tasks &lt;a href="https://github.com/google/BIG-bench/blob/main/bigbench/benchmark%5C_tasks/keywords%5C_to%5C_tasks.md#summary-table"&gt;https://github.com/google/BIG-bench/blob/main/bigbench/benchmark\_tasks/keywords\_to\_tasks.md#summary-table&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mcb690qly8ve1.png?width=692&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfc9f267cd65168feae2650b4af56a0c1ac5370f"&gt;https://preview.redd.it/mcb690qly8ve1.png?width=692&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfc9f267cd65168feae2650b4af56a0c1ac5370f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Intuitively, you can see that the jumps in performance gets smaller and smaller the bigger the models you pick.&lt;/p&gt; &lt;h1&gt;Processing engine&lt;/h1&gt; &lt;p&gt;There will be lots of small queries, so vLLM makes sense, but I used Aphrodite engine due to tests with speculative decoding.&lt;/p&gt; &lt;h1&gt;Model Quantization&lt;/h1&gt; &lt;p&gt;Now, with 2x 3090's theres plenty of VRAM, so there shouldn't be any issue running it, however I was thinking of perhaps a larger KV cache or whatever might increase processing speed. It indeed did, on a test dataset of randomly selected documents, these were the results;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Prompt throughput t/s&lt;/th&gt; &lt;th align="left"&gt;Generation throughput t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Unquantized&lt;/td&gt; &lt;td align="left"&gt;1000&lt;/td&gt; &lt;td align="left"&gt;300&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AWQ / GPTQ&lt;/td&gt; &lt;td align="left"&gt;1300&lt;/td&gt; &lt;td align="left"&gt;400&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;W4A16-G128 / W8A8&lt;/td&gt; &lt;td align="left"&gt;2000&lt;/td&gt; &lt;td align="left"&gt;500&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Performance of AWQ / GTPQ and W4A16-G128 was very similar in terms of MMLU &amp;amp; BBH, however W8A8 was clearly superior (using llm_eval);&lt;/p&gt; &lt;p&gt;&lt;code&gt;lm_eval --model vllm \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--model_args YOUR_MODEL,add_bos_token=true \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--tasks TASKHERE \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--num_fewshot 3 for BBH, 5 for MMLU_PRO\&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--batch_size 'auto'&lt;/code&gt;&lt;/p&gt; &lt;p&gt;So, I continued with the W8A8&lt;/p&gt; &lt;h1&gt;Speculative Decoding&lt;/h1&gt; &lt;p&gt;Unfortunately, 7B has a different tokenizer than the smaller models, so I cannot use 0.5, 1.5 or 3B as draft model. Aphrodite supports speculative decoding through ngram, but this rougly halves performance &lt;a href="https://aphrodite.pygmalion.chat/spec-decoding/ngram/"&gt;https://aphrodite.pygmalion.chat/spec-decoding/ngram/&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Final optimizations&lt;/h1&gt; &lt;p&gt;Here's the command to run an OpenAI REST API:&lt;/p&gt; &lt;p&gt;&lt;code&gt;aphrodite run ./Qwen2.5-7B-Instruct_W8A8_custom --port 8000 -tp 2 --max_seq_len 8192 --max_model_len 8192 --max_num_seqs 32 --tensor-parallel-size 2 --gpu-memory-utilization 0.75&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Note the parameter &amp;quot;&lt;code&gt;max_num_seqs&lt;/code&gt;&amp;quot; , this is the number of concurrent requests in a batch, how many requests the GPU processes at the same time. I did some benchmarking on my test set and got this results:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;max_num_seqs&lt;/th&gt; &lt;th align="left"&gt;ingest t/s&lt;/th&gt; &lt;th align="left"&gt;generate&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;64&lt;/td&gt; &lt;td align="left"&gt;1000&lt;/td&gt; &lt;td align="left"&gt;200&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;3000&lt;/td&gt; &lt;td align="left"&gt;1000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;2500&lt;/td&gt; &lt;td align="left"&gt;750&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;They fluctuate so these are a ballpark, but the difference is clear if you run it. I chose the 32 one. Running things then in &amp;quot;production&amp;quot;:&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pe7vam5q29ve1.png?width=725&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=91cd4c10ab713481d093c43cd83ad4d160be6fa5"&gt;https://preview.redd.it/pe7vam5q29ve1.png?width=725&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=91cd4c10ab713481d093c43cd83ad4d160be6fa5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4500 t/s ingesting&lt;/p&gt; &lt;p&gt;825 t/s generation&lt;/p&gt; &lt;p&gt;with +- 5k tokens context.&lt;/p&gt; &lt;p&gt;I think even higher numbers are possible, perhaps quantized KV, better grouping of documents so KV cache gets used more? Smaller context size. However, this speed is sufficient for me, so no more finetuning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/woozzz123"&gt; /u/woozzz123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0tkca/massive_5000_tokens_per_second_on_2x3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0tkca/massive_5000_tokens_per_second_on_2x3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0tkca/massive_5000_tokens_per_second_on_2x3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T19:47:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0mesv</id>
    <title>IBM Granite 3.3 Models</title>
    <updated>2025-04-16T14:54:48+00:00</updated>
    <author>
      <name>/u/suitable_cowboy</name>
      <uri>https://old.reddit.com/user/suitable_cowboy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0mesv/ibm_granite_33_models/"&gt; &lt;img alt="IBM Granite 3.3 Models" src="https://external-preview.redd.it/Di-LJPiKH5-hlOr8JOzFQOIzNY3wtbEXkzZj38FaUy4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cea64ef12850fb58da12ba852867d09166207a09" title="IBM Granite 3.3 Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;&lt;a href="https://www.ibm.com/new/announcements/ibm-granite-3-3-speech-recognition-refined-reasoning-rag-loras"&gt;Announcement Post&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-speech-3.3-8b"&gt;3.3 Speech Model&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/suitable_cowboy"&gt; /u/suitable_cowboy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-33-language-models-67f65d0cca24bcbd1d3a08e3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0mesv/ibm_granite_33_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0mesv/ibm_granite_33_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T14:54:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0u8ew</id>
    <title>Somebody needs to tell Nvidia to calm down with these new model names.</title>
    <updated>2025-04-16T20:14:27+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0u8ew/somebody_needs_to_tell_nvidia_to_calm_down_with/"&gt; &lt;img alt="Somebody needs to tell Nvidia to calm down with these new model names." src="https://preview.redd.it/hl0xrywo89ve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba3293f40fb091a49f266882c48318181875c821" title="Somebody needs to tell Nvidia to calm down with these new model names." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hl0xrywo89ve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0u8ew/somebody_needs_to_tell_nvidia_to_calm_down_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0u8ew/somebody_needs_to_tell_nvidia_to_calm_down_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T20:14:27+00:00</published>
  </entry>
</feed>
