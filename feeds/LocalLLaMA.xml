<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-12T19:34:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hzrgd5</id>
    <title>Are there any use cases for text (non-instruct) models?</title>
    <updated>2025-01-12T17:13:00+00:00</updated>
    <author>
      <name>/u/OneFanFare</name>
      <uri>https://old.reddit.com/user/OneFanFare</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For example, llama3.2 is available as instruct models, or as text models. I'm wondering if there are any good usecases for the latter? &lt;/p&gt; &lt;p&gt;I remember hearing some people using them for creative writing (start with a sample paragraph and let 'em go) but curious if there's anything else there.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OneFanFare"&gt; /u/OneFanFare &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzrgd5/are_there_any_use_cases_for_text_noninstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzrgd5/are_there_any_use_cases_for_text_noninstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzrgd5/are_there_any_use_cases_for_text_noninstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T17:13:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzlyf5</id>
    <title>MCPAdapt leverage 650+ MCP server as tools in any agentic framework.</title>
    <updated>2025-01-12T12:48:10+00:00</updated>
    <author>
      <name>/u/gaarll</name>
      <uri>https://old.reddit.com/user/gaarll</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there,&lt;/p&gt; &lt;p&gt;I just open-sourced a repository that allows to seamlessly integrate MCP server as tools in any agentic framework starting with smolagents: &lt;a href="https://github.com/grll/mcpadapt"&gt;https://github.com/grll/mcpadapt&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Have a look let me know what you think and join me in adapting MCP servers for other frameworks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gaarll"&gt; /u/gaarll &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzlyf5/mcpadapt_leverage_650_mcp_server_as_tools_in_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzlyf5/mcpadapt_leverage_650_mcp_server_as_tools_in_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzlyf5/mcpadapt_leverage_650_mcp_server_as_tools_in_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T12:48:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzpfwk</id>
    <title>Is there any LLM that is made to teach programming?</title>
    <updated>2025-01-12T15:46:09+00:00</updated>
    <author>
      <name>/u/Comrade_United-World</name>
      <uri>https://old.reddit.com/user/Comrade_United-World</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If there were any It would be much better, just like I am learning from a dedicated teacher.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comrade_United-World"&gt; /u/Comrade_United-World &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzpfwk/is_there_any_llm_that_is_made_to_teach_programming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzpfwk/is_there_any_llm_that_is_made_to_teach_programming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzpfwk/is_there_any_llm_that_is_made_to_teach_programming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T15:46:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzi6ed</id>
    <title>What can i do with 6GB of VRAM ?</title>
    <updated>2025-01-12T08:16:41+00:00</updated>
    <author>
      <name>/u/vsh46</name>
      <uri>https://old.reddit.com/user/vsh46</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had built a small rig for mining and learning AI in 2019.&lt;/p&gt; &lt;p&gt;Following are the system specs :- - GPU: Asus GTX 1660 Ti (6GB VRAM) - CPU: Intel Core i5-9400F - RAM: Corsair 8GB - SSD: WD Blue NAND SATA (250GB) - HDD: WD Blue SATA (1TB)&lt;/p&gt; &lt;p&gt;I wanted to make use of this system by running some local llm with high toks / sec with decent use for tasks like tool calling, instruction following, coding and basic home assistant based QnA. Mainly planning to use this system like a jarvis at home which does basic tasks but is very good at those.&lt;/p&gt; &lt;p&gt;Any recommendations of some LLMs or Frameworks that can help me achieve this would really help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vsh46"&gt; /u/vsh46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzi6ed/what_can_i_do_with_6gb_of_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzi6ed/what_can_i_do_with_6gb_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzi6ed/what_can_i_do_with_6gb_of_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T08:16:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzcxby</id>
    <title>moondream CAPTCHAs test. It's surprisingly accurate at solving rotation CAPTCHAs, but not so much for the others.</title>
    <updated>2025-01-12T02:46:02+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzcxby/moondream_captchas_test_its_surprisingly_accurate/"&gt; &lt;img alt="moondream CAPTCHAs test. It's surprisingly accurate at solving rotation CAPTCHAs, but not so much for the others." src="https://preview.redd.it/c73tl9ko7hce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ecac7f710d04af5ce2b21d2e0825db84a321f21" title="moondream CAPTCHAs test. It's surprisingly accurate at solving rotation CAPTCHAs, but not so much for the others." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c73tl9ko7hce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzcxby/moondream_captchas_test_its_surprisingly_accurate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzcxby/moondream_captchas_test_its_surprisingly_accurate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T02:46:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzsvr5</id>
    <title>What are the current best low spec LLMs</title>
    <updated>2025-01-12T18:13:22+00:00</updated>
    <author>
      <name>/u/tuxPT</name>
      <uri>https://old.reddit.com/user/tuxPT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello.&lt;/p&gt; &lt;p&gt;I'm looking either for advice or a benchmark with the best low spec LLMs. I define low spec as any llm that can run locally in a mobile device or in low spec laptop(integrated GPU+8/12gb ram).&lt;/p&gt; &lt;p&gt;As for tasks, mainly text transformation or questions about the text. No translation needed, the input and output would be in English.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tuxPT"&gt; /u/tuxPT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzsvr5/what_are_the_current_best_low_spec_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzsvr5/what_are_the_current_best_low_spec_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzsvr5/what_are_the_current_best_low_spec_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T18:13:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzt0a6</id>
    <title>Could I just add a 50xx card to my setup? Should I?</title>
    <updated>2025-01-12T18:18:45+00:00</updated>
    <author>
      <name>/u/Soggy_Wallaby_8130</name>
      <uri>https://old.reddit.com/user/Soggy_Wallaby_8130</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So currently rocking 2x P40 on a weird mobo with plenty of room to spare. Iâ€™m not normally one to buy new graphics cards but the product announcements are not as expensive as expected, right? Iâ€™m like â€˜yeah itâ€™s nice to have this vram on these P40s but they are kinda slow and I would still like a little more to run qwen 2 audio or whateverâ€™. &lt;/p&gt; &lt;p&gt;What are the new cards like? Are they any good? Discussion tag really just if anyone had opinionsâ€¦ &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Soggy_Wallaby_8130"&gt; /u/Soggy_Wallaby_8130 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzt0a6/could_i_just_add_a_50xx_card_to_my_setup_should_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzt0a6/could_i_just_add_a_50xx_card_to_my_setup_should_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzt0a6/could_i_just_add_a_50xx_card_to_my_setup_should_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T18:18:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzqgqm</id>
    <title>How to effectively use AI (Llama) for larger coding projects? Hitting some roadblocks</title>
    <updated>2025-01-12T16:31:03+00:00</updated>
    <author>
      <name>/u/MacDevs</name>
      <uri>https://old.reddit.com/user/MacDevs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mobile and web development with AI is incredibly convenient. Even though I have coding knowledge, I now prefer to let AI handle the coding based on my requirements (100%).&lt;/p&gt; &lt;p&gt;I've noticed it's straightforward to create small websites or applications. However, things get more complicated when dealing with multiple files.&lt;/p&gt; &lt;p&gt;First, there's a limit to the number of files we can use. I found a workaround using the Combine Files app on macOS, which allows combining multiple files into a single file.&lt;/p&gt; &lt;p&gt;But then I face a new issue I can't solve: the AI starts removing features without asking (even if I asked not to change the current features). This requires carefully reviewing the submitted code, which is time-consuming.&lt;/p&gt; &lt;p&gt;Have you found any solutions (methods, workflows, prompts) that allow AI to develop projects with over 2000 lines of code?&lt;/p&gt; &lt;p&gt;I'm new to AI development and would appreciate any insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MacDevs"&gt; /u/MacDevs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzqgqm/how_to_effectively_use_ai_llama_for_larger_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzqgqm/how_to_effectively_use_ai_llama_for_larger_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzqgqm/how_to_effectively_use_ai_llama_for_larger_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T16:31:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz2rar</id>
    <title>Why we don't know researchers behind DeepSeek?</title>
    <updated>2025-01-11T18:48:03+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Zero interviews, zero social activity. Zero group photos, none about us page.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz2rar/why_we_dont_know_researchers_behind_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz2rar/why_we_dont_know_researchers_behind_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz2rar/why_we_dont_know_researchers_behind_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T18:48:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzuouz</id>
    <title>Are you using different model families in your LLM apps/agents for better task performance?</title>
    <updated>2025-01-12T19:29:54+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anecdotally, I have seen Claude sonet3.5 perform better on structured outputs vs GPT4-o. But conversely see OpenAI model families perform better on other tasks (like creative writing). This experience is amplified for open source models.&lt;/p&gt; &lt;p&gt;So the broader community question is: are you using multiple models from different model families in your apps? If so whatâ€™s your use case and what models are you using?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzuouz/are_you_using_different_model_families_in_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzuouz/are_you_using_different_model_families_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzuouz/are_you_using_different_model_families_in_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T19:29:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzqkkd</id>
    <title>Vector database with LinkedIn Posts plus Engagement Metrics</title>
    <updated>2025-01-12T16:35:45+00:00</updated>
    <author>
      <name>/u/davidmezzetti</name>
      <uri>https://old.reddit.com/user/davidmezzetti</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzqkkd/vector_database_with_linkedin_posts_plus/"&gt; &lt;img alt="Vector database with LinkedIn Posts plus Engagement Metrics" src="https://external-preview.redd.it/1rxY6Io0zblOYmTaFApGbf9v15x7lpEraQqnP6ffWwA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07f5f414b66bcd1c1c75a11a1648cffb6fdba754" title="Vector database with LinkedIn Posts plus Engagement Metrics" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davidmezzetti"&gt; /u/davidmezzetti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/NeuML/txtai-neuml-linkedin"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzqkkd/vector_database_with_linkedin_posts_plus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzqkkd/vector_database_with_linkedin_posts_plus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T16:35:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzsvkz</id>
    <title>Volo: An easy and local way to RAG with Wikipedia!</title>
    <updated>2025-01-12T18:13:08+00:00</updated>
    <author>
      <name>/u/procraftermc</name>
      <uri>https://old.reddit.com/user/procraftermc</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzsvkz/volo_an_easy_and_local_way_to_rag_with_wikipedia/"&gt; &lt;img alt="Volo: An easy and local way to RAG with Wikipedia!" src="https://external-preview.redd.it/fpG2F-y2fJG3IOgvl1H5IK_WS4ZurjXL_2_4lQMcpvY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b4fece65aa394b3776194c084c3d99fcc5693c8" title="Volo: An easy and local way to RAG with Wikipedia!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One of the biggest problems with AI models is their tendency to hallucinate. This project aims to fix that by giving them access to an offline copy of Wikipedia (about 57 GB)&lt;/p&gt; &lt;p&gt;It uses a copy of Wikipedia created by Kiwix as the offline database and Qwen2.5:3B as the LLM.&lt;/p&gt; &lt;p&gt;Install instructions are on the Github: &lt;a href="https://github.com/AdyTech99/volo/"&gt;https://github.com/AdyTech99/volo/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ye31knzzslce1.png?width=3015&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f15216c70371d13352667b4ddce95e3b57e1ffc5"&gt;Example of Volo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/procraftermc"&gt; /u/procraftermc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzsvkz/volo_an_easy_and_local_way_to_rag_with_wikipedia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzsvkz/volo_an_easy_and_local_way_to_rag_with_wikipedia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzsvkz/volo_an_easy_and_local_way_to_rag_with_wikipedia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T18:13:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1hze1xk</id>
    <title>6x AMD Instinct Mi60 AI Server vs Llama 405B + vLLM + Open-WebUI - Impressive!</title>
    <updated>2025-01-12T03:48:19+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hze1xk/6x_amd_instinct_mi60_ai_server_vs_llama_405b_vllm/"&gt; &lt;img alt="6x AMD Instinct Mi60 AI Server vs Llama 405B + vLLM + Open-WebUI - Impressive!" src="https://external-preview.redd.it/eDd2Nndjb3ppaGNlMUVpVcA3yZ4wjFwvAE4TdXYq4bpkJwG-QulsV1F4T0eu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=475a3c9c78a9d27249a2c72bcf664f75b7f9639c" title="6x AMD Instinct Mi60 AI Server vs Llama 405B + vLLM + Open-WebUI - Impressive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r3w7zbozihce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hze1xk/6x_amd_instinct_mi60_ai_server_vs_llama_405b_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hze1xk/6x_amd_instinct_mi60_ai_server_vs_llama_405b_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T03:48:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz7l7d</id>
    <title>OpenAI is losing money , meanwhile qwen is planning voice mode , imagine if they manage to make o1 level model</title>
    <updated>2025-01-11T22:23:27+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz7l7d/openai_is_losing_money_meanwhile_qwen_is_planning/"&gt; &lt;img alt="OpenAI is losing money , meanwhile qwen is planning voice mode , imagine if they manage to make o1 level model" src="https://preview.redd.it/nhsep8z3xfce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7b460c09bdabd5d02e8e8e46757749c4711b75f" title="OpenAI is losing money , meanwhile qwen is planning voice mode , imagine if they manage to make o1 level model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nhsep8z3xfce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz7l7d/openai_is_losing_money_meanwhile_qwen_is_planning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz7l7d/openai_is_losing_money_meanwhile_qwen_is_planning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T22:23:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz5caf</id>
    <title>Tutorial: Run Moondream 2b's new gaze detection on any video</title>
    <updated>2025-01-11T20:42:31+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz5caf/tutorial_run_moondream_2bs_new_gaze_detection_on/"&gt; &lt;img alt="Tutorial: Run Moondream 2b's new gaze detection on any video" src="https://external-preview.redd.it/a2VmczhmdHllZmNlMTF40J1mEmizgXzWsZQRgxJwv14NVEzVGBQqF-uixs9J.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6d357b58a592f06ed596b1615e185d70bfedfdf" title="Tutorial: Run Moondream 2b's new gaze detection on any video" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i9ofbftyefce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz5caf/tutorial_run_moondream_2bs_new_gaze_detection_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz5caf/tutorial_run_moondream_2bs_new_gaze_detection_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T20:42:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzd3xs</id>
    <title>Qwen releases Qwen Chat (online)</title>
    <updated>2025-01-12T02:56:09+00:00</updated>
    <author>
      <name>/u/Many_SuchCases</name>
      <uri>https://old.reddit.com/user/Many_SuchCases</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Many_SuchCases"&gt; /u/Many_SuchCases &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://chat.qwenlm.ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzd3xs/qwen_releases_qwen_chat_online/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzd3xs/qwen_releases_qwen_chat_online/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T02:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzfjmp</id>
    <title>Parking Systems analysis and Report Generation with Computer vision and Ollama</title>
    <updated>2025-01-12T05:15:40+00:00</updated>
    <author>
      <name>/u/oridnary_artist</name>
      <uri>https://old.reddit.com/user/oridnary_artist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzfjmp/parking_systems_analysis_and_report_generation/"&gt; &lt;img alt="Parking Systems analysis and Report Generation with Computer vision and Ollama " src="https://external-preview.redd.it/ZDUxcHMwOGt5aGNlMZNdfj6QUni_z9Bf_NJiTzUymfkgPwnfSrss06zjR7A1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f892ce79dd89dd0ae0171dc7ac8e70e942f8504" title="Parking Systems analysis and Report Generation with Computer vision and Ollama " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oridnary_artist"&gt; /u/oridnary_artist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2tf8yz7kyhce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzfjmp/parking_systems_analysis_and_report_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzfjmp/parking_systems_analysis_and_report_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T05:15:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzmljb</id>
    <title>In the Terminator's vision overlay, the "ANALYSIS" is probably the image embedding ðŸ¤”</title>
    <updated>2025-01-12T13:25:15+00:00</updated>
    <author>
      <name>/u/Reddactor</name>
      <uri>https://old.reddit.com/user/Reddactor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzmljb/in_the_terminators_vision_overlay_the_analysis_is/"&gt; &lt;img alt="In the Terminator's vision overlay, the &amp;quot;ANALYSIS&amp;quot; is probably the image embedding ðŸ¤”" src="https://preview.redd.it/b3if9y5tdkce1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cdf7470becd28e71636379b90a99d0da4fd0cc3" title="In the Terminator's vision overlay, the &amp;quot;ANALYSIS&amp;quot; is probably the image embedding ðŸ¤”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reddactor"&gt; /u/Reddactor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b3if9y5tdkce1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzmljb/in_the_terminators_vision_overlay_the_analysis_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzmljb/in_the_terminators_vision_overlay_the_analysis_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T13:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz97my</id>
    <title>they donâ€™t know how good gaze detection is on moondream</title>
    <updated>2025-01-11T23:38:28+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz97my/they_dont_know_how_good_gaze_detection_is_on/"&gt; &lt;img alt="they donâ€™t know how good gaze detection is on moondream" src="https://external-preview.redd.it/anBia3RnaGhhZ2NlMSTi0DO1FtxEm4mYFQVOtZR8uuj4lv59wjB_E-Pc4Mjr.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3206ad0d968f5e06525f4113c574566e35551fb1" title="they donâ€™t know how good gaze detection is on moondream" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xgysp5nhagce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz97my/they_dont_know_how_good_gaze_detection_is_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz97my/they_dont_know_how_good_gaze_detection_is_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T23:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzn5b6</id>
    <title>Forget AI waifus. Are there local AI assistants to increase my productivity?</title>
    <updated>2025-01-12T13:54:45+00:00</updated>
    <author>
      <name>/u/-oshino_shinobu-</name>
      <uri>https://old.reddit.com/user/-oshino_shinobu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As title suggests, lots of lonely men out there looking to fine tune their own AI gf. But I really just want an AI secretary who can help me make plans, trivial tasks like respond to messages/emails, and generally increase my productivity.&lt;/p&gt; &lt;p&gt;What model do you guys suggest? I assume itâ€™ll need huge context length to fit enough data about me? Also hoping thereâ€™s a way to make AI periodically text me and give me updates. I have 48GB of vram to spare for this LLM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-oshino_shinobu-"&gt; /u/-oshino_shinobu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzn5b6/forget_ai_waifus_are_there_local_ai_assistants_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzn5b6/forget_ai_waifus_are_there_local_ai_assistants_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzn5b6/forget_ai_waifus_are_there_local_ai_assistants_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T13:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzany5</id>
    <title>We are an AI company now!</title>
    <updated>2025-01-12T00:47:37+00:00</updated>
    <author>
      <name>/u/Brilliant-Day2748</name>
      <uri>https://old.reddit.com/user/Brilliant-Day2748</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzany5/we_are_an_ai_company_now/"&gt; &lt;img alt="We are an AI company now!" src="https://preview.redd.it/0yl0970umgce1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53963d0db45722eea8467f27c91ca48e5a7cf6fc" title="We are an AI company now!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brilliant-Day2748"&gt; /u/Brilliant-Day2748 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0yl0970umgce1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzany5/we_are_an_ai_company_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzany5/we_are_an_ai_company_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T00:47:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzp789</id>
    <title>Mark Zuckerberg believes in 2025, Meta will probably have a mid-level engineer AI that can write code, and over time it will replace people engineers.</title>
    <updated>2025-01-12T15:34:50+00:00</updated>
    <author>
      <name>/u/Admirable-Star7088</name>
      <uri>https://old.reddit.com/user/Admirable-Star7088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/slow_developer/status/1877798620692422835?mx=2"&gt;https://x.com/slow_developer/status/1877798620692422835?mx=2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=USBW0ESLEK0"&gt;https://www.youtube.com/watch?v=USBW0ESLEK0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://tribune.com.pk/story/2521499/zuckerberg-announces-meta-plans-to-replace-mid-level-engineers-with-ais-this-year"&gt;https://tribune.com.pk/story/2521499/zuckerberg-announces-meta-plans-to-replace-mid-level-engineers-with-ais-this-year&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What do you think, is he a bit too optimistic here, or should we expect greatly improved (coding) LLMs very soon? Will this be Llama 4? :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable-Star7088"&gt; /u/Admirable-Star7088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzp789/mark_zuckerberg_believes_in_2025_meta_will/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzp789/mark_zuckerberg_believes_in_2025_meta_will/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzp789/mark_zuckerberg_believes_in_2025_meta_will/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T15:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz28ld</id>
    <title>Bro whaaaat?</title>
    <updated>2025-01-11T18:24:57+00:00</updated>
    <author>
      <name>/u/Specter_Origin</name>
      <uri>https://old.reddit.com/user/Specter_Origin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz28ld/bro_whaaaat/"&gt; &lt;img alt="Bro whaaaat?" src="https://preview.redd.it/cwi5l2ziqece1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6895d12163dd294798940a5c5b6368da7f91b2f" title="Bro whaaaat?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specter_Origin"&gt; /u/Specter_Origin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cwi5l2ziqece1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz28ld/bro_whaaaat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz28ld/bro_whaaaat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T18:24:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzkw3f</id>
    <title>DeepSeek V3 is the gift that keeps on giving!</title>
    <updated>2025-01-12T11:37:25+00:00</updated>
    <author>
      <name>/u/indicava</name>
      <uri>https://old.reddit.com/user/indicava</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzkw3f/deepseek_v3_is_the_gift_that_keeps_on_giving/"&gt; &lt;img alt="DeepSeek V3 is the gift that keeps on giving!" src="https://preview.redd.it/fj10nizoujce1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b294748a76dfcaf7f0f25300479cd3ea3b25308" title="DeepSeek V3 is the gift that keeps on giving!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/indicava"&gt; /u/indicava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fj10nizoujce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzkw3f/deepseek_v3_is_the_gift_that_keeps_on_giving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzkw3f/deepseek_v3_is_the_gift_that_keeps_on_giving/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T11:37:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzmpuq</id>
    <title>VLC to add offline, real-time AI subtitles. What do you think the tech stack for this is?</title>
    <updated>2025-01-12T13:31:43+00:00</updated>
    <author>
      <name>/u/SpudMonkApe</name>
      <uri>https://old.reddit.com/user/SpudMonkApe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzmpuq/vlc_to_add_offline_realtime_ai_subtitles_what_do/"&gt; &lt;img alt="VLC to add offline, real-time AI subtitles. What do you think the tech stack for this is?" src="https://external-preview.redd.it/aphKSMbfvfDHStraL4JSGgDfke__oze-3mdG_k4jOVQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37b50e3ca1b1a72567f853cc77c80c80b325c53a" title="VLC to add offline, real-time AI subtitles. What do you think the tech stack for this is?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpudMonkApe"&gt; /u/SpudMonkApe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.pcmag.com/news/vlc-media-player-to-use-ai-to-generate-subtitles-for-videos"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzmpuq/vlc_to_add_offline_realtime_ai_subtitles_what_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzmpuq/vlc_to_add_offline_realtime_ai_subtitles_what_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T13:31:43+00:00</published>
  </entry>
</feed>
