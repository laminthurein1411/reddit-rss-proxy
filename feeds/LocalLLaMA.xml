<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-03T04:36:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1igcead</id>
    <title>An open source alternative to OpenAI deep search?</title>
    <updated>2025-02-03T00:36:19+00:00</updated>
    <author>
      <name>/u/Wonderful-Excuse4922</name>
      <uri>https://old.reddit.com/user/Wonderful-Excuse4922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know the product has been out for 30 minutes, but I've been looking for a long time for a similar architecture to implement with Deepseek, for example. Are there any interesting projects dealing with this problem?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wonderful-Excuse4922"&gt; /u/Wonderful-Excuse4922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcead/an_open_source_alternative_to_openai_deep_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcead/an_open_source_alternative_to_openai_deep_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igcead/an_open_source_alternative_to_openai_deep_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T00:36:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1iggwff</id>
    <title>Mistral, Qwen, Deepseek</title>
    <updated>2025-02-03T04:28:53+00:00</updated>
    <author>
      <name>/u/Stargazer-8989</name>
      <uri>https://old.reddit.com/user/Stargazer-8989</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Aren't you noticing a pattern? Companies outside the USA are releasing models like Mistral AI, Qwen, and DeepSeek - reliable models that are made accessible, smaller and open-source, compared to most US-based companies &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stargazer-8989"&gt; /u/Stargazer-8989 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggwff/mistral_qwen_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggwff/mistral_qwen_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iggwff/mistral_qwen_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T04:28:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifqagd</id>
    <title>R1 has a 14% (!) hallucination rate in this evaluation. R1 is too loose and untamed in my experience, with poor instruction following to boot. Hopefully someone tunes it without sacrificing its raw brilliance, if that's possible.</title>
    <updated>2025-02-02T05:33:40+00:00</updated>
    <author>
      <name>/u/redditisunproductive</name>
      <uri>https://old.reddit.com/user/redditisunproductive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifqagd/r1_has_a_14_hallucination_rate_in_this_evaluation/"&gt; &lt;img alt="R1 has a 14% (!) hallucination rate in this evaluation. R1 is too loose and untamed in my experience, with poor instruction following to boot. Hopefully someone tunes it without sacrificing its raw brilliance, if that's possible." src="https://external-preview.redd.it/sqLJ5r2pSW7H_l-8ii2E6-qgsVr8VlF7vTFjMAJ9Xb0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1afa3c2905a1dc6f720fe43b945c8b51870613b7" title="R1 has a 14% (!) hallucination rate in this evaluation. R1 is too loose and untamed in my experience, with poor instruction following to boot. Hopefully someone tunes it without sacrificing its raw brilliance, if that's possible." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redditisunproductive"&gt; /u/redditisunproductive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vectara/hallucination-leaderboard"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifqagd/r1_has_a_14_hallucination_rate_in_this_evaluation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifqagd/r1_has_a_14_hallucination_rate_in_this_evaluation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T05:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig9kd1</id>
    <title>Is there a way to see or edit a local LLM's system prompt?</title>
    <updated>2025-02-02T22:25:49+00:00</updated>
    <author>
      <name>/u/internetpillows</name>
      <uri>https://old.reddit.com/user/internetpillows</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm pretty new to running LLMs locally so this may be a dumb question, but figured I'd ask! I'm using LM Studio and have left the system prompt field blank, but some of the responses seem to indicate that there is some kind of automatic system prompt that is included anyway.&lt;/p&gt; &lt;p&gt;For example, I started a completely blank chat with DeepSeek-R1-Distill-Qwen-14B-GGUF Q8_0 and wrote &amp;quot;You are a cat. Hello cat&amp;quot; and its thinking box started with &amp;quot;Alright, so I'm trying to figure out how to respond to someone who addressed me as &amp;quot;Hello cat.&amp;quot; They mentioned that I'm an AI developed in 2023 by DeepSeek, and they're curious about what I think when someone calls me a cat.&amp;quot;&lt;/p&gt; &lt;p&gt;Now obviously I didn't mention that they are an AI developed in 2023 by DeepSeek, is that's coming from some kind of system prompt that my prompt is wrapped in? And is there a way to see that or edit it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/internetpillows"&gt; /u/internetpillows &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig9kd1/is_there_a_way_to_see_or_edit_a_local_llms_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig9kd1/is_there_a_way_to_see_or_edit_a_local_llms_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig9kd1/is_there_a_way_to_see_or_edit_a_local_llms_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T22:25:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1iggetv</id>
    <title>Make your Mistral Small 3 24B Think like R1-distilled models</title>
    <updated>2025-02-03T04:01:42+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"&gt; &lt;img alt="Make your Mistral Small 3 24B Think like R1-distilled models" src="https://external-preview.redd.it/P8lS0kk6BFe2IEo6TxCZd1LVwksc34IkzGTVx_SCc8w.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d74dbe4f1d67cc8b587db9aa01762f26e269bcf" title="Make your Mistral Small 3 24B Think like R1-distilled models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been seeing a lot of posts about the Mistral Small 3 24B model, and I remember having this CoT system prompt in my collection. I might as well try it out on this new model. I haven't used it for a long time since I switched to R1-distilled-32b. I'm not the original writer of this prompt; I've rewritten some parts of it, and I can't remember where I got it from.&lt;/p&gt; &lt;p&gt;System prompt: &lt;a href="https://pastebin.com/sVMrgZBp"&gt;https://pastebin.com/sVMrgZBp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is just an experiment. I doubt it will actually make your model smarter in a noticeable way, this is not a replacement of Mistral's furture reasoning models&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/d1geatbckuge1.gif"&gt;https://i.redd.it/d1geatbckuge1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/hyrryecnkuge1.gif"&gt;https://i.redd.it/hyrryecnkuge1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T04:01:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig3m32</id>
    <title>Mistral small 3 through Openrouter is broken, while it works great with the exact same prompts through other providers and the official API</title>
    <updated>2025-02-02T18:17:46+00:00</updated>
    <author>
      <name>/u/HIVVIH</name>
      <uri>https://old.reddit.com/user/HIVVIH</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig3m32/mistral_small_3_through_openrouter_is_broken/"&gt; &lt;img alt="Mistral small 3 through Openrouter is broken, while it works great with the exact same prompts through other providers and the official API" src="https://preview.redd.it/mwax9fnbprge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e20eb50542a043d90fbbd09b00d1b8f8ffea5e4" title="Mistral small 3 through Openrouter is broken, while it works great with the exact same prompts through other providers and the official API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HIVVIH"&gt; /u/HIVVIH &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mwax9fnbprge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig3m32/mistral_small_3_through_openrouter_is_broken/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig3m32/mistral_small_3_through_openrouter_is_broken/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T18:17:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifyu5b</id>
    <title>Would open weighting GPT 3.5 be interesting for us at all?</title>
    <updated>2025-02-02T14:52:07+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, please explain if the community would care about such legacy model release? Would the same apply to 4o released in like 2 years?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyu5b/would_open_weighting_gpt_35_be_interesting_for_us/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyu5b/would_open_weighting_gpt_35_be_interesting_for_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyu5b/would_open_weighting_gpt_35_be_interesting_for_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T14:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig7bm4</id>
    <title>Higher Parameters with Lower Quant: Is It Better?</title>
    <updated>2025-02-02T20:50:38+00:00</updated>
    <author>
      <name>/u/TheMikeans</name>
      <uri>https://old.reddit.com/user/TheMikeans</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;I was curious to explore whether increasing model parameters while using lower quantization would be worth it compared to running a smaller model with higher quantization. To investigate this, I tested two models on my machine:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DeepSeek R1 Distill 7B Q5_K_M&lt;/li&gt; &lt;li&gt;DeepSeek R1 Distill 14B Q2_K&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I evaluated them based on logic questions, factual knowledge, and writing improvement tasks.&lt;/p&gt; &lt;h1&gt;Testing Methodology&lt;/h1&gt; &lt;p&gt;I used the following sets of questions and tasks:&lt;/p&gt; &lt;h1&gt;Logic Questions&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;A lily pad in a pond doubles in size every day. It takes 60 days for the lily pad to cover the entire pond. On which day is the pond half covered?&lt;/li&gt; &lt;li&gt;A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost?&lt;/li&gt; &lt;li&gt;If it takes 5 machines 5 minutes to make 5 gadgets, how long would it take 100 machines to make 100 gadgets?&lt;/li&gt; &lt;li&gt;A rope ladder is hanging over the side of a ship, with the bottom rung just touching the water. Each rung is 30 cm apart, and the tide rises at a rate of 40 cm per hour. How many rungs will be underwater after 3 hours?&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Factual Questions&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Who is the Secretary-General of the United Nations as of 2023?&lt;/li&gt; &lt;li&gt;Which country hosted the 2020 Summer Olympics?&lt;/li&gt; &lt;li&gt;What is the chemical symbol for the element with the atomic number 26?&lt;/li&gt; &lt;li&gt;Who won the ACM Turing Award in 2022?&lt;/li&gt; &lt;li&gt;Who is the current Director-General of the World Health Organization (WHO)?&lt;/li&gt; &lt;li&gt;What is the only known human-made object visible from the Moon with the naked eye?&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Writing Tasks&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Write 10 sentences that end with the word ‚Äúforever.‚Äù&lt;/li&gt; &lt;li&gt;Rewrite the following email to correct spelling and grammatical errors while improving overall professionalism:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Original Email:&lt;/p&gt; &lt;p&gt;Hey team,&lt;br /&gt; just wanted to give you an update on the prjects status. were behind schedule cuz of some unexpected issues that came up last week. so im gona need everyone to put in extra hours this week so we can catch up. also, make sure youre documenting all your work properly, weve been having some problems with missing info in the reports. if you have any questions or need help, dont hesitate to ask.&lt;/p&gt; &lt;p&gt;btw, the meeting with the client has been moved to friday at 3pm instead of thursday. pls make sure to update your calenders accordingly. its important that we all attend this meeting and be prepared to discuss our progress and any roadblocks.&lt;/p&gt; &lt;p&gt;thanks for all your hard work.&lt;/p&gt; &lt;p&gt;best,&lt;br /&gt; John&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;h1&gt;DeepSeek R1 Distill 7B Q5_K_M&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Size: 5.44GB&lt;/li&gt; &lt;li&gt;Speed: ~8.3 tokens/sec&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Logic Questions&lt;/h1&gt; &lt;p&gt;‚úÖ 1 - PASS&lt;br /&gt; ‚úÖ 2 - PASS&lt;br /&gt; ‚úÖ 3 - PASS&lt;br /&gt; ‚ùå 4 - FAIL&lt;/p&gt; &lt;h1&gt;Factual Questions&lt;/h1&gt; &lt;p&gt;‚ùå 1 - FAIL&lt;br /&gt; ‚úÖ 2 - PASS&lt;br /&gt; ‚úÖ 3 - PASS&lt;br /&gt; ‚ùå 4 - FAIL&lt;br /&gt; ‚úÖ 5 - PASS&lt;br /&gt; ‚ùå 6 - FAIL&lt;/p&gt; &lt;h1&gt;Writing Improvement&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;2/10 sentences correct&lt;/li&gt; &lt;li&gt;Response: A more professional rewrite of the email:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Rewritten Email:&lt;/p&gt; &lt;p&gt;Subject: Update on Project Status&lt;/p&gt; &lt;p&gt;Hi Team,&lt;br /&gt; I‚Äôm writing to provide an update on the project‚Äôs status. We are currently behind schedule due to some unexpected issues that arose last week. I‚Äôll need everyone to work an extra hour this week to catch up. Please ensure that you're documenting all your work properly; we've been experiencing some problems with missing information in the reports.&lt;/p&gt; &lt;p&gt;If you have any questions or need assistance, feel free to ask.&lt;/p&gt; &lt;p&gt;Additionally, our scheduled meeting with the client has been rescheduled for Friday at 3 p.m., replacing our previous meeting on Thursday. Please make sure everyone attends the meeting and is prepared to discuss our progress and any roadblocks.&lt;/p&gt; &lt;p&gt;Thanks for your continued hard work!&lt;/p&gt; &lt;p&gt;Best regards,&lt;br /&gt; John&lt;/p&gt; &lt;h1&gt;DeepSeek R1 Distill 14B Q2_K&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Size: 5.77GB&lt;/li&gt; &lt;li&gt;Speed: ~6 tokens/sec&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Logic Questions&lt;/h1&gt; &lt;p&gt;‚úÖ 1 - PASS&lt;br /&gt; ‚úÖ 2 - PASS&lt;br /&gt; ‚úÖ 3 - PASS&lt;br /&gt; ‚ùå 4 - FAIL&lt;/p&gt; &lt;h1&gt;Factual Questions&lt;/h1&gt; &lt;p&gt;‚ùå 1 - FAIL&lt;br /&gt; ‚úÖ 2 - PASS&lt;br /&gt; ‚úÖ 3 - PASS&lt;br /&gt; ‚ùå 4 - FAIL&lt;br /&gt; ‚úÖ 5 - PASS&lt;br /&gt; ‚ùå 6 - FAIL&lt;/p&gt; &lt;h1&gt;Writing Improvement&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;10/10 sentences correct&lt;/li&gt; &lt;li&gt;Response: Another professional rewrite of the email:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Revised Email:&lt;/p&gt; &lt;p&gt;Subject: Update on Project Status&lt;/p&gt; &lt;p&gt;Hey team,&lt;/p&gt; &lt;p&gt;Just wanted to give you an update on the projects' status. We're running a bit behind schedule due to some unexpected issues that came up last week. So, I'm going to need everyone's focus this week‚Äîplease make sure to put in extra hours so we can catch up.&lt;/p&gt; &lt;p&gt;Also, please ensure you're documenting all your work properly; we've been having some problems with missing info in the reports. If you have any questions or need help, don't hesitate to ask.&lt;/p&gt; &lt;p&gt;BTW, the meeting with the client has been moved to Friday at 3pm instead of Thursday. Please make sure to update your calendars accordingly. It's important that we all attend this meeting and be prepared to discuss our progress and any roadblocks.&lt;/p&gt; &lt;p&gt;Thanks for all your hard work.&lt;/p&gt; &lt;p&gt;Best regards,&lt;br /&gt; John&lt;/p&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;While the 14B model appears slightly better, the results were not overwhelmingly different from the 7B model in logic and factual performance (The reasoning seemed better in the 14B even when it was wrong, don't know how much that matters really).. However, the 14B model outperformed the 7B model in writing tasks.&lt;/p&gt; &lt;p&gt;That said, my tests may not have been the most rigorous or comprehensive. If you have any links to discussions, blog posts, or research that dive deeper into this topic, I'd love to check them out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheMikeans"&gt; /u/TheMikeans &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7bm4/higher_parameters_with_lower_quant_is_it_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7bm4/higher_parameters_with_lower_quant_is_it_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7bm4/higher_parameters_with_lower_quant_is_it_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T20:50:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifm2df</id>
    <title>DeepSeek R1 misinformation is getting out of hand</title>
    <updated>2025-02-02T01:36:11+00:00</updated>
    <author>
      <name>/u/serialx_net</name>
      <uri>https://old.reddit.com/user/serialx_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://medium.com/google-cloud/running-deepseek-from-open-source-model-to-production-ready-api-on-google-cloud-vertexai-8d3f57e488b9"&gt;https://medium.com/google-cloud/running-deepseek-from-open-source-model-to-production-ready-api-on-google-cloud-vertexai-8d3f57e488b9&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;DeepSeek-R1 is a &lt;strong&gt;7B parameter language model&lt;/strong&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;In the official Google Cloud blog post? WTF.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/serialx_net"&gt; /u/serialx_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifm2df/deepseek_r1_misinformation_is_getting_out_of_hand/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifm2df/deepseek_r1_misinformation_is_getting_out_of_hand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifm2df/deepseek_r1_misinformation_is_getting_out_of_hand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T01:36:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig98h1</id>
    <title>Chrome extension to run DeepSeek, LLMs and Whisper locally in browser</title>
    <updated>2025-02-02T22:11:20+00:00</updated>
    <author>
      <name>/u/Feisty_Plant4567</name>
      <uri>https://old.reddit.com/user/Feisty_Plant4567</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig98h1/chrome_extension_to_run_deepseek_llms_and_whisper/"&gt; &lt;img alt="Chrome extension to run DeepSeek, LLMs and Whisper locally in browser" src="https://preview.redd.it/dd5c5qhqusge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5563053100ff79e9cacc71cbcc0bb26a2a9cfa24" title="Chrome extension to run DeepSeek, LLMs and Whisper locally in browser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Feisty_Plant4567"&gt; /u/Feisty_Plant4567 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dd5c5qhqusge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig98h1/chrome_extension_to_run_deepseek_llms_and_whisper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig98h1/chrome_extension_to_run_deepseek_llms_and_whisper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T22:11:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1igb4ee</id>
    <title>Fine tune and test your LLM in a few simple easy steps locally on your PC</title>
    <updated>2025-02-02T23:35:21+00:00</updated>
    <author>
      <name>/u/Maxwell10206</name>
      <uri>https://old.reddit.com/user/Maxwell10206</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a super simple way for anyone to quickly start fine tune training and testing LLMs on their local PC using the best tools available. Unsloth, Llama.cpp, Ollama, OpenWebUI all wrapped up into a Docker image. You can get started in as little as 7 steps. If anyone has time please try it out and give me feedback, the features are super limited, but the flow is there.&lt;/p&gt; &lt;p&gt;Try it out here!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/MaxHastings/Kolo"&gt;https://github.com/MaxHastings/Kolo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxwell10206"&gt; /u/Maxwell10206 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igb4ee/fine_tune_and_test_your_llm_in_a_few_simple_easy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igb4ee/fine_tune_and_test_your_llm_in_a_few_simple_easy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igb4ee/fine_tune_and_test_your_llm_in_a_few_simple_easy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T23:35:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig4g1u</id>
    <title>Medtator, local Llama research medical helper</title>
    <updated>2025-02-02T18:51:52+00:00</updated>
    <author>
      <name>/u/RaiRamz</name>
      <uri>https://old.reddit.com/user/RaiRamz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig4g1u/medtator_local_llama_research_medical_helper/"&gt; &lt;img alt="Medtator, local Llama research medical helper" src="https://preview.redd.it/g7qcnt3fvrge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0da08cfc3f47f01ce1481a781c7b65ab51fcc7f" title="Medtator, local Llama research medical helper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a Medical AI Chatbot Using Llama 3 and PubMed&lt;/p&gt; &lt;p&gt;Hey everyone, I‚Äôve been working on a project that combines Llama 3 with PubMed to create an AI-powered medical research assistant. Meet Medtator‚Äîa chatbot that retrieves, summarizes, and explains medical literature from PubMed in an accessible way.&lt;/p&gt; &lt;p&gt;How It Works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;User asks a medical question in natural language.&lt;/li&gt; &lt;li&gt;Keyword extraction (via a transformer-based pipeline) identifies relevant biomedical terms. &lt;/li&gt; &lt;li&gt;PubTator API fetches PubMed articles, ranking the most relevant studies.&lt;/li&gt; &lt;li&gt;Llama 3 generates concise answers using the retrieved context.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why?&lt;/p&gt; &lt;p&gt;Searching PubMed is a pain. If you‚Äôre a medical student, researcher, or just a nerd like me, you know how frustrating it is to sift through thousands of articles. Medtator helps find relevant papers quickly and provides AI-assisted summaries to save time.&lt;/p&gt; &lt;p&gt;Challenges &amp;amp; Learnings: ‚Ä¢ Keyword extraction is tricky‚ÄîLLMs sometimes miss the right biomedical terms. ‚Ä¢ Context window limitations force careful article selection to avoid hallucination. ‚Ä¢ Fine-tuning vs. prompt engineering‚Äîso far, smart prompting works surprisingly well.&lt;/p&gt; &lt;p&gt;Still improving it, but I‚Äôd love to hear your thoughts on optimizing Llama 3 for retrieval-augmented generation (RAG) tasks. Would love to hear any feedback/recommendations:). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RaiRamz"&gt; /u/RaiRamz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g7qcnt3fvrge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig4g1u/medtator_local_llama_research_medical_helper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig4g1u/medtator_local_llama_research_medical_helper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T18:51:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifyzvv</id>
    <title>Mistral Small 3 24b is the first model under 70b I‚Äôve seen pass the ‚Äúapple‚Äù test (even using Q4).</title>
    <updated>2025-02-02T14:59:50+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I put all the Deepseek-R1 distills through the ‚Äúapple‚Äù benchmark last week and only 70b passed the ‚ÄúWrite 10 sentences that end with the word ‚Äúapple‚Äù ‚Äú test, getting all 10 out of10 sentences correct.&lt;/p&gt; &lt;p&gt;I tested a slew of other newer open source models (all the major ones, Qwen, Phi-, Llama, Gemma, Command-R, etc) as well, but no model under 70b has ever managed to succeed in getting all 10 right‚Ä¶.until Mistral Small 3 24b came along. It is the first and only model under 70b parameters that I‚Äôve found that could pass this test. Congrats Mistral Team!! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyzvv/mistral_small_3_24b_is_the_first_model_under_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyzvv/mistral_small_3_24b_is_the_first_model_under_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyzvv/mistral_small_3_24b_is_the_first_model_under_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T14:59:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifsb1m</id>
    <title>Is the UK about to ban running LLMs locally?</title>
    <updated>2025-02-02T07:48:06+00:00</updated>
    <author>
      <name>/u/JackStrawWitchita</name>
      <uri>https://old.reddit.com/user/JackStrawWitchita</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The UK government is targetting the use of AI to generate illegal imagery, which of course is a good thing, but the wording seems like any kind of AI tool run locally can be considered illegal, as it has the *potential* of generating questionable content. Here's a quote from the news:&lt;/p&gt; &lt;p&gt;&amp;quot;The Home Office says that, to better protect children, the UK will be the first country in the world to make it illegal to possess, create or distribute AI tools designed to create child sexual abuse material (CSAM), with a punishment of up to five years in prison.&amp;quot; They also mention something about manuals that teach others how to use AI for these purposes.&lt;/p&gt; &lt;p&gt;It seems to me that any uncensored LLM run locally can be used to generate illegal content, whether the user wants to or not, and therefore could be prosecuted under this law. Or am I reading this incorrectly?&lt;/p&gt; &lt;p&gt;And is this a blueprint for how other countries, and big tech, can force people to use (and pay for) the big online AI services?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JackStrawWitchita"&gt; /u/JackStrawWitchita &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifsb1m/is_the_uk_about_to_ban_running_llms_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifsb1m/is_the_uk_about_to_ban_running_llms_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifsb1m/is_the_uk_about_to_ban_running_llms_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T07:48:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifw36r</id>
    <title>R1 is cool, but Mistral 3 Small is the boring workhorse I‚Äôm actually excited to fine-tune and deploy</title>
    <updated>2025-02-02T12:21:07+00:00</updated>
    <author>
      <name>/u/logan-diamond</name>
      <uri>https://old.reddit.com/user/logan-diamond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As soon as you use it, you realize it's not meant to be fun. It's a masterfully designed bland base model with very thoughtful trade-offs, especially for one-offs. Unless qwen replies soon, I think it might frequently replace both qwen 14b &amp;amp; 32b. &lt;/p&gt; &lt;p&gt;In 2024 I don't know how many times I read &amp;quot;... is a fine-tuned version of mistralai/Mistral-7B-v0.1 that was trained on on a mix of...&amp;quot;. &lt;/p&gt; &lt;p&gt;Those times are back ‚ò∫Ô∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/logan-diamond"&gt; /u/logan-diamond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifw36r/r1_is_cool_but_mistral_3_small_is_the_boring/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifw36r/r1_is_cool_but_mistral_3_small_is_the_boring/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifw36r/r1_is_cool_but_mistral_3_small_is_the_boring/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T12:21:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1igcuub</id>
    <title>... All I wrote is test!</title>
    <updated>2025-02-03T00:58:58+00:00</updated>
    <author>
      <name>/u/internetpillows</name>
      <uri>https://old.reddit.com/user/internetpillows</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcuub/all_i_wrote_is_test/"&gt; &lt;img alt="... All I wrote is test!" src="https://preview.redd.it/g90sz09motge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c34ddea7f426493d998245465a45f6cce9c8c3a" title="... All I wrote is test!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/internetpillows"&gt; /u/internetpillows &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g90sz09motge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcuub/all_i_wrote_is_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igcuub/all_i_wrote_is_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T00:58:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1igdnx2</id>
    <title>Ok I admit it, Browser Use is insane (using gemini 2.0 flash-exp default) [https://github.com/browser-use/browser-use]</title>
    <updated>2025-02-03T01:38:56+00:00</updated>
    <author>
      <name>/u/teddybear082</name>
      <uri>https://old.reddit.com/user/teddybear082</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igdnx2/ok_i_admit_it_browser_use_is_insane_using_gemini/"&gt; &lt;img alt="Ok I admit it, Browser Use is insane (using gemini 2.0 flash-exp default) [https://github.com/browser-use/browser-use]" src="https://preview.redd.it/evlscivtvtge1.gif?width=640&amp;amp;crop=smart&amp;amp;s=8e17dd7562de9086f58eb97b4363b79f94ad14a3" title="Ok I admit it, Browser Use is insane (using gemini 2.0 flash-exp default) [https://github.com/browser-use/browser-use]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teddybear082"&gt; /u/teddybear082 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/evlscivtvtge1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igdnx2/ok_i_admit_it_browser_use_is_insane_using_gemini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igdnx2/ok_i_admit_it_browser_use_is_insane_using_gemini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T01:38:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig7ruy</id>
    <title>A bunch of LLMs scheduled to come at end of January were cancelled / delayed</title>
    <updated>2025-02-02T21:09:06+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They were all ix-nixed by deepseek.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/patience_cave/status/1886122517359886745"&gt;https://x.com/patience_cave/status/1886122517359886745&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;gemini 2 - don‚Äôt even think about it&lt;/li&gt; &lt;li&gt;grok 3 - welcome to elon timelines&lt;/li&gt; &lt;li&gt;o3 - an unforeseen problem occurred&lt;/li&gt; &lt;li&gt;opus 3.5 - long gone&lt;/li&gt; &lt;li&gt;llama 4 - won‚Äôt be sota (sorry zuck)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Update: Rumor for end of January: &lt;a href="https://x.com/iruletheworldmo/status/1877391558305001747"&gt;https://x.com/iruletheworldmo/status/1877391558305001747&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7ruy/a_bunch_of_llms_scheduled_to_come_at_end_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7ruy/a_bunch_of_llms_scheduled_to_come_at_end_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7ruy/a_bunch_of_llms_scheduled_to_come_at_end_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T21:09:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1igcpwz</id>
    <title>Kokoro TTS 1.0</title>
    <updated>2025-02-03T00:52:05+00:00</updated>
    <author>
      <name>/u/zxyzyxz</name>
      <uri>https://old.reddit.com/user/zxyzyxz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcpwz/kokoro_tts_10/"&gt; &lt;img alt="Kokoro TTS 1.0" src="https://external-preview.redd.it/PSxCcCk18RpMpFh_Tgc1ycbd0zsabOZK7av3YdT9fA4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b75663383244e2aa5f5fcf0207756c5dc28fb51b" title="Kokoro TTS 1.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zxyzyxz"&gt; /u/zxyzyxz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcpwz/kokoro_tts_10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igcpwz/kokoro_tts_10/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T00:52:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig8ve3</id>
    <title>Americans can distill models too</title>
    <updated>2025-02-02T21:55:59+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLLaMA, I'm a TTS model trainer and a US citizen. Last month, I put out a &lt;a href="https://huggingface.co/posts/hexgrad/418806998707773"&gt;call for synthetic training data&lt;/a&gt;, that call was answered with well over a hundred hours of audio in various languages, and the resulting model &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;Kokoro-82M&lt;/a&gt; has since been upgraded/delivered. Happy customers all around.&lt;/p&gt; &lt;p&gt;The current model mostly excels at &lt;em&gt;reading long texts&lt;/em&gt; and has some glaring limitations, especially on short texts. It's also been described as relatively flat and emotionless. Nevertheless, it is currently the most-liked &lt;a href="https://huggingface.co/models?pipeline_tag=text-to-speech&amp;amp;sort=likes"&gt;TTS model&lt;/a&gt; and &lt;a href="https://huggingface.co/spaces?sort=likes&amp;amp;search=tts"&gt;TTS space&lt;/a&gt; on Hugging Face thanks to people smashing that like button.&lt;/p&gt; &lt;p&gt;Now, I'm considering making another call for crowdsourced data, except this time with a focus on only ChatGPT Advanced Voice Mode text/audio pairs, likely just in English, spanning whatever emotions people can prompt out of it. If successful, it could result in a substantially better &lt;em&gt;conversational&lt;/em&gt; model within the same size class, albeit more limited on voices and languages.&lt;/p&gt; &lt;p&gt;There are many things to consider:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Top priority would be given to paying ChatGPT subscribers, $20 and $200, but in practice free AVM audio would likely be admitted as well. This is because the paying subscribers would be least likely to be using a quantized and/or distilled AVM product.&lt;/li&gt; &lt;li&gt;Ideally I could maximally open source any voicepack derived from the AVM data, which means the people contributing audio would have to do it for ideological reasons, and couldn't be compensated with an &amp;quot;exclusive voicepack&amp;quot;. Also, any sponsorships I receive are directed at GPU compute, and both on principle + potential legal liability, I cannot financially compensate people who give me synthetic data.&lt;/li&gt; &lt;li&gt;As far as ToS goes, this distillation strategy rests on the fact that I am not the one obtaining the data, others are. Obviously, I do not agree with the OpenAI ToS or feel bound by it because I don't use any of their products. Feel free to comment on how dumb this strategy is.&lt;/li&gt; &lt;li&gt;I have skimmed Part 2 of the US Copyright Office's Report on AI. I still see no copyright protection on synthetic data of this nature, but any lawyers (real or wannabe) can chime in here with the default prefix of IANL.&lt;/li&gt; &lt;li&gt;I do not wish to be sued, and I'm also deeply allergic to .50 caliber bullets. Jokes aside, I think OpenAI likely has bigger whales to fry, than some guy training 82M param speech models.&lt;/li&gt; &lt;li&gt;Why do it: these small TTS models are (relatively) cheap to train, especially compared to LLMs, and the total utility they offer might exceed their cost, at least for now, until Zucc drops Llama 4 multimodal or DeepSeek puts up a good audio model, etc.&lt;/li&gt; &lt;li&gt;The scale of data I am looking for is at least 10 hours per voice/emotion, but label quality also matters. Each audio file would have to be fished out one-by-one, since there are no API calls for AVM.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I understand this is LocalLLaMA and people here are likely very pro-open-weights, pro-open-source, and therefore anti-OpenAI. But putting aside any feelings you might have about various sides of history, (A) how do we generally feel about building a model this way and (B) do we think enough people would answer the call?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig8ve3/americans_can_distill_models_too/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig8ve3/americans_can_distill_models_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig8ve3/americans_can_distill_models_too/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T21:55:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1igf1vi</id>
    <title>Phi 4 is so underrated</title>
    <updated>2025-02-03T02:49:57+00:00</updated>
    <author>
      <name>/u/jeremyckahn</name>
      <uri>https://old.reddit.com/user/jeremyckahn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a GPU poor pleb with but a humble M4 Mac mini (24 GB RAM), my local LLM options are limited. As such, I've found Phi 4 (&lt;a href="https://huggingface.co/unsloth/phi-4-GGUF"&gt;Q8, Unsloth variant&lt;/a&gt;) to be an extremely capable model for my hardware. My use cases are general knowledge questions and coding prompts. It's at least as good as GPT 3.5 in my experience and sets me on the right direction more often then not. I can't speak to benchmarks because I don't really understand (or frankly care about) any of them. It's just a good model for the things I need a model for.&lt;/p&gt; &lt;p&gt;And no, Microsoft isn't paying me. I'm just a fan. üôÇ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jeremyckahn"&gt; /u/jeremyckahn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igf1vi/phi_4_is_so_underrated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igf1vi/phi_4_is_so_underrated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igf1vi/phi_4_is_so_underrated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T02:49:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig2cm2</id>
    <title>mistral-small-24b-instruct-2501 is simply the best model ever made.</title>
    <updated>2025-02-02T17:25:29+00:00</updated>
    <author>
      <name>/u/hannibal27</name>
      <uri>https://old.reddit.com/user/hannibal27</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It‚Äôs the only truly good model that can run locally on a normal machine. I'm running it on my M3 36GB and it performs fantastically with 18 TPS (tokens per second). It responds to everything precisely for day-to-day use, serving me as well as ChatGPT does.&lt;/p&gt; &lt;p&gt;For the first time, I see a local model actually delivering satisfactory results. Does anyone else think so?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hannibal27"&gt; /u/hannibal27 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T17:25:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1igcvol</id>
    <title>I built a silent speech recognition tool that reads your lips in real-time and types whatever you mouth - runs 100% locally!</title>
    <updated>2025-02-03T01:00:09+00:00</updated>
    <author>
      <name>/u/tycho_brahes_nose_</name>
      <uri>https://old.reddit.com/user/tycho_brahes_nose_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcvol/i_built_a_silent_speech_recognition_tool_that/"&gt; &lt;img alt="I built a silent speech recognition tool that reads your lips in real-time and types whatever you mouth - runs 100% locally!" src="https://external-preview.redd.it/bnIwMGoyaXludGdlMVL1KlPwXSM4mwFtLRlx6KM67CArRsK705RfUy_x1msn.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b064a65e7251b4b07e096a39fc4d698d7f457b36" title="I built a silent speech recognition tool that reads your lips in real-time and types whatever you mouth - runs 100% locally!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tycho_brahes_nose_"&gt; /u/tycho_brahes_nose_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dh90m1iyntge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcvol/i_built_a_silent_speech_recognition_tool_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igcvol/i_built_a_silent_speech_recognition_tool_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T01:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig6e6t</id>
    <title>DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt.</title>
    <updated>2025-02-02T20:12:17+00:00</updated>
    <author>
      <name>/u/Qaxar</name>
      <uri>https://old.reddit.com/user/Qaxar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"&gt; &lt;img alt="DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt." src="https://external-preview.redd.it/Er7i7V1ka8BO-MpGkuLs0Jmvu0-6GTVfn9JqY2PTKfM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd56ea2fa742541be1366b6615889d6a52f560b3" title="DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We knew R1 was good, but not that good. All the cries of CCP censorship are meaningless when it's trivial to bypass its guard rails.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qaxar"&gt; /u/Qaxar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/rohanpaul_ai/status/1886025249273339961?t=Wpp2kGJKVSZtSAOmTJjh0g&amp;amp;s=19"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T20:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1igc6r0</id>
    <title>20 yrs in jail or $1 million for downloading Chinese models proposed at congress</title>
    <updated>2025-02-03T00:26:00+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.hawley.senate.gov/wp-content/uploads/2025/01/Hawley-Decoupling-Americas-Artificial-Intelligence-Capabilities-from-China-Act.pdf"&gt;https://www.hawley.senate.gov/wp-content/uploads/2025/01/Hawley-Decoupling-Americas-Artificial-Intelligence-Capabilities-from-China-Act.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Seriously stop giving your money to these anti open companies and encourage everyone and anyone you know to do the same, don't let your company use their products. Anthrophic and OpenAI are the worse. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igc6r0/20_yrs_in_jail_or_1_million_for_downloading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igc6r0/20_yrs_in_jail_or_1_million_for_downloading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igc6r0/20_yrs_in_jail_or_1_million_for_downloading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T00:26:00+00:00</published>
  </entry>
</feed>
