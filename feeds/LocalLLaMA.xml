<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-10T04:07:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hwzmqc</id>
    <title>Phi-4 Llamafied + 4 Bug Fixes + GGUFs, Dynamic 4bit Quants</title>
    <updated>2025-01-09T00:20:50+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwzmqc/phi4_llamafied_4_bug_fixes_ggufs_dynamic_4bit/"&gt; &lt;img alt="Phi-4 Llamafied + 4 Bug Fixes + GGUFs, Dynamic 4bit Quants" src="https://b.thumbs.redditmedia.com/0aD2C0A02Gg08d6G5AgWeGQyp6FZmThRfyFVw15avlQ.jpg" title="Phi-4 Llamafied + 4 Bug Fixes + GGUFs, Dynamic 4bit Quants" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ! I've uploaded &lt;strong&gt;fixed versions&lt;/strong&gt; of &lt;a href="https://unsloth.ai/blog/phi4"&gt;Phi-4&lt;/a&gt;, including GGUF + 4-bit + 16-bit versions on HuggingFace!&lt;/p&gt; &lt;p&gt;We’ve fixed over &lt;strong&gt;4 bugs (3 major ones)&lt;/strong&gt; in Phi-4, mainly related to tokenizers and chat templates which affected inference and finetuning workloads. If you were experiencing poor results, we recommend trying our GGUF upload. A detailed post on the fixes will be released tomorrow.&lt;/p&gt; &lt;p&gt;We also &lt;strong&gt;Llamafied&lt;/strong&gt; the model meaning it should work out of the box with every framework including &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;. Fine-tuning is &lt;strong&gt;2x faster, uses 70% VRAM&lt;/strong&gt; &amp;amp; has 9x longer context lengths with Unsloth.&lt;/p&gt; &lt;p&gt;View all Phi-4 versions with our bug fixes: &lt;a href="https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa"&gt;https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Phi-4 Uploads (with our bug fixes)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4-GGUF"&gt;GGUFs&lt;/a&gt; including 2, 3, 4, 5, 6, 8, 16-bit&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4-unsloth-bnb-4bit"&gt;Unsloth Dynamic 4-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4-bnb-4bit"&gt;4-bit Bnb&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4"&gt;Original 16-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I uploaded Q2_K_L quants which works well as well - they are Q2_K quants, but leaves the embedding as Q4 and lm_head as Q6 - this should increase accuracy by a bit!&lt;/p&gt; &lt;p&gt;To use Phi-4 in llama.cpp, do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama.cpp/llama-cli --model unsloth/phi-4-GGUF/phi-4-Q2_K_L.gguf --prompt '&amp;lt;|im_start|&amp;gt;user&amp;lt;|im_sep|&amp;gt;Provide all combinations of a 5 bit binary number.&amp;lt;|im_end|&amp;gt;&amp;lt;|im_start|&amp;gt;assistant&amp;lt;|im_sep|&amp;gt;' --threads 16 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Which will produce:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;A 5-bit binary number consists of 5 positions, each of which can be either 0 or 1. Therefore, there are \(2^5 = 32\) possible combinations. Here they are, listed in ascending order: 1. 00000 2. 00001 3. 00010 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I also uploaded &lt;strong&gt;Dynamic 4bit quants&lt;/strong&gt; which don't quantize every layer to 4bit, and leaves some in 16bit - by using only an extra 1GB of VRAM, you get superior accuracy, especially for finetuning! - Head over to &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt; to finetune LLMs and Vision models 2x faster and use 70% less VRAM!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/74tja0m83vbe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=99a09dc6e9ad8c1d8e8e19ad519ccaaabebde3a8"&gt;Dynamic 4bit quants leave some layers as 16bit and not 4bit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwzmqc/phi4_llamafied_4_bug_fixes_ggufs_dynamic_4bit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwzmqc/phi4_llamafied_4_bug_fixes_ggufs_dynamic_4bit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwzmqc/phi4_llamafied_4_bug_fixes_ggufs_dynamic_4bit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T00:20:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx6sjc</id>
    <title>Weirdly good finetune - QwQ-LCoT-7B-Instruct</title>
    <updated>2025-01-09T06:33:12+00:00</updated>
    <author>
      <name>/u/1ncehost</name>
      <uri>https://old.reddit.com/user/1ncehost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use a lot of complex, large-context coding prompts that are high on the difficulty scale using &lt;a href="https://github.com/curvedinf/dir-assistant"&gt;https://github.com/curvedinf/dir-assistant&lt;/a&gt; . I've been using APIs for a number of months since prices have come down, but I just did a round of tests in the 7B-14B range. I found this tune randomly while browsing huggingface and it has a whole 304 downloads, but damn is it good. Its consistently outperforming newer 32B models, and older 70B models in my tests. I don't know what the secret is here, but I just wanted to pass this along. I test a LOT of models, and this one is weirdly good for coding.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/prithivMLmods/QwQ-LCoT-7B-Instruct"&gt;https://huggingface.co/prithivMLmods/QwQ-LCoT-7B-Instruct&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/bartowski/QwQ-LCoT-7B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/QwQ-LCoT-7B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1ncehost"&gt; /u/1ncehost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx6sjc/weirdly_good_finetune_qwqlcot7binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx6sjc/weirdly_good_finetune_qwqlcot7binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx6sjc/weirdly_good_finetune_qwqlcot7binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T06:33:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxh3ln</id>
    <title>Open-source AI web agent powered by Llama-3.3</title>
    <updated>2025-01-09T16:46:42+00:00</updated>
    <author>
      <name>/u/spacespacespapce</name>
      <uri>https://old.reddit.com/user/spacespacespapce</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I previously shared a &lt;a href="https://sh.reddit.com/r/LocalLLaMA/comments/1hlzja2/asking_an_ai_agent_powered_by_llama33_find_me_2/"&gt;demo&lt;/a&gt; of my web agent that the community loved, so wanted to share another update:&lt;/p&gt; &lt;p&gt;The agent is now fully &lt;a href="https://github.com/addy999/onequery"&gt;open source&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;You can run it with Ollama, Anthropic or even DeepSeek. All work well but I haven't done a deep comparison yet.&lt;/p&gt; &lt;p&gt;Comments and contributions are welcome - the project is still under development. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spacespacespapce"&gt; /u/spacespacespapce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxh3ln/opensource_ai_web_agent_powered_by_llama33/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxh3ln/opensource_ai_web_agent_powered_by_llama33/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxh3ln/opensource_ai_web_agent_powered_by_llama33/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T16:46:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxl5ze</id>
    <title>Thoughts on using Ampere Altra for running LLMs locally?</title>
    <updated>2025-01-09T19:37:32+00:00</updated>
    <author>
      <name>/u/cafedude</name>
      <uri>https://old.reddit.com/user/cafedude</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ampere ARM Altra offers a memory bandwidth supported by eight 72-bit DDR4-3200 channels. Motherboards and cpus can be bought now in retail channels like newegg, for example: &lt;a href="https://www.newegg.com/asrock-rack-altrad8ud-1l2t-q64-22-ampere-altra-max-ampere-altra-processors/p/N82E16813140134"&gt;https://www.newegg.com/asrock-rack-altrad8ud-1l2t-q64-22-ampere-altra-max-ampere-altra-processors/p/N82E16813140134&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Was seeing some disappointment in the low memory bandwidth for the NVidia Digits and Strix Halo systems and wondered if perhaps a system like this based on Ampere Altra having 8 DDR4 channels might be another way to go here.&lt;/p&gt; &lt;p&gt;EDIT: Oh, I see it's DDR4 not DDR5 (AmpereOne supports DDR5, but not seeing motherboards supporting that in a consumer price range) so maybe that answers my question.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cafedude"&gt; /u/cafedude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxl5ze/thoughts_on_using_ampere_altra_for_running_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxl5ze/thoughts_on_using_ampere_altra_for_running_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxl5ze/thoughts_on_using_ampere_altra_for_running_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T19:37:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxledt</id>
    <title>Agent Laboratory: Using LLM Agents as Research Assistants - Autonomous LLM-based Framework Capable of Completing the Entire Research Process</title>
    <updated>2025-01-09T19:47:06+00:00</updated>
    <author>
      <name>/u/Singularian2501</name>
      <uri>https://old.reddit.com/user/Singularian2501</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxledt/agent_laboratory_using_llm_agents_as_research/"&gt; &lt;img alt="Agent Laboratory: Using LLM Agents as Research Assistants - Autonomous LLM-based Framework Capable of Completing the Entire Research Process " src="https://b.thumbs.redditmedia.com/zIICBVnKc8rB7boEj9p5Y9BQP83ermOVLEkfX87XWKk.jpg" title="Agent Laboratory: Using LLM Agents as Research Assistants - Autonomous LLM-based Framework Capable of Completing the Entire Research Process " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/pdf/2501.04227"&gt;https://arxiv.org/pdf/2501.04227&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/SamuelSchmidgall/AgentLaboratory?tab=readme-ov-file"&gt;https://github.com/SamuelSchmidgall/AgentLaboratory?tab=readme-ov-file&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://agentlaboratory.github.io/"&gt;https://agentlaboratory.github.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Abstract:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (&lt;strong&gt;1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sgnmwjgiu0ce1.jpg?width=1591&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f0de015073ef3809ce1613cbeba4b73de167c71c"&gt;https://preview.redd.it/sgnmwjgiu0ce1.jpg?width=1591&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f0de015073ef3809ce1613cbeba4b73de167c71c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Singularian2501"&gt; /u/Singularian2501 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxledt/agent_laboratory_using_llm_agents_as_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxledt/agent_laboratory_using_llm_agents_as_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxledt/agent_laboratory_using_llm_agents_as_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T19:47:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxp4ts</id>
    <title>Dir-assistant 1.2.0 release announcement</title>
    <updated>2025-01-09T22:27:36+00:00</updated>
    <author>
      <name>/u/1ncehost</name>
      <uri>https://old.reddit.com/user/1ncehost</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxp4ts/dirassistant_120_release_announcement/"&gt; &lt;img alt="Dir-assistant 1.2.0 release announcement" src="https://external-preview.redd.it/cbCrbPfInObUjKiPPu0cM8HUQ3itne_1HWLAeqIxB-0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ed377ba64d4878334b6daad2e0ddda96ed951b0" title="Dir-assistant 1.2.0 release announcement" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Dir-assistant: Chat with your current directory's files using a local or API LLM.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is the first minor release I've pushed up in a few months and it is adds a major feature that I've been looking forward to. Previously you could only use local open source models for generating the embedding vectors used by the RAG, but now I've added the ability to use API embeddings. Compared to local embedding models, API embedding models are generally higher quality and will improve RAG lookup quality if used, which will further improve context generation and thus results.&lt;/p&gt; &lt;p&gt;Please report bugs to the github issues.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/curvedinf/dir-assistant/"&gt;https://github.com/curvedinf/dir-assistant/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/biml6z2on1ce1.gif"&gt;https://i.redd.it/biml6z2on1ce1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1ncehost"&gt; /u/1ncehost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxp4ts/dirassistant_120_release_announcement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxp4ts/dirassistant_120_release_announcement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxp4ts/dirassistant_120_release_announcement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T22:27:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxe2cy</id>
    <title>is QwQ the best local model for CoT/reasoning?</title>
    <updated>2025-01-09T14:32:59+00:00</updated>
    <author>
      <name>/u/Moreh</name>
      <uri>https://old.reddit.com/user/Moreh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As per title. I can go up to 100b parameters.&lt;/p&gt; &lt;p&gt;I'm running a script that requires an llm to classify text for a charity project. The concept that is being classified is quite complex and subjective that requires multiple tests to pass before it can be labelled as positive (this is why CoT works well). QwQ seems to do better than 72b Qwen 2.5, but given i have the hardware i wonder if there is a larger/better alternative. &lt;/p&gt; &lt;p&gt;I know I can implement my own kind of CoT but if there's one fine tuned already i thought i may as well look at that!&lt;/p&gt; &lt;p&gt;thankyou&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moreh"&gt; /u/Moreh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxe2cy/is_qwq_the_best_local_model_for_cotreasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxe2cy/is_qwq_the_best_local_model_for_cotreasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxe2cy/is_qwq_the_best_local_model_for_cotreasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T14:32:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxu0om</id>
    <title>Open-source web crawler with markdown output?</title>
    <updated>2025-01-10T02:21:36+00:00</updated>
    <author>
      <name>/u/mxdamp</name>
      <uri>https://old.reddit.com/user/mxdamp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m looking for an open-source web crawler that can recursively crawl a website (e.g., API documentation) and convert the content into markdown files. I'd like to use the markdown files in RAG applications. I've found &lt;a href="https://github.com/unclecode/crawl4ai"&gt;Crawl4AI&lt;/a&gt; and &lt;a href="https://www.firecrawl.dev"&gt;Firecrawl&lt;/a&gt;, but I'd prefer a free TUI or GUI application.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mxdamp"&gt; /u/mxdamp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxu0om/opensource_web_crawler_with_markdown_output/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxu0om/opensource_web_crawler_with_markdown_output/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxu0om/opensource_web_crawler_with_markdown_output/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T02:21:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxgfu5</id>
    <title>We've just released LLM Pools, end-to-end deployment of Large Language Models that can be installed anywhere</title>
    <updated>2025-01-09T16:19:15+00:00</updated>
    <author>
      <name>/u/Good-Coconut3907</name>
      <uri>https://old.reddit.com/user/Good-Coconut3907</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLM Pools are all inclusive environments that can be installed on everyday hardware to simplify LLM deployment. Compatible with a multitude of model engines, out-of-the-box single and multi-node friendly, with a single API endpoint + UI playground.&lt;/p&gt; &lt;p&gt;Currently supported model engines: vLLM, llama.cpp, Aphrodite Engine and Petals, all in single node and multinode fashion. More to come!&lt;/p&gt; &lt;p&gt;You can install your own for free, but the easiest way to get started is joining our public LLM pool (also free, and you get to share each other models): &lt;a href="https://kalavai-net.github.io/kalavai-client/public_llm_pool/"&gt;https://kalavai-net.github.io/kalavai-client/public_llm_pool/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Open source: &lt;a href="https://github.com/kalavai-net/kalavai-client"&gt;https://github.com/kalavai-net/kalavai-client&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Good-Coconut3907"&gt; /u/Good-Coconut3907 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxgfu5/weve_just_released_llm_pools_endtoend_deployment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxgfu5/weve_just_released_llm_pools_endtoend_deployment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxgfu5/weve_just_released_llm_pools_endtoend_deployment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T16:19:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxqql3</id>
    <title>What is your favorite model for fine-tuning?</title>
    <updated>2025-01-09T23:39:05+00:00</updated>
    <author>
      <name>/u/Suitable-Name</name>
      <uri>https://old.reddit.com/user/Suitable-Name</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt; &lt;p&gt;question for those who did fine-tuning on current models. What is your favorite so far and why do you like it more than others?&lt;/p&gt; &lt;p&gt;For example, I'm reading often the Phi models aren't that bad, but often a bit &amp;quot;sterile&amp;quot;. I wonder if it still would be a good base model for a fine-tuning? Did someone compare results between a tuned llama and phi for example?&lt;/p&gt; &lt;p&gt;Thanks for your input!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suitable-Name"&gt; /u/Suitable-Name &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxqql3/what_is_your_favorite_model_for_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxqql3/what_is_your_favorite_model_for_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxqql3/what_is_your_favorite_model_for_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T23:39:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx8nex</id>
    <title>"rStar-Math demonstrates that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising "deep thinking" through Monte Carlo Tree Search (MCTS)....."</title>
    <updated>2025-01-09T08:51:09+00:00</updated>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx8nex/rstarmath_demonstrates_that_small_language_models/"&gt; &lt;img alt="&amp;quot;rStar-Math demonstrates that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising &amp;quot;deep thinking&amp;quot; through Monte Carlo Tree Search (MCTS).....&amp;quot;" src="https://b.thumbs.redditmedia.com/HJz73domuc-71OAJfI-YWNodO1IrhFgRKSFmybqiieU.jpg" title="&amp;quot;rStar-Math demonstrates that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising &amp;quot;deep thinking&amp;quot; through Monte Carlo Tree Search (MCTS).....&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising &amp;quot;deep thinking&amp;quot; through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids naïve step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students. &lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2501.04519"&gt;https://arxiv.org/abs/2501.04519&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1hx8nex"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx8nex/rstarmath_demonstrates_that_small_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx8nex/rstarmath_demonstrates_that_small_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T08:51:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxg435</id>
    <title>Introducing LongTalk-CoT v0.1: A Very Long Chain-of-Thought Dataset</title>
    <updated>2025-01-09T16:05:42+00:00</updated>
    <author>
      <name>/u/Financial_Counter199</name>
      <uri>https://old.reddit.com/user/Financial_Counter199</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m excited to release &lt;a href="https://huggingface.co/datasets/kenhktsui/longtalk-cot-v0.1"&gt;LongTalk-CoT v0.1&lt;/a&gt;, a dataset designed for training o1-like reasoning model. Each response is prompted using QwQ-32B-Preview, and specifically handcrafted system message that encourages &lt;strong&gt;more vocalised thinking&lt;/strong&gt;, and &lt;strong&gt;self reflection&lt;/strong&gt;. As such, its output token length is 5.29x longer than HuggingFaceTB/smoltalk&lt;/p&gt; &lt;p&gt;It is able to solve &amp;quot;Is 9.11 greater than 9.9&amp;quot; and &amp;quot;How many letters R in the word strawberry?&amp;quot;!&lt;/p&gt; &lt;p&gt;It is overthinking sometimes as expected, but I love its behavior of trying multiple approaches to solve a question.&lt;/p&gt; &lt;h1&gt;Quick Test 1: Is 9.11 greater than 9.9?&lt;/h1&gt; &lt;h1&gt;After finetuning&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;No, 9.11 is not greater than 9.9. Let's think about this step by step. First, it helps to understand that both numbers are decimal numbers, specifically they have two digits after the decimal point. So, 9.11 and 9.9 can be compared directly based on their values in each place value: whole number part and the tenths and hundredths parts. Let's break them down: * For 9.11: * The whole number part is 9. * The tenths place is 1 (which means one tenth). * The hundredths place is also 1 (one hundredth). * For 9.9: * The whole number part is also 9. * But here, the tenths place is 9 (nine tenths), and there's no hundredths digit after that; it's considered to have zero in the hundredths place because decimal numbers with fewer digits are filled with zeros. So, comparing them: * Both have the same whole number part: 9. * For the tenth place: * 1 vs. 9. Since 9 is greater than 1, this already tells us that 9.9 is larger than 9.11. But to be thorough, let's look at the hundredths places as well: * In 9.11: hundredths = 1 * In 9.9: hundredths = 0 Even though there are no digits in the hundredths place for 9.9, it's still considered zero because of the way decimals work. Therefore, overall, 9.11 &amp;lt; 9.9. Wait a minute, but I should make sure that I'm not missing anything here. Sometimes, people might think about adding or subtracting numbers differently, but in this case, since both numbers have the same whole number part and one has a larger tenths digit, it's clear which is larger. Another way to look at it is to consider them as percentages: * 9.11 = 911% * 9.9 = 990% But that might be confusing; in percentage terms, 990% is still greater than 911%. Alternatively, if we multiply both numbers by 100 to eliminate the decimal point: * 9.11 \* 100 = 911 * 9.9 \* 100 = 990 Again, 990 is greater than 911. So, no matter how we look at it, 9.9 is larger than 9.11. I think that covers the comparison thoroughly. There doesn't seem to be any trick or special case here; it's a straightforward decimal comparison once you understand place values. **Final Answer** \[ \\boxed{9.11 &amp;lt; 9.9} \] &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Before finetuning&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;No, 9.11 is not greater than 9.9. When comparing decimal numbers, you can compare them digit by digit from left to right. In this case: * The whole number part of both numbers is the same (9). * Comparing the tenths place, we see that 1 in 9.11 is larger than 9 in 9.9. * Therefore, 9.11 &amp;gt; 9.9. So, 9.11 is greater than 9.9. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial_Counter199"&gt; /u/Financial_Counter199 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxg435/introducing_longtalkcot_v01_a_very_long/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxg435/introducing_longtalkcot_v01_a_very_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxg435/introducing_longtalkcot_v01_a_very_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T16:05:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx60t4</id>
    <title>New Microsoft research - rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking</title>
    <updated>2025-01-09T05:43:37+00:00</updated>
    <author>
      <name>/u/Ok_Landscape_6819</name>
      <uri>https://old.reddit.com/user/Ok_Landscape_6819</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2501.04519"&gt;https://arxiv.org/abs/2501.04519&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen2.5-Math-7B reaches 90% on MATH with this new technique. Phi3-mini-3.8B reaches 86.4%..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Landscape_6819"&gt; /u/Ok_Landscape_6819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx60t4/new_microsoft_research_rstarmath_small_llms_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx60t4/new_microsoft_research_rstarmath_small_llms_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx60t4/new_microsoft_research_rstarmath_small_llms_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T05:43:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx5i8u</id>
    <title>Phi 4 is just 14B But Better than llama 3.1 70b for several tasks.</title>
    <updated>2025-01-09T05:12:08+00:00</updated>
    <author>
      <name>/u/Vishnu_One</name>
      <uri>https://old.reddit.com/user/Vishnu_One</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx5i8u/phi_4_is_just_14b_but_better_than_llama_31_70b/"&gt; &lt;img alt="Phi 4 is just 14B But Better than llama 3.1 70b for several tasks. " src="https://preview.redd.it/uwfo8ig8jwbe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06707621fd601095edb87fea07007bbf976f658b" title="Phi 4 is just 14B But Better than llama 3.1 70b for several tasks. " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vishnu_One"&gt; /u/Vishnu_One &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uwfo8ig8jwbe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx5i8u/phi_4_is_just_14b_but_better_than_llama_31_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx5i8u/phi_4_is_just_14b_but_better_than_llama_31_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T05:12:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwwvuz</id>
    <title>This sums my experience with models on Groq</title>
    <updated>2025-01-08T22:27:22+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwwvuz/this_sums_my_experience_with_models_on_groq/"&gt; &lt;img alt="This sums my experience with models on Groq" src="https://preview.redd.it/7tqzm8bsiube1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bebd5aeeb53de86390096a6377b9d96fe453c674" title="This sums my experience with models on Groq" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7tqzm8bsiube1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwwvuz/this_sums_my_experience_with_models_on_groq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwwvuz/this_sums_my_experience_with_models_on_groq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T22:27:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxitud</id>
    <title>Ask, and it shall be given: Turing completeness of prompting</title>
    <updated>2025-01-09T17:59:36+00:00</updated>
    <author>
      <name>/u/CommunismDoesntWork</name>
      <uri>https://old.reddit.com/user/CommunismDoesntWork</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CommunismDoesntWork"&gt; /u/CommunismDoesntWork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2411.01992"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxitud/ask_and_it_shall_be_given_turing_completeness_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxitud/ask_and_it_shall_be_given_turing_completeness_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T17:59:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxibs2</id>
    <title>vikhyatk/moondream2 · Hugging Face (New 2B &amp; 0.5B)</title>
    <updated>2025-01-09T17:38:26+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxibs2/vikhyatkmoondream2_hugging_face_new_2b_05b/"&gt; &lt;img alt="vikhyatk/moondream2 · Hugging Face (New 2B &amp;amp; 0.5B)" src="https://external-preview.redd.it/mBbXqyUSUFt03PXE9oC_lqnJ1RBSkMJjxd4Z-R3eomo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a26635c940a449443fafcc84f621627681d2cfdb" title="vikhyatk/moondream2 · Hugging Face (New 2B &amp;amp; 0.5B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/vikhyatk/moondream2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxibs2/vikhyatkmoondream2_hugging_face_new_2b_05b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxibs2/vikhyatkmoondream2_hugging_face_new_2b_05b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T17:38:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx99oi</id>
    <title>Former OpenAI employee Miles Brundage: "o1 is just an LLM though, no reasoning infrastructure. The reasoning is in the chain of thought." Current OpenAI employee roon: "Miles literally knows what o1 does."</title>
    <updated>2025-01-09T09:39:31+00:00</updated>
    <author>
      <name>/u/Wiskkey</name>
      <uri>https://old.reddit.com/user/Wiskkey</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx99oi/former_openai_employee_miles_brundage_o1_is_just/"&gt; &lt;img alt="Former OpenAI employee Miles Brundage: &amp;quot;o1 is just an LLM though, no reasoning infrastructure. The reasoning is in the chain of thought.&amp;quot; Current OpenAI employee roon: &amp;quot;Miles literally knows what o1 does.&amp;quot;" src="https://a.thumbs.redditmedia.com/jvhF7IPWy1xMdzkV7pN0IGKD30vdwZ9p8xqaMkVtK34.jpg" title="Former OpenAI employee Miles Brundage: &amp;quot;o1 is just an LLM though, no reasoning infrastructure. The reasoning is in the chain of thought.&amp;quot; Current OpenAI employee roon: &amp;quot;Miles literally knows what o1 does.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wiskkey"&gt; /u/Wiskkey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1hx99oi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx99oi/former_openai_employee_miles_brundage_o1_is_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx99oi/former_openai_employee_miles_brundage_o1_is_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T09:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx7421</id>
    <title>TransPixar: a new generative model that preserves transparency,</title>
    <updated>2025-01-09T06:55:51+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx7421/transpixar_a_new_generative_model_that_preserves/"&gt; &lt;img alt="TransPixar: a new generative model that preserves transparency," src="https://external-preview.redd.it/aHFsc2gwdXExeGJlMYcphE9YFRyNCrr76DvwiShDtswDzb2s93cwaOHelUg2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd6433e0c057e8d365fab3d5289d9a28fb641a1f" title="TransPixar: a new generative model that preserves transparency," /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8fhb41uq1xbe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx7421/transpixar_a_new_generative_model_that_preserves/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx7421/transpixar_a_new_generative_model_that_preserves/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T06:55:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxoh4q</id>
    <title>The Best Embedding Models for Information Retrieval in 2025</title>
    <updated>2025-01-09T21:58:49+00:00</updated>
    <author>
      <name>/u/philnash</name>
      <uri>https://old.reddit.com/user/philnash</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxoh4q/the_best_embedding_models_for_information/"&gt; &lt;img alt="The Best Embedding Models for Information Retrieval in 2025" src="https://external-preview.redd.it/VnuuZheactI7QPo2JPvHBZE9r-qgyUUp4UDj8IpsDuI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7b8b3d34b72d1de75ead7b836c4ed7867bf2f4eb" title="The Best Embedding Models for Information Retrieval in 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philnash"&gt; /u/philnash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.datastax.com/blog/best-embedding-models-information-retrieval-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxoh4q/the_best_embedding_models_for_information/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxoh4q/the_best_embedding_models_for_information/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T21:58:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxuprn</id>
    <title>Project Digits: How NVIDIA's $3,000 AI Supercomputer Could Democratize Local AI Development | Caveman Press</title>
    <updated>2025-01-10T02:57:55+00:00</updated>
    <author>
      <name>/u/Far_Monk</name>
      <uri>https://old.reddit.com/user/Far_Monk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxuprn/project_digits_how_nvidias_3000_ai_supercomputer/"&gt; &lt;img alt="Project Digits: How NVIDIA's $3,000 AI Supercomputer Could Democratize Local AI Development | Caveman Press" src="https://external-preview.redd.it/HgKLjMbpNpthwaWMjENNhdcaMCpcPUXsoT1-E8Xzlu0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=277871f1227bb080da6275c3cfd987301cfc058a" title="Project Digits: How NVIDIA's $3,000 AI Supercomputer Could Democratize Local AI Development | Caveman Press" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far_Monk"&gt; /u/Far_Monk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.caveman.press/article/project-digits-nvidia-3000-ai-supercomputer-democratize-development"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxuprn/project_digits_how_nvidias_3000_ai_supercomputer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxuprn/project_digits_how_nvidias_3000_ai_supercomputer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T02:57:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxq364</id>
    <title>RTX 4090 48GB - $4700 on eBay. Is it legit?</title>
    <updated>2025-01-09T23:09:20+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxq364/rtx_4090_48gb_4700_on_ebay_is_it_legit/"&gt; &lt;img alt="RTX 4090 48GB - $4700 on eBay. Is it legit?" src="https://external-preview.redd.it/bMoZdiDXs9kYJMXSF2LPqzveI23P9l8lgEc9DnQ7Gg8.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b4491fc39b4da2e3a9466072f17bde0fad92791" title="RTX 4090 48GB - $4700 on eBay. Is it legit?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just came across this listing on eBay: &lt;a href="https://www.ebay.com/itm/226494741895"&gt;https://www.ebay.com/itm/226494741895&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is listing dual slot RTX 4090 48GB for $4700. I thought 48GB were not manufactured. Is it legit?&lt;/p&gt; &lt;p&gt;Screenshot here if it gets lost.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0ggim0y1v1ce1.png?width=3302&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51e939b07a3a8b3bf9e6cebd0e5fe4822664f985"&gt;RTX 4090 48GB for $4700!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I found out in this post (&lt;a href="https://github.com/ggerganov/llama.cpp/discussions/9193"&gt;https://github.com/ggerganov/llama.cpp/discussions/9193&lt;/a&gt;) that one could buy it for ~$3500. I think RTX 4090 48GB would sell instantly if it was $3k.&lt;/p&gt; &lt;p&gt;Update: for me personally, It is better to buy 2x 5090 for the same price to get 64GB total VRAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxq364/rtx_4090_48gb_4700_on_ebay_is_it_legit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxq364/rtx_4090_48gb_4700_on_ebay_is_it_legit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxq364/rtx_4090_48gb_4700_on_ebay_is_it_legit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T23:09:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxpjey</id>
    <title>Phi-3.5-MoE support merged into llama.cpp</title>
    <updated>2025-01-09T22:45:29+00:00</updated>
    <author>
      <name>/u/skeeto</name>
      <uri>https://old.reddit.com/user/skeeto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxpjey/phi35moe_support_merged_into_llamacpp/"&gt; &lt;img alt="Phi-3.5-MoE support merged into llama.cpp" src="https://external-preview.redd.it/m74SPCYv3b2-Etc8gMsq03I8DcmHrV56tfhuw4oGpHM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5aef7783c2f074e3447066229f618ac46fcef97" title="Phi-3.5-MoE support merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skeeto"&gt; /u/skeeto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggerganov/llama.cpp/pull/11003"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxpjey/phi35moe_support_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxpjey/phi35moe_support_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T22:45:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxjzol</id>
    <title>New Moondream 2B vision language model release</title>
    <updated>2025-01-09T18:48:07+00:00</updated>
    <author>
      <name>/u/radiiquark</name>
      <uri>https://old.reddit.com/user/radiiquark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxjzol/new_moondream_2b_vision_language_model_release/"&gt; &lt;img alt="New Moondream 2B vision language model release" src="https://preview.redd.it/oyxiuuxok0ce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad794e5897db0faf8abb4f80573d4a142cd70939" title="New Moondream 2B vision language model release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/radiiquark"&gt; /u/radiiquark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oyxiuuxok0ce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxjzol/new_moondream_2b_vision_language_model_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxjzol/new_moondream_2b_vision_language_model_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T18:48:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxm0ep</id>
    <title>Anyone want the script to run Moondream 2b's new gaze detection on any video?</title>
    <updated>2025-01-09T20:12:41+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxm0ep/anyone_want_the_script_to_run_moondream_2bs_new/"&gt; &lt;img alt="Anyone want the script to run Moondream 2b's new gaze detection on any video?" src="https://external-preview.redd.it/cmk5cnZsYXZ6MGNlMeEGpTWo5MaI3KFBwDeey6o_wri3pXWzYnC4YTD3TTIr.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0330607dc613a7fe62b098a05b7a0c3c3e3495dc" title="Anyone want the script to run Moondream 2b's new gaze detection on any video?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n9beslavz0ce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxm0ep/anyone_want_the_script_to_run_moondream_2bs_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxm0ep/anyone_want_the_script_to_run_moondream_2bs_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T20:12:41+00:00</published>
  </entry>
</feed>
