<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-19T00:26:19+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k1xgco</id>
    <title>vLLM with transformers backend</title>
    <updated>2025-04-18T05:32:35+00:00</updated>
    <author>
      <name>/u/Disastrous-Work-1632</name>
      <uri>https://old.reddit.com/user/Disastrous-Work-1632</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can try out the new integration with which you can run ANY transformers model with vLLM (even if it is not natively supported by vLLM)&lt;/p&gt; &lt;p&gt;Read more about it here: &lt;a href="https://blog.vllm.ai/2025/04/11/transformers-backend.html"&gt;https://blog.vllm.ai/2025/04/11/transformers-backend.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What can one do with this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;1. Read the blog ðŸ˜Œ&lt;/li&gt; &lt;li&gt;2. Contribute to transformers - making models vLLM compatible&lt;/li&gt; &lt;li&gt;3. Raise issues if you spot a bug with the integration&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Vision Language Model support is coming very soon! Until any further announcements, we would love for everyone to stick using this integration with text only models ðŸ¤—&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous-Work-1632"&gt; /u/Disastrous-Work-1632 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1xgco/vllm_with_transformers_backend/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1xgco/vllm_with_transformers_backend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1xgco/vllm_with_transformers_backend/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T05:32:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2htv9</id>
    <title>Critizize and suggest optimizations for my AI rig</title>
    <updated>2025-04-18T22:45:38+00:00</updated>
    <author>
      <name>/u/Excellent-Amount-277</name>
      <uri>https://old.reddit.com/user/Excellent-Amount-277</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Well so I had to chose something - small startup here so the boss said 1000 Euro is the limit. Obviously I wanted to get max VRAM so i talked him into buying a used RTX 3090 from a local classified which imho is the best part of the system. Rest had to be very simple and when chosing I ran a little bit over budget. Well we ended up at 1110.14 Euro total - which was OK...&lt;/p&gt; &lt;p&gt;In general I am satisfied with the system for the price. But before I go into bitchin about parts - here's what we got (Was delivered in January 2025, most parts ordered late cencember 2024):&lt;/p&gt; &lt;p&gt;Intel core i5 12600K 157,90&lt;/p&gt; &lt;p&gt;Asus Prime H610M-K argb 87,31&lt;/p&gt; &lt;p&gt;Xilence M403pro 21,00&lt;/p&gt; &lt;p&gt;Team Group 16gb DDR5-6000 41,17&lt;/p&gt; &lt;p&gt;Team Group 16gb DDR5-6000 41,17&lt;/p&gt; &lt;p&gt;Rajintek Arcadia III case 41,93&lt;/p&gt; &lt;p&gt;Enermax Marblebron RGB 850W 69,66&lt;/p&gt; &lt;p&gt;Nvidia RTX 3090 USED 650,00&lt;/p&gt; &lt;p&gt;KXG50ZNV1T02 TOSHIBA NVME free&lt;/p&gt; &lt;p&gt;-------------------------------------&lt;/p&gt; &lt;p&gt;Total 1110.14&lt;/p&gt; &lt;p&gt;Well the CPU - 10 cores and boost quite OK, for the price I can't complain. I think AMD might have given a bit more for the money, but I used the 12600K before so it was a quick choice. K seems unnecessary with the board but it didn't make much difference i felt. So with the CPU I am quite happy. Ain#t no threadripper but for the price it's OK. and 12th gen doesn't have these quality issues.&lt;/p&gt; &lt;p&gt;Board - that was as low as i could go. 610 - no real tuning chip. At least DDR5 which I insisted on. What I hate most about the board is the lack of slots. ONE PCIE 4.0x16 is enough for the RTX 3090. Sure. But besides that only one PCIE 3.0x1. Mew. I have some cards here like nvme cards to get more storage, but oh well, not gonna use them with this precious single slot I have. Why? It lacks USB-C!!! So maybe gonna get a USB-C controller for that slot. Not having even ONE lame USB-C port in 2025? Come on... Also just ONE nvme slot, so no raid... Got one nvme -that's it. You get what you pay for...&lt;/p&gt; &lt;p&gt;Case - Also terrible choice... No USB-C either... Didn't even think of that It's 2025. Also the case came with 4 (!!!) fans - which I can't connect to the board due to their 3-pin plug. Currently I got it just open but for the summer I may need to either replace the fans or look for some kinda adaptor.&lt;/p&gt; &lt;p&gt;Xilence CPU fan - nothing to complain. Well no AIO, nothing fancy, but for the price it's a really good one. And it desrves the name.&lt;/p&gt; &lt;p&gt;PSU - No idea. Some china stuff I guess. For 70 bucks it does it's job pretty well however. 850W yeah. It had RGB, but personally I could have gone without RGB. It's modular, so that makes it nice and clean. Imma prolly have to attach these SATA cables to it though. Thought SATA is old school but with just one nvme imma need old sata HDDs i fear.&lt;/p&gt; &lt;p&gt;RAM - DDR5-6000 sounds neat. But was a dumb idea since with the 12th gen i5 I run it at 4800. Board won't really let me run more. Seems they lack xmp or i am doing something wrong. Should have gotten cheap 64GB instead. 32 GB is... well bare minimum for some stuff.&lt;/p&gt; &lt;p&gt;GPU - nothing to complain here. 24 GB VRAM and the thing costed us 650 Bucks. Yeah used. But look at current prices and you know why I wanted to build the whole rig around it. It's an ASUS TUF gaming 3090.&lt;/p&gt; &lt;p&gt;NVME - was from the junk pile of a friend who rescued it from an old office PC. 1TB, - for nvme slow as fuck, over 20.000 hours logged - but yeah it still works.&lt;/p&gt; &lt;p&gt;My verdict about the future of this rig and upgrades:&lt;/p&gt; &lt;p&gt;Here and now it's OK for the price. You get what you paid for. &lt;/p&gt; &lt;p&gt;- Can't use my VR headset (HP Reverb G2) due to the lack of USB-C. Not like windows would still support it, but i uninstalled windows update especially for that. So prolly gonna get a pcie USB-C controller for like 20 bucks from aliexpress or ebay. And my last pcie slot gone.&lt;/p&gt; &lt;p&gt;- Fans. Loads of fans. Prolly gonna get some cheap 4-pin fans to replace the ones in the case.&lt;/p&gt; &lt;p&gt;- Nvme. Yeah the Toshiba one still works. 1 TB is...meh. Something faster like a Samsung 980 pro would be nice. And a bit bigger. 2 TB would be nice.&lt;/p&gt; &lt;p&gt;- RAM. 64 GB would be nice. Even at 4800 MHz. Really.&lt;/p&gt; &lt;p&gt;What I would recommend: CPU, PSU, GPU, CPU Fan&lt;/p&gt; &lt;p&gt;What I would not recommend: The case. No USB-C. Stinks. The Board. Just one nvme stinks. Lack of slots stinks. The case. No USB-C stinks. It has a window and 4 fans. 2/5 stars. add one star if you can connect the 3pin fans to your board. DDR5 barely makes sense over 4800 with 12th gen. Read the manual. RAM - 6000 MHz sounds nice. But no xmp? Better make sure this runs as you expect or go straight to the 4800 trash bin.&lt;/p&gt; &lt;p&gt;Bonus thoughts: The board - as shitty as it is - has a PS2 controller. Yeah the 90s just called they want their ports back. But cool thing is that PS2 has N-Key rollover. In a nutshell - using old keyboards you can press more keys at once. For 99% of all users this is uninteresting. But if you really want PS2 on a modern board - here you get it on a budget.&lt;/p&gt; &lt;p&gt;Any thoughts? Experience with 3 and 4 pin fan woes? Calling me names?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent-Amount-277"&gt; /u/Excellent-Amount-277 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2htv9/critizize_and_suggest_optimizations_for_my_ai_rig/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2htv9/critizize_and_suggest_optimizations_for_my_ai_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2htv9/critizize_and_suggest_optimizations_for_my_ai_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T22:45:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1k238lw</id>
    <title>Does anyone else feel guilty using big models for tiny tasks?</title>
    <updated>2025-04-18T12:02:06+00:00</updated>
    <author>
      <name>/u/RightCup5772</name>
      <uri>https://old.reddit.com/user/RightCup5772</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't know if anyone else feels this way, but sometimes when I use a huge model for something super simple, I feel bad, like I'm wasting resources or something.&lt;/p&gt; &lt;p&gt;It feels like these LLMs are way too powerful for little tasks, and I shouldn't be wasting their &amp;quot;time&amp;quot; (even though I know it's not alive lol) or the computational resources.&lt;/p&gt; &lt;p&gt;Because of that, I set up Gemma 3 locally and now I use it for all my tiny tasks.&lt;/p&gt; &lt;p&gt;I can't fully explain why I feel like this â€” it's not really logical â€” but it's there.&lt;/p&gt; &lt;p&gt;Does anyone else feel the same way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RightCup5772"&gt; /u/RightCup5772 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k238lw/does_anyone_else_feel_guilty_using_big_models_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k238lw/does_anyone_else_feel_guilty_using_big_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k238lw/does_anyone_else_feel_guilty_using_big_models_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T12:02:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2fb67</id>
    <title>Save 13W of idle power on your 3090?</title>
    <updated>2025-04-18T20:51:07+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A comment on my other post (see: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k22e41/comment/mnr7mk5/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1k22e41/comment/mnr7mk5/&lt;/a&gt; ) led me to do some testing.&lt;/p&gt; &lt;p&gt;With my old drivers:&lt;/p&gt; &lt;p&gt;``` +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.144.03 Driver Version: 550.144.03 CUDA Version: 12.4 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 On | 00000000:00:10.0 Off | N/A | | 0% 39C P8 21W / 255W | 15967MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 3090 Ti On | 00000000:00:11.0 Off | Off | | 0% 35C P8 26W / 255W | 15977MiB / 24564MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;After updating OS/drivers/CUDA:&lt;/p&gt; &lt;p&gt;``` +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 570.124.06 Driver Version: 570.124.06 CUDA Version: 12.8 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 On | 00000000:00:10.0 Off | N/A | | 0% 32C P8 8W / 285W | 1MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 3090 Ti On | 00000000:00:11.0 Off | Off | | 0% 41C P8 15W / 285W | 1MiB / 24564MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Holy crap!&lt;/p&gt; &lt;p&gt;13W savings on 3090 and 11W saving on the 3090 Ti!&lt;/p&gt; &lt;p&gt;Now, I just need to check whether these are really 'at the wall' savings, or just 'nvidia-smi reporting differences'.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Old setup: Ubuntu 20.04, CUDA 12.4, 550 driver&lt;/li&gt; &lt;li&gt;New setup: Ubuntu 24.04, CUDA 12.8, 570 driver&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;EDIT: verified wall power:&lt;/p&gt; &lt;p&gt;I just rebooted to the old image to do powerwall test and found this at start-up:&lt;/p&gt; &lt;p&gt;``` +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.144.03 Driver Version: 550.144.03 CUDA Version: 12.4 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 On | 00000000:00:10.0 Off | N/A | | 0% 32C P8 8W / 255W | 2MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 3090 Ti On | 00000000:00:11.0 Off | Off | | 0% 34C P8 18W / 255W | 2MiB / 24564MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;So also same low idle power (before models are loaded).&lt;/p&gt; &lt;p&gt;And after models are loaded:&lt;/p&gt; &lt;p&gt;&lt;code&gt; +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.144.03 Driver Version: 550.144.03 CUDA Version: 12.4 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 On | 00000000:00:10.0 Off | N/A | | 54% 49C P8 22W / 255W | 15967MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 3090 Ti On | 00000000:00:11.0 Off | Off | | 0% 37C P8 25W / 255W | 15979MiB / 24564MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Aftermodels are unloaded, the idle power is not recovered:&lt;/p&gt; &lt;p&gt;&lt;code&gt; +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.144.03 Driver Version: 550.144.03 CUDA Version: 12.4 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 On | 00000000:00:10.0 Off | N/A | | 0% 43C P8 22W / 255W | 2MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 3090 Ti On | 00000000:00:11.0 Off | Off | | 0% 41C P8 26W / 255W | 2MiB / 24564MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ &lt;/code&gt; Wall power: 105W +/- 3W&lt;/p&gt; &lt;p&gt;New setup before model loads:&lt;/p&gt; &lt;p&gt;&lt;code&gt; +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 570.124.06 Driver Version: 570.124.06 CUDA Version: 12.8 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 On | 00000000:00:10.0 Off | N/A | | 53% 44C P8 8W / 355W | 1MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 3090 Ti On | 00000000:00:11.0 Off | Off | | 0% 41C P8 19W / 355W | 1MiB / 24564MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Wall power: 73W +/- 1W&lt;/p&gt; &lt;p&gt;Now tried loading a model:&lt;/p&gt; &lt;p&gt;&lt;code&gt; +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 570.124.06 Driver Version: 570.124.06 CUDA Version: 12.8 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 On | 00000000:00:10.0 Off | N/A | | 53% 45C P8 8W / 275W | 22759MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 3090 Ti On | 00000000:00:11.0 Off | Off | | 0% 37C P8 19W / 275W | 22769MiB / 24564MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Wall power: 75W +/- 2W&lt;/p&gt; &lt;p&gt;OK. It looks like these are real power savings!&lt;/p&gt; &lt;p&gt;I think more work needs to be done:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is the saving permanent or does it degrade after time&lt;/li&gt; &lt;li&gt;What causes the saving? The original comment said saving was triggered by an OS update - but it could be an interaction of different elements perhaps kernel + driver?&lt;/li&gt; &lt;li&gt;Does this also fix the P40 idle power issue? (which can currently be worked around with pstated)&lt;/li&gt; &lt;li&gt;Dare I dream that it could help with P100 idle power?&lt;/li&gt; &lt;li&gt;What about other cards e.g. 2080 Ti?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2fb67/save_13w_of_idle_power_on_your_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2fb67/save_13w_of_idle_power_on_your_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2fb67/save_13w_of_idle_power_on_your_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T20:51:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1k28cqz</id>
    <title>I tried fine-tuning Qwen2.5 to generate git commit messages</title>
    <updated>2025-04-18T15:56:26+00:00</updated>
    <author>
      <name>/u/m19990328</name>
      <uri>https://old.reddit.com/user/m19990328</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I recently tried fine-tuning Qwen2.5-Coder-3B-Instruct to generate better commit messages. The main goal is to let it understand the idea behind code changes instead of simply repeating them. Qwen2.5-Coder-3B-Instruct is a sweet model that is capable in coding tasks and lightweight to run. Then, I fine tune it on the dataset &lt;a href="https://huggingface.co/datasets/Maxscha/commitbench"&gt;Maxscha/commitbench&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I think the results are honestly not bad. If the code changes focus on a main goal, the model can guess it pretty well. I released it as a python package and it is available now. You may check the fine tune script to see the training details as well. Hope you find them useful.&lt;/p&gt; &lt;p&gt;You can use it by first installing &lt;code&gt;pip install git-gen-utils&lt;/code&gt; and running &lt;code&gt;git-gen&lt;/code&gt;&lt;/p&gt; &lt;p&gt;ðŸ”—Source: &lt;a href="https://github.com/CyrusCKF/git-gen"&gt;https://github.com/CyrusCKF/git-gen&lt;/a&gt;&lt;br /&gt; ðŸ¤–Script: &lt;a href="https://github.com/CyrusCKF/git-gen/blob/main/finetune/finetune.ipynb"&gt;https://github.com/CyrusCKF/git-gen/blob/main/finetune/finetune.ipynb&lt;/a&gt;&lt;br /&gt; ðŸ¤—Model (on HuggingFace): &lt;a href="https://huggingface.co/CyrusCheungkf/git-commit-3B"&gt;https://huggingface.co/CyrusCheungkf/git-commit-3B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m19990328"&gt; /u/m19990328 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28cqz/i_tried_finetuning_qwen25_to_generate_git_commit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28cqz/i_tried_finetuning_qwen25_to_generate_git_commit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k28cqz/i_tried_finetuning_qwen25_to_generate_git_commit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T15:56:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1fi5w</id>
    <title>New society is taking shape</title>
    <updated>2025-04-17T15:24:23+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1fi5w/new_society_is_taking_shape/"&gt; &lt;img alt="New society is taking shape" src="https://preview.redd.it/05n7cxquxeve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3503ecb35d404084ae9522bb771aecd69177ee0c" title="New society is taking shape" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/05n7cxquxeve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1fi5w/new_society_is_taking_shape/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1fi5w/new_society_is_taking_shape/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T15:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1vvy3</id>
    <title>No API keys, no cloud. Just local Al + tools that actually work. Too much to ask?</title>
    <updated>2025-04-18T03:56:37+00:00</updated>
    <author>
      <name>/u/aruntemme</name>
      <uri>https://old.reddit.com/user/aruntemme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been about a month since we first posted Clara here.&lt;/p&gt; &lt;p&gt;Clara is a local-first AI assistant - think of it like ChatGPT, but fully private and running on your own machine using Ollama.&lt;/p&gt; &lt;p&gt;Since the initial release, I've had a small group of users try it out, and I've pushed several updates based on real usage and feedback.&lt;/p&gt; &lt;p&gt;The biggest update is that Clara now comes with n8n built-in.&lt;/p&gt; &lt;p&gt;That means you can now build and run your own tools directly inside the assistant - no setup needed, no external services. Just open Clara and start automating.&lt;/p&gt; &lt;p&gt;With the n8n integration, Clara can now do more than chat. You can use it to:&lt;/p&gt; &lt;p&gt;â€¢ Check your emails â€¢ Manage your calendar â€¢ Call APIs â€¢ Run scheduled tasks â€¢ Process webhooks â€¢ Connect to databases â€¢ And anything else you can wire up using n8n's visual flow builder&lt;/p&gt; &lt;p&gt;The assistant can trigger these workflows directly - so you can talk to Clara and ask it to do real tasks, using tools that run entirely on your&lt;/p&gt; &lt;p&gt;device.&lt;/p&gt; &lt;p&gt;Everything happens locally. No data goes out, no accounts, no cloud dependency.&lt;/p&gt; &lt;p&gt;If you're someone who wants full control of your AI and automation setup, this might be something worth trying.&lt;/p&gt; &lt;p&gt;You can check out the project here:&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/badboysm890/ClaraVerse"&gt;https://github.com/badboysm890/ClaraVerse&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks to everyone who's been trying it and sending feedback. Still improving things - more updates soon.&lt;/p&gt; &lt;p&gt;Note: I'm aware of great projects like OpenWebUI and LibreChat. Clara takes a slightly different approach - focusing on reducing dependencies, offering a native desktop app, and making the overall experience more user-friendly so that more people can easily get started with local AI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aruntemme"&gt; /u/aruntemme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1vvy3/no_api_keys_no_cloud_just_local_al_tools_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1vvy3/no_api_keys_no_cloud_just_local_al_tools_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1vvy3/no_api_keys_no_cloud_just_local_al_tools_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T03:56:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1k22e41</id>
    <title>Good news: 5090s now in stock in my local market. Bad news: cheapest is $3,550</title>
    <updated>2025-04-18T11:13:40+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now I wonder if I should have just bought the 2nd hand 3090s that were on sale for $700.&lt;/p&gt; &lt;p&gt;Can someone tell me what the typical 'street price' for 5090s in the US?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k22e41/good_news_5090s_now_in_stock_in_my_local_market/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k22e41/good_news_5090s_now_in_stock_in_my_local_market/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k22e41/good_news_5090s_now_in_stock_in_my_local_market/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T11:13:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1qpr6</id>
    <title>microsoft/MAI-DS-R1, DeepSeek R1 Post-Trained by Microsoft</title>
    <updated>2025-04-17T23:22:11+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1qpr6/microsoftmaidsr1_deepseek_r1_posttrained_by/"&gt; &lt;img alt="microsoft/MAI-DS-R1, DeepSeek R1 Post-Trained by Microsoft" src="https://external-preview.redd.it/oacNTVfe15Ozahhiv8YMZ-Teu__pPBVygtAgzE9FP3c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=140a805e8a294974bbef97d4e1035ef969130c5a" title="microsoft/MAI-DS-R1, DeepSeek R1 Post-Trained by Microsoft" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/MAI-DS-R1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1qpr6/microsoftmaidsr1_deepseek_r1_posttrained_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1qpr6/microsoftmaidsr1_deepseek_r1_posttrained_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T23:22:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1v9rq</id>
    <title>CSM 1B is real-time now and has fine-tuning</title>
    <updated>2025-04-18T03:21:15+00:00</updated>
    <author>
      <name>/u/SovietWarBear17</name>
      <uri>https://old.reddit.com/user/SovietWarBear17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/davidbrowne17/csm-streaming"&gt;https://github.com/davidbrowne17/csm-streaming&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Not sure if many of you have been following this model, but the open-source community has managed to reach real-time with streaming and figured out fine-tuning. This is my repo with fine-tuning and a real-time local chat demo, my version of fine-tuning is lora but there is also full fine tuning out there as well. Give it a try and let me know how it compares to other TTS models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SovietWarBear17"&gt; /u/SovietWarBear17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1v9rq/csm_1b_is_realtime_now_and_has_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1v9rq/csm_1b_is_realtime_now_and_has_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1v9rq/csm_1b_is_realtime_now_and_has_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T03:21:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2ex99</id>
    <title>Gemma3-4b-qat-int4 for OpenVINO is up</title>
    <updated>2025-04-18T20:34:13+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Echo9Zulu/gemma-3-4b-it-qat-int4_asym-ov"&gt;https://huggingface.co/Echo9Zulu/gemma-3-4b-it-qat-int4_asym-ov&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ex99/gemma34bqatint4_for_openvino_is_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ex99/gemma34bqatint4_for_openvino_is_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ex99/gemma34bqatint4_for_openvino_is_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T20:34:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1k22lyx</id>
    <title>FULL LEAKED Replit Agent System Prompts and Tools</title>
    <updated>2025-04-18T11:26:40+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Latest system prompt: 18/04/2025)&lt;/p&gt; &lt;p&gt;I managed to get full official Replit Agent system prompts, including its tools (JSON). Over 400 lines.&lt;/p&gt; &lt;p&gt;You can check it out at: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k22lyx/full_leaked_replit_agent_system_prompts_and_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k22lyx/full_leaked_replit_agent_system_prompts_and_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k22lyx/full_leaked_replit_agent_system_prompts_and_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T11:26:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1k28j02</id>
    <title>Llama 4 Maverick MLX performance on M3 Ultra</title>
    <updated>2025-04-18T16:03:30+00:00</updated>
    <author>
      <name>/u/nomorebuttsplz</name>
      <uri>https://old.reddit.com/user/nomorebuttsplz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LM studio released an MLX update today so we can run Maverick in MLX format.&lt;/p&gt; &lt;p&gt;Q4 version numbers:&lt;/p&gt; &lt;p&gt;Prompt size: 12405&lt;br /&gt; Prompt eval rate: 332 t/s&lt;br /&gt; Token gen rate: 47.42&lt;/p&gt; &lt;p&gt;Right now for me there is a bug where it's not using prompt caching. Promising initial results though.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomorebuttsplz"&gt; /u/nomorebuttsplz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28j02/llama_4_maverick_mlx_performance_on_m3_ultra/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28j02/llama_4_maverick_mlx_performance_on_m3_ultra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k28j02/llama_4_maverick_mlx_performance_on_m3_ultra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T16:03:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2b75l</id>
    <title>Anyone having voice conversations? Whatâ€™s your setup?</title>
    <updated>2025-04-18T17:55:39+00:00</updated>
    <author>
      <name>/u/markosolo</name>
      <uri>https://old.reddit.com/user/markosolo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apologies to anyone whoâ€™s already seen this posted - I thought this might be a better place to ask.&lt;/p&gt; &lt;p&gt;I want something similar to Googles AI Studio where I can call a model and chat with it. Ideally I'd like that to look something like voice conversation where I can brainstorm and do planning sessions with my &amp;quot;AI&amp;quot;. &lt;/p&gt; &lt;p&gt;Is anyone doing anything like this? What's your setup? Would love to hear from anyone having regular voice conversations with AI as part of their daily workflow.&lt;/p&gt; &lt;p&gt;In terms of resources I have plenty of compute, 20GB of GPU I can use. I prefer local if thereâ€™s are viable local options I can cobble together even if itâ€™s a bit of work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/markosolo"&gt; /u/markosolo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2b75l/anyone_having_voice_conversations_whats_your_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2b75l/anyone_having_voice_conversations_whats_your_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2b75l/anyone_having_voice_conversations_whats_your_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T17:55:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2ahak</id>
    <title>Built a Chrome extension to organize chats on DeepSeek</title>
    <updated>2025-04-18T17:25:12+00:00</updated>
    <author>
      <name>/u/cedparadis</name>
      <uri>https://old.reddit.com/user/cedparadis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ahak/built_a_chrome_extension_to_organize_chats_on/"&gt; &lt;img alt="Built a Chrome extension to organize chats on DeepSeek" src="https://external-preview.redd.it/ZGkzaHZmajlvbXZlMTULimii-_X8OCS81GF8X6Vu1vt1qbza3aRbGy-kyjt6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6bff768996433c8136b0e96997b920b2ed78b6ed" title="Built a Chrome extension to organize chats on DeepSeek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve been using DeepSeek a lot recently as a faster, free alternative to ChatGPT.&lt;/p&gt; &lt;p&gt;After a while your chat history gets messy and pretty long.&lt;/p&gt; &lt;p&gt;So I tried a couple of Chrome extensions to have folders or pin my important conversations but either they were broken or felt out of place with the DeepSeek UI.&lt;/p&gt; &lt;p&gt;I kind of scratch my own itch by building my own. I made it super integrated in the UI so it feels its part of the native Deepseek interface.&lt;/p&gt; &lt;p&gt;It's pretty simple: you can have folders and subfolders for your convos, pin chats as favorite and even resize the sidebar.&lt;/p&gt; &lt;p&gt;Just pushed it live on the Chrome Store: &lt;a href="https://chromewebstore.google.com/detail/deepseek-folders-chat-org/mlfbmcmkefmdhnnkecdoegomcikmbaac"&gt;https://chromewebstore.google.com/detail/deepseek-folders-chat-org/mlfbmcmkefmdhnnkecdoegomcikmbaac&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now I am working on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clipping specific parts of chats&lt;/li&gt; &lt;li&gt;Secret section with PIN access&lt;/li&gt; &lt;li&gt;&lt;p&gt;Prompt Genie - one click prompt enhancement&lt;/p&gt; &lt;p&gt;Happy to hear feedback or questions â€” first real project Iâ€™ve built and shipped solo.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cedparadis"&gt; /u/cedparadis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lbx60gj9omve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ahak/built_a_chrome_extension_to_organize_chats_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ahak/built_a_chrome_extension_to_organize_chats_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T17:25:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2asly</id>
    <title>I wrote a memory system with GUI for Gemma3 using the Kobold.cpp API</title>
    <updated>2025-04-18T17:38:29+00:00</updated>
    <author>
      <name>/u/PSInvader</name>
      <uri>https://old.reddit.com/user/PSInvader</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PSInvader"&gt; /u/PSInvader &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Asagix/RecallWeaver"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2asly/i_wrote_a_memory_system_with_gui_for_gemma3_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2asly/i_wrote_a_memory_system_with_gui_for_gemma3_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T17:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1xvvr</id>
    <title>Where is the promised open Grok 2?</title>
    <updated>2025-04-18T06:01:19+00:00</updated>
    <author>
      <name>/u/AlexBefest</name>
      <uri>https://old.reddit.com/user/AlexBefest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As far as I know, Grok 2 was supposed to be open-sourced some time after Grok 3's release. But I'm afraid that by the time they decide to open-source Grok 2, it will already be completely obsolete. This is because even now, it significantly lags behind in performance compared to the likes of DeepSeek V3, and we also have Qwen 3 and Llama 4 Reasoning on the horizon (not to mention a potential open model from OpenAI). I believe that when they eventually decide to release it to the community, it will be of no use to anyone anymore, much like what happened with Grok 1. What are your thoughts on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexBefest"&gt; /u/AlexBefest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1xvvr/where_is_the_promised_open_grok_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1xvvr/where_is_the_promised_open_grok_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1xvvr/where_is_the_promised_open_grok_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T06:01:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1k250fu</id>
    <title>Gemma 3 QAT launch with MLX, llama.cpp, Ollama, LM Studio, and Hugging Face</title>
    <updated>2025-04-18T13:31:34+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Some weeks ago we released GGUFs corresponding to the QAT checkpoints of Gemma 3. Thanks to QAT, the model is able to preserve similar quality as &lt;code&gt;bfloat16&lt;/code&gt; while significantly reducing the memory requirements to load the model. That is, QAT is an additional fine-tuning that makes the model more rigorous to quantization.&lt;/p&gt; &lt;p&gt;As we only released the GGUFs, we got feedback that it would be great to have the unquantized QAT-based checkpoints to allow people to quantize for their own tools. So...we did it! Today we're releasing the unquantized QAT-based checkpoints. The models preserve quality better than naive quantization. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;We also collaborated with Prince (from MLX), llama.cpp, Ollama, LM Studio, and Hugging Face to make sure you can use the models in all your favorite tools!&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blog post : &lt;a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/"&gt;https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Unquantized checkpoints: &lt;a href="https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b"&gt;https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ollama: &lt;a href="https://ollama.com/library/gemma3"&gt;https://ollama.com/library/gemma3&lt;/a&gt; (try ollama run gemma3:12b-it-qat)&lt;/li&gt; &lt;li&gt;LM Studio: &lt;a href="https://lmstudio.ai/model/gemma-3-12b-it-qat"&gt;https://lmstudio.ai/model/gemma-3-12b-it-qat&lt;/a&gt; &lt;/li&gt; &lt;li&gt;MLX: &lt;a href="https://huggingface.co/collections/mlx-community/gemma-3-qat-68002674cd5afc6f9022a0ae"&gt;https://huggingface.co/collections/mlx-community/gemma-3-qat-68002674cd5afc6f9022a0ae&lt;/a&gt;&lt;/li&gt; &lt;li&gt;llama.cpp: &lt;a href="https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b"&gt;https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Enjoy! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250fu/gemma_3_qat_launch_with_mlx_llamacpp_ollama_lm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250fu/gemma_3_qat_launch_with_mlx_llamacpp_ollama_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k250fu/gemma_3_qat_launch_with_mlx_llamacpp_ollama_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T13:31:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k29oe2</id>
    <title>QAT is slowly becoming mainstream now?</title>
    <updated>2025-04-18T16:52:07+00:00</updated>
    <author>
      <name>/u/__amberluz__</name>
      <uri>https://old.reddit.com/user/__amberluz__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google just released a QAT optimized Gemma 3 - 27 billion parameter model. The quantization aware training claims to recover close to 97% of the accuracy loss that happens during the quantization. Do you think this is slowly becoming the norm? Will non-quantized safetensors slowly become obsolete?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__amberluz__"&gt; /u/__amberluz__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k29oe2/qat_is_slowly_becoming_mainstream_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k29oe2/qat_is_slowly_becoming_mainstream_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k29oe2/qat_is_slowly_becoming_mainstream_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T16:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1k27fz2</id>
    <title>I created an interactive tool to visualize *every* attention weight matrix within GPT-2!</title>
    <updated>2025-04-18T15:18:17+00:00</updated>
    <author>
      <name>/u/tycho_brahes_nose_</name>
      <uri>https://old.reddit.com/user/tycho_brahes_nose_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k27fz2/i_created_an_interactive_tool_to_visualize_every/"&gt; &lt;img alt="I created an interactive tool to visualize *every* attention weight matrix within GPT-2!" src="https://external-preview.redd.it/YW45M2FibXYwbXZlMWaepLM_4Oin4KjR_zAxiUwp5NOaLzCHkxa3urw0ZqL6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a60742f26e3a407482898d8e82f2a5d6e8f6ee5f" title="I created an interactive tool to visualize *every* attention weight matrix within GPT-2!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tycho_brahes_nose_"&gt; /u/tycho_brahes_nose_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dgo9qamv0mve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k27fz2/i_created_an_interactive_tool_to_visualize_every/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k27fz2/i_created_an_interactive_tool_to_visualize_every/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T15:18:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2chcw</id>
    <title>Gemma 27B QAT works surprisingly well at Q2_K</title>
    <updated>2025-04-18T18:49:11+00:00</updated>
    <author>
      <name>/u/MaruluVR</name>
      <uri>https://old.reddit.com/user/MaruluVR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to test how well QAT models do at a lower quant size so I grabbed the smallest quant currently out for it, Q2_K at 10.5 GB. &lt;a href="https://huggingface.co/bartowski/google_gemma-3-27b-it-qat-GGUF"&gt;https://huggingface.co/bartowski/google_gemma-3-27b-it-qat-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I use my models mostly for my Japanese indie game, so following instructions, custom formatting and if it can roleplay or not is what I look for in models. My tests were all done in Japanese, which many models already have issues with at Q4 so I mostly use Q5. In my testing there were no grammatical errors, no random English or Chinese characters. It was able to roleplay in a custom format where I split the spoken words, the actions and the thoughts of the character into different brackets like ()&amp;lt;&amp;gt;ã€Œã€without any issues. I also asked it basic questions about celebrities, and historical events, it got names and basic information right but dates were all wrong. My tests were done in Ollama with the standard Gemma3 settings.&lt;/p&gt; &lt;p&gt;Overall I am really impressed by the performance of the model especially for being a 27B at Q2. In theory running a 70B model at Q2 would fit into a single 24GB GPU so this technology is very interesting and could allow us to fit even larger models into our cards. After testing it I am really excited for more QAT models to come out in the future.&lt;/p&gt; &lt;p&gt;Have you guys tried running them at smaller quants?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaruluVR"&gt; /u/MaruluVR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2chcw/gemma_27b_qat_works_surprisingly_well_at_q2_k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2chcw/gemma_27b_qat_works_surprisingly_well_at_q2_k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2chcw/gemma_27b_qat_works_surprisingly_well_at_q2_k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T18:49:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1k250r6</id>
    <title>New QAT-optimized int4 Gemma 3 models by Google, slash VRAM needs (54GB -&gt; 14.1GB) while maintaining quality.</title>
    <updated>2025-04-18T13:32:01+00:00</updated>
    <author>
      <name>/u/Sea_Sympathy_495</name>
      <uri>https://old.reddit.com/user/Sea_Sympathy_495</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250r6/new_qatoptimized_int4_gemma_3_models_by_google/"&gt; &lt;img alt="New QAT-optimized int4 Gemma 3 models by Google, slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality." src="https://external-preview.redd.it/5lq32BTIzHqmPYcHvNrCp8JMhag9gsSSkR3cQgoYZBU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed6a861b423ef5ef481e863b5c6947b3cef14c0c" title="New QAT-optimized int4 Gemma 3 models by Google, slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea_Sympathy_495"&gt; /u/Sea_Sympathy_495 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/?linkId=14034718"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250r6/new_qatoptimized_int4_gemma_3_models_by_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k250r6/new_qatoptimized_int4_gemma_3_models_by_google/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T13:32:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1k28ulo</id>
    <title>Time to step up the /local reasoning game</title>
    <updated>2025-04-18T16:17:11+00:00</updated>
    <author>
      <name>/u/vornamemitd</name>
      <uri>https://old.reddit.com/user/vornamemitd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28ulo/time_to_step_up_the_local_reasoning_game/"&gt; &lt;img alt="Time to step up the /local reasoning game" src="https://preview.redd.it/wtibm8c3cmve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93f80a0bad3e3f79619d29663e49d519eaa7898d" title="Time to step up the /local reasoning game" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Latest OAI models tucked away behind intrusive &amp;quot;ID verification&amp;quot;....&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vornamemitd"&gt; /u/vornamemitd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wtibm8c3cmve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28ulo/time_to_step_up_the_local_reasoning_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k28ulo/time_to_step_up_the_local_reasoning_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T16:17:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1k25876</id>
    <title>Google QAT - optimized int4 Gemma 3 slash VRAM needs (54GB -&gt; 14.1GB) while maintaining quality - llama.cpp, lmstudio, MLX, ollama</title>
    <updated>2025-04-18T13:41:47+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k25876/google_qat_optimized_int4_gemma_3_slash_vram/"&gt; &lt;img alt="Google QAT - optimized int4 Gemma 3 slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality - llama.cpp, lmstudio, MLX, ollama" src="https://preview.redd.it/23ut7jd3klve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f940165ab5ba660103d9f5f61872b1dc70698cbb" title="Google QAT - optimized int4 Gemma 3 slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality - llama.cpp, lmstudio, MLX, ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/23ut7jd3klve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k25876/google_qat_optimized_int4_gemma_3_slash_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k25876/google_qat_optimized_int4_gemma_3_slash_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T13:41:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k28f3f</id>
    <title>Playing DOOM II and 19 other DOS/GB games with LLMs as a new benchmark</title>
    <updated>2025-04-18T15:59:16+00:00</updated>
    <author>
      <name>/u/ZhalexDev</name>
      <uri>https://old.reddit.com/user/ZhalexDev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28f3f/playing_doom_ii_and_19_other_dosgb_games_with/"&gt; &lt;img alt="Playing DOOM II and 19 other DOS/GB games with LLMs as a new benchmark" src="https://external-preview.redd.it/d3J6N2xwMm84bXZlMeIZf5sR-oXFPwhpDTHMtN-Je-w0GMxJeu96UcIYpm6F.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74e3a1f897d051cfccf4d8820a610d3c5dbe54b1" title="Playing DOOM II and 19 other DOS/GB games with LLMs as a new benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From AK (@akhaliq)&lt;/p&gt; &lt;p&gt;&amp;quot;We introduce a research preview of VideoGameBench, a benchmark which challenges vision-language models to complete, in real-time, a suite of 20 different popular video games from both hand-held consoles and PC &lt;/p&gt; &lt;p&gt;GPT-4o, Claude Sonnet 3.7, Gemini 2.5 Pro, and Gemini 2.0 Flash playing Doom II (default difficulty) on VideoGameBench-Lite with the same input prompt! Models achieve varying levels of success but none are able to pass even the first level.&amp;quot;&lt;/p&gt; &lt;p&gt;project page: &lt;a href="https://vgbench.com"&gt;https://vgbench.com&lt;/a&gt; &lt;/p&gt; &lt;p&gt;try on other games: &lt;a href="https://github.com/alexzhang13/VideoGameBench"&gt;https://github.com/alexzhang13/VideoGameBench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZhalexDev"&gt; /u/ZhalexDev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/u1i2op2o8mve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28f3f/playing_doom_ii_and_19_other_dosgb_games_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k28f3f/playing_doom_ii_and_19_other_dosgb_games_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T15:59:16+00:00</published>
  </entry>
</feed>
