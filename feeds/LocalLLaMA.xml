<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-20T20:35:06+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iu738d</id>
    <title>Homeserver</title>
    <updated>2025-02-20T19:31:53+00:00</updated>
    <author>
      <name>/u/techmago</name>
      <uri>https://old.reddit.com/user/techmago</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu738d/homeserver/"&gt; &lt;img alt="Homeserver" src="https://b.thumbs.redditmedia.com/yVYwSLkmUbkx3v5JdpvwEJrw7P9gCxVncs_eWedehWg.jpg" title="Homeserver" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My turn!&lt;br /&gt; We work with what we have avaliable.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6dqmhl2gicke1.jpg?width=1599&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=980f1cde1cc64228f026ddd9844c4c4ff73dcd3e"&gt;https://preview.redd.it/6dqmhl2gicke1.jpg?width=1599&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=980f1cde1cc64228f026ddd9844c4c4ff73dcd3e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;2x24 GB on quadro p6000.&lt;br /&gt; I can run 70B models, with ollama and 8k context size 100% from the GPU.&lt;/p&gt; &lt;p&gt;A little underwhelming... improved my generation from ~2 token/sec to ~5.2 token sec.&lt;/p&gt; &lt;p&gt;And i dont think the SLI bridge is working XD&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techmago"&gt; /u/techmago &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu738d/homeserver/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu738d/homeserver/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu738d/homeserver/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T19:31:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1itslms</id>
    <title>Magma: A Foundation Model for Multimodal AI Agents</title>
    <updated>2025-02-20T07:03:11+00:00</updated>
    <author>
      <name>/u/pkmxtw</name>
      <uri>https://old.reddit.com/user/pkmxtw</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pkmxtw"&gt; /u/pkmxtw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://microsoft.github.io/Magma/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itslms/magma_a_foundation_model_for_multimodal_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itslms/magma_a_foundation_model_for_multimodal_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T07:03:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu8f7s</id>
    <title>Speculative decoding can identify broken quants?</title>
    <updated>2025-02-20T20:26:14+00:00</updated>
    <author>
      <name>/u/NickNau</name>
      <uri>https://old.reddit.com/user/NickNau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu8f7s/speculative_decoding_can_identify_broken_quants/"&gt; &lt;img alt="Speculative decoding can identify broken quants?" src="https://a.thumbs.redditmedia.com/XqCzop9kq738DVsg8GJQXPOa46DC9kly1ZUdEC5f5l4.jpg" title="Speculative decoding can identify broken quants?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NickNau"&gt; /u/NickNau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iu8f7s"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu8f7s/speculative_decoding_can_identify_broken_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu8f7s/speculative_decoding_can_identify_broken_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T20:26:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iteaew</id>
    <title>Google releases PaliGemma 2 mix - a VLM for many tasks</title>
    <updated>2025-02-19T19:32:14+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! Gemma tech lead over here :)&lt;/p&gt; &lt;p&gt;Today, we released a new model, PaliGemma 2 mix! It's the same architecture as PaliGemma 2, but these are some checkpoints that work well for a bunch of tasks without having to fine-tune it.&lt;/p&gt; &lt;p&gt;Some links first&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Official Google blog &lt;a href="https://developers.googleblog.com/en/introducing-paligemma-2-mix/?linkId=13028688"&gt;https://developers.googleblog.com/en/introducing-paligemma-2-mix/?linkId=13028688&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The Hugging Face blog &lt;a href="https://huggingface.co/blog/paligemma2mix"&gt;https://huggingface.co/blog/paligemma2mix&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Open models in &lt;a href="https://huggingface.co/collections/google/paligemma-2-mix-67ac6a251aaf3ee73679dcc4"&gt;https://huggingface.co/collections/google/paligemma-2-mix-67ac6a251aaf3ee73679dcc4&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Free demo to try out &lt;a href="https://huggingface.co/spaces/google/paligemma2-10b-mix"&gt;https://huggingface.co/spaces/google/paligemma2-10b-mix&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So what can this model do?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Image captioning (both short and long captions)&lt;/li&gt; &lt;li&gt;OCR&lt;/li&gt; &lt;li&gt;Question answering&lt;/li&gt; &lt;li&gt;Object detection&lt;/li&gt; &lt;li&gt;Image segmentation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So you can use the model for localization, image understanding, document understanding, and more! And as always, if you want even better results for your task, you can pick the base models and fine-tune them. The goal of this release was to showcase what can be done with PG2, which is a very good model for fine-tuning.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iteaew/google_releases_paligemma_2_mix_a_vlm_for_many/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iteaew/google_releases_paligemma_2_mix_a_vlm_for_many/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iteaew/google_releases_paligemma_2_mix_a_vlm_for_many/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T19:32:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1itc3h7</id>
    <title>Training LLM on 1000s of GPUs made simple</title>
    <updated>2025-02-19T18:06:09+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itc3h7/training_llm_on_1000s_of_gpus_made_simple/"&gt; &lt;img alt="Training LLM on 1000s of GPUs made simple" src="https://preview.redd.it/2wk7ntxpy4ke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04be9bf215a025ca522b0d41193331c0824a527c" title="Training LLM on 1000s of GPUs made simple" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2wk7ntxpy4ke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itc3h7/training_llm_on_1000s_of_gpus_made_simple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itc3h7/training_llm_on_1000s_of_gpus_made_simple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T18:06:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1itsnun</id>
    <title>Explanation &amp; Results of NSA - DeepSeek Introduces Ultra-Fast Long-Context Model Training and Inference</title>
    <updated>2025-02-20T07:07:21+00:00</updated>
    <author>
      <name>/u/YiPherng</name>
      <uri>https://old.reddit.com/user/YiPherng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itsnun/explanation_results_of_nsa_deepseek_introduces/"&gt; &lt;img alt="Explanation &amp;amp; Results of NSA - DeepSeek Introduces Ultra-Fast Long-Context Model Training and Inference" src="https://external-preview.redd.it/cW8EKiWvzQbht7_TCSgxsKZtbhoVL_boLT3a2KLDi7c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0997935da6291fe427ee6657d27d2cb0957b424" title="Explanation &amp;amp; Results of NSA - DeepSeek Introduces Ultra-Fast Long-Context Model Training and Inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YiPherng"&gt; /u/YiPherng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://shockbs.pro/blog/deepseek-introduces-nsa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itsnun/explanation_results_of_nsa_deepseek_introduces/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itsnun/explanation_results_of_nsa_deepseek_introduces/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T07:07:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu7jnw</id>
    <title>Were successful hobbyist finetunes just a part of the Llama2 era?</title>
    <updated>2025-02-20T19:50:42+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A year ago when Llama2 was the star of the show, it seems like the best models for all purposes were community fine-tunes. Wizard was a way better general-purpose model than Llama2, there were writing models of all different flavors, hermes was a big power boost, dolphin made instruct better, etc.. etc.. I could go on. There were fine tunes from smaller groups of people that kicked ass and became community favorites.&lt;/p&gt; &lt;p&gt;You don't see those nowadays though. Is Llama3 just better? Has increased context size taken the fun out of fine-tuning? Are modern foundational models just &lt;em&gt;harder&lt;/em&gt; to fine-tune?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7jnw/were_successful_hobbyist_finetunes_just_a_part_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7jnw/were_successful_hobbyist_finetunes_just_a_part_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7jnw/were_successful_hobbyist_finetunes_just_a_part_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T19:50:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu01d8</id>
    <title>What‚Äôs recent open source LLMs have the largest context windows?</title>
    <updated>2025-02-20T14:42:43+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open WebUI 0.5.15 just added a new RAG feature called ‚ÄúFull Context Mode for Local Document Search (RAG). It says it ‚Äúinjects entire document content into context, improving accuracy for models with large context windows -ideal for deep context understanding‚Äù. Obviously I want to try this out and use a model with a larger context window. My limitations are 48 GB VRAM and 64 GB system memory. What are my best options given these limitations. I‚Äôm seeing most models are limited to 128k. What can I run beyond 128k at Q4 and still have enough VRAM for large context without absolutely killing my tokens per second? I just need like 2-3 t/s. I‚Äôm pretty patient. P.S. I know this question has been asked before, however, most of the results were from like 8 months ago. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu01d8/whats_recent_open_source_llms_have_the_largest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu01d8/whats_recent_open_source_llms_have_the_largest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu01d8/whats_recent_open_source_llms_have_the_largest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T14:42:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu4hid</id>
    <title>CloseAI's DeepResearch is insanely good... do we have open source replacements?</title>
    <updated>2025-02-20T17:47:37+00:00</updated>
    <author>
      <name>/u/TimAndTimi</name>
      <uri>https://old.reddit.com/user/TimAndTimi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IDK if such thing exists outside openai. If so, please let me know.&lt;/p&gt; &lt;p&gt;I am actually feeling okay with the crazy subscription fee for now because of deep research is actually very useful in terms of reading a ton of online resources in depth. (vastly superior than 4o's ordinary online search).&lt;/p&gt; &lt;p&gt;Still, it would be nice to run it with open sourced weights.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TimAndTimi"&gt; /u/TimAndTimi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4hid/closeais_deepresearch_is_insanely_good_do_we_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4hid/closeais_deepresearch_is_insanely_good_do_we_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4hid/closeais_deepresearch_is_insanely_good_do_we_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T17:47:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1itt2np</id>
    <title>The AI CUDA Engineer</title>
    <updated>2025-02-20T07:35:22+00:00</updated>
    <author>
      <name>/u/NunyaBuzor</name>
      <uri>https://old.reddit.com/user/NunyaBuzor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itt2np/the_ai_cuda_engineer/"&gt; &lt;img alt="The AI CUDA Engineer" src="https://external-preview.redd.it/aGM3dDZnNDR6OGtlMSXELHvaEW2TWH4Y-jofml_DDu990hjShx1lPnkzI0Tg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=802747a6a727b96cb8564ca2740aef0075558eee" title="The AI CUDA Engineer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NunyaBuzor"&gt; /u/NunyaBuzor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/u8ipxi34z8ke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itt2np/the_ai_cuda_engineer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itt2np/the_ai_cuda_engineer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T07:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1itxm6l</id>
    <title>Linux Lazy Unmap Flush "LUF" Reducing TLB Shootdowns By 97%, Faster AI LLM Performance</title>
    <updated>2025-02-20T12:43:29+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itxm6l/linux_lazy_unmap_flush_luf_reducing_tlb/"&gt; &lt;img alt="Linux Lazy Unmap Flush &amp;quot;LUF&amp;quot; Reducing TLB Shootdowns By 97%, Faster AI LLM Performance" src="https://external-preview.redd.it/XFz-Ged5fhiZf00xJLPX8r3w0bcDxstbdfJqpWstxas.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6aae0dfa26ec9efd8319878b27776e8805f5eab" title="Linux Lazy Unmap Flush &amp;quot;LUF&amp;quot; Reducing TLB Shootdowns By 97%, Faster AI LLM Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/Linux-Lazy-Unmap-Flush"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itxm6l/linux_lazy_unmap_flush_luf_reducing_tlb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itxm6l/linux_lazy_unmap_flush_luf_reducing_tlb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T12:43:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1itvu5m</id>
    <title>R1 is insanely good, but falls short of o1 in generalization</title>
    <updated>2025-02-20T10:53:08+00:00</updated>
    <author>
      <name>/u/EmptyTuple</name>
      <uri>https://old.reddit.com/user/EmptyTuple</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itvu5m/r1_is_insanely_good_but_falls_short_of_o1_in/"&gt; &lt;img alt="R1 is insanely good, but falls short of o1 in generalization" src="https://b.thumbs.redditmedia.com/6dj1VvGs8U5eMA2APXR7Pu0Q2eB3-XWpiEXhDcu6Msk.jpg" title="R1 is insanely good, but falls short of o1 in generalization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmptyTuple"&gt; /u/EmptyTuple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1itvu5m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itvu5m/r1_is_insanely_good_but_falls_short_of_o1_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itvu5m/r1_is_insanely_good_but_falls_short_of_o1_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T10:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1itr9th</id>
    <title>New AI Model | Ozone AI</title>
    <updated>2025-02-20T05:37:54+00:00</updated>
    <author>
      <name>/u/Perfect-Bowl-1601</name>
      <uri>https://old.reddit.com/user/Perfect-Bowl-1601</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;We're excited to announce the release of our latest model: **Reverb-7b!** The Ozone AI team has been hard at work, and we believe this model represents a significant step forward in 7B performance. This model was trained on over 200 million tokens of distilled data from Claude 3.5 Sonnet and GPT-4o. This model is a fine-tune of Qwen 2.5 7b.&lt;/p&gt; &lt;p&gt;Based on our benchmarks, Reverb-7b is showing impressive results, particularly on MMLU Pro. We're seeing performance that appears to surpass other 7B models on the Open LLM Leaderboard, specifically with the challenging MMLU Pro dataset (see: &lt;a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard"&gt;https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;Our MMLU Pro results:&lt;/p&gt; &lt;p&gt;Biology: 0.6904 Business: 0.3143 Chemistry: 0.2314 Computer Science: 0.4000 Economics: 0.5758 Engineering: 0.3148 Health: 0.5183 History: 0.4934 Law: 0.3315 Math: 0.2983 Other: 0.4372 Philosophy: 0.4409 Physics: 0.2910 Psychology: 0.5990&lt;/p&gt; &lt;p&gt;Average Accuracy (across all MMLU Pro subjects): 0.4006&lt;/p&gt; &lt;p&gt;(More benchmarks are coming soon!)&lt;/p&gt; &lt;p&gt;Model Card &amp;amp; Download: &lt;a href="https://huggingface.co/ozone-ai/Reverb-7b"&gt;https://huggingface.co/ozone-ai/Reverb-7b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is only our third model release, and we're committed to pushing the boundaries of open-source LLMs. We have a 14B and 2B models currently in the works, so stay tuned for those releases in the coming days!&lt;/p&gt; &lt;p&gt;EDIT: Started training 14b version.&lt;/p&gt; &lt;p&gt;We're eager to hear your feedback! Download Reverb, give it a try, and let us know what you think.&lt;/p&gt; &lt;p&gt;Thanks for your support and we're excited to see what you do with Reverb-7b!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect-Bowl-1601"&gt; /u/Perfect-Bowl-1601 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itr9th/new_ai_model_ozone_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itr9th/new_ai_model_ozone_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itr9th/new_ai_model_ozone_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T05:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu7e0n</id>
    <title>arcee-ai/Arcee-Maestro-7B-Preview, DeepSeek-R1-Distill-Qwen-7B with further GPRO training</title>
    <updated>2025-02-20T19:44:09+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7e0n/arceeaiarceemaestro7bpreview/"&gt; &lt;img alt="arcee-ai/Arcee-Maestro-7B-Preview, DeepSeek-R1-Distill-Qwen-7B with further GPRO training" src="https://external-preview.redd.it/kXmp-4xU9RU4uIBtaGUmIjjXZHg6PBZeRqkSIrz5cEE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=989184e6cf8fabf2f16b5eb65f889cafbaec4454" title="arcee-ai/Arcee-Maestro-7B-Preview, DeepSeek-R1-Distill-Qwen-7B with further GPRO training" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/arcee-ai/Arcee-Maestro-7B-Preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7e0n/arceeaiarceemaestro7bpreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7e0n/arceeaiarceemaestro7bpreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T19:44:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu7c24</id>
    <title>arcee-ai/Arcee-Blitz, Mistral-Small-24B-Instruct-2501 Finetune</title>
    <updated>2025-02-20T19:41:51+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7c24/arceeaiarceeblitz_mistralsmall24binstruct2501/"&gt; &lt;img alt="arcee-ai/Arcee-Blitz, Mistral-Small-24B-Instruct-2501 Finetune" src="https://external-preview.redd.it/nAJEyVNIP8SWOhtVuMgvqEBinfu5P4u1oNYU06SZdto.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e08074fa2ad405c1ee359322e0fc01d2fb5148f8" title="arcee-ai/Arcee-Blitz, Mistral-Small-24B-Instruct-2501 Finetune" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/arcee-ai/Arcee-Blitz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7c24/arceeaiarceeblitz_mistralsmall24binstruct2501/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7c24/arceeaiarceeblitz_mistralsmall24binstruct2501/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T19:41:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu4gvf</id>
    <title>I changed my mind about DeepSeek-R1-Distill-Llama-70B</title>
    <updated>2025-02-20T17:46:52+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4gvf/i_changed_my_mind_about_deepseekr1distillllama70b/"&gt; &lt;img alt="I changed my mind about DeepSeek-R1-Distill-Llama-70B" src="https://preview.redd.it/zknh3vk6xbke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7defd62a3749692bb67bb3c597046e8dd3da633" title="I changed my mind about DeepSeek-R1-Distill-Llama-70B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zknh3vk6xbke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4gvf/i_changed_my_mind_about_deepseekr1distillllama70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4gvf/i_changed_my_mind_about_deepseekr1distillllama70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T17:46:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1itytpy</id>
    <title>Reasoning model based on Qwen2.5-Max will soon be released</title>
    <updated>2025-02-20T13:46:26+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I guess new &amp;amp; larger QwQ models are also coming soon?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;On February 20th, during Alibaba's earnings call, Alibaba Group CEO Wu Yongming stated that looking ahead, Alibaba will continue to focus on three main business types: domestic and international e-commerce, AI + cloud computing technology, and internet platform products. Over the next three years, Alibaba will increase investment in three areas around the strategic core of AI: AI infrastructure, basic model platforms and AI native applications, and the AI transformation of existing businesses.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;At the same time, Wu Yongming revealed that Alibaba will also release a deep reasoning model based on Qwen2.5-Max in the near future.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itytpy/reasoning_model_based_on_qwen25max_will_soon_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itytpy/reasoning_model_based_on_qwen25max_will_soon_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itytpy/reasoning_model_based_on_qwen25max_will_soon_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T13:46:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1itv9ia</id>
    <title>Agent using Canva. Things are getting wild now...</title>
    <updated>2025-02-20T10:13:03+00:00</updated>
    <author>
      <name>/u/ljhskyso</name>
      <uri>https://old.reddit.com/user/ljhskyso</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itv9ia/agent_using_canva_things_are_getting_wild_now/"&gt; &lt;img alt="Agent using Canva. Things are getting wild now..." src="https://external-preview.redd.it/NTlhcjg4czRyOWtlMY88yKM0XPFK9vDNwHuU8bb82IoeEzVPUXcqILOpddQA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d10dc91d97d69b43afa2b66abd3152b04b563263" title="Agent using Canva. Things are getting wild now..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ljhskyso"&gt; /u/ljhskyso &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hjbttwq4r9ke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itv9ia/agent_using_canva_things_are_getting_wild_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itv9ia/agent_using_canva_things_are_getting_wild_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T10:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu6egg</id>
    <title>Even AI has some personality :)</title>
    <updated>2025-02-20T19:04:20+00:00</updated>
    <author>
      <name>/u/_idkwhattowritehere_</name>
      <uri>https://old.reddit.com/user/_idkwhattowritehere_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu6egg/even_ai_has_some_personality/"&gt; &lt;img alt="Even AI has some personality :)" src="https://preview.redd.it/3dlpqfoydcke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfb9019fc88f0d5f525fe174f20b501b99bf0c66" title="Even AI has some personality :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_idkwhattowritehere_"&gt; /u/_idkwhattowritehere_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3dlpqfoydcke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu6egg/even_ai_has_some_personality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu6egg/even_ai_has_some_personality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T19:04:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ityftd</id>
    <title>Samsung is working on its own on-device LLM.</title>
    <updated>2025-02-20T13:26:39+00:00</updated>
    <author>
      <name>/u/WordyBug</name>
      <uri>https://old.reddit.com/user/WordyBug</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ityftd/samsung_is_working_on_its_own_ondevice_llm/"&gt; &lt;img alt="Samsung is working on its own on-device LLM." src="https://preview.redd.it/cgbfpkphpake1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5bd286726a3a9cd81d352de72126809656fd7e96" title="Samsung is working on its own on-device LLM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WordyBug"&gt; /u/WordyBug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cgbfpkphpake1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ityftd/samsung_is_working_on_its_own_ondevice_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ityftd/samsung_is_working_on_its_own_ondevice_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T13:26:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1itq30t</id>
    <title>Qwen/Qwen2.5-VL-3B/7B/72B-Instruct are out!!</title>
    <updated>2025-02-20T04:28:52+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ"&gt;https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct-AWQ"&gt;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct-AWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct-AWQ"&gt;https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct-AWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The key enhancements of Qwen2.5-VL are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Visual Understanding: Improved ability to recognize and analyze objects, text, charts, and layouts within images.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Agentic Capabilities: Acts as a visual agent capable of reasoning and dynamically interacting with tools (e.g., using a computer or phone).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Long Video Comprehension: Can understand videos longer than 1 hour and pinpoint relevant segments for event detection.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Visual Localization: Accurately identifies and localizes objects in images with bounding boxes or points, providing stable JSON outputs.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Structured Output Generation: Can generate structured outputs for complex data like invoices, forms, and tables, useful in domains like finance and commerce.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itq30t/qwenqwen25vl3b7b72binstruct_are_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itq30t/qwenqwen25vl3b7b72binstruct_are_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itq30t/qwenqwen25vl3b7b72binstruct_are_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T04:28:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu56o1</id>
    <title>10x longer contexts for reasoning training - 90% less memory GRPO in Unsloth</title>
    <updated>2025-02-20T18:15:26+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! Thanks so much for the support on our GRPO release 2 weeks ago! Today, we're excited to announce that you can now train your own reasoning model with just &lt;strong&gt;5GB VRAM&lt;/strong&gt; for Qwen2.5 (1.5B) - down from 7GB in the previous &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; release!&lt;/p&gt; &lt;ol&gt; &lt;li&gt;This is thanks to our newly derived Efficient GRPO algorithm which enables &lt;strong&gt;&lt;em&gt;10x longer context&lt;/em&gt;&lt;/strong&gt; lengths while using &lt;strong&gt;&lt;em&gt;90% less VRAM&lt;/em&gt;&lt;/strong&gt; vs. all other GRPO LoRA/QLoRA implementations, even those utilizing Flash Attention 2 (FA2).&lt;/li&gt; &lt;li&gt;With a GRPO setup using TRL + FA2, Llama 3.1 (8B) training at 20K context length demands &lt;strong&gt;510.8G&lt;/strong&gt; of VRAM. However, Unsloth‚Äôs 90% VRAM reduction brings the requirement down to &lt;strong&gt;just 54.3GB&lt;/strong&gt; in the same setup.&lt;/li&gt; &lt;li&gt;We leverage our &lt;a href="https://unsloth.ai/blog/long-context"&gt;gradient checkpointing&lt;/a&gt; algorithm which we released a while ago. It smartly offloads intermediate activations to system RAM asynchronously whilst being only 1% slower. &lt;strong&gt;&lt;em&gt;This shaves a whopping 372GB VRAM&lt;/em&gt;&lt;/strong&gt; since we need num_generations = 8. We can reduce this memory usage even further through intermediate gradient accumulation.&lt;/li&gt; &lt;li&gt;We also implemented a highly memory efficient GRPO loss, which saves memory usage by 8x. Before 78GB was needed for 20K context length - now only 10GB!&lt;/li&gt; &lt;li&gt;Try our free GRPO notebook with 10x longer context: Llama 3.1 (8B) on Colab: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb&lt;/a&gt;-GRPO.ipynb)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Blog for more details on the algorithm, the Maths behind GRPO, issues we found and more: &lt;a href="https://unsloth.ai/blog/grpo"&gt;https://unsloth.ai/blog/grpo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GRPO VRAM Breakdown:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Unsloth&lt;/th&gt; &lt;th align="left"&gt;TRL + FA2&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Training Memory Cost (GB)&lt;/td&gt; &lt;td align="left"&gt;42GB&lt;/td&gt; &lt;td align="left"&gt;414GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GRPO Memory Cost (GB)&lt;/td&gt; &lt;td align="left"&gt;9.8GB&lt;/td&gt; &lt;td align="left"&gt;78.3GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Inference Cost (GB)&lt;/td&gt; &lt;td align="left"&gt;0GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Inference KV Cache for 20K context (GB)&lt;/td&gt; &lt;td align="left"&gt;2.5GB&lt;/td&gt; &lt;td align="left"&gt;2.5GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Total Memory Usage&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;54.3GB (90% less)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;510.8GB&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;ul&gt; &lt;li&gt;We also now provide full logging details for all reward functions now! Previously we only showed the total aggregated reward function itself.&lt;/li&gt; &lt;li&gt;You can now run and do inference with our &lt;a href="https://unsloth.ai/blog/dynamic-4bit"&gt;4-bit dynamic quants&lt;/a&gt; directly in vLLM.&lt;/li&gt; &lt;li&gt;Also we spent a lot of time on our Guide for everything on GRPO + reward functions/verifiers so would highly recommend you guys to read it: &lt;a href="https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"&gt;docs.unsloth.ai/basics/reasoning&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thank you guys once again for all the support it truly means so much to us! We also have a major release coming within the next few weeks which I know you guys have been waiting for - and we're also excited for it!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu56o1/10x_longer_contexts_for_reasoning_training_90/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu56o1/10x_longer_contexts_for_reasoning_training_90/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu56o1/10x_longer_contexts_for_reasoning_training_90/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T18:15:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu2sdk</id>
    <title>SmolVLM2: New open-source video models running on your toaster</title>
    <updated>2025-02-20T16:39:27+00:00</updated>
    <author>
      <name>/u/unofficialmerve</name>
      <uri>https://old.reddit.com/user/unofficialmerve</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! It's Merve from Hugging Face, working on zero-shot vision/multimodality üëãüèª&lt;/p&gt; &lt;p&gt;Today we released SmolVLM2, new vision LMs in three sizes: 256M, 500M, 2.2B. This release comes with zero-day support for transformers and MLX, and we built applications based on these, along with video captioning fine-tuning tutorial. &lt;/p&gt; &lt;p&gt;We release the following:&lt;br /&gt; &amp;gt; an iPhone app (runs on 500M model in MLX)&lt;br /&gt; &amp;gt; integration with VLC for segmentation of descriptions (based on 2.2B)&lt;br /&gt; &amp;gt; a video highlights extractor (based on 2.2B)&lt;/p&gt; &lt;p&gt;Here's a video from the iPhone app ‚§µÔ∏è you can read and learn more from our blog and check everything in our collection ü§ó&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1iu2sdk/video/fzmniv61obke1/player"&gt;https://reddit.com/link/1iu2sdk/video/fzmniv61obke1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialmerve"&gt; /u/unofficialmerve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu2sdk/smolvlm2_new_opensource_video_models_running_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu2sdk/smolvlm2_new_opensource_video_models_running_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu2sdk/smolvlm2_new_opensource_video_models_running_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T16:39:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu4bc0</id>
    <title>New QwQ Confirmed to be in the works ‚Äúno hurries‚Äù</title>
    <updated>2025-02-20T17:40:30+00:00</updated>
    <author>
      <name>/u/YTLupo</name>
      <uri>https://old.reddit.com/user/YTLupo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4bc0/new_qwq_confirmed_to_be_in_the_works_no_hurries/"&gt; &lt;img alt="New QwQ Confirmed to be in the works ‚Äúno hurries‚Äù" src="https://preview.redd.it/7e0x8lh3zbke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88979b41d573ee37d2cb08acc56da58b982176c2" title="New QwQ Confirmed to be in the works ‚Äúno hurries‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A lot of interesting replies &lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/justinlin610/status/1892625351664099613?s=46&amp;amp;t=4SUD3tHKISm8olRn08tH1A"&gt;https://x.com/justinlin610/status/1892625351664099613?s=46&amp;amp;t=4SUD3tHKISm8olRn08tH1A&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As someone who uses QWEN2.5 and the existing QwQ model I‚Äôm pretty hype to see what happens. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YTLupo"&gt; /u/YTLupo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7e0x8lh3zbke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4bc0/new_qwq_confirmed_to_be_in_the_works_no_hurries/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4bc0/new_qwq_confirmed_to_be_in_the_works_no_hurries/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T17:40:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu19zy</id>
    <title>2025 is an AI madhouse</title>
    <updated>2025-02-20T15:36:23+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu19zy/2025_is_an_ai_madhouse/"&gt; &lt;img alt="2025 is an AI madhouse" src="https://preview.redd.it/ferhsryxcbke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc1632f508c8f4f33f22d5753531a2d6bc7a1ca3" title="2025 is an AI madhouse" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2025 is straight-up wild for AI development. Just last year, it was mostly ChatGPT, Claude, and Gemini running the show. &lt;/p&gt; &lt;p&gt;Now? We‚Äôve got an AI battle royale with everyone jumping in Deepseek, Kimi, Meta, Perplexity, Elon‚Äôs Grok&lt;/p&gt; &lt;p&gt;With all these options, the real question is: which one are you actually using daily?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ferhsryxcbke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu19zy/2025_is_an_ai_madhouse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu19zy/2025_is_an_ai_madhouse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T15:36:23+00:00</published>
  </entry>
</feed>
