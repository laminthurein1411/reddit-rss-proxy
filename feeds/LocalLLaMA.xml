<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-19T23:48:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kq1g7s</id>
    <title>The first author of the ParScale paper discusses how they turned ParScale from an idea into reality</title>
    <updated>2025-05-19T02:55:28+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq1g7s/the_first_author_of_the_parscale_paper_discusses/"&gt; &lt;img alt="The first author of the ParScale paper discusses how they turned ParScale from an idea into reality" src="https://b.thumbs.redditmedia.com/bAgKbu4x_vB4NNm42eNbQsAZxlwnjN3U6xEN99bLNKc.jpg" title="The first author of the ParScale paper discusses how they turned ParScale from an idea into reality" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Because many friends have given feedback that Zhihu cannot be accessed without registration, I am simply using a translation plugin to translate posts from Zhihu into English and taking screenshots. &lt;/p&gt; &lt;p&gt;The original author is keytoyze, who holds all rights to the article. The original address is:&lt;/p&gt; &lt;p&gt;&lt;a href="http://www.zhihu.com/question/1907422978985169131/answer/1907565157103694086"&gt;www.zhihu.com/question/1907422978985169131/answer/1907565157103694086&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/coxrzxd6ln1f1.png?width=869&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55637a7888ae9396e88a09ea0ed134bd153e7dcb"&gt;https://preview.redd.it/coxrzxd6ln1f1.png?width=869&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55637a7888ae9396e88a09ea0ed134bd153e7dcb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hudkuuf7ln1f1.png?width=862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c9af9f77370961a07bdc6876c6be9e84c3ff2de"&gt;https://preview.redd.it/hudkuuf7ln1f1.png?width=862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c9af9f77370961a07bdc6876c6be9e84c3ff2de&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xebnsy18ln1f1.png?width=877&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b8c78a0d42bead0e4838d2f6f24da84d5a706b3a"&gt;https://preview.redd.it/xebnsy18ln1f1.png?width=877&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b8c78a0d42bead0e4838d2f6f24da84d5a706b3a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yuzdfp8ln1f1.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a03790528375bd05619f79e335c08cafa9659595"&gt;https://preview.redd.it/3yuzdfp8ln1f1.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a03790528375bd05619f79e335c08cafa9659595&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z07wi6f9ln1f1.png?width=855&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=230c6c9bba3ae8d72838c06d5ae6c0f7fdab16d3"&gt;https://preview.redd.it/z07wi6f9ln1f1.png?width=855&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=230c6c9bba3ae8d72838c06d5ae6c0f7fdab16d3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bs6cecy9ln1f1.png?width=856&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b948927ff6a3edeea98ddc37377eac53e5a968fd"&gt;https://preview.redd.it/bs6cecy9ln1f1.png?width=856&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b948927ff6a3edeea98ddc37377eac53e5a968fd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq1g7s/the_first_author_of_the_parscale_paper_discusses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq1g7s/the_first_author_of_the_parscale_paper_discusses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq1g7s/the_first_author_of_the_parscale_paper_discusses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T02:55:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpw9nw</id>
    <title>Unlimited text-to-speech using Kokoro-JS, 100% local, 100% open source</title>
    <updated>2025-05-18T22:26:10+00:00</updated>
    <author>
      <name>/u/paranoidray</name>
      <uri>https://old.reddit.com/user/paranoidray</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paranoidray"&gt; /u/paranoidray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://streaming-kokoro.glitch.me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpw9nw/unlimited_texttospeech_using_kokorojs_100_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpw9nw/unlimited_texttospeech_using_kokorojs_100_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T22:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq4ey4</id>
    <title>NVIDIA says DGX Spark releasing in July</title>
    <updated>2025-05-19T05:56:22+00:00</updated>
    <author>
      <name>/u/Aplakka</name>
      <uri>https://old.reddit.com/user/Aplakka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DGX Spark should be available in July.&lt;/p&gt; &lt;p&gt;The 128 GB unified memory amount is nice, but there's been discussions about whether the bandwidth will be too slow to be practical. Will be interesting to see what independent benchmarks will show, I don't think it's had any outsider reviews yet. I couldn't find a price yet, that of course will be quite important too.&lt;/p&gt; &lt;p&gt;&lt;a href="https://nvidianews.nvidia.com/news/nvidia-launches-ai-first-dgx-personal-computing-systems-with-global-computer-makers"&gt;https://nvidianews.nvidia.com/news/nvidia-launches-ai-first-dgx-personal-computing-systems-with-global-computer-makers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;|| || |System Memory|128 GB LPDDR5x, unified system memory|&lt;/p&gt; &lt;p&gt;|| || |Memory Bandwidth|273 GB/s|&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aplakka"&gt; /u/Aplakka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq4ey4/nvidia_says_dgx_spark_releasing_in_july/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq4ey4/nvidia_says_dgx_spark_releasing_in_july/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq4ey4/nvidia_says_dgx_spark_releasing_in_july/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T05:56:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqfu8l</id>
    <title>Local speech chat with Gemma3, speaking like a polyglot with multiple-personalities</title>
    <updated>2025-05-19T16:19:07+00:00</updated>
    <author>
      <name>/u/QuantuisBenignus</name>
      <uri>https://old.reddit.com/user/QuantuisBenignus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Low-latency, speech-to(text-to)-speech conversation in any Linux window:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/QuantiusBenignus/BlahST/blob/main/SPEECH-CHAT.md"&gt;Demo video here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is &lt;strong&gt;blahstbot&lt;/strong&gt;, part of the UI-less, text-in-any-window, BlahST for Linux.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuantuisBenignus"&gt; /u/QuantuisBenignus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqfu8l/local_speech_chat_with_gemma3_speaking_like_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqfu8l/local_speech_chat_with_gemma3_speaking_like_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqfu8l/local_speech_chat_with_gemma3_speaking_like_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T16:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqp46f</id>
    <title>I added automatic language detection and text-to-speech response to AI Runner</title>
    <updated>2025-05-19T22:25:00+00:00</updated>
    <author>
      <name>/u/w00fl35</name>
      <uri>https://old.reddit.com/user/w00fl35</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqp46f/i_added_automatic_language_detection_and/"&gt; &lt;img alt="I added automatic language detection and text-to-speech response to AI Runner" src="https://external-preview.redd.it/bnh3ZjBiaDJldDFmMTuTBNgqFywp7VxarWureDzUbFixi-3H8s4hiED7R6fh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a39625bae2610f5e342462766769754fbda8bc6" title="I added automatic language detection and text-to-speech response to AI Runner" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w00fl35"&gt; /u/w00fl35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/of7p2tkzdt1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqp46f/i_added_automatic_language_detection_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqp46f/i_added_automatic_language_detection_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T22:25:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqq9t9</id>
    <title>Using your local Models to run Agents! (Open Source, 100% local)</title>
    <updated>2025-05-19T23:18:17+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqq9t9/using_your_local_models_to_run_agents_open_source/"&gt; &lt;img alt="Using your local Models to run Agents! (Open Source, 100% local)" src="https://external-preview.redd.it/d2Fkam5hbDludDFmMUseoVY8fQTbYJfjqlW4w2NBhsFRYZKCiBtmbkUNYsUI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ea3254a21d24b2aa2390d05a60fb21282bf02a7" title="Using your local Models to run Agents! (Open Source, 100% local)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5p9moal9nt1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqq9t9/using_your_local_models_to_run_agents_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqq9t9/using_your_local_models_to_run_agents_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T23:18:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqi3m0</id>
    <title>Microsoft On-Device AI Local Foundry (Windows &amp; Mac)</title>
    <updated>2025-05-19T17:46:57+00:00</updated>
    <author>
      <name>/u/AngryBirdenator</name>
      <uri>https://old.reddit.com/user/AngryBirdenator</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqi3m0/microsoft_ondevice_ai_local_foundry_windows_mac/"&gt; &lt;img alt="Microsoft On-Device AI Local Foundry (Windows &amp;amp; Mac)" src="https://external-preview.redd.it/s7HfWyX7uW6coTKNdAxswziHueC-nss9O7Clu8I3zyI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9fdb1d3bcf87ad29b196536b22c8576e153d5560" title="Microsoft On-Device AI Local Foundry (Windows &amp;amp; Mac)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AngryBirdenator"&gt; /u/AngryBirdenator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://devblogs.microsoft.com/foundry/unlock-instant-on-device-ai-with-foundry-local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqi3m0/microsoft_ondevice_ai_local_foundry_windows_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqi3m0/microsoft_ondevice_ai_local_foundry_windows_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T17:46:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq9mfl</id>
    <title>Intel Announces Arc Pro B-Series, "Project Battlematrix" Linux Software Improvements</title>
    <updated>2025-05-19T11:46:51+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq9mfl/intel_announces_arc_pro_bseries_project/"&gt; &lt;img alt="Intel Announces Arc Pro B-Series, &amp;quot;Project Battlematrix&amp;quot; Linux Software Improvements" src="https://external-preview.redd.it/GNAZ-cVMVgweIup1G1242zhFgkq9dtWIloVxhHHhjYo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a042c504c995ad73ede19093c65cdc62c3f82fe9" title="Intel Announces Arc Pro B-Series, &amp;quot;Project Battlematrix&amp;quot; Linux Software Improvements" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/review/intel-arc-pro-b-series"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq9mfl/intel_announces_arc_pro_bseries_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq9mfl/intel_announces_arc_pro_bseries_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T11:46:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpyn8g</id>
    <title>Qwen released new paper and model: ParScale, ParScale-1.8B-(P1-P8)</title>
    <updated>2025-05-19T00:24:28+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpyn8g/qwen_released_new_paper_and_model_parscale/"&gt; &lt;img alt="Qwen released new paper and model: ParScale, ParScale-1.8B-(P1-P8)" src="https://preview.redd.it/7q0xsc86um1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=952df9feb0cce10d5227340e9e367e9fc6939abe" title="Qwen released new paper and model: ParScale, ParScale-1.8B-(P1-P8)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The original text says, 'We theoretically and empirically establish that scaling with P parallel streams is comparable to scaling the number of parameters by O(log P).' Does this mean that a 30B model can achieve the effect of a 45B model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7q0xsc86um1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpyn8g/qwen_released_new_paper_and_model_parscale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpyn8g/qwen_released_new_paper_and_model_parscale/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T00:24:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqbh7g</id>
    <title>Been away for two months.. what's the new hotness?</title>
    <updated>2025-05-19T13:18:32+00:00</updated>
    <author>
      <name>/u/bigattichouse</name>
      <uri>https://old.reddit.com/user/bigattichouse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the new hotness? Saw a Qwen model? I'm usually able to run things in the 20-23B range... but if there's low end stuff, I'm interested in that as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigattichouse"&gt; /u/bigattichouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqbh7g/been_away_for_two_months_whats_the_new_hotness/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqbh7g/been_away_for_two_months_whats_the_new_hotness/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqbh7g/been_away_for_two_months_whats_the_new_hotness/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T13:18:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqab4m</id>
    <title>llama.cpp now supports Llama 4 vision</title>
    <updated>2025-05-19T12:22:27+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqab4m/llamacpp_now_supports_llama_4_vision/"&gt; &lt;img alt="llama.cpp now supports Llama 4 vision" src="https://external-preview.redd.it/B8o6PBUKxWoyfTLHowtPtTQrUM4omNNyOv5t_-1MIqk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd011b3d7cd43123e6bb6b624eb22b92c82f10f5" title="llama.cpp now supports Llama 4 vision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Vision support is picking up speed with the recent refactoring to better support it in general. Note that there's a minor(?) &lt;a href="https://github.com/ggml-org/llama.cpp/pull/13282"&gt;issue with Llama 4 vision&lt;/a&gt; in general, as you can see below. It's most likely with the model, not with the implementation in llama.cpp, as the issue also occurs on other inference engines than just llama.cpp.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c25p83fheq1f1.png?width=503&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6eeb50199641034f38969eb526581fe95ef46498"&gt;https://preview.redd.it/c25p83fheq1f1.png?width=503&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6eeb50199641034f38969eb526581fe95ef46498&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqab4m/llamacpp_now_supports_llama_4_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqab4m/llamacpp_now_supports_llama_4_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqab4m/llamacpp_now_supports_llama_4_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T12:22:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqa6l0</id>
    <title>KTransformers v0.3.1 now supports Intel Arc GPUs (A770 + new B-series): 7 tps DeepSeek R1 decode speed for a single CPU + a single A770</title>
    <updated>2025-05-19T12:16:17+00:00</updated>
    <author>
      <name>/u/CombinationNo780</name>
      <uri>https://old.reddit.com/user/CombinationNo780</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As shared in &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kq9294/intel_launches_299_arc_pro_b50_with_16gb_of/"&gt;this post&lt;/a&gt;, Intel just dropped their new Arc Pro B-series GPUs today.&lt;/p&gt; &lt;p&gt;Thanks to early collaboration with Intel, KTransformers v0.3.1 is out now with Day 0 support for these new cards ‚Äî including the previously supported A-series like the A770.&lt;/p&gt; &lt;p&gt;In our test setup with a single-socket Xeon 5 + DDR5 4800MT/s + Arc A770, we‚Äôre seeing around 7.5 tokens/sec decoding speed on &lt;em&gt;deepseek-r1 Q4&lt;/em&gt;. Enabling dual NUMA gives you even better throughput.&lt;/p&gt; &lt;p&gt;More details and setup instructions:&lt;br /&gt; &lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/xpu.md"&gt;https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/xpu.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for all the support, and more updates soon!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombinationNo780"&gt; /u/CombinationNo780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqa6l0/ktransformers_v031_now_supports_intel_arc_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqa6l0/ktransformers_v031_now_supports_intel_arc_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqa6l0/ktransformers_v031_now_supports_intel_arc_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T12:16:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq6ysz</id>
    <title>OuteTTS 1.0 (0.6B) ‚Äî Apache 2.0, Batch Inference (~0.1‚Äì0.02 RTF)</title>
    <updated>2025-05-19T08:56:52+00:00</updated>
    <author>
      <name>/u/OuteAI</name>
      <uri>https://old.reddit.com/user/OuteAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq6ysz/outetts_10_06b_apache_20_batch_inference_01002_rtf/"&gt; &lt;img alt="OuteTTS 1.0 (0.6B) ‚Äî Apache 2.0, Batch Inference (~0.1‚Äì0.02 RTF)" src="https://external-preview.redd.it/mkj0c5KE7uG2t5lRcNFyEg2Rx_CYpgOSNHlXCK0pNG4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9ca9a806b15f609c1dba2685c905de1ca8099ac2" title="OuteTTS 1.0 (0.6B) ‚Äî Apache 2.0, Batch Inference (~0.1‚Äì0.02 RTF)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I just released OuteTTS-1.0-0.6B, a lighter variant built on Qwen-3 0.6B.&lt;/p&gt; &lt;p&gt;OuteTTS-1.0-0.6B&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model Architecture: Based on Qwen-3 0.6B.&lt;/li&gt; &lt;li&gt;License: Apache 2.0 (free for commercial and personal use)&lt;/li&gt; &lt;li&gt;Multilingual: 14 supported languages: English, Chinese, Dutch, French, Georgian, German, Hungarian, Italian, Japanese, Korean, Latvian, Polish, Russian, Spanish&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Python Package Update: outetts v0.4.2&lt;/p&gt; &lt;ul&gt; &lt;li&gt;EXL2 Async: batched inference&lt;/li&gt; &lt;li&gt;vLLM (Experimental): batched inference&lt;/li&gt; &lt;li&gt;Llama.cpp Async Server: continuous batching&lt;/li&gt; &lt;li&gt;Llama.cpp Server: external-URL model inference&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚ö° Benchmarks (Single NVIDIA L40S GPU)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Batch‚ÜíRTF&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;vLLM OuteTTS-1.0-0.6B FP8&lt;/td&gt; &lt;td&gt;16‚Üí0.11, 24‚Üí0.08, 32‚Üí0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;vLLM Llama-OuteTTS-1.0-1B FP8&lt;/td&gt; &lt;td&gt;32‚Üí0.04, 64‚Üí0.03, 128‚Üí0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;EXL2 OuteTTS-1.0-0.6B 8bpw&lt;/td&gt; &lt;td&gt;32‚Üí0.108&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;EXL2 OuteTTS-1.0-0.6B 6bpw&lt;/td&gt; &lt;td&gt;32‚Üí0.106&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;EXL2 Llama-OuteTTS-1.0-1B 8bpw&lt;/td&gt; &lt;td&gt;32‚Üí0.105&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama.cpp server OuteTTS-1.0-0.6B Q8_0&lt;/td&gt; &lt;td&gt;16‚Üí0.22, 32‚Üí0.20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama.cpp server OuteTTS-1.0-0.6B Q6_K&lt;/td&gt; &lt;td&gt;16‚Üí0.21, 32‚Üí0.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama.cpp server Llama-OuteTTS-1.0-1B Q8_0&lt;/td&gt; &lt;td&gt;16‚Üí0.172, 32‚Üí0.166&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama.cpp server Llama-OuteTTS-1.0-1B Q6_K&lt;/td&gt; &lt;td&gt;16‚Üí0.165, 32‚Üí0.164&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;üì¶ Model Weights (ST, GGUF, EXL2, FP8): &lt;a href="https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B"&gt;https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üìÇ Python Inference Library: &lt;a href="https://github.com/edwko/OuteTTS"&gt;https://github.com/edwko/OuteTTS&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OuteAI"&gt; /u/OuteAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq6ysz/outetts_10_06b_apache_20_batch_inference_01002_rtf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq6ysz/outetts_10_06b_apache_20_batch_inference_01002_rtf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T08:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqpemo</id>
    <title>Demo of Sleep-time Compute to Reduce LLM Response Latency</title>
    <updated>2025-05-19T22:37:52+00:00</updated>
    <author>
      <name>/u/Ok_Employee_6418</name>
      <uri>https://old.reddit.com/user/Ok_Employee_6418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqpemo/demo_of_sleeptime_compute_to_reduce_llm_response/"&gt; &lt;img alt="Demo of Sleep-time Compute to Reduce LLM Response Latency" src="https://preview.redd.it/h9iyy36cgt1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6500cd6c480df68fff0b2950464bdf67612a84b6" title="Demo of Sleep-time Compute to Reduce LLM Response Latency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a demo of Sleep-time compute to reduce LLM response latency. &lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/ronantakizawa/sleeptimecompute"&gt;https://github.com/ronantakizawa/sleeptimecompute&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sleep-time compute improves LLM response latency by using the idle time between interactions to pre-process the context, allowing the model to think offline about potential questions before they‚Äôre even asked. &lt;/p&gt; &lt;p&gt;While regular LLM interactions involve the context processing to happen with the prompt input, Sleep-time compute already has the context loaded before the prompt is received, so it requires less time and compute for the LLM to send responses. &lt;/p&gt; &lt;p&gt;The demo demonstrates an average of 6.4x fewer tokens per query and 5.2x speedup in response time for Sleep-time Compute. &lt;/p&gt; &lt;p&gt;The implementation was based on the original paper from Letta / UC Berkeley. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Employee_6418"&gt; /u/Ok_Employee_6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h9iyy36cgt1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqpemo/demo_of_sleeptime_compute_to_reduce_llm_response/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqpemo/demo_of_sleeptime_compute_to_reduce_llm_response/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T22:37:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqa7vx</id>
    <title>Intel Arc B60 DUAL-GPU 48GB Video Card Tear-Down | MAXSUN Arc Pro B60 Dual</title>
    <updated>2025-05-19T12:18:09+00:00</updated>
    <author>
      <name>/u/Optifnolinalgebdirec</name>
      <uri>https://old.reddit.com/user/Optifnolinalgebdirec</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqa7vx/intel_arc_b60_dualgpu_48gb_video_card_teardown/"&gt; &lt;img alt="Intel Arc B60 DUAL-GPU 48GB Video Card Tear-Down | MAXSUN Arc Pro B60 Dual" src="https://external-preview.redd.it/7FOJ3auvDt89Hg_vMibknvVEFUx6iCFTiqBX1eYmFSA.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39cffe3cb1c59d44099155324ec96bb08d047073" title="Intel Arc B60 DUAL-GPU 48GB Video Card Tear-Down | MAXSUN Arc Pro B60 Dual" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/@GamersNexus"&gt;Gamers Nexus&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Optifnolinalgebdirec"&gt; /u/Optifnolinalgebdirec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=Y8MWbPBP9i0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqa7vx/intel_arc_b60_dualgpu_48gb_video_card_teardown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqa7vx/intel_arc_b60_dualgpu_48gb_video_card_teardown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T12:18:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqgwh2</id>
    <title>Drummer's Valkyrie 49B v1 - A strong, creative finetune of Nemotron 49B</title>
    <updated>2025-05-19T17:00:51+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqgwh2/drummers_valkyrie_49b_v1_a_strong_creative/"&gt; &lt;img alt="Drummer's Valkyrie 49B v1 - A strong, creative finetune of Nemotron 49B" src="https://external-preview.redd.it/7-TxzLinFktWh46KZdKq3Yh3o06ZI3kUSrN3cJLAfu4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=899485ad26383ade67df07fffa408803ffb0b047" title="Drummer's Valkyrie 49B v1 - A strong, creative finetune of Nemotron 49B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Valkyrie-49B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqgwh2/drummers_valkyrie_49b_v1_a_strong_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqgwh2/drummers_valkyrie_49b_v1_a_strong_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T17:00:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqh56l</id>
    <title>MLX LM now integrated within Hugging Face</title>
    <updated>2025-05-19T17:09:53+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqh56l/mlx_lm_now_integrated_within_hugging_face/"&gt; &lt;img alt="MLX LM now integrated within Hugging Face" src="https://external-preview.redd.it/ejhyMW5ocXN0cjFmMeXek4ObJQU75YpzzSznbvZU2j6Nva4vduBEs8qjugv3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe76777105e1f4e318005e5384d9b053060a5946" title="MLX LM now integrated within Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;thread: &lt;a href="https://x.com/victormustar/status/1924510517311287508"&gt;https://x.com/victormustar/status/1924510517311287508&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bvoizhqstr1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqh56l/mlx_lm_now_integrated_within_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqh56l/mlx_lm_now_integrated_within_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T17:09:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqiwu2</id>
    <title>Evaluating the best models at translating German - open models beat DeepL!</title>
    <updated>2025-05-19T18:17:55+00:00</updated>
    <author>
      <name>/u/Nuenki</name>
      <uri>https://old.reddit.com/user/Nuenki</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqiwu2/evaluating_the_best_models_at_translating_german/"&gt; &lt;img alt="Evaluating the best models at translating German - open models beat DeepL!" src="https://external-preview.redd.it/DtSOjZDQhCIQrR9MXzfYsDwli-PvO8iAuPXRBhYivls.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=33bb3dd09e1348f194cfb304ced2dd662da82a0f" title="Evaluating the best models at translating German - open models beat DeepL!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nuenki"&gt; /u/Nuenki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nuenki.app/blog/best_language_models_for_german_translation"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqiwu2/evaluating_the_best_models_at_translating_german/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqiwu2/evaluating_the_best_models_at_translating_german/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T18:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq8wo4</id>
    <title>Computex: Intel Unveils New GPUs for AI and Workstations</title>
    <updated>2025-05-19T11:05:10+00:00</updated>
    <author>
      <name>/u/MR_-_501</name>
      <uri>https://old.reddit.com/user/MR_-_501</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq8wo4/computex_intel_unveils_new_gpus_for_ai_and/"&gt; &lt;img alt="Computex: Intel Unveils New GPUs for AI and Workstations" src="https://external-preview.redd.it/a_EOuFMT3wImCaaTP_AxZtoh2M_kQm2Ho4iekIvJrVk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=296206ba445fb5eff3bb134ade0c97c527f347ae" title="Computex: Intel Unveils New GPUs for AI and Workstations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;24GB for $500&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MR_-_501"&gt; /u/MR_-_501 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://newsroom.intel.com/client-computing/computex-intel-unveils-new-gpus-ai-workstations"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq8wo4/computex_intel_unveils_new_gpus_for_ai_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq8wo4/computex_intel_unveils_new_gpus_for_ai_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T11:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq590b</id>
    <title>Clara ‚Äî A fully offline, Modular AI workspace (LLMs + Agents + Automation + Image Gen)</title>
    <updated>2025-05-19T06:53:01+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq590b/clara_a_fully_offline_modular_ai_workspace_llms/"&gt; &lt;img alt="Clara ‚Äî A fully offline, Modular AI workspace (LLMs + Agents + Automation + Image Gen)" src="https://preview.redd.it/u6niruxjqo1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92c4cac8e33b1fe68fdc0af3f66d45dcdcf1c55a" title="Clara ‚Äî A fully offline, Modular AI workspace (LLMs + Agents + Automation + Image Gen)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I‚Äôve been working on this for the past few months and finally feel good enough to share it.&lt;/p&gt; &lt;p&gt;It‚Äôs called &lt;strong&gt;Clara&lt;/strong&gt; ‚Äî and the idea is simple:&lt;/p&gt; &lt;p&gt;üß© &lt;strong&gt;Imagine building your own workspace for AI&lt;/strong&gt; ‚Äî with local tools, agents, automations, and image generation.&lt;/p&gt; &lt;p&gt;Note: Created this becoz i hated the ChatUI for everything, I want everything in one place but i don't wanna jump between apps and its completely opensource with MIT Lisence &lt;/p&gt; &lt;p&gt;Clara lets you do exactly that ‚Äî fully offline, fully modular.&lt;/p&gt; &lt;p&gt;You can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üß± Drop everything as widgets on a dashboard ‚Äî rearrange, resize, and make it &lt;em&gt;yours with all the stuff mentioned below&lt;/em&gt;&lt;/li&gt; &lt;li&gt;üí¨ Chat with local LLMs with Rag, Image, Documents, Run Code like ChatGPT - Supports both Ollama and Any OpenAI Like API&lt;/li&gt; &lt;li&gt;‚öôÔ∏è Create agents with built-in logic &amp;amp; memory &lt;/li&gt; &lt;li&gt;üîÅ Run automations via native N8N integration (1000+ Free Templates in ClaraVerse Store)&lt;/li&gt; &lt;li&gt;üé® Generate images locally using Stable Diffusion (ComfyUI) - (Native Build without ComfyUI Coming Soon)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Clara has app for everything - Mac, Windows, Linux&lt;/p&gt; &lt;p&gt;It‚Äôs like‚Ä¶ instead of opening a bunch of apps, you build your own AI control room. And it all runs on your machine. No cloud. No API keys. No bs.&lt;/p&gt; &lt;p&gt;Would love to hear what y‚Äôall think ‚Äî ideas, bugs, roast me if needed üòÑ&lt;br /&gt; If you're into local-first tooling, this might actually be useful.&lt;/p&gt; &lt;p&gt;Peace ‚úåÔ∏è&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;br /&gt; I built Clara because honestly... I was sick of bouncing between 10 different ChatUIs just to get basic stuff done.&lt;br /&gt; I wanted one place ‚Äî where I could run LLMs, trigger workflows, write code, generate images ‚Äî without switching tabs or tools.&lt;br /&gt; So I made it.&lt;/p&gt; &lt;p&gt;And yeah ‚Äî it‚Äôs fully open-source, MIT licensed, no gatekeeping. Use it, break it, fork it, whatever you want.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u6niruxjqo1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq590b/clara_a_fully_offline_modular_ai_workspace_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq590b/clara_a_fully_offline_modular_ai_workspace_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T06:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqhljr</id>
    <title>VS Code: Open Source Copilot</title>
    <updated>2025-05-19T17:27:31+00:00</updated>
    <author>
      <name>/u/DonTizi</name>
      <uri>https://old.reddit.com/user/DonTizi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqhljr/vs_code_open_source_copilot/"&gt; &lt;img alt="VS Code: Open Source Copilot" src="https://external-preview.redd.it/7Ri8YRwu_7FpWFvmcgOzjF960jd6eY_pMWtoGfUyNOA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=717dd6dca05edfe21ffc3b5167abc2a06a881f81" title="VS Code: Open Source Copilot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you think of this move by Microsoft? Is it just me, or are the possibilities endless? We can build customizable IDEs with an entire company‚Äôs tech stack by integrating MCPs on top, without having to build everything from scratch.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DonTizi"&gt; /u/DonTizi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://code.visualstudio.com/blogs/2025/05/19/openSourceAIEditor"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqhljr/vs_code_open_source_copilot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqhljr/vs_code_open_source_copilot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T17:27:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqkhhy</id>
    <title>Be confident in your own judgement and reject benchmark JPEG's</title>
    <updated>2025-05-19T19:18:50+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqkhhy/be_confident_in_your_own_judgement_and_reject/"&gt; &lt;img alt="Be confident in your own judgement and reject benchmark JPEG's" src="https://preview.redd.it/1wtj3q6ngs1f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a71e631166bd010fd1e72d10e1ef80ceda179b6" title="Be confident in your own judgement and reject benchmark JPEG's" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1wtj3q6ngs1f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqkhhy/be_confident_in_your_own_judgement_and_reject/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqkhhy/be_confident_in_your_own_judgement_and_reject/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T19:18:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqaqmr</id>
    <title>Is Intel Arc GPU with 48GB of memory going to take over for $1k?</title>
    <updated>2025-05-19T12:43:45+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At the 3:58 mark video says cost is expected to be less than $1K: &lt;a href="https://www.youtube.com/watch?v=Y8MWbPBP9i0"&gt;https://www.youtube.com/watch?v=Y8MWbPBP9i0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://videocardz.com/newz/intel-announces-arc-pro-b60-24gb-and-b50-16gb-cards-dual-b60-features-48gb-memory"&gt;https://videocardz.com/newz/intel-announces-arc-pro-b60-24gb-and-b50-16gb-cards-dual-b60-features-48gb-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The 24GB costs $500, which also seems like a no brainer.&lt;/p&gt; &lt;p&gt;Info on 24gb card:&lt;/p&gt; &lt;p&gt;&lt;a href="https://videocardz.com/newz/intel-announces-arc-pro-b60-24gb-and-b50-16gb-cards-dual-b60-features-48gb-memory"&gt;https://videocardz.com/newz/intel-announces-arc-pro-b60-24gb-and-b50-16gb-cards-dual-b60-features-48gb-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://wccftech.com/intel-arc-pro-b60-24-gb-b50-16-gb-battlemage-gpus-pro-ai-3x-faster-dual-gpu-variant/"&gt;https://wccftech.com/intel-arc-pro-b60-24-gb-b50-16-gb-battlemage-gpus-pro-ai-3x-faster-dual-gpu-variant/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://newsroom.intel.com/client-computing/computex-intel-unveils-new-gpus-ai-workstations"&gt;https://newsroom.intel.com/client-computing/computex-intel-unveils-new-gpus-ai-workstations&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqaqmr/is_intel_arc_gpu_with_48gb_of_memory_going_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqaqmr/is_intel_arc_gpu_with_48gb_of_memory_going_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqaqmr/is_intel_arc_gpu_with_48gb_of_memory_going_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T12:43:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqluy9</id>
    <title>üëÄ Microsoft just created an MCP Registry for Windows</title>
    <updated>2025-05-19T20:12:32+00:00</updated>
    <author>
      <name>/u/eternviking</name>
      <uri>https://old.reddit.com/user/eternviking</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqluy9/microsoft_just_created_an_mcp_registry_for_windows/"&gt; &lt;img alt="üëÄ Microsoft just created an MCP Registry for Windows" src="https://preview.redd.it/6lwf9y6eqs1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3580c11b0dace946cb1f140d2732484fdb0916e4" title="üëÄ Microsoft just created an MCP Registry for Windows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eternviking"&gt; /u/eternviking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6lwf9y6eqs1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqluy9/microsoft_just_created_an_mcp_registry_for_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqluy9/microsoft_just_created_an_mcp_registry_for_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T20:12:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq9294</id>
    <title>Intel launches $299 Arc Pro B50 with 16GB of memory, 'Project Battlematrix' workstations with 24GB Arc Pro B60 GPUs</title>
    <updated>2025-05-19T11:14:29+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq9294/intel_launches_299_arc_pro_b50_with_16gb_of/"&gt; &lt;img alt="Intel launches $299 Arc Pro B50 with 16GB of memory, 'Project Battlematrix' workstations with 24GB Arc Pro B60 GPUs" src="https://external-preview.redd.it/lJpkUaWR7aRg9qhyrcIgwW2kvtG6PxI9-Hw_9dnqBZU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64c87f9f3217c313d6276262cf0a6572a7d3d2af" title="Intel launches $299 Arc Pro B50 with 16GB of memory, 'Project Battlematrix' workstations with 24GB Arc Pro B60 GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;While the B60 is designed for powerful 'Project Battlematrix' AI workstations... will carry a roughly $500 per-unit price tag&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/intel-launches-usd299-arc-pro-b50-with-16gb-of-memory-project-battlematrix-workstations-with-24gb-arc-pro-b60-gpus"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq9294/intel_launches_299_arc_pro_b50_with_16gb_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq9294/intel_launches_299_arc_pro_b50_with_16gb_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T11:14:29+00:00</published>
  </entry>
</feed>
