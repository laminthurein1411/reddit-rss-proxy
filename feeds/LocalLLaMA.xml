<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-16T19:22:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lcz8lg</id>
    <title>What do we need for Qwen 3 235?</title>
    <updated>2025-06-16T17:37:07+00:00</updated>
    <author>
      <name>/u/Fant1xX</name>
      <uri>https://old.reddit.com/user/Fant1xX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My company plans to acquire hardware to do local offline sensitive document processing. We do not need super high throughput, maybe 3 or 4 batches of document processing at a time, but we have the means to spend up to 30.000‚Ç¨. I was thinking about a small Apple Silicon cluster, but is that the way to go in that budget range?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fant1xX"&gt; /u/Fant1xX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcz8lg/what_do_we_need_for_qwen_3_235/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcz8lg/what_do_we_need_for_qwen_3_235/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcz8lg/what_do_we_need_for_qwen_3_235/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T17:37:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcl2m1</id>
    <title>An experimental yet useful On-device Android LLM Assistant</title>
    <updated>2025-06-16T05:43:26+00:00</updated>
    <author>
      <name>/u/abskvrm</name>
      <uri>https://old.reddit.com/user/abskvrm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcl2m1/an_experimental_yet_useful_ondevice_android_llm/"&gt; &lt;img alt="An experimental yet useful On-device Android LLM Assistant" src="https://external-preview.redd.it/MTFkemE2b2g3ODdmMTOQZ3728JJZIuKLMMMDfapcgNjPcOG-8WNcw_29393l.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=361ab76343ab0abc440cf0afb6a2a06c99d5af9e" title="An experimental yet useful On-device Android LLM Assistant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw the recent post (at last) where the OP was looking for a digital assistant for android where they didn't want to access the LLM through any other app's interface. After looking around for something like this, I'm happy to say that I've managed to build one myself.&lt;/p&gt; &lt;p&gt;My Goal: To have a local LLM that can instantly answer questions, summarize text, or manipulate content from anywhere on my phone, basically extend the use of LLM from chatbot to more integration with phone. You can ask your phone &amp;quot;What's the highest mountain?&amp;quot; while in WhatsApp and get an immediate, private answer.&lt;/p&gt; &lt;p&gt;How I Achieved It: * Local LLM Backend: The core of this setup is MNNServer by sunshine0523. This incredible project allows you to run small-ish LLMs directly on your Android device, creating a local API endpoint (e.g., &lt;a href="http://127.0.0.1:8080/v1/chat/completions"&gt;http://127.0.0.1:8080/v1/chat/completions&lt;/a&gt;). The key advantage here is that the models run comfortably in the background without needing to reload them constantly, making for very fast inference. It is interesting to note than I didn't dare try this setup when backend such as llama.cpp through termux or ollamaserver by same developer was available. MNN is practical, llama.cpp on phone is only as good as a chatbot. * My Model Choice: For my 8GB RAM phone, I found taobao-mnn/Qwen2.5-1.5B-Instruct-MNN to be the best performer. It handles assistant-like functions (summarizing/manipulating clipboard text, answering quick questions, manipulating text) really well and for more advance functions it like very promising. Llama 3.2 1b and 3b are good too. (Just make sure to enter the correct model name in http request) * Automation Apps for Frontend &amp;amp; Logic: Interaction with the API happens here. I experimented with two Android automation apps: 1. Macrodroid: I could trigger actions based on a floating button, send clipboard text or voice transcript to the LLM via HTTP POST, give a nice prompt with the input (eg. &amp;quot;content&amp;quot;: &amp;quot;Summarize the text: [lv=UserInput]&amp;quot;) , and receive the response in a notification/TTS/back to clipboard. 2. Tasker: This brings more nuts and bolts to play around. For most, it is more like a DIY project, many moving parts and so is more functional. * Context and Memory: Tasker allows you to feed back previous interactions to the LLM, simulating a basic &amp;quot;memory&amp;quot; function. I haven't gotten this working right now because it's going to take a little time to set it up. Very very experimental.&lt;/p&gt; &lt;p&gt;Features &amp;amp; How they work: * Voice-to-Voice Interaction: * Voice Input: Trigger the assistant. Use Android's built-in voice-to-text (or use Whisper) to capture your spoken query. * LLM Inference: The captured text is sent to the local MNNServer API. * Voice Output: The LLM's response is then passed to a text-to-speech engine (like Google's TTS or another on-device TTS engine) and read aloud. * Text Generation (Clipboard Integration): * Trigger: Summon the assistant (e.g., via floating button). * Clipboard Capture: The automation app (Macrodroid/Tasker) grabs the current text from your clipboard. * LLM Processing: This text is sent to your local LLM with your specific instruction (e.g., &amp;quot;Summarize this:&amp;quot;, &amp;quot;Rewrite this in a professional tone:&amp;quot;). * Automatic Copy to Clipboard: After inference, the LLM's generated response is automatically copied back to your clipboard, ready for you to paste into any app (WhatsApp, email, notes, etc.). * Read Aloud After Inference: * Once the LLM provides its response, the text can be automatically sent to your device's text-to-speech engine (get better TTS than Google's: (&lt;a href="https://k2-fsa.github.io/sherpa/onnx/tts/apk-engine.html"&gt;https://k2-fsa.github.io/sherpa/onnx/tts/apk-engine.html&lt;/a&gt;) and read out loud.&lt;/p&gt; &lt;p&gt;I think there are plenty other ways to use these small with Tasker, though. But it's like going down a rabbithole.&lt;/p&gt; &lt;p&gt;I'll attach the macro in the reply for you try it yourself. (Enable or disable actions and triggers based on your liking) Tasker needs refining, if any one wants I'll share it soon.&lt;/p&gt; &lt;p&gt;The post in question: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ixgvhh/android_digital_assistant/?utm_source=share&amp;amp;utm_medium=mweb3x&amp;amp;utm_name=mweb3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ixgvhh/android_digital_assistant/?utm_source=share&amp;amp;utm_medium=mweb3x&amp;amp;utm_name=mweb3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abskvrm"&gt; /u/abskvrm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/s7noh3oh787f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcl2m1/an_experimental_yet_useful_ondevice_android_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcl2m1/an_experimental_yet_useful_ondevice_android_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T05:43:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbrnod</id>
    <title>Jan-nano, a 4B model that can outperform 671B on MCP</title>
    <updated>2025-06-15T04:24:03+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbrnod/jannano_a_4b_model_that_can_outperform_671b_on_mcp/"&gt; &lt;img alt="Jan-nano, a 4B model that can outperform 671B on MCP" src="https://external-preview.redd.it/cHZ1c3hxZW9wMDdmMa4t04YB4a4x402rBK-VNPFlhWpjFF6pjwxUI9ThBGZC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91ac073491f0e5dcff93b653851cbf8fdeb441de" title="Jan-nano, a 4B model that can outperform 671B on MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone it's me from Menlo Research again,&lt;/p&gt; &lt;p&gt;Today, I‚Äôd like to introduce our latest model: &lt;strong&gt;Jan-nano&lt;/strong&gt; - a model fine-tuned with DAPO on Qwen3-4B. Jan-nano comes with some unique capabilities:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It can perform deep research (with the right prompting)&lt;/li&gt; &lt;li&gt;It picks up relevant information effectively from search results&lt;/li&gt; &lt;li&gt;It uses tools efficiently&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our original goal was to build a super small model that excels at using search tools to extract high-quality information. To evaluate this, we chose &lt;strong&gt;SimpleQA&lt;/strong&gt; - a relatively straightforward benchmark to test whether the model can find and extract the right answers.&lt;/p&gt; &lt;p&gt;Again, Jan-nano only outperforms Deepseek-671B on this metric, using an agentic and tool-usage-based approach. &lt;strong&gt;We are fully aware that a 4B model has its limitations&lt;/strong&gt;, but it's always interesting to see how far you can push it. Jan-nano can serve as your self-hosted Perplexity alternative on a budget. (We're aiming to improve its performance to 85%, or even close to 90%).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;We will be releasing technical report very soon, stay tuned!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can find the model at:&lt;br /&gt; &lt;a href="https://huggingface.co/Menlo/Jan-nano"&gt;https://huggingface.co/Menlo/Jan-nano&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also have gguf at:&lt;br /&gt; &lt;a href="https://huggingface.co/Menlo/Jan-nano-gguf"&gt;https://huggingface.co/Menlo/Jan-nano-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I saw some users have technical challenges on prompt template of the gguf model, please raise it on the issues we will fix one by one. However at the moment &lt;strong&gt;the model can run well in Jan app and llama.server.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmark&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The evaluation was done using agentic setup, which let the model to freely choose tools to use and generate the answer instead of handheld approach of workflow based deep-research repo that you come across online. So basically it's just input question, then model call tool and generate the answer, like you use MCP in the chat app.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SimpleQA:&lt;/strong&gt;&lt;br /&gt; - OpenAI o1: 42.6&lt;br /&gt; - Grok 3: 44.6&lt;br /&gt; - 03: 49.4&lt;br /&gt; - Claude-3.7-Sonnet: 50.0&lt;br /&gt; - Gemini-2.5 pro: 52.9&lt;br /&gt; &lt;strong&gt;- baseline-with-MCP: 59.2&lt;/strong&gt;&lt;br /&gt; - ChatGPT-4.5: 62.5&lt;br /&gt; &lt;strong&gt;- deepseek-671B-with-MCP: 78.2&lt;/strong&gt; (we benchmark using openrouter)&lt;br /&gt; &lt;strong&gt;- jan-nano-v0.4-with-MCP: 80.7&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/p52b768jp07f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbrnod/jannano_a_4b_model_that_can_outperform_671b_on_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbrnod/jannano_a_4b_model_that_can_outperform_671b_on_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T04:24:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcp5rg</id>
    <title>Looking for Unfiltered LLM for making AI Character dialogue</title>
    <updated>2025-06-16T10:18:50+00:00</updated>
    <author>
      <name>/u/mohmar2010</name>
      <uri>https://old.reddit.com/user/mohmar2010</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im just gonna be honest, i want to get dialogue for character chatbots, but unfiltered is what i need, that's pretty much it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mohmar2010"&gt; /u/mohmar2010 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcp5rg/looking_for_unfiltered_llm_for_making_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcp5rg/looking_for_unfiltered_llm_for_making_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcp5rg/looking_for_unfiltered_llm_for_making_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T10:18:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lc2pv9</id>
    <title>PSA: 2 * 3090 with Nvlink can cause depression*</title>
    <updated>2025-06-15T15:15:53+00:00</updated>
    <author>
      <name>/u/cuckfoders</name>
      <uri>https://old.reddit.com/user/cuckfoders</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc2pv9/psa_2_3090_with_nvlink_can_cause_depression/"&gt; &lt;img alt="PSA: 2 * 3090 with Nvlink can cause depression*" src="https://preview.redd.it/sy4x3c4ft37f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b06f38f038f50f6e67a1b6715463246bf3738e46" title="PSA: 2 * 3090 with Nvlink can cause depression*" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. I was enjoying my 3090 so much. So I thought why not get a second? My use case is local coding models, and Gemma 3 mostly. &lt;/p&gt; &lt;p&gt;It's been nothing short of a nightmare to get working. Just about everything that could go wrong, has gone wrong. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mining rig frame took a day to put together &lt;/li&gt; &lt;li&gt;Power supply so huge it's just hanging out of said rig&lt;/li&gt; &lt;li&gt;Pci-e extender cables are a pain&lt;/li&gt; &lt;li&gt;My OS nvme died during this process&lt;/li&gt; &lt;li&gt;Fiddling with bios options to get both to work&lt;/li&gt; &lt;li&gt;Nvlink wasn't clipped on properly at first&lt;/li&gt; &lt;li&gt;I have a pci-e bifurcation card that I'm not using because I'm too scared to see what happens if I plug that in (it has a sata power connector and I'm scared it will just blow up)&lt;/li&gt; &lt;li&gt;Wouldn't turn on this morning (I've snapped my pci-e clips off my motherboard so maybe it's that)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I have a desk fan nearby for when I finish getting vLLM setup. I will try and clip some case fans near them. &lt;/p&gt; &lt;p&gt;I suppose the point of this post and my advice is, if you are going to mess around - build a second machine, don't take your workstation and try make it be something it isn't. &lt;/p&gt; &lt;p&gt;Cheers.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Just trying to have some light humour about self inflicted problems and hoping to help anyone who might be thinking of doing the same to themselves. ‚ù§Ô∏è&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cuckfoders"&gt; /u/cuckfoders &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sy4x3c4ft37f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc2pv9/psa_2_3090_with_nvlink_can_cause_depression/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lc2pv9/psa_2_3090_with_nvlink_can_cause_depression/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T15:15:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcfvw8</id>
    <title>Augmentoolkit just got a major update - huge advance for dataset generation and fine-tuning</title>
    <updated>2025-06-16T00:55:29+00:00</updated>
    <author>
      <name>/u/mj3815</name>
      <uri>https://old.reddit.com/user/mj3815</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share that Augmentoolkit got a significant update that's worth checking out if you're into fine-tuning or dataset generation. Augmentoolkit 3.0 is a major upgrade from the previous version.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/e-p-armstrong/augmentoolkit"&gt;https://github.com/e-p-armstrong/augmentoolkit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For context - I've been using it to create QA datasets from historical texts, and Augmentoolkit filled a big void in my workflow. The previous version was more bare-bones but got the job done for cranking out datasets. This new version is highly polished with a much expanded set of capabilities that could bring fine-tuning to a wider group of people - it now supports going all the way from input data to working fine-tuned model in a single pipeline.&lt;/p&gt; &lt;p&gt;What's new and improved in v3.0:&lt;/p&gt; &lt;p&gt;-Production-ready pipeline that automatically generates training data and trains models for you&lt;/p&gt; &lt;p&gt;-Comes with a custom fine-tuned model specifically built for generating high-quality QA datasets locally (LocalLLaMA, rejoice!)&lt;/p&gt; &lt;p&gt;-Built-in no-code interface so you don't need to mess with command line stuff&lt;/p&gt; &lt;p&gt;-Plus many other improvements under the hood&lt;/p&gt; &lt;p&gt;If you're working on domain-specific fine-tuning or need to generate training data from longer documents, I recommend taking a look. The previous version of the tool has been solid for automating the tedious parts of dataset creation for me. &lt;/p&gt; &lt;p&gt;Anyone else been using Augmentoolkit for their projects?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mj3815"&gt; /u/mj3815 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcfvw8/augmentoolkit_just_got_a_major_update_huge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcfvw8/augmentoolkit_just_got_a_major_update_huge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcfvw8/augmentoolkit_just_got_a_major_update_huge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T00:55:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcglze</id>
    <title>üß¨üß´ü¶† Introducing project hormones: Runtime behavior modification</title>
    <updated>2025-06-16T01:33:19+00:00</updated>
    <author>
      <name>/u/Combinatorilliance</name>
      <uri>https://old.reddit.com/user/Combinatorilliance</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all!&lt;/p&gt; &lt;p&gt;Bored of endless repetitive behavior of LLMs? Want to see your coding agent get insecure and shut up with its endless confidence after it made the same mistake seven times?&lt;/p&gt; &lt;p&gt;Inspired both by &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/18toidc/stop_messing_with_sampling_parameters_and_just/"&gt;drugs&lt;/a&gt; and by my obsessive reading of biology textbooks (biology is fun!)&lt;/p&gt; &lt;p&gt;I am happy to announce &lt;strong&gt;PROJECT HORMONES&lt;/strong&gt; üéâüéâüéâüéäü•≥ü™Ö&lt;/p&gt; &lt;h2&gt;What?&lt;/h2&gt; &lt;p&gt;While large language models are amazing, there's an issue with how they seem to lack inherent adaptability to complex situations.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An LLM runs into to the same error three times in a row? Let's try again with full confidence!&lt;/li&gt; &lt;li&gt;&amp;quot;It's not just X ‚Äî It's Y!&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;What you said is &lt;strong&gt;Genius&lt;/strong&gt;!&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Even though LLMs have achieved metacognition, they completely lack meta-adaptability.&lt;/p&gt; &lt;p&gt;Therefore! Hormones!&lt;/p&gt; &lt;h2&gt;How??&lt;/h2&gt; &lt;p&gt;A hormone is a super simple program with just a few parameters&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A name&lt;/li&gt; &lt;li&gt;A trigger (when should the hormone be released? And how much of the hormone gets released?)&lt;/li&gt; &lt;li&gt;An effect (Should generation temperature go up? Or do you want to intercept and replace tokens during generation? Insert text before and after a message by the user or by the AI! Or temporarily apply a steering vector!)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Or the formal interface expressed in typescript:&lt;/p&gt; &lt;p&gt;``` interface Hormone { name: string; // when should the hormone be released? trigger: (context: Context) =&amp;gt; number; // amount released, [0, 1.0]&lt;/p&gt; &lt;p&gt;// hormones can mess with temperature, top_p etc modifyParams?: (params: GenerationParams, level: number) =&amp;gt; GenerationParams; // this runs are each token generated, the hormone can alter the output of the LLM if it wishes to do so interceptToken?: (token: string, logits: number[], level: number) =&amp;gt; TokenInterceptResult; }&lt;/p&gt; &lt;p&gt;// Internal hormone state (managed by system) interface HormoneState { level: number; // current accumulated amount depletionRate: number; // how fast it decays } ```&lt;/p&gt; &lt;p&gt;What's particularly interesting is that hormones are &lt;em&gt;stochastic&lt;/em&gt;. Meaning that even if a hormone is active, the chance that it will be called is random! The more of the hormone present in the system? The higher the change of it being called!&lt;/p&gt; &lt;p&gt;Not only that, but hormones naturally deplete over time, meaning that your stressed out LLM will chill down after a while.&lt;/p&gt; &lt;p&gt;Additionally, hormones can also act as &lt;em&gt;inhibitors&lt;/em&gt; or &lt;em&gt;amplifiers&lt;/em&gt; for other hormones. Accidentally stressed the hell out of your LLM? Calm it down with some soothing words and release some friendly serotonin, calming acetylcholine and oxytocin for bonding.&lt;/p&gt; &lt;h3&gt;For example, make the LLM more insecure!&lt;/h3&gt; &lt;p&gt;&lt;code&gt; const InsecurityHormone: Hormone = { name: &amp;quot;insecurity&amp;quot;, trigger: (context) =&amp;gt; { // Builds with each &amp;quot;actually that's wrong&amp;quot; or correction const corrections = context.recent_corrections.length * 0.4; const userSighs = context.user_message.match(/no|wrong|sigh|facepalm/gi)?.length || 0; return corrections + (userSighs * 0.3); }, modifyParams: (params, level) =&amp;gt; ({ ...params, temperatureDelta: -0.35 * level }), interceptToken: (token, logits, level) =&amp;gt; { if (token === '.' &amp;amp;&amp;amp; level &amp;gt; 0.7) { return { replace_token: '... umm.. well' }; } return {}; } }; &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;2. Stress the hell out of your LLM with cortisol and adrenaline&lt;/h3&gt; &lt;p&gt;``` const CortisolHormone: Hormone = { name: &amp;quot;cortisol&amp;quot;, trigger: (context) =&amp;gt; { return context.evaluateWith(&amp;quot;stress_threat_detection.prompt&amp;quot;, { user_message: context.user_message, complexity_level: context.user_message.length }); },&lt;/p&gt; &lt;p&gt;modifyParams: (params, level) =&amp;gt; ({ ...params, temperatureDelta: -0.5 * level, // Stress increases accuracy but reduces speed &lt;a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC2568977/?&amp;amp;%20level%20%3E%200.9"&gt;Nih&lt;/a&gt; { const stress_level = Math.floor(level * 5); const cs = 'C'.repeat(stress_level); return { replace_token: &lt;code&gt;. FU${cs}K!!&lt;/code&gt; }; }&lt;/p&gt; &lt;pre&gt;&lt;code&gt;// Stress reallocates from executive control to salience network [Nih](https://pmc.ncbi.nlm.nih.gov/articles/PMC2568977/?&amp;amp; /comprehensive|thorough|multifaceted|intricate/.test(token)) { return { skip_token: true }; } return {}; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;} }; ```&lt;/p&gt; &lt;h3&gt;3. Make your LLM more collaborative with oestrogen&lt;/h3&gt; &lt;p&gt;```typescript const EstrogenHormone: Hormone = { name: &amp;quot;estrogen&amp;quot;, trigger: (context) =&amp;gt; { // Use meta-LLM to evaluate collaborative state return context.evaluateWith(&amp;quot;collaborative_social_state.prompt&amp;quot;, { recent_messages: context.last_n_messages.slice(-3), user_message: context.user_message }); },&lt;/p&gt; &lt;p&gt;modifyParams: (params, level) =&amp;gt; ({ ...params, temperatureDelta: 0.15 * level }),&lt;/p&gt; &lt;p&gt;interceptToken: (token, logits, level) =&amp;gt; { if (token === '.' &amp;amp;&amp;amp; level &amp;gt; 0.6) { return { replace_token: '. What do you think about this approach?' }; } return {}; } }; ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Combinatorilliance"&gt; /u/Combinatorilliance &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcglze/introducing_project_hormones_runtime_behavior/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcglze/introducing_project_hormones_runtime_behavior/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcglze/introducing_project_hormones_runtime_behavior/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T01:33:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcq4gt</id>
    <title>FuturixAI - Cost-Effective Online RFT with Plug-and-Play LoRA Judge</title>
    <updated>2025-06-16T11:15:54+00:00</updated>
    <author>
      <name>/u/Aquaaa3539</name>
      <uri>https://old.reddit.com/user/Aquaaa3539</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A tiny LoRA adapter and a simple JSON prompt turn a 7B LLM into a powerful reward model that beats much larger ones - saving massive compute. It even helps a 7B model outperform top 70B baselines on GSM-8K using online RLHF&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aquaaa3539"&gt; /u/Aquaaa3539 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.futurixai.com/publications"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcq4gt/futurixai_costeffective_online_rft_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcq4gt/futurixai_costeffective_online_rft_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T11:15:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld08xa</id>
    <title>Real Time Speech to Text</title>
    <updated>2025-06-16T18:13:14+00:00</updated>
    <author>
      <name>/u/ThomasSparrow0511</name>
      <uri>https://old.reddit.com/user/ThomasSparrow0511</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As an intern in a finance related company, I need to know about realtime speech to text solutions for our product. I don't have advance knowledge in STT. 1) Any resources to know more about real time STT 2) Best existing products for real time audio (like phone calls) to text for our MLOps pipeline &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasSparrow0511"&gt; /u/ThomasSparrow0511 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld08xa/real_time_speech_to_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld08xa/real_time_speech_to_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld08xa/real_time_speech_to_text/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T18:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld0mo1</id>
    <title>What Really Happens When You Ask a Cursor a Question with GitHub MCP Integrated</title>
    <updated>2025-06-16T18:27:39+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld0mo1/what_really_happens_when_you_ask_a_cursor_a/"&gt; &lt;img alt="What Really Happens When You Ask a Cursor a Question with GitHub MCP Integrated" src="https://external-preview.redd.it/PzaEpIAh22P5SJX20euddepGax_6lKEPNF_QD8rqFzU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7fe90b5d38391dbffdced29cecbb9249ce93c128" title="What Really Happens When You Ask a Cursor a Question with GitHub MCP Integrated" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/vqsjkdjq0c7f1.gif"&gt;https://i.redd.it/vqsjkdjq0c7f1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Have you ever wondered what really happens when you type a prompt like ‚ÄúShow my open PRs‚Äù in Cursor, connected via the&lt;/em&gt; &lt;a href="https://github.com/github/github-mcp-server"&gt;&lt;em&gt;GitHub MCP server&lt;/em&gt;&lt;/a&gt; &lt;em&gt;and Cursor‚Äôs own Model Context Protocol integration? This article breaks down every step, revealing how your simple request triggers a sophisticated pipeline of AI reasoning, tool calls, and secure data handling.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ej86utj30c7f1.png?width=1616&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76b7d27483a1e70369d38c4fc173d2f0dcad5909"&gt;https://preview.redd.it/ej86utj30c7f1.png?width=1616&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76b7d27483a1e70369d38c4fc173d2f0dcad5909&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;You type into Cursor:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;&amp;quot;&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;Show my open PRs from the 100daysofdevops/100daysofdevops repo&lt;/em&gt;&lt;strong&gt;&lt;em&gt;&amp;quot;&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Hit Enter. Done, right?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Beneath that single prompt lies a sophisticated orchestration layer: Cursor‚Äôs cloud-hosted AI models interpret your intent, select the appropriate tool, and trigger the necessary GitHub APIs, all coordinated through the Model Context Protocol (MCP).&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Let‚Äôs look at each layer and walk through the entire lifecycle of your request from keystroke to output.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Step 1: Cursor builds the initial request&lt;/h1&gt; &lt;p&gt;&lt;em&gt;It all starts in the Cursor chat interface. You ask a natural question like:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;&amp;quot;Show my open PRs.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Your prompt &amp;amp; recent chat&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;‚Äì exactly what you typed, plus a short window of chat history.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Relevant code snippets&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;‚Äì any files you‚Äôve recently opened or are viewing in the editor.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;System instructions &amp;amp; metadata&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;‚Äì things like file paths (hashed), privacy flags, and model parameters.&lt;/em&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Cursor bundles all three into a single payload and sends it to the cloud model you picked (e.g., Claude, OpenAI, Anthropic, or Google).&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Nothing is executed yet; the model only receives context.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Step 2: Cursor Realizes It Needs a Tool&lt;/h1&gt; &lt;p&gt;&lt;em&gt;The model reads your intent: &amp;quot;Show my open PRs&amp;quot; It realises plain text isn‚Äôt enough, it needs live data from GitHub.&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;In this case, Cursor identifies that it needs to use the list_pull_requests tool provided by the GitHub MCP server.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;It collects the essential parameters:&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Repository name and owner&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Your GitHub username&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Your stored Personal Access Token (PAT)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;These are wrapped in a structured context object, a powerful abstraction that contains both the user's input and everything the tool needs to respond intelligently.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Step 3: The MCP Tool Call Is Made&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Cursor formats a JSON-RPC request to the GitHub MCP server. Here's what it looks like:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;jsonrpc&amp;quot;: &amp;quot;2.0&amp;quot;, &amp;quot;method&amp;quot;: &amp;quot;tool/list_pull_requests&amp;quot;, &amp;quot;params&amp;quot;: { &amp;quot;owner&amp;quot;: &amp;quot;100daysofdevops&amp;quot;, &amp;quot;repo&amp;quot;: &amp;quot;100daysofdevops&amp;quot;, &amp;quot;state&amp;quot;: &amp;quot;open&amp;quot; }, &amp;quot;id&amp;quot;: &amp;quot;req-42&amp;quot;, &amp;quot;context&amp;quot;: { &amp;quot;conversation&amp;quot;: &amp;quot;...&amp;quot;, &amp;quot;client&amp;quot;: &amp;quot;cursor-ide&amp;quot;, &amp;quot;auth&amp;quot;: { &amp;quot;PAT&amp;quot;: &amp;quot;ghp_****&amp;quot; } } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;NOTE: The context here (including your PAT) is never sent to GitHub. It‚Äôs used locally by the MCP server to authenticate and reason about the request securely (it lives just long enough to fulfil the request).&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Step 4: GitHub MCP Server Does Its Job&lt;/h1&gt; &lt;p&gt;The GitHub MCP server:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Authenticates with GitHub using your PAT&lt;/li&gt; &lt;li&gt;Calls the GitHub REST or GraphQL API to fetch open pull requests&lt;/li&gt; &lt;li&gt;&lt;p&gt;Returns a structured JSON response, for example:&lt;/p&gt; &lt;p&gt;{ &amp;quot;result&amp;quot;: [ { &amp;quot;number&amp;quot;: 17, &amp;quot;title&amp;quot;: &amp;quot;Add MCP demo&amp;quot;, &amp;quot;author&amp;quot;: &amp;quot;PrashantLakhera&amp;quot;, &amp;quot;url&amp;quot;: &amp;quot;&lt;a href="https://github.com/.../pull/17"&gt;https://github.com/.../pull/17&lt;/a&gt;&amp;quot; }, ... ] }&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This response becomes part of the evolving context, enriching the next steps.&lt;/p&gt; &lt;h1&gt;Step 5: Cursor Embeds the Tool Result into the LLM‚Äôs Prompt&lt;/h1&gt; &lt;p&gt;Cursor now reassembles a fresh prompt for the LLM. It includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A system message: &amp;quot;User asked about open pull requests.&amp;quot;&lt;/li&gt; &lt;li&gt;A delimited JSON block: resource://github:list_pull_requests ‚Üí {...}&lt;/li&gt; &lt;li&gt;A short instruction like: &amp;quot;Summarize these PRs for the user.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This grounding ensures the model doesn‚Äôt hallucinate. It just reformats verified data.&lt;/p&gt; &lt;h1&gt;Step 6: The LLM Responds with a Human-Readable Answer&lt;/h1&gt; &lt;p&gt;The LLM converts the structured data into something readable and useful:&lt;/p&gt; &lt;p&gt;&lt;em&gt;You currently have 3 open PRs:&lt;/em&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;#17 Add MCP demo (needs review) &lt;/li&gt; &lt;li&gt;#15 Fix CI timeout (status: failing)&lt;/li&gt; &lt;li&gt;#12 Refactor logging (waiting for approvals)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Cursor streams this back into your chat pane.&lt;/p&gt; &lt;h1&gt;Step 7: The Cycle Continues with Context-Aware Intelligence&lt;/h1&gt; &lt;p&gt;You respond:&lt;/p&gt; &lt;p&gt;&lt;em&gt;&amp;quot;Merge the first one.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Cursor interprets this follow-up, extracts the relevant PR number, and reruns the loop, this time calling merge_pull_request.&lt;/p&gt; &lt;p&gt;Each new call builds on the existing context.&lt;/p&gt; &lt;h1&gt;Why This Matters&lt;/h1&gt; &lt;p&gt;This whole lifecycle showcases how tools like Cursor + MCP redefine developer workflows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Secure, tokenized access to real services&lt;/li&gt; &lt;li&gt;Stateful interaction using structured memory&lt;/li&gt; &lt;li&gt;Tool-enhanced LLMs that go beyond chat&lt;/li&gt; &lt;li&gt;Minimal latency with local reasoning&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You‚Äôre not just chatting with a model; you‚Äôre orchestrating an AI-agentic workflow, backed by tools and context.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Complete Workflow&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hdqeiwf80c7f1.png?width=1152&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8e8c086f3d07d7028758bd6e33429a938070444d"&gt;https://preview.redd.it/hdqeiwf80c7f1.png?width=1152&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8e8c086f3d07d7028758bd6e33429a938070444d&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;Next time you ask Cursor a question, remember: it's not just an API call, it's a mini orchestration pipeline powered by:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cursor‚Äôs intelligent router&lt;/li&gt; &lt;li&gt;GitHub MCP‚Äôs extensible tool interface&lt;/li&gt; &lt;li&gt;Contextual reasoning and secure memory&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That‚Äôs how Cursor evolves from ‚Äújust another chatbot‚Äù into a development companion integrated directly into your workflow.&lt;/p&gt; &lt;p&gt;üìå If you're looking for a single tool to simplify your GenAI workflow and MCP integration, check out IdeaWeaver, your one-stop shop for Generative AI.Comprehensive documentation and examples&lt;br /&gt; üîó Docs: &lt;a href="https://ideaweaver-ai-code.github.io/ideaweaver-docs/"&gt;https://ideaweaver-ai-code.github.io/ideaweaver-docs/&lt;/a&gt;&lt;br /&gt; üîó GitHub: &lt;a href="https://github.com/ideaweaver-ai-code/ideaweaver"&gt;https://github.com/ideaweaver-ai-code/ideaweaver&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld0mo1/what_really_happens_when_you_ask_a_cursor_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld0mo1/what_really_happens_when_you_ask_a_cursor_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld0mo1/what_really_happens_when_you_ask_a_cursor_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T18:27:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lchamn</id>
    <title>What‚Äôs your current tech stack</title>
    <updated>2025-06-16T02:08:55+00:00</updated>
    <author>
      <name>/u/hokies314</name>
      <uri>https://old.reddit.com/user/hokies314</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm using Ollama for local models (but I‚Äôve been following the threads that talk about ditching it) and LiteLLM as a proxy layer so I can connect to OpenAI and Anthropic models too. I have a Postgres database for LiteLLM to use. All but Ollama is orchestrated through a docker compose and Portainer for docker management.&lt;/p&gt; &lt;p&gt;The I have OpenWebUI as the frontend and it connects to LiteLLM or I‚Äôm using Langgraph for my agents. &lt;/p&gt; &lt;p&gt;I‚Äôm kinda exploring my options and want to hear what everyone is using. (And I ditched Docker desktop for Rancher but I‚Äôm exploring other options there too)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hokies314"&gt; /u/hokies314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lchamn/whats_your_current_tech_stack/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lchamn/whats_your_current_tech_stack/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lchamn/whats_your_current_tech_stack/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T02:08:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld11x4</id>
    <title>Humanity's last library, which locally ran LLM would be best?</title>
    <updated>2025-06-16T18:43:42+00:00</updated>
    <author>
      <name>/u/TheCuriousBread</name>
      <uri>https://old.reddit.com/user/TheCuriousBread</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An apocalypse has come upon us. The internet is no more. Libraries are no more. The only things left are local networks and people with the electricity to run them. &lt;/p&gt; &lt;p&gt;If you were to create humanity's last library, a distilled LLM with the entirety of human knowledge. What would be a good model for that? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheCuriousBread"&gt; /u/TheCuriousBread &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld11x4/humanitys_last_library_which_locally_ran_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld11x4/humanitys_last_library_which_locally_ran_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld11x4/humanitys_last_library_which_locally_ran_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T18:43:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lc6tii</id>
    <title>I wrapped Apple‚Äôs new on-device models in an OpenAI-compatible API</title>
    <updated>2025-06-15T18:06:56+00:00</updated>
    <author>
      <name>/u/FixedPt</name>
      <uri>https://old.reddit.com/user/FixedPt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent the weekend vibe-coding in Cursor and ended up with a small Swift app that turns the new macOS 26 on-device Apple Intelligence models into a local server you can hit with standard OpenAI &lt;code&gt;/v1/chat/completions&lt;/code&gt; calls. Point any client you like at &lt;code&gt;http://127.0.0.1:11535&lt;/code&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nothing leaves your Mac&lt;/li&gt; &lt;li&gt;Works with any OpenAI-compatible client&lt;/li&gt; &lt;li&gt;Open source, MIT-licensed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo‚Äôs here ‚Üí &lt;a href="https://github.com/gety-ai/apple-on-device-openai"&gt;&lt;strong&gt;https://github.com/gety-ai/apple-on-device-openai&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It was a fun hack‚Äîlet me know if you try it out or run into any weirdness. Cheers! üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FixedPt"&gt; /u/FixedPt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc6tii/i_wrapped_apples_new_ondevice_models_in_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lc6tii/i_wrapped_apples_new_ondevice_models_in_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lc6tii/i_wrapped_apples_new_ondevice_models_in_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T18:06:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcbs7z</id>
    <title>FULL LEAKED v0 System Prompts and Tools [UPDATED]</title>
    <updated>2025-06-15T21:37:55+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Latest system prompt: 15/06/2025)&lt;/p&gt; &lt;p&gt;I managed to get FULL updated v0 system prompt and internal tools info. Over 900 lines&lt;/p&gt; &lt;p&gt;You can it out at: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcbs7z/full_leaked_v0_system_prompts_and_tools_updated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcbs7z/full_leaked_v0_system_prompts_and_tools_updated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcbs7z/full_leaked_v0_system_prompts_and_tools_updated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T21:37:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lco9ik</id>
    <title>Recommendations for Local LLMs (Under 70B) with Cline/Roo Code</title>
    <updated>2025-06-16T09:20:49+00:00</updated>
    <author>
      <name>/u/AMOVCS</name>
      <uri>https://old.reddit.com/user/AMOVCS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd like to know what, if any, are some good local models under 70b that can handle tasks well when using Cline/Roo Code. I‚Äôve tried a &lt;em&gt;lot&lt;/em&gt; to use Cline or Roo Code for various things, and most of the time it's simple tasks, but the agents often get stuck in loops or make things worse. It feels like the size of the instructions is too much for these smaller LLMs to handle well ‚Äì many times I see the task using 15k+ tokens just to edit a couple lines of code. Maybe I‚Äôm doing something very wrong, maybe it's a configuration issue with the agents? Anyway, I was hoping you guys could recommend some models (could also be configurations, advice, anything) that work well with Cline/Roo Code.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some information for context:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I always use at least Q5 or better (sometimes I use Q4_UD from Unsloth).&lt;/li&gt; &lt;li&gt;Most of the time I give 20k+ context window to the agents.&lt;/li&gt; &lt;li&gt;My projects are a reasonable size, between 2k and 10k lines, but I only open the files needed when asking the agents to code.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Models I've Tried:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Devistral - Bad in general; I was on high expectations for this one but it didn‚Äôt work.&lt;/li&gt; &lt;li&gt;Magistral - Even worse.&lt;/li&gt; &lt;li&gt;Qwen 3 series (and R1 distilled versions) - Not that bad, but just works when the project is very, very small.&lt;/li&gt; &lt;li&gt;GLM4 - Very good at coding on its own, not so good when using it with agents.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;So, are there any recommendations for models to use with Cline/Roo Code that actually work well?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AMOVCS"&gt; /u/AMOVCS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lco9ik/recommendations_for_local_llms_under_70b_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lco9ik/recommendations_for_local_llms_under_70b_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lco9ik/recommendations_for_local_llms_under_70b_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T09:20:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcziww</id>
    <title>Recommending Practical Experiments from Research Papers</title>
    <updated>2025-06-16T17:47:17+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcziww/recommending_practical_experiments_from_research/"&gt; &lt;img alt="Recommending Practical Experiments from Research Papers" src="https://preview.redd.it/y35s13wkrb7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86e22fa7387d9780a5686b29439d2c933cb0510a" title="Recommending Practical Experiments from Research Papers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lately, I've been using LLMs to rank new arXiv papers based on the context of my own work.&lt;/p&gt; &lt;p&gt;This has helped me find relevant results hours after they've been posted, regardless of the virality.&lt;/p&gt; &lt;p&gt;Historically, I've been finetuning VLMs with LoRA, so &lt;a href="https://hsi-che-lin.github.io/EMLoC/"&gt;EMLoC&lt;/a&gt; recently came recommended.&lt;/p&gt; &lt;p&gt;Ultimately, I want to go beyond supporting my own intellectual curiosity to make suggestions rooted in my application context: constraints, hardware, prior experiments, and what has worked in the past.&lt;/p&gt; &lt;p&gt;I'm building toward a workflow where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Past experiment logs feed into paper recommendations&lt;/li&gt; &lt;li&gt;AI proposes lightweight trials using existing code, models, datasets&lt;/li&gt; &lt;li&gt;I can test methods fast and learn what transfers to my use case&lt;/li&gt; &lt;li&gt;Feed the results back into the loop&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Think of it as a &lt;strong&gt;knowledge flywheel&lt;/strong&gt; assisted with an experiment copilot to help you &lt;strong&gt;decide what to try next&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;How are you discovering your next great idea? &lt;/p&gt; &lt;p&gt;Looking to make research more reproducible and relevant, let's chat!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y35s13wkrb7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcziww/recommending_practical_experiments_from_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcziww/recommending_practical_experiments_from_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T17:47:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcy6fc</id>
    <title>DeepSeek R1 0528 Ties Claude Opus 4 for #1 in WebDev Arena ‚Äî [Ranks #6 Overall, #2 in Coding, #4 in Hard Prompts, &amp; #5 in Math]</title>
    <updated>2025-06-16T16:58:09+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcy6fc/deepseek_r1_0528_ties_claude_opus_4_for_1_in/"&gt; &lt;img alt="DeepSeek R1 0528 Ties Claude Opus 4 for #1 in WebDev Arena ‚Äî [Ranks #6 Overall, #2 in Coding, #4 in Hard Prompts, &amp;amp; #5 in Math]" src="https://b.thumbs.redditmedia.com/w2fxA8vcEvjyTTb7SE1rMd45lIMFNP_po--PCvpiWNc.jpg" title="DeepSeek R1 0528 Ties Claude Opus 4 for #1 in WebDev Arena ‚Äî [Ranks #6 Overall, #2 in Coding, #4 in Hard Prompts, &amp;amp; #5 in Math]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/pqp7qmk8kb7f1.png?width=1970&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7357f25c7c29c823ca444d21cd450535b0473912"&gt;https://preview.redd.it/pqp7qmk8kb7f1.png?width=1970&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7357f25c7c29c823ca444d21cd450535b0473912&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hser7yj9kb7f1.png?width=1970&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75e29998321e3d94009940d1fe181606352977cc"&gt;https://preview.redd.it/hser7yj9kb7f1.png?width=1970&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75e29998321e3d94009940d1fe181606352977cc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qjy2shnakb7f1.png?width=1970&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4febbdb1c60a985f5ab48b312a063fe2d5f45984"&gt;https://preview.redd.it/qjy2shnakb7f1.png?width=1970&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4febbdb1c60a985f5ab48b312a063fe2d5f45984&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/leaderboard"&gt;&lt;em&gt;https://lmarena.ai/leaderboard&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcy6fc/deepseek_r1_0528_ties_claude_opus_4_for_1_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcy6fc/deepseek_r1_0528_ties_claude_opus_4_for_1_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcy6fc/deepseek_r1_0528_ties_claude_opus_4_for_1_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T16:58:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcksww</id>
    <title>Do AI wrapper startups have a real future?</title>
    <updated>2025-06-16T05:26:41+00:00</updated>
    <author>
      <name>/u/Samonji</name>
      <uri>https://old.reddit.com/user/Samonji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been thinking about how many startups right now are essentially just wrappers around GPT or Claude, where they take the base model, add a nice UI or some prompt chains, and maybe tailor it to a niche, all while calling it a product.&lt;/p&gt; &lt;p&gt;Some of them are even making money, but I keep wondering‚Ä¶ how long can that really last?&lt;/p&gt; &lt;p&gt;Like, once OpenAI or whoever bakes those same features into their platform, what‚Äôs stopping these wrapper apps from becoming irrelevant overnight? Can any of them actually build a moat?&lt;/p&gt; &lt;p&gt;Or is the only real path to focus super hard on a specific vertical (like legal or finance), gather your own data, and basically evolve beyond being just a wrapper?&lt;/p&gt; &lt;p&gt;Curious what you all think. Are these wrapper apps legit businesses, or just temporary hacks riding the hype wave?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Samonji"&gt; /u/Samonji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcksww/do_ai_wrapper_startups_have_a_real_future/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcksww/do_ai_wrapper_startups_have_a_real_future/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcksww/do_ai_wrapper_startups_have_a_real_future/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T05:26:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcxcuv</id>
    <title>Which vectorDB do you use? and why?</title>
    <updated>2025-06-16T16:26:48+00:00</updated>
    <author>
      <name>/u/Expert-Address-2918</name>
      <uri>https://old.reddit.com/user/Expert-Address-2918</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hate pinecone, why do you hate it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expert-Address-2918"&gt; /u/Expert-Address-2918 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcxcuv/which_vectordb_do_you_use_and_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcxcuv/which_vectordb_do_you_use_and_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcxcuv/which_vectordb_do_you_use_and_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T16:26:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcuglb</id>
    <title>MiniMax-M1 - a MiniMaxAI Collection</title>
    <updated>2025-06-16T14:35:55+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcuglb/minimaxm1_a_minimaxai_collection/"&gt; &lt;img alt="MiniMax-M1 - a MiniMaxAI Collection" src="https://external-preview.redd.it/KeaWNzZG0TAkUEwWyVGmsizl5dXuAOVMgFreGf02gFI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2286c64db955bf2850b44ae4b5c870213ee65afe" title="MiniMax-M1 - a MiniMaxAI Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/MiniMaxAI/minimax-m1-68502ad9634ec0eeac8cf094"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcuglb/minimaxm1_a_minimaxai_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcuglb/minimaxm1_a_minimaxai_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T14:35:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld116d</id>
    <title>MiniMax latest open-sourcing LLM, MiniMax-M1 ‚Äî setting new standards in long-context reasoning,m</title>
    <updated>2025-06-16T18:42:52+00:00</updated>
    <author>
      <name>/u/srtng</name>
      <uri>https://old.reddit.com/user/srtng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld116d/minimax_latest_opensourcing_llm_minimaxm1_setting/"&gt; &lt;img alt="MiniMax latest open-sourcing LLM, MiniMax-M1 ‚Äî setting new standards in long-context reasoning,m" src="https://external-preview.redd.it/NmY1emg2N3kzYzdmMYrLLSKpxq16_nlRw_xdAcAPTlqNhk8r4UDdsUawD6kP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3275f690016b299979a56d72371c6133b5aa21d3" title="MiniMax latest open-sourcing LLM, MiniMax-M1 ‚Äî setting new standards in long-context reasoning,m" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The coding demo in video is so amazing!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;World‚Äôs longest context window: 1M-token input, 80k-token output&lt;/li&gt; &lt;li&gt;State-of-the-art agentic use among open-source models&lt;/li&gt; &lt;li&gt;&lt;p&gt;RL at unmatched efficiency: trained with just $534,700&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;40k: ‚Äã&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-40k"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M1-40k&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;80k: ‚Äã&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-80k"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M1-80k&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Space: ‚Äã&lt;a href="https://huggingface.co/spaces/MiniMaxAI/MiniMax-M1"&gt;https://huggingface.co/spaces/MiniMaxAI/MiniMax-M1&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/MiniMax-AI/MiniMax-M1"&gt;https://github.com/MiniMax-AI/MiniMax-M1&lt;/a&gt; &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Tech Report: &lt;a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/MiniMax_M1_tech_report.pdf"&gt;https://github.com/MiniMax-AI/MiniMax-M1/blob/main/MiniMax_M1_tech_report.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Apache 2.0 license&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/srtng"&gt; /u/srtng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t859utey3c7f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld116d/minimax_latest_opensourcing_llm_minimaxm1_setting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld116d/minimax_latest_opensourcing_llm_minimaxm1_setting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T18:42:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcud8j</id>
    <title>Local Open Source VScode Copilot model with MCP</title>
    <updated>2025-06-16T14:32:16+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You don't need remote APIs for a coding copliot, or the MCP Course! Set up a fully local IDE with MCP integration using Continue. In this tutorial Continue guides you through setting it up.&lt;/p&gt; &lt;p&gt;This is what you need to do to take control of your copilot:&lt;br /&gt; &lt;strong&gt;-&lt;/strong&gt; Get the Continue extension from the &lt;a href="https://marketplace.visualstudio.com/items?itemName=Continue.continue"&gt;VS Code marketplace&lt;/a&gt; to serve as the AI coding assistant.&lt;br /&gt; - Serve the model with an OpenAI compatible server in Llama.cpp / LmStudio/ etc.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server -hf unsloth/Devstral-Small-2505-GGUF:Q4_K_M &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;-&lt;/strong&gt; Create a &lt;code&gt;.continue/models/llama-max.yaml&lt;/code&gt; file in your project to tell Continue how to use the local Ollama model.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;name: Llama.cpp model version: 0.0.1 schema: v1 models: - provider: llama.cpp model: unsloth/Devstral-Small-2505-GGUF apiBase: http://localhost:8080 defaultCompletionOptions: contextLength: 8192 # Adjust based on the model name: Llama.cpp Devstral-Small roles: - chat - edit &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;-&lt;/strong&gt; Create a &lt;code&gt;.continue/mcpServers/playwright-mcp.yaml&lt;/code&gt; file to integrate a tool, like the Playwright browser automation tool, with your assistant.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;name: Playwright mcpServer version: 0.0.1 schema: v1 mcpServers: - name: Browser search command: npx args: - &amp;quot;@playwright/mcp@latest&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Check out the full tutorial here: &lt;a href="https://huggingface.co/learn/mcp-course/unit2/continue-client"&gt;https://huggingface.co/learn/mcp-course/unit2/continue-client&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcud8j/local_open_source_vscode_copilot_model_with_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcud8j/local_open_source_vscode_copilot_model_with_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcud8j/local_open_source_vscode_copilot_model_with_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T14:32:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcrt1k</id>
    <title>Just finished recording 29 videos on "How to Build DeepSeek from Scratch"</title>
    <updated>2025-06-16T12:43:06+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Playlist link: &lt;a href="https://www.youtube.com/playlist?list=PLPTV0NXA_ZSiOpKKlHCyOq9lnp-dLvlms"&gt;https://www.youtube.com/playlist?list=PLPTV0NXA_ZSiOpKKlHCyOq9lnp-dLvlms&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are the 29 videos and their title:&lt;/p&gt; &lt;p&gt;(1) DeepSeek series introduction&lt;/p&gt; &lt;p&gt;(2) DeepSeek basics&lt;/p&gt; &lt;p&gt;(3) Journey of a token into the LLM architecture&lt;/p&gt; &lt;p&gt;(4) Attention mechanism explained in 1 hour&lt;/p&gt; &lt;p&gt;(5) Self Attention Mechanism - Handwritten from scratch&lt;/p&gt; &lt;p&gt;(6) Causal Attention Explained: Don't Peek into the Future&lt;/p&gt; &lt;p&gt;(7) Multi-Head Attention Visually Explained&lt;/p&gt; &lt;p&gt;(8) Multi-Head Attention Handwritten from Scratch&lt;/p&gt; &lt;p&gt;(9) Key Value Cache from Scratch&lt;/p&gt; &lt;p&gt;(10) Multi-Query Attention Explained&lt;/p&gt; &lt;p&gt;(11) Understand Grouped Query Attention (GQA)&lt;/p&gt; &lt;p&gt;(12) Multi-Head Latent Attention From Scratch&lt;/p&gt; &lt;p&gt;(13) Multi-Head Latent Attention Coded from Scratch in Python&lt;/p&gt; &lt;p&gt;(14) Integer and Binary Positional Encodings&lt;/p&gt; &lt;p&gt;(15) All about Sinusoidal Positional Encodings&lt;/p&gt; &lt;p&gt;(16) Rotary Positional Encodings&lt;/p&gt; &lt;p&gt;(17) How DeepSeek exactly implemented Latent Attention | MLA + RoPE&lt;/p&gt; &lt;p&gt;(18) Mixture of Experts (MoE) Introduction&lt;/p&gt; &lt;p&gt;(19) Mixture of Experts Hands on Demonstration&lt;/p&gt; &lt;p&gt;(20) Mixture of Experts Balancing Techniques&lt;/p&gt; &lt;p&gt;(21) How DeepSeek rewrote Mixture of Experts (MoE)?&lt;/p&gt; &lt;p&gt;(22) Code Mixture of Experts (MoE) from Scratch in Python&lt;/p&gt; &lt;p&gt;(23) Multi-Token Prediction Introduction&lt;/p&gt; &lt;p&gt;(24) How DeepSeek rewrote Multi-Token Prediction&lt;/p&gt; &lt;p&gt;(25) Multi-Token Prediction coded from scratch&lt;/p&gt; &lt;p&gt;(26) Introduction to LLM Quantization&lt;/p&gt; &lt;p&gt;(27) How DeepSeek rewrote Quantization Part 1&lt;/p&gt; &lt;p&gt;(28) How DeepSeek rewrote Quantization Part 2&lt;/p&gt; &lt;p&gt;(29) Build DeepSeek from Scratch 20 minute summary&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcrt1k/just_finished_recording_29_videos_on_how_to_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcrt1k/just_finished_recording_29_videos_on_how_to_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcrt1k/just_finished_recording_29_videos_on_how_to_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T12:43:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcn0vz</id>
    <title>Qwen releases official MLX quants for Qwen3 models in 4 quantization levels: 4bit, 6bit, 8bit, and BF16</title>
    <updated>2025-06-16T07:54:58+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcn0vz/qwen_releases_official_mlx_quants_for_qwen3/"&gt; &lt;img alt="Qwen releases official MLX quants for Qwen3 models in 4 quantization levels: 4bit, 6bit, 8bit, and BF16" src="https://preview.redd.it/5jpskt9dw87f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3979f7c8b5f11e8d9ae4fd59f4defeeebd8adae2" title="Qwen releases official MLX quants for Qwen3 models in 4 quantization levels: 4bit, 6bit, 8bit, and BF16" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ Excited to launch Qwen3 models in MLX format today!&lt;/p&gt; &lt;p&gt;Now available in 4 quantization levels: 4bit, 6bit, 8bit, and BF16 ‚Äî Optimized for MLX framework. &lt;/p&gt; &lt;p&gt;üëâ Try it now!&lt;/p&gt; &lt;p&gt;X post: &lt;a href="https://x.com/alibaba_qwen/status/1934517774635991412?s=46"&gt;https://x.com/alibaba_qwen/status/1934517774635991412?s=46&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f"&gt;https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5jpskt9dw87f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcn0vz/qwen_releases_official_mlx_quants_for_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcn0vz/qwen_releases_official_mlx_quants_for_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T07:54:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcw50r</id>
    <title>Kimi-Dev-72B</title>
    <updated>2025-06-16T15:40:31+00:00</updated>
    <author>
      <name>/u/realJoeTrump</name>
      <uri>https://old.reddit.com/user/realJoeTrump</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcw50r/kimidev72b/"&gt; &lt;img alt="Kimi-Dev-72B" src="https://external-preview.redd.it/1kvJDTWOvntivVoW834gDLI4V0P6WaqmrGfz5xyEWNU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b09e977edec166ad9c212551ee72f79018be5fa2" title="Kimi-Dev-72B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realJoeTrump"&gt; /u/realJoeTrump &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Dev-72B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcw50r/kimidev72b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcw50r/kimidev72b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T15:40:31+00:00</published>
  </entry>
</feed>
