<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-21T19:48:51+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i64up9</id>
    <title>Model comparision in Advent of Code 2024</title>
    <updated>2025-01-20T23:45:32+00:00</updated>
    <author>
      <name>/u/Gusanidas</name>
      <uri>https://old.reddit.com/user/Gusanidas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i64up9/model_comparision_in_advent_of_code_2024/"&gt; &lt;img alt="Model comparision in Advent of Code 2024" src="https://b.thumbs.redditmedia.com/sRQFbDFvSKbKZCu2dEGlXMPg9D_wJv2kXAjDQpose_U.jpg" title="Model comparision in Advent of Code 2024" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gusanidas"&gt; /u/Gusanidas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i64up9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i64up9/model_comparision_in_advent_of_code_2024/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i64up9/model_comparision_in_advent_of_code_2024/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T23:45:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6k2s1</id>
    <title>New LLaMA model on lmarena?</title>
    <updated>2025-01-21T14:42:48+00:00</updated>
    <author>
      <name>/u/heyhellousername</name>
      <uri>https://old.reddit.com/user/heyhellousername</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hit a model called &amp;quot;experimental-router-0112&amp;quot; on lmarena and asked &amp;quot;Who made you and what is your model name&amp;quot; 3 times. Every time, it told me it is a model made by Meta based on LLaMA. 2 of the 3 times, it took quite a long time to answer (~12 seconds) and the other time it almost immediately answered which considering the name leads me to speculate it is a router picking between a very large or a reasoning model and a smaller model. I saw some people on reddit say it reasons well and may be o3-mini. What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/heyhellousername"&gt; /u/heyhellousername &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6k2s1/new_llama_model_on_lmarena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6k2s1/new_llama_model_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6k2s1/new_llama_model_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T14:42:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6qar0</id>
    <title>kluster.ai now hosts deepseek R1</title>
    <updated>2025-01-21T19:02:49+00:00</updated>
    <author>
      <name>/u/swarmster</name>
      <uri>https://old.reddit.com/user/swarmster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw on their Twitter account and tried it out on their platform. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swarmster"&gt; /u/swarmster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6qar0/klusterai_now_hosts_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6qar0/klusterai_now_hosts_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6qar0/klusterai_now_hosts_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T19:02:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6jdlp</id>
    <title>I'm finding code produced by R1 to contain less fluff than Sonnet</title>
    <updated>2025-01-21T14:09:23+00:00</updated>
    <author>
      <name>/u/EmbarrassedBiscotti9</name>
      <uri>https://old.reddit.com/user/EmbarrassedBiscotti9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've only done a little testing so far, but this has been true throughout. This is only a comment on the style of code rather than the quality/functionality.&lt;/p&gt; &lt;p&gt;The absence of comments for every little thing is noticeable. With Sonnet, it is like it includes a full beginning-to-end guide for the code it writes via comments. R1 doesn't seem to do this at all - using comments only for occasional clarity or to denote sections.&lt;/p&gt; &lt;p&gt;Overall, it seems to be less verbose. That feels like a small difference which is actually a pretty big win, since it means feeding it back into R1 uses fewer tokens.&lt;/p&gt; &lt;p&gt;This has been a long-standing gripe of mine with Sonnet, and pretty much every model since 3.5. Less fluff is nice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmbarrassedBiscotti9"&gt; /u/EmbarrassedBiscotti9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6jdlp/im_finding_code_produced_by_r1_to_contain_less/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6jdlp/im_finding_code_produced_by_r1_to_contain_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6jdlp/im_finding_code_produced_by_r1_to_contain_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T14:09:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i69dhz</id>
    <title>Deepseek R1 (Ollama) Hardware benchmark for LocalLLM</title>
    <updated>2025-01-21T03:26:07+00:00</updated>
    <author>
      <name>/u/Joehua87</name>
      <uri>https://old.reddit.com/user/Joehua87</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i69dhz/deepseek_r1_ollama_hardware_benchmark_for_localllm/"&gt; &lt;img alt="Deepseek R1 (Ollama) Hardware benchmark for LocalLLM" src="https://a.thumbs.redditmedia.com/0brRhG_B3bCoFCYWOckQ1pYJx6cjvM78JeFuv79AeL8.jpg" title="Deepseek R1 (Ollama) Hardware benchmark for LocalLLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek R1 was released and looks like one of the best models for local LLM.&lt;/p&gt; &lt;p&gt;I tested it on some GPUs to see how many tps it can achieve.&lt;/p&gt; &lt;p&gt;Tests were run on Ollama.&lt;/p&gt; &lt;p&gt;Input prompt: How to {build a pc|build a website|build xxx}?&lt;/p&gt; &lt;p&gt;Thoughts:&lt;/p&gt; &lt;p&gt;- `deepseek-r1:14b` can run on any GPU without a significant performance gap.&lt;/p&gt; &lt;p&gt;- `deepseek-r1:32b` runs better on a single GPU with ~24GB VRAM: RTX 3090 offers the best price/performance. RTX Titan is acceptable.&lt;/p&gt; &lt;p&gt;- `deepseek-r1:70b` performs best with 2 x RTX 3090 (17tps) in terms of price/performance. However, it doubles the electricity cost compared to RTX 6000 ADA (19tps) or RTX A6000 (12tps).&lt;/p&gt; &lt;p&gt;- `M3 Max 40GPU` has high memory but only delivers 3-7 tps for `deepseek-r1:70b`. It is also loud, and the GPU temperature is high (&amp;gt; 90 C).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8r7cwajfn9ee1.png?width=1014&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06a7b471338980df1ddba053ad765a6259a3fd9e"&gt;https://preview.redd.it/8r7cwajfn9ee1.png?width=1014&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06a7b471338980df1ddba053ad765a6259a3fd9e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yw73dokgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dd429963cc141005dfd36c0c422e0fe016b8fd42"&gt;https://preview.redd.it/yw73dokgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dd429963cc141005dfd36c0c422e0fe016b8fd42&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/91flfnkgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1952823659437ba7e18741bb667c2cb694082d7"&gt;https://preview.redd.it/91flfnkgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1952823659437ba7e18741bb667c2cb694082d7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nver8nkgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ef10eb60e80fd4e531ab1ca96e401db44a10020"&gt;https://preview.redd.it/nver8nkgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ef10eb60e80fd4e531ab1ca96e401db44a10020&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jnfv9okgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=527d2e9bf7f0bb162c7feabf5a2c950a09f81da9"&gt;https://preview.redd.it/jnfv9okgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=527d2e9bf7f0bb162c7feabf5a2c950a09f81da9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3fu1mpkgn9ee1.png?width=560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b2c144f1fa57cd6574858d456e41ee790fe8b89c"&gt;https://preview.redd.it/3fu1mpkgn9ee1.png?width=560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b2c144f1fa57cd6574858d456e41ee790fe8b89c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rc7tnpkgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67d89c86c533e833a2b0872990c54a1429793109"&gt;https://preview.redd.it/rc7tnpkgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67d89c86c533e833a2b0872990c54a1429793109&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/03gezokgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1871405ec5a0cb64c6b5ae6505f18d3314f54ec9"&gt;https://preview.redd.it/03gezokgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1871405ec5a0cb64c6b5ae6505f18d3314f54ec9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ouilsqkgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d2dc1b04806a10fa99e55fa4fcf09d1a489d8d0"&gt;https://preview.redd.it/ouilsqkgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d2dc1b04806a10fa99e55fa4fcf09d1a489d8d0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Joehua87"&gt; /u/Joehua87 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i69dhz/deepseek_r1_ollama_hardware_benchmark_for_localllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i69dhz/deepseek_r1_ollama_hardware_benchmark_for_localllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i69dhz/deepseek_r1_ollama_hardware_benchmark_for_localllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T03:26:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i65c2g</id>
    <title>A new TTS model but it's llama in disguise</title>
    <updated>2025-01-21T00:07:23+00:00</updated>
    <author>
      <name>/u/Eastwindy123</name>
      <uri>https://old.reddit.com/user/Eastwindy123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i65c2g/a_new_tts_model_but_its_llama_in_disguise/"&gt; &lt;img alt="A new TTS model but it's llama in disguise" src="https://external-preview.redd.it/YTF3ZDhodHVuOGVlMfWwWiuiXWd3G-eDkJvYJT1msjq8KPmaEpaXQEEuQ3ap.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62d195d1c99e91b40a229bfcd76149483328400c" title="A new TTS model but it's llama in disguise" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I stumbled across an amazing model that some researchers released before they released their paper. An open source llama3 3B finetune/continued pretraining that acts as a text to speech model. Not only does it do incredibly realistic text to speech, it can also clone any voice with only a couple seconds of sample audio.&lt;/p&gt; &lt;p&gt;I wrote a blog about it on huggingface and created a ZERO space for people to try it out. &lt;/p&gt; &lt;p&gt;blog: &lt;a href="https://huggingface.co/blog/srinivasbilla/llasa-tts"&gt;https://huggingface.co/blog/srinivasbilla/llasa-tts&lt;/a&gt; space : &lt;a href="https://huggingface.co/spaces/srinivasbilla/llasa-3b-tts"&gt;https://huggingface.co/spaces/srinivasbilla/llasa-3b-tts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eastwindy123"&gt; /u/Eastwindy123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/deqxwvwun8ee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i65c2g/a_new_tts_model_but_its_llama_in_disguise/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i65c2g/a_new_tts_model_but_its_llama_in_disguise/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T00:07:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6b65q</id>
    <title>Better R1 Experience in open webui</title>
    <updated>2025-01-21T05:05:31+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6b65q/better_r1_experience_in_open_webui/"&gt; &lt;img alt="Better R1 Experience in open webui" src="https://external-preview.redd.it/YSLEhbNfDODEMmwhWhGpt4WFY87CROwBvvlWm6wYG0k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76fb2b05f6cb5884488366b666d5cd74022016bf" title="Better R1 Experience in open webui" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just created a simple open webui function for R1 models, it can do the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Replace the simple &amp;lt;think&amp;gt; tags with &amp;lt;details&amp;gt;&amp;amp; &amp;lt;summary&amp;gt; tags, which makes R1's thoughts collapsible.&lt;/li&gt; &lt;li&gt;Remove R1's old thoughts in multi-turn conversation, according to deepseeks API docs you should always remove R1's previous thoughts in a multi-turn conversation.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Github:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/AaronFeng753/Better-R1"&gt;https://github.com/AaronFeng753/Better-R1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note: This function is only designed for those who run R1 (-distilled) models locally. It does not work with the DeepSeek API.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/ynq9hqjg8aee1.gif"&gt;https://i.redd.it/ynq9hqjg8aee1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6b65q/better_r1_experience_in_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6b65q/better_r1_experience_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6b65q/better_r1_experience_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T05:05:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5s5hk</id>
    <title>OpenAI sweating bullets rn</title>
    <updated>2025-01-20T15:05:09+00:00</updated>
    <author>
      <name>/u/ThroughForests</name>
      <uri>https://old.reddit.com/user/ThroughForests</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s5hk/openai_sweating_bullets_rn/"&gt; &lt;img alt="OpenAI sweating bullets rn" src="https://preview.redd.it/b2fm3y9uy5ee1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3a6e07ad1ccbf40dcef766d2a3fa367543a642e" title="OpenAI sweating bullets rn" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThroughForests"&gt; /u/ThroughForests &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b2fm3y9uy5ee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s5hk/openai_sweating_bullets_rn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s5hk/openai_sweating_bullets_rn/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T15:05:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6on4k</id>
    <title>Local Llasa TTS (followup)</title>
    <updated>2025-01-21T17:56:33+00:00</updated>
    <author>
      <name>/u/Eastwindy123</name>
      <uri>https://old.reddit.com/user/Eastwindy123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6on4k/local_llasa_tts_followup/"&gt; &lt;img alt="Local Llasa TTS (followup)" src="https://external-preview.redd.it/1TMC5wCp9v4FuG7z72DixwLUgFNuIDXDGk55piOPdlc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2941de9d4a500f20469258b9e1920c8999cbd5b2" title="Local Llasa TTS (followup)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, lots of people asked about using the llasa TTS model locally. So I made a quick repo with some examples on how to run it in colab and locally with native hf transformers. It takes about 8.5gb of vram with whisper large turbo. And 6.5gb without. Runs fine on colab though&lt;/p&gt; &lt;p&gt;I'm not too sure how to run it with llama cpp/ollama since it requires the xcodec2 model and also very specific prompt templating. If someone knows feel free to pr.&lt;/p&gt; &lt;p&gt;See my first post for context &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i65c2g/a_new_tts_model_but_its_llama_in_disguise/?utm_source=share&amp;amp;utm_medium=mweb3x&amp;amp;utm_name=mweb3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1i65c2g/a_new_tts_model_but_its_llama_in_disguise/?utm_source=share&amp;amp;utm_medium=mweb3x&amp;amp;utm_name=mweb3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eastwindy123"&gt; /u/Eastwindy123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/nivibilla/local-llasa-tts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6on4k/local_llasa_tts_followup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6on4k/local_llasa_tts_followup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T17:56:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i66j4f</id>
    <title>DeepSeek-R1 Training Pipeline Visualized</title>
    <updated>2025-01-21T01:02:38+00:00</updated>
    <author>
      <name>/u/incarnadine72</name>
      <uri>https://old.reddit.com/user/incarnadine72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i66j4f/deepseekr1_training_pipeline_visualized/"&gt; &lt;img alt="DeepSeek-R1 Training Pipeline Visualized" src="https://preview.redd.it/jf6vo05hx8ee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07742a4a4aced788c72a6c14554e543cd85ea73d" title="DeepSeek-R1 Training Pipeline Visualized" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/incarnadine72"&gt; /u/incarnadine72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jf6vo05hx8ee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i66j4f/deepseekr1_training_pipeline_visualized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i66j4f/deepseekr1_training_pipeline_visualized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T01:02:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6note</id>
    <title>Testing the new Deepseek models on my Cybersecurity test</title>
    <updated>2025-01-21T17:17:39+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ran the new Deepseek models and distils through my multiple choice cyber security test:&lt;br /&gt; A good score requires heavy world knowledge and some reasoning.&lt;/p&gt; &lt;p&gt;1st - 01-preview - 95.72%&lt;br /&gt; &lt;strong&gt;2nd - Deepseek-R1-API - 94.06%&lt;/strong&gt;&lt;br /&gt; *** - Meta-Llama3.1-405b-FP8 - 94.06% (Modified dual prompt to allow CoT)&lt;br /&gt; 3rd - Claude-3.5-October - 92.92%&lt;br /&gt; 4th - O1-mini - 92.87%&lt;br /&gt; 5th - Meta-Llama3.1-405b-FP8 - 92.64%&lt;br /&gt; &lt;strong&gt;*** - Deepseek-v3-api - 92.64% (Modified dual prompt to allow CoT)&lt;/strong&gt;&lt;br /&gt; 6th - GPT-4o - 92.45%&lt;br /&gt; 7th - Mistral-Large-123b-2411-FP16 92.40%&lt;br /&gt; &lt;strong&gt;8th - Deepseek-v3-api - 91.92%&lt;/strong&gt;&lt;br /&gt; 9th - GPT-4o-mini - 91.75%&lt;br /&gt; *** - Sky-T1-32B-BF16 - 91.45&lt;br /&gt; *** - Qwen-QwQ-32b-AWQ - 90.74% (Modified dual prompt to allow CoT)&lt;br /&gt; 10th - DeepSeek-v2.5-1210-BF16 - 90.50%&lt;br /&gt; 11th - Meta-LLama3.3-70b-FP8 - 90.26%&lt;br /&gt; 11th - Qwen-2.5-72b-FP8 - 90.09%&lt;br /&gt; 13th - Meta-Llama3.1-70b-FP8 - 89.15%&lt;br /&gt; &lt;strong&gt;14th - DeepSeek-R1-Distill-Qwen-32B-FP16 - 89.31%&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;15th - DeepSeek-R1-Distill-Llama-70B-GGUF-Q5 - 89.07%&lt;/strong&gt;&lt;br /&gt; 16th - Phi-4-GGUF-Fixed-Q4 - 88.6%&lt;br /&gt; 16th - Hunyuan-Large-389b-FP8 - 88.60%&lt;br /&gt; &lt;strong&gt;18th - DeepSeek-R1-Distill-Qwen-32B-GGUF - 87.65%&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Fun fact not seen in the scores above, cost to run my ~420 question test&lt;br /&gt; DeepSeek V3 without COT: 3 Cents&lt;br /&gt; DeepSeek V3 with my COT: 9 Cents&lt;br /&gt; DeepSeek R1: 71 Cents&lt;br /&gt; O1 Mini: 196 Cents&lt;br /&gt; O1 Preview: 1600 Cents&lt;/p&gt; &lt;p&gt;Typically a score on my test drops by 0.5% or less going from full precision to Q4,&lt;br /&gt; Qwen going down 3% seems suspicious, wonder if there are GGUF issues?&lt;br /&gt; Llama distil scoring below llama 3.1 and 3.3 is also a little odd.&lt;br /&gt; I was running unsloth GGUF's in VLLM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6note/testing_the_new_deepseek_models_on_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6note/testing_the_new_deepseek_models_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6note/testing_the_new_deepseek_models_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T17:17:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i62a0k</id>
    <title>Personal experience with Deepseek R1: it is noticeably better than claude sonnet 3.5</title>
    <updated>2025-01-20T21:55:14+00:00</updated>
    <author>
      <name>/u/sebastianmicu24</name>
      <uri>https://old.reddit.com/user/sebastianmicu24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My usecases are mainly python and R for biological data analysis, as well as a little Frontend to build some interface for my colleagues. Where deepseek V3 was failing and claude sonnet needed 4-5 prompts, R1 creates instantly whatever file I need with one prompt. I only had one case where it did not succed with one prompt, but then accidentally solved the bug when asking him to add some logs for debugging lol. It is faster and just as reliable to ask him to build me a specific python code for a one time operation than wait for excel to open my 300 Mb csv. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sebastianmicu24"&gt; /u/sebastianmicu24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i62a0k/personal_experience_with_deepseek_r1_it_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i62a0k/personal_experience_with_deepseek_r1_it_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i62a0k/personal_experience_with_deepseek_r1_it_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T21:55:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6n87h</id>
    <title>DeepSeek-R1 PlanBench benchmark results</title>
    <updated>2025-01-21T16:58:38+00:00</updated>
    <author>
      <name>/u/Wiskkey</name>
      <uri>https://old.reddit.com/user/Wiskkey</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6n87h/deepseekr1_planbench_benchmark_results/"&gt; &lt;img alt="DeepSeek-R1 PlanBench benchmark results" src="https://preview.redd.it/qa5yh1w3odee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4bfe107587dff4e6f5da9478af9b4fd49c54d16" title="DeepSeek-R1 PlanBench benchmark results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wiskkey"&gt; /u/Wiskkey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qa5yh1w3odee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6n87h/deepseekr1_planbench_benchmark_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6n87h/deepseekr1_planbench_benchmark_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T16:58:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6ku6i</id>
    <title>You can now use both R1 and Search Web (see the comparison with and without R1)</title>
    <updated>2025-01-21T15:16:32+00:00</updated>
    <author>
      <name>/u/mw11n19</name>
      <uri>https://old.reddit.com/user/mw11n19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6ku6i/you_can_now_use_both_r1_and_search_web_see_the/"&gt; &lt;img alt="You can now use both R1 and Search Web (see the comparison with and without R1)" src="https://b.thumbs.redditmedia.com/1p8-1CMzNGqsdKfM6gDHFVIvZPeHyEW7IhtpMIuEe9A.jpg" title="You can now use both R1 and Search Web (see the comparison with and without R1)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mw11n19"&gt; /u/mw11n19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i6ku6i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6ku6i/you_can_now_use_both_r1_and_search_web_see_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6ku6i/you_can_now_use_both_r1_and_search_web_see_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T15:16:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1i615u1</id>
    <title>The first time I've felt a LLM wrote *well*, not just well *for a LLM*.</title>
    <updated>2025-01-20T21:09:18+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i615u1/the_first_time_ive_felt_a_llm_wrote_well_not_just/"&gt; &lt;img alt="The first time I've felt a LLM wrote *well*, not just well *for a LLM*." src="https://preview.redd.it/48kw0dyao7ee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85f94bde55ce83180ff26c640d9632cd2e976d23" title="The first time I've felt a LLM wrote *well*, not just well *for a LLM*." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/48kw0dyao7ee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i615u1/the_first_time_ive_felt_a_llm_wrote_well_not_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i615u1/the_first_time_ive_felt_a_llm_wrote_well_not_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T21:09:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6dlvj</id>
    <title>Inside DeepSeekâ€™s Bold Mission (CEO Liang Wenfeng Interview)</title>
    <updated>2025-01-21T07:49:33+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After yesterdayâ€™s release of DeepSeek R1 reasoning model, which has sent ripples through the LLM community, I revisited a fascinating series of interviews with their CEO Liang Wenfeng from May 2023 and July 2024. &lt;/p&gt; &lt;p&gt;&lt;a href="https://drive.google.com/file/d/1gLw9jpp61ybainydNa2kXpNs0PLiICn5/view"&gt;May 2023&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://drive.google.com/file/d/1DW5ohZWxoCEOdrUQjokKreuArHqJdtKb/view"&gt;July 2024&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Key takeaways from the interviews with DeepSeek's founder Liang Wenfeng:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Innovation-First Approach&lt;/strong&gt;: Unlike other Chinese AI companies focused on rapid commercialization, DeepSeek exclusively focuses on fundamental AGI research and innovation. They believe China must transition from being a &amp;quot;free rider&amp;quot; to a &amp;quot;contributor&amp;quot; in global AI development. Liang emphasizes that true innovation comes not just from commercial incentives, but from curiosity and the desire to create.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Revolutionary Architecture&lt;/strong&gt;: DeepSeek V2's MLA (Multi-head Latent Attention) architecture reduces memory usage to 5-13% of conventional MHA, leading to significantly lower costs. Their inference costs are about 1/7th of Llama3 70B and 1/70th of GPT-4 Turbo. This wasn't meant to start a price war - they simply priced based on actual costs plus modest margins.(This innovative architecture has been carried forward into their V3 and R1 models.)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Unique Cultural Philosophy and Talent Strategy&lt;/strong&gt;: DeepSeek maintains a completely bottom-up organizational structure, giving unlimited computing resources to researchers and prioritizing passion over credentials. Their breakthrough innovations come from young local talent - recent graduates and young professionals from Chinese universities, rather than overseas recruitment. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Commitment to Open Source&lt;/strong&gt;: Despite industry trends toward closed-source models (like OpenAI and Mistral), DeepSeek remains committed to open-source, viewing it as crucial for building a strong technological ecosystem. Liang believes that in the face of disruptive technology, a closed-source moat is temporary - their real value lies in consistently building an organization that can innovate.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;The Challenge of Compute Access&lt;/strong&gt;: Despite having sufficient funding and technological capability, DeepSeek faces its biggest challenge from U.S. chip export restrictions. The company doesn't have immediate fundraising plans, as Liang notes their primary constraint isn't capital but access to high-end chips, which are crucial for training advanced AI models.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Looking at their recent release, it seems they're really delivering on these promises. The interview from July 2024 shows their commitment to pushing technological boundaries while keeping everything open source, and their recent achievements suggest they're successfully executing on this vision.&lt;/p&gt; &lt;p&gt;What do you think about their approach of focusing purely on research and open-source development? Could this &amp;quot;DeepSeek way&amp;quot; become a viable alternative to the increasingly closed-source trend we're seeing in AI development?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6dlvj/inside_deepseeks_bold_mission_ceo_liang_wenfeng/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6dlvj/inside_deepseeks_bold_mission_ceo_liang_wenfeng/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6dlvj/inside_deepseeks_bold_mission_ceo_liang_wenfeng/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T07:49:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6fxxy</id>
    <title>Literally unusable</title>
    <updated>2025-01-21T10:47:42+00:00</updated>
    <author>
      <name>/u/WarlaxZ</name>
      <uri>https://old.reddit.com/user/WarlaxZ</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6fxxy/literally_unusable/"&gt; &lt;img alt="Literally unusable" src="https://preview.redd.it/iatgsah1ubee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24ef211ed58771aa9136fe74e30af422f415bc31" title="Literally unusable" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WarlaxZ"&gt; /u/WarlaxZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iatgsah1ubee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6fxxy/literally_unusable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6fxxy/literally_unusable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T10:47:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6kwy7</id>
    <title>just tell it to be logical</title>
    <updated>2025-01-21T15:19:52+00:00</updated>
    <author>
      <name>/u/spirobel</name>
      <uri>https://old.reddit.com/user/spirobel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6kwy7/just_tell_it_to_be_logical/"&gt; &lt;img alt="just tell it to be logical" src="https://preview.redd.it/j6ax54qh6dee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f2541b2247a45ef3e36854545cd09c866802e79" title="just tell it to be logical" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spirobel"&gt; /u/spirobel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j6ax54qh6dee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6kwy7/just_tell_it_to_be_logical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6kwy7/just_tell_it_to_be_logical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T15:19:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6pra7</id>
    <title>Spanish government releases some official models</title>
    <updated>2025-01-21T18:40:57+00:00</updated>
    <author>
      <name>/u/xdoso</name>
      <uri>https://old.reddit.com/user/xdoso</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spanish government has fund the training of official and public LLMs, mainly trained on Spanish and co-official spanish languages.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Main page&lt;/strong&gt;: &lt;a href="https://alia.gob.es/"&gt;https://alia.gob.es/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Huggingface models&lt;/strong&gt;: &lt;a href="https://huggingface.co/BSC-LT"&gt;https://huggingface.co/BSC-LT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The main released models are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Alia 40b&lt;/strong&gt; (base model still on training, published intermediate result; instruct version will be released in the future)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Salamandra 2b/7b&lt;/strong&gt; (base and instruct available)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The main model has been trained using the Spanish Marenostrum 5 with a total of 2048 GPUs (H100 64Gb). They are all Apache 2.0 license and most datasets have been published also. They are mainly trained on European languages.&lt;/p&gt; &lt;p&gt;Also some translation models have been published:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Salamandra TA 2b:&lt;/strong&gt; translation between 30 main European languages directly&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plume 256k, 128, 32k&lt;/strong&gt;: finetuning of gemma2 models for translation between Spanish languages.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Aina models&lt;/strong&gt;: a list of 1to1 models for translation between Spanish languages.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Alia 40b is the latest release and the most important one, although for the moments the results that we are seeing during the tests are pretty bad. &lt;/p&gt; &lt;p&gt;Posts about the results: &lt;/p&gt; &lt;p&gt;- &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i6qecq/spanish_alia_model_has_been_trained_with_porn_and/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1i6qecq/spanish_alia_model_has_been_trained_with_porn_and/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xdoso"&gt; /u/xdoso &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6pra7/spanish_government_releases_some_official_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6pra7/spanish_government_releases_some_official_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6pra7/spanish_government_releases_some_official_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T18:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6e2ni</id>
    <title>Trump Revokes Biden Executive Order on Addressing AI Risks</title>
    <updated>2025-01-21T08:24:46+00:00</updated>
    <author>
      <name>/u/logicchains</name>
      <uri>https://old.reddit.com/user/logicchains</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6e2ni/trump_revokes_biden_executive_order_on_addressing/"&gt; &lt;img alt="Trump Revokes Biden Executive Order on Addressing AI Risks" src="https://external-preview.redd.it/kafi4nTKNA_q_kd_b8L_HlG2-8aXgOzhra9KW3niFio.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c38fa66355083c8c8e1692586840218d34f2bf17" title="Trump Revokes Biden Executive Order on Addressing AI Risks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/logicchains"&gt; /u/logicchains &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.usnews.com/news/top-news/articles/2025-01-20/trump-revokes-biden-executive-order-on-addressing-ai-risks"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6e2ni/trump_revokes_biden_executive_order_on_addressing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6e2ni/trump_revokes_biden_executive_order_on_addressing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T08:24:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6mjxv</id>
    <title>Deploy any LLM on Huggingface at 3-10x Speed</title>
    <updated>2025-01-21T16:30:20+00:00</updated>
    <author>
      <name>/u/avianio</name>
      <uri>https://old.reddit.com/user/avianio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6mjxv/deploy_any_llm_on_huggingface_at_310x_speed/"&gt; &lt;img alt="Deploy any LLM on Huggingface at 3-10x Speed" src="https://preview.redd.it/8dsnudtrhdee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4bf725a99b4877d47c42a9a0a7d813a8339eb266" title="Deploy any LLM on Huggingface at 3-10x Speed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avianio"&gt; /u/avianio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8dsnudtrhdee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6mjxv/deploy_any_llm_on_huggingface_at_310x_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6mjxv/deploy_any_llm_on_huggingface_at_310x_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T16:30:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6n7jf</id>
    <title>Pretty sure OpenAI has their devs working 24/7 to not lose their throne to DeepSeek ðŸ˜­</title>
    <updated>2025-01-21T16:57:48+00:00</updated>
    <author>
      <name>/u/Condomphobic</name>
      <uri>https://old.reddit.com/user/Condomphobic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And DeepSeek is making the same progress at a much faster pace than OpenAI is. They are definitely in a rock situation &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Condomphobic"&gt; /u/Condomphobic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6n7jf/pretty_sure_openai_has_their_devs_working_247_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6n7jf/pretty_sure_openai_has_their_devs_working_247_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6n7jf/pretty_sure_openai_has_their_devs_working_247_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T16:57:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6l3ms</id>
    <title>Thanks to DeepSeek other open model releases with "research" license will be laughable</title>
    <updated>2025-01-21T15:28:03+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Imagine labs like Mistral, Cohere (do you remember them?) release open-weight model with so called research purposes only license. Comedy Central would call them for movie rights ;) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6l3ms/thanks_to_deepseek_other_open_model_releases_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6l3ms/thanks_to_deepseek_other_open_model_releases_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6l3ms/thanks_to_deepseek_other_open_model_releases_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T15:28:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6jbur</id>
    <title>DeepSeek R1 (Qwen 32B Distill) is now available for free on HuggingChat!</title>
    <updated>2025-01-21T14:07:01+00:00</updated>
    <author>
      <name>/u/SensitiveCranberry</name>
      <uri>https://old.reddit.com/user/SensitiveCranberry</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6jbur/deepseek_r1_qwen_32b_distill_is_now_available_for/"&gt; &lt;img alt="DeepSeek R1 (Qwen 32B Distill) is now available for free on HuggingChat!" src="https://external-preview.redd.it/8HwSiZPd8K_hder46_kXYrWF23xE0qYwa1myzoXXUfM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32931922738bce5caa4226b968a442514cf96587" title="DeepSeek R1 (Qwen 32B Distill) is now available for free on HuggingChat!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SensitiveCranberry"&gt; /u/SensitiveCranberry &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hf.co/chat/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6jbur/deepseek_r1_qwen_32b_distill_is_now_available_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6jbur/deepseek_r1_qwen_32b_distill_is_now_available_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T14:07:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6lsgo</id>
    <title>From llama2 --&gt; DeepSeek R1 things have gone a long way in a 1 year</title>
    <updated>2025-01-21T15:58:08+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was blown away by llama2 70b when it came out. I felt so empowered having so much knowledge spun up locally on my M3 Max. &lt;/p&gt; &lt;p&gt;Just over a year, and DeepSeek R1 makes Llama 2 seem like a little child. It's crazy how good the outputs are, and how fast it spits out tokens in just 40GB.&lt;/p&gt; &lt;p&gt;Can't imagine where things will be in another year.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6lsgo/from_llama2_deepseek_r1_things_have_gone_a_long/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6lsgo/from_llama2_deepseek_r1_things_have_gone_a_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6lsgo/from_llama2_deepseek_r1_things_have_gone_a_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T15:58:08+00:00</published>
  </entry>
</feed>
