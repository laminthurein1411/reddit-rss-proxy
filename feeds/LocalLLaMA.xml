<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-23T06:37:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ivwnkq</id>
    <title>Looking for recommended tools and guides to create local ai agents and workflows</title>
    <updated>2025-02-22T23:51:55+00:00</updated>
    <author>
      <name>/u/crispyfrybits</name>
      <uri>https://old.reddit.com/user/crispyfrybits</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to create a simple reasoning workflow that can do some business research and financial analysis for me. I have found regular llm chats have been very useful so far and I want to take it to the next level.&lt;/p&gt; &lt;p&gt;I don't have a tonne of LLM experience but I am a developer by day. That said I don't want to make a big project out of it, I would prefer a simpler solution/approach so I don't procrastinate by creating new projects. &lt;/p&gt; &lt;p&gt;Any help and places to start would be greatly appreciateed!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crispyfrybits"&gt; /u/crispyfrybits &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivwnkq/looking_for_recommended_tools_and_guides_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivwnkq/looking_for_recommended_tools_and_guides_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivwnkq/looking_for_recommended_tools_and_guides_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T23:51:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivn6pj</id>
    <title>Google AI Studio Free - What's the daily limits?</title>
    <updated>2025-02-22T16:55:01+00:00</updated>
    <author>
      <name>/u/Sostrene_Blue</name>
      <uri>https://old.reddit.com/user/Sostrene_Blue</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I'm new to Google AI Studio Free and I'm really impressed so far. &lt;/p&gt; &lt;p&gt;It seems like I can use it daily without any restrictions. Is that actually true? Are there any limitations I should be aware of? &lt;/p&gt; &lt;p&gt;Also, does the &amp;quot;grounding with Google Search&amp;quot; feature also have unlimited usage? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sostrene_Blue"&gt; /u/Sostrene_Blue &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivn6pj/google_ai_studio_free_whats_the_daily_limits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivn6pj/google_ai_studio_free_whats_the_daily_limits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivn6pj/google_ai_studio_free_whats_the_daily_limits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T16:55:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivo0gv</id>
    <title>DarkRapids, Local GPU rig build with style (water cooling)</title>
    <updated>2025-02-22T17:28:50+00:00</updated>
    <author>
      <name>/u/berni8k</name>
      <uri>https://old.reddit.com/user/berni8k</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivo0gv/darkrapids_local_gpu_rig_build_with_style_water/"&gt; &lt;img alt="DarkRapids, Local GPU rig build with style (water cooling)" src="https://b.thumbs.redditmedia.com/k7wIbfmhIPkjHny_WwxeIVUQKpigJzDbZhExxg-v5To.jpg" title="DarkRapids, Local GPU rig build with style (water cooling)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It started off as a PC build but then progressed into a local GPU rig.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9z2qwggz1qke1.jpg?width=3468&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e0c9836a1cbacbc4a2d3ee492ca1cdebe88ff89e"&gt;https://preview.redd.it/9z2qwggz1qke1.jpg?width=3468&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e0c9836a1cbacbc4a2d3ee492ca1cdebe88ff89e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Its main feature are its quad RTX 3090 that were collected overtime off the used market in various states of disrepair. All of them upgraded to watercoolng with AlphaCool waterblocks (that went for cheap because they are clearing stock). Most of these GPUs are also the higher power 420W TDP third party variants making this a lot of heat to deal with.&lt;/p&gt; &lt;p&gt;The 3 radiators packed into the case are all sitting on exhaust vents to keep the interior cool. This is important because the water temperature is ran very high at ~55°C making for rather hot exhaust air. This is the only way this amount of radiators can deal with dissipating the ~1700W of heat and still have fans run at reasonable speeds so that it doesn't sound like a server taking off.&lt;/p&gt; &lt;p&gt;CPU Cooling is done using a separate AIO on the front intake. This is because the TIM under the heat spreader of AMD Threadripper CPUs is still not very good, so even such a low core count chip can't deal with high ambient temperature, so the water that is cooling the GPUs is too hot. For this reason the CPU radiator is taking in fresh cold air and doing it at a rate that even the radiator exhaust is barely warm. This air can then still cool the GPU radiators.&lt;/p&gt; &lt;p&gt;For running LLM inference many of you on this reddit will know that you do not hit very high power usage per card. So running LLMs this rig can stay pretty quiet, even long prompt processing wont bother it since there is a lot of thermal mass to heat up. However other things like StableDifusion or training will make it pull some serious power and require the fans to ramp up a fair bit.&lt;/p&gt; &lt;p&gt;Total weight of this computer is 32kg ~70lb so i ended up adding a pair of handles to the top in order to make moving it a bit easier.&lt;/p&gt; &lt;p&gt;As for performance. Well it performs like a 4x RTX 3090 rig, plenty of LLM benchmarks for that on this corner of reddit.&lt;/p&gt; &lt;p&gt;Specs:&lt;br /&gt; - AMD Threadripper Pro 3945WX 12 core&lt;br /&gt; - ASRock WRX80 Creator R2.0 board&lt;br /&gt; - 256GB of DDR4 3200 (8 sticks)&lt;br /&gt; - 4x GPUs: RTX 3090 (all on PCIe 16x 4.0)&lt;br /&gt; - 2x PSUs: 1500W Silverstone SST-ST1500 + 1000W Corsair HX1000&lt;br /&gt; - Thermaltake Core X71 case&lt;br /&gt; - AIO CPU cooler Enermax Liqtech II 240mm&lt;br /&gt; - GPU cooling custom loop with 360mm + 360mm + 240mm radiators&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mhtue5q02qke1.jpg?width=3468&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7661e688704dce04b4d31e659ddc3dd357bce917"&gt;https://preview.redd.it/mhtue5q02qke1.jpg?width=3468&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7661e688704dce04b4d31e659ddc3dd357bce917&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dp0p0qh12qke1.jpg?width=3468&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e66aa50b217c088d9926f95659d5e78d54a4fbbf"&gt;https://preview.redd.it/dp0p0qh12qke1.jpg?width=3468&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e66aa50b217c088d9926f95659d5e78d54a4fbbf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qsls22u14qke1.jpg?width=3468&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2dacfcd4fcfe078a29ffbc93fb81b251da7fc474"&gt;https://preview.redd.it/qsls22u14qke1.jpg?width=3468&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2dacfcd4fcfe078a29ffbc93fb81b251da7fc474&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/berni8k"&gt; /u/berni8k &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivo0gv/darkrapids_local_gpu_rig_build_with_style_water/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivo0gv/darkrapids_local_gpu_rig_build_with_style_water/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivo0gv/darkrapids_local_gpu_rig_build_with_style_water/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T17:28:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw2v7j</id>
    <title>Current SOTA for voice to voice opensource</title>
    <updated>2025-02-23T05:25:09+00:00</updated>
    <author>
      <name>/u/International_Bid950</name>
      <uri>https://old.reddit.com/user/International_Bid950</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the current SOTA for voice-to-voice models to tun locally? Not just large parameter models but also models around ~1B range.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/International_Bid950"&gt; /u/International_Bid950 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw2v7j/current_sota_for_voice_to_voice_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw2v7j/current_sota_for_voice_to_voice_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw2v7j/current_sota_for_voice_to_voice_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T05:25:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw2z5w</id>
    <title>Surprising Performance on CPU-only Ryzen 9 9950x | 64 GB DDR5 Build</title>
    <updated>2025-02-23T05:31:50+00:00</updated>
    <author>
      <name>/u/gmdtrn</name>
      <uri>https://old.reddit.com/user/gmdtrn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While I wait for my GPU to arrive, I decided to give my CPU-only system a run. I just purchased a bundle from Microcenter for a MSI X870E MAG Tomahawk WiFi motherboard, Ryzen 9 9950x CPU (16 cores, 32 threads), and G.Skill Flare X5 DDR5 RAM (though I upgraded to 64 GB). The OS I'm running is PopOS (Ubuntu derivative).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I'm getting ~12 tokens/sec on `deepseek-r1:8b` (which is build on Llama3.1:8b) running off the CPU alone&lt;/strong&gt;. I was quite impressed by this as it's out-performing my RTX 2060 mobile by about 30-35%. Thus, it may make for a solid LLM budget build. So, I wanted to share it here.&lt;/p&gt; &lt;p&gt;I hope some of you find this useful. And, I apologize for not performing a more thorough analysis and presenting it here. However, I am up against the clock on a quiz I must take tomorrow that I need to study for.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gmdtrn"&gt; /u/gmdtrn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw2z5w/surprising_performance_on_cpuonly_ryzen_9_9950x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw2z5w/surprising_performance_on_cpuonly_ryzen_9_9950x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw2z5w/surprising_performance_on_cpuonly_ryzen_9_9950x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T05:31:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivgqhe</id>
    <title>What are your use cases for small (1-3-8B) models?</title>
    <updated>2025-02-22T11:29:03+00:00</updated>
    <author>
      <name>/u/silveroff</name>
      <uri>https://old.reddit.com/user/silveroff</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m curious what you guys doing with tiny 1-3B or little bigger like 8-9B?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silveroff"&gt; /u/silveroff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivgqhe/what_are_your_use_cases_for_small_138b_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivgqhe/what_are_your_use_cases_for_small_138b_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivgqhe/what_are_your_use_cases_for_small_138b_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T11:29:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivfddq</id>
    <title>Is it worth spending so much time and money on small LLMs?</title>
    <updated>2025-02-22T09:52:13+00:00</updated>
    <author>
      <name>/u/ML-Future</name>
      <uri>https://old.reddit.com/user/ML-Future</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivfddq/is_it_worth_spending_so_much_time_and_money_on/"&gt; &lt;img alt="Is it worth spending so much time and money on small LLMs?" src="https://preview.redd.it/n9rafd86xnke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1046f7f56c2b33ae73d2249b62632828de46d3b5" title="Is it worth spending so much time and money on small LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ML-Future"&gt; /u/ML-Future &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n9rafd86xnke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivfddq/is_it_worth_spending_so_much_time_and_money_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivfddq/is_it_worth_spending_so_much_time_and_money_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T09:52:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivkpdo</id>
    <title>Wayfarer Large is (surprisingly) great + Example Chats</title>
    <updated>2025-02-22T15:06:10+00:00</updated>
    <author>
      <name>/u/Sunija_Dev</name>
      <uri>https://old.reddit.com/user/Sunija_Dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; &lt;a href="https://pastebin.com/BEXz8399"&gt;Example Chat 1&lt;/a&gt; &lt;a href="https://pastebin.com/nH5iYzFe"&gt;/ 2&lt;/a&gt; It works with normal RP (= not text adventure). And it's great.&lt;/p&gt; &lt;p&gt;Maybe you had the same situation as me, seeing the &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iteeqv/new_wayfarer_large_model_a_brutally_challenging/"&gt;announcement of Wayfarer Large 70b&lt;/a&gt;...&lt;/p&gt; &lt;ul&gt; &lt;li&gt;a textadventure model&lt;/li&gt; &lt;li&gt;that is brutal and will kill you&lt;/li&gt; &lt;li&gt;and is a Llama3.3 finetune&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;...and thinking: &lt;strong&gt;Wow, that's like a who's-who of things that I'm not interested in.&lt;/strong&gt; I don't use a textadventure style, I usually don't want to die in my RP, and Llama3 is so sloppy/repetitiony that even finetunes usually don't get rid of it. So, it was rather desperation when I downloaded Wayfarer Large, threw it in my normal setup aaaand... well, you read the title. Let's talk details.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Works with &amp;quot;normal&amp;quot; RP&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Despite it being a textadventure model, you can just use it like any other model without adapting your setup. My example character has an adventurey setting, but the models also works with slice-of-life cards. Or whatever you're into.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Shortform RP&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Wayfarer is one of the few models that writes short posts (&lt;a href="https://pastebin.com/BEXz8399"&gt;see example&lt;/a&gt;). If you like that is definitely subjective. But there are some advantages:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No space for slop/repeptiton (and even if, you'd notice it quickly)&lt;/li&gt; &lt;li&gt;Usable even with 1.5 tok/s&lt;/li&gt; &lt;li&gt;You get to interact more without waiting for generation/reading&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Simply good RP&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Often finetunes just focus on &amp;quot;less slop&amp;quot;, but I think there are more things that make RP good (&lt;a href="https://docs.google.com/document/d/1OwBgvdQkz57g6uNzeBGk3ozru8jwga3NdGahKOfuhc8/edit?usp=sharing"&gt;you can read more on my RP ramblings here&lt;/a&gt;). And despite the posts being short, Wayfarer fits everything necessary in them.&lt;/p&gt; &lt;p&gt;It moves the plot forward and is fairly intelligent. The dialog feels natural, sometimes cracking jokes and being witty. And it references the context (surroundings and stuff) properly, which is a bit of a pet-peeve for me.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Not crazy evil&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;They advertised it as a maniac, but it's... fine. I bet you can prompt it to be a crazy murder-hobo, but it never randomly tried to kill me. It just doesn't have a strong positivity bias and you can have fun arguments with it. Which, I guess (?) is what people rather want, than a murder-hobo. I'd say it has great &amp;quot;emotional range&amp;quot; - it can be angry at you, but it doesn't have to.&lt;/p&gt; &lt;p&gt;It is not as crazy as DeepSeek-R1 that suddenly throws mass murder in your highschool drama. If R1 is &lt;em&gt;Game of Thrones&lt;/em&gt;, Wayfarer is &lt;em&gt;Lord of the Rings&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Keep in mind: I didn't adapt my prompts at all to fit Wayfarer. You can find my system prompt and char card at the end of the example chat. So, with better prompting, you can definitely get more out of the model.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Rarely gets stuck in situations where it doesn't progress the story.&lt;/li&gt; &lt;li&gt;Very rarely switches to &amp;quot;You&amp;quot; style.&lt;/li&gt; &lt;li&gt;Shortform isn't everbodies favorite. But you might be able to change that via prompts?&lt;/li&gt; &lt;li&gt;Doesn't like to write character's thoughts.&lt;/li&gt; &lt;li&gt;Doesn't super strictly follow character cards. Maybe an issue with my prompt.&lt;/li&gt; &lt;li&gt;Doesn't not describes surroundings as much as I'd like.&lt;/li&gt; &lt;li&gt;Still some positivity bias in normal prompting...?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How can I run it?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I run this quant (&lt;a href="https://huggingface.co/VoidStare/Wayfarer-Large-70B-Llama-3.3-EXL2-4.65bpw-h6"&gt;VoidStare_Wayfarer-Large-70B-Llama-3.3-EXL2-4.65bpw-h6&lt;/a&gt;) on 2x3090 (48GB vram). With a 3090+3060 (=36GB vram) you can run a 3bpw quant. Since it's posts are short, running it partially on CPU could be fine too.&lt;/p&gt; &lt;p&gt;Also, if you want to support the creators, you can run it with an &lt;a href="https://aidungeon.com/"&gt;aidungeon subscription&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;So, is it a perfect model? No, obviously not.&lt;/p&gt; &lt;p&gt;But to me, it's the most interesting since Mistral-123b large finetunes. And, besides using it as-is, I bet merging it or finetuning on top could be very interesting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sunija_Dev"&gt; /u/Sunija_Dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivkpdo/wayfarer_large_is_surprisingly_great_example_chats/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivkpdo/wayfarer_large_is_surprisingly_great_example_chats/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivkpdo/wayfarer_large_is_surprisingly_great_example_chats/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T15:06:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivsku7</id>
    <title>Reliability layer to prevent LLM hallucinations</title>
    <updated>2025-02-22T20:43:53+00:00</updated>
    <author>
      <name>/u/mbartu</name>
      <uri>https://old.reddit.com/user/mbartu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's nearly impossible to prevent LLMs from hallucinating, which creates a significant reliability problem. Enterprise companies think, &amp;quot;Using agents could save me money, but if they do the job wrong, the damage outweighs the benefits.&amp;quot; However, there's openness to using agents for non-customer-facing parts and non-critical tasks within the company.&lt;/p&gt; &lt;p&gt;The developers of an e-commerce infrastructure approached us because the format of manufacturer's files doesn't match their e-commerce site's Excel format, and they can't solve it with RPA due to minor differences. They asked if we could perform this data transformation reliably. After two weeks of development, we implemented a reliability layer in our open-source repository. The results were remarkable:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pre-reliability layer: 28.75% accuracy (23/80 successful transfers)&lt;/li&gt; &lt;li&gt;Post-reliability layer: 98.75% accuracy (79/80 successful transfers)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At Upsonic, we use verifier agents and editor agents for this. We didn't expect such high success rates from the agents. I'm surprised by how common these data transformation tasks are. This could be a great vertical agent idea. Btw we use this &lt;a href="https://arxiv.org/pdf/2501.13946"&gt;source&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mbartu"&gt; /u/mbartu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivsku7/reliability_layer_to_prevent_llm_hallucinations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivsku7/reliability_layer_to_prevent_llm_hallucinations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivsku7/reliability_layer_to_prevent_llm_hallucinations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T20:43:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivq82p</id>
    <title>PocketPal Update: Roleplay &amp; AI Assistant Management Made Easy</title>
    <updated>2025-02-22T19:01:25+00:00</updated>
    <author>
      <name>/u/Ill-Still-6859</name>
      <uri>https://old.reddit.com/user/Ill-Still-6859</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivq82p/pocketpal_update_roleplay_ai_assistant_management/"&gt; &lt;img alt="PocketPal Update: Roleplay &amp;amp; AI Assistant Management Made Easy" src="https://external-preview.redd.it/xFz-PydBf6J5PbD1cEzU0c0JBwrOIBAU7nRQ1kNYrMk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbe4bf095fcf49915b200d8be106d02079e7e96a" title="PocketPal Update: Roleplay &amp;amp; AI Assistant Management Made Easy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;just released &lt;strong&gt;Pals&lt;/strong&gt; in &lt;a href="https://github.com/a-ghorbani/pocketpal-ai"&gt;PocketPal&lt;/a&gt; &lt;strong&gt;(v1.8.3+)&lt;/strong&gt;, so wanted to share with you folks.&lt;/p&gt; &lt;p&gt;Pals is basically a feature to make it easier to manage AI assistants and roleplay system prompts/setups. If you often tweak system prompts for LLMs on device, this should save you some time I guess.&lt;/p&gt; &lt;h1&gt;What Pals Lets You Do&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Define Assistant Types:&lt;/strong&gt; simple system prompt.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Roleplay Mode:&lt;/strong&gt; quickly create structured roleplay scenarios.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Choose Default Models:&lt;/strong&gt; assign a specific model for each Pal, so as soon as you select to use it it loads the model too.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quick Activation:&lt;/strong&gt; select or switch Pals directly from the chat input.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;System Prompt Generation:&lt;/strong&gt; llms help also generating system prompts.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you need help writing system prompts, you can use an LLM to generate them inside PocketPal. I've found Llama 3.2 3B works well for this.&lt;/p&gt; &lt;p&gt;For more details and usage instructions check out this: &lt;a href="https://github.com/a-ghorbani/pocketpal-ai/discussions/221"&gt;https://github.com/a-ghorbani/pocketpal-ai/discussions/221&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;As always would love to hear what you think.&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Update &lt;strong&gt;PocketPal&lt;/strong&gt; to the latest version. &lt;ul&gt; &lt;li&gt;&lt;strong&gt;iOS&lt;/strong&gt;: atm through TestFlight: &lt;a href="https://testflight.apple.com/join/B3KE74MS"&gt;https://testflight.apple.com/join/B3KE74MS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Android&lt;/strong&gt;: &lt;a href="https://play.google.com/store/apps/details?id=com.pocketpalai"&gt;https://play.google.com/store/apps/details?id=com.pocketpalai&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Go to the &lt;strong&gt;Pals&lt;/strong&gt; screen and set up an assistant or roleplay character.&lt;/li&gt; &lt;li&gt;Assign a model, system prompt, or generate one using an LLM, etc&lt;/li&gt; &lt;li&gt;Use your Pal from the chat whenever you need it.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ivq82p/video/m2zgersxmqke1/player"&gt;https://reddit.com/link/1ivq82p/video/m2zgersxmqke1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill-Still-6859"&gt; /u/Ill-Still-6859 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivq82p/pocketpal_update_roleplay_ai_assistant_management/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivq82p/pocketpal_update_roleplay_ai_assistant_management/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivq82p/pocketpal_update_roleplay_ai_assistant_management/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T19:01:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivrtrq</id>
    <title>open source, local AI companion that learns about you and handles tasks for you</title>
    <updated>2025-02-22T20:10:28+00:00</updated>
    <author>
      <name>/u/therealkabeer</name>
      <uri>https://old.reddit.com/user/therealkabeer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/existence-master/sentient"&gt;https://github.com/existence-master/sentient&lt;/a&gt; &lt;/p&gt; &lt;p&gt;my team and I have been building this for a while and we just open-sourced it!&lt;/p&gt; &lt;p&gt;its a personal AI companion that learns facts about you and saves them in a knowledge graph. it can use these &amp;quot;memories&amp;quot; to respond to queries and perform actions like sending emails, preparing presentations and docs, adding calendar events, etc with personal context&lt;/p&gt; &lt;p&gt;it runs fully locally, powered by Ollama and can even search the web, if required. (all user data also stays local) &lt;/p&gt; &lt;p&gt;an initial base graph is prepared from your responses to a personality test and by pulling data from your linkedin, reddit and twitter profiles - this gives the companion some initial context about you.&lt;/p&gt; &lt;p&gt;knowledge graphs are maintained in a neo4j database using a GraphRAG pipeline we built from scratch to retrieve and update knowledge efficiently&lt;/p&gt; &lt;p&gt;future plans include voice mode, browser-use capabilities, the ability to perform actions autonomously, better UI/UX and more!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/therealkabeer"&gt; /u/therealkabeer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrtrq/open_source_local_ai_companion_that_learns_about/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrtrq/open_source_local_ai_companion_that_learns_about/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrtrq/open_source_local_ai_companion_that_learns_about/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T20:10:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw3mjz</id>
    <title>Why don’t LLMs use alibi? Were these result found be non-reproducible? I’ve only read of the failed Bloom model. Anyone else?</title>
    <updated>2025-02-23T06:13:14+00:00</updated>
    <author>
      <name>/u/grey-seagull</name>
      <uri>https://old.reddit.com/user/grey-seagull</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw3mjz/why_dont_llms_use_alibi_were_these_result_found/"&gt; &lt;img alt="Why don’t LLMs use alibi? Were these result found be non-reproducible? I’ve only read of the failed Bloom model. Anyone else?" src="https://preview.redd.it/67czpmm7ztke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebb21e546af6ea899f35bd1b3facbe9552d08b76" title="Why don’t LLMs use alibi? Were these result found be non-reproducible? I’ve only read of the failed Bloom model. Anyone else?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grey-seagull"&gt; /u/grey-seagull &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/67czpmm7ztke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw3mjz/why_dont_llms_use_alibi_were_these_result_found/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw3mjz/why_dont_llms_use_alibi_were_these_result_found/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T06:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivfwta</id>
    <title>Finally stable</title>
    <updated>2025-02-22T10:30:39+00:00</updated>
    <author>
      <name>/u/StandardLovers</name>
      <uri>https://old.reddit.com/user/StandardLovers</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivfwta/finally_stable/"&gt; &lt;img alt="Finally stable" src="https://preview.redd.it/67mcka284oke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d58089abaf3cb6614bb83158b3a7bf5ad67876d7" title="Finally stable" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Project Lazarus – Dual RTX 3090 Build&lt;/p&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;p&gt;GPUs: 2x RTX 3090 @ 70% TDP&lt;/p&gt; &lt;p&gt;CPU: Ryzen 9 9950X&lt;/p&gt; &lt;p&gt;RAM: 64GB DDR5 @ 5600MHz&lt;/p&gt; &lt;p&gt;Total Power Draw (100% Load): ~700watts&lt;/p&gt; &lt;p&gt;GPU temps are stable at 60-70c at max load.&lt;/p&gt; &lt;p&gt;These RTX 3090s were bought used with water damage, and I’ve spent the last month troubleshooting and working on stability. After extensive cleaning, diagnostics, and BIOS troubleshooting, today I finally managed to fit a full 70B model entirely in GPU memory.&lt;/p&gt; &lt;p&gt;Since both GPUs are running at 70% TDP, I’ve temporarily allowed one PCIe power cable to feed two PCIe inputs, though it's still not optimal for long-term stability.&lt;/p&gt; &lt;p&gt;Currently monitoring temps and perfmance—so far, so good!&lt;/p&gt; &lt;p&gt;Let me know if you have any questions or suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StandardLovers"&gt; /u/StandardLovers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/67mcka284oke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivfwta/finally_stable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivfwta/finally_stable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T10:30:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivpa6r</id>
    <title>Abusing WebUI Artifacts (Again)</title>
    <updated>2025-02-22T18:21:42+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivpa6r/abusing_webui_artifacts_again/"&gt; &lt;img alt="Abusing WebUI Artifacts (Again)" src="https://external-preview.redd.it/cDhodWx2cjZncWtlMQu5ROv8uFTKESGnRAtwEPoYjV9P5uC6sL5S6dTjkckO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96024f10c2840d152729af0e38edf6258ccf9cd5" title="Abusing WebUI Artifacts (Again)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1eav6zr6gqke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivpa6r/abusing_webui_artifacts_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivpa6r/abusing_webui_artifacts_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T18:21:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivvxep</id>
    <title>L2E llama2.c on Commodore C-64</title>
    <updated>2025-02-22T23:16:37+00:00</updated>
    <author>
      <name>/u/AMICABoard</name>
      <uri>https://old.reddit.com/user/AMICABoard</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvxep/l2e_llama2c_on_commodore_c64/"&gt; &lt;img alt="L2E llama2.c on Commodore C-64" src="https://b.thumbs.redditmedia.com/_5JXsW38zuiDL6biPVP13Lmb8qwW0s56fYU2rf1xacY.jpg" title="L2E llama2.c on Commodore C-64" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have you ever wanted to inference tiny stories on a C64 while going about your daily life and then return after many years to read a story? No? Well, as luck would have it, now YOU CAN!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w8yc07k7wrke1.png?width=725&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9ae9426ef9ae4cea7c8acd33772becebfcf0044"&gt;https://preview.redd.it/w8yc07k7wrke1.png?width=725&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9ae9426ef9ae4cea7c8acd33772becebfcf0044&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/trholding/semu-c64"&gt;https://github.com/trholding/semu-c64&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/VulcanIgnis/status/1893420241310335329"&gt;VulcanIgnis&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AMICABoard"&gt; /u/AMICABoard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvxep/l2e_llama2c_on_commodore_c64/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvxep/l2e_llama2c_on_commodore_c64/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvxep/l2e_llama2c_on_commodore_c64/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T23:16:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivtr02</id>
    <title>Local TTS document reader web app (EPUB/PDF)</title>
    <updated>2025-02-22T21:35:57+00:00</updated>
    <author>
      <name>/u/richardr1126</name>
      <uri>https://old.reddit.com/user/richardr1126</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivtr02/local_tts_document_reader_web_app_epubpdf/"&gt; &lt;img alt="Local TTS document reader web app (EPUB/PDF)" src="https://external-preview.redd.it/N3NpNmJkYnFlcmtlMZSfHsyHsWneNrgFt80RlQiAMZETD9pSG3kMthVQacWZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee91a64c4cffd32d5265d584881203470826fdc5" title="Local TTS document reader web app (EPUB/PDF)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/richardr1126"&gt; /u/richardr1126 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/olajvdbqerke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivtr02/local_tts_document_reader_web_app_epubpdf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivtr02/local_tts_document_reader_web_app_epubpdf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T21:35:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw0zgo</id>
    <title>It's not that mistral 24b is dry, it's parsable and it rocks!</title>
    <updated>2025-02-23T03:36:00+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to say that, what are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw0zgo/its_not_that_mistral_24b_is_dry_its_parsable_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw0zgo/its_not_that_mistral_24b_is_dry_its_parsable_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw0zgo/its_not_that_mistral_24b_is_dry_its_parsable_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T03:36:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivvoto</id>
    <title>Qwen2.5 VL 7B Instruct GGUF + Benchmarks</title>
    <updated>2025-02-22T23:05:11+00:00</updated>
    <author>
      <name>/u/Ragecommie</name>
      <uri>https://old.reddit.com/user/Ragecommie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;We were able to get Qwen2.5 VL working on llama.cpp!&lt;br /&gt; It is not official yet, but it's pretty easy to get going with a custom build.&lt;br /&gt; Instructions &lt;a href="https://github.com/ggml-org/llama.cpp/issues/11483#issuecomment-2676422772"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Over the next couple of days, we'll upload quants, along with tests / performance evals here:&lt;br /&gt; &lt;a href="https://huggingface.co/IAILabs/Qwen2.5-VL-7b-Instruct-GGUF/tree/main"&gt;https://huggingface.co/IAILabs/Qwen2.5-VL-7b-Instruct-GGUF/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Original 16-bit and Q8_0 are up along with the mmproj model.&lt;/p&gt; &lt;p&gt;First impressions are pretty good, not only in terms of quality, but speed as well.&lt;/p&gt; &lt;p&gt;Will post updates and more info as we go!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ragecommie"&gt; /u/Ragecommie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvoto/qwen25_vl_7b_instruct_gguf_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvoto/qwen25_vl_7b_instruct_gguf_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvoto/qwen25_vl_7b_instruct_gguf_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T23:05:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivtten</id>
    <title>Perplexity R1 Llama 70B Uncensored GGUFs &amp; Dynamic 4bit quant</title>
    <updated>2025-02-22T21:39:03+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perplexity I think quietly released uncensored versions of DeepSeek R1 Llama 70B Distilled versions - I actually totally missed this - did anyone see an announcement or know about this?&lt;/p&gt; &lt;p&gt;I uploaded 2bit all the way until 16bit GGUFs for the model: &lt;a href="https://huggingface.co/unsloth/r1-1776-distill-llama-70b-GGUF"&gt;https://huggingface.co/unsloth/r1-1776-distill-llama-70b-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also uploaded dynamic 4bit quants for finetuning and vLLM serving: &lt;a href="https://huggingface.co/unsloth/r1-1776-distill-llama-70b-unsloth-bnb-4bit"&gt;https://huggingface.co/unsloth/r1-1776-distill-llama-70b-unsloth-bnb-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few days ago I uploaded dynamic 2bit, 3bit and 4bit quants for the full R1 Uncensored 671B MoE version, which dramatically increase accuracy by not quantizing certain modules. This is similar to the 1.58bit quant of DeepSeek R1 we did! &lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF"&gt;https://huggingface.co/unsloth/r1-1776-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivtten/perplexity_r1_llama_70b_uncensored_ggufs_dynamic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivtten/perplexity_r1_llama_70b_uncensored_ggufs_dynamic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivtten/perplexity_r1_llama_70b_uncensored_ggufs_dynamic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T21:39:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw1xn7</id>
    <title>The Paradox of Open Weights, but Closed Source</title>
    <updated>2025-02-23T04:29:18+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;- An open-weight model has public weights, which you can download from sites like Hugging Face.&lt;/p&gt; &lt;p&gt;- An open-source model has public training code and training dataset, allowing full reproduction. (I didn't come up with that definition, personally I think the dataset requirement is too strict, because then nearly every major model is closed-source.)&lt;/p&gt; &lt;p&gt;- A permissive model has a permissive license, like MIT or Apache 2.0, which means you can do many things with the weights, like serve them over a commercialized inference endpoint. A license like CC-BY-NC is often considered &amp;quot;non-permissive&amp;quot; since the NC means non-commercial.&lt;/p&gt; &lt;p&gt;Kokoro-82M is an Apache 2.0 model that I trained and uploaded to HF &lt;em&gt;without also uploading the accompanying training code or dataset&lt;/em&gt;, thus making it permissive and open-weight, yet also closed-source under the above definitions.&lt;/p&gt; &lt;p&gt;As I've said in the past, there is already MIT-licensed training code at &lt;a href="https://github.com/yl4579/StyleTTS2"&gt;https://github.com/yl4579/StyleTTS2&lt;/a&gt; which others have already used/modified to produce models comparable to, or in some cases better than, Kokoro. But nobody seems to care about that that, they want &lt;em&gt;my&lt;/em&gt; specific training code. Many have speculated why I have not (yet) done this. I'll offer two very practical reasons here—there may be others, but these ones are critical &amp;amp; sufficient.&lt;/p&gt; &lt;p&gt;First, commercial. Obviously, there is commercial value (to me &amp;amp; others) in the code I write, including the training code. Many of those calling for me to release my training code would, undoubtedly, turn around and commercialize that code. On the inference side, I have understood and accepted this reality, and that does not deter me from releasing and improving inference code, especially for other languages. I cannot promise that I'll get there on training.&lt;/p&gt; &lt;p&gt;Second, surge pricing, or basic supply and demand. I have no local NVIDIA GPU and therefore rely on A100 80GB cloud rentals. My training code is specifically configured (in some places hardcoded) for A100 80GB, since these training runs are often vRAM intensive. Unless (or even if) I refactor, open sourcing the training code would probably lead to increased rental demand for the same machines I want, making current and future training runs more expensive. The lowest five A100 80GB prices I see on Vast.ai are $1.1, $1.35, $1.35, $1.41, $1.47, which is typical pricing depth (or lack thereof). Even a handful of people scooping up the cheapest A100s moves the needle quite a lot.&lt;/p&gt; &lt;p&gt;Despite my own training code currently not being released:&lt;/p&gt; &lt;p&gt;- You can train StyleTTS2 models today using the aforementioned MIT training code. I have not gatekept or obfuscated the StyleTTS2 roots of Kokoro—it has been in the README since day 0. Sure, I picked a new model name, but in line with industry standards, it is generally acceptable to name a model when it has substantially new weights.&lt;/p&gt; &lt;p&gt;- Others have/will publish their own training code, for StyleTTS2 models and others.&lt;/p&gt; &lt;p&gt;- There will simply be better open models, in the Kokoro series, in TTS at large, and all modalities in general.&lt;/p&gt; &lt;p&gt;This particular post was motivated by a back-and-forth I had with &lt;a href="/u/Fold-Plastic"&gt;u/Fold-Plastic&lt;/a&gt;. To those who think I am The Enemy for not releasing the training code: I think you are directing way too much animosity towards a permissive-open-weight solo dev operating in a field of non-permissive and closed-weight orgs. It's that sort of animosity that makes open source exhausting rather than rewarding, and pushes devs to leave for the warm embrace of money-printing closed source.&lt;/p&gt; &lt;p&gt;Some other notes:&lt;/p&gt; &lt;p&gt;- I have not yet made a decision on voice cloning, although unlike training code, an encoder release won't spike my A100 costs by +50%, so it is more likely than a training code release.&lt;/p&gt; &lt;p&gt;- For Kokoro, take your voice cloning performance expectations and divide them by 10, since the volume of audio seen during training remains OOMs lower than other TTS models.&lt;/p&gt; &lt;p&gt;- In the meantime, for voice cloning you should be looking at larger TTS models trained on more audio, like XTTS Fish Zonos etc.&lt;/p&gt; &lt;p&gt;- Voice cloning Trump TSwift or Obama may be less &amp;quot;dark magic&amp;quot; and more &amp;quot;retrieval&amp;quot;, assuming those celebrities are in the training dataset (not currently the case for Kokoro).&lt;/p&gt; &lt;p&gt;- Future Kokoro models (i.e. above v1.0) will likely follow a naming scheme like `hexgrad/Kokoro-82M-vX.Y`.&lt;/p&gt; &lt;p&gt;- If voice cloning were to be released, it would change the model naming to `hexgrad/Kokoro-vX.Y`. This is because the encoder is ~25M params, and summing the params across the encoder and the 82M decoder does not feel appropriate.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw1xn7/the_paradox_of_open_weights_but_closed_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw1xn7/the_paradox_of_open_weights_but_closed_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw1xn7/the_paradox_of_open_weights_but_closed_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T04:29:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivyc62</id>
    <title>Chirp 3b | Ozone AI</title>
    <updated>2025-02-23T01:15:13+00:00</updated>
    <author>
      <name>/u/Perfect-Bowl-1601</name>
      <uri>https://old.reddit.com/user/Perfect-Bowl-1601</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;From the same creators of Reverb 7b, we present, &lt;strong&gt;CHIRP 3b&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We’re excited to introduce our latest model: &lt;strong&gt;Chirp-3b!&lt;/strong&gt; The Ozone AI team has been pouring effort into this one, and we think it’s a big step up for 3B performance. Chirp-3b was trained on over 50 million tokens of distilled data from GPT-4o, fine-tuned from a solid base model to bring some serious capability to the table.&lt;/p&gt; &lt;p&gt;The benchmarks are in, and Chirp-3b is shining! It’s delivering standout results on both MMLU Pro and IFEval, exceeding what we’d expect from a model this size. Check out the details:&lt;/p&gt; &lt;h3&gt;MMLU Pro&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Subject&lt;/th&gt; &lt;th&gt;Average Accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Biology&lt;/td&gt; &lt;td&gt;0.6234&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Business&lt;/td&gt; &lt;td&gt;0.5032&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Chemistry&lt;/td&gt; &lt;td&gt;0.3701&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Computer Science&lt;/td&gt; &lt;td&gt;0.4268&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Economics&lt;/td&gt; &lt;td&gt;0.5284&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Engineering&lt;/td&gt; &lt;td&gt;0.3013&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Health&lt;/td&gt; &lt;td&gt;0.3900&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;History&lt;/td&gt; &lt;td&gt;0.3885&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Law&lt;/td&gt; &lt;td&gt;0.2252&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Math&lt;/td&gt; &lt;td&gt;0.5736&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Other&lt;/td&gt; &lt;td&gt;0.4145&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Philosophy&lt;/td&gt; &lt;td&gt;0.3687&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Physics&lt;/td&gt; &lt;td&gt;0.3995&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Psychology&lt;/td&gt; &lt;td&gt;0.5589&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Overall Average&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.4320&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;That’s a 9-point boost over the base model—pretty remarkable!&lt;/p&gt; &lt;h3&gt;IFEval&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;72%&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;These gains make Chirp-3b a compelling option for its class. (More benchmarks are on the way!)&lt;/p&gt; &lt;p&gt;Model Card &amp;amp; Download: &lt;a href="https://huggingface.co/ozone-research/Chirp-01"&gt;https://huggingface.co/ozone-research/Chirp-01&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’re passionate about advancing open-source LLMs, and Chirp-3b is a proud part of that journey. We’ve got more models cooking, including 2B and bigger versions, so watch this space!&lt;/p&gt; &lt;p&gt;We’re pumped to get your feedback! Download Chirp-3b, give it a spin, and let us know how it performs for you. Your input helps us keep improving.&lt;/p&gt; &lt;p&gt;Thanks for the support—we’re eager to see what you create with Chirp-3b!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect-Bowl-1601"&gt; /u/Perfect-Bowl-1601 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivyc62/chirp_3b_ozone_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivyc62/chirp_3b_ozone_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivyc62/chirp_3b_ozone_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T01:15:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivrprb</id>
    <title>Kimi.ai released Moonlight a 3B/16B MoE model trained with their improved Muon optimizer.</title>
    <updated>2025-02-22T20:05:32+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrprb/kimiai_released_moonlight_a_3b16b_moe_model/"&gt; &lt;img alt="Kimi.ai released Moonlight a 3B/16B MoE model trained with their improved Muon optimizer." src="https://external-preview.redd.it/8-2Sl3ne20MUsYCwIhDQN3Ob-UIeeembj6eG4654s7k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=41c166131e1d55a90ce452e5b495385744c6917d" title="Kimi.ai released Moonlight a 3B/16B MoE model trained with their improved Muon optimizer." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Moonlight beats other similar SOTA models in most of the benchmarks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MoonshotAI/Moonlight?tab=readme-ov-file"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrprb/kimiai_released_moonlight_a_3b16b_moe_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrprb/kimiai_released_moonlight_a_3b16b_moe_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T20:05:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivrtqk</id>
    <title>DeepSeek Founders Are Worth $1 Billion or $150 Billion Depending Who You Ask</title>
    <updated>2025-02-22T20:10:26+00:00</updated>
    <author>
      <name>/u/cramdev</name>
      <uri>https://old.reddit.com/user/cramdev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrtqk/deepseek_founders_are_worth_1_billion_or_150/"&gt; &lt;img alt="DeepSeek Founders Are Worth $1 Billion or $150 Billion Depending Who You Ask" src="https://external-preview.redd.it/Y2j22dshKg69yVQTELClk4zSnJfoKi77KX2nOwS6buo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba4da483ee892a27f534028e7c20f82a3a3b889f" title="DeepSeek Founders Are Worth $1 Billion or $150 Billion Depending Who You Ask" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cramdev"&gt; /u/cramdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-02-10/deepseek-could-make-founder-liang-wenfeng-one-of-the-world-s-richest-people?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTczOTIzNzk1NywiZXhwIjoxNzM5ODQyNzU3LCJhcnRpY2xlSWQiOiJTUjhYTTdUMEcxS1cwMCIsImJjb25uZWN0SWQiOiI0MUVGMDc3MjI0RTM0MDhFOTNFMDdFQkY0RDc3QzI1QiJ9.kqtC_AK59CyhVfXIjYbRqB5ymi-WS52icc0pzlfX74E"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrtqk/deepseek_founders_are_worth_1_billion_or_150/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrtqk/deepseek_founders_are_worth_1_billion_or_150/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T20:10:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivua9y</id>
    <title>For the love of God, stop abusing the word "multi"</title>
    <updated>2025-02-22T22:00:30+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;We trained a SOTA multimodal LLM&amp;quot; and then you dig deep and find it only supports text and vision. These are only two modalities. You trained a SOTA BI-MODAL LLM. &lt;/p&gt; &lt;p&gt;&amp;quot;Our model shows significant improvement in multilingual applications.... The model supports English and Chinese text&amp;quot; yeah... This is a BILINGUAL model. &lt;/p&gt; &lt;p&gt;The word &amp;quot;multi&amp;quot; means &amp;quot;many&amp;quot;. While two is technically &amp;quot;many&amp;quot;, there's a better prefix for that and it is &amp;quot;bi&amp;quot;.&lt;/p&gt; &lt;p&gt;I can't count the number of times people claim they trained a SOTA open model that &amp;quot;beats gpt-4o in multimodal tasks&amp;quot; only to find out the model only supports image and text and not audio (which was the whole point behind gpt-4o anyway) &lt;/p&gt; &lt;p&gt;TLDR: Use &amp;quot;bi&amp;quot; when talking about 2 modalities and languages, use &amp;quot;multi&amp;quot; when talking about 3 or mode.&lt;/p&gt; &lt;p&gt;P.S. I am not downplaying the importance and significance of these open models, but it's better to avoid hyping and deceiving the community.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivua9y/for_the_love_of_god_stop_abusing_the_word_multi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivua9y/for_the_love_of_god_stop_abusing_the_word_multi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivua9y/for_the_love_of_god_stop_abusing_the_word_multi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T22:00:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw35xy</id>
    <title>SanDisk's new High Bandwidth Flash memory enables 4TB of VRAM on GPUs, matches HBM bandwidth at higher capacity</title>
    <updated>2025-02-23T05:43:36+00:00</updated>
    <author>
      <name>/u/Durian881</name>
      <uri>https://old.reddit.com/user/Durian881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw35xy/sandisks_new_high_bandwidth_flash_memory_enables/"&gt; &lt;img alt="SanDisk's new High Bandwidth Flash memory enables 4TB of VRAM on GPUs, matches HBM bandwidth at higher capacity" src="https://external-preview.redd.it/TCljVIqB29jZGbvnEemLaBHNh4_np29Eo1N9f7IuxMc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dbdd3cfb8dd25b151a368faf5c7855efb0390fd" title="SanDisk's new High Bandwidth Flash memory enables 4TB of VRAM on GPUs, matches HBM bandwidth at higher capacity" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Durian881"&gt; /u/Durian881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/dram/sandisks-new-hbf-memory-enables-up-to-4tb-of-vram-on-gpus-matches-hbm-bandwidth-at-higher-capacity"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw35xy/sandisks_new_high_bandwidth_flash_memory_enables/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw35xy/sandisks_new_high_bandwidth_flash_memory_enables/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T05:43:36+00:00</published>
  </entry>
</feed>
