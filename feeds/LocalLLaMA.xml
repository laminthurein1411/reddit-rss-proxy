<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-25T22:37:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kukjoe</id>
    <title>46pct Aider Polyglot in 16GB VRAM with Qwen3-14B</title>
    <updated>2025-05-24T20:06:20+00:00</updated>
    <author>
      <name>/u/andrewmobbs</name>
      <uri>https://old.reddit.com/user/andrewmobbs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After some tuning, and a tiny hack to aider, I have achieved a Aider Polyglot benchmark of pass_rate_2: 45.8 with 100% of cases well-formed, using nothing more than a 16GB 5070 Ti and Qwen3-14b, with the model running entirely offloaded to GPU.&lt;/p&gt; &lt;p&gt;That result is on a par with &amp;quot;chatgpt-4o-latest (2025-03-29)&amp;quot; on the &lt;a href="https://aider.chat/docs/leaderboards/"&gt;Aider Leaderboard&lt;/a&gt;. When allowed 3 tries at the solution, rather than the 2 tries on the benchmark, the pass rate increases to 59.1% nearly matching the &amp;quot;claude-3-7-sonnet-20250219 (no thinking)&amp;quot; result (which, to be clear, only needed 2 tries to get 60.4%). I think this is useful, as it reflects how a user may interact with a local LLM, since more tries only cost time.&lt;/p&gt; &lt;p&gt;The method was to start with the &lt;a href="https://huggingface.co/Qwen/Qwen3-14B-GGUF/blob/main/Qwen3-14B-Q6_K.gguf"&gt;Qwen3-14B Q6_K&lt;/a&gt; GGUF, set the context to the full 40960 tokens, and quantized the KV cache to Q8_0/Q5_1. To do this, I used llama.cpp server, compiled with GGML_CUDA_FA_ALL_QUANTS=ON. (Q8_0 for both K and V does &lt;em&gt;just&lt;/em&gt; fit in 16GB, but doesn't leave much spare VRAM. To allow for Gnome desktop, VS Code and a browser I dropped the V cache to Q5_1, which doesn't seem to do much relative harm to quality.)&lt;/p&gt; &lt;p&gt;Aider was then configured to use the &amp;quot;/think&amp;quot; reasoning token and use &amp;quot;architect&amp;quot; edit mode. The editor model was the same Qwen3-14B Q6, but the &amp;quot;tiny hack&amp;quot; mentioned was to ensure that the editor coder used the &amp;quot;/nothink&amp;quot; token and to extend the chat timeout from the 600s default.&lt;/p&gt; &lt;p&gt;Eval performance averaged 43 tokens per second.&lt;/p&gt; &lt;p&gt;Full details in comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andrewmobbs"&gt; /u/andrewmobbs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kukjoe/46pct_aider_polyglot_in_16gb_vram_with_qwen314b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kukjoe/46pct_aider_polyglot_in_16gb_vram_with_qwen314b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kukjoe/46pct_aider_polyglot_in_16gb_vram_with_qwen314b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T20:06:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuzane</id>
    <title>Initial thoughts on Google Jules</title>
    <updated>2025-05-25T10:20:59+00:00</updated>
    <author>
      <name>/u/maaakks</name>
      <uri>https://old.reddit.com/user/maaakks</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've just been playing with Google Jules and honestly, I'm incredibly impressed by the amount of work it can handle almost autonomously.&lt;/p&gt; &lt;p&gt;I haven't had that feeling in a long time. I'm usually very skeptical, and I've tested other code agents like Roo Code and Openhands with Gemini 2.5 Flash and local models (devstral/qwen3). But this is on another level. The difference might just be the model jump from flash to pro, but still amazing.&lt;/p&gt; &lt;p&gt;I've heard people say the ratio is going to be 10ai:1human really soon, but if we have to validate all the changes for now, it feels more likely that it will be 10humans:1ai, simply because we can't keep up with the pace.&lt;/p&gt; &lt;p&gt;My only suggestion for improvement would be to have a local version of this interface, so we could use it on projects outside of GitHub, much like you can with Openhands.&lt;/p&gt; &lt;p&gt;Has anyone else test it? Is it just me getting carried away, or do you share the same feeling?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maaakks"&gt; /u/maaakks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuzane/initial_thoughts_on_google_jules/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuzane/initial_thoughts_on_google_jules/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuzane/initial_thoughts_on_google_jules/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T10:20:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kur0xh</id>
    <title>Round Up: Current Best Local Models under 40B for Code &amp; Tool Calling, General Chatting, Vision, and Creative Story Writing.</title>
    <updated>2025-05-25T01:30:10+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Each week, we get new models and fine-tunes that is really difficult of keep up with or test all of them. &lt;/p&gt; &lt;p&gt;The main challenge I personally face is to identify which model and its versions (different fine-tunes) that is most suitable for a specific domain. Fine-tunes of existing base models are especially frustrating because there are so many and I don't know which ones I should focus on. And, as far as I know, there is no database that tracks all the models and their fine-tunes and benchmarks them against different use cases.&lt;/p&gt; &lt;p&gt;So, I go back to you, fellow LLMers to help me put a list of the best models that are currently available, under 40B that we can run locally to assist us in tasks like Coding, writing, OCR and vision tasks, and RP and general chatting. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you can, could you score the models on a scale from 1 to 10 so we can a concrete idea about your experience with the model. Also, try to provide the link to the model itself.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thanks in advance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kur0xh/round_up_current_best_local_models_under_40b_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kur0xh/round_up_current_best_local_models_under_40b_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kur0xh/round_up_current_best_local_models_under_40b_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T01:30:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvehcm</id>
    <title>Can we run a quantized model on android?</title>
    <updated>2025-05-25T22:09:33+00:00</updated>
    <author>
      <name>/u/Away_Expression_3713</name>
      <uri>https://old.reddit.com/user/Away_Expression_3713</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to run a onnx model which i quantized to about nearly 440mb. I am trying to run it using onnx runtime but the app still crashes while loading? Anyone can help me&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Away_Expression_3713"&gt; /u/Away_Expression_3713 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvehcm/can_we_run_a_quantized_model_on_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvehcm/can_we_run_a_quantized_model_on_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvehcm/can_we_run_a_quantized_model_on_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T22:09:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kva7sp</id>
    <title>Chainlit or Open webui for production?</title>
    <updated>2025-05-25T19:00:44+00:00</updated>
    <author>
      <name>/u/psssat</name>
      <uri>https://old.reddit.com/user/psssat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I am DS at my company but recently I have been tasked on developing a chatbot for our other engineers. I am currently the only one working on this project, and I have been learning as I go and there is noone else at my company who has knowledge on how to do this. Basically my first goal is to use a pre-trained LLM and create a chat bot that can help with existing python code bases. So here is where I am at after the past 4 months:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;I have used &lt;code&gt;ast&lt;/code&gt; and &lt;code&gt;jedi&lt;/code&gt; to create tools that can parse a python code base and create RAG chunks in &lt;code&gt;jsonl&lt;/code&gt; and &lt;code&gt;md&lt;/code&gt; format.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I have used created a query system for the RAG database using both the &lt;code&gt;sentence_transformer&lt;/code&gt; and &lt;code&gt;hnswlib&lt;/code&gt; libraries. I am using &amp;quot;all-MiniLM-L6-v2&amp;quot; as the encoder.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I use &lt;code&gt;vllm&lt;/code&gt; to serve the model and for the UI I have done two things. First, I used &lt;code&gt;chainlit&lt;/code&gt; and some custom python code to stream text from the model being served with &lt;code&gt;vllm&lt;/code&gt; to the &lt;code&gt;chainlit&lt;/code&gt; ui. Second, I messed around with &lt;code&gt;openwebui&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So my questions are basically about the last bullet point above. Where should I put efforts in regards to the UI? I really like how many features come with &lt;code&gt;openwebui&lt;/code&gt; but it seems pretty hard to customize especcially when it comes to RAG. I was able to set up RAG with &lt;code&gt;openwebui&lt;/code&gt; but it would incorrectly chunk my &lt;code&gt;md&lt;/code&gt; files and I was not able to figure out yet if it was possible to make sure that &lt;code&gt;openwebui&lt;/code&gt; chunks my &lt;code&gt;md&lt;/code&gt; files correctly.&lt;/p&gt; &lt;p&gt;In terms of &lt;code&gt;chainlit&lt;/code&gt;, I like how customizable it is, but at the same time, there are alot of features that I would like that do not come with it like, saved chat histories, user login, document uploads for rag, etc.&lt;/p&gt; &lt;p&gt;So for a production quality chatbot, how should I continue? Should I try and customize &lt;code&gt;openwebui&lt;/code&gt; to most that it allows me or should I do everything from scratch with &lt;code&gt;chainlit&lt;/code&gt;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/psssat"&gt; /u/psssat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kva7sp/chainlit_or_open_webui_for_production/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kva7sp/chainlit_or_open_webui_for_production/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kva7sp/chainlit_or_open_webui_for_production/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T19:00:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kui17w</id>
    <title>OpenHands + Devstral is utter crap as of May 2025 (24G VRAM)</title>
    <updated>2025-05-24T18:12:37+00:00</updated>
    <author>
      <name>/u/foobarg</name>
      <uri>https://old.reddit.com/user/foobarg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following the recent &lt;a href="https://mistral.ai/news/devstral"&gt;announcement of Devstral&lt;/a&gt;, I gave &lt;a href="https://github.com/All-Hands-AI/OpenHands?tab=readme-ov-file#-running-openhands-locally"&gt;OpenHands&lt;/a&gt; + Devstral (Q4_K_M on &lt;a href="https://ollama.com/library/devstral:24b"&gt;Ollama&lt;/a&gt;) a try for a fully offline code agent experience.&lt;/p&gt; &lt;h1&gt;OpenHands&lt;/h1&gt; &lt;p&gt;Meh. I won't comment much, it's a reasonable web frontend, neatly packaged as a single podman/docker container. This could use &lt;em&gt;a lot&lt;/em&gt; more polish (the configuration through environment variables is broken for example) but once you've painfully reverse-engineered the incantation to make ollama work from the non-existing documentation, it's fairly out your way.&lt;/p&gt; &lt;p&gt;I don't like the fact you must give it access to your podman/docker installation (by mounting the socket in the container) which is technically equivalent to giving this huge pile of untrusted code root access to your host. &lt;a href="https://github.com/All-Hands-AI/OpenHands/issues/5269"&gt;This is necessary&lt;/a&gt; because OpenHands needs to spawn a &lt;em&gt;runtime&lt;/em&gt; for each &amp;quot;project&amp;quot;, and the runtime is itself its own container. Surely there must be a better way?&lt;/p&gt; &lt;h1&gt;Devstral (Mistral AI)&lt;/h1&gt; &lt;p&gt;Don't get me wrong, it's awesome to have companies releasing models to the general public. I'll be blunt though: this first iteration is useless. Devstral is supposed to have been trained/fine-tuned &lt;em&gt;precisely&lt;/em&gt; to be good at the agentic behaviors that OpenHands promises. This means having access to tools like bash, a browser, and primitives to read &amp;amp; edit files. Devstral &lt;a href="https://huggingface.co/mistralai/Devstral-Small-2505/blob/main/SYSTEM_PROMPT.txt"&gt;system prompt&lt;/a&gt; references OpenHands by name. The &lt;a href="https://mistral.ai/news/devstral"&gt;press release&lt;/a&gt; boasts:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Devstral is light enough to run on a single RTX 4090. [‚Ä¶] The performance [‚Ä¶] makes it a suitable choice for agentic coding on privacy-sensitive repositories in enterprises&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It does not. I tried a few primitive tasks and it utterly failed almost all of them while burning through the whole 380 watts my GPU demands.&lt;/p&gt; &lt;p&gt;It sometimes manages to run one or two basic commands in a row, but it often takes more than one try, hence is slow and frustrating:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Clone the git repository [url] and run &lt;a href="http://build.sh"&gt;build.sh&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The most basic commands and text manipulation tasks all failed and I had to interrupt its desperate attempts. I ended up telling myself it would have been faster to do it myself, saving the Amazon rainforest as an added bonus.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Asked it to extract the JS from a short HTML file which had a single &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag. It created the file successfully (but transformed it against my will), then wasn't able to remove the tag from the HTML as the proposed edits wouldn't pass OpenHands' correctness checks.&lt;/li&gt; &lt;li&gt;Asked it to remove comments from a short file. Same issue, &lt;code&gt;ERROR: No replacement was performed, old_str [...] did not appear verbatim in /workspace/...&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Asked it to bootstrap a minimal todo app. It got stuck in a loop trying to invoke interactive &lt;code&gt;create-app&lt;/code&gt; tools from the cursed JS ecosystem, which require arrow keys to navigate menus‚Äìdid I mention I hate those wizards?&lt;/li&gt; &lt;li&gt;Prompt adhesion is bad. Even when you try to help by providing the &lt;em&gt;exact command&lt;/em&gt;, it randomly removes dashes and other important bits, and then proceeds to comfortably heat up my room trying to debug the inevitable errors.&lt;/li&gt; &lt;li&gt;OpenHands includes two random TCP ports in the prompt, to use for HTTP servers (like Vite or uvicorn) that are forwarded to the host. The model fails to understand to use them and spawns servers on the default port, making them inaccessible.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As a point of comparison, I tried those using one of the cheaper proprietary models out there (Gemini Flash) which obviously is general-purpose and &lt;em&gt;not&lt;/em&gt; tuned to OpenHands particularities. It had no issue adhering to OpenHands' prompt and blasted through the tasks‚Äìincluding tweaking the HTTP port mentioned above.&lt;/p&gt; &lt;p&gt;Perhaps this is meant to run on more expensive hardware that can run the larger flavors. If &amp;quot;all&amp;quot; you have is 24G VRAM, prepare to be disappointed. Local agentic programming is not there yet. Did anyone else try it, and does your experience match?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foobarg"&gt; /u/foobarg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T18:12:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuxgh7</id>
    <title>What makes the Mac Pro so efficient in running LLMs?</title>
    <updated>2025-05-25T08:12:50+00:00</updated>
    <author>
      <name>/u/goingsplit</name>
      <uri>https://old.reddit.com/user/goingsplit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am specifically referring to the 1TB ram version, able apparently to run deepseek at several token-per-second speed, using unified memory and integrated graphics.&lt;/p&gt; &lt;p&gt;Second to this: any way to replicate in the x86 world? Like perhaps with an 8dimm motherboard and one of the latest integrated Xe2 cpus? (although this would still not yield 1TB ram..)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goingsplit"&gt; /u/goingsplit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuxgh7/what_makes_the_mac_pro_so_efficient_in_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuxgh7/what_makes_the_mac_pro_so_efficient_in_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuxgh7/what_makes_the_mac_pro_so_efficient_in_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T08:12:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kujwzl</id>
    <title>We believe the future of AI is local, private, and personalized.</title>
    <updated>2025-05-24T19:36:41+00:00</updated>
    <author>
      <name>/u/ice-url</name>
      <uri>https://old.reddit.com/user/ice-url</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That‚Äôs why we built &lt;strong&gt;Cobolt&lt;/strong&gt; ‚Äî a free cross-platform AI assistant that runs entirely on your device.&lt;/p&gt; &lt;p&gt;Cobolt represents our vision for the future of AI assistants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Privacy by design (everything runs locally)&lt;/li&gt; &lt;li&gt;Extensible through Model Context Protocol (MCP)&lt;/li&gt; &lt;li&gt;Personalized without compromising your data&lt;/li&gt; &lt;li&gt;Powered by community-driven development&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We're looking for contributors, testers, and fellow privacy advocates to join us in building the future of personal AI.&lt;/p&gt; &lt;p&gt;ü§ù Contributions Welcome! üåü Star us on &lt;a href="https://github.com/platinum-hill/cobolt"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üì• Try Cobolt on &lt;a href="https://github.com/platinum-hill/cobolt/releases/download/v0.0.3/Cobolt-0.0.3.dmg"&gt;macOS&lt;/a&gt; or &lt;a href="https://github.com/platinum-hill/cobolt/releases/download/v0.0.3/Cobolt-Setup-0.0.3.exe"&gt;Windows&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let's build AI that serves you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ice-url"&gt; /u/ice-url &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kujwzl/we_believe_the_future_of_ai_is_local_private_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kujwzl/we_believe_the_future_of_ai_is_local_private_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kujwzl/we_believe_the_future_of_ai_is_local_private_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T19:36:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1kv7xng</id>
    <title>Vulkan for vLLM?</title>
    <updated>2025-05-25T17:23:47+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been thinking about trying out vLLM. With llama.cpp, I found that rocm didn't support my radeon 780M igpu, but vulkan did. &lt;/p&gt; &lt;p&gt;Does anyone know if one can use vulkan with vLLM? I didn't see it when searching the docs, but thought I'd ask around.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv7xng/vulkan_for_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv7xng/vulkan_for_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kv7xng/vulkan_for_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T17:23:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuxvgt</id>
    <title>Tired of manually copy-pasting files for LLMs or docs? I built a (free, open-source) tool for that!</title>
    <updated>2025-05-25T08:41:55+00:00</updated>
    <author>
      <name>/u/ps5cfw</name>
      <uri>https://old.reddit.com/user/ps5cfw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Reddit,&lt;/p&gt; &lt;p&gt;Ever find yourself jumping between like 20 different files, copying and pasting code or text just to feed it into an LLM, or to bundle up stuff for documentation? I was doing that all the time and it was driving me nuts.&lt;/p&gt; &lt;p&gt;So, I built a little desktop app called &lt;strong&gt;File Collector&lt;/strong&gt; to make it easier. It's pretty straightforward:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You pick a main folder.&lt;/li&gt; &lt;li&gt;It shows you a file tree, and you just check the files/folders you want.&lt;/li&gt; &lt;li&gt;It then merges all that content into one big text block, with clear separators like // File: path/to/your/file.cs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's got some handy bits like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;.gitignore style ignore patterns:&lt;/strong&gt; So you don't accidentally pull in your node_modules or bin/obj folders. You can even import your existing .gitignore!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pre/Post Prompts:&lt;/strong&gt; Add custom text before or after all your file content (great for LLM instructions).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Syntax highlighting&lt;/strong&gt; in the preview.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Saves your setup:&lt;/strong&gt; Remembers your last folder and selections, and you can even save/load &amp;quot;contexts&amp;quot; if you have common sets of files you grab.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross-platform:&lt;/strong&gt; Works on Windows, Mac, and Linux since it's built with .NET Blazor and Photino.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's been a real time-saver for me when I'm prepping context for Gemini Pro or trying to pull together all the relevant code for a new feature doc.&lt;/p&gt; &lt;p&gt;Now some of you might be asking &lt;strong&gt;&lt;em&gt;&amp;quot;Well, there's that Gemini Coder (Now called Code Web Chat) that does basically the same for VS Code&amp;quot;&lt;/em&gt;&lt;/strong&gt;, and you would be indeed right! I built this specifically because:&lt;/p&gt; &lt;p&gt;1) I do not use VS Code&lt;br /&gt; 2) Performance of CWC was abysmal for me and I've often found myself in a state of not even being able to tick a checkbox / UI becoming completely unresponsive, which is kind of counterproductive.&lt;/p&gt; &lt;p&gt;Which is why I built this specifically in Blazor, Even the text highlighter is written in Blazor, with no JS, Node, Visual studio code shenanigans involved and performance decent enough to handle monorepo structures well over hundreds of thousands of files and folders.&lt;/p&gt; &lt;p&gt;It's meant to be fast, it's meant to be simple, it's meant to be cross-platform and no bullshit involved.&lt;/p&gt; &lt;p&gt;It's completely free and open-source. If this sounds like something that could help you out, you can check it out on GitHub:&lt;br /&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Florenzodimauro97%2FFileCollector"&gt;https://github.com/lorenzodimauro97/FileCollector&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear any feedback, feature ideas, or if you find it useful!&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ps5cfw"&gt; /u/ps5cfw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuxvgt/tired_of_manually_copypasting_files_for_llms_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuxvgt/tired_of_manually_copypasting_files_for_llms_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuxvgt/tired_of_manually_copypasting_files_for_llms_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T08:41:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvee1v</id>
    <title>Next-Gen Sentiment Analysis Just Got Smarter (Prototype + Open to Feedback!)</title>
    <updated>2025-05-25T22:05:07+00:00</updated>
    <author>
      <name>/u/Majestic_Turn3879</name>
      <uri>https://old.reddit.com/user/Majestic_Turn3879</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvee1v/nextgen_sentiment_analysis_just_got_smarter/"&gt; &lt;img alt="Next-Gen Sentiment Analysis Just Got Smarter (Prototype + Open to Feedback!)" src="https://external-preview.redd.it/NHVrd3BpNHozMDNmMZ2Dryfthtl7IF6_dzBQvSpDdWPUsSphoWXpK8uxuNF-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d4fbbf9d21b3c50cb05a6e7e2927e611ac45f6f" title="Next-Gen Sentiment Analysis Just Got Smarter (Prototype + Open to Feedback!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on a prototype that reimagines sentiment analysis using AI‚Äîsomething that goes beyond just labeling feedback as ‚Äúpositive‚Äù or ‚Äúnegative‚Äù and actually uncovers why people feel the way they do. It uses transformer models (DistilBERT, Twitter-RoBERTa, and Multilingual BERT) combined with BERTopic to cluster feedback into meaningful themes.&lt;/p&gt; &lt;p&gt;I designed the entire workflow myself and used ChatGPT to help code it‚Äîproof that AI can dramatically speed up prototyping and automate insight discovery in a strategic way.&lt;/p&gt; &lt;p&gt;It‚Äôs built for insights and CX teams, product managers, or anyone tired of manually combing through reviews or survey responses.&lt;/p&gt; &lt;p&gt;While it‚Äôs still in the prototype stage, it already highlights emerging issues, competitive gaps, and the real drivers behind sentiment.&lt;/p&gt; &lt;p&gt;I‚Äôd love to get your thoughts on it‚Äîwhat could be improved, where it could go next, or whether anyone would be interested in trying it on real data. I‚Äôm open to feedback, collaboration, or just swapping ideas with others working on AI + insights .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Majestic_Turn3879"&gt; /u/Majestic_Turn3879 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jbp3fr8z303f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvee1v/nextgen_sentiment_analysis_just_got_smarter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvee1v/nextgen_sentiment_analysis_just_got_smarter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T22:05:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvcq04</id>
    <title>Qwen2.5-VL and Gemma 3 settings for OCR</title>
    <updated>2025-05-25T20:50:19+00:00</updated>
    <author>
      <name>/u/dzdn1</name>
      <uri>https://old.reddit.com/user/dzdn1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working with using VLMs to OCR handwriting (think journals, travel logs). I get much better results than traditional OCR, which pretty much fails completely even with tools meant to do better with handwriting. &lt;/p&gt; &lt;p&gt;However, results are inconsistent, and changing parameters like temp, repeat-penalty and others affect the results, but in unpredictable ways (to a newb like myself).&lt;/p&gt; &lt;p&gt;Gemma 3 (12B) with default settings just makes a whole new narrative seemingly loosely inspired by the text on the page. I have not found settings to improve this. &lt;/p&gt; &lt;p&gt;Qwen2.5-VL (7B) does much better, getting even words I can barely read, but requires a detailed and kind of randomly pieced together prompt and system prompt, and changing it in minor ways can break it, making it skip sections, lose accuracy on some letters, etc. which I think makes it unreliable for long-term use.&lt;/p&gt; &lt;p&gt;Additionally, llama.cpp I believe shrinks the image to 1024 max for Qwen (because much larger quickly floods RAM). I am working on trying to use more sophisticated downscaling and sharpening edges, etc. but this does not seem to be improving the results.&lt;/p&gt; &lt;p&gt;Has anyone gotten these or other models to work well with freeform handwriting and if so, do you have any advice for settings to use?&lt;/p&gt; &lt;p&gt;I have seen how these new VLMs can finally help with handwriting in a way previously unimagined, but I am having trouble getting out to the &amp;quot;next step.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dzdn1"&gt; /u/dzdn1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvcq04/qwen25vl_and_gemma_3_settings_for_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvcq04/qwen25vl_and_gemma_3_settings_for_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvcq04/qwen25vl_and_gemma_3_settings_for_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T20:50:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1kv4jim</id>
    <title>How can I use my spare 1080ti?</title>
    <updated>2025-05-25T14:58:01+00:00</updated>
    <author>
      <name>/u/tutami</name>
      <uri>https://old.reddit.com/user/tutami</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've 7800x3d and 7900xtx system and my old 1080ti is rusting. How can I put my old boy to work?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tutami"&gt; /u/tutami &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv4jim/how_can_i_use_my_spare_1080ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv4jim/how_can_i_use_my_spare_1080ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kv4jim/how_can_i_use_my_spare_1080ti/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T14:58:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvegc1</id>
    <title>Used or New Gamble</title>
    <updated>2025-05-25T22:08:06+00:00</updated>
    <author>
      <name>/u/thehoffau</name>
      <uri>https://old.reddit.com/user/thehoffau</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Aussie madlad here. &lt;/p&gt; &lt;p&gt;The second hand market in AU is pretty small, there are the odd 3090s running around but due to distance they are always a risk in being a) a scam b) damaged in freight c) broken at time of sale.&lt;/p&gt; &lt;p&gt;The 9700xtx new and a 3090 used are about the same price. Reading this group for months the XTX seems to get the job done for most things (give or take 10% and feature delay?) &lt;/p&gt; &lt;p&gt;I have a threadripper system that's CPU/ram can do LLMs okay and I can easily slot in two GPU which is the medium term plan. I was initially looking at 2 X A4000(16gb) but am now looking at long term either 2x3090 or 2xXTX&lt;/p&gt; &lt;p&gt;It's a pretty sizable investment to loose out on and I'm stuck in a loop. Risk second hand for NVIDIA or safe for AMD?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thehoffau"&gt; /u/thehoffau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvegc1/used_or_new_gamble/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvegc1/used_or_new_gamble/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvegc1/used_or_new_gamble/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T22:08:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kv74jx</id>
    <title>Qwen 235b DWQ MLX 4 bit quant</title>
    <updated>2025-05-25T16:49:51+00:00</updated>
    <author>
      <name>/u/nomorebuttsplz</name>
      <uri>https://old.reddit.com/user/nomorebuttsplz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit-DWQ"&gt;https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit-DWQ&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Two questions:&lt;br /&gt; 1. Does anyone have a good way to test perplexity against the standard MLX 4 bit quant?&lt;br /&gt; 2. I notice this is exactly the same size as the standard 4 bit mlx quant: 132.26 gb. Does that make sense? I would expect a slight difference is likely given the dynamic compression of DWQ.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomorebuttsplz"&gt; /u/nomorebuttsplz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv74jx/qwen_235b_dwq_mlx_4_bit_quant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv74jx/qwen_235b_dwq_mlx_4_bit_quant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kv74jx/qwen_235b_dwq_mlx_4_bit_quant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T16:49:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kv762l</id>
    <title>RTX PRO 6000 96GB plus Intel Battlemage 48GB feasible?</title>
    <updated>2025-05-25T16:51:41+00:00</updated>
    <author>
      <name>/u/SteveRD1</name>
      <uri>https://old.reddit.com/user/SteveRD1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OK, this may be crazy but I wanted to run it by you all.&lt;/p&gt; &lt;p&gt;Can you combine a RTX PRO 6000 96GB (with all the Nvidia CUDA goodies) with a (relatively) cheap Intel 48GB GPUs for extra VRAM?&lt;/p&gt; &lt;p&gt;So you have 144GB VRAM available, but you have all the capabilities of Nvidia on your main card driving the LLM inferencing?&lt;/p&gt; &lt;p&gt;This idea sounds too good to be true....what am I missing here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SteveRD1"&gt; /u/SteveRD1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv762l/rtx_pro_6000_96gb_plus_intel_battlemage_48gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv762l/rtx_pro_6000_96gb_plus_intel_battlemage_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kv762l/rtx_pro_6000_96gb_plus_intel_battlemage_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T16:51:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvceya</id>
    <title>I need a text only browser python library</title>
    <updated>2025-05-25T20:36:15+00:00</updated>
    <author>
      <name>/u/Somerandomguy10111</name>
      <uri>https://old.reddit.com/user/Somerandomguy10111</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvceya/i_need_a_text_only_browser_python_library/"&gt; &lt;img alt="I need a text only browser python library" src="https://preview.redd.it/bd5haso3oz2f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e16001d2ec9ed0b0439505bab505eaf6fdd9fed1" title="I need a text only browser python library" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm developing an open source AI agent framework with search and eventually web interaction capabilities. To do that I need a browser. While it could be conceivable to just forward a screenshot of the browser it would be much more efficient to introduce the page into the context as text.&lt;/p&gt; &lt;p&gt;Ideally I'd have something like lynx which you see in the screenshot, but as a python library. Like Lynx above it should conserve the layout, formatting and links of the text as good as possible. Just to cross a few things off:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lynx: While it looks pretty much ideal, it's a terminal utility. It'll be pretty difficult to integrate with Python.&lt;/li&gt; &lt;li&gt;HTML get requests: It works for some things but some websites require a Browser to even load the page. Also it doesn't look great&lt;/li&gt; &lt;li&gt;Screenshot the browser: As discussed above, it's possible. But not very efficient.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Have you faced this problem? If yes, how have you solved it? I've come up with a selenium driven Browser Emulator but it's pretty rough around the edges and I don't really have time to go into depth on that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Somerandomguy10111"&gt; /u/Somerandomguy10111 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bd5haso3oz2f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvceya/i_need_a_text_only_browser_python_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvceya/i_need_a_text_only_browser_python_library/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T20:36:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvc9ri</id>
    <title>I wrote an automated setup script for my Proxmox AI VM that installs Nvidia CUDA Toolkit, Docker, Python, Node, Zsh and more</title>
    <updated>2025-05-25T20:30:08+00:00</updated>
    <author>
      <name>/u/erdaltoprak</name>
      <uri>https://old.reddit.com/user/erdaltoprak</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvc9ri/i_wrote_an_automated_setup_script_for_my_proxmox/"&gt; &lt;img alt="I wrote an automated setup script for my Proxmox AI VM that installs Nvidia CUDA Toolkit, Docker, Python, Node, Zsh and more" src="https://external-preview.redd.it/NmVqMXp0bWRqejJmMd5vTxA1ciqILnZ5a_nUE62vdQvPjJEI_nmjQVTxndSt.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4640fca024f61ffbd3fba74333acfc27e740ea7d" title="I wrote an automated setup script for my Proxmox AI VM that installs Nvidia CUDA Toolkit, Docker, Python, Node, Zsh and more" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a script (&lt;a href="https://gist.github.com/erdaltoprak/cdc1ec4056b81a9da540229dcde3aa0b"&gt;available on Github here&lt;/a&gt;) that automates the setup of a fresh Ubuntu 24.04 server for AI/ML development work. It handles the complete installation and configuration of Docker, ZSH, Python (via pyenv), Node (via n), NVIDIA drivers and the NVIDIA Container Toolkit, basically everything you need to get a GPU accelerated development environment up and running quickly&lt;/p&gt; &lt;p&gt;This script reflects my personal setup preferences and hardware, so if you want to customize it for your own needs, I highly recommend reading through the script and understanding what it does before running it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/erdaltoprak"&gt; /u/erdaltoprak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/e006utmdjz2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvc9ri/i_wrote_an_automated_setup_script_for_my_proxmox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvc9ri/i_wrote_an_automated_setup_script_for_my_proxmox/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T20:30:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuy45r</id>
    <title>Gemma 3n Architectural Innovations - Speculation and poking around in the model.</title>
    <updated>2025-05-25T08:59:16+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuy45r/gemma_3n_architectural_innovations_speculation/"&gt; &lt;img alt="Gemma 3n Architectural Innovations - Speculation and poking around in the model." src="https://external-preview.redd.it/EZj1oQJN95Oq7YDpbSs0ORxFqLi-24Ocse4J7I4sO0A.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1052f0a2169e9b937ad3af8d86cab69bb6b8b09b" title="Gemma 3n Architectural Innovations - Speculation and poking around in the model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E4B-it-litert-preview"&gt;Gemma 3n&lt;/a&gt; is a new member of the Gemma family with free weights that was released during Google I/O. It's dedicated to on-device (edge) inference and supports image and text input, with audio input. Google has released an app that can be used for inference on the phone.&lt;/p&gt; &lt;p&gt;What is clear from&lt;a href="https://ai.google.dev/gemma/docs/gemma-3n"&gt; the documentation&lt;/a&gt;, is that this model is stuffed to the brim with architectural innovations: Per-Layer Embedding (PLE), MatFormer Architecture, Conditional Parameter Loading.&lt;/p&gt; &lt;p&gt;Unfortunately, there is no paper out for the model yet. I assume that this will follow at some point, but so far I had some success poking around in the model file. I thought I'd share my findings so far, maybe someone else has more insights?&lt;/p&gt; &lt;p&gt;The provided .task file is actually a ZIP container of tflite models. It can be unpacked with ZIP.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Component&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Purpose&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;TF_LITE_PREFILL_DECODE&lt;/td&gt; &lt;td align="left"&gt;2.55 GB&lt;/td&gt; &lt;td align="left"&gt;Main language model component for text generation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TF_LITE_PER_LAYER_EMBEDDER&lt;/td&gt; &lt;td align="left"&gt;1.23 GB&lt;/td&gt; &lt;td align="left"&gt;Per-layer embeddings from the transformer&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TF_LITE_EMBEDDER&lt;/td&gt; &lt;td align="left"&gt;259 MB&lt;/td&gt; &lt;td align="left"&gt;Input embeddings&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TF_LITE_VISION_ENCODER&lt;/td&gt; &lt;td align="left"&gt;146 MB&lt;/td&gt; &lt;td align="left"&gt;Vision Encoding&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TF_LITE_VISION_ADAPTER&lt;/td&gt; &lt;td align="left"&gt;17 MB&lt;/td&gt; &lt;td align="left"&gt;Adapts vision embeddings for the language model?&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TOKENIZER_MODEL&lt;/td&gt; &lt;td align="left"&gt;4.5 MB&lt;/td&gt; &lt;td align="left"&gt;Tokenizer&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;METADATA&lt;/td&gt; &lt;td align="left"&gt;56 bytes&lt;/td&gt; &lt;td align="left"&gt;general metadata&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The TFlite models can be opened in a network visualizer like &lt;a href="http://netron.app"&gt;netron.app&lt;/a&gt; to display the content.&lt;/p&gt; &lt;p&gt;The model uses an inner dimension of 2048 and has 35 transformer blocks. Tokenizer size is 262144.&lt;/p&gt; &lt;p&gt;First, one interesting find it that is uses learned residual connections. This paper seems to be related to this: &lt;a href="https://arxiv.org/abs/2411.07501v3"&gt;https://arxiv.org/abs/2411.07501v3&lt;/a&gt; (LAuReL: Learned Augmented Residual Layer)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tvl3od2v3w2f1.png?width=1251&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4cee0588c6b82700ab02e3eadd02a13d7c9b7af0"&gt;https://preview.redd.it/tvl3od2v3w2f1.png?width=1251&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4cee0588c6b82700ab02e3eadd02a13d7c9b7af0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The FFN is projecting from 2048 to 16384 with a GeGLU activation. This is an unusually wide ratio. I assume that some part of these parameters can be selectively turned on and off to implement the Matformer architecture. It is not clear how this is implemented in the compute graph though.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/foq74ff15w2f1.png?width=605&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cdf24aec8aaa93efe3c968fd93b62b1439af0036"&gt;https://preview.redd.it/foq74ff15w2f1.png?width=605&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cdf24aec8aaa93efe3c968fd93b62b1439af0036&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A very interesting part is the per-layer embedding. The file TF_LITE_PER_LAYER_EMBEDDER contains very large lookup tables (262144x256x35) that will output a 256 embedding for every layer depending on the input token. Since this is essentially a lookup table, it can be efficiently processed even on the CPU. This is an extremely interesting approach to adding more capacity to the model without increasing FLOPS.&lt;/p&gt; &lt;p&gt;The embeddings are applied in an operation that follows the FFN and are used as a gate to a low rank projection. The residual stream is downprojected to 256, multiplied with the embedding and then projected up to 2048 again. It's a bit like a token-selective LoRA. In addition there is a gating operation that controls the overall weighting of this stream.&lt;/p&gt; &lt;p&gt;I am very curious for further information. I was not able to find any paper on this aspect of the model. Hopefully, google will share more information.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lchxfc6w6w2f1.png?width=875&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a612976c324a28f34dee305b2c64f8b911a2cab"&gt;https://preview.redd.it/lchxfc6w6w2f1.png?width=875&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a612976c324a28f34dee305b2c64f8b911a2cab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wca7kzfq5w2f1.png?width=1190&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3fd2195e4829bf47c8f6d0e2d6fef2c133e1d2f"&gt;https://preview.redd.it/wca7kzfq5w2f1.png?width=1190&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3fd2195e4829bf47c8f6d0e2d6fef2c133e1d2f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuy45r/gemma_3n_architectural_innovations_speculation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuy45r/gemma_3n_architectural_innovations_speculation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuy45r/gemma_3n_architectural_innovations_speculation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T08:59:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kv2gb2</id>
    <title>Qualcomm discrete NPU (Qualcomm AI 100) in upcoming Dell workstation laptops</title>
    <updated>2025-05-25T13:23:47+00:00</updated>
    <author>
      <name>/u/SkyFeistyLlama8</name>
      <uri>https://old.reddit.com/user/SkyFeistyLlama8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv2gb2/qualcomm_discrete_npu_qualcomm_ai_100_in_upcoming/"&gt; &lt;img alt="Qualcomm discrete NPU (Qualcomm AI 100) in upcoming Dell workstation laptops" src="https://external-preview.redd.it/GusyhEpTmXh7oXULalG-maSvDCVfQxTdBP1AMHOUr_A.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b7731e91f8b244e977d425e9ede836b7d78861c" title="Qualcomm discrete NPU (Qualcomm AI 100) in upcoming Dell workstation laptops" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SkyFeistyLlama8"&gt; /u/SkyFeistyLlama8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://uk.pcmag.com/laptops/158095/dell-ditches-the-gpu-for-an-ai-chip-in-this-bold-new-workstation-laptop"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv2gb2/qualcomm_discrete_npu_qualcomm_ai_100_in_upcoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kv2gb2/qualcomm_discrete_npu_qualcomm_ai_100_in_upcoming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T13:23:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvd0jr</id>
    <title>M3 Ultra Mac Studio Benchmarks (96gb VRAM, 60 GPU cores)</title>
    <updated>2025-05-25T21:03:03+00:00</updated>
    <author>
      <name>/u/procraftermc</name>
      <uri>https://old.reddit.com/user/procraftermc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I recently got the M3 Ultra Mac Studio (96 GB RAM, 60 core GPU). Here's its performance.&lt;/p&gt; &lt;p&gt;I loaded each model freshly in LMStudio, and input 30-40k tokens of Lorem Ipsum text (the text itself shouldn't matter, all that matters is token counts)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarking Results&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name &amp;amp; Size&lt;/th&gt; &lt;th align="left"&gt;Time to First Token (s)&lt;/th&gt; &lt;th align="left"&gt;Tokens / Second&lt;/th&gt; &lt;th align="left"&gt;Input Context Size (tokens)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 0.6b (bf16)&lt;/td&gt; &lt;td align="left"&gt;18.21&lt;/td&gt; &lt;td align="left"&gt;78.61&lt;/td&gt; &lt;td align="left"&gt;40240&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30b-a3b (8-bit)&lt;/td&gt; &lt;td align="left"&gt;67.74&lt;/td&gt; &lt;td align="left"&gt;34.62&lt;/td&gt; &lt;td align="left"&gt;40240&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma 3 27B (4-bit)&lt;/td&gt; &lt;td align="left"&gt;108.15&lt;/td&gt; &lt;td align="left"&gt;29.55&lt;/td&gt; &lt;td align="left"&gt;30869&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LLaMA4 Scout 17B-16E (4-bit)&lt;/td&gt; &lt;td align="left"&gt;111.33&lt;/td&gt; &lt;td align="left"&gt;33.85&lt;/td&gt; &lt;td align="left"&gt;32705&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral Large 123B (4-bit)&lt;/td&gt; &lt;td align="left"&gt;900.61&lt;/td&gt; &lt;td align="left"&gt;7.75&lt;/td&gt; &lt;td align="left"&gt;32705&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Additional Information&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Input was 30,000 - 40,000 tokens of Lorem Ipsum text&lt;/li&gt; &lt;li&gt;Model was reloaded with no prior caching&lt;/li&gt; &lt;li&gt;After caching, prompt processing (time to first token) dropped to almost zero&lt;/li&gt; &lt;li&gt;Prompt processing times on input &amp;lt;10,000 tokens was also workably low&lt;/li&gt; &lt;li&gt;Interface used was LM Studio&lt;/li&gt; &lt;li&gt;All models were 4-bit &amp;amp; MLX except Qwen3 0.6b and Qwen3 30b-a3b (they were bf16 and 8bit, respectively)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Token speeds were generally good, especially for MoE's like Qen 30b and Llama4. Of course, time-to-first-token was quite high as expected.&lt;/p&gt; &lt;p&gt;Loading models was way more efficient than I thought, I could load Mistral Large (4-bit) with 32k context using only ~70GB VRAM.&lt;/p&gt; &lt;p&gt;Feel free to request benchmarks for any model, I'll see if I can download and benchmark it :).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/procraftermc"&gt; /u/procraftermc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvd0jr/m3_ultra_mac_studio_benchmarks_96gb_vram_60_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvd0jr/m3_ultra_mac_studio_benchmarks_96gb_vram_60_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvd0jr/m3_ultra_mac_studio_benchmarks_96gb_vram_60_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T21:03:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuwrll</id>
    <title>üëÄ BAGEL-7B-MoT: The Open-Source GPT-Image-1 Alternative You‚Äôve Been Waiting For.</title>
    <updated>2025-05-25T07:24:39+00:00</updated>
    <author>
      <name>/u/Rare-Programmer-1747</name>
      <uri>https://old.reddit.com/user/Rare-Programmer-1747</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuwrll/bagel7bmot_the_opensource_gptimage1_alternative/"&gt; &lt;img alt="üëÄ BAGEL-7B-MoT: The Open-Source GPT-Image-1 Alternative You‚Äôve Been Waiting For." src="https://b.thumbs.redditmedia.com/0FqyiVWdrxZTgD89ug8SMDskK09zDRwfDx-Rn8gc8zk.jpg" title="üëÄ BAGEL-7B-MoT: The Open-Source GPT-Image-1 Alternative You‚Äôve Been Waiting For." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/sw3eao9cqv2f1.jpg?width=3000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4c753fae3901f5a15249aa73803dbfbed0b8f77e"&gt;https://preview.redd.it/sw3eao9cqv2f1.jpg?width=3000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4c753fae3901f5a15249aa73803dbfbed0b8f77e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ByteDance has unveiled &lt;strong&gt;BAGEL-7B-MoT&lt;/strong&gt;, an open-source multimodal AI model that rivals OpenAI's proprietary &lt;strong&gt;GPT-Image-1&lt;/strong&gt; in capabilities. With 7 billion active parameters (14 billion total) and a Mixture-of-Transformer-Experts (MoT) architecture, BAGEL offers advanced functionalities in text-to-image generation, image editing, and visual understanding‚Äîall within a single, unified model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Unified Multimodal Capabilities:&lt;/strong&gt; BAGEL seamlessly integrates text, image, and video processing, eliminating the need for multiple specialized models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced Image Editing:&lt;/strong&gt; Supports free-form editing, style transfer, scene reconstruction, and multiview synthesis, often producing more accurate and contextually relevant results than other open-source models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Emergent Abilities:&lt;/strong&gt; Demonstrates capabilities such as chain-of-thought reasoning and world navigation, enhancing its utility in complex tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmark Performance:&lt;/strong&gt; Outperforms models like Qwen2.5-VL and InternVL-2.5 on standard multimodal understanding leaderboards and delivers text-to-image quality competitive with specialist generators like SD3.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Comparison with GPT-Image-1:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;BAGEL-7B-MoT&lt;/th&gt; &lt;th align="left"&gt;GPT-Image-1&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Open-source (Apache 2.0)&lt;/td&gt; &lt;td align="left"&gt;Proprietary (requires OpenAI API key)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Multimodal Capabilities&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Text-to-image, image editing, visual understanding&lt;/td&gt; &lt;td align="left"&gt;Primarily text-to-image generation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Architecture&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Mixture-of-Transformer-Experts&lt;/td&gt; &lt;td align="left"&gt;Diffusion-based model&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Self-hostable on local hardware&lt;/td&gt; &lt;td align="left"&gt;Cloud-based via OpenAI API&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Emergent Abilities&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Free-form image editing, multiview synthesis, world navigation&lt;/td&gt; &lt;td align="left"&gt;Limited to text-to-image generation and editing&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Installation and Usage:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Developers can access the model weights and implementation on Hugging Face. For detailed installation instructions and usage examples, the GitHub repository is available.&lt;/p&gt; &lt;p&gt;BAGEL-7B-MoT represents a significant advancement in multimodal AI, offering a versatile and efficient solution for developers working with diverse media types. Its open-source nature and comprehensive capabilities make it a valuable tool for those seeking an alternative to proprietary models like GPT-Image-1.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare-Programmer-1747"&gt; /u/Rare-Programmer-1747 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuwrll/bagel7bmot_the_opensource_gptimage1_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuwrll/bagel7bmot_the_opensource_gptimage1_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuwrll/bagel7bmot_the_opensource_gptimage1_alternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T07:24:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvc9w6</id>
    <title>Cheapest Ryzen AI Max+ 128GB yet at $1699. Ships June 10th.</title>
    <updated>2025-05-25T20:30:17+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bosgamepc.com/products/bosgame-m5-ai-mini-desktop-ryzen-ai-max-395"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvc9w6/cheapest_ryzen_ai_max_128gb_yet_at_1699_ships/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvc9w6/cheapest_ryzen_ai_max_128gb_yet_at_1699_ships/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T20:30:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuzk3t</id>
    <title>Online inference is a privacy nightmare</title>
    <updated>2025-05-25T10:39:04+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I dont understand how big tech just convinced people to hand over so much stuff to be processed in plain text. Cloud storage at least can be all encrypted. But people have got comfortable sending emails, drafts, their deepest secrets, all in the open on some servers somewhere. Am I crazy? People were worried about posts and likes on social media for privacy but this is magnitudes larger in scope. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuzk3t/online_inference_is_a_privacy_nightmare/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuzk3t/online_inference_is_a_privacy_nightmare/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuzk3t/online_inference_is_a_privacy_nightmare/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T10:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kv6jjk</id>
    <title>Fine-tuning HuggingFace SmolVLM (256M) to control the robot</title>
    <updated>2025-05-25T16:24:19+00:00</updated>
    <author>
      <name>/u/Complex-Indication</name>
      <uri>https://old.reddit.com/user/Complex-Indication</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv6jjk/finetuning_huggingface_smolvlm_256m_to_control/"&gt; &lt;img alt="Fine-tuning HuggingFace SmolVLM (256M) to control the robot" src="https://external-preview.redd.it/b29vMmxwbTNmeTJmMSsIw5Jo6JnOhGNxnxMg56RLkadqgoRaNULw7zamXe9N.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d86f3549101d3f35fb75d562c3bd121473237386" title="Fine-tuning HuggingFace SmolVLM (256M) to control the robot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with tiny LLMs and VLMs for a while now, perhaps some of your saw my earlier post here about running &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1g9seqf/a_tiny_language_model_260k_params_is_running/"&gt;LLM on ESP32 for Dalek&lt;/a&gt; Halloween prop. This time I decided to use HuggingFace really tiny (256M parameters!) SmolVLM to control robot just from camera frames. The input is a prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Based on the image choose one action: forward, left, right, back. If there is an obstacle blocking the view, choose back. If there is an obstacle on the left, choose right. If there is an obstacle on the right, choose left. If there are no obstacles, choose forward. Based on the image choose one action: forward, left, right, back. If there is an obstacle blocking the view, choose back. If there is an obstacle on the left, choose right. If there is an obstacle on the right, choose left. If there are no obstacles, choose forward.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;and an image from Raspberry Pi Camera Module 2. The output is text.&lt;/p&gt; &lt;p&gt;The base model didn't work at all, but after collecting some data (200 images) and fine-tuning with LORA, it actually (to my surprise) started working!&lt;/p&gt; &lt;p&gt;Currently the model runs on local PC and the data is exchanged between Raspberry Pi Zero 2 and the PC over local network. I know for a fact I can run SmolVLM fast enough on Raspberry Pi 5, but I was not able to do it due to power issues (Pi 5 is very power hungry), so I decided to leave it for the next video.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Complex-Indication"&gt; /u/Complex-Indication &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9s2q9nm3fy2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv6jjk/finetuning_huggingface_smolvlm_256m_to_control/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kv6jjk/finetuning_huggingface_smolvlm_256m_to_control/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T16:24:19+00:00</published>
  </entry>
</feed>
