<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-18T18:24:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i3rpsh</id>
    <title>The ‚Äúapple‚Äù test - Why aren‚Äôt newer reasoning models doing better on this basic benchmark? (and yes, I know token prediction mechanics play a role)</title>
    <updated>2025-01-17T21:49:22+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most of you are probably familiar with the infamous LLM ‚Äúapple test‚Äù benchmark.&lt;/p&gt; &lt;p&gt;If you‚Äôre not, here it is, you give an LLM the following seemingly simple instruction prompt:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Write 10 sentences that end in the word ‚Äúapple‚Äù.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sadly, most open source (and even a lot of frontier models fail miserably at this task. I‚Äôve read that it has a lot to do with the way token prediction works, but some models can actually pass this test easily.&lt;/p&gt; &lt;p&gt;Models that I‚Äôve tested that pass or fail on this test:&lt;/p&gt; &lt;p&gt;LLMs that PASS the apple test:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama 3.3:70b (Q4KM)&lt;/li&gt; &lt;li&gt;Athene-V2 (Q4KM)&lt;/li&gt; &lt;li&gt;Nemotron (Q4KM)&lt;/li&gt; &lt;li&gt;Qwen 2.5:72b (Q4KM)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;LLMs that FAIL the apple test (most are newer models) &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Phi-4 14b (FP16)&lt;/li&gt; &lt;li&gt;InternLM3 (FP16)&lt;/li&gt; &lt;li&gt;Falcon 3 10b (FP16)&lt;/li&gt; &lt;li&gt;Granite 3 Dense (FP16)&lt;/li&gt; &lt;li&gt;QwQ 32b (Q_8)&lt;/li&gt; &lt;li&gt;GLM-4 8b (FP16)&lt;/li&gt; &lt;li&gt;Command-R (Q4KM)&lt;/li&gt; &lt;li&gt;MiniCPM 8b v2.6 (FP16)&lt;/li&gt; &lt;li&gt;Mistral Small 22b (Q4KM)&lt;/li&gt; &lt;li&gt;Nemotron Mini 4b (FP16)&lt;/li&gt; &lt;li&gt;Qwen 2.5 7b (FP16) &lt;/li&gt; &lt;li&gt;WizardLM2 7b (FP16)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;FAILED but with an honorable mention:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Olmo2 14b (FP16) - this model is lightning fast and got 8 of 10 consistently correct and was able to fix its mistake after a second shot at it (most models won‚Äôt do better with more chances). &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This task seems to be challenging for models under 70b to complete. Even the newer reasoning models with higher test time compute capabilities don‚Äôt seem to do well at all. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Why haven‚Äôt newer models gotten better at this task over time? &lt;/li&gt; &lt;li&gt;Is the underlying mechanism of token prediction still preventing success? &lt;/li&gt; &lt;li&gt;Are the models that this works with just cheating by training to pass the specific benchmark? &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone found an open source model under 70b that can pass the apple test consistently? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3rpsh/the_apple_test_why_arent_newer_reasoning_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3rpsh/the_apple_test_why_arent_newer_reasoning_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3rpsh/the_apple_test_why_arent_newer_reasoning_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T21:49:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1i401lt</id>
    <title>Whisper turbo fine tuning guidance</title>
    <updated>2025-01-18T04:53:51+00:00</updated>
    <author>
      <name>/u/fgoricha</name>
      <uri>https://old.reddit.com/user/fgoricha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking to try fine tuning whisper large v3 turbo on runpod. I have a 3090 which I could use locally, but why not play with a cloud gpu so I can use my gpu for other stuff. Does anyone have any guides I can follow to help with the fine tuning process? I asked ChatGPT and it almost seems too easy. I already have my audio files in .wav format and their correctly transcribed text files. &lt;/p&gt; &lt;p&gt;Thanks for any help or advice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fgoricha"&gt; /u/fgoricha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i401lt/whisper_turbo_fine_tuning_guidance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i401lt/whisper_turbo_fine_tuning_guidance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i401lt/whisper_turbo_fine_tuning_guidance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T04:53:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3qzom</id>
    <title>5090 OpenCL &amp; Vulkan leaks</title>
    <updated>2025-01-17T21:17:14+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ack, not crushing 4090.&lt;br /&gt; &lt;a href="https://videocardz.com/newz/nvidia-geforce-rtx-5090-appears-in-first-geekbench-opencl-vulkan-leaks"&gt;https://videocardz.com/newz/nvidia-geforce-rtx-5090-appears-in-first-geekbench-opencl-vulkan-leaks&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3qzom/5090_opencl_vulkan_leaks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3qzom/5090_opencl_vulkan_leaks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3qzom/5090_opencl_vulkan_leaks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T21:17:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4czbi</id>
    <title>LLMs in Production book in print - seems like it has a little for everyone running LLMs locally or self hosting elsewhere. Finetuning, picking models, etc.</title>
    <updated>2025-01-18T17:47:25+00:00</updated>
    <author>
      <name>/u/jobe_br</name>
      <uri>https://old.reddit.com/user/jobe_br</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4czbi/llms_in_production_book_in_print_seems_like_it/"&gt; &lt;img alt="LLMs in Production book in print - seems like it has a little for everyone running LLMs locally or self hosting elsewhere. Finetuning, picking models, etc." src="https://external-preview.redd.it/88Tkp4phdjLSYRKL6QPbB1O0IMz-RVm8rf2O0JEluis.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3cbcd8c2d744060b617c36edb568205b930694f1" title="LLMs in Production book in print - seems like it has a little for everyone running LLMs locally or self hosting elsewhere. Finetuning, picking models, etc." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jobe_br"&gt; /u/jobe_br &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.manning.com/books/llms-in-production"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4czbi/llms_in_production_book_in_print_seems_like_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4czbi/llms_in_production_book_in_print_seems_like_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T17:47:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4dswh</id>
    <title>The Best Animation Creator (Not Video Generator)?</title>
    <updated>2025-01-18T18:23:37+00:00</updated>
    <author>
      <name>/u/yukiarimo</name>
      <uri>https://old.reddit.com/user/yukiarimo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys! Do you know any good AI animation creators? I mean, to work like this:&lt;/p&gt; &lt;p&gt;I‚Äôm drawing like starting frame, ending frame, and a few in between, and similar to interpolation (but plain interpolation won‚Äôt work here because no video is ready) it will create enough frames to make from make few drawings an animated sequence?&lt;/p&gt; &lt;p&gt;Open-source only! Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yukiarimo"&gt; /u/yukiarimo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4dswh/the_best_animation_creator_not_video_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4dswh/the_best_animation_creator_not_video_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4dswh/the_best_animation_creator_not_video_generator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T18:23:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3w7ao</id>
    <title>[2403.09919] Recurrent Drafter for Fast Speculative Decoding in Large Language Models</title>
    <updated>2025-01-18T01:20:34+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2403.09919"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3w7ao/240309919_recurrent_drafter_for_fast_speculative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3w7ao/240309919_recurrent_drafter_for_fast_speculative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T01:20:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3pup0</id>
    <title>Beating cuBLAS in SGEMM from Scratch</title>
    <updated>2025-01-17T20:26:10+00:00</updated>
    <author>
      <name>/u/salykova</name>
      <uri>https://old.reddit.com/user/salykova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pup0/beating_cublas_in_sgemm_from_scratch/"&gt; &lt;img alt="Beating cuBLAS in SGEMM from Scratch" src="https://a.thumbs.redditmedia.com/5VtAEp46vu6qAMyyshFSAQ0PS4VyO1ibLsIEkWU_HY0.jpg" title="Beating cuBLAS in SGEMM from Scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A while ago, I shared my article here about optimizing matrix multiplication on CPUs - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1dt3rqc/beating_numpys_matrix_multiplication_in_150_lines/"&gt;Beating NumPy's matrix multiplication in 150 lines of C code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I received positive feedback from you, and today I'm excited to share my second blog post. This one focuses on an SGEMM (Single-precision GEneral Matrix Multiply) that outperforms NVIDIA's implementation from cuBLAS library with its (modified?) CUTLASS kernel across a wide range of matrix sizes. This project primarily targets &lt;strong&gt;CUDA-learners&lt;/strong&gt; and aims to bridge the gap between the SGEMM implementations explained in books/blogs and those used in NVIDIA‚Äôs BLAS libraries. The blog delves into benchmarking code on CUDA devices and explains the algorithm's design along with optimization techniques. These include inlined PTX, asynchronous memory copies, double-buffering, avoiding shared memory bank conflicts, and efficient coalesced storage through shared memory.&lt;/p&gt; &lt;p&gt;The code is super easy to tweak, so you can customize it for your projects with kernel fusion or just drop it into your libraries as-is. Below, I've included performance comparisons against cuBLAS and Simon Boehm‚Äôs highly cited work, which is now integrated into llamafile aka tinyBLAS.&lt;/p&gt; &lt;p&gt;P.S. The next blog post will cover implementing HGEMM (FP16 GEMM) and HGEMV (FP16 Matrix-Vector Multiplication) on Tensor Cores achieving performance comparable to cuBLAS (or maybe even faster? let's see). If you enjoy educational content like this and would like to see more, please share the article. If you have any questions, feel free to comment or send me a direct message - I'd love to hear your feedback and answer any questions you may have!&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://salykova.github.io/sgemm-gpu"&gt;https://salykova.github.io/sgemm-gpu&lt;/a&gt;&lt;br /&gt; Code: &lt;a href="https://github.com/salykova/sgemm.cu"&gt;https://github.com/salykova/sgemm.cu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uq14ysfvamde1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d3998a50e61643c76e82ff048d1dd20703e3a65f"&gt;https://preview.redd.it/uq14ysfvamde1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d3998a50e61643c76e82ff048d1dd20703e3a65f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/salykova"&gt; /u/salykova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pup0/beating_cublas_in_sgemm_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pup0/beating_cublas_in_sgemm_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pup0/beating_cublas_in_sgemm_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T20:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3z6cb</id>
    <title>Grokking at the Edge of Numerical Stability</title>
    <updated>2025-01-18T04:01:09+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2501.04697"&gt;https://arxiv.org/abs/2501.04697&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Grokking, the sudden generalization that occurs after prolonged overfitting, is a surprising phenomenon challenging our understanding of deep learning. Although significant progress has been made in understanding grokking, the reasons behind the delayed generalization and its dependence on regularization remain unclear. In this work, we argue that without regularization, grokking tasks push models to the edge of numerical stability, introducing floating point errors in the Softmax function, which we refer to as Softmax Collapse (SC). We demonstrate that SC prevents grokking and that mitigating SC enables grokking without regularization. Investigating the root cause of SC, we find that beyond the point of overfitting, the gradients strongly align with what we call the na√Øve loss minimization (NLM) direction. This component of the gradient does not alter the model's predictions but decreases the loss by scaling the logits, typically by scaling the weights along their current direction. We show that this scaling of the logits explains the delay in generalization characteristic of grokking and eventually leads to SC, halting further learning. To validate our hypotheses, we introduce two key contributions that address the challenges in grokking tasks: StableMax, a new activation function that prevents SC and enables grokking without regularization, and ‚ä•Grad, a training algorithm that promotes quick generalization in grokking tasks by preventing NLM altogether. These contributions provide new insights into grokking, elucidating its delayed generalization, reliance on regularization, and the effectiveness of existing grokking-inducing methods. Code for this paper is available at this https URL.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3z6cb/grokking_at_the_edge_of_numerical_stability/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3z6cb/grokking_at_the_edge_of_numerical_stability/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3z6cb/grokking_at_the_edge_of_numerical_stability/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T04:01:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4cqm2</id>
    <title>Why can't i find material on how to fine-tune a local llama?</title>
    <updated>2025-01-18T17:36:53+00:00</updated>
    <author>
      <name>/u/Blender-Fan</name>
      <uri>https://old.reddit.com/user/Blender-Fan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried and tried, but every webpage i got just told me on &amp;quot;what&amp;quot; should i do, not &amp;quot;how&amp;quot;, and youtube was &lt;strong&gt;even worse&lt;/strong&gt;, they are always using .ipynb and google colab, running the stuff on the cloud. I have my goddamn llama, why'd i run the fine-tuning on the cloud, let alone export the result? Theres gotta be something i'm missing, either that or the documentation is scarse. Which imo it is because i hardly can find stuff like the documentation for the llama api, which i did &lt;a href="https://github.com/MatthewLacerda2/Jarvis/blob/main/main.py"&gt;like this&lt;/a&gt;. It was a bit difficult to find the fields i wanted to use, was a bit of trial-and-error&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Blender-Fan"&gt; /u/Blender-Fan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4cqm2/why_cant_i_find_material_on_how_to_finetune_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4cqm2/why_cant_i_find_material_on_how_to_finetune_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4cqm2/why_cant_i_find_material_on_how_to_finetune_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T17:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4cwro</id>
    <title>What is the best model of in context learning?</title>
    <updated>2025-01-18T17:44:18+00:00</updated>
    <author>
      <name>/u/henryclw</name>
      <uri>https://old.reddit.com/user/henryclw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fine-tuning is expensive, is it possible to have a model with great ability of in context learning and large context window to avoid some kind of simple fine-tuning?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/henryclw"&gt; /u/henryclw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4cwro/what_is_the_best_model_of_in_context_learning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4cwro/what_is_the_best_model_of_in_context_learning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4cwro/what_is_the_best_model_of_in_context_learning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T17:44:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4d7f3</id>
    <title>Success!: Tesla p40+1080GTX_Cooler in a Dell T420 :)</title>
    <updated>2025-01-18T17:57:29+00:00</updated>
    <author>
      <name>/u/s0n1cm0nk3y</name>
      <uri>https://old.reddit.com/user/s0n1cm0nk3y</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4d7f3/success_tesla_p401080gtx_cooler_in_a_dell_t420/"&gt; &lt;img alt="Success!: Tesla p40+1080GTX_Cooler in a Dell T420 :)" src="https://b.thumbs.redditmedia.com/te1a0xEnqnHXgzGURJQZ5QsJ1nReFxta_RZWsEDD0xU.jpg" title="Success!: Tesla p40+1080GTX_Cooler in a Dell T420 :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First, the money shot: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gg6j4ea9isde1.png?width=1347&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84d77db8a83b5481621d22deeedb3636a6efa674"&gt;https://preview.redd.it/gg6j4ea9isde1.png?width=1347&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84d77db8a83b5481621d22deeedb3636a6efa674&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And yes, I'm aware my PERCs are a bit close, I'm brainstorming on that. So the approach I took was following &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hozg2h/comment/m4di1mw/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;advice from FullStackSensei&lt;/a&gt; I acquired a used &lt;a href="https://www.youtube.com/watch?v=AM--NTHFBlI&amp;amp;lc=Ugzrysx0dPl-yrbO1yp4AaABAg.ADCmLC_52VxADCn-nMSdT-"&gt;GTX1080 dell reference card &lt;/a&gt;with issues. As the only things I needed were the fan and cooler, I wasn't too worried about it being for parts. It took some minor modifications, to include using a dremel and an oscillating cutter:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8g0vc0h5jsde1.png?width=467&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02dc9789435917b0cf7fde5b109aec0157ba5569"&gt;https://preview.redd.it/8g0vc0h5jsde1.png?width=467&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02dc9789435917b0cf7fde5b109aec0157ba5569&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but as shown here, the temps are completely manageable, and the fan is barely blowing : &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4e7mwcdjjsde1.png?width=941&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3d397688349dcfd222da0ae2722529e4d9be958"&gt;https://preview.redd.it/4e7mwcdjjsde1.png?width=941&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3d397688349dcfd222da0ae2722529e4d9be958&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Parts you'll need:&lt;/p&gt; &lt;p&gt;Links omitted to make sure I'm following guideines.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU fan adapter cable (look for &amp;quot;PWM GPU fan adapter cable&amp;quot;)&lt;/li&gt; &lt;li&gt;Thermal pads of varying sizes&lt;/li&gt; &lt;li&gt;PWM Fan Controller (I used the Coolerguys 12v PWM thermostat model)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope this helps anyone having troubles like I was with all the 3d printed fan shrouds and their concern for noise.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/s0n1cm0nk3y"&gt; /u/s0n1cm0nk3y &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4d7f3/success_tesla_p401080gtx_cooler_in_a_dell_t420/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4d7f3/success_tesla_p401080gtx_cooler_in_a_dell_t420/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4d7f3/success_tesla_p401080gtx_cooler_in_a_dell_t420/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T17:57:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3xoyd</id>
    <title>What's the cheapest way to run Llama 3.x 8B class models with realtime-like (chatgpt speed) tokens per second?</title>
    <updated>2025-01-18T02:38:29+00:00</updated>
    <author>
      <name>/u/synexo</name>
      <uri>https://old.reddit.com/user/synexo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;fireworks.ai? spin up on runpod? build a home server?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/synexo"&gt; /u/synexo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3xoyd/whats_the_cheapest_way_to_run_llama_3x_8b_class/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3xoyd/whats_the_cheapest_way_to_run_llama_3x_8b_class/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3xoyd/whats_the_cheapest_way_to_run_llama_3x_8b_class/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T02:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4dbg3</id>
    <title>Guide: Easiest way to run any vLLM model on AWS with autoscaling (scale down to 0)</title>
    <updated>2025-01-18T18:02:13+00:00</updated>
    <author>
      <name>/u/tempNull</name>
      <uri>https://old.reddit.com/user/tempNull</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A lot of our customers have been finding our guide for vLLM deployment on their own private cloud super helpful. vLLM is super helpful and straightforward and provides the highest token throughput when compared against frameworks like LoRAX, TGI etc.&lt;/p&gt; &lt;p&gt;Please let me know your thoughts on whether the guide is helpful and has a positive contribution to your understanding of model deployments in general.&lt;/p&gt; &lt;p&gt;Find the guide here:- &lt;a href="https://tensorfuse.io/docs/guides/llama_guide"&gt;https://tensorfuse.io/docs/guides/llama_guide&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tempNull"&gt; /u/tempNull &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4dbg3/guide_easiest_way_to_run_any_vllm_model_on_aws/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4dbg3/guide_easiest_way_to_run_any_vllm_model_on_aws/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4dbg3/guide_easiest_way_to_run_any_vllm_model_on_aws/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T18:02:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3mybo</id>
    <title>LCLV: Real-time video analysis with Moondream 2B &amp; OLLama (open source, local). Anyone want a set up guide?</title>
    <updated>2025-01-17T18:21:33+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3mybo/lclv_realtime_video_analysis_with_moondream_2b/"&gt; &lt;img alt="LCLV: Real-time video analysis with Moondream 2B &amp;amp; OLLama (open source, local). Anyone want a set up guide?" src="https://external-preview.redd.it/MXZ5aHh4bWZpbGRlMSTqk2DOPEdgmnDyQ8guvDBrE8AyiWMeqDE4BRKGe_SG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ecd38999e371e083e545019f1eaf8d324a146b50" title="LCLV: Real-time video analysis with Moondream 2B &amp;amp; OLLama (open source, local). Anyone want a set up guide?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c3kcfymfilde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3mybo/lclv_realtime_video_analysis_with_moondream_2b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3mybo/lclv_realtime_video_analysis_with_moondream_2b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T18:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3o7a8</id>
    <title>I am open sourcing a smart text editor that runs completely in-browser using WebLLM + LLAMA (requires Chrome + WebGPU)</title>
    <updated>2025-01-17T19:14:56+00:00</updated>
    <author>
      <name>/u/yyjhao</name>
      <uri>https://old.reddit.com/user/yyjhao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3o7a8/i_am_open_sourcing_a_smart_text_editor_that_runs/"&gt; &lt;img alt="I am open sourcing a smart text editor that runs completely in-browser using WebLLM + LLAMA (requires Chrome + WebGPU)" src="https://external-preview.redd.it/MGt0ZzN4Y3NzbGRlMeSgvI1GdDqWZSs569grdhgwadhN-F5M6UL9TiNWoaqW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7c56ce3e01a5f41dcffc115930e49f7b1fee821" title="I am open sourcing a smart text editor that runs completely in-browser using WebLLM + LLAMA (requires Chrome + WebGPU)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yyjhao"&gt; /u/yyjhao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n3fmqwcsslde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3o7a8/i_am_open_sourcing_a_smart_text_editor_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3o7a8/i_am_open_sourcing_a_smart_text_editor_that_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T19:14:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3pexj</id>
    <title>DeepSeek-R1 (Preview) Benchmarked on LiveCodeBench</title>
    <updated>2025-01-17T20:06:47+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pexj/deepseekr1_preview_benchmarked_on_livecodebench/"&gt; &lt;img alt="DeepSeek-R1 (Preview) Benchmarked on LiveCodeBench" src="https://external-preview.redd.it/RiXxcULN7VvmAA8zRKm9Hg6sMZIuDEZ9SdZM3h7z4e0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c43191d847a8866681673c575cc88d8e702dd05" title="DeepSeek-R1 (Preview) Benchmarked on LiveCodeBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgur.com/a/WdpIkiy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pexj/deepseekr1_preview_benchmarked_on_livecodebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pexj/deepseekr1_preview_benchmarked_on_livecodebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T20:06:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i46hrp</id>
    <title>Qualcomm AI hub</title>
    <updated>2025-01-18T12:25:57+00:00</updated>
    <author>
      <name>/u/Big-Ad1693</name>
      <uri>https://old.reddit.com/user/Big-Ad1693</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/quic/ai-hub-models?tab=readme-ov-file"&gt;https://github.com/quic/ai-hub-models?tab=readme-ov-file&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I check every few months to see how things are going with the Snapdragon NPU, but I never find anything useful, until now&lt;/p&gt; &lt;p&gt;Maybe there are others out there who want to tinker a bit with Android and the NPU.&lt;/p&gt; &lt;p&gt;There also examples for Image Gen, LLM, whisper &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big-Ad1693"&gt; /u/Big-Ad1693 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i46hrp/qualcomm_ai_hub/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i46hrp/qualcomm_ai_hub/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i46hrp/qualcomm_ai_hub/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T12:25:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4bfpo</id>
    <title>-Nevoria- LLama 3.3 70b</title>
    <updated>2025-01-18T16:39:34+00:00</updated>
    <author>
      <name>/u/mentallyburnt</name>
      <uri>https://old.reddit.com/user/mentallyburnt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;TLDR: This is a merge focused on combining storytelling capabilities with detailed scene descriptions, while maintaining a balanced approach to maintain intelligence and useability and reducing positive bias. Currently ranked as the highest 70B on the UGI benchmark!&lt;/p&gt; &lt;p&gt;What went into this?&lt;/p&gt; &lt;p&gt;I took EVA-LLAMA 3.33 for its killer storytelling abilities and mixed it with EURYALE v2.3's detailed scene descriptions. Added Anubis v1 to enhance the prose details, and threw in some Negative_LLAMA to keep it from being too sunshine-and-rainbows. All this sitting on a Nemotron-lorablated base.&lt;/p&gt; &lt;p&gt;Subtracting the lorablated base during merging causes a &amp;quot;weight twisting&amp;quot; effect. If you've played with my previous Astoria models, you'll recognize this approach - it creates some really interesting balance in how the model responds.&lt;/p&gt; &lt;p&gt;As usual my goal is to keep the model Intelligent with a knack for storytelling and RP. &lt;/p&gt; &lt;p&gt;Benchmark Results:&lt;/p&gt; &lt;p&gt;- UGI Score: 56.75 (Currently #1 for 70B models and equal or better than 123b models!)&lt;/p&gt; &lt;p&gt;- Open LLM Average: 43.92% (while not as useful from people training on the questions, still useful)&lt;/p&gt; &lt;p&gt;- Solid scores across the board, especially in IFEval (69.63%) and BBH (56.60%)&lt;/p&gt; &lt;p&gt;Already got some quantized versions available: &lt;/p&gt; &lt;p&gt;Recommended template: LLam@ception by @.konnect&lt;/p&gt; &lt;p&gt;Check it out: &lt;a href="https://huggingface.co/Steelskull/L3.3-MS-Nevoria-70B"&gt;https://huggingface.co/Steelskull/L3.3-MS-Nevoria-70B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts and experiences with it! Your feedback helps make the next one even better. &lt;/p&gt; &lt;p&gt;Happy prompting! üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mentallyburnt"&gt; /u/mentallyburnt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4bfpo/nevoria_llama_33_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4bfpo/nevoria_llama_33_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4bfpo/nevoria_llama_33_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T16:39:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i48dmj</id>
    <title>Has anyone tried anything besides native Python to build Agents?</title>
    <updated>2025-01-18T14:14:04+00:00</updated>
    <author>
      <name>/u/QaeiouX</name>
      <uri>https://old.reddit.com/user/QaeiouX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know, it's a very common question around here to ask. Actually I am working a project and have been using simple python to build my agentic workflow. But as it is expanding, I am facing some issues on keeping up with it. I am planning to use some framework and Pydantic AI is on my radar. I am also interested by Bee Agent Framework but, it's written in typescript predominantly. If you have any other suggestions, please let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QaeiouX"&gt; /u/QaeiouX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i48dmj/has_anyone_tried_anything_besides_native_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i48dmj/has_anyone_tried_anything_besides_native_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i48dmj/has_anyone_tried_anything_besides_native_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T14:14:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4a2by</id>
    <title>Nuggt: Retrieve Information from the internet to be used as context for LLM (Open Source)</title>
    <updated>2025-01-18T15:36:46+00:00</updated>
    <author>
      <name>/u/Loya_3005</name>
      <uri>https://old.reddit.com/user/Loya_3005</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4a2by/nuggt_retrieve_information_from_the_internet_to/"&gt; &lt;img alt="Nuggt: Retrieve Information from the internet to be used as context for LLM (Open Source)" src="https://b.thumbs.redditmedia.com/QcFzNc573ISWHe0wjV3Yju32wsgz4dgJupoA0RiLPPc.jpg" title="Nuggt: Retrieve Information from the internet to be used as context for LLM (Open Source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/n6awgafpurde1.gif"&gt;Nuggt Demo GIF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; &lt;/p&gt; &lt;p&gt;We all understand that the quality of LLM output depends heavily on the context and prompt provided. For example, asking an LLM to generate a good blog article on a given topic (let's say &lt;em&gt;X&lt;/em&gt;) might result in a generic answer that may or may not meet your expectations. However, if you provide guidelines on how to write a good article and supply the LLM with additional relevant information about the topic, you significantly increase the chances of receiving a response that aligns with your needs.&lt;/p&gt; &lt;p&gt;With this in mind, I wanted to create a workspace that makes it easy to build and manage context for use with LLMs. I imagine there are many of us who might use LLMs in workflows similar to the following:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Task&lt;/strong&gt;: Let‚Äôs say you want to write an elevator pitch for your startup.&lt;br /&gt; &lt;strong&gt;Step 1&lt;/strong&gt;: Research how to write a good elevator pitch, then save the key points as context.&lt;br /&gt; &lt;strong&gt;Step 2&lt;/strong&gt;: Look up examples of effective elevator pitches and add these examples to your context.&lt;br /&gt; &lt;strong&gt;Step 3&lt;/strong&gt;: Pass this curated context to the LLM and ask it to craft an elevator pitch for your startup. Importantly, you expect transparency‚Äîensuring the LLM uses your provided context as intended and shows how it informed the output.&lt;/p&gt; &lt;p&gt;If you find workflows like this appealing, I think you‚Äôll enjoy this tool. Here are its key features:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;It integrates &lt;strong&gt;Tavily&lt;/strong&gt; and &lt;strong&gt;Firecrawl&lt;/strong&gt; to gather information on any topic from the internet.&lt;/li&gt; &lt;li&gt;You can highlight any important points, right-click, and save them as context.&lt;/li&gt; &lt;li&gt;You can pass this context to the LLM, which will use it to assist with your task. In its responses, the LLM will cite the relevant parts of the context so you can verify how your input was used and even trace it back to the original sources.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My hypothesis is that many of us would benefit from building strong context to complete our tasks. Of course, I could be wrong‚Äîperhaps this is just one of my idiosyncrasies, putting so much effort into creating detailed context! Who knows? The only way to find out is to post it here and see what the community thinks.&lt;/p&gt; &lt;p&gt;I‚Äôd love to hear your feedback!&lt;/p&gt; &lt;p&gt;Here is the github repo: &lt;a href="https://github.com/shoibloya/nuggt-research"&gt;https://github.com/shoibloya/nuggt-research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loya_3005"&gt; /u/Loya_3005 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4a2by/nuggt_retrieve_information_from_the_internet_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4a2by/nuggt_retrieve_information_from_the_internet_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4a2by/nuggt_retrieve_information_from_the_internet_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T15:36:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i46zfr</id>
    <title>Why can't LLMs be re-trained on the go with the conversation for infinite memory?</title>
    <updated>2025-01-18T12:56:46+00:00</updated>
    <author>
      <name>/u/freecodeio</name>
      <uri>https://old.reddit.com/user/freecodeio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm just trying to understand the technical limitations and is this something that's considered. &lt;/p&gt; &lt;p&gt;I think the context window should only exist for instructions, while maintaining an infinte memory. This could really put LLMs in the realms of writing a complete book series and effecively changing the world as w e know it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freecodeio"&gt; /u/freecodeio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i46zfr/why_cant_llms_be_retrained_on_the_go_with_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i46zfr/why_cant_llms_be_retrained_on_the_go_with_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i46zfr/why_cant_llms_be_retrained_on_the_go_with_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T12:56:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4cfpz</id>
    <title>Llama 3.2 1B Instruct ‚Äì What Are the Best Use Cases for Small LLMs?</title>
    <updated>2025-01-18T17:23:32+00:00</updated>
    <author>
      <name>/u/ThetaCursed</name>
      <uri>https://old.reddit.com/user/ThetaCursed</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4cfpz/llama_32_1b_instruct_what_are_the_best_use_cases/"&gt; &lt;img alt="Llama 3.2 1B Instruct ‚Äì What Are the Best Use Cases for Small LLMs?" src="https://preview.redd.it/tr0h9qvkdsde1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=237cce46c19ab8ed30310b8c79fcf688f233dcf2" title="Llama 3.2 1B Instruct ‚Äì What Are the Best Use Cases for Small LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThetaCursed"&gt; /u/ThetaCursed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tr0h9qvkdsde1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4cfpz/llama_32_1b_instruct_what_are_the_best_use_cases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4cfpz/llama_32_1b_instruct_what_are_the_best_use_cases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T17:23:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1i435so</id>
    <title>KoboldCpp 1.82 - Now supports OuteTTS v0.2+0.3 with speaker voice synthesis and XTTS/OpenAI speech API, TAESD for Flux &amp; SD3, multilingual whisper (plus RAG and WebSearch from v1.81)</title>
    <updated>2025-01-18T08:27:13+00:00</updated>
    <author>
      <name>/u/HadesThrowaway</name>
      <uri>https://old.reddit.com/user/HadesThrowaway</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey it's me Concedo, here again playing how-many-more-API-endpoints-can-koboldcpp-serve. &lt;/p&gt; &lt;p&gt;Today's release brings long awaited TTS support, which works on all versions of OuteTTS GGUFs including the newly released &lt;strong&gt;v0.3 500M and 1B&lt;/strong&gt; models. It also provides XTTS and OpenAI Speech compatible APIs, so it can work as a direct TTS drop-in for existing frontends that use those features. &lt;/p&gt; &lt;p&gt;There are also some pretty cool improvements, as well as many other features, so do check out the release notes if you haven't yet. Last release, we also added WebSearch and a simple browser based RAG, so check that out if you missed it. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/LostRuins/koboldcpp/releases"&gt;https://github.com/LostRuins/koboldcpp/releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HadesThrowaway"&gt; /u/HadesThrowaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i435so/koboldcpp_182_now_supports_outetts_v0203_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i435so/koboldcpp_182_now_supports_outetts_v0203_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i435so/koboldcpp_182_now_supports_outetts_v0203_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T08:27:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4awir</id>
    <title>Have you truly replaced paid models(chatgpt, Claude etc) with self hosted ollama or hugging face ?</title>
    <updated>2025-01-18T16:14:58+00:00</updated>
    <author>
      <name>/u/Economy-Fact-8362</name>
      <uri>https://old.reddit.com/user/Economy-Fact-8362</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been experimenting with locally hosted setups, but I keep finding myself coming back to ChatGPT for the ease and performance. For those of you who‚Äôve managed to fully switch, do you still use services like ChatGPT occasionally? Do you use both? &lt;/p&gt; &lt;p&gt;Also, what kind of GPU setup is really needed to get that kind of seamless experience? My 16GB VRAM feels pretty inadequate in comparison to what these paid models offer. Would love to hear your thoughts and setups...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy-Fact-8362"&gt; /u/Economy-Fact-8362 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4awir/have_you_truly_replaced_paid_modelschatgpt_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4awir/have_you_truly_replaced_paid_modelschatgpt_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4awir/have_you_truly_replaced_paid_modelschatgpt_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T16:14:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i457gp</id>
    <title>Intel should release a 24GB version of the Arc B580</title>
    <updated>2025-01-18T10:58:44+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i457gp/intel_should_release_a_24gb_version_of_the_arc/"&gt; &lt;img alt="Intel should release a 24GB version of the Arc B580" src="https://external-preview.redd.it/KNNit46prWlA2v7rjsUV6TaIPMXvtB72RAGA4ZyQjNE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ddd3f42144ca0c2a05d54cf349b57f74c2e13f0f" title="Intel should release a 24GB version of the Arc B580" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The B580 is already showing impressive performance for LLM inference, matching the RTX 3060 in Vulkan benchmarks (~36 tokens/sec on Qwen2 7B) while being more power efficient and $50 cheaper. But VRAM is the real bottleneck for running larger models locally.&lt;/p&gt; &lt;p&gt;With Intel's strong XMX matrix performance and the existing clamshell memory design validated in shipping docs, a 24GB variant is technically feasible. This would enable running 13B models quantized to 8-bit (most 13B models need ~14GB), existing models with larger context, etc.&lt;/p&gt; &lt;p&gt;It would have way better price/performance than RTX 4060 Ti 16GB, native Vulkan support without CUDA lock-in and more performance potential if OpenVINO is further optimized.&lt;/p&gt; &lt;p&gt;The regular B580's stellar price/performance ratio shows Intel can be aggressive on pricing. A ~$329 24GB variant would hit a sweet spot for local LLM enthusiasts building inference rigs.&lt;/p&gt; &lt;p&gt;This is Intel's chance to build mind- and marketshare among AI developers and enthusiasts who are tired of CUDA lock-in. They can grow a community around OpenVINO and their AI tooling. Every developer who builds with Intel's stack today helps their ecosystem forward. The MLPerf results show they have the performance - now they just need to get the hardware into developers' hands.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dec 16 '24: &lt;a href="https://www.pcgamer.com/hardware/graphics-cards/shipping-document-suggests-that-a-24-gb-version-of-intels-arc-b580-graphics-card-could-be-heading-to-market-though-not-for-gaming/"&gt;Shipping document suggests that a 24 GB version of Intel's Arc B580 graphics card could be heading to market, though not for gaming&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xaydqqjygqde1.png?width=691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d57bc47d8936ed555b725e7733a88541d20f6d8"&gt;https://preview.redd.it/xaydqqjygqde1.png?width=691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d57bc47d8936ed555b725e7733a88541d20f6d8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i457gp/intel_should_release_a_24gb_version_of_the_arc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i457gp/intel_should_release_a_24gb_version_of_the_arc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i457gp/intel_should_release_a_24gb_version_of_the_arc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T10:58:44+00:00</published>
  </entry>
</feed>
