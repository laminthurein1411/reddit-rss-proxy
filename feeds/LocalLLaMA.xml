<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-19T17:05:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1isfjvc</id>
    <title>You guys made my model trending on Hugging Face‚Äîso I dropped a 14B and 7B upgrade with better reasoning! UIGEN-T1.1 (with gguf)</title>
    <updated>2025-02-18T15:41:24+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isfjvc/you_guys_made_my_model_trending_on_hugging_faceso/"&gt; &lt;img alt="You guys made my model trending on Hugging Face‚Äîso I dropped a 14B and 7B upgrade with better reasoning! UIGEN-T1.1 (with gguf)" src="https://external-preview.redd.it/am14MjN6ZnEzeGplMdXCBGqa_ZAaRyIjjsCd7dXIDjyHMhPivqJqFI-7KUQp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa567cf4c858c035a4999d41fb73458b5162ecd2" title="You guys made my model trending on Hugging Face‚Äîso I dropped a 14B and 7B upgrade with better reasoning! UIGEN-T1.1 (with gguf)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/65qy23gq3xje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isfjvc/you_guys_made_my_model_trending_on_hugging_faceso/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isfjvc/you_guys_made_my_model_trending_on_hugging_faceso/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T15:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1isofoj</id>
    <title>My new game, Craft to Infinity, is an infinite craft-style RPG that runs entirely locally on your PC. Using Qwen 2.5 instruct 1.5B.</title>
    <updated>2025-02-18T21:35:21+00:00</updated>
    <author>
      <name>/u/Salt-Frosting-7930</name>
      <uri>https://old.reddit.com/user/Salt-Frosting-7930</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isofoj/my_new_game_craft_to_infinity_is_an_infinite/"&gt; &lt;img alt="My new game, Craft to Infinity, is an infinite craft-style RPG that runs entirely locally on your PC. Using Qwen 2.5 instruct 1.5B." src="https://external-preview.redd.it/N3o0bzNxaHV1eWplMYLfc7KrRYLtf8MU_N3qPFBaU60Tn2QznaGhFG2NBzjF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02a7e70081720aca16adc66a96076eb71d2e83ba" title="My new game, Craft to Infinity, is an infinite craft-style RPG that runs entirely locally on your PC. Using Qwen 2.5 instruct 1.5B." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salt-Frosting-7930"&gt; /u/Salt-Frosting-7930 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/so8xoqhuuyje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isofoj/my_new_game_craft_to_infinity_is_an_infinite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isofoj/my_new_game_craft_to_infinity_is_an_infinite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T21:35:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iserf9</id>
    <title>Deepseek R1 Distilled Models MMLU Pro Benchmarks</title>
    <updated>2025-02-18T15:07:30+00:00</updated>
    <author>
      <name>/u/RedditsBestest</name>
      <uri>https://old.reddit.com/user/RedditsBestest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iserf9/deepseek_r1_distilled_models_mmlu_pro_benchmarks/"&gt; &lt;img alt="Deepseek R1 Distilled Models MMLU Pro Benchmarks" src="https://preview.redd.it/s006z4fbnwje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=68c5d9bdcbc0635cebefe1ea4e81c89993146c77" title="Deepseek R1 Distilled Models MMLU Pro Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedditsBestest"&gt; /u/RedditsBestest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s006z4fbnwje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iserf9/deepseek_r1_distilled_models_mmlu_pro_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iserf9/deepseek_r1_distilled_models_mmlu_pro_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T15:07:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1it5qw9</id>
    <title>At a friend's company, they have developed their own "secure GPT", but all their company uses the Google Workplace Environment already. Are they cooked?</title>
    <updated>2025-02-19T13:42:13+00:00</updated>
    <author>
      <name>/u/hjras</name>
      <uri>https://old.reddit.com/user/hjras</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So my friend was telling me about AI implementation at his company the othe day. he said they developed their own &amp;quot;secure GPT&amp;quot; which while limited compared to flagship models can do basic tasks like OCR, translation, generate text, and query PDF/txt files. &lt;/p&gt; &lt;p&gt;But then he mentioned the whole company went full google a couple of years ago. From chromebooks to literally every single piece of sensitive or industry secret being in some google sheet or google doc file somewhere. &lt;/p&gt; &lt;p&gt;I wondered then if the employees couldn't just use the more recent Gemini models or the Google AI studio or NotebookLM since it's plausible these were already trained on their company data? Or is there some legal protection that prevents Alphabet from training AIs based on google workplace environment?&lt;/p&gt; &lt;p&gt;I just told my friend he's be better off running Deepseek R1 locally if he's that worried about security, but ofc it's still a considerable investment. What do y'all think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hjras"&gt; /u/hjras &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it5qw9/at_a_friends_company_they_have_developed_their/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it5qw9/at_a_friends_company_they_have_developed_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it5qw9/at_a_friends_company_they_have_developed_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T13:42:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1it9pte</id>
    <title>Tried out llama33-70b-instruct-abliterated-q2_K - Not Bad!</title>
    <updated>2025-02-19T16:33:29+00:00</updated>
    <author>
      <name>/u/LaughterOnWater</name>
      <uri>https://old.reddit.com/user/LaughterOnWater</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Windows 10, 64GB RAM, RTX 3090, Ryzen 7 5800X&lt;br /&gt; The Llama 3.3 quantized/ablated file is 26GB, so part of this gets managed in RAM.&lt;/p&gt; &lt;p&gt;Used LM Studio to download llama33-70b-instruct-abliterated-q2_K:latest. I don't use LM Studio as the engine because it's too slow, but it's search engine is superior to hugging face for drilling down to actual available models. It also shows models not available at the moment from ollama's site.&lt;/p&gt; &lt;p&gt;Ported it over to Ollama via commandline:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd D:\work\ai\lm-studio\models\bartowski\Llama-3.3-70B-Instruct-abliterated-GGUF FROM Llama-3.3-70B-Instruct-abliterated-Q2_K.gguf ollama create Llama-3.3-70B-Instruct-abliterated-Q2_K -f Modelfile &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Deleted the model from LM Studio because it's now superfluous.&lt;/p&gt; &lt;p&gt;Opened up Ollama with Open-Webui, selected llama 3.3 and started asking questions.&lt;/p&gt; &lt;p&gt;The below linked thread is the result. 27 replies to questions about ŒõCDM and additional concepts, averaging about 5tok/s, so it's fast enough that you can't read out loud before it finishes.&lt;/p&gt; &lt;p&gt;I'm frankly pretty amazed at the capabilities. I couldn't get it to fully hallucinate.&lt;/p&gt; &lt;p&gt;C.&lt;/p&gt; &lt;p&gt;Here's the thread.&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.google.com/document/d/1o40hrAkv_Dp-dNqJB43VINsFcnMT3_MQ27PAB5iFBO0/edit?usp=sharing"&gt;https://docs.google.com/document/d/1o40hrAkv_Dp-dNqJB43VINsFcnMT3_MQ27PAB5iFBO0/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The beginning of the thread below... We went pretty far afield after that.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;User&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Tell me a little about Lambda Cold Dark Matter, the theory, its controversies, and give a frank assessment of how much of it is actually verifiable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Helix&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Lambda Cold Dark Matter (ŒõCDM) is a leading cosmological model that attempts to explain the evolution and structure of the universe on large scales.&lt;/p&gt; &lt;p&gt;Overview:&lt;/p&gt; &lt;p&gt;The ŒõCDM model combines two key components:&lt;/p&gt; &lt;p&gt;Cold Dark Matter (CDM): A type of dark matter that interacts with normal matter only through gravity, making it invisible but detectable through its gravitational effects.&lt;/p&gt; &lt;p&gt;Lambda (Œõ): The cosmological constant, representing the energy density of the vacuum, which is thought to be responsible for the accelerating expansion of the universe.&lt;/p&gt; &lt;p&gt;Theoretical Framework:&lt;/p&gt; &lt;p&gt;The ŒõCDM model is based on the following assumptions:&lt;/p&gt; &lt;p&gt;The universe is homogeneous and isotropic on large scales.&lt;/p&gt; &lt;p&gt;General Relativity (GR) describes gravity.&lt;/p&gt; &lt;p&gt;The universe evolved from a very hot, dense state in the Big Bang.&lt;/p&gt; &lt;p&gt;Predictions and Successes:&lt;/p&gt; &lt;p&gt;Large-scale structure: ŒõCDM predicts the formation of galaxy clusters, superclusters, and the web-like structure of the universe.&lt;/p&gt; &lt;p&gt;Cosmic Microwave Background (CMB): The model accurately predicts the CMB's black-body spectrum and anisotropies.&lt;/p&gt; &lt;p&gt;Baryon Acoustic Oscillations (BAO): ŒõCDM agrees with observations of BAO, which are a signature of the universe's expansion history.&lt;/p&gt; &lt;p&gt;Controversies and Challenges:&lt;/p&gt; &lt;p&gt;Dark Matter: The existence of dark matter is inferred, but its nature remains unknown.&lt;/p&gt; &lt;p&gt;Dark Energy: The cosmological constant (Œõ) is not well understood, and its value is difficult to predict.&lt;/p&gt; &lt;p&gt;Tension between predictions and observations: Some observed features, such as the &amp;quot;galaxy rotation curve problem&amp;quot; or the &amp;quot;missing satellites&amp;quot; issue, are not fully explained by ŒõCDM.&lt;/p&gt; &lt;p&gt;Verifiability:&lt;/p&gt; &lt;p&gt;While the ŒõCDM model has been successful in explaining many large-scale phenomena, its verifiability is limited by:&lt;/p&gt; &lt;p&gt;Lack of direct detection: Dark matter and dark energy remain invisible, making it challenging to directly verify their existence.&lt;/p&gt; &lt;p&gt;Model dependencies: The interpretation of observations relies heavily on the underlying assumptions and the specific implementation of the ŒõCDM model.&lt;/p&gt; &lt;p&gt;Assessment:&lt;/p&gt; &lt;p&gt;The ŒõCDM model is a well-established framework for understanding the universe's evolution, but its limitations and controversies should be acknowledged:&lt;/p&gt; &lt;p&gt;Approximately 95% of the universe's mass-energy budget is composed of dark matter and dark energy, which are not directly observable.&lt;/p&gt; &lt;p&gt;Ongoing research: New observations, such as those from the Simons Observatory or the Square Kilometre Array, will help refine the model and potentially resolve some of the existing tensions.&lt;/p&gt; &lt;p&gt;In conclusion, while the ŒõCDM model has been successful in explaining many phenomena, its verifiability is limited by the lack of direct detection and the reliance on indirect inference. Ongoing research and new observations will continue to refine our understanding of the universe's evolution and structure.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;User&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Neil DeGrasse Tyson said that we should likely be calling it &amp;quot;Dark Gravity&amp;quot; rather than &amp;quot;Dark Matter&amp;quot; because we really don't know if some indeterminate form of matter is the cause of the additional gravity we measure. What are your thoughts on this?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Helix&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Understanding Dark Matter and Dark Gravity&lt;/p&gt; &lt;p&gt;The concept of dark matter has been a subject of interest in astrophysics, with scientists like Neil DeGrasse Tyson proposing alternative explanations for the observed phenomena. One such proposal is to refer to it as &amp;quot;Dark Gravity&amp;quot; instead of &amp;quot;Dark Matter&amp;quot;. This idea is based on the notion that the effects attributed to dark matter could be a manifestation of a new type of gravity or a modification of the existing gravitational theory.&lt;/p&gt; &lt;p&gt;Rationale behind Dark Gravity&lt;/p&gt; &lt;p&gt;The term &amp;quot;Dark Gravity&amp;quot; is motivated by the fact that the observed effects are primarily gravitational in nature. The rotation curves of galaxies and the large-scale structure of the universe can be explained by an additional, unseen form of mass. However, it's possible that this effect is not due to a new type of matter, but rather a consequence of a more complex or modified gravity theory.&lt;/p&gt; &lt;p&gt;Key Considerations&lt;/p&gt; &lt;p&gt;Gravitational effects : The observed phenomena are indeed gravitational in nature.&lt;/p&gt; &lt;p&gt;Modified gravity theories : Some alternative theories, such as MOND (Modified Newtonian Dynamics) or TeVeS (Tensor-Vector-Scalar), have been proposed to explain the observed effects without invoking dark matter.&lt;/p&gt; &lt;p&gt;Lack of direct detection : Despite extensive searches, there is no direct evidence for dark matter particles.&lt;/p&gt; &lt;p&gt;Implications and Future Directions&lt;/p&gt; &lt;p&gt;Reevaluating assumptions : Considering &amp;quot;Dark Gravity&amp;quot; as an alternative explanation encourages a reevaluation of the underlying assumptions in cosmology and astrophysics.&lt;/p&gt; &lt;p&gt;New areas of research : The concept of &amp;quot;Dark Gravity&amp;quot; might lead to new areas of investigation, such as exploring modified gravity theories or searching for alternative explanations.&lt;/p&gt; &lt;p&gt;Conclusion&lt;/p&gt; &lt;p&gt;In conclusion, the idea of referring to dark matter as &amp;quot;Dark Gravity&amp;quot; is an intriguing one that challenges our current understanding of the universe's structure and evolution. While it's essential to acknowledge the possibility of alternative explanations, it's equally important to recognize that the standard model of cosmology, based on general relativity and the existence of dark matter, remains a well-established framework for understanding the universe.&lt;/p&gt; &lt;p&gt;Final Thoughts&lt;/p&gt; &lt;p&gt;The concept of &amp;quot;Dark Gravity&amp;quot; serves as a reminder of the importance of questioning assumptions and exploring new ideas in scientific inquiry. By considering alternative explanations, we can refine our understanding of the universe and potentially uncover new insights into its workings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;User&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Is gravity a wave, a particle or both? Does it abide by similar concepts observed for other phenomena at this level?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Helix&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Gravity: Wave or Particle?&lt;/p&gt; &lt;p&gt;The nature of gravity is a complex and multifaceted topic. According to our current understanding, gravity is mediated by a hypothetical particle called the graviton , which is thought to be a massless, spin-2 particle. However, gravity also exhibits wave-like behavior, as evidenced by the observation of gravitational waves .&lt;/p&gt; &lt;p&gt;Gravitational Waves&lt;/p&gt; &lt;p&gt;Gravitational waves are ripples in the fabric of spacetime that are produced by the acceleration of massive objects, such as black holes or neutron stars. These waves were first predicted by Einstein's theory of general relativity and were only recently directly detected by the Laser Interferometer Gravitational-Wave Observatory (LIGO) and Virgo Collaborations.&lt;/p&gt; &lt;p&gt;Wave-Particle Duality&lt;/p&gt; &lt;p&gt;The wave-particle duality is a fundamental concept in quantum mechanics, where particles, such as electrons or photons, can exhibit both wave-like and particle-like behavior depending on how they are observed. Gravity, as a force, seems to follow a similar pattern:&lt;/p&gt; &lt;p&gt;Gravitational waves : Exhibit wave-like behavior, with characteristics like frequency, amplitude, and polarization.&lt;/p&gt; &lt;p&gt;Gravitons : Hypothetical particles that mediate the gravitational force, which would exhibit particle-like behavior.&lt;/p&gt; &lt;p&gt;Similarities with Other Phenomena&lt;/p&gt; &lt;p&gt;Gravity shares similarities with other phenomena at this level:&lt;/p&gt; &lt;p&gt;Electromagnetism : Like gravity, electromagnetism is a fundamental force that exhibits both wave-like (photons) and particle-like behavior.&lt;/p&gt; &lt;p&gt;Quantum Mechanics : Gravity, like other forces, is expected to be subject to the principles of quantum mechanics, including wave-particle duality.&lt;/p&gt; &lt;p&gt;Open Questions&lt;/p&gt; &lt;p&gt;While our understanding of gravity has advanced significantly, there are still open questions:&lt;/p&gt; &lt;p&gt;Graviton detection : Direct detection of gravitons remains an ongoing challenge.&lt;/p&gt; &lt;p&gt;Quantum Gravity : Developing a theory that merges quantum mechanics and general relativity is an active area of research.&lt;/p&gt; &lt;p&gt;Gravity's Nature : The exact nature of gravity, whether it is a wave, particle, or both, remains a topic of investigation and debate.&lt;/p&gt; &lt;p&gt;Conclusion&lt;/p&gt; &lt;p&gt;In conclusion, gravity appears to exhibit both wave-like and particle-like behavior, similar to other phenomena at this level. However, the exact nature of gravity remains an open question, with ongoing research aimed at resolving the mysteries surrounding this fundamental force.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;User&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Okay, we've been able to measure gravitic waves, but not yet particles. I suppose a key to determining if particles even exist is to determine if wave amplitudes are quantized. Is there evidence in our peer-reviewed journals that suggest gravitic wave amplitudes are quantized? What spectrum would we use to measure quantization?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Helix&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Quantization of Gravitational Wave Amplitudes&lt;/p&gt; &lt;p&gt;The search for quantization of gravitational wave amplitudes is an active area of research. While there is no direct evidence for the existence of gravitons, researchers have been exploring ways to indirectly detect their presence through the analysis of gravitational wave signals.&lt;/p&gt; &lt;p&gt;Current State of Research&lt;/p&gt; &lt;p&gt;Peer-reviewed journals have published studies on the potential detection of gravitational wave quantization, but these findings are often based on theoretical models and simulations rather than direct observations. For example:&lt;/p&gt; &lt;p&gt;Gravitational wave astronomy : Studies have proposed methods to detect graviton-mediated effects in gravitational wave signals, such as [1] and [2].&lt;/p&gt; &lt;p&gt;Quantum gravity phenomenology : Research has explored the potential signatures of quantum gravity in gravitational wave observations, including [3] and [4].&lt;/p&gt; &lt;p&gt;Measuring Quantization&lt;/p&gt; &lt;p&gt;To measure quantization, researchers would need to analyze the spectrum of gravitational waves. The most relevant spectrum for this purpose is likely to be:&lt;/p&gt; &lt;p&gt;Power spectral density (PSD) : This spectrum characterizes the distribution of power across different frequencies in the gravitational wave signal.&lt;/p&gt; &lt;p&gt;Waveform spectra : These spectra describe the shape and structure of the gravitational wave waveform, which could potentially reveal quantization effects.&lt;/p&gt; &lt;p&gt;Challenges and Limitations&lt;/p&gt; &lt;p&gt;However, there are significant challenges and limitations to consider:&lt;/p&gt; &lt;p&gt;Noise and instrumental artifacts : Gravitational wave detectors are prone to noise and instrumental artifacts that can mimic or obscure potential signs of quantization.&lt;/p&gt; &lt;p&gt;Limited sensitivity : Current gravitational wave detectors may not have the necessary sensitivity to detect the subtle effects of graviton-mediated quantization.&lt;/p&gt; &lt;p&gt;Theoretical uncertainties : Theoretical models for quantum gravity and graviton physics are still in development, making it difficult to predict exact signatures or spectra.&lt;/p&gt; &lt;p&gt;Future Directions&lt;/p&gt; &lt;p&gt;To move forward, researchers will need to:&lt;/p&gt; &lt;p&gt;Improve detector sensitivity : Enhance the sensitivity of gravitational wave detectors to increase the chances of detecting subtle effects.&lt;/p&gt; &lt;p&gt;Develop new analysis techniques : Create novel methods for analyzing gravitational wave data that can better reveal potential signs of quantization.&lt;/p&gt; &lt;p&gt;Refine theoretical models : Continue developing and refining theoretical models for quantum gravity and graviton physics to provide more accurate predictions.&lt;/p&gt; &lt;p&gt;References&lt;/p&gt; &lt;p&gt;[1] arXiv:1904.06245&lt;/p&gt; &lt;p&gt;[2] Physical Review D 102, 024001 (2020)&lt;/p&gt; &lt;p&gt;[3] Journal of Cosmology and Astroparticle Physics 2020, no. 11 (2020)&lt;/p&gt; &lt;p&gt;[4] Classical and Quantum Gravity 38, no. 15 (2021)&lt;/p&gt; &lt;p&gt;Please note that the references provided are fictional and used only for illustration purposes.&lt;/p&gt; &lt;p&gt;&lt;em&gt;[ user note: The first three articles actually exist. I couldn't verify the fourth.]&lt;/em&gt;&lt;/p&gt; &lt;p&gt;In conclusion, while there is no direct evidence for quantization of gravitational wave amplitudes, researchers continue to explore ways to indirectly detect potential signs of graviton-mediated effects in gravitational wave signals. Further advancements in detector sensitivity, analysis techniques, and theoretical models will be crucial for making progress in this area.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;User&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;So in essence, we don't even have the tools to measure quantization of gravitic waves.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Helix&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Current Limitations&lt;/p&gt; &lt;p&gt;You've accurately summarized the current state of research. We don't yet have the tools or sensitivity to directly measure the quantization of gravitational wave amplitudes. The detection of gravitational waves is a significant achievement, but exploring their quantization requires further advancements in detector technology and analysis techniques.&lt;/p&gt; &lt;p&gt;... see link above for the rest ...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LaughterOnWater"&gt; /u/LaughterOnWater &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it9pte/tried_out_llama3370binstructabliteratedq2_k_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it9pte/tried_out_llama3370binstructabliteratedq2_k_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it9pte/tried_out_llama3370binstructabliteratedq2_k_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T16:33:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1is7yei</id>
    <title>DeepSeek is still cooking</title>
    <updated>2025-02-18T08:20:59+00:00</updated>
    <author>
      <name>/u/FeathersOfTheArrow</name>
      <uri>https://old.reddit.com/user/FeathersOfTheArrow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is7yei/deepseek_is_still_cooking/"&gt; &lt;img alt="DeepSeek is still cooking" src="https://preview.redd.it/ikhcif5gxuje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=659543f3bf6bd7985e0d2e63418ae9c0ba570196" title="DeepSeek is still cooking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Babe wake up, a new Attention just dropped&lt;/p&gt; &lt;p&gt;Sources: &lt;a href="https://x.com/deepseek_ai/status/1891745487071609327"&gt;Tweet&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2502.11089"&gt;Paper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeathersOfTheArrow"&gt; /u/FeathersOfTheArrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ikhcif5gxuje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is7yei/deepseek_is_still_cooking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is7yei/deepseek_is_still_cooking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T08:20:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1it0ith</id>
    <title>Dans-PersonalityEngine-24b</title>
    <updated>2025-02-19T08:13:32+00:00</updated>
    <author>
      <name>/u/PocketDocLabs</name>
      <uri>https://old.reddit.com/user/PocketDocLabs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The largest model I've released under the personality engine name in over a year. With any luck it should retain the mold-able nature of the 12b model while lending additional nuance to your generations.&lt;/p&gt; &lt;p&gt;This version of the dataset has been packed with over 3x the literature and some unique datasets previously tested through the Sakura Kaze and Dangerous Winds series models.&lt;/p&gt; &lt;p&gt;Aside from packing more in I also went through and pruned some of the older less potent data that I feel the model no longer benefits from.&lt;/p&gt; &lt;p&gt;The composition of the training set is constantly in flux and if you have a problem, idea, or even something as intangible as a pipe dream speak up and It might end up enhancing the next release!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/PocketDoc/Dans-PersonalityEngine-V1.2.0-24b"&gt;Model weights available on HF in this repo along with some additional information.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PocketDocLabs"&gt; /u/PocketDocLabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it0ith/danspersonalityengine24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it0ith/danspersonalityengine24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it0ith/danspersonalityengine24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T08:13:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1it9vkz</id>
    <title>BEST hardware for local LLMs</title>
    <updated>2025-02-19T16:39:56+00:00</updated>
    <author>
      <name>/u/kintrith</name>
      <uri>https://old.reddit.com/user/kintrith</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are some of the best hardware choices for running LLMs locally? 3080s? 5090s? Mac Mini's? NVIDIA DIGITS? P40s?&lt;/p&gt; &lt;p&gt;For my use case I'm looking to be able to run state of the art models like r1-1776 at high speeds. Budget is around $3-4k.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kintrith"&gt; /u/kintrith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it9vkz/best_hardware_for_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it9vkz/best_hardware_for_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it9vkz/best_hardware_for_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T16:39:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ise5ly</id>
    <title>Speed up downloading Hugging Face models by 100x</title>
    <updated>2025-02-18T14:40:25+00:00</updated>
    <author>
      <name>/u/alew3</name>
      <uri>https://old.reddit.com/user/alew3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure this is common knowledge, so sharing it here.&lt;/p&gt; &lt;p&gt;You may have noticed HF downloads caps at around 10.4MB/s (at least for me).&lt;/p&gt; &lt;p&gt;But if you install hf_transfer, which is written in Rust, you get uncapped speeds! I'm getting speeds of over &amp;gt; 1GB/s, and this saves me so much time!&lt;/p&gt; &lt;p&gt;Edit: The 10.4MB limitation I‚Äôm getting is not related to Python. Probably a bandwidth limit that doesn‚Äôt exist when using hf_transfer. &lt;/p&gt; &lt;p&gt;Edit2: To clarify, I get this cap of 10.4MB/s when downloading a model with command line Python. When I download via the website I get capped at around +-40MB/s. When I enable hf_transfer I get over 1GB/s. &lt;/p&gt; &lt;p&gt;Here is the step by step process to do it:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Install the HuggingFace CLI pip install -U &amp;quot;huggingface_hub[cli]&amp;quot; # Install hf_transfer for blazingly fast speeds pip install hf_transfer # Login to your HF account huggingface-cli login # Now you can download any model with uncapped speeds HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download &amp;lt;model-id&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alew3"&gt; /u/alew3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ise5ly/speed_up_downloading_hugging_face_models_by_100x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ise5ly/speed_up_downloading_hugging_face_models_by_100x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ise5ly/speed_up_downloading_hugging_face_models_by_100x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T14:40:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iskrsp</id>
    <title>Quantized DeepSeek R1 Distill Model With Original Model Accuracy</title>
    <updated>2025-02-18T19:09:29+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iskrsp/quantized_deepseek_r1_distill_model_with_original/"&gt; &lt;img alt="Quantized DeepSeek R1 Distill Model With Original Model Accuracy" src="https://external-preview.redd.it/jZumEOo5cwGIYKUmVlgM_-JZx8lE5uOoa94hgKEcVik.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=30d829ba9aedf7b3112605866862b09d48061c3c" title="Quantized DeepSeek R1 Distill Model With Original Model Accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We all love DeepSeek R1 Distill models. It can solve BrainTeaser Question with only 1.5B parameters, which normal 3B model cannot do. However, quantized DeepSeek-R1-Distill models often lose up to 22% accuracy, making it not as useful. &lt;strong&gt;We‚Äôve solved the trade-off with NexaQuant, compressing DeepSeek R1 Distill models to 1/4 of their original size (4 bit) while maintaining original accuracy.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;We open sourced NexaQuant DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Llama-8B on Hugging Face:&lt;/p&gt; &lt;p&gt;ü§ó Llama8B &lt;a href="https://huggingface.co/NexaAIDev/DeepSeek-R1-Distill-Llama-8B-NexaQuant"&gt;https://huggingface.co/NexaAIDev/DeepSeek-R1-Distill-Llama-8B-NexaQuant&lt;/a&gt;&lt;br /&gt; ü§ó Qwen1.5B &lt;a href="https://huggingface.co/NexaAIDev/DeepSeek-R1-Distill-Qwen-1.5B-NexaQuant"&gt;https://huggingface.co/NexaAIDev/DeepSeek-R1-Distill-Qwen-1.5B-NexaQuant&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;They are compatible with your favorite llama.cpp ‚ù§Ô∏è based projects: Ollama, LMStudio, Jan AI, AnythingLLM, Nexa-SDK, and more. Try them out now and let us know what you think!&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Benchmarks&lt;/h1&gt; &lt;p&gt;Full Blog &amp;amp; Benchmarks: &lt;a href="https://nexa.ai/blogs/deepseek-r1-nexaquant"&gt;https://nexa.ai/blogs/deepseek-r1-nexaquant&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7pzzri9n4yje1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=83ebd1b8b16ddb550a36daf090ccebc161473d66"&gt;https://preview.redd.it/7pzzri9n4yje1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=83ebd1b8b16ddb550a36daf090ccebc161473d66&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;NexaQuant Use Case Demo&lt;/h1&gt; &lt;p&gt;Here‚Äôs a comparison of how a standard Q4_K_M and NexaQuant-4Bit handle a common investment banking brain teaser question. NexaQuant excels in accuracy while shrinking the model file size by 4 times.&lt;/p&gt; &lt;p&gt;Prompt: A Common Investment Banking BrainTeaser Question&lt;/p&gt; &lt;p&gt;There is a 6x8 rectangular chocolate bar made up of small 1x1 bits. We want to break it into the 48 bits. We can break one piece of chocolate horizontally or vertically, but cannot break two pieces together! What is the minimum number of breaks required?&lt;/p&gt; &lt;p&gt;Right Answer: 47&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mfa8ocfw4yje1.png?width=2112&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09f0fab5aaa6ecba1b33405cf80265f31ddcecad"&gt;https://preview.redd.it/mfa8ocfw4yje1.png?width=2112&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09f0fab5aaa6ecba1b33405cf80265f31ddcecad&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iskrsp/quantized_deepseek_r1_distill_model_with_original/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iskrsp/quantized_deepseek_r1_distill_model_with_original/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iskrsp/quantized_deepseek_r1_distill_model_with_original/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T19:09:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1it8m6k</id>
    <title>Hugging Face open sourced the first course on FINE-TUNING for AGENTS</title>
    <updated>2025-02-19T15:49:36+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you follow these two hugging face courses you get an end to end programming in fine-tuning models specifically for agents.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;New Supervised Fine-tuning unit in the NLP Course, for general sft knowledge.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;New Fine-tuning for agents bonus module in the Agents Course, for agent specific stuff.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Links in this post &lt;a href="https://huggingface.co/posts/burtenshaw/189514834246661"&gt;https://huggingface.co/posts/burtenshaw/189514834246661&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it8m6k/hugging_face_open_sourced_the_first_course_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it8m6k/hugging_face_open_sourced_the_first_course_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it8m6k/hugging_face_open_sourced_the_first_course_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T15:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1islqfc</id>
    <title>DeepSeek GPU smuggling probe shows Nvidia's Singapore GPU sales are 28% of its revenue, but only 1% are delivered to the country: Report</title>
    <updated>2025-02-18T19:47:57+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1islqfc/deepseek_gpu_smuggling_probe_shows_nvidias/"&gt; &lt;img alt="DeepSeek GPU smuggling probe shows Nvidia's Singapore GPU sales are 28% of its revenue, but only 1% are delivered to the country: Report" src="https://external-preview.redd.it/hJ3pkUjJF7yX73X9K7Oc8CgPe7fBFkAVbL8oSeXP3AI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62828968e35d7fbd2b70d2aa440fcdeb80a5d7ce" title="DeepSeek GPU smuggling probe shows Nvidia's Singapore GPU sales are 28% of its revenue, but only 1% are delivered to the country: Report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/tech-industry/deepseek-gpu-smuggling-probe-shows-nvidias-singapore-gpu-sales-are-28-percent-of-its-revenue-but-only-1-percent-are-delivered-to-the-country-report"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1islqfc/deepseek_gpu_smuggling_probe_shows_nvidias/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1islqfc/deepseek_gpu_smuggling_probe_shows_nvidias/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T19:47:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1issbzc</id>
    <title>MoonshotAI release 10m Mixture of Block Attention for Long-Context LLMs, longer than deepseek's NSA</title>
    <updated>2025-02-19T00:43:49+00:00</updated>
    <author>
      <name>/u/No_Assistance_7508</name>
      <uri>https://old.reddit.com/user/No_Assistance_7508</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/MoonshotAI/MoBA"&gt;https://github.com/MoonshotAI/MoBA&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Assistance_7508"&gt; /u/No_Assistance_7508 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1issbzc/moonshotai_release_10m_mixture_of_block_attention/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1issbzc/moonshotai_release_10m_mixture_of_block_attention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1issbzc/moonshotai_release_10m_mixture_of_block_attention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T00:43:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1it96ex</id>
    <title>Managed to have a local LLaMA on my desktop, what now?</title>
    <updated>2025-02-19T16:12:08+00:00</updated>
    <author>
      <name>/u/CornerLimits</name>
      <uri>https://old.reddit.com/user/CornerLimits</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm an electronic engineer and for work I code and design both IC and PCBs. Yesterday I got Deepseek R1-14b running on my 6800XT-16GB and I‚Äôm pretty happy with that! &lt;/p&gt; &lt;p&gt;What do you use your local LLM for? &lt;/p&gt; &lt;p&gt;I feel I have a powerful tool in my hands now but I don‚Äôt know how to make it productive in some way.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CornerLimits"&gt; /u/CornerLimits &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it96ex/managed_to_have_a_local_llama_on_my_desktop_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it96ex/managed_to_have_a_local_llama_on_my_desktop_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it96ex/managed_to_have_a_local_llama_on_my_desktop_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T16:12:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1it46dv</id>
    <title>AMD mi300x deployment and tests.</title>
    <updated>2025-02-19T12:19:53+00:00</updated>
    <author>
      <name>/u/Shivacious</name>
      <uri>https://old.reddit.com/user/Shivacious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with system configurations to optimize the deployment of DeepSeek R1, focusing on enhancing throughput and response times. By fine-tuning the GIMM (GPU Interconnect Memory Management), I've achieved significant performance improvements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Throughput increase&lt;/strong&gt;: 30-40 tokens per second&lt;/li&gt; &lt;li&gt;&lt;strong&gt;With caching&lt;/strong&gt;: Up to 90 tokens per second for 20 concurrent 10k prompt requests&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;System Specifications&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Component&lt;/th&gt; &lt;th align="left"&gt;Details&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU&lt;/td&gt; &lt;td align="left"&gt;2x AMD EPYC 9664 (96 cores/192 threads each)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM&lt;/td&gt; &lt;td align="left"&gt;Approximately 2TB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU&lt;/td&gt; &lt;td align="left"&gt;8x AMD Instinct MI300X (connected via Infinity Fabric)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;analysis of gpu: &lt;a href="https://github.com/ShivamB25/analysis/blob/main/README.md"&gt;https://github.com/ShivamB25/analysis/blob/main/README.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do you guys want me to deploy any other model or make the endpoint public ? open to running it for a month.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shivacious"&gt; /u/Shivacious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it46dv/amd_mi300x_deployment_and_tests/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it46dv/amd_mi300x_deployment_and_tests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it46dv/amd_mi300x_deployment_and_tests/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T12:19:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ita5lk</id>
    <title>Microsoft‚Äôs Majorana 1 chip carves new path for quantum computing</title>
    <updated>2025-02-19T16:51:05+00:00</updated>
    <author>
      <name>/u/sapiensush</name>
      <uri>https://old.reddit.com/user/sapiensush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ita5lk/microsofts_majorana_1_chip_carves_new_path_for/"&gt; &lt;img alt="Microsoft‚Äôs Majorana 1 chip carves new path for quantum computing" src="https://b.thumbs.redditmedia.com/Ou_u9B88oC1UU7368rTjaK99wMvjoHzEkv0AirwbX4o.jpg" title="Microsoft‚Äôs Majorana 1 chip carves new path for quantum computing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft‚Äôs Majorana 1 Quantum Chip: Key Takeaways&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Breakthrough Quantum Architecture&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Microsoft introduced Majorana 1, the first quantum chip based on Topological Core architecture.&lt;/p&gt; &lt;p&gt;It uses topoconductors‚Äîa new type of material that enables stable and scalable qubits.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Scalability to One Million Qubits&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Microsoft aims to fit one million qubits on a single chip, a critical milestone for solving industrial-scale problems.&lt;/p&gt; &lt;p&gt;Unlike current quantum approaches, this design is more error-resistant and digitally controlled.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Majorana Particles &amp;amp; Topoconductors&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The chip relies on Majorana particles, which don‚Äôt exist naturally and had never been reliably created before.&lt;/p&gt; &lt;p&gt;Microsoft‚Äôs research confirms that they can now be created and measured, a major scientific milestone.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Applications &amp;amp; Potential Impact&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Quantum computing could revolutionize chemistry, materials science, and AI by:&lt;/p&gt; &lt;p&gt;Developing self-healing materials (e.g., bridges, aircraft parts, phone screens).&lt;/p&gt; &lt;p&gt;Creating enzymes to tackle global food shortages.&lt;/p&gt; &lt;p&gt;Designing catalysts to break down plastics and combat pollution.&lt;/p&gt; &lt;p&gt;The combination of quantum computing and AI could allow scientists to design perfect materials instantly.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Microsoft‚Äôs Quantum Roadmap&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The company has been working on topological qubits for nearly 20 years, despite challenges.&lt;/p&gt; &lt;p&gt;It has now reached the final phase of DARPA‚Äôs US2QC program to develop commercially viable quantum systems.&lt;/p&gt; &lt;p&gt;The technology is designed to integrate with Azure Quantum for hybrid computing solutions.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Engineering Challenges &amp;amp; Future Steps&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The chip is built using indium arsenide and aluminum at an atomic scale.&lt;/p&gt; &lt;p&gt;Microsoft is refining the materials and control systems to accelerate large-scale deployment.&lt;/p&gt; &lt;p&gt;Bottom Line&lt;/p&gt; &lt;p&gt;Microsoft‚Äôs Majorana 1 chip represents a major leap forward in quantum computing, offering a realistic path to solving complex global challenges within years, not decades.&lt;/p&gt; &lt;p&gt;&lt;a href="https://aka.ms/AAu76rr"&gt;https://aka.ms/AAu76rr&lt;/a&gt; - Summarised with ChatGPT&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sapiensush"&gt; /u/sapiensush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ita5lk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ita5lk/microsofts_majorana_1_chip_carves_new_path_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ita5lk/microsofts_majorana_1_chip_carves_new_path_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T16:51:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1isxyo9</id>
    <title>New LLM tech running on diffusion just dropped</title>
    <updated>2025-02-19T05:24:42+00:00</updated>
    <author>
      <name>/u/LorestForest</name>
      <uri>https://old.reddit.com/user/LorestForest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isxyo9/new_llm_tech_running_on_diffusion_just_dropped/"&gt; &lt;img alt="New LLM tech running on diffusion just dropped" src="https://external-preview.redd.it/P8PjCGtAk7UmVMdaNZLIptoXPYYcXmgBOsABEcRwX6E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9fe7f123e659e819d44d4238f56ef7e2825d7a43" title="New LLM tech running on diffusion just dropped" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claims to mitigate hallucinations unless you use it as a chat application.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LorestForest"&gt; /u/LorestForest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://timkellogg.me/blog/2025/02/17/diffusion"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isxyo9/new_llm_tech_running_on_diffusion_just_dropped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isxyo9/new_llm_tech_running_on_diffusion_just_dropped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T05:24:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1it912t</id>
    <title>SOCAMM is not a rumours anymore</title>
    <updated>2025-02-19T16:06:07+00:00</updated>
    <author>
      <name>/u/Cane_P</name>
      <uri>https://old.reddit.com/user/Cane_P</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kwak No-jung, CEO of SK Hynix, have confirmed that they are working on the next memory standard, that NVIDIA previously where rumoured to develop for DIGITS and their AI PC's:&lt;/p&gt; &lt;p&gt;President Kwak also mentioned SOCAMM, a next-generation memory that connects HBM and Compute Express Link (CXL). SOCAMM is drawing attention as Nvidia's new memory standard for AI PCs.&lt;/p&gt; &lt;p&gt;President Kwak said, &amp;quot;As semiconductor applications are diversifying, applications are also diversifying, not just in their past forms. (SOCAMM) is one of the trends of this change, and customers will comprehensively consider cost and performance.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.mk.co.kr/en/it/11245259"&gt;https://www.mk.co.kr/en/it/11245259&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The details that was leaked before, is that NVIDIA have teamed up with SK hynix, Micron and Samsung, to develop the new standard called System On Chip Advanced Memory Module (SOCAMM).&lt;/p&gt; &lt;p&gt;It is said to be more cost-effective when compared to traditional DRAM that uses the SO-DIMM form-factor, and that it may place LPDDR5X memory directly onto the substrate, offering further power efficiency.&lt;/p&gt; &lt;p&gt;It is reported to feature a significant number of I/O ports when compared to other stands. SOCAMM has up to 694 I/O ports, LPCAMM's have 644 and traditional DRAM's 260.&lt;/p&gt; &lt;p&gt;One reason for the lack of details is that it seems like NVIDIA isn't making the standard in collaboration with the Joint Electron Device Engineering council (JEDEC).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cane_P"&gt; /u/Cane_P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it912t/socamm_is_not_a_rumours_anymore/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it912t/socamm_is_not_a_rumours_anymore/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it912t/socamm_is_not_a_rumours_anymore/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T16:06:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1it4aw9</id>
    <title>Audio chat model came out. Anyone tried it? One of the metrics is RP.</title>
    <updated>2025-02-19T12:26:57+00:00</updated>
    <author>
      <name>/u/a_beautiful_rhind</name>
      <uri>https://old.reddit.com/user/a_beautiful_rhind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it4aw9/audio_chat_model_came_out_anyone_tried_it_one_of/"&gt; &lt;img alt="Audio chat model came out. Anyone tried it? One of the metrics is RP." src="https://external-preview.redd.it/UqpDdVhhcFGOCCh36p2Wt1DEREyb0kimRstI82rsrWk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92d3600b91ab5534a05b7683e4e45f623c80e511" title="Audio chat model came out. Anyone tried it? One of the metrics is RP." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a_beautiful_rhind"&gt; /u/a_beautiful_rhind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/stepfun-ai/Step-Audio-Chat"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it4aw9/audio_chat_model_came_out_anyone_tried_it_one_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it4aw9/audio_chat_model_came_out_anyone_tried_it_one_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T12:26:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1it1xd7</id>
    <title>Train a Little(39M) Language Model</title>
    <updated>2025-02-19T09:55:23+00:00</updated>
    <author>
      <name>/u/RoyalMaterial9614</name>
      <uri>https://old.reddit.com/user/RoyalMaterial9614</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've started getting more into LLMs this year, looking for resources has always been easy as we can find blogs organizing everything into one place but simply understanding the model architecture is not enough to fully grasp how these models are trained. &lt;/p&gt; &lt;p&gt;As I couldn't find any code with recent architecture's implementation in one place, I've made my own.&lt;/p&gt; &lt;p&gt;My aim with this project is to help anyone who has basic understanding of transformer architectures but wants to train their own model from scratch with recent architectural changes. (I include the resources + my own notes along the way)&lt;/p&gt; &lt;p&gt;So this project is my effort for training a small language model i.e 39M parameter model from scratch that can converse well.&lt;/p&gt; &lt;p&gt;It was trained on 2xA100 for approx. 2.5 hours on ~8B tokens.&lt;/p&gt; &lt;p&gt;I plan to include everything in this project!!!!&lt;/p&gt; &lt;p&gt;Right now it includes a basic Llama-like architecture.&lt;/p&gt; &lt;p&gt;- RMSNorm instead of LayerNorm&lt;/p&gt; &lt;p&gt;- Rotary Positional Embedding instead of Absolute Positional Embedding&lt;/p&gt; &lt;p&gt;- SwiGLU activations instead of ReLU&lt;/p&gt; &lt;p&gt;- Grouped Query Attention instead of Multi-head Attention&lt;/p&gt; &lt;p&gt;- Implementation of KV cache&lt;/p&gt; &lt;p&gt;TODO inclues&lt;/p&gt; &lt;p&gt;- Finetuning using SFT and DPO&lt;/p&gt; &lt;p&gt;- Adding Mixture of Experts (MoE) architecture&lt;/p&gt; &lt;p&gt;- And much more&lt;/p&gt; &lt;p&gt;It would be great if anyone's is willing to contribute to this project.&lt;/p&gt; &lt;p&gt;Please find the project here: &lt;a href="https://github.com/CohleM/lilLM"&gt;https://github.com/CohleM/lilLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RoyalMaterial9614"&gt; /u/RoyalMaterial9614 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it1xd7/train_a_little39m_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it1xd7/train_a_little39m_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it1xd7/train_a_little39m_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T09:55:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iskklo</id>
    <title>PerplexityAI releases R1-1776, a DeepSeek-R1 finetune that removes Chinese censorship while maintaining reasoning capabilities</title>
    <updated>2025-02-18T19:01:54+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iskklo/perplexityai_releases_r11776_a_deepseekr1/"&gt; &lt;img alt="PerplexityAI releases R1-1776, a DeepSeek-R1 finetune that removes Chinese censorship while maintaining reasoning capabilities" src="https://external-preview.redd.it/fBZy2Z7nV_YeGpu-AjWKc9CfcCYUjRJwHSvwj2-4VbI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0eb7d3b3713b1cc2f605e5e87e0a44f39b650f01" title="PerplexityAI releases R1-1776, a DeepSeek-R1 finetune that removes Chinese censorship while maintaining reasoning capabilities" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/perplexity-ai/r1-1776"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iskklo/perplexityai_releases_r11776_a_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iskklo/perplexityai_releases_r11776_a_deepseekr1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T19:01:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1it0ocl</id>
    <title>R1-1776 Dynamic GGUFs by Unsloth</title>
    <updated>2025-02-19T08:24:48+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, we uploaded 2bit to 16bit GGUFs for R1-1776, Perplexity's new DeepSeek-R1 finetune that removes all censorship while maintaining reasoning capabilities: &lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF"&gt;https://huggingface.co/unsloth/r1-1776-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also upload Dynamic 2-bit, 3 and 4-bit versions and standard 3, 4, etc bit versions. The Dynamic 4-bit is even smaller than the medium one and achieves even higher accuracy. 1.58-bit and 1-bit will have to be done later as it relies on imatrix quants, which take more time.&lt;/p&gt; &lt;p&gt;Instructions to run the model are in the model card we provided. Do not forget about &lt;code&gt;&amp;lt;ÔΩúUserÔΩú&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;ÔΩúAssistantÔΩú&amp;gt;&lt;/code&gt; tokens! - Or use a chat template formatter. Also do not forget about &lt;code&gt;&amp;lt;think&amp;gt;\n&lt;/code&gt;! Prompt format: &lt;code&gt;&amp;quot;&amp;lt;ÔΩúUserÔΩú&amp;gt;Create a Flappy Bird game in Python.&amp;lt;ÔΩúAssistantÔΩú&amp;gt;&amp;lt;think&amp;gt;\n&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;You can also refer to our previous blog for 1.58-bit R1 GGUF for hints and results: &lt;a href="https://unsloth.ai/blog/r1-reasoning"&gt;https://unsloth.ai/blog/r1-reasoning&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;MoE Bits&lt;/th&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Disk Size&lt;/th&gt; &lt;th align="left"&gt;HF Link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;2-bit Dynamic&lt;/td&gt; &lt;td align="left"&gt;UD-Q2_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;211GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF/tree/main/UD-Q2_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3-bit Dynamic&lt;/td&gt; &lt;td align="left"&gt;UD-Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;298.8GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF/tree/main/UD-Q3_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4-bit Dynamic&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;377.1GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF/tree/main/UD-Q4_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2-bit extra small&lt;/td&gt; &lt;td align="left"&gt;Q2_K_XS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;206.1GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF/tree/main"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4-bit&lt;/td&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;405GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF/tree/main/Q4_K_M"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And you can find the rest like 6-bit, 8-bit etc on the model card. Happy running!&lt;/p&gt; &lt;p&gt;P.S. we have a new update coming very soon which you guys will absolutely love! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it0ocl/r11776_dynamic_ggufs_by_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it0ocl/r11776_dynamic_ggufs_by_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it0ocl/r11776_dynamic_ggufs_by_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T08:24:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1isxhoy</id>
    <title>New laptops with AMD chips have 128 GB unified memory (up to 96 GB of which can be assigned as VRAM)</title>
    <updated>2025-02-19T04:58:16+00:00</updated>
    <author>
      <name>/u/zxyzyxz</name>
      <uri>https://old.reddit.com/user/zxyzyxz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isxhoy/new_laptops_with_amd_chips_have_128_gb_unified/"&gt; &lt;img alt="New laptops with AMD chips have 128 GB unified memory (up to 96 GB of which can be assigned as VRAM)" src="https://external-preview.redd.it/rA82auRK9iN2YS0cZz7VCIDRx_3W3gxSA4bgLmUO0L8.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f52aaef3d5bfe41e70571f92b4c1d36cf9abcc18" title="New laptops with AMD chips have 128 GB unified memory (up to 96 GB of which can be assigned as VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zxyzyxz"&gt; /u/zxyzyxz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=IVbm2a6lVBo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isxhoy/new_laptops_with_amd_chips_have_128_gb_unified/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isxhoy/new_laptops_with_amd_chips_have_128_gb_unified/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T04:58:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1isu4un</id>
    <title>o3-mini won the poll! We did it guys!</title>
    <updated>2025-02-19T02:06:12+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isu4un/o3mini_won_the_poll_we_did_it_guys/"&gt; &lt;img alt="o3-mini won the poll! We did it guys!" src="https://preview.redd.it/ogpvvrth70ke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=441716ec57e99b756f365455cea717ed23f4f00b" title="o3-mini won the poll! We did it guys!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted a lot here yesterday to vote for the o3-mini. Thank you all!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ogpvvrth70ke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isu4un/o3mini_won_the_poll_we_did_it_guys/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isu4un/o3mini_won_the_poll_we_did_it_guys/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T02:06:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1it36b0</id>
    <title>Gemini 2.0 is shockingly good at transcribing audio with Speaker labels, timestamps to the second;</title>
    <updated>2025-02-19T11:18:31+00:00</updated>
    <author>
      <name>/u/philschmid</name>
      <uri>https://old.reddit.com/user/philschmid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it36b0/gemini_20_is_shockingly_good_at_transcribing/"&gt; &lt;img alt="Gemini 2.0 is shockingly good at transcribing audio with Speaker labels, timestamps to the second;" src="https://preview.redd.it/d3bl014yx2ke1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ddda4efa5606005673aaca2845b18430aa309c24" title="Gemini 2.0 is shockingly good at transcribing audio with Speaker labels, timestamps to the second;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philschmid"&gt; /u/philschmid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d3bl014yx2ke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it36b0/gemini_20_is_shockingly_good_at_transcribing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it36b0/gemini_20_is_shockingly_good_at_transcribing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T11:18:31+00:00</published>
  </entry>
</feed>
