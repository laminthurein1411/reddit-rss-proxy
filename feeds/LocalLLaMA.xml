<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-11T09:05:37+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hymyky</id>
    <title>LLM has trouble understanding tabular data (.csv) relationships with RAG</title>
    <updated>2025-01-11T03:28:46+00:00</updated>
    <author>
      <name>/u/ItsBlueSkyz</name>
      <uri>https://old.reddit.com/user/ItsBlueSkyz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I'm currently having trouble getting a LLM to understand relationships in a table using RAG. For example i have:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;age&lt;/th&gt; &lt;th align="center"&gt;name&lt;/th&gt; &lt;th align="right"&gt;favorite_color&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="center"&gt;alice&lt;/td&gt; &lt;td align="right"&gt;blue&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="center"&gt;bob&lt;/td&gt; &lt;td align="right"&gt;green&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="center"&gt;cesar&lt;/td&gt; &lt;td align="right"&gt;yellow&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And I'll query: &amp;quot;What is alice's favorite color?&amp;quot; And it might get it correct to be blue as it's the 1st entry. But if i start asking about the 20th row: &amp;quot;Who is aged 20?&amp;quot; It'll always get it wrong by not correctly associating the row.&lt;/p&gt; &lt;p&gt;I was wondering if anyone has had any better luck with getting LLMs to understand tables with RAG. Currently using llama 3.1-8b-instruct with ollama and haystack for RAG. One idea was to turn this into a text document and sentences such as &amp;quot;alice is 1 year old and her favorite color is blue.&amp;quot; But since i have quite a bit of data (~10k rows), I was wondering if anyone else had something better. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ItsBlueSkyz"&gt; /u/ItsBlueSkyz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hymyky/llm_has_trouble_understanding_tabular_data_csv/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hymyky/llm_has_trouble_understanding_tabular_data_csv/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hymyky/llm_has_trouble_understanding_tabular_data_csv/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T03:28:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyg0f5</id>
    <title>What is the best Text-to-speech model to run locally?</title>
    <updated>2025-01-10T21:51:20+00:00</updated>
    <author>
      <name>/u/fewsats</name>
      <uri>https://old.reddit.com/user/fewsats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think llama 4 will have multimodality (including audio input/output) but until then, what do people use for going from text to speech + how do they run it locally (Ollama does not support this kind of models does it?)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fewsats"&gt; /u/fewsats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyg0f5/what_is_the_best_texttospeech_model_to_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyg0f5/what_is_the_best_texttospeech_model_to_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyg0f5/what_is_the_best_texttospeech_model_to_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T21:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy60n8</id>
    <title>why is there no LMStudio/Msty/GPT4All type app that supports backends other than llama.cpp?</title>
    <updated>2025-01-10T14:48:48+00:00</updated>
    <author>
      <name>/u/gaspoweredcat</name>
      <uri>https://old.reddit.com/user/gaspoweredcat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im curious, ive heard that other backends, especially exllamav2 can be faster than llama.cpp in many cases especially when multiple cards or even multiple machines are running on it, model files are readily available so there is demand for it&lt;/p&gt; &lt;p&gt;yet any of the apps i find even ones that support a sort of pluggable backend generally offer llama.cpp cpu, llama.cpp metal, llama.cpp cuda, llama.cpp vulkan and thats it, exllama seems to only be supported by the often somewhat janky and not that great to use webUIs like oogabooga or LoLLMs &lt;/p&gt; &lt;p&gt;so my question is why not? are exllama and other backends really that difficult to implement that no one wants to even touch it, llama.cpp has LM studio, Msty, GPT4All, Jan, Jellybox, and several other options, some even support stablediffusion models but for text gen it seems no one wants to integrate it and i just wondered if theres a good reason most apps etc generally use llama.cpp over anything else &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gaspoweredcat"&gt; /u/gaspoweredcat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy60n8/why_is_there_no_lmstudiomstygpt4all_type_app_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy60n8/why_is_there_no_lmstudiomstygpt4all_type_app_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy60n8/why_is_there_no_lmstudiomstygpt4all_type_app_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T14:48:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyi24a</id>
    <title>Help With Chunking a PDF Textbook for GraphRAG Applications</title>
    <updated>2025-01-10T23:22:31+00:00</updated>
    <author>
      <name>/u/GapElectrical8507</name>
      <uri>https://old.reddit.com/user/GapElectrical8507</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm building a project where we're trying to create a physics course based off a knowledge graph database we're building. I've found a couple open source physics textbooks which I'm trying to essentially extract concepts, definitions, and equations out of to put in a knowledge graph that maps relationships between concepts, equations, and definitions. &lt;/p&gt; &lt;p&gt;However, I'm not sure how to get started on chunking the PDF to extract relevant information because it's a massive PDF, and because there's images, tables, and examples that are used throughout the textbook content when I'm just trying to extract concepts, definitions, and derivations of equations out of the PDF. Any help would be appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GapElectrical8507"&gt; /u/GapElectrical8507 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyi24a/help_with_chunking_a_pdf_textbook_for_graphrag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyi24a/help_with_chunking_a_pdf_textbook_for_graphrag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyi24a/help_with_chunking_a_pdf_textbook_for_graphrag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T23:22:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyrke5</id>
    <title>The ASRock Radeon RX 7900 XTX Creator</title>
    <updated>2025-01-11T08:27:26+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People building AI PCs with multiple GPUs on a budget love the RTX 3090 2-slot &amp;quot;Turbo&amp;quot;/&amp;quot;Aero&amp;quot;/&amp;quot;Classic&amp;quot; blower cards that pretty much disappeared from production shortly after the launch of the chip.&lt;/p&gt; &lt;p&gt;That's why i'm surprised these same people (hi!) aren't talking more about the ASRock Radeon RX 7900 XTX Creator card. It's a 2-slot blower card with a single fan. It's 1100€ new so 18% more expensive than the cheapest RX 7900 XTX cards. With a Threadripper mainboard you can easily stick four of these cards (96GB VRAM) into a large PC case without having to deal with PCIe port extenders that can cause instability.&lt;/p&gt; &lt;p&gt;Has someone already done this and want to share? How hard is it to get them cooled? Which case did you use? Which software is best for inferencing with multiple of these AMD GPUs? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyrke5/the_asrock_radeon_rx_7900_xtx_creator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyrke5/the_asrock_radeon_rx_7900_xtx_creator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyrke5/the_asrock_radeon_rx_7900_xtx_creator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T08:27:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1hys13h</id>
    <title>New Model from https://novasky-ai.github.io/ Sky-T1-32B-Preview, open-source reasoning model that matches o1-preview on popular reasoning and coding benchmarks — trained under $450!</title>
    <updated>2025-01-11T09:02:18+00:00</updated>
    <author>
      <name>/u/appakaradi</name>
      <uri>https://old.reddit.com/user/appakaradi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"&gt; &lt;img alt="New Model from https://novasky-ai.github.io/ Sky-T1-32B-Preview, open-source reasoning model that matches o1-preview on popular reasoning and coding benchmarks — trained under $450! " src="https://external-preview.redd.it/d-6wrohyuoqlKc4TV9mDxgh4ErmzgT4n7gTbj9xeln4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8734d59c4128e9b5f68dcc670051d2d7f3e7fe12" title="New Model from https://novasky-ai.github.io/ Sky-T1-32B-Preview, open-source reasoning model that matches o1-preview on popular reasoning and coding benchmarks — trained under $450! " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/NovaSkyAI/status/1877793041957933347"&gt;https://x.com/NovaSkyAI/status/1877793041957933347&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/64qbzi7pxbce1.png?width=1201&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc1a698cd51f4e6e2775d3117ca91f88253478df"&gt;https://preview.redd.it/64qbzi7pxbce1.png?width=1201&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc1a698cd51f4e6e2775d3117ca91f88253478df&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/appakaradi"&gt; /u/appakaradi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T09:02:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy8ii4</id>
    <title>freeact: A Lightweight Library for Code-Action Based Agents</title>
    <updated>2025-01-10T16:37:39+00:00</updated>
    <author>
      <name>/u/krasserm</name>
      <uri>https://old.reddit.com/user/krasserm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ii4/freeact_a_lightweight_library_for_codeaction/"&gt; &lt;img alt="freeact: A Lightweight Library for Code-Action Based Agents" src="https://external-preview.redd.it/LzADpwKw8mUc5MZG38ssaaBnDyw6elEdCSjne9inj0g.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=19b672149c7219c55d19b0e99adc3b4bbb5b158c" title="freeact: A Lightweight Library for Code-Action Based Agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! We just released &lt;a href="https://github.com/gradion-ai/freeact"&gt;freeact&lt;/a&gt;, a lightweight agent library that empowers language models to act as autonomous agents through executable &lt;strong&gt;code actions&lt;/strong&gt;. By enabling agents to express their actions directly in code rather than through constrained formats like JSON, freeact provides a flexible and powerful approach to solving complex, open-ended problems that require dynamic solution paths.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports dynamic installation and utilization of Python packages at runtime&lt;/li&gt; &lt;li&gt;Agents learn from feedback and store successful code actions as reusable skills in long-term memory&lt;/li&gt; &lt;li&gt;Skills can be interactively developed and refined in collaboration with freeact agents&lt;/li&gt; &lt;li&gt;Agents compose skills and any other Python modules to build increasingly sophisticated capabilities&lt;/li&gt; &lt;li&gt;Code actions are executed in &lt;a href="https://github.com/gradion-ai/ipybox"&gt;ipybox&lt;/a&gt;, a secure Docker + IPython sandbox that runs locally or remotely&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/gradion-ai/freeact"&gt;https://github.com/gradion-ai/freeact&lt;/a&gt;&lt;br /&gt; Evaluation: &lt;a href="https://gradion-ai.github.io/freeact/evaluation/"&gt;https://gradion-ai.github.io/freeact/evaluation/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We'd love to hear your feedback!&lt;/p&gt; &lt;p&gt;See it in action:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1hy8ii4/video/rs73092327ce1/player"&gt;https://reddit.com/link/1hy8ii4/video/rs73092327ce1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/krasserm"&gt; /u/krasserm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ii4/freeact_a_lightweight_library_for_codeaction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ii4/freeact_a_lightweight_library_for_codeaction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ii4/freeact_a_lightweight_library_for_codeaction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:37:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyhyoo</id>
    <title>Notate 1.0.3 - Lots of platform fixes + I've included installers on Github. If you have requests or feedback or issues lemme know!</title>
    <updated>2025-01-10T23:17:59+00:00</updated>
    <author>
      <name>/u/Hairetsu</name>
      <uri>https://old.reddit.com/user/Hairetsu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyhyoo/notate_103_lots_of_platform_fixes_ive_included/"&gt; &lt;img alt="Notate 1.0.3 - Lots of platform fixes + I've included installers on Github. If you have requests or feedback or issues lemme know!" src="https://external-preview.redd.it/Ol0yzFMM8hLJHUMhOdEYg0wDLH6o_8qd6oG6sXR9vKE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=daf80009e9ca850a0a5df31935aecd6e4c27a673" title="Notate 1.0.3 - Lots of platform fixes + I've included installers on Github. If you have requests or feedback or issues lemme know!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairetsu"&gt; /u/Hairetsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/CNTRLAI/notate"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyhyoo/notate_103_lots_of_platform_fixes_ive_included/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyhyoo/notate_103_lots_of_platform_fixes_ive_included/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T23:17:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyqcir</id>
    <title>stable-diffusion.cpp context size</title>
    <updated>2025-01-11T06:56:43+00:00</updated>
    <author>
      <name>/u/goingsplit</name>
      <uri>https://old.reddit.com/user/goingsplit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anybody using this tool? I noticed the context size being clipped to some given size when the inference starts. I wonder if anybody figured how to control that parameter?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goingsplit"&gt; /u/goingsplit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyqcir/stablediffusioncpp_context_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyqcir/stablediffusioncpp_context_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyqcir/stablediffusioncpp_context_size/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T06:56:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxm0ep</id>
    <title>Anyone want the script to run Moondream 2b's new gaze detection on any video?</title>
    <updated>2025-01-09T20:12:41+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxm0ep/anyone_want_the_script_to_run_moondream_2bs_new/"&gt; &lt;img alt="Anyone want the script to run Moondream 2b's new gaze detection on any video?" src="https://external-preview.redd.it/cmk5cnZsYXZ6MGNlMeEGpTWo5MaI3KFBwDeey6o_wri3pXWzYnC4YTD3TTIr.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0330607dc613a7fe62b098a05b7a0c3c3e3495dc" title="Anyone want the script to run Moondream 2b's new gaze detection on any video?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n9beslavz0ce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxm0ep/anyone_want_the_script_to_run_moondream_2bs_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxm0ep/anyone_want_the_script_to_run_moondream_2bs_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T20:12:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy8ehf</id>
    <title>Does anyone know how to replicate this setup for coding ?</title>
    <updated>2025-01-10T16:32:48+00:00</updated>
    <author>
      <name>/u/Alive-Tax3189</name>
      <uri>https://old.reddit.com/user/Alive-Tax3189</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ehf/does_anyone_know_how_to_replicate_this_setup_for/"&gt; &lt;img alt="Does anyone know how to replicate this setup for coding ?" src="https://external-preview.redd.it/enU0ZnVheGwxN2NlMZs7egYfaDsCtkR_AYCrnVuq-88BdYMxPb_V_Fpy742y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=311e897110b819f7974eee9811f460d255b45faf" title="Does anyone know how to replicate this setup for coding ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alive-Tax3189"&gt; /u/Alive-Tax3189 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wz5qfaxl17ce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ehf/does_anyone_know_how_to_replicate_this_setup_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ehf/does_anyone_know_how_to_replicate_this_setup_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyruya</id>
    <title>Where is everyone sourcing their hardware?</title>
    <updated>2025-01-11T08:49:26+00:00</updated>
    <author>
      <name>/u/hainesk</name>
      <uri>https://old.reddit.com/user/hainesk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After looking for awhile I finally decided to purchase 3 identical refurbished 3090s through a manufacturer refurb online store and all of them have turned out to be unstable.&lt;br /&gt; One of them locking up the system within a few minutes of being turned on. I thought that by getting a manufacturer refurbished card directly from them that it would be less likely to be an issue. I looked around a lot before purchasing and this seemed like the safest option for a reasonable price ($699 per card).&lt;/p&gt; &lt;p&gt;I am in the process of RMA’ing them, but where does everyone else get their hardware? Has anyone else had issues with bad video cards? Any tips on good places to order from?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hainesk"&gt; /u/hainesk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyruya/where_is_everyone_sourcing_their_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyruya/where_is_everyone_sourcing_their_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyruya/where_is_everyone_sourcing_their_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T08:49:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyjhch</id>
    <title>Any good LLM benchmarks that rank ability to document code and explain code?</title>
    <updated>2025-01-11T00:29:02+00:00</updated>
    <author>
      <name>/u/palindsay</name>
      <uri>https://old.reddit.com/user/palindsay</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems current coding benchmarks like Aider and bigcode, etc. focus on code refactoring and generation. What about strength in code documentation and explanation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/palindsay"&gt; /u/palindsay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjhch/any_good_llm_benchmarks_that_rank_ability_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjhch/any_good_llm_benchmarks_that_rank_ability_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjhch/any_good_llm_benchmarks_that_rank_ability_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T00:29:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy4onq</id>
    <title>OCR tools for really very bad handwriting!</title>
    <updated>2025-01-10T13:44:05+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy4onq/ocr_tools_for_really_very_bad_handwriting/"&gt; &lt;img alt="OCR tools for really very bad handwriting!" src="https://preview.redd.it/ww1i5y5h76ce1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09c64901dd13fc181007e945126d45f11e6e021c" title="OCR tools for really very bad handwriting!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ww1i5y5h76ce1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy4onq/ocr_tools_for_really_very_bad_handwriting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy4onq/ocr_tools_for_really_very_bad_handwriting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T13:44:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyio5w</id>
    <title>Beginner Guide - Creating LLM Datasets with Python</title>
    <updated>2025-01-10T23:50:58+00:00</updated>
    <author>
      <name>/u/0xlisykes</name>
      <uri>https://old.reddit.com/user/0xlisykes</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0xlisykes"&gt; /u/0xlisykes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://toolworks.dev/docs/Guides/creating-llm-datasets-python"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyio5w/beginner_guide_creating_llm_datasets_with_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyio5w/beginner_guide_creating_llm_datasets_with_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T23:50:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy5l18</id>
    <title>Local TTS models that can match ElevenLabs in terms of quality and consistency</title>
    <updated>2025-01-10T14:28:17+00:00</updated>
    <author>
      <name>/u/_megazz</name>
      <uri>https://old.reddit.com/user/_megazz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I should probably start by stating that I'm somewhat new to running AI models locally, but I've tinkered with Ollama + Open WebUI before and was able to get some models running through WSL2 on my RTX 4080 and was pretty impressed with the results.&lt;/p&gt; &lt;p&gt;With that said, I'm now looking for a good local TTS model and I was honestly disappointed with what I could find. Most projects seem to not be updated in months or are simply dead.&lt;/p&gt; &lt;p&gt;From what I've read, the general consensus seems to be that XTTS-v2 is still the best overall model to this day, which is from a startup that has &lt;a href="https://coqui.ai/"&gt;shut down&lt;/a&gt;. I figured I'd try it anyway and I was able to get it running through &lt;a href="https://github.com/daswer123/xtts-webui"&gt;this simple portable version&lt;/a&gt;, but I was honestly disappointed with the results I got, all very inconsistent and not natural sounding, even after tinkering a lot with its different parameters and voices. Not even close to what I can get from ElevenLabs, which could easily pass as real person speaking, but that service is very pricey for me, unfortunately.&lt;/p&gt; &lt;p&gt;There are other popular suggestions like Fish Speech or F5-TTS, but since I need the model to speak Portuguese, that limits my options a lot.&lt;/p&gt; &lt;p&gt;Right now I feel like I'm just wasting my time and that nothing that I can run locally can match EvenLabs currently, but as I said, I'm new to this and maybe I'm missing something obvious. In any case, I'd appreciate any input!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_megazz"&gt; /u/_megazz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy5l18/local_tts_models_that_can_match_elevenlabs_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy5l18/local_tts_models_that_can_match_elevenlabs_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy5l18/local_tts_models_that_can_match_elevenlabs_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T14:28:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyh9y3</id>
    <title>DeepSeek-V3 imatrix quants by team mradermacher</title>
    <updated>2025-01-10T22:47:05+00:00</updated>
    <author>
      <name>/u/oobabooga4</name>
      <uri>https://old.reddit.com/user/oobabooga4</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyh9y3/deepseekv3_imatrix_quants_by_team_mradermacher/"&gt; &lt;img alt="DeepSeek-V3 imatrix quants by team mradermacher" src="https://external-preview.redd.it/m-G04wn3IB1jswcKbDUS8jJlKetCzX6HK1WoeuTcULY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=059108de9020787af36b4f6d446ccbfc92d4ba7e" title="DeepSeek-V3 imatrix quants by team mradermacher" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oobabooga4"&gt; /u/oobabooga4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mradermacher/DeepSeek-V3-i1-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyh9y3/deepseekv3_imatrix_quants_by_team_mradermacher/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyh9y3/deepseekv3_imatrix_quants_by_team_mradermacher/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T22:47:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy91m1</id>
    <title>0.5B Distilled QwQ, runnable on IPhone</title>
    <updated>2025-01-10T16:59:44+00:00</updated>
    <author>
      <name>/u/Lord_of_Many_Memes</name>
      <uri>https://old.reddit.com/user/Lord_of_Many_Memes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy91m1/05b_distilled_qwq_runnable_on_iphone/"&gt; &lt;img alt="0.5B Distilled QwQ, runnable on IPhone" src="https://external-preview.redd.it/hOvT7Zh2EDTGcuqajUYbM7IboIMuAwdCFsY0UWAS0pU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85274e9584cd8dc27f3835483f32b47ea48f28f0" title="0.5B Distilled QwQ, runnable on IPhone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lord_of_Many_Memes"&gt; /u/Lord_of_Many_Memes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/kz919/Mini-QwQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy91m1/05b_distilled_qwq_runnable_on_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy91m1/05b_distilled_qwq_runnable_on_iphone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:59:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyapzu</id>
    <title>Phi-4 Finetuning - now with &gt;128K context length + Bug Fix Details</title>
    <updated>2025-01-10T18:09:05+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyapzu/phi4_finetuning_now_with_128k_context_length_bug/"&gt; &lt;img alt="Phi-4 Finetuning - now with &amp;gt;128K context length + Bug Fix Details" src="https://external-preview.redd.it/GToYANeKQHKhFdKJjLK03Emv1ylZ0l8jeD1iuQJ8-dE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57fbf9c89972d5c31e3bd2d3354696be4e8d5b9d" title="Phi-4 Finetuning - now with &amp;gt;128K context length + Bug Fix Details" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! You can now fine-tune Phi-4 with &amp;gt;128K context lengths using &lt;a href="https://github.com/unslothai/unsloth/"&gt;Unsloth&lt;/a&gt;! That's 12x longer than Hugging Face + FA2’s 11K on a 48GB GPU.&lt;/p&gt; &lt;p&gt;Phi-4 Finetuning Colab: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also previously announced bug fixes for Phi-4 and so we’ll reveal the details.&lt;/p&gt; &lt;p&gt;But, before we do, some of you were curious if our fixes actually worked? Yes! Our fixed Phi-4 uploads show clear performance gains, with even better scores than Microsoft's original uploads on the &lt;a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=phi-4"&gt;Open LLM Leaderboard&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d8hew26e06ce1.png?width=2366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=173c23feacc625566271470839fe7a5e25eb860e"&gt;https://preview.redd.it/d8hew26e06ce1.png?width=2366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=173c23feacc625566271470839fe7a5e25eb860e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some of you even tested it to show greatly improved results in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Example 1: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hwzmqc/comment/m665h08/"&gt;Multiple-choice tasks&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qx50pkq706ce1.png?width=1579&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=437da2cabdbf98ef5a8b8cbdc5592907a20e2316"&gt;https://preview.redd.it/qx50pkq706ce1.png?width=1579&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=437da2cabdbf98ef5a8b8cbdc5592907a20e2316&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Example 2: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hwzmqc/comment/m65wr3e/"&gt;ASCII art generation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ircz0pnc06ce1.png?width=1433&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16c770a0fd58a469af3b98216844447845b98ada"&gt;https://preview.redd.it/ircz0pnc06ce1.png?width=1433&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16c770a0fd58a469af3b98216844447845b98ada&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Bug Fix Details&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Tokenizer Fix: Phi-4 incorrectly uses &amp;lt;|endoftext|&amp;gt; as EOS instead of &amp;lt;|im_end|&amp;gt;.&lt;/li&gt; &lt;li&gt;Finetuning Fix: Use a proper padding token (e.g., &amp;lt;|dummy_87|&amp;gt;).&lt;/li&gt; &lt;li&gt;Chat Template Fix: Avoid adding an assistant prompt unless specified to prevent serving issues.&lt;/li&gt; &lt;li&gt;More in-depth in our blog: &lt;a href="https://unsloth.ai/blog/phi4"&gt;https://unsloth.ai/blog/phi4&lt;/a&gt; or &lt;a href="https://twitter.com/danielhanchen/status/1877781452818968615"&gt;tweet&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Phi-4 Uploads (with our bug fixes)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4-GGUF"&gt;GGUFs&lt;/a&gt; including 2, 3, 4, 5, 6, 8, 16-bit&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4-unsloth-bnb-4bit"&gt;Unsloth Dynamic 4-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4"&gt;Original 16-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;For all other model uploads, see &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;our docs&lt;/a&gt;&lt;br /&gt; I know this post was a bit long, but I hope it was informative and please ask any questions!! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyapzu/phi4_finetuning_now_with_128k_context_length_bug/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyapzu/phi4_finetuning_now_with_128k_context_length_bug/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyapzu/phi4_finetuning_now_with_128k_context_length_bug/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T18:09:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy34ir</id>
    <title>WebGPU-accelerated reasoning LLMs running 100% locally in-browser w/ Transformers.js</title>
    <updated>2025-01-10T12:16:13+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy34ir/webgpuaccelerated_reasoning_llms_running_100/"&gt; &lt;img alt="WebGPU-accelerated reasoning LLMs running 100% locally in-browser w/ Transformers.js" src="https://external-preview.redd.it/a3B0bmYzbTJyNWNlMYVrWG7q5Ym6r9MYEdNpGfavLsbyjmwCsGU7oHTw1w8w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06dd6f09c82183918afdcca9863994fcffe8274f" title="WebGPU-accelerated reasoning LLMs running 100% locally in-browser w/ Transformers.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vmfpb2m2r5ce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy34ir/webgpuaccelerated_reasoning_llms_running_100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy34ir/webgpuaccelerated_reasoning_llms_running_100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T12:16:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1hydavt</id>
    <title>New open source SAEs for model steering, including the first ever SAE for Llama 3.3 70b</title>
    <updated>2025-01-10T19:56:16+00:00</updated>
    <author>
      <name>/u/iamephemeral</name>
      <uri>https://old.reddit.com/user/iamephemeral</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hydavt/new_open_source_saes_for_model_steering_including/"&gt; &lt;img alt="New open source SAEs for model steering, including the first ever SAE for Llama 3.3 70b" src="https://external-preview.redd.it/r4CGqgcRPLr1eA9JfvNHSBaN_-4tgT5j575hGH0pgUU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=239946d045e3a552b2d863b9157de34884befd7f" title="New open source SAEs for model steering, including the first ever SAE for Llama 3.3 70b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamephemeral"&gt; /u/iamephemeral &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Goodfire/Llama-3.3-70B-Instruct-SAE-l50"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hydavt/new_open_source_saes_for_model_steering_including/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hydavt/new_open_source_saes_for_model_steering_including/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T19:56:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyomxu</id>
    <title>Biden to Further Limit Nvidia AI Chip Exports in Final Push</title>
    <updated>2025-01-11T05:04:42+00:00</updated>
    <author>
      <name>/u/nate4t</name>
      <uri>https://old.reddit.com/user/nate4t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyomxu/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt; &lt;img alt="Biden to Further Limit Nvidia AI Chip Exports in Final Push" src="https://external-preview.redd.it/JzTX2qRvXmqwwoj4NDPkmIrtAEnnpeeSx1wuD-VSMTA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3dc3abfba0983054d99719792b1d834321b47c7b" title="Biden to Further Limit Nvidia AI Chip Exports in Final Push" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nate4t"&gt; /u/nate4t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-01-08/biden-to-further-limit-nvidia-amd-ai-chip-exports-in-final-push?leadSource=reddit_wall"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyomxu/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyomxu/biden_to_further_limit_nvidia_ai_chip_exports_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T05:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyf1pf</id>
    <title>Text to speech in 82m params is perfect for edge AI. Who's building an audio assistant with Kokoro?</title>
    <updated>2025-01-10T21:09:12+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyf1pf/text_to_speech_in_82m_params_is_perfect_for_edge/"&gt; &lt;img alt="Text to speech in 82m params is perfect for edge AI. Who's building an audio assistant with Kokoro? " src="https://external-preview.redd.it/PSxCcCk18RpMpFh_Tgc1ycbd0zsabOZK7av3YdT9fA4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b75663383244e2aa5f5fcf0207756c5dc28fb51b" title="Text to speech in 82m params is perfect for edge AI. Who's building an audio assistant with Kokoro? " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyf1pf/text_to_speech_in_82m_params_is_perfect_for_edge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyf1pf/text_to_speech_in_82m_params_is_perfect_for_edge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T21:09:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyjoau</id>
    <title>This is my Powermac G3 sleeper AI workstation. 80gb total ram(32gb vram + 48gb ram)</title>
    <updated>2025-01-11T00:38:10+00:00</updated>
    <author>
      <name>/u/PraxisOG</name>
      <uri>https://old.reddit.com/user/PraxisOG</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjoau/this_is_my_powermac_g3_sleeper_ai_workstation/"&gt; &lt;img alt="This is my Powermac G3 sleeper AI workstation. 80gb total ram(32gb vram + 48gb ram)" src="https://b.thumbs.redditmedia.com/niNscGOj9hur8A-QVwFzrElx4sAsFt-GLXQ2A5RCLGw.jpg" title="This is my Powermac G3 sleeper AI workstation. 80gb total ram(32gb vram + 48gb ram)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PraxisOG"&gt; /u/PraxisOG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1hyjoau"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjoau/this_is_my_powermac_g3_sleeper_ai_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjoau/this_is_my_powermac_g3_sleeper_ai_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T00:38:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy8733</id>
    <title>Biden to Further Limit Nvidia AI Chip Exports in Final Push Restricting US Allies Such As Poland, Portugal, India or UAE Maker Of Falcon Models</title>
    <updated>2025-01-10T16:24:05+00:00</updated>
    <author>
      <name>/u/holamifuturo</name>
      <uri>https://old.reddit.com/user/holamifuturo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8733/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt; &lt;img alt="Biden to Further Limit Nvidia AI Chip Exports in Final Push Restricting US Allies Such As Poland, Portugal, India or UAE Maker Of Falcon Models" src="https://external-preview.redd.it/JzTX2qRvXmqwwoj4NDPkmIrtAEnnpeeSx1wuD-VSMTA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3dc3abfba0983054d99719792b1d834321b47c7b" title="Biden to Further Limit Nvidia AI Chip Exports in Final Push Restricting US Allies Such As Poland, Portugal, India or UAE Maker Of Falcon Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/holamifuturo"&gt; /u/holamifuturo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-01-08/biden-to-further-limit-nvidia-amd-ai-chip-exports-in-final-push"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8733/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8733/biden_to_further_limit_nvidia_ai_chip_exports_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:24:05+00:00</published>
  </entry>
</feed>
