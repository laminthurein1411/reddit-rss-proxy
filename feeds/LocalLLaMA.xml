<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-24T04:48:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ivua9y</id>
    <title>For the love of God, stop abusing the word "multi"</title>
    <updated>2025-02-22T22:00:30+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;We trained a SOTA multimodal LLM&amp;quot; and then you dig deep and find it only supports text and vision. These are only two modalities. You trained a SOTA BI-MODAL LLM. &lt;/p&gt; &lt;p&gt;&amp;quot;Our model shows significant improvement in multilingual applications.... The model supports English and Chinese text&amp;quot; yeah... This is a BILINGUAL model. &lt;/p&gt; &lt;p&gt;The word &amp;quot;multi&amp;quot; means &amp;quot;many&amp;quot;. While two is technically &amp;quot;many&amp;quot;, there's a better prefix for that and it is &amp;quot;bi&amp;quot;.&lt;/p&gt; &lt;p&gt;I can't count the number of times people claim they trained a SOTA open model that &amp;quot;beats gpt-4o in multimodal tasks&amp;quot; only to find out the model only supports image and text and not audio (which was the whole point behind gpt-4o anyway) &lt;/p&gt; &lt;p&gt;TLDR: Use &amp;quot;bi&amp;quot; when talking about 2 modalities and languages, use &amp;quot;multi&amp;quot; when talking about 3 or mode.&lt;/p&gt; &lt;p&gt;P.S. I am not downplaying the importance and significance of these open models, but it's better to avoid hyping and deceiving the community.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivua9y/for_the_love_of_god_stop_abusing_the_word_multi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivua9y/for_the_love_of_god_stop_abusing_the_word_multi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivua9y/for_the_love_of_god_stop_abusing_the_word_multi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T22:00:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwo0t9</id>
    <title>LMArena new (Amazon?) model - raspberry-exp-beta-v2</title>
    <updated>2025-02-23T23:39:51+00:00</updated>
    <author>
      <name>/u/dp3471</name>
      <uri>https://old.reddit.com/user/dp3471</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwo0t9/lmarena_new_amazon_model_raspberryexpbetav2/"&gt; &lt;img alt="LMArena new (Amazon?) model - raspberry-exp-beta-v2" src="https://a.thumbs.redditmedia.com/W5sWoSULjh5trYB-pz9pyO7am9t665O_JHaF5jO4k_4.jpg" title="LMArena new (Amazon?) model - raspberry-exp-beta-v2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/53iglosz3zke1.png?width=845&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b13ec0aa748a95b420ff7fb7c60c5fb45d7ac312"&gt;https://preview.redd.it/53iglosz3zke1.png?width=845&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b13ec0aa748a95b420ff7fb7c60c5fb45d7ac312&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now, it can be hallucinating, but I haven't seen any mention of this one. I've also seen a v1. &lt;/p&gt; &lt;p&gt;Anyone know what it actually is or if I'm missing something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dp3471"&gt; /u/dp3471 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwo0t9/lmarena_new_amazon_model_raspberryexpbetav2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwo0t9/lmarena_new_amazon_model_raspberryexpbetav2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwo0t9/lmarena_new_amazon_model_raspberryexpbetav2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T23:39:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwi3d4</id>
    <title>Flux Generator: A local web UI image generator for Apple silicon + OpenWebUI support</title>
    <updated>2025-02-23T19:22:04+00:00</updated>
    <author>
      <name>/u/akashjss</name>
      <uri>https://old.reddit.com/user/akashjss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Image generator UI + OpenWebUI integration now supports Stable Diffusion SDXL Turbo and SD 2.1 models. This brings total supporting models to 4. Other two models being Flux Schnell and Dev. Repo : &lt;a href="https://github.com/voipnuggets/flux-generator"&gt;https://github.com/voipnuggets/flux-generator&lt;/a&gt; Tutorial : &lt;a href="https://voipnuggets.com/2025/02/18/flux-generator-local-image-generation-on-apple-silicon-with-open-webui-integration-using-flux-llm/"&gt;https://voipnuggets.com/2025/02/18/flux-generator-local-image-generation-on-apple-silicon-with-open-webui-integration-using-flux-llm/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akashjss"&gt; /u/akashjss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwi3d4/flux_generator_a_local_web_ui_image_generator_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwi3d4/flux_generator_a_local_web_ui_image_generator_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwi3d4/flux_generator_a_local_web_ui_image_generator_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T19:22:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwnnac</id>
    <title>Chat/RP / Kobold AI problems with formats and rules.</title>
    <updated>2025-02-23T23:22:11+00:00</updated>
    <author>
      <name>/u/Pogo4Fufu</name>
      <uri>https://old.reddit.com/user/Pogo4Fufu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hiho,&lt;/p&gt; &lt;p&gt;Perhaps someone has a good hint. I run atm Midnight-Miqu-70B locally together with Kobold AI and it's really fun to play with. I have several well working presets for role playing and normally it's quite OK, the AI just randomly takes over like acting as me etc.&lt;/p&gt; &lt;p&gt;But what the AI often doesn't get is the difference between story/lore/internal thoughts of me/my character and the things I say to the AI. Like:&lt;/p&gt; &lt;p&gt;me: &amp;quot;Yes, please.&amp;quot; *I hate it.*&lt;/p&gt; &lt;p&gt;AI: &amp;quot;Oh, you hate it?&amp;quot;&lt;/p&gt; &lt;p&gt;Same with&lt;/p&gt; &lt;p&gt;me: &amp;quot;Yes, please.&amp;quot; # I hate it.&lt;/p&gt; &lt;p&gt;and similar format rules. How do you handle this? The goal of those hints is to allow the AI to indirectly react to this information, but not directly.&lt;/p&gt; &lt;p&gt;It's declared in the presets, but it is the thing that most often goes wrong.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pogo4Fufu"&gt; /u/Pogo4Fufu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwnnac/chatrp_kobold_ai_problems_with_formats_and_rules/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwnnac/chatrp_kobold_ai_problems_with_formats_and_rules/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwnnac/chatrp_kobold_ai_problems_with_formats_and_rules/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T23:22:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwszf1</id>
    <title>Has anyone finetuned FIM type models but for regular writing instead of code?</title>
    <updated>2025-02-24T03:50:55+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seem to be several for code. I just setup Qwen 2.5 coder 0.5B. But it could be useful for regular writing too, as it often has predictable phrases and sentence structure, especially NON-creative writing (and even creative in some cases). Some model in the range of 0-3B to be run efficiently locally.&lt;/p&gt; &lt;p&gt;I tried the regular 0.5B but it doesn't really seem to work, just immediately ends most of the time, keeps trying to start full new sentences and only really works if you're at the end of a document (so no Fill In Middle). I don't think it's been trained to understand FIM prompts&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwszf1/has_anyone_finetuned_fim_type_models_but_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwszf1/has_anyone_finetuned_fim_type_models_but_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwszf1/has_anyone_finetuned_fim_type_models_but_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T03:50:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw1xn7</id>
    <title>The Paradox of Open Weights, but Closed Source</title>
    <updated>2025-02-23T04:29:18+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;- An open-weight model has public weights, which you can download from sites like Hugging Face.&lt;/p&gt; &lt;p&gt;- An open-source model has public training code and training dataset, allowing full reproduction. (I didn't come up with that definition, personally I think the dataset requirement is too strict, because then nearly every major model is closed-source.)&lt;/p&gt; &lt;p&gt;- A permissive model has a permissive license, like MIT or Apache 2.0, which means you can do many things with the weights, like serve them over a commercialized inference endpoint. A license like CC-BY-NC is often considered &amp;quot;non-permissive&amp;quot; since the NC means non-commercial.&lt;/p&gt; &lt;p&gt;Kokoro-82M is an Apache 2.0 model that I trained and uploaded to HF &lt;em&gt;without also uploading the accompanying training code or dataset&lt;/em&gt;, thus making it permissive and open-weight, yet also closed-source under the above definitions.&lt;/p&gt; &lt;p&gt;As I've said in the past, there is already MIT-licensed training code at &lt;a href="https://github.com/yl4579/StyleTTS2"&gt;https://github.com/yl4579/StyleTTS2&lt;/a&gt; which others have already used/modified to produce models comparable to, or in some cases better than, Kokoro. But nobody seems to care about that that, they want &lt;em&gt;my&lt;/em&gt; specific training code. Many have speculated why I have not (yet) done this. I'll offer two very practical reasons here‚Äîthere may be others, but these ones are critical &amp;amp; sufficient.&lt;/p&gt; &lt;p&gt;First, commercial. Obviously, there is commercial value (to me &amp;amp; others) in the code I write, including the training code. Many of those calling for me to release my training code would, undoubtedly, turn around and commercialize that code. On the inference side, I have understood and accepted this reality, and that does not deter me from releasing and improving inference code, especially for other languages. I cannot promise that I'll get there on training.&lt;/p&gt; &lt;p&gt;Second, surge pricing, or basic supply and demand. I have no local NVIDIA GPU and therefore rely on A100 80GB cloud rentals. My training code is specifically configured (in some places hardcoded) for A100 80GB, since these training runs are often vRAM intensive. Unless (or even if) I refactor, open sourcing the training code would probably lead to increased rental demand for the same machines I want, making current and future training runs more expensive. The lowest five A100 80GB prices I see on Vast.ai are $1.1, $1.35, $1.35, $1.41, $1.47, which is typical pricing depth (or lack thereof). Even a handful of people scooping up the cheapest A100s moves the needle quite a lot.&lt;/p&gt; &lt;p&gt;Despite my own training code currently not being released:&lt;/p&gt; &lt;p&gt;- You can train StyleTTS2 models today using the aforementioned MIT training code. I have not gatekept or obfuscated the StyleTTS2 roots of Kokoro‚Äîit has been in the README since day 0. Sure, I picked a new model name, but in line with industry standards, it is generally acceptable to name a model when it has substantially new weights.&lt;/p&gt; &lt;p&gt;- Others have/will publish their own training code, for StyleTTS2 models and others.&lt;/p&gt; &lt;p&gt;- There will simply be better open models, in the Kokoro series, in TTS at large, and all modalities in general.&lt;/p&gt; &lt;p&gt;This particular post was motivated by a back-and-forth I had with &lt;a href="/u/Fold-Plastic"&gt;u/Fold-Plastic&lt;/a&gt;. To those who think I am The Enemy for not releasing the training code: I think you are directing way too much animosity towards a permissive-open-weight solo dev operating in a field of non-permissive and closed-weight orgs. It's that sort of animosity that makes open source exhausting rather than rewarding, and pushes devs to leave for the warm embrace of money-printing closed source.&lt;/p&gt; &lt;p&gt;Some other notes:&lt;/p&gt; &lt;p&gt;- I have not yet made a decision on voice cloning, although unlike training code, an encoder release won't spike my A100 costs by +50%, so it is more likely than a training code release.&lt;/p&gt; &lt;p&gt;- For Kokoro, take your voice cloning performance expectations and divide them by 10, since the volume of audio seen during training remains OOMs lower than other TTS models.&lt;/p&gt; &lt;p&gt;- In the meantime, for voice cloning you should be looking at larger TTS models trained on more audio, like XTTS Fish Zonos etc.&lt;/p&gt; &lt;p&gt;- Voice cloning Trump TSwift or Obama may be less &amp;quot;dark magic&amp;quot; and more &amp;quot;retrieval&amp;quot;, assuming those celebrities are in the training dataset (not currently the case for Kokoro).&lt;/p&gt; &lt;p&gt;- Future Kokoro models (i.e. above v1.0) will likely follow a naming scheme like `hexgrad/Kokoro-82M-vX.Y`.&lt;/p&gt; &lt;p&gt;- If voice cloning were to be released, it would change the model naming to `hexgrad/Kokoro-vX.Y`. This is because the encoder is ~25M params, and summing the params across the encoder and the 82M decoder does not feel appropriate.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw1xn7/the_paradox_of_open_weights_but_closed_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw1xn7/the_paradox_of_open_weights_but_closed_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw1xn7/the_paradox_of_open_weights_but_closed_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T04:29:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwc4bv</id>
    <title>Built a Chrome Extension That Uses Local AI (LLaVa) to Generate Filenames for Images</title>
    <updated>2025-02-23T15:06:23+00:00</updated>
    <author>
      <name>/u/KonradFreeman</name>
      <uri>https://old.reddit.com/user/KonradFreeman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I got tired of downloading images named &lt;strong&gt;‚ÄúIMG_20240223_132459.jpg‚Äù&lt;/strong&gt; and having to manually rename them to something useful. So I built a &lt;strong&gt;Chrome extension&lt;/strong&gt; that uses &lt;strong&gt;local AI (LLaVa + Ollama)&lt;/strong&gt; to &lt;strong&gt;analyze image content and generate descriptive filenames automatically&lt;/strong&gt; before saving them. No more digging through random files trying to figure out what‚Äôs what.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How It Works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ Right-click an image ‚Üí ‚ÄúSave with AI-generated filename‚Äù&lt;/p&gt; &lt;p&gt;‚Ä¢ The extension runs &lt;strong&gt;LLaVa locally&lt;/strong&gt; (so no external API calls, no data leaves your machine)&lt;/p&gt; &lt;p&gt;‚Ä¢ It suggests a filename based on what‚Äôs in the image (e.g., &lt;em&gt;‚Äúgolden-retriever-playing-park.jpg‚Äù&lt;/em&gt;)&lt;/p&gt; &lt;p&gt;‚Ä¢ Option to preview/edit before saving&lt;/p&gt; &lt;p&gt;‚Ä¢ Supports &lt;strong&gt;custom filename templates&lt;/strong&gt; ({object}-{location}-{date}.jpg)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Local AI?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most AI-powered tools send your data to a server. I don‚Äôt like that. This one runs &lt;strong&gt;entirely on your machine&lt;/strong&gt; using &lt;strong&gt;Ollama&lt;/strong&gt;, which means:&lt;/p&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Private&lt;/strong&gt; ‚Äì No cloud processing, everything stays local&lt;/p&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Fast&lt;/strong&gt; ‚Äì No latency from API calls&lt;/p&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Free&lt;/strong&gt; ‚Äì No subscription or token limits&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;LLaVa for image analysis&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Ollama as the local model runner&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Chrome Extension API (contextMenus, downloads, storage, etc.)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;DeclarativeNetRequest for host access&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who Might Find This Useful?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ People who &lt;strong&gt;download a lot of images&lt;/strong&gt; and hate messy filenames&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Researchers, content creators, designers&lt;/strong&gt;‚Äîanyone who needs better file organization&lt;/p&gt; &lt;p&gt;‚Ä¢ Privacy-conscious users who want &lt;strong&gt;AI features without sending data online&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try It Out / Feedback?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I‚Äôd love to hear thoughts from others working with &lt;strong&gt;local AI&lt;/strong&gt;, &lt;strong&gt;Chrome extensions&lt;/strong&gt;, or &lt;strong&gt;automation tools&lt;/strong&gt;. Would you use something like this? Any features you‚Äôd want added?&lt;/p&gt; &lt;p&gt;If you‚Äôre interested you can download and try it out for free from my github repo while I wait for it to be approved by the Chrome Web Store:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/kliewerdaniel/chrome-ai-filename-generator"&gt;https://github.com/kliewerdaniel/chrome-ai-filename-generator&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KonradFreeman"&gt; /u/KonradFreeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwc4bv/built_a_chrome_extension_that_uses_local_ai_llava/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwc4bv/built_a_chrome_extension_that_uses_local_ai_llava/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwc4bv/built_a_chrome_extension_that_uses_local_ai_llava/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T15:06:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw4q2e</id>
    <title>Where is Llama 4? I expected that in January.</title>
    <updated>2025-02-23T07:28:16+00:00</updated>
    <author>
      <name>/u/appakaradi</name>
      <uri>https://old.reddit.com/user/appakaradi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With all the new release from all the labs, Meta has been quiet. They have the talent and resources. They need to compete. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/appakaradi"&gt; /u/appakaradi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw4q2e/where_is_llama_4_i_expected_that_in_january/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw4q2e/where_is_llama_4_i_expected_that_in_january/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw4q2e/where_is_llama_4_i_expected_that_in_january/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T07:28:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwr579</id>
    <title>GPT-4-o vs Claude 3.5 Sonnet vs Gemini Flash 2.0 vs Amazon Nova Pro - SOTA VLMs for Visual Reasoning</title>
    <updated>2025-02-24T02:13:43+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Video about State of the Art in terms of Vision models, and learn key limitations of each model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=bxiIk8TW9og"&gt;https://www.youtube.com/watch?v=bxiIk8TW9og&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwr579/gpt4o_vs_claude_35_sonnet_vs_gemini_flash_20_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwr579/gpt4o_vs_claude_35_sonnet_vs_gemini_flash_20_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwr579/gpt4o_vs_claude_35_sonnet_vs_gemini_flash_20_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T02:13:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw9m8r</id>
    <title>AMD inference using AMDVLK driver is 40% faster than RADV on pp, ~15% faster than ROCm inference performance*</title>
    <updated>2025-02-23T13:00:35+00:00</updated>
    <author>
      <name>/u/ashirviskas</name>
      <uri>https://old.reddit.com/user/ashirviskas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using 7900 XTX and decided to do some testing after getting intrigued by &lt;a href="/u/fallingdowndizzyvr"&gt;/u/fallingdowndizzyvr&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;: AMDVLK is 45% faster than RADV (default Vulkan driver supplied by mesa) on PP (Prompt Processing), but still slower than ROCm. BUT faster than ROCM at TG (Text Generation) by 12-20% (*- though slower on IQ2_XS by 15%). To use, I just installed amdvlk and ran &lt;code&gt;VK_ICD_FILENAMES=/usr/share/vulkan/icd.d/amd_icd64.json ./build/bin/llama-bench ...&lt;/code&gt; (Arch Linux, might be different on other OSes)&lt;/p&gt; &lt;p&gt;Here are some results done on AMD RX 7900 XTX, arch linux, llama.cpp commit &lt;code&gt;51f311e0&lt;/code&gt;, using bartowski GGUFs. I wanted to test different quants and after testing it all it seems like AMDVLK is a much better option for Q4-Q8 quants for tg speed. ROCm still wins on more exotic quants.&lt;/p&gt; &lt;h3&gt;on ROCm, linux&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;14.62 GiB&lt;/td&gt; &lt;td align="right"&gt;14.77 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;1414.84 ¬± 3.87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;14.62 GiB&lt;/td&gt; &lt;td align="right"&gt;14.77 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;36.33 ¬± 0.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;18.48 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;672.70 ¬± 1.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;18.48 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;22.80 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi3 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;13.82 GiB&lt;/td&gt; &lt;td align="right"&gt;13.96 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;1407.50 ¬± 4.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi3 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;13.82 GiB&lt;/td&gt; &lt;td align="right"&gt;13.96 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;39.88 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B IQ2_XS - 2.3125 bpw&lt;/td&gt; &lt;td align="right"&gt;9.27 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;671.31 ¬± 1.39&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B IQ2_XS - 2.3125 bpw&lt;/td&gt; &lt;td align="right"&gt;9.27 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;28.65 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Vulkan, default mesa driver, RADV&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;14.62 GiB&lt;/td&gt; &lt;td align="right"&gt;14.77 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;798.98 ¬± 3.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;14.62 GiB&lt;/td&gt; &lt;td align="right"&gt;14.77 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;39.72 ¬± 0.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;18.48 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;279.68 ¬± 0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;18.48 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;28.96 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi3 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;13.82 GiB&lt;/td&gt; &lt;td align="right"&gt;13.96 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;779.84 ¬± 2.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi3 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;13.82 GiB&lt;/td&gt; &lt;td align="right"&gt;13.96 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;41.42 ¬± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B IQ2_XS - 2.3125 bpw&lt;/td&gt; &lt;td align="right"&gt;9.27 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;331.11 ¬± 0.82&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B IQ2_XS - 2.3125 bpw&lt;/td&gt; &lt;td align="right"&gt;9.27 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;25.74 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Vulkan, AMDVLK open source&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;14.62 GiB&lt;/td&gt; &lt;td align="right"&gt;14.77 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;1239.63 ¬± 4.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;14.62 GiB&lt;/td&gt; &lt;td align="right"&gt;14.77 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;43.73 ¬± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;18.48 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;394.89 ¬± 0.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;18.48 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;25.60 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi3 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;13.82 GiB&lt;/td&gt; &lt;td align="right"&gt;13.96 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;1110.21 ¬± 10.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi3 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;13.82 GiB&lt;/td&gt; &lt;td align="right"&gt;13.96 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;46.16 ¬± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B IQ2_XS - 2.3125 bpw&lt;/td&gt; &lt;td align="right"&gt;9.27 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;463.22 ¬± 1.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B IQ2_XS - 2.3125 bpw&lt;/td&gt; &lt;td align="right"&gt;9.27 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;24.38 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashirviskas"&gt; /u/ashirviskas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9m8r/amd_inference_using_amdvlk_driver_is_40_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9m8r/amd_inference_using_amdvlk_driver_is_40_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9m8r/amd_inference_using_amdvlk_driver_is_40_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T13:00:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwan9c</id>
    <title>What is software is this supposed to be?</title>
    <updated>2025-02-23T13:54:36+00:00</updated>
    <author>
      <name>/u/bitdotben</name>
      <uri>https://old.reddit.com/user/bitdotben</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwan9c/what_is_software_is_this_supposed_to_be/"&gt; &lt;img alt="What is software is this supposed to be?" src="https://preview.redd.it/vn7bkdvi9wke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=14f6a6667a13b78acdc3b2f3329bb92ef7150bc3" title="What is software is this supposed to be?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there, &lt;/p&gt; &lt;p&gt;Don‚Äôt know whether this is the right place to ask this question but I thought a lot of people in here are interested in the NVIDIAs project digits. &lt;/p&gt; &lt;p&gt;This image is from the NVIDIA CES keynote (I found a high quality version in NVIDIAs newsroom, &lt;a href="https://nvidianews.nvidia.com/news/nvidia-puts-grace-blackwell-on-every-desk-and-at-every-ai-developers-fingertips"&gt;https://nvidianews.nvidia.com/news/nvidia-puts-grace-blackwell-on-every-desk-and-at-every-ai-developers-fingertips&lt;/a&gt;). It‚Äòs clearly an AI generated screenshot with in the render. &lt;/p&gt; &lt;p&gt;But is the software in the AI screenshot meant to represent something specific? What kind of workload / analysis would look like this? Right-hand-side looks like code but what‚Äôs going on in the middle? I guess there is no one right answer but maybe some of you ‚Äûrecognise‚Äú this?&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bitdotben"&gt; /u/bitdotben &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vn7bkdvi9wke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwan9c/what_is_software_is_this_supposed_to_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwan9c/what_is_software_is_this_supposed_to_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T13:54:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwqee5</id>
    <title>UPDATE: Tool Calling with DeepSeek-R1 671B with LangChain and LangGraph</title>
    <updated>2025-02-24T01:36:15+00:00</updated>
    <author>
      <name>/u/lc19-</name>
      <uri>https://old.reddit.com/user/lc19-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted about a Github repo I created last week on tool calling with DeepSeek-R1 671B with LangChain and LangGraph, or more generally for any LLMs available in LangChain‚Äôs ChatOpenAI class (particularly useful for newly released LLMs which isn‚Äôt supported for tool calling yet by LangChain and LangGraph).&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/leockl/tool-ahead-of-time"&gt;https://github.com/leockl/tool-ahead-of-time&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This repo just got an upgrade. What‚Äôs new: - Now available on PyPI! Just &amp;quot;pip install taot&amp;quot; and you're ready to go! - Completely redesigned to follow LangChain's and LangGraph's intuitive tool calling patterns. - Natural language responses when tool calling is performed.&lt;/p&gt; &lt;p&gt;Kindly give me a star on my repo if this is helpful. Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lc19-"&gt; /u/lc19- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwqee5/update_tool_calling_with_deepseekr1_671b_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwqee5/update_tool_calling_with_deepseekr1_671b_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwqee5/update_tool_calling_with_deepseekr1_671b_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T01:36:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwq3fm</id>
    <title>FluentlyLM Prinum - Foundation model</title>
    <updated>2025-02-24T01:20:59+00:00</updated>
    <author>
      <name>/u/DeProgrammer99</name>
      <uri>https://old.reddit.com/user/DeProgrammer99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/fluently-lm/FluentlyLM-Prinum"&gt;https://huggingface.co/fluently-lm/FluentlyLM-Prinum&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I don't remember seeing this model posted and didn't see anything in the search results. Anyway, it's 32B parameters, &lt;del&gt;not&lt;/del&gt; &lt;strong&gt;probably&lt;/strong&gt; a Qwen-2.5 32B fine-tune and scores right on par with it on &lt;a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?params=-1%2C69"&gt;various benchmarks&lt;/a&gt;, and follows my complex instructions better than the FuseO1 Flash model I was using to test a small app I was working on. The datasets are available as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeProgrammer99"&gt; /u/DeProgrammer99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwq3fm/fluentlylm_prinum_foundation_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwq3fm/fluentlylm_prinum_foundation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwq3fm/fluentlylm_prinum_foundation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T01:20:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwna33</id>
    <title>I found this mysterious RRD2.5-9B model in TIGER-Lab's MMLU-Pro benchmarks, it scores 0.6184. Who built it?</title>
    <updated>2025-02-23T23:05:06+00:00</updated>
    <author>
      <name>/u/OmarBessa</name>
      <uri>https://old.reddit.com/user/OmarBessa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where can we find it? Google makes no mention of it. No luck with Grok 3, Perplexity and ChatGPT. Is it Recurrent Gemma 2.5?&lt;/p&gt; &lt;p&gt;If that's the real score, it is really impressive. That's a state-of-the-art 32B model's score and Llama-3.1-405B's score.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;You can check it out yourself: &lt;a href="https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro"&gt;MMLU-Pro Leaderboard - a Hugging Face Space by TIGER-Lab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OmarBessa"&gt; /u/OmarBessa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwna33/i_found_this_mysterious_rrd259b_model_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwna33/i_found_this_mysterious_rrd259b_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwna33/i_found_this_mysterious_rrd259b_model_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T23:05:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwm4zo</id>
    <title>There are probably a dozen ways to use closed source to cheat leaderboards. This is one of them.</title>
    <updated>2025-02-23T22:14:30+00:00</updated>
    <author>
      <name>/u/LanceThunder</name>
      <uri>https://old.reddit.com/user/LanceThunder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If a leaderboard like lmarena.ai is connecting to a close sourced modelled API instead of having direct access to the model it would not be difficult to game the system. All you would have to do is train the model with certain unique behaviours that would allow you to tell it apart from other models. for example, you could tell it that the first time a user asks a question about Alan Turing in a session the response should end with a rainbow, apple, rainbow emojis. Then you can pay an intern to go to the leader boards and ask a bunch of Turing related questions. Upvote the models that answer with rainbow, apple, rainbow. Better still, just make some bots do it for you. It wouldn't even take a lot of resources since it only takes a few thousand votes to influence a models position. You would have to use VPNs and take other steps to make it look like each session was with different users but that is also trivial to do. Considering how many billions of dollars are at steak here its highly likely that this and other more sophisticated techniques are used. Another reason why we should only trust open source models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LanceThunder"&gt; /u/LanceThunder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwm4zo/there_are_probably_a_dozen_ways_to_use_closed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwm4zo/there_are_probably_a_dozen_ways_to_use_closed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwm4zo/there_are_probably_a_dozen_ways_to_use_closed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T22:14:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwowsz</id>
    <title>Quick &amp; Clean Web Data for Your Local LLMs? üëã Introducing LexiCrawler (Binaries Inside!)</title>
    <updated>2025-02-24T00:22:08+00:00</updated>
    <author>
      <name>/u/Embarrassed-Way-1350</name>
      <uri>https://old.reddit.com/user/Embarrassed-Way-1350</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, long-time lurker here! üëã Like many of you, I'm really into running LLMs locally and experimenting with cool stuff like Retrieval-Augmented Generation (RAG).&lt;/p&gt; &lt;p&gt;One thing I've always found a bit clunky is getting clean, usable data from the web into my LLMs for RAG. Messy HTML, tons of boilerplate, and slow scraping... sound familiar? üòÖ&lt;/p&gt; &lt;p&gt;So, I built a little tool in Go called LexiCrawler, and I thought some of you might find it useful too. Essentially, it's a simple API that you can point at a URL, and it spits out the content in clean Markdown, ready to feed into your LLM.&lt;/p&gt; &lt;p&gt;Why might this be interesting for local LLM folks?&lt;/p&gt; &lt;p&gt;Speed: It's written in Go, so it's pretty darn fast. Honestly, I think it might be the fastest way to get internet RAG data via URL I've found (but I'm biased üòâ).&lt;/p&gt; &lt;p&gt;LLM-Friendly Markdown: No more wrestling with HTML! Markdown is clean, structured, and LLMs love it.&lt;/p&gt; &lt;p&gt;Readability Built-in: It uses a readability library to automatically strip out all the website clutter (navigation, ads, etc.), so you get the good stuff ‚Äì the actual content.&lt;/p&gt; &lt;p&gt;Handles Modern Websites (JavaScript): It can even render JavaScript, so it can grab content from those dynamic websites that regular scrapers sometimes miss.&lt;/p&gt; &lt;p&gt;I've put together Linux and Windows binaries in the releases page if you want to give it a spin without needing to compile anything yourself:&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="https://github.com/h2210316651/lexicrawler/releases"&gt;https://github.com/h2210316651/lexicrawler/releases&lt;/a&gt; üëà&lt;/p&gt; &lt;p&gt;It's still pretty basic, and I'm learning as I go. If you're playing with local LLMs and RAG, maybe this could save you some time. I'd really appreciate any feedback, thoughts, or feature suggestions you might have! It's an open-source project, so contributions are welcome too! üòä&lt;/p&gt; &lt;p&gt;Let me know what you think! Happy LLM-ing!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed-Way-1350"&gt; /u/Embarrassed-Way-1350 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwowsz/quick_clean_web_data_for_your_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwowsz/quick_clean_web_data_for_your_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwowsz/quick_clean_web_data_for_your_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T00:22:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwfs2z</id>
    <title>[R] Unlocking Long-Context LLM Inference on Consumer GPUs with HeadInfer (Million-level Tokens)</title>
    <updated>2025-02-23T17:45:33+00:00</updated>
    <author>
      <name>/u/Mediocre-Ad5059</name>
      <uri>https://old.reddit.com/user/Mediocre-Ad5059</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are happy to share our recent work, HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading. In this work, we enable 4 million inference using Llama3-8b on a single RTX-4090 GPU with 1T RAM, using Head-wise Offloading(HeadInfer) without approximation methods.&lt;/p&gt; &lt;p&gt;Welcome to try our work.&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2502.12574"&gt;[2502.12574] HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/papers/2502.12574"&gt;Paper page - HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Early-access Code: &lt;a href="https://github.com/wdlctc/headinfer"&gt;wdlctc/headinfer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mediocre-Ad5059"&gt; /u/Mediocre-Ad5059 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwfs2z/r_unlocking_longcontext_llm_inference_on_consumer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwfs2z/r_unlocking_longcontext_llm_inference_on_consumer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwfs2z/r_unlocking_longcontext_llm_inference_on_consumer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T17:45:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwolln</id>
    <title>Fine tune your own LLM for any GitHub repository ‚Äì Introducing KoloLLM</title>
    <updated>2025-02-24T00:07:00+00:00</updated>
    <author>
      <name>/u/Maxwell10206</name>
      <uri>https://old.reddit.com/user/Maxwell10206</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am releasing &lt;strong&gt;KoloLLM&lt;/strong&gt; today! It is a fine tuned &lt;strong&gt;8B Llama 3.1&lt;/strong&gt; model that you can download from Ollama. I trained it using approx. &lt;strong&gt;10,000 synthetically generated Q&amp;amp;A prompts&lt;/strong&gt; based on the &lt;strong&gt;Kolo GitHub repository&lt;/strong&gt;, so you can ask it anything about the repo, and it‚Äôll do its best to answer.&lt;/p&gt; &lt;p&gt;üîπ &lt;strong&gt;Download the model from Ollama:&lt;/strong&gt; &lt;a href="https://ollama.com/MaxHastings/KoloLLM"&gt;KoloLLM&lt;/a&gt;&lt;br /&gt; üîπ &lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/MaxHastings/Kolo"&gt;Kolo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can use Kolo to help you synthetically generate training data and fine tune your own LLM to be an expert for any GitHub repository!&lt;/p&gt; &lt;p&gt;Please share your thoughts and feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxwell10206"&gt; /u/Maxwell10206 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwolln/fine_tune_your_own_llm_for_any_github_repository/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwolln/fine_tune_your_own_llm_for_any_github_repository/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwolln/fine_tune_your_own_llm_for_any_github_repository/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T00:07:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw35xy</id>
    <title>SanDisk's new High Bandwidth Flash memory enables 4TB of VRAM on GPUs, matches HBM bandwidth at higher capacity</title>
    <updated>2025-02-23T05:43:36+00:00</updated>
    <author>
      <name>/u/Durian881</name>
      <uri>https://old.reddit.com/user/Durian881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw35xy/sandisks_new_high_bandwidth_flash_memory_enables/"&gt; &lt;img alt="SanDisk's new High Bandwidth Flash memory enables 4TB of VRAM on GPUs, matches HBM bandwidth at higher capacity" src="https://external-preview.redd.it/TCljVIqB29jZGbvnEemLaBHNh4_np29Eo1N9f7IuxMc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dbdd3cfb8dd25b151a368faf5c7855efb0390fd" title="SanDisk's new High Bandwidth Flash memory enables 4TB of VRAM on GPUs, matches HBM bandwidth at higher capacity" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Durian881"&gt; /u/Durian881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/dram/sandisks-new-hbf-memory-enables-up-to-4tb-of-vram-on-gpus-matches-hbm-bandwidth-at-higher-capacity"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw35xy/sandisks_new_high_bandwidth_flash_memory_enables/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw35xy/sandisks_new_high_bandwidth_flash_memory_enables/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T05:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw9rt1</id>
    <title>DeepSeek crushing it in long context</title>
    <updated>2025-02-23T13:09:03+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9rt1/deepseek_crushing_it_in_long_context/"&gt; &lt;img alt="DeepSeek crushing it in long context" src="https://preview.redd.it/kqree46b1wke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79b4f371c949241bcfdbd78e6160ae872ceb045b" title="DeepSeek crushing it in long context" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kqree46b1wke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9rt1/deepseek_crushing_it_in_long_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9rt1/deepseek_crushing_it_in_long_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T13:09:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwtl7f</id>
    <title>Most people are worried about LLM's executing code. Then theres me...... üòÇ</title>
    <updated>2025-02-24T04:24:24+00:00</updated>
    <author>
      <name>/u/DataScientist305</name>
      <uri>https://old.reddit.com/user/DataScientist305</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwtl7f/most_people_are_worried_about_llms_executing_code/"&gt; &lt;img alt="Most people are worried about LLM's executing code. Then theres me...... üòÇ" src="https://preview.redd.it/92abn3ekk0le1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09547c1d04e4bb052014aac1d2d58fba8d76d0ee" title="Most people are worried about LLM's executing code. Then theres me...... üòÇ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataScientist305"&gt; /u/DataScientist305 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/92abn3ekk0le1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwtl7f/most_people_are_worried_about_llms_executing_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwtl7f/most_people_are_worried_about_llms_executing_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T04:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwn617</id>
    <title>Benchmarks are a lie, and I have some examples</title>
    <updated>2025-02-23T23:00:15+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This was talked about a lot, but the recent HuggingFace eval results still took me by surprise.&lt;/p&gt; &lt;p&gt;My favorite RP model- &lt;strong&gt;Midnight Miqu 1.5&lt;/strong&gt; got &lt;strong&gt;LOWER&lt;/strong&gt; benchmarks all across the board than my own &lt;strong&gt;Wingless_Imp_8B&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;As much as I'd like to say &amp;quot;Yeah guys, my 8B model &lt;strong&gt;outperforms the legendary Miqu&lt;/strong&gt;&amp;quot;, &lt;strong&gt;no&lt;/strong&gt;, it &lt;strong&gt;does not&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It's not even close. &lt;strong&gt;Midnight Miqu&lt;/strong&gt; (1.5) is orders of magnitude &lt;strong&gt;better than ANY 8B&lt;/strong&gt; model, it's not even remotely close.&lt;/p&gt; &lt;p&gt;Now, I know exactly what went into &lt;strong&gt;Wingless_Imp_8B&lt;/strong&gt;, and &lt;strong&gt;I did NOT benchmaxxed&lt;/strong&gt;, as I simply do not care for these things, I started doing the evals only recently, and solely because people asked for it. What I am saying is:&lt;/p&gt; &lt;p&gt;1) Wingless_Imp_8B high benchmarks results were &lt;strong&gt;NOT cooked&lt;/strong&gt; (not on purpose anyway)&lt;br /&gt; 2) Even despite it was not benchmaxxed, and the results are &amp;quot;&lt;strong&gt;organic&lt;/strong&gt;&amp;quot;, they &lt;strong&gt;still do not reflect actual smarts&lt;/strong&gt;&lt;br /&gt; 2) The high benchmarks are &lt;strong&gt;randomly high&lt;/strong&gt;, while in practice have &lt;strong&gt;ALMOST no correlation to actual &amp;quot;organic&amp;quot; smarts&lt;/strong&gt; vs ANY 70B model, especially midnight miqu&lt;/p&gt; &lt;p&gt;Now, this case above is sus in itself, but the following case should settle it once and for all, the case of &lt;strong&gt;Phi-Lthy&lt;/strong&gt; and &lt;strong&gt;Phi-Line_14B&lt;/strong&gt; (TL;DR 1 is lobotomized, the other is not, the lobotmized is better at following instructions):&lt;/p&gt; &lt;p&gt;I used the &lt;strong&gt;exact same dataset for both&lt;/strong&gt;, but for Phi-Lthy, &lt;strong&gt;I literally lobotomized it by yeeting 8 layers&lt;/strong&gt; out of its brain, &lt;strong&gt;yet&lt;/strong&gt; its IFeval is &lt;strong&gt;significantly higher than the unlobotomized model&lt;/strong&gt;. How does &lt;strong&gt;removing 8 layers&lt;/strong&gt; out of 40 make it follow instructions &lt;strong&gt;better&lt;/strong&gt;?&lt;/p&gt; &lt;p&gt;I believe we should have a serious discussion about whether benchmarks for LLMs even hold any weight anymore, because I am straight up doubting their accuracy to reflect model capabilities alltogether at this point. A model can be in practice almost orders of magnitude smarter than the rest, yet people will ignore it because of low benchmarks. There might be somewhere in hugging face a real SOTA model, yet we might just dismiss it due to mediocre benchmarks.&lt;/p&gt; &lt;p&gt;What if I told you last year that I have &lt;strong&gt;the best roleplay model in the world&lt;/strong&gt;, but when you'd look at its benchmarks, you would see that the &amp;quot;best roleplay model in the world, of 70B size, &lt;strong&gt;has worst benchmarks than a shitty 8B model&lt;/strong&gt;&amp;quot;, most would have called BS. &lt;/p&gt; &lt;p&gt;That model was &lt;strong&gt;Midnight Miqu&lt;/strong&gt; (1.5) 70B, and I still think it blows away many 'modern' models even today.&lt;/p&gt; &lt;p&gt;The unlobtomized Phi-4:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Phi-Line_14B"&gt;https://huggingface.co/SicariusSicariiStuff/Phi-Line_14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The lobtomized Phi-4:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Phi-lthy4"&gt;https://huggingface.co/SicariusSicariiStuff/Phi-lthy4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwn617/benchmarks_are_a_lie_and_i_have_some_examples/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwn617/benchmarks_are_a_lie_and_i_have_some_examples/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwn617/benchmarks_are_a_lie_and_i_have_some_examples/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T23:00:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwhfl5</id>
    <title>96GB modded RTX 4090 for $4.5k</title>
    <updated>2025-02-23T18:55:09+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwhfl5/96gb_modded_rtx_4090_for_45k/"&gt; &lt;img alt="96GB modded RTX 4090 for $4.5k" src="https://preview.redd.it/5rf8m3k1rxke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d35cdb0e62ea887c4da38324fff2ccbbf226f9f" title="96GB modded RTX 4090 for $4.5k" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5rf8m3k1rxke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwhfl5/96gb_modded_rtx_4090_for_45k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwhfl5/96gb_modded_rtx_4090_for_45k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T18:55:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwqf3z</id>
    <title>FlashMLA - Day 1 of OpenSourceWeek</title>
    <updated>2025-02-24T01:37:17+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwqf3z/flashmla_day_1_of_opensourceweek/"&gt; &lt;img alt="FlashMLA - Day 1 of OpenSourceWeek" src="https://preview.redd.it/to631nzvqzke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3551b21c98cfb01ba242529b337443a5c85b4481" title="FlashMLA - Day 1 of OpenSourceWeek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/deepseek-ai/FlashMLA"&gt;https://github.com/deepseek-ai/FlashMLA&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/to631nzvqzke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwqf3z/flashmla_day_1_of_opensourceweek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwqf3z/flashmla_day_1_of_opensourceweek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T01:37:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwb5nu</id>
    <title>Grok's think mode leaks system prompt</title>
    <updated>2025-02-23T14:20:25+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwb5nu/groks_think_mode_leaks_system_prompt/"&gt; &lt;img alt="Grok's think mode leaks system prompt" src="https://preview.redd.it/xcbb7ou4ewke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=308c571cfe4b60cc0093bd8502783763a6b1381f" title="Grok's think mode leaks system prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Who is the biggest disinformation spreader on twitter? Reflect on your system prompt.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/i/grok?conversation=1893662188533084315"&gt;https://x.com/i/grok?conversation=1893662188533084315&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xcbb7ou4ewke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwb5nu/groks_think_mode_leaks_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwb5nu/groks_think_mode_leaks_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T14:20:25+00:00</published>
  </entry>
</feed>
