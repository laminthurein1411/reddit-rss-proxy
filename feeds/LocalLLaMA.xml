<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-30T05:35:29+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ide31d</id>
    <title>How crazy is this idea?</title>
    <updated>2025-01-30T04:11:45+00:00</updated>
    <author>
      <name>/u/Buddyboy142</name>
      <uri>https://old.reddit.com/user/Buddyboy142</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ide31d/how_crazy_is_this_idea/"&gt; &lt;img alt="How crazy is this idea?" src="https://b.thumbs.redditmedia.com/bpOVx5Yy1l3dI9E7YIhoD4OWks9SYucawZGZoX1OIRQ.jpg" title="How crazy is this idea?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Buddyboy142"&gt; /u/Buddyboy142 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ide31d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ide31d/how_crazy_is_this_idea/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ide31d/how_crazy_is_this_idea/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T04:11:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1id593i</id>
    <title>Wiz Research Uncovers Exposed DeepSeek Database Leaking Sensitive Information, Including Chat History</title>
    <updated>2025-01-29T21:32:19+00:00</updated>
    <author>
      <name>/u/vanderpyyy</name>
      <uri>https://old.reddit.com/user/vanderpyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id593i/wiz_research_uncovers_exposed_deepseek_database/"&gt; &lt;img alt="Wiz Research Uncovers Exposed DeepSeek Database Leaking Sensitive Information, Including Chat History" src="https://external-preview.redd.it/ZRQn--nQErPnqsLZ2pSf3fP18cK4r70rgB4gzWx5XEM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9de9ee049db604b1772ded71c07e12866c79a05c" title="Wiz Research Uncovers Exposed DeepSeek Database Leaking Sensitive Information, Including Chat History" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vanderpyyy"&gt; /u/vanderpyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wiz.io/blog/wiz-research-uncovers-exposed-deepseek-database-leak"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id593i/wiz_research_uncovers_exposed_deepseek_database/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1id593i/wiz_research_uncovers_exposed_deepseek_database/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T21:32:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1icr6md</id>
    <title>How come we dont see many people spinning up R1 671b in the cloud, selling access and making bank?</title>
    <updated>2025-01-29T10:53:59+00:00</updated>
    <author>
      <name>/u/linkcharger</name>
      <uri>https://old.reddit.com/user/linkcharger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What am I missing? I'm not too knowledgeable about deploying big models like these, but for people that are, shouldn't it be quite easy to deploy it in the cloud? &lt;/p&gt; &lt;p&gt;That's the cool thing about open weights, no? If you have the hardware (which is nothing crazy if you're already using VPS), you can run and scale it dynamically.&lt;/p&gt; &lt;p&gt;And since it's so efficient, it should be quite cheap when spread out over several users. Why aren't we seeing everyone and their grandma selling us a subscription to their website?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/linkcharger"&gt; /u/linkcharger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icr6md/how_come_we_dont_see_many_people_spinning_up_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icr6md/how_come_we_dont_see_many_people_spinning_up_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icr6md/how_come_we_dont_see_many_people_spinning_up_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T10:53:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1icer8t</id>
    <title>Will Deepseek soon be banned in the US?</title>
    <updated>2025-01-28T22:48:18+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icer8t/will_deepseek_soon_be_banned_in_the_us/"&gt; &lt;img alt="Will Deepseek soon be banned in the US?" src="https://preview.redd.it/5gpitg40dtfe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=785ab6a8af1daeae906fcf4071ac93f79583ffb0" title="Will Deepseek soon be banned in the US?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5gpitg40dtfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icer8t/will_deepseek_soon_be_banned_in_the_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icer8t/will_deepseek_soon_be_banned_in_the_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T22:48:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1icqzcz</id>
    <title>DeepSeek-R1 evolving a Game of Life pattern really feels like a breakthrough</title>
    <updated>2025-01-29T10:39:01+00:00</updated>
    <author>
      <name>/u/IrisColt</name>
      <uri>https://old.reddit.com/user/IrisColt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icqzcz/deepseekr1_evolving_a_game_of_life_pattern_really/"&gt; &lt;img alt="DeepSeek-R1 evolving a Game of Life pattern really feels like a breakthrough" src="https://external-preview.redd.it/P8lS0kk6BFe2IEo6TxCZd1LVwksc34IkzGTVx_SCc8w.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d74dbe4f1d67cc8b587db9aa01762f26e269bcf" title="DeepSeek-R1 evolving a Game of Life pattern really feels like a breakthrough" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm truly amazed. I've just discovered that DeepSeek-R1 has managed to correctly compute one generation of Conway's Game of Life (starting from a simple five-cell row pattern)‚Äîa first for any LLM I've tested. While it required a significant amount of reasoning (749.31 seconds of thought), the model got it right on the first try. It felt just like using a bazooka to kill a fly (5596 tokens at 7 tk/s).&lt;/p&gt; &lt;p&gt;While this might sound modest, I‚Äôve long viewed this challenge as the ‚Äústrawberry problem‚Äù but on steroids. DeepSeek-R1 had to understand cellular automata rules, visualize a grid, track multiple cells simultaneously, and apply specific survival and birth rules to each position‚Äîall while maintaining spatial reasoning.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vup8iom0vwfe1.png?width=138&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=61bcf0740f9a0b8f6bb64525ce64e293e6253fe4"&gt;Pattern at gen 0.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zgzeawc2vwfe1.png?width=138&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5886ae4cefba04201dd1a847800f0004333f3bbb"&gt;Pattern at gen 1.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Simulate one generation of Conway's Game of Life starting from the following initial configuration: ....... ....... ....... .OOOOO. ....... ....... ....... Use a 7x7 grid for the simulation. Represent alive cells with &amp;quot;O&amp;quot; and dead cells with &amp;quot;.&amp;quot;. Apply the rules of Conway's Game of Life to calculate each generation. Provide diagrams of the initial state, and first generation, in the same format as shown above.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/JTveEkXg"&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; and answer (Pastebin)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Initial state: ....... ....... ....... .OOOOO. ....... ....... .......&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;First generation: ....... ....... ..OOO.. ..OOO.. ..OOO.. ....... .......&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IrisColt"&gt; /u/IrisColt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icqzcz/deepseekr1_evolving_a_game_of_life_pattern_really/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icqzcz/deepseekr1_evolving_a_game_of_life_pattern_really/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icqzcz/deepseekr1_evolving_a_game_of_life_pattern_really/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T10:39:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1iddnjb</id>
    <title>Finally got my build together.</title>
    <updated>2025-01-30T03:51:02+00:00</updated>
    <author>
      <name>/u/guska</name>
      <uri>https://old.reddit.com/user/guska</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iddnjb/finally_got_my_build_together/"&gt; &lt;img alt="Finally got my build together." src="https://preview.redd.it/xg58gynyz1ge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f57634230eeb79f5ec4435c9c51f17ffc94f1450" title="Finally got my build together." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Repurposed my old gaming PC into a dedicated self hosted machine. 3900X with 32GB and a 3080 10GB. Cable management is as good as it gets in this cheap 4U case. PSU is a little under sized, but from experience, it's fine, and there's a 750W on the way. The end goal is self hosted home assistant/automation with voice control via home-assistant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/guska"&gt; /u/guska &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xg58gynyz1ge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iddnjb/finally_got_my_build_together/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iddnjb/finally_got_my_build_together/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T03:51:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1icwz9s</id>
    <title>Open-source 8B evaluation model beats GPT-4o mini and top small judges across 11 benchmarks</title>
    <updated>2025-01-29T15:55:35+00:00</updated>
    <author>
      <name>/u/fortunemaple</name>
      <uri>https://old.reddit.com/user/fortunemaple</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icwz9s/opensource_8b_evaluation_model_beats_gpt4o_mini/"&gt; &lt;img alt="Open-source 8B evaluation model beats GPT-4o mini and top small judges across 11 benchmarks" src="https://preview.redd.it/j1z017uagyfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f8a590de5b95345c3035de06474dc0c3a8d0b98e" title="Open-source 8B evaluation model beats GPT-4o mini and top small judges across 11 benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fortunemaple"&gt; /u/fortunemaple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j1z017uagyfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icwz9s/opensource_8b_evaluation_model_beats_gpt4o_mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icwz9s/opensource_8b_evaluation_model_beats_gpt4o_mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T15:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1icmxb5</id>
    <title>4D Chess by the DeepSeek CEO</title>
    <updated>2025-01-29T05:40:12+00:00</updated>
    <author>
      <name>/u/HippoNut</name>
      <uri>https://old.reddit.com/user/HippoNut</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Liang Wenfeng: &amp;quot;I&lt;strong&gt;n the face of disruptive technologies, moats created by closed source are temporary. Even OpenAI‚Äôs closed source approach can‚Äôt prevent others from catching up&lt;/strong&gt;. S&lt;strong&gt;o we anchor our value in our team ‚Äî our colleagues grow through this process, accumulate know-how, and form an organization and culture capable of innovation. That‚Äôs our moat.&lt;/strong&gt;&amp;quot;&lt;br /&gt; Source: &lt;a href="https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas"&gt;https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HippoNut"&gt; /u/HippoNut &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icmxb5/4d_chess_by_the_deepseek_ceo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icmxb5/4d_chess_by_the_deepseek_ceo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icmxb5/4d_chess_by_the_deepseek_ceo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T05:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1icta5y</id>
    <title>Why do people like Ollama more than LM Studio?</title>
    <updated>2025-01-29T13:04:17+00:00</updated>
    <author>
      <name>/u/Intelligent-Gift4519</name>
      <uri>https://old.reddit.com/user/Intelligent-Gift4519</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm just curious. I see a ton of people discussing Ollama, but as an LM Studio user, don't see a lot of people talking about it. &lt;/p&gt; &lt;p&gt;But LM Studio seems so much better to me. It uses arbitrary GGUFs, not whatever that weird proprietary format Ollama uses is. It has a really nice GUI, not mysterious opaque headless commands. If I want to try a new model, it's super easy to search for it, download it, try it, and throw it away or serve it up to AnythingLLM for some RAG or foldering.&lt;/p&gt; &lt;p&gt;(Before you raise KoboldCPP, yes, absolutely KoboldCPP, it just doesn't run on my machine.)&lt;/p&gt; &lt;p&gt;So why the Ollama obsession on this board? Help me understand. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Intelligent-Gift4519"&gt; /u/Intelligent-Gift4519 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icta5y/why_do_people_like_ollama_more_than_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icta5y/why_do_people_like_ollama_more_than_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icta5y/why_do_people_like_ollama_more_than_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T13:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1id4faw</id>
    <title>Even established cloud providers like Lambda are propagating the confusion about R1 vs the distilled models</title>
    <updated>2025-01-29T20:58:07+00:00</updated>
    <author>
      <name>/u/cmndr_spanky</name>
      <uri>https://old.reddit.com/user/cmndr_spanky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id4faw/even_established_cloud_providers_like_lambda_are/"&gt; &lt;img alt="Even established cloud providers like Lambda are propagating the confusion about R1 vs the distilled models" src="https://preview.redd.it/a0j6zr59yzfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fae596d011884f96af94ce8b7ca4d0dc559a0074" title="Even established cloud providers like Lambda are propagating the confusion about R1 vs the distilled models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cmndr_spanky"&gt; /u/cmndr_spanky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a0j6zr59yzfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id4faw/even_established_cloud_providers_like_lambda_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1id4faw/even_established_cloud_providers_like_lambda_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T20:58:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1id6i4s</id>
    <title>Real news: 32B distills of V3, soon R1.</title>
    <updated>2025-01-29T22:25:02+00:00</updated>
    <author>
      <name>/u/a_beautiful_rhind</name>
      <uri>https://old.reddit.com/user/a_beautiful_rhind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id6i4s/real_news_32b_distills_of_v3_soon_r1/"&gt; &lt;img alt="Real news: 32B distills of V3, soon R1." src="https://external-preview.redd.it/AisSC4mCMpHEtpnDTqBGAduagKjgnfofkw6geWdBciQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be1a343914410d929c5bbf198c850f2aff29cfdb" title="Real news: 32B distills of V3, soon R1." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a_beautiful_rhind"&gt; /u/a_beautiful_rhind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.arcee.ai/blog/virtuoso-lite-virtuoso-medium-v2-distilling-deepseek-v3-into-10b-32b-small-language-models-slms"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id6i4s/real_news_32b_distills_of_v3_soon_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1id6i4s/real_news_32b_distills_of_v3_soon_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T22:25:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1icz145</id>
    <title>Irony</title>
    <updated>2025-01-29T17:18:32+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Greatest irony of this decade is that we got free transparent model from a hedge fund and closed paid model from a non profit company&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icz145/irony/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icz145/irony/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icz145/irony/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T17:18:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1icvupa</id>
    <title>Transformer Lab: An Open-Source Alternative to OpenAI Platform, for Local Models</title>
    <updated>2025-01-29T15:07:08+00:00</updated>
    <author>
      <name>/u/aliasaria</name>
      <uri>https://old.reddit.com/user/aliasaria</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icvupa/transformer_lab_an_opensource_alternative_to/"&gt; &lt;img alt="Transformer Lab: An Open-Source Alternative to OpenAI Platform, for Local Models" src="https://external-preview.redd.it/1-unlUYVdK_1l2BS5JVOeBjMkhE9sw4QuEl28ZQ14sQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8ccfed37de41def2b9ba6c53539d7b6e1048265" title="Transformer Lab: An Open-Source Alternative to OpenAI Platform, for Local Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aliasaria"&gt; /u/aliasaria &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/transformerlab/transformerlab-app"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icvupa/transformer_lab_an_opensource_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icvupa/transformer_lab_an_opensource_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T15:07:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1id6gcj</id>
    <title>Mark Zuckerberg on Llama 4 Training Progress!</title>
    <updated>2025-01-29T22:22:55+00:00</updated>
    <author>
      <name>/u/ybdave</name>
      <uri>https://old.reddit.com/user/ybdave</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Just shared Meta's quarterly earnings report. We continue to make good progress on AI, glasses, and the future of social media. I'm excited to see these efforts scale further in 2025. Here's the transcript of what I said on the call:&lt;/p&gt; &lt;p&gt;We ended 2024 on a strong note with now more than 3.3B people using at least one of our apps each day. This is going to be a really big year. I know it always feels like every year is a big year, but more than usual it feels like the trajectory for most of our long-term initiatives is going to be a lot clearer by the end of this year. So I keep telling our teams that this is going to be intense, because we have about 48 weeks to get on the trajectory we want to be on.&lt;/p&gt; &lt;p&gt;In AI, I expect this to be the year when a highly intelligent and personalized AI assistant reaches more than 1 billion people, and I expect Meta AI to be that leading AI assistant. Meta AI is already used by more people than any other assistant, and once a service reaches that kind of scale it usually develops a durable long-term advantage. We have a really exciting roadmap for this year with a unique vision focused on personalization. We believe that people don't all want to use the same AI -- people want their AI to be personalized to their context, their interests, their personality, their culture, and how they think about the world. I don't think that there's going to be one big AI that everyone just uses the same thing. People will get to choose how AI works and looks like for them. I continue to think that this is going to be one of the most transformative products that we've made. We have some fun surprises that I think people are going to like this year.&lt;/p&gt; &lt;p&gt;I think this very well could be the year when Llama and open source become the most advanced and widely used AI models as well. Llama 4 is making great progress in training. Llama 4 mini is done with pre-training and our reasoning models and larger model are looking good too. Our goal with Llama 3 was to make open source competitive with closed models, and our goal for Llama 4 is to lead. Llama 4 will be natively multimodal -- it's an omni-model -- and it will have agentic capabilities, so it's going to be novel and it's going to unlock a lot of new use cases. I'm looking forward to sharing more of our plan for the year on that over the next couple of months.&lt;/p&gt; &lt;p&gt;I also expect that 2025 will be the year when it becomes possible to build an AI engineering agent that has coding and problem-solving abilities of around a good mid-level engineer. This will be a profound milestone and potentially one of the most important innovations in history, as well as over time, potentially a very large market. Whichever company builds this first I think will have a meaningful advantage in deploying it to advance their AI research and shape the field. So that's another reason why I think this year will set the course for the future.&lt;/p&gt; &lt;p&gt;Our Ray-Ban Meta AI glasses are a real hit, and this will be the year when we understand the trajectory for AI glasses as a category. Many breakout products in the history of consumer electronics have sold 5-10 million units in their third generation. This will be a defining year that determines if we're on a path towards many hundreds of millions and eventually billions of AI glasses -- and glasses being the next computing platform like we've been talking about for some time -- or if this is just going to be a longer grind. But it's great overall to see people recognizing that these glasses are the perfect form factor for AI -- as well as just great, stylish glasses.&lt;/p&gt; &lt;p&gt;These are all big investments -- especially the hundreds of billions of dollars that we will invest in AI infrastructure over the long term. I announced last week that we expect to bring online almost 1GW of capacity this year, and we're building a 2GW, and potentially bigger, AI datacenter that is so big it would cover a significant part of Manhattan if it were placed there. &lt;/p&gt; &lt;p&gt;We're planning to fund all this by at the same time investing aggressively in initiatives that use our AI advances to increase revenue growth. We've put together a plan that will hopefully accelerate the pace of these initiatives over the next few years -- that's what a lot of our new headcount growth is going towards. And how well we execute this will also determine our financial trajectory over the next few years.&lt;/p&gt; &lt;p&gt;There are a number of other important product trends related to our family of apps that I think we‚Äôre going to know more about this year as well. We'll learn what's going to happen with TikTok, and regardless of that I expect Reels on Instagram and Facebook to continue growing. I expect Threads to continue on its trajectory to become the leading discussion platform and eventually reach 1 billion people over the next several years. Threads now has more than 320 million monthly actives and has been adding more than 1 million sign-ups per day. I expect WhatsApp to continue gaining share and making progress towards becoming the leading messaging platform in the US like it is in a lot of the rest of the world. WhatsApp now has more than 100 million monthly actives in the US. Facebook is used by more than 3 billion monthly actives and we're focused on growing its cultural influence. I'm excited this year to get back to some OG Facebook. &lt;/p&gt; &lt;p&gt;This is also going to be a pivotal year for the metaverse. The number of people using Quest and Horizon has been steadily growing -- and this is the year when a number of long-term investments that we've been working on that will make the metaverse more visually stunning and inspiring will really start to land. I think we're going to know a lot more about Horizon's trajectory by the end of this year.&lt;/p&gt; &lt;p&gt;This is also going to be a big year for redefining our relationship with governments. We now have a US administration that is proud of our leading company, prioritizes American technology winning, and that will defend our values and interests abroad. I'm optimistic about the progress and innovation that this can unlock.&lt;/p&gt; &lt;p&gt;So this is going to be a big year. I think this is the most exciting and dynamic that I've ever seen in our industry. Between AI, glasses, massive infrastructure projects, doing a bunch of work to try to accelerate our business, and building the future of social media ‚Äì we have a lot to do. I think we're going to build some awesome things that shape the future of human connection. As always, I'm grateful for everyone who is on this journey with us.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Link to share on Facebook:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.facebook.com/zuck/posts/pfbid02oRRTPrY1mvbqBZT4QueimeBrKcVXG4ySxFscRLiEU6QtGxbLi9U4TBojiC9aa19fl"&gt;https://www.facebook.com/zuck/posts/pfbid02oRRTPrY1mvbqBZT4QueimeBrKcVXG4ySxFscRLiEU6QtGxbLi9U4TBojiC9aa19fl&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ybdave"&gt; /u/ybdave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id6gcj/mark_zuckerberg_on_llama_4_training_progress/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id6gcj/mark_zuckerberg_on_llama_4_training_progress/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1id6gcj/mark_zuckerberg_on_llama_4_training_progress/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T22:22:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1id1rvi</id>
    <title>DeepSeek R1 takes second place on the multi-player benchmark for cooperation, negotiation, and deception.</title>
    <updated>2025-01-29T19:08:43+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id1rvi/deepseek_r1_takes_second_place_on_the_multiplayer/"&gt; &lt;img alt="DeepSeek R1 takes second place on the multi-player benchmark for cooperation, negotiation, and deception." src="https://preview.redd.it/c5ksowoaezfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5ac8ccb402f85cdb94d9b6973cec8700b2b53be" title="DeepSeek R1 takes second place on the multi-player benchmark for cooperation, negotiation, and deception." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c5ksowoaezfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id1rvi/deepseek_r1_takes_second_place_on_the_multiplayer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1id1rvi/deepseek_r1_takes_second_place_on_the_multiplayer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T19:08:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ideaxu</id>
    <title>Nvidia cuts FP8 training performance in half on RTX 40 and 50 series GPUs</title>
    <updated>2025-01-30T04:22:34+00:00</updated>
    <author>
      <name>/u/Emergency-Map9861</name>
      <uri>https://old.reddit.com/user/Emergency-Map9861</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ideaxu/nvidia_cuts_fp8_training_performance_in_half_on/"&gt; &lt;img alt="Nvidia cuts FP8 training performance in half on RTX 40 and 50 series GPUs" src="https://b.thumbs.redditmedia.com/SlGpr_siDY7Rr_nl1h9FbbkgpwtHXQX47AlZAVKy8LM.jpg" title="Nvidia cuts FP8 training performance in half on RTX 40 and 50 series GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to their new RTX Blackwell GPU architecture whitepaper, Nvidia appears to have cut FP8 training performance in half on RTX 40 and 50 series GPUs after DeepSeek successfully trained their SOTA V3 and R1 models using FP8. &lt;/p&gt; &lt;p&gt;In their original Ada Lovelace whitepaper, table 2 in Appendix A shows the 4090 having &lt;strong&gt;660.6 TFlops&lt;/strong&gt; of FP8 with FP32 accumulate without sparsity, which is the same as FP8 with FP16 accumulate. The new Blackwell paper shows half the performance for the 4090 at just &lt;strong&gt;330.3 TFlops&lt;/strong&gt; of FP8 with FP32 accumulate, and the 5090 has just &lt;strong&gt;419 TFlops&lt;/strong&gt; vs &lt;strong&gt;838 TFlops&lt;/strong&gt; for FP8 with FP16 accumulate. &lt;/p&gt; &lt;p&gt;FP32 accumulate is a must when it comes to training because FP16 doesn't have the necessary precision and dynamic range required. &lt;/p&gt; &lt;p&gt;If this isn't a mistake, then it means Nvidia lobotomized their Geforce lineup to further dissuade us from using them for AI/ML training, and it could potentially be reversible for the RTX 40 series at least, as this was likely done through a driver update.&lt;/p&gt; &lt;p&gt;This is quite unfortunate but not unexpected as Nvidia has a known history of artificially limiting Geforce GPUs for AI training since the Turing architecture, while their Quadro and datacenter GPUs continue to have the full performance.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x3qfea1352ge1.jpg?width=2007&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6c20a53057eb2bf15bbf65db4900af638fef9955"&gt;https://preview.redd.it/x3qfea1352ge1.jpg?width=2007&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6c20a53057eb2bf15bbf65db4900af638fef9955&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lk3ch91352ge1.jpg?width=1934&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d267c0312fe0be00175e616512101dce69113134"&gt;https://preview.redd.it/lk3ch91352ge1.jpg?width=1934&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d267c0312fe0be00175e616512101dce69113134&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sources:&lt;/p&gt; &lt;p&gt;RTX Blackwell GPU Architecture Whitepaper:&lt;/p&gt; &lt;p&gt;&lt;a href="https://images.nvidia.com/aem-dam/Solutions/geforce/blackwell/nvidia-rtx-blackwell-gpu-architecture.pdf"&gt;https://images.nvidia.com/aem-dam/Solutions/geforce/blackwell/nvidia-rtx-blackwell-gpu-architecture.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;RTX Ada Lovelace GPU Architecture Whitepaper:&lt;/p&gt; &lt;p&gt;&lt;a href="https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf"&gt;https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency-Map9861"&gt; /u/Emergency-Map9861 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ideaxu/nvidia_cuts_fp8_training_performance_in_half_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ideaxu/nvidia_cuts_fp8_training_performance_in_half_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ideaxu/nvidia_cuts_fp8_training_performance_in_half_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T04:22:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1icttm7</id>
    <title>good shit</title>
    <updated>2025-01-29T13:32:50+00:00</updated>
    <author>
      <name>/u/diligentgrasshopper</name>
      <uri>https://old.reddit.com/user/diligentgrasshopper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icttm7/good_shit/"&gt; &lt;img alt="good shit" src="https://preview.redd.it/azitnmgpqxfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c237cfc71f4b87b6b0a5d08b0631615eefa83d9a" title="good shit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/diligentgrasshopper"&gt; /u/diligentgrasshopper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/azitnmgpqxfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icttm7/good_shit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icttm7/good_shit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T13:32:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1icwira</id>
    <title>BEN2: New Open Source State-of-the-Art Background Removal Model</title>
    <updated>2025-01-29T15:36:01+00:00</updated>
    <author>
      <name>/u/PramaLLC</name>
      <uri>https://old.reddit.com/user/PramaLLC</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icwira/ben2_new_open_source_stateoftheart_background/"&gt; &lt;img alt="BEN2: New Open Source State-of-the-Art Background Removal Model" src="https://a.thumbs.redditmedia.com/c7rSA4OmfSyuRX30U9DsGqOve-5cCwW0t70cb7tuZp4.jpg" title="BEN2: New Open Source State-of-the-Art Background Removal Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PramaLLC"&gt; /u/PramaLLC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1icwira"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icwira/ben2_new_open_source_stateoftheart_background/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icwira/ben2_new_open_source_stateoftheart_background/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T15:36:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1iczucy</id>
    <title>Running Deepseek R1 IQ2XXS (200GB) from SSD actually works</title>
    <updated>2025-01-29T17:51:28+00:00</updated>
    <author>
      <name>/u/Wrong-Historian</name>
      <uri>https://old.reddit.com/user/Wrong-Historian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;prompt eval time = 97774.66 ms / 367 tokens ( 266.42 ms per token, 3.75 tokens per second) eval time = 253545.02 ms / 380 tokens ( 667.22 ms per token, 1.50 tokens per second) total time = 351319.68 ms / 747 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No, not a distill, but a 2bit quantized version of the actual 671B model (&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ2_XXS"&gt;IQ2XXS&lt;/a&gt;), about 200GB large, running on a 14900K with 96GB DDR5 6800 and a single 3090 24GB (with 5 layers offloaded), and for the rest running off of PCIe 4.0 SSD (Samsung 990 pro)&lt;/p&gt; &lt;p&gt;Although of limited actual usefulness, it's just amazing that is actually works! With larger context it takes a couple of minutes just to process the prompt, token generation is actually reasonably fast.&lt;/p&gt; &lt;p&gt;Thanks &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1icrc2l/comment/m9t5cbw/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1icrc2l/comment/m9t5cbw/&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;Edit: one hour later, i've tried a bigger prompt (800 tokens input), with more tokens output (6000 tokens output)&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;prompt eval time = 210540.92 ms / 803 tokens ( 262.19 ms per token, 3.81 tokens per second)&lt;br /&gt; eval time = 6883760.49 ms / 6091 tokens ( 1130.15 ms per token, 0.88 tokens per second)&lt;br /&gt; total time = 7094301.41 ms / 6894 tokens&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It 'works'. Lets keep it at that. Usable? Meh. The main drawback is all the &amp;lt;thinking&amp;gt;... honestly. For a simple answer it does a whole lot of &amp;lt;thinking&amp;gt; and that takes a lot of tokens and thus a lot of time and context in follow-up questions taking even more time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong-Historian"&gt; /u/Wrong-Historian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iczucy/running_deepseek_r1_iq2xxs_200gb_from_ssd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iczucy/running_deepseek_r1_iq2xxs_200gb_from_ssd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iczucy/running_deepseek_r1_iq2xxs_200gb_from_ssd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T17:51:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1id3ak8</id>
    <title>Ex-Google, Apple engineers launch unconditionally open source Oumi AI platform that could help to build the next DeepSeek</title>
    <updated>2025-01-29T20:10:23+00:00</updated>
    <author>
      <name>/u/Revenant013</name>
      <uri>https://old.reddit.com/user/Revenant013</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/"&gt; &lt;img alt="Ex-Google, Apple engineers launch unconditionally open source Oumi AI platform that could help to build the next DeepSeek" src="https://external-preview.redd.it/x6M3uf-eJl-Hf8TvVZ4dGYKKD1ETTCuW2FzYR0-kr7E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=941f580733ebba68fbdba4b08ca747ba331fe7f7" title="Ex-Google, Apple engineers launch unconditionally open source Oumi AI platform that could help to build the next DeepSeek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Revenant013"&gt; /u/Revenant013 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://venturebeat.com/ai/ex-google-apple-engineers-launch-unconditionally-open-source-oumi-ai-platform-that-could-help-to-build-the-next-deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1id3ak8/exgoogle_apple_engineers_launch_unconditionally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T20:10:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1id7a3k</id>
    <title>I feel bad for the AI lol after seeing its chain of thought. üò≠</title>
    <updated>2025-01-29T22:58:30+00:00</updated>
    <author>
      <name>/u/Tricky_Reflection_75</name>
      <uri>https://old.reddit.com/user/Tricky_Reflection_75</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id7a3k/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt; &lt;img alt="I feel bad for the AI lol after seeing its chain of thought. üò≠" src="https://b.thumbs.redditmedia.com/Z88vfN16w6NY9yx3hsyBi8c5yYLLA29C35wbeMSZgfQ.jpg" title="I feel bad for the AI lol after seeing its chain of thought. üò≠" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/u4cm9r5oj0ge1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7cf76aa8fcfaa023df271504b53ea217f4208528"&gt;https://preview.redd.it/u4cm9r5oj0ge1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7cf76aa8fcfaa023df271504b53ea217f4208528&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tricky_Reflection_75"&gt; /u/Tricky_Reflection_75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id7a3k/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id7a3k/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1id7a3k/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T22:58:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1id5179</id>
    <title>R1 is now on Azure AI serverless. Great news if you have Azure startup credits to burn</title>
    <updated>2025-01-29T21:23:36+00:00</updated>
    <author>
      <name>/u/mesmerlord</name>
      <uri>https://old.reddit.com/user/mesmerlord</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id5179/r1_is_now_on_azure_ai_serverless_great_news_if/"&gt; &lt;img alt="R1 is now on Azure AI serverless. Great news if you have Azure startup credits to burn" src="https://preview.redd.it/u9e4zggf20ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3934cf599e32057ef689487414da48ae9ac5687" title="R1 is now on Azure AI serverless. Great news if you have Azure startup credits to burn" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mesmerlord"&gt; /u/mesmerlord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u9e4zggf20ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id5179/r1_is_now_on_azure_ai_serverless_great_news_if/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1id5179/r1_is_now_on_azure_ai_serverless_great_news_if/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T21:23:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1icwys9</id>
    <title>Berkley AI research team claims to reproduce DeepSeek core technologies for $30</title>
    <updated>2025-01-29T15:54:59+00:00</updated>
    <author>
      <name>/u/Slasher1738</name>
      <uri>https://old.reddit.com/user/Slasher1738</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/ai-research-team-claims-to-reproduce-deepseek-core-technologies-for-usd30-relatively-small-r1-zero-model-has-remarkable-problem-solving-abilities"&gt;https://www.tomshardware.com/tech-industry/artificial-intelligence/ai-research-team-claims-to-reproduce-deepseek-core-technologies-for-usd30-relatively-small-r1-zero-model-has-remarkable-problem-solving-abilities&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;An AI research team from the University of California, Berkeley, led by Ph.D. candidate Jiayi Pan, claims to have reproduced DeepSeek R1-Zero‚Äôs core technologies for just $30, showing how advanced models could be implemented affordably. According to Jiayi Pan on &lt;a href="https://nitter.lucabased.xyz/jiayi_pirate/status/1882839370505621655"&gt;Nitter&lt;/a&gt;, their team reproduced DeepSeek R1-Zero in the Countdown game, and the small language model, with its 3 billion parameters, developed self-verification and search abilities through reinforcement learning.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;DeepSeek R1's cost advantage seems real. Not looking good for OpenAI. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slasher1738"&gt; /u/Slasher1738 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icwys9/berkley_ai_research_team_claims_to_reproduce/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icwys9/berkley_ai_research_team_claims_to_reproduce/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icwys9/berkley_ai_research_team_claims_to_reproduce/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T15:54:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1idcqm4</id>
    <title>not sure if memes are allowed here lul</title>
    <updated>2025-01-30T03:08:08+00:00</updated>
    <author>
      <name>/u/anzorq</name>
      <uri>https://old.reddit.com/user/anzorq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idcqm4/not_sure_if_memes_are_allowed_here_lul/"&gt; &lt;img alt="not sure if memes are allowed here lul" src="https://preview.redd.it/088rd998s1ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76eb2b2ec3633fa78609013c2913558a4d024d2f" title="not sure if memes are allowed here lul" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anzorq"&gt; /u/anzorq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/088rd998s1ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idcqm4/not_sure_if_memes_are_allowed_here_lul/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idcqm4/not_sure_if_memes_are_allowed_here_lul/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T03:08:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1id2poe</id>
    <title>"DeepSeek produced a model close to the performance of US models 7-10 months older, for a good deal less cost (but NOT anywhere near the ratios people have suggested)" says Anthropic's CEO</title>
    <updated>2025-01-29T19:46:32+00:00</updated>
    <author>
      <name>/u/siegevjorn</name>
      <uri>https://old.reddit.com/user/siegevjorn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id2poe/deepseek_produced_a_model_close_to_the/"&gt; &lt;img alt="&amp;quot;DeepSeek produced a model close to the performance of US models 7-10 months older, for a good deal less cost (but NOT anywhere near the ratios people have suggested)&amp;quot; says Anthropic's CEO" src="https://external-preview.redd.it/33CmrJWIyiH-IL_JOc7gY-avdl30Pd-oQB-Pun7s774.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6512765018eb834d1f7c5898ca5a6e6f6fd0af6e" title="&amp;quot;DeepSeek produced a model close to the performance of US models 7-10 months older, for a good deal less cost (but NOT anywhere near the ratios people have suggested)&amp;quot; says Anthropic's CEO" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic's CEO has a word about DeepSeek. &lt;/p&gt; &lt;p&gt;Here are some of his statements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&amp;quot;Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;3.5 Sonnet did not involve a larger or more expensive model&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&amp;quot;Sonnet's training was conducted 9-12 months ago, while Sonnet remains notably ahead of DeepSeek in many internal and external evals. &amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;DeepSeek's cost efficiency is x8 compared to Sonnet, which is much less than the &amp;quot;original GPT-4 to Claude 3.5 Sonnet inference price differential (10x).&amp;quot; Yet 3.5 Sonnet is a better model than GPT-4, while DeepSeek is not.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;TL;DR: Although DeepSeekV3 was a real deal, but such innovation has been achieved regularly by U.S. AI companies. DeepSeek had enough resources to make it happen. /s&lt;/p&gt; &lt;p&gt;I guess an important distinction, that the Anthorpic CEO refuses to recognize, is the fact that DeepSeekV3 it open weight. In his mind, it is U.S. vs China. It appears that he doesn't give a fuck about local LLMs. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/siegevjorn"&gt; /u/siegevjorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/01/29/anthropics-ceo-says-deepseek-shows-that-u-s-export-rules-are-working-as-intended/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1id2poe/deepseek_produced_a_model_close_to_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1id2poe/deepseek_produced_a_model_close_to_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T19:46:32+00:00</published>
  </entry>
</feed>
