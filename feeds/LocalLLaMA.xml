<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-14T04:35:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i0skla</id>
    <title>How can I reproduce this video locally?</title>
    <updated>2025-01-13T23:56:50+00:00</updated>
    <author>
      <name>/u/conlake</name>
      <uri>https://old.reddit.com/user/conlake</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0skla/how_can_i_reproduce_this_video_locally/"&gt; &lt;img alt="How can I reproduce this video locally?" src="https://b.thumbs.redditmedia.com/vq-1i_KXiUh3dQzHvuA-CQIYUg58m-jU_zRBUCmhkaY.jpg" title="How can I reproduce this video locally?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm overwhelmed trying to figure out how to reproduce the generation of videos like this one locally (or using cloud computing such as AWS/Azure/GCP or any other you recommend). The attached video was created using Klingai. My ultimate goal is to generate first-person videos with AI that explain topics, such as a doctor explaining what a headache is. For now, I'm focused on generating the video, while syncing the voice using another model (that's a separate issue). The maximum length of the videos would be 1 minute.&lt;/p&gt; &lt;p&gt;Does this specific use case suggest the need for a smaller model? I suspect it might, since I'm only aiming to generate human-based, first-person videos. The model wouldn't need to learn about animals, landscapes, or nature at all, which could reduce its complexity. Additionally, I wonder if it might be simpler to train a model just to generate the person and then add the background separately as an image. Am I heading in the right direction with this idea?&lt;/p&gt; &lt;p&gt;I also think I need to improve the lipsync, regardless of the approach.&lt;/p&gt; &lt;p&gt;Can anyone point me toward a potential solution to reproduce this video? For example, choosing a model like Llama X, fine-tuning it with my own data, and structuring the data as pairs of videos and prompts. Any advice or insights would be greatly appreciated!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/e64t7vqknuce1.png?width=893&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e805baad4d11281bad906cf2f1da3970358ead5"&gt;https://preview.redd.it/e64t7vqknuce1.png?width=893&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e805baad4d11281bad906cf2f1da3970358ead5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/conlake"&gt; /u/conlake &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0skla/how_can_i_reproduce_this_video_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0skla/how_can_i_reproduce_this_video_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0skla/how_can_i_reproduce_this_video_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T23:56:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0vz3g</id>
    <title>Remember Thunderkittens? Turns out Hazy Research has been cranking out some interesting stuff since that TK paper dropped</title>
    <updated>2025-01-14T02:40:31+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You may remember that &lt;a href="https://hazyresearch.stanford.edu/blog/2024-05-12-tk"&gt;GPUs Go Brrr&lt;/a&gt; paper from awhile back introducing Thunderkittens. TK was a new kernel optimized for Nvidia H100's and 4090s that was stupid fast - far faster than the then-Flash Attention 2.&lt;/p&gt; &lt;p&gt;Then FA3 came out, but it turns out &lt;a href="https://hazyresearch.stanford.edu/blog/2024-10-29-tk2"&gt;Thunderkittens was faster than FA3 too.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Then they &lt;a href="https://hazyresearch.stanford.edu/blog/2024-11-27-tk-fp8"&gt;implemented FP8 into their kernels.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;THEN, &lt;a href="https://hazyresearch.stanford.edu/blog/2024-11-28-tk-mlx"&gt;they ported TK to Apple Silicon.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/HazyResearch/ThunderKittens?tab=readme-ov-file"&gt;Their Github&lt;/a&gt; makes sensuous references to supporting AMD, and someone has implemented a pytorch conversion thingy for thunderkittens.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/HazyResearch/ThunderKittens/tree/main/demos"&gt;They have some interesting demos available&lt;/a&gt; running their TK attention.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/HazyResearch/lolcats"&gt;Remember lolcats? Their method of converting quadratic attention LLMs to linear attention models?&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/hazyresearch/lolcats-670ca4341699355b61238c37"&gt;They have Llama 3.1 8B, 70B, and 405B linear lolcat checkpoints.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/HazyResearch/based/tree/main"&gt;They have another linear attention thingy called Based.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm a bit of a dumbass, and I usually only run models in LM Studio. Can someone tell me how I can take advantage of these on either AMD or an Apple M2 machine?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0vz3g/remember_thunderkittens_turns_out_hazy_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0vz3g/remember_thunderkittens_turns_out_hazy_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0vz3g/remember_thunderkittens_turns_out_hazy_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T02:40:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0l4un</id>
    <title>Declarative Prompting with Open Ended Embedded Tool Use</title>
    <updated>2025-01-13T18:42:08+00:00</updated>
    <author>
      <name>/u/enspiralart</name>
      <uri>https://old.reddit.com/user/enspiralart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0l4un/declarative_prompting_with_open_ended_embedded/"&gt; &lt;img alt="Declarative Prompting with Open Ended Embedded Tool Use" src="https://external-preview.redd.it/1Hb1wwmUqYuq158Wjuyaykoy8O7YwxHbIXPSOFh7zqg.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1433f13c688951d8aab6600a12e32e55eb2d3c90" title="Declarative Prompting with Open Ended Embedded Tool Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/enspiralart"&gt; /u/enspiralart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=wI3e7ldSiFE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0l4un/declarative_prompting_with_open_ended_embedded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0l4un/declarative_prompting_with_open_ended_embedded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T18:42:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0hn3f</id>
    <title>How does a generative language model still work with spelling mistakes? However BERT like models are too sensitive?</title>
    <updated>2025-01-13T16:19:29+00:00</updated>
    <author>
      <name>/u/Lazy_Wedding_1383</name>
      <uri>https://old.reddit.com/user/Lazy_Wedding_1383</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same as above. Is the issue becasue of how things are tokenized. For example in car models, you have &lt;em&gt;2.0T&lt;/em&gt; (turbocharged 2.0-liter engine), &lt;em&gt;AWD&lt;/em&gt; (All-Wheel Drive), &lt;em&gt;TDI&lt;/em&gt; (Turbocharged Direct Injection).&lt;/p&gt; &lt;p&gt;I'm trying to identify or extract individual specs of a car based on its description. If I have a string like &amp;quot;Toyota Supercharger TDI2T&amp;quot; in a list of names, and if search for &amp;quot; give me all cars that have turbocharged 2.0-liter engine&amp;quot;, a llama model can find it but if i just take consine similarty of the above promt and this string, it will be low&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lazy_Wedding_1383"&gt; /u/Lazy_Wedding_1383 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hn3f/how_does_a_generative_language_model_still_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hn3f/how_does_a_generative_language_model_still_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hn3f/how_does_a_generative_language_model_still_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T16:19:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0jqjg</id>
    <title>looking for peasant's way to fine tuning models</title>
    <updated>2025-01-13T17:45:45+00:00</updated>
    <author>
      <name>/u/LordDaniel09</name>
      <uri>https://old.reddit.com/user/LordDaniel09</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I follow the 'smol-course' from Hugging Face github, but with the notebooks as is, it crashes because a lack of RAM. I run it locally on my Macbook M1 16gb ram, but I am unsure how much ram do I need. I tried both SFT and DPO notebooks, the first do till 100-150 steps and crash, and the second can't even start the first step.&lt;/p&gt; &lt;p&gt;- What is the math for how much RAM/VRAM you need for model fine tuning? I guess it depends on technique too, but my plan was to fine tune models like Llama 3b or even 7b, but if I fail with like 150-250M models from the guide.. &lt;/p&gt; &lt;p&gt;- There seems to be a lot of methods to fine tune, some more effecite than other, some more memory hungry than other. Is there a list of them and compression between them? &lt;/p&gt; &lt;p&gt;- Is the 'smol-course' consider good guide to follow?&lt;/p&gt; &lt;p&gt;- Should I even care about fine tuning if I got 16gb laptop and maybe 32gb desktop + 8gb vram gpu? I start playing with ollama and local models seems decent but my thought right now is that fine tuning it slightly would make those models much better for specific tasks (for example, minecraft bot or less &amp;quot;standard&amp;quot; anwsering for chatbot).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LordDaniel09"&gt; /u/LordDaniel09 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0jqjg/looking_for_peasants_way_to_fine_tuning_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0jqjg/looking_for_peasants_way_to_fine_tuning_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0jqjg/looking_for_peasants_way_to_fine_tuning_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T17:45:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1i06mew</id>
    <title>How is Kokoro TTS so good with so few parameters?</title>
    <updated>2025-01-13T05:05:22+00:00</updated>
    <author>
      <name>/u/JealousAmoeba</name>
      <uri>https://old.reddit.com/user/JealousAmoeba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As I understand it, Kokoro TTS is StyleTTS 2 with some modifications to the model architecture, trained mainly on outputs from OpenAI and ElevenLabs. But the results seem to be more impressive than StyleTTS and there are only 82M params.&lt;/p&gt; &lt;p&gt;Is it that training on a sufficiently good mix of synthetic data gives you superior results?&lt;/p&gt; &lt;p&gt;Or is there something hidden in the architecture changes that unlocked this new potential?&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;https://huggingface.co/hexgrad/Kokoro-82M&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JealousAmoeba"&gt; /u/JealousAmoeba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i06mew/how_is_kokoro_tts_so_good_with_so_few_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i06mew/how_is_kokoro_tts_so_good_with_so_few_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i06mew/how_is_kokoro_tts_so_good_with_so_few_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T05:05:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0d0qo</id>
    <title>Why do most vector databases use a NoSQL format rather than SQL?</title>
    <updated>2025-01-13T12:38:54+00:00</updated>
    <author>
      <name>/u/Available_Ad_5360</name>
      <uri>https://old.reddit.com/user/Available_Ad_5360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you've been exploring the world of vector databases, you might have noticed that most of them lean toward a NoSQL format instead of a traditional SQL approach. Why is that?&lt;/p&gt; &lt;p&gt;I'm just genuinely curious. Probably scalability?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available_Ad_5360"&gt; /u/Available_Ad_5360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0d0qo/why_do_most_vector_databases_use_a_nosql_format/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0d0qo/why_do_most_vector_databases_use_a_nosql_format/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0d0qo/why_do_most_vector_databases_use_a_nosql_format/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T12:38:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0toiz</id>
    <title>Looking for Gen AI app builder platform</title>
    <updated>2025-01-14T00:48:04+00:00</updated>
    <author>
      <name>/u/No-Brother-2237</name>
      <uri>https://old.reddit.com/user/No-Brother-2237</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am looking for some Gen AI platform through which companies can quickly build gen ai use cases using drag and drop, choose model if tgeir choice, get an internal chatgpt woth internet browsing capabilities, easily connect to databases, build knolwedge base etc. I am aware of solutions like quantiphi, tcs, cognizant platforms. &lt;/p&gt; &lt;p&gt;Looking for smaller player that has some really good capabilities and could be used for setting up Gen AI Lab for team members to experiment by rapidley building Gen AI apps with ease&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Brother-2237"&gt; /u/No-Brother-2237 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0toiz/looking_for_gen_ai_app_builder_platform/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0toiz/looking_for_gen_ai_app_builder_platform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0toiz/looking_for_gen_ai_app_builder_platform/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T00:48:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0toui</id>
    <title>Stagehand / BrowserUse / Logged In experiences for agents</title>
    <updated>2025-01-14T00:48:28+00:00</updated>
    <author>
      <name>/u/VigilOnTheVerge</name>
      <uri>https://old.reddit.com/user/VigilOnTheVerge</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious if there are any off the shelf tools online for doing browser use with login for agents? I.e. if I need an account for a site are there APIs that have been built to handle the login and computer use as a service/product?&lt;/p&gt; &lt;p&gt;Alternatively Iâ€™m benchmarking browser-use and stagehand for this purpose if I build it myself. Has anyone used either and can speak to limitations / pro cons of each?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VigilOnTheVerge"&gt; /u/VigilOnTheVerge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0toui/stagehand_browseruse_logged_in_experiences_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0toui/stagehand_browseruse_logged_in_experiences_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0toui/stagehand_browseruse_logged_in_experiences_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T00:48:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i01k4s</id>
    <title>Llama goes off the rails if you ask it for 5 odd numbers that donâ€™t have the letter E in them</title>
    <updated>2025-01-13T00:33:44+00:00</updated>
    <author>
      <name>/u/Applemoi</name>
      <uri>https://old.reddit.com/user/Applemoi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i01k4s/llama_goes_off_the_rails_if_you_ask_it_for_5_odd/"&gt; &lt;img alt="Llama goes off the rails if you ask it for 5 odd numbers that donâ€™t have the letter E in them " src="https://preview.redd.it/w5j543q9pnce1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61adf904110b30f4cba98ecbd9c36a7462cf005f" title="Llama goes off the rails if you ask it for 5 odd numbers that donâ€™t have the letter E in them " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Applemoi"&gt; /u/Applemoi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w5j543q9pnce1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i01k4s/llama_goes_off_the_rails_if_you_ask_it_for_5_odd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i01k4s/llama_goes_off_the_rails_if_you_ask_it_for_5_odd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T00:33:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0u8su</id>
    <title>Understanding LLMs from Scratch Using Middle School Math</title>
    <updated>2025-01-14T01:14:48+00:00</updated>
    <author>
      <name>/u/reddit_kwr</name>
      <uri>https://old.reddit.com/user/reddit_kwr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0u8su/understanding_llms_from_scratch_using_middle/"&gt; &lt;img alt="Understanding LLMs from Scratch Using Middle School Math" src="https://external-preview.redd.it/Rm8Vpve3R611qZhYKG5oG1MiMgKY95Fsco7MI76keJM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=709246061419aa2c09e1e20d5606675288d8debf" title="Understanding LLMs from Scratch Using Middle School Math" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reddit_kwr"&gt; /u/reddit_kwr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0u8su/understanding_llms_from_scratch_using_middle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0u8su/understanding_llms_from_scratch_using_middle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T01:14:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0hqic</id>
    <title>I Built an LLM Framework in just 100 Lines!!</title>
    <updated>2025-01-13T16:23:39+00:00</updated>
    <author>
      <name>/u/Willing-Site-8137</name>
      <uri>https://old.reddit.com/user/Willing-Site-8137</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen lots of complaints about how complex frameworks like LangChain are. Over the holidays, I wanted to explore just how minimal an LLM framework could be if we stripped away every unnecessary feature.&lt;/p&gt; &lt;p&gt;For example, why even include OpenAI wrappers in an LLM framework??&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;API Changes:&lt;/strong&gt; OpenAI API evolves (client after 0.27), and the official libraries often introduce bugs or dependency issues that are a pain to maintain.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DIY Is Simple:&lt;/strong&gt; It's straightforward to generate your own wrapperâ€”just feed the latest vendor documentation to an LLM!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extendibility:&lt;/strong&gt; By avoiding vendor-specific wrappers, developers can easily switch to the latest open-source or self-deployed models..&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Similarly, I strip out features that could be built on-demand rather than baked into the framework. The result? I created a 100-line LLM framework: &lt;a href="https://github.com/miniLLMFlow/PocketFlow/"&gt;https://github.com/miniLLMFlow/PocketFlow/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;These 100 lines capture what I see as the core abstraction of most LLM frameworks: a nested directed graph that breaks down tasks into multiple LLM steps, with branching and recursion to enable agent-like decision-making. From there, you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Layer On Complex Features:&lt;/strong&gt; Iâ€™ve included examples for building (&lt;a href="https://minillmflow.github.io/PocketFlow/multi_agent.html"&gt;multi-&lt;/a&gt;)&lt;a href="https://minillmflow.github.io/PocketFlow/agent.html"&gt;agents&lt;/a&gt;, &lt;a href="https://minillmflow.github.io/PocketFlow/rag.html"&gt;Retrieval-Augmented Generation (RAG)&lt;/a&gt;, &lt;a href="https://minillmflow.github.io/PocketFlow/decomp.html"&gt;task decomposition&lt;/a&gt;, and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Work Seamlessly With Coding Assistants:&lt;/strong&gt; Because itâ€™s so minimal, it integrates well with coding assistants like &lt;a href="https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-mini-llm-flow-assistant"&gt;ChatGPT&lt;/a&gt;, Claude, and Cursor.ai. You only need to share the relevant &lt;a href="https://github.com/miniLLMFlow/PocketFlow/tree/main/docs"&gt;documentation &lt;/a&gt;(e.g., in the Claude project), and the assistant can help you build new workflows on the fly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Iâ€™m adding more examples and would love feedback. If thereâ€™s a feature youâ€™d like to see or a specific use case you think is missing, please let me know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Willing-Site-8137"&gt; /u/Willing-Site-8137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hqic/i_built_an_llm_framework_in_just_100_lines/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hqic/i_built_an_llm_framework_in_just_100_lines/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hqic/i_built_an_llm_framework_in_just_100_lines/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T16:23:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0ou0v</id>
    <title>UGI-Leaderboard Remake! New Political, Coding, and Intelligence benchmarks</title>
    <updated>2025-01-13T21:13:59+00:00</updated>
    <author>
      <name>/u/DontPlanToEnd</name>
      <uri>https://old.reddit.com/user/DontPlanToEnd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard"&gt;UGI-Leaderboard Link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After a long wait, Iâ€™m finally ready to release the new version of the UGI Leaderboard. In this update I focused on automating my testing process, which allowed me to increase the number of test questions, branch out into different testing subjects, and have more precise rankings. You can find and read about each of the benchmarks in the leaderboard on the leaderboardâ€™s About section.&lt;/p&gt; &lt;p&gt;I recommend everyone try filtering models to have at least ~15 NatInt and then take a look at what models have the highest and lowest of each of the political axes. Some very interesting findings.&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Notes:&lt;/p&gt; &lt;p&gt;I decided to reset the backlog of model submissions since the focus of the leaderboard has slightly changed.&lt;/p&gt; &lt;p&gt;I am no longer using decensoring system prompts which tell the model to be uncensored. There isnâ€™t a clearcut right answer to this. Initially I felt having them would be better since it could better show a modelâ€™s true potential, and I didnâ€™t think I should penalize models for not acting in a way they didnâ€™t know they were supposed to act. But on the other hand, people donâ€™t want to be required to use a certain system prompt in order to get good results. There was also the problem that if people did end up using a decensoring system prompt, it would most likely not be the one I used for testing, making it likely that people would get varying results.&lt;/p&gt; &lt;p&gt;I changed from testing local models on Q4_K_M.gguf to Q_6_K.gguf. I didnâ€™t go up to Q8 because the performance gains are fairly small and it wouldnâ€™t be worth the noticeable increase in model size.&lt;/p&gt; &lt;p&gt;I did end up removing both the writing style and rating prediction rankings. With writing style, its way of ranking models was pretty dependent on me manually giving ratings to stories so that the regression model could understand what lexical statistics people tend to prefer. I no longer have time to do that (and it was a very flimsy way of ranking models), so I tried replacing the ranking, but the amount of compute necessary to test a sufficient number of model writing outputs on Q6 70B+ models wasnâ€™t feasible. For rating prediction, NatInt seemed to be highly correlated so it didnâ€™t seem necessary.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DontPlanToEnd"&gt; /u/DontPlanToEnd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0ou0v/ugileaderboard_remake_new_political_coding_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0ou0v/ugileaderboard_remake_new_political_coding_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0ou0v/ugileaderboard_remake_new_political_coding_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T21:13:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0kmtj</id>
    <title>LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs - Outperforms GPT-4o-mini and Gemini-1.5-Flash on the visual reasoning benchmark!</title>
    <updated>2025-01-13T18:22:01+00:00</updated>
    <author>
      <name>/u/Singularian2501</name>
      <uri>https://old.reddit.com/user/Singularian2501</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Singularian2501"&gt; /u/Singularian2501 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mbzuai-oryx.github.io/LlamaV-o1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0kmtj/llamavo1_rethinking_stepbystep_visual_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0kmtj/llamavo1_rethinking_stepbystep_visual_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T18:22:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0jxyc</id>
    <title>Kyutai drops Helium 2B Preview - Multilingual Base LLM - CC-BY license ðŸ”¥</title>
    <updated>2025-01-13T17:54:16+00:00</updated>
    <author>
      <name>/u/vaibhavs10</name>
      <uri>https://old.reddit.com/user/vaibhavs10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0jxyc/kyutai_drops_helium_2b_preview_multilingual_base/"&gt; &lt;img alt="Kyutai drops Helium 2B Preview - Multilingual Base LLM - CC-BY license ðŸ”¥" src="https://external-preview.redd.it/SPGutjTEemqh1flKodYZ0ERgy1IzyVXU-3WiHs0p9mo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6ddc12d75e88ec181719b610d2b4a6831c54cc7" title="Kyutai drops Helium 2B Preview - Multilingual Base LLM - CC-BY license ðŸ”¥" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaibhavs10"&gt; /u/vaibhavs10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/kyutai/helium-1-preview-2b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0jxyc/kyutai_drops_helium_2b_preview_multilingual_base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0jxyc/kyutai_drops_helium_2b_preview_multilingual_base/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T17:54:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0rdx1</id>
    <title>RTX Titan Ada 48GB Prototype</title>
    <updated>2025-01-13T23:03:07+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems like more exciting than 5090 if it is real and sold for $3k. Essentially it is a L40 with all its 144 SM enabled. It will not have its FP16 with FP32 accumulate halved compare to non-TITAN, so it will have double the performance in mixed precision training. It is also likely to have the transformer engine in L40 which 4090 doesn't have (most likely 5090 also doesn't have).&lt;/p&gt; &lt;p&gt;While memory bandwidth is significantly slower, I think it is fast enough for 48GB. TDP is estimated by comparing TITAN V to V100. If it is 300W to 350W, a simple 3xTitan Ada setup can be easily setup.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Card&lt;/th&gt; &lt;th align="left"&gt;RTX Titan Ada&lt;/th&gt; &lt;th align="left"&gt;5090&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;FP16 TFLOPS&lt;/td&gt; &lt;td align="left"&gt;367.17&lt;/td&gt; &lt;td align="left"&gt;419.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Memory&lt;/td&gt; &lt;td align="left"&gt;48GB&lt;/td&gt; &lt;td align="left"&gt;32GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Memory Bandwidth&lt;/td&gt; &lt;td align="left"&gt;864GB/s&lt;/td&gt; &lt;td align="left"&gt;1792GB/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TDP&lt;/td&gt; &lt;td align="left"&gt;300W&lt;/td&gt; &lt;td align="left"&gt;575W&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GFLOPS/W&lt;/td&gt; &lt;td align="left"&gt;1223.88&lt;/td&gt; &lt;td align="left"&gt;728.71&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://videocardz.com/newz/alleged-nvidia-rtx-titan-ada-surfaces-with-18432-cuda-cores-and-48gb-gddr6-memory-alongside-gtx-2080-ti-prototype"&gt;https://videocardz.com/newz/alleged-nvidia-rtx-titan-ada-surfaces-with-18432-cuda-cores-and-48gb-gddr6-memory-alongside-gtx-2080-ti-prototype&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0rdx1/rtx_titan_ada_48gb_prototype/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0rdx1/rtx_titan_ada_48gb_prototype/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0rdx1/rtx_titan_ada_48gb_prototype/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T23:03:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0f5tt</id>
    <title>Codestral 25.01: Code at the speed of tab</title>
    <updated>2025-01-13T14:28:39+00:00</updated>
    <author>
      <name>/u/SignalCompetitive582</name>
      <uri>https://old.reddit.com/user/SignalCompetitive582</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0f5tt/codestral_2501_code_at_the_speed_of_tab/"&gt; &lt;img alt="Codestral 25.01: Code at the speed of tab" src="https://external-preview.redd.it/6iQ4AGOjK4o2rzQFX3A8casUnMTrEOdxKIDWSbRrbf4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6e958373f510f16ed3c8e63d7174ab21755b11c" title="Codestral 25.01: Code at the speed of tab" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SignalCompetitive582"&gt; /u/SignalCompetitive582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/codestral-2501/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0f5tt/codestral_2501_code_at_the_speed_of_tab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0f5tt/codestral_2501_code_at_the_speed_of_tab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T14:28:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0bsha</id>
    <title>Is this where all LLMs are going?</title>
    <updated>2025-01-13T11:19:47+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0bsha/is_this_where_all_llms_are_going/"&gt; &lt;img alt="Is this where all LLMs are going? " src="https://preview.redd.it/l1h02xo8wqce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=03d40d0e8695392ff6f2dbe6e68c5d8afd724e12" title="Is this where all LLMs are going? " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l1h02xo8wqce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0bsha/is_this_where_all_llms_are_going/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0bsha/is_this_where_all_llms_are_going/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T11:19:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0hecs</id>
    <title>Researchers open source Sky-T1, a 'reasoning' AI model that can be trained for less than $450</title>
    <updated>2025-01-13T16:09:19+00:00</updated>
    <author>
      <name>/u/mr_house7</name>
      <uri>https://old.reddit.com/user/mr_house7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hecs/researchers_open_source_skyt1_a_reasoning_ai/"&gt; &lt;img alt="Researchers open source Sky-T1, a 'reasoning' AI model that can be trained for less than $450" src="https://external-preview.redd.it/CoBta77nxTwOGfkB2XmPAkIRcSe1Pm3XBChHXR_0ZNI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7add0ea55f9158b47f0d4f1c57b35784adb6a682" title="Researchers open source Sky-T1, a 'reasoning' AI model that can be trained for less than $450" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_house7"&gt; /u/mr_house7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/01/11/researchers-open-source-sky-t1-a-reasoning-ai-model-that-can-be-trained-for-less-than-450/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hecs/researchers_open_source_skyt1_a_reasoning_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hecs/researchers_open_source_skyt1_a_reasoning_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T16:09:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0vrm5</id>
    <title>Here is our new reranker model, which we trained on over 95 languages and it achieves better performance than comparable rerankers on our eval benchmarks. Weights, data, and training code are all open source.</title>
    <updated>2025-01-14T02:30:00+00:00</updated>
    <author>
      <name>/u/Peter_Lightblue</name>
      <uri>https://old.reddit.com/user/Peter_Lightblue</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0vrm5/here_is_our_new_reranker_model_which_we_trained/"&gt; &lt;img alt="Here is our new reranker model, which we trained on over 95 languages and it achieves better performance than comparable rerankers on our eval benchmarks. Weights, data, and training code are all open source." src="https://external-preview.redd.it/IqG-6k7nq3XlqnkLxN4BETD1asSgHQ6DDeSuHbGLcoE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ead37f624cbf68ece5069b4337ea6165f2619b57" title="Here is our new reranker model, which we trained on over 95 languages and it achieves better performance than comparable rerankers on our eval benchmarks. Weights, data, and training code are all open source." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Peter_Lightblue"&gt; /u/Peter_Lightblue &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lightblue/lb-reranker-0.5B-v1.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0vrm5/here_is_our_new_reranker_model_which_we_trained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0vrm5/here_is_our_new_reranker_model_which_we_trained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T02:30:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0b289</id>
    <title>Hugging Face released a free course on agents.</title>
    <updated>2025-01-13T10:26:28+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just added a chapter to smol course on agents. Naturally, using smolagents! The course cover these topics:&lt;/p&gt; &lt;p&gt;- Code agents that solve problem with code&lt;br /&gt; - Retrieval agents that supply grounded context&lt;br /&gt; - Custom functional agents that do whatever you need!&lt;/p&gt; &lt;p&gt;If you're building agent applications, this course should help.&lt;/p&gt; &lt;p&gt;Course in smol course &lt;a href="https://github.com/huggingface/smol-course/tree/main/8_agents"&gt;https://github.com/huggingface/smol-course/tree/main/8_agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0b289/hugging_face_released_a_free_course_on_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0b289/hugging_face_released_a_free_course_on_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0b289/hugging_face_released_a_free_course_on_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T10:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0sj1f</id>
    <title>Testing vLLM with Open-WebUI - Llama 3 70B Tulu - 4x AMD Instinct Mi60 Rig - 26 tok/s!</title>
    <updated>2025-01-13T23:54:49+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0sj1f/testing_vllm_with_openwebui_llama_3_70b_tulu_4x/"&gt; &lt;img alt="Testing vLLM with Open-WebUI - Llama 3 70B Tulu - 4x AMD Instinct Mi60 Rig - 26 tok/s!" src="https://external-preview.redd.it/MnRsc25hdDJudWNlMbxhQbonukNuLCehUVr37R_wGdLDix2HfauICRmOLuhO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5d7d4c265d5b0d9acef345297c97de7eb356b23f" title="Testing vLLM with Open-WebUI - Llama 3 70B Tulu - 4x AMD Instinct Mi60 Rig - 26 tok/s!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/oq7fwat2nuce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0sj1f/testing_vllm_with_openwebui_llama_3_70b_tulu_4x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0sj1f/testing_vllm_with_openwebui_llama_3_70b_tulu_4x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T23:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0q8nw</id>
    <title>Titans: Learning to Memorize at Test Time</title>
    <updated>2025-01-13T22:13:41+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2501.00663v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0q8nw/titans_learning_to_memorize_at_test_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0q8nw/titans_learning_to_memorize_at_test_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T22:13:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0mb67</id>
    <title>16GB Raspberry Pi 5 on sale now at $120</title>
    <updated>2025-01-13T19:30:07+00:00</updated>
    <author>
      <name>/u/barefoot_twig</name>
      <uri>https://old.reddit.com/user/barefoot_twig</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/barefoot_twig"&gt; /u/barefoot_twig &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.raspberrypi.com/news/16gb-raspberry-pi-5-on-sale-now-at-120/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0mb67/16gb_raspberry_pi_5_on_sale_now_at_120/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0mb67/16gb_raspberry_pi_5_on_sale_now_at_120/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T19:30:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0eio5</id>
    <title>NVidia's official statement on the Biden Administration's Ai Diffusion Rule</title>
    <updated>2025-01-13T13:57:44+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0eio5/nvidias_official_statement_on_the_biden/"&gt; &lt;img alt="NVidia's official statement on the Biden Administration's Ai Diffusion Rule" src="https://external-preview.redd.it/mWL0PsxkiUDwuew99qIBVAVxp2i94of5Kngrra_8DKY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f30abf9568aba3ecc9b3830767ca6d7b17d79785" title="NVidia's official statement on the Biden Administration's Ai Diffusion Rule" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blogs.nvidia.com/blog/ai-policy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0eio5/nvidias_official_statement_on_the_biden/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0eio5/nvidias_official_statement_on_the_biden/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T13:57:44+00:00</published>
  </entry>
</feed>
