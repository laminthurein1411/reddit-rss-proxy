<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-15T23:34:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i29xar</id>
    <title>Are 1B (llama 3.2) models usually this capable?</title>
    <updated>2025-01-15T22:38:51+00:00</updated>
    <author>
      <name>/u/Tobias783</name>
      <uri>https://old.reddit.com/user/Tobias783</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i29xar/are_1b_llama_32_models_usually_this_capable/"&gt; &lt;img alt="Are 1B (llama 3.2) models usually this capable?" src="https://a.thumbs.redditmedia.com/4npAa0pns_zkNlwqaV0hx9eK5KKsZhCreYn6ZFwDC20.jpg" title="Are 1B (llama 3.2) models usually this capable?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tobias783"&gt; /u/Tobias783 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i29xar"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i29xar/are_1b_llama_32_models_usually_this_capable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i29xar/are_1b_llama_32_models_usually_this_capable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T22:38:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1p0yb</id>
    <title>Running Deepseek V3 with a box of scraps (but not in a cave)</title>
    <updated>2025-01-15T03:58:00+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got Deepseek running on a bunch of old 10GB Nvidia P102-100's on PCIE 1.0 x1 risers. (GPU's built for mining)&lt;br /&gt; Spread across 3 machines, connected via 1gb lan and through a firewall! &lt;/p&gt; &lt;p&gt;Bought these GPU's for $30 each, (not for this purpose lol)&lt;/p&gt; &lt;p&gt;Funnily enough the hardest part is that Llama.cpp wanted enough cpu ram to load the model before moving it to VRAM. Had to run it at Q2 because of this.&lt;br /&gt; Will try again at Q4 when I get some more.&lt;/p&gt; &lt;p&gt;Speed, a whopping &lt;strong&gt;3.6 T/s.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Considering this setup has literally everything going against it, not half bad really.&lt;/p&gt; &lt;p&gt;If you are curious, without the GPUs, the CPU server alone starts around 2.4T/s but even after 1k tokens it was down to 1.8T/s&lt;/p&gt; &lt;p&gt;Was only seeing like 30MB/s on the network, but might try upgrading everything to 10G lan just to see if it matters.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1p0yb/running_deepseek_v3_with_a_box_of_scraps_but_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1p0yb/running_deepseek_v3_with_a_box_of_scraps_but_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1p0yb/running_deepseek_v3_with_a_box_of_scraps_but_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T03:58:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1zbul</id>
    <title>First Intel B580 inference speed test</title>
    <updated>2025-01-15T15:03:33+00:00</updated>
    <author>
      <name>/u/ComprehensiveQuail77</name>
      <uri>https://old.reddit.com/user/ComprehensiveQuail77</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1zbul/first_intel_b580_inference_speed_test/"&gt; &lt;img alt="First Intel B580 inference speed test " src="https://b.thumbs.redditmedia.com/nf6RzwI74zZMHig11fK5CgyR1IIx4RjHmOSfgV0lXSA.jpg" title="First Intel B580 inference speed test " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Upon my request someone agreed to test his B580 and the result is this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vhjixnb7a6de1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2413cfd6985fecdcd88e4f17fd0d0844f2d1d70e"&gt;https://preview.redd.it/vhjixnb7a6de1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2413cfd6985fecdcd88e4f17fd0d0844f2d1d70e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComprehensiveQuail77"&gt; /u/ComprehensiveQuail77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1zbul/first_intel_b580_inference_speed_test/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1zbul/first_intel_b580_inference_speed_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1zbul/first_intel_b580_inference_speed_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T15:03:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1owp1</id>
    <title>OpenRouter Users: What feature are you missing?</title>
    <updated>2025-01-15T03:51:27+00:00</updated>
    <author>
      <name>/u/punkpeye</name>
      <uri>https://old.reddit.com/user/punkpeye</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I accidentally built an &lt;a href="https://glama.ai/gateway"&gt;OpenRouter alternative&lt;/a&gt;. I say accidentally because that wasn‚Äôt the goal of my project, but as people and companies adopted it, they requested similar features. Over time, I ended up with something that feels like an alternative.&lt;/p&gt; &lt;p&gt;The main benefit of both services is elevated rate limits without subscription, and the ability to easily switch models using OpenAI-compatible API. That's not different.&lt;/p&gt; &lt;p&gt;The unique benefits to my gateway include integration with the Chat and &lt;a href="https://glama.ai/mcp/servers"&gt;MCP ecosystem&lt;/a&gt;, more advanced analytics/logging, and reportedly lower latency and greater stability than OpenRouter. Pricing is similar, and we process several billion tokens daily. Having addressed feedback from current users, I‚Äôm now looking to the broader community for ideas on where to take the project next.&lt;/p&gt; &lt;p&gt;What are your painpoints with OpenRouter?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/punkpeye"&gt; /u/punkpeye &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1owp1/openrouter_users_what_feature_are_you_missing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1owp1/openrouter_users_what_feature_are_you_missing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1owp1/openrouter_users_what_feature_are_you_missing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T03:51:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1i21y9r</id>
    <title>Has anyone cracked "proactive" LLMs that can actually monitor stuff in real-time?</title>
    <updated>2025-01-15T16:58:05+00:00</updated>
    <author>
      <name>/u/No-Conference-8133</name>
      <uri>https://old.reddit.com/user/No-Conference-8133</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been thinking about this limitation with LLMs - they're all just sitting there waiting for us to say something before they do anything.&lt;/p&gt; &lt;p&gt;You know how it always goes:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Human: blah AI: blah Human: blah AI: blah &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Anyone seen projects or research about LLMs that can actually monitor stuff in real-time and pipe up when they notice something? Not just reacting to prompts, but actually having some kind of ongoing awareness?&lt;/p&gt; &lt;p&gt;Been searching but most &amp;quot;autonomous&amp;quot; agents I've found still use that basic input/output loop, just automated.&lt;/p&gt; &lt;p&gt;Example: &lt;code&gt; AI: [watches data] AI: &amp;quot;I see that...&amp;quot; AI: &amp;quot;Okay, now it's more clear&amp;quot; Human: &amp;quot;how's it looking?&amp;quot; AI: &amp;quot;It's looking decent...&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Edit: Not talking about basic monitoring with predetermined triggers - mean actual AI that can decide on its own when to speak up based on what it's seeing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Conference-8133"&gt; /u/No-Conference-8133 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i21y9r/has_anyone_cracked_proactive_llms_that_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i21y9r/has_anyone_cracked_proactive_llms_that_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i21y9r/has_anyone_cracked_proactive_llms_that_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T16:58:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1i25jas</id>
    <title>Beating cuBLAS in Single-Precision General Matrix Multiplication</title>
    <updated>2025-01-15T19:27:58+00:00</updated>
    <author>
      <name>/u/salykova</name>
      <uri>https://old.reddit.com/user/salykova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i25jas/beating_cublas_in_singleprecision_general_matrix/"&gt; &lt;img alt="Beating cuBLAS in Single-Precision General Matrix Multiplication" src="https://b.thumbs.redditmedia.com/IOKPnc_BPIVWwt4NejPWJ-vflqaNw99lgiCBjBXb6lk.jpg" title="Beating cuBLAS in Single-Precision General Matrix Multiplication" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A while ago, I shared my article here about optimizing matrix multiplication on CPUs, achieving performance that outpaced NumPy - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1dt3rqc/beating_numpys_matrix_multiplication_in_150_lines/"&gt;Beating NumPy's matrix multiplication in 150 lines of C code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I received positive feedback from your community, and today I'm excited to share my second blog post. This one focuses on a matrix multiplication implementation that outperforms cuBLAS with its (modified?) CUTLASS kernel across a wide range of matrix sizes. The blog delves into benchmarking code on CUDA devices and explains the algorithm's design along with optimization techniques. These include inlined PTX, asynchronous memory copies, double-buffering, avoiding shared memory bank conflicts, and efficient coalesced storage using shared memory. The code is super easy to tweak, so you can customize it for your projects with kernel fusion or just drop it into your libraries as-is. If you have any questions, feel free to comment or send me a direct message - I'd love to hear your feedback and answer any questions you may have! Below, I've included performance comparisons (with both locked and unlocked clocks) against cuBLAS and Simon Boehm‚Äôs highly cited work, which is now integrated into llamafile aka tinyBLAS.&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://salykova.github.io/sgemm-gpu"&gt;https://salykova.github.io/sgemm-gpu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/salykova/sgemm.cu"&gt;https://github.com/salykova/sgemm.cu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y3pgixh4l7de1.png?width=1256&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cce30a78791965503a0fa340646a537de1d57195"&gt;https://preview.redd.it/y3pgixh4l7de1.png?width=1256&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cce30a78791965503a0fa340646a537de1d57195&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b5vqz7c5l7de1.png?width=1253&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2cf2103c8c7426dedfb70deb05d9c54c320e6808"&gt;https://preview.redd.it/b5vqz7c5l7de1.png?width=1253&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2cf2103c8c7426dedfb70deb05d9c54c320e6808&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/salykova"&gt; /u/salykova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i25jas/beating_cublas_in_singleprecision_general_matrix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i25jas/beating_cublas_in_singleprecision_general_matrix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i25jas/beating_cublas_in_singleprecision_general_matrix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T19:27:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i20y53</id>
    <title>Jina releases ReaderLM V2, 1.5B model for HTML-to-Markdown/JSON conversion</title>
    <updated>2025-01-15T16:14:41+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i20y53/jina_releases_readerlm_v2_15b_model_for/"&gt; &lt;img alt="Jina releases ReaderLM V2, 1.5B model for HTML-to-Markdown/JSON conversion" src="https://external-preview.redd.it/OawXnZHfMYYebdHrDHFeKzgBRPwqxQJE51C0bsjvWqk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59c70093e9abc7da1b0d7c8f17972ecb8b87d217" title="Jina releases ReaderLM V2, 1.5B model for HTML-to-Markdown/JSON conversion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/jinaai/ReaderLM-v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i20y53/jina_releases_readerlm_v2_15b_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i20y53/jina_releases_readerlm_v2_15b_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T16:14:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i20dka</id>
    <title>Speculative decoding isn't a silver bullet - but it can get you 3x speed-ups</title>
    <updated>2025-01-15T15:49:40+00:00</updated>
    <author>
      <name>/u/mark-lord</name>
      <uri>https://old.reddit.com/user/mark-lord</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i20dka/speculative_decoding_isnt_a_silver_bullet_but_it/"&gt; &lt;img alt="Speculative decoding isn't a silver bullet - but it can get you 3x speed-ups" src="https://external-preview.redd.it/DaucjXMGsNHM-CtmdilC9-Be6MC8V2z4ykjVCgOkTFc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62ca4cb88917f17e7200a6f1c665b5d959713745" title="Speculative decoding isn't a silver bullet - but it can get you 3x speed-ups" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! Quick benchmark today - did this using Exaone-32b-4bit*, running with latest MLX_LM backend using &lt;a href="https://gist.github.com/mark-lord/93a9f53f4f1e230e7bd5828357649f89"&gt;this script&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;No speculative decoding:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1i20dka/video/bqtvz9rah6de1/player"&gt;Prompt: 44.608 tps | Generation: 6.274 tps | Avg power: ~9w | Total energy used: ~400J | Time taken: 48.226s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Speculative decoding:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1i20dka/video/ji82cmcfh6de1/player"&gt;Prompt: 37.170 tps | Generation: 24.140 tps | Avg power: ~13w | Total energy used: ~300J | Time taken: 22.880s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;*Benchmark done using my M1 Max 64gb in low power mode, using Exaone-2.4b-4bit as the draft model with 31 draft tokens&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Prompt processing speed was a little bit slower - dropping by about 20%. Power draw was also higher, even in low power mode. &lt;/p&gt; &lt;p&gt;But the time taken from start-&amp;gt;finish was reduced by 53% overall&lt;br /&gt; (The reduction in time taken means the total energy used was also reduced from 400-&amp;gt;300J.)&lt;/p&gt; &lt;p&gt;Pretty damn good I think üòÑ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mark-lord"&gt; /u/mark-lord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i20dka/speculative_decoding_isnt_a_silver_bullet_but_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i20dka/speculative_decoding_isnt_a_silver_bullet_but_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i20dka/speculative_decoding_isnt_a_silver_bullet_but_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T15:49:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1ty0e</id>
    <title>405B MiniMax MoE technical deepdive</title>
    <updated>2025-01-15T09:41:33+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tl;dr very (very) nice paper/model, lot of details and experiment details, hybrid with 7/8 Lightning attn, different MoE strategy than deepseek, deepnorm, WSD schedule, ~2000 H800 for training, ~12T token.&lt;br /&gt; blog: &lt;a href="https://huggingface.co/blog/eliebak/minimax01-deepdive"&gt;https://huggingface.co/blog/eliebak/minimax01-deepdive&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ty0e/405b_minimax_moe_technical_deepdive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ty0e/405b_minimax_moe_technical_deepdive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ty0e/405b_minimax_moe_technical_deepdive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T09:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1ffid</id>
    <title>I accidentally built an open alternative to Google AI Studio</title>
    <updated>2025-01-14T20:18:39+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday, I had a mini heart attack when I discovered Google AI Studio, a product that looked (at first glance) just like the tool I've been building for 5 months. However, I dove in and was super relieved once I got into the details. There were a bunch of differences, which I've detailed below.&lt;/p&gt; &lt;p&gt;I thought I‚Äôd share what I have, in case anyone has been using G AI Sudio, and might want to check out my &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;rapid prototyping tool on Github, called Kiln&lt;/a&gt;. There are some similarities, but there are also some big differences when it comes to privacy, collaboration, model support, fine-tuning, and ML techniques. I built Kiln because I've been building AI products for ~10 years (most recently at Apple, and my own startup &amp;amp; MSFT before that), and I wanted to build an easy to use, privacy focused, open source AI tooling.&lt;/p&gt; &lt;p&gt;Differences:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model Support: Kiln allows any LLM (including Gemini/Gemma) through a ton of hosts: Ollama, OpenRouter, OpenAI, etc. Google supports only Gemini &amp;amp; Gemma via Google Cloud.&lt;/li&gt; &lt;li&gt;Fine Tuning: Google lets you fine tune only Gemini, with at most 500 samples. Kiln has no limits on data size, 9 models you can tune in a few clicks (no code), and support for tuning any open model via Unsloth.&lt;/li&gt; &lt;li&gt;Data Privacy: Kiln can't access your data (it runs locally, data stays local); Google stores everything. Kiln can run/train local models (Ollama/Unsloth/LiteLLM); Google always uses their cloud.&lt;/li&gt; &lt;li&gt;Collaboration: Google is single user, while Kiln allows unlimited users/collaboration.&lt;/li&gt; &lt;li&gt;ML Techniques: Google has standard prompting. Kiln has standard prompts, chain-of-thought/reasoning, and auto-prompts (using your dataset for multi-shot).&lt;/li&gt; &lt;li&gt;Dataset management: Google has a table with max 500 rows. Kiln has powerful dataset management for teams with Git sync, tags, unlimited rows, human ratings, and more.&lt;/li&gt; &lt;li&gt;Python Library: Google is UI only. Kiln has a python library for extending it for when you need more than the UI can offer.&lt;/li&gt; &lt;li&gt;Open Source: Google‚Äôs is completely proprietary and private source. Kiln‚Äôs library is MIT open source; the UI isn‚Äôt MIT, but it is 100% source-available, on Github, and free.&lt;/li&gt; &lt;li&gt;Similarities: Both handle structured data well, both have a prompt library, both have similar ‚ÄúRun‚Äù UX, both had user friendly UIs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If anyone wants to check Kiln out, &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;here's the GitHub repository&lt;/a&gt; and &lt;a href="https://docs.getkiln.ai"&gt;docs are here&lt;/a&gt;. Getting started is super easy - it's a one-click install to get setup and running.&lt;/p&gt; &lt;p&gt;I‚Äôm very interested in any feedback or feature requests (model requests, integrations with other tools, etc.) I'm currently working on comprehensive evals, so feedback on what you'd like to see in that area would be super helpful. My hope is to make something as easy to use as G AI Studio, as powerful as Vertex AI, all while open and private.&lt;/p&gt; &lt;p&gt;Thanks in advance! I‚Äôm happy to answer any questions.&lt;/p&gt; &lt;p&gt;Side note: I‚Äôm usually pretty good at competitive research before starting a project. I had looked up Google's &amp;quot;AI Studio&amp;quot; before I started. However, I found and looked at &amp;quot;Vertex AI Studio&amp;quot;, which is a completely different type of product. How one company can have 2 products with almost identical names is beyond me...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ffid/i_accidentally_built_an_open_alternative_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ffid/i_accidentally_built_an_open_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1ffid/i_accidentally_built_an_open_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T20:18:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1yuke</id>
    <title>Sakana.ai proposes Transformer-squared - Adaptive AI that adjusts its own weights dynamically and eveolves as it learns</title>
    <updated>2025-01-15T14:41:59+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1yuke/sakanaai_proposes_transformersquared_adaptive_ai/"&gt; &lt;img alt="Sakana.ai proposes Transformer-squared - Adaptive AI that adjusts its own weights dynamically and eveolves as it learns" src="https://external-preview.redd.it/ll0sI2kj9OWJW1iOriHpZm1jSfC278YnLF-jisELKs4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5f30bf0b3bae15b4dee53ba7bd37f2486072c04" title="Sakana.ai proposes Transformer-squared - Adaptive AI that adjusts its own weights dynamically and eveolves as it learns" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Arxiv paper - &lt;a href="https://arxiv.org/abs/2501.06252"&gt;https://arxiv.org/abs/2501.06252&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sakana.ai/transformer-squared/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1yuke/sakanaai_proposes_transformersquared_adaptive_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1yuke/sakanaai_proposes_transformersquared_adaptive_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T14:41:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i20211</id>
    <title>Train 400x faster Static Embedding Models; 2 open models released</title>
    <updated>2025-01-15T15:35:40+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i20211/train_400x_faster_static_embedding_models_2_open/"&gt; &lt;img alt="Train 400x faster Static Embedding Models; 2 open models released" src="https://external-preview.redd.it/7KIBT99WqmowwDguikX7zoXpmvjI60Ua61vkPn6VgEU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c7a6ae7297ed561b65221ae6db7678244b6b8a1" title="Train 400x faster Static Embedding Models; 2 open models released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/static-embeddings"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i20211/train_400x_faster_static_embedding_models_2_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i20211/train_400x_faster_static_embedding_models_2_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T15:35:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1rgn9</id>
    <title>New model....</title>
    <updated>2025-01-15T06:29:44+00:00</updated>
    <author>
      <name>/u/Many_SuchCases</name>
      <uri>https://old.reddit.com/user/Many_SuchCases</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1rgn9/new_model/"&gt; &lt;img alt="New model...." src="https://preview.redd.it/curwy8vkq3de1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a262daf4d3df2ddd46d444593d7171ed6352a6c5" title="New model...." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Many_SuchCases"&gt; /u/Many_SuchCases &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/curwy8vkq3de1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1rgn9/new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1rgn9/new_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T06:29:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i21x7z</id>
    <title>Deepseek is officially available on Android and iOS!</title>
    <updated>2025-01-15T16:56:48+00:00</updated>
    <author>
      <name>/u/Available-Stress8598</name>
      <uri>https://old.reddit.com/user/Available-Stress8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i21x7z/deepseek_is_officially_available_on_android_and/"&gt; &lt;img alt="Deepseek is officially available on Android and iOS!" src="https://preview.redd.it/47xatq2hu6de1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c38b07010ec1163e4765356a3b6c3e3a5dea964" title="Deepseek is officially available on Android and iOS!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available-Stress8598"&gt; /u/Available-Stress8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/47xatq2hu6de1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i21x7z/deepseek_is_officially_available_on_android_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i21x7z/deepseek_is_officially_available_on_android_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T16:56:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1i23dhv</id>
    <title>Dell T5820 w/ 2x Dell RTX 3090 for less than $2k - eBay sourced</title>
    <updated>2025-01-15T17:57:35+00:00</updated>
    <author>
      <name>/u/_Boffin_</name>
      <uri>https://old.reddit.com/user/_Boffin_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i23dhv/dell_t5820_w_2x_dell_rtx_3090_for_less_than_2k/"&gt; &lt;img alt="Dell T5820 w/ 2x Dell RTX 3090 for less than $2k - eBay sourced" src="https://preview.redd.it/qi354b2457de1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=189fed98ad2b787956105b481e3545ba80e093c7" title="Dell T5820 w/ 2x Dell RTX 3090 for less than $2k - eBay sourced" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_Boffin_"&gt; /u/_Boffin_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qi354b2457de1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i23dhv/dell_t5820_w_2x_dell_rtx_3090_for_less_than_2k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i23dhv/dell_t5820_w_2x_dell_rtx_3090_for_less_than_2k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T17:57:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1z0ur</id>
    <title>NVIDIA unveils Sana for ultra HD image generation on laptops</title>
    <updated>2025-01-15T14:50:16+00:00</updated>
    <author>
      <name>/u/nate4t</name>
      <uri>https://old.reddit.com/user/nate4t</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nate4t"&gt; /u/nate4t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nvlabs.github.io/Sana/?utm_source=substack&amp;amp;utm_medium=email"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1z0ur/nvidia_unveils_sana_for_ultra_hd_image_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1z0ur/nvidia_unveils_sana_for_ultra_hd_image_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T14:50:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1xqrk</id>
    <title>Finally got my second 3090</title>
    <updated>2025-01-15T13:47:51+00:00</updated>
    <author>
      <name>/u/fizzy1242</name>
      <uri>https://old.reddit.com/user/fizzy1242</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1xqrk/finally_got_my_second_3090/"&gt; &lt;img alt="Finally got my second 3090" src="https://preview.redd.it/3zf958trw5de1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a2de0310b00a316ad7343179f08849b452eeb969" title="Finally got my second 3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any good model recommendations for story writing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fizzy1242"&gt; /u/fizzy1242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3zf958trw5de1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1xqrk/finally_got_my_second_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1xqrk/finally_got_my_second_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T13:47:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i28pfq</id>
    <title>UMbreLLa: Llama3.3-70B INT4 on RTX 4070Ti Achieving up to 9.6 Tokens/s! üöÄ</title>
    <updated>2025-01-15T21:45:42+00:00</updated>
    <author>
      <name>/u/Otherwise_Respect_22</name>
      <uri>https://old.reddit.com/user/Otherwise_Respect_22</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i28pfq/umbrella_llama3370b_int4_on_rtx_4070ti_achieving/"&gt; &lt;img alt="UMbreLLa: Llama3.3-70B INT4 on RTX 4070Ti Achieving up to 9.6 Tokens/s! üöÄ" src="https://external-preview.redd.it/26QQz0CLDmbOepvqEr2GHQPtKxnRN6Ls42dwfYRkZX0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a39bedabfd09f0d001c7b69b76443f91d59ef8fd" title="UMbreLLa: Llama3.3-70B INT4 on RTX 4070Ti Achieving up to 9.6 Tokens/s! üöÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;UMbreLLa: Unlocking Llama3.3-70B Performance on Consumer GPUs&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Have you ever imagined running &lt;strong&gt;70B models&lt;/strong&gt; on a consumer GPU at blazing-fast speeds? With &lt;strong&gt;UMbreLLa&lt;/strong&gt;, it's now a reality! Here's what it delivers:&lt;/p&gt; &lt;p&gt;üéØ &lt;strong&gt;Inference Speeds:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;1 x RTX 4070 Ti&lt;/strong&gt;: Up to &lt;strong&gt;9.7 tokens/sec&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1 x RTX 4090&lt;/strong&gt;: Up to &lt;strong&gt;11.4 tokens/sec&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚ú® &lt;strong&gt;What makes it possible?&lt;/strong&gt;&lt;br /&gt; UMbreLLa combines &lt;strong&gt;parameter&lt;/strong&gt; &lt;strong&gt;offloading&lt;/strong&gt;, &lt;strong&gt;speculative decoding&lt;/strong&gt;, and &lt;strong&gt;quantization (AWQ Q4)&lt;/strong&gt;, perfectly tailored for single-user LLM deployment scenarios.&lt;/p&gt; &lt;p&gt;üíª &lt;strong&gt;Why does it matter?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Run &lt;strong&gt;70B models&lt;/strong&gt; on &lt;strong&gt;affordable hardware&lt;/strong&gt; with near-human responsiveness.&lt;/li&gt; &lt;li&gt;Expertly optimized for &lt;strong&gt;coding tasks&lt;/strong&gt; and beyond.&lt;/li&gt; &lt;li&gt;Consumer GPUs finally punching above their weight for high-end LLM inference!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Whether you‚Äôre a developer, researcher, or just an AI enthusiast, this tech transforms how we think about personal AI deployment.&lt;/p&gt; &lt;p&gt;What do you think? Could UMbreLLa be the game-changer we've been waiting for? Let me know your thoughts!&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/Infini-AI-Lab/UMbreLLa"&gt;https://github.com/Infini-AI-Lab/UMbreLLa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;#AI #LLM #RTX4070Ti #RTX4090 #TechInnovation&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1i28pfq/video/z3vvxzjxa8de1/player"&gt;Running UMbreLLa on RTX4070Ti&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otherwise_Respect_22"&gt; /u/Otherwise_Respect_22 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i28pfq/umbrella_llama3370b_int4_on_rtx_4070ti_achieving/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i28pfq/umbrella_llama3370b_int4_on_rtx_4070ti_achieving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i28pfq/umbrella_llama3370b_int4_on_rtx_4070ti_achieving/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T21:45:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1x1mm</id>
    <title>Flow charts, flow charts everywhere</title>
    <updated>2025-01-15T13:10:56+00:00</updated>
    <author>
      <name>/u/AnotherSoftEng</name>
      <uri>https://old.reddit.com/user/AnotherSoftEng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1x1mm/flow_charts_flow_charts_everywhere/"&gt; &lt;img alt="Flow charts, flow charts everywhere" src="https://preview.redd.it/su32pem6q5de1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48d3c26cb427054e3de9f38bb7fa6f4ea73f3686" title="Flow charts, flow charts everywhere" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnotherSoftEng"&gt; /u/AnotherSoftEng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/su32pem6q5de1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1x1mm/flow_charts_flow_charts_everywhere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1x1mm/flow_charts_flow_charts_everywhere/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T13:10:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1xbv1</id>
    <title>OuteTTS 0.3: New 1B &amp; 500M Models</title>
    <updated>2025-01-15T13:26:15+00:00</updated>
    <author>
      <name>/u/OuteAI</name>
      <uri>https://old.reddit.com/user/OuteAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1xbv1/outetts_03_new_1b_500m_models/"&gt; &lt;img alt="OuteTTS 0.3: New 1B &amp;amp; 500M Models" src="https://external-preview.redd.it/MnR2a241bWpzNWRlMS8HG7_sP5Xscyq5qRLwQkOnJIWAwD3-JkIhoicGw7Ke.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=30902d1d9fa313454bf58da04ab6d0ae30505a12" title="OuteTTS 0.3: New 1B &amp;amp; 500M Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OuteAI"&gt; /u/OuteAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rb1px5mjs5de1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1xbv1/outetts_03_new_1b_500m_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1xbv1/outetts_03_new_1b_500m_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T13:26:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1i21u4x</id>
    <title>‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ Would not buy again</title>
    <updated>2025-01-15T16:53:05+00:00</updated>
    <author>
      <name>/u/MoffKalast</name>
      <uri>https://old.reddit.com/user/MoffKalast</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i21u4x/would_not_buy_again/"&gt; &lt;img alt="‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ Would not buy again" src="https://preview.redd.it/rmea76m6s6de1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=013fbcd3bc5ce9ff62b442bfa22ea1b33a661040" title="‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ Would not buy again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoffKalast"&gt; /u/MoffKalast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rmea76m6s6de1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i21u4x/would_not_buy_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i21u4x/would_not_buy_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T16:53:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1i26nk4</id>
    <title>ATTENTION IS ALL YOU NEED PT. 2 - TITANS: Learning to Memorize at Test Time</title>
    <updated>2025-01-15T20:16:09+00:00</updated>
    <author>
      <name>/u/AIGuy3000</name>
      <uri>https://old.reddit.com/user/AIGuy3000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2501.00663v1"&gt;https://arxiv.org/pdf/2501.00663v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The innovation in this field has been iterating at light speed, and I think we have something special here. I tried something similar but I‚Äôm no PhD student and the Math is beyond me. &lt;/p&gt; &lt;p&gt;TLDR; Google Research introduces Titans, a new Al model that learns to store information in a dedicated &amp;quot;long-term memory&amp;quot; at test time. This means it can adapt whenever it sees something surprising, updating its memory on-the-fly. Unlike standard Transformers that handle only the current text window, Titans keep a deeper, more permanent record-similar to short-term vs. long-term memory in humans. The method scales more efficiently (linear time) than traditional Transformers(qudratic time) for very long input sequences. i.e theoretically infinite context windows.&lt;/p&gt; &lt;p&gt;Don‚Äôt be mistaken, this isn‚Äôt just a next-gen ‚Äúartificial intelligence‚Äù, but a step towards to ‚Äúartificial consciousness‚Äù with persistent memory - IF we define consciousness as the ability to model internally(self-modeling), organize, integrate, and recollect of data (with respect to a real-time input)as posited by IIT‚Ä¶ would love to hear y‚Äôall‚Äôs thoughts üß†üëÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIGuy3000"&gt; /u/AIGuy3000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i26nk4/attention_is_all_you_need_pt_2_titans_learning_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i26nk4/attention_is_all_you_need_pt_2_titans_learning_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i26nk4/attention_is_all_you_need_pt_2_titans_learning_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T20:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i29wz5</id>
    <title>Google just released a new architecture</title>
    <updated>2025-01-15T22:38:26+00:00</updated>
    <author>
      <name>/u/FeathersOfTheArrow</name>
      <uri>https://old.reddit.com/user/FeathersOfTheArrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like a big deal? &lt;a href="https://x.com/behrouz_ali/status/1878859086227255347"&gt;Thread by lead author&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeathersOfTheArrow"&gt; /u/FeathersOfTheArrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2501.00663"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i29wz5/google_just_released_a_new_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i29wz5/google_just_released_a_new_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T22:38:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1zcnq</id>
    <title>Hugging Face is doing a FREE and CERTIFIED course on LLM Agents!</title>
    <updated>2025-01-15T15:04:30+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1zcnq/hugging_face_is_doing_a_free_and_certified_course/"&gt; &lt;img alt="Hugging Face is doing a FREE and CERTIFIED course on LLM Agents!" src="https://external-preview.redd.it/JOoVE9yE3UMtoGIBlW4phQep83QjxjwMVZvxo1yvB-4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f737a2f98cecfb91b00fb821dd675f7c44f7794" title="Hugging Face is doing a FREE and CERTIFIED course on LLM Agents!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Learn to build AI agents that can automate tasks, generate code, and more!&lt;/strong&gt; ü§ñ&lt;/p&gt; &lt;p&gt;Hugging Face just launched a &lt;strong&gt;free, certified course&lt;/strong&gt; on building and deploying AI agents.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Learn what Agents are&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Build your own Agents&lt;/strong&gt; using the latest libraries and tools.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Earn a certificate of completion&lt;/strong&gt; to showcase your achievement.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pb6tsyaoa6de1.png?width=720&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5274f727b9755934bfc21f2ec298a48aeaa4c0a"&gt;https://preview.redd.it/pb6tsyaoa6de1.png?width=720&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5274f727b9755934bfc21f2ec298a48aeaa4c0a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Link in here &lt;a href="https://huggingface.co/posts/burtenshaw/334573649974058"&gt;https://huggingface.co/posts/burtenshaw/334573649974058&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1zcnq/hugging_face_is_doing_a_free_and_certified_course/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1zcnq/hugging_face_is_doing_a_free_and_certified_course/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1zcnq/hugging_face_is_doing_a_free_and_certified_course/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T15:04:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i27l37</id>
    <title>Deepseek is overthinking</title>
    <updated>2025-01-15T20:57:13+00:00</updated>
    <author>
      <name>/u/Mr_Jericho</name>
      <uri>https://old.reddit.com/user/Mr_Jericho</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i27l37/deepseek_is_overthinking/"&gt; &lt;img alt="Deepseek is overthinking" src="https://preview.redd.it/rz378lgd18de1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=deff4f920457d1affd3bc98d78e4fc3601dda4b9" title="Deepseek is overthinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Jericho"&gt; /u/Mr_Jericho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rz378lgd18de1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i27l37/deepseek_is_overthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i27l37/deepseek_is_overthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T20:57:13+00:00</published>
  </entry>
</feed>
