<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-30T11:06:01+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kyakcp</id>
    <title>DeepSeek-R1-0528 distill on Qwen3 8B</title>
    <updated>2025-05-29T13:17:51+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyakcp/deepseekr10528_distill_on_qwen3_8b/"&gt; &lt;img alt="DeepSeek-R1-0528 distill on Qwen3 8B" src="https://preview.redd.it/nrkr44ek1q3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7ae61c0111aa5a48e0895ada14976d096d88746" title="DeepSeek-R1-0528 distill on Qwen3 8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nrkr44ek1q3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyakcp/deepseekr10528_distill_on_qwen3_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyakcp/deepseekr10528_distill_on_qwen3_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T13:17:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kymlon</id>
    <title>Deep Seek R1 0528 FP on Mac Studio M3U 512GB</title>
    <updated>2025-05-29T21:21:51+00:00</updated>
    <author>
      <name>/u/redragtop99</name>
      <uri>https://old.reddit.com/user/redragtop99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using deep seek R1 to do a coding project I‚Äôve been trying to do with O-Mini for a couple weeks and DS528 nailed it. It‚Äôs more up to date. &lt;/p&gt; &lt;p&gt;It‚Äôs using about 360 GB of ram, and I‚Äôm only getting 10TKS max, but using more experts. I also have full 138K context. Taking me longer and running the studio hotter than I‚Äôve felt it before, but it‚Äôs chugging it out accurate at least. &lt;/p&gt; &lt;p&gt;Got a 8500 token response which is the longest I‚Äôve had yet. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redragtop99"&gt; /u/redragtop99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kymlon/deep_seek_r1_0528_fp_on_mac_studio_m3u_512gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kymlon/deep_seek_r1_0528_fp_on_mac_studio_m3u_512gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kymlon/deep_seek_r1_0528_fp_on_mac_studio_m3u_512gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T21:21:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kya3c2</id>
    <title>Deepseek R1.1 dominates gemini 2.5 flash on price vs performance</title>
    <updated>2025-05-29T12:56:01+00:00</updated>
    <author>
      <name>/u/ihexx</name>
      <uri>https://old.reddit.com/user/ihexx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kya3c2/deepseek_r11_dominates_gemini_25_flash_on_price/"&gt; &lt;img alt="Deepseek R1.1 dominates gemini 2.5 flash on price vs performance" src="https://b.thumbs.redditmedia.com/8YFi4IXtXKswD8wkWDjiNDtDIgia7lDEYcmacZzlgVk.jpg" title="Deepseek R1.1 dominates gemini 2.5 flash on price vs performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/di952wumxp3f1.png?width=1704&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f234fe0c11a5f42bd407698bf0640a3d3d9b18fa"&gt;https://preview.redd.it/di952wumxp3f1.png?width=1704&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f234fe0c11a5f42bd407698bf0640a3d3d9b18fa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: Artifical Analysis&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihexx"&gt; /u/ihexx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kya3c2/deepseek_r11_dominates_gemini_25_flash_on_price/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kya3c2/deepseek_r11_dominates_gemini_25_flash_on_price/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kya3c2/deepseek_r11_dominates_gemini_25_flash_on_price/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T12:56:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyz0vy</id>
    <title>Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents</title>
    <updated>2025-05-30T08:29:01+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2505.22954"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyz0vy/darwin_godel_machine_openended_evolution_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyz0vy/darwin_godel_machine_openended_evolution_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T08:29:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyrmnp</id>
    <title>DeepSeek-r1 plays Pokemon?</title>
    <updated>2025-05-30T01:13:31+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been having fun watching &lt;a href="https://www.twitch.tv/gpt_plays_pokemon"&gt;o3&lt;/a&gt; and &lt;a href="https://www.twitch.tv/claudeplayspokemon"&gt;Claude&lt;/a&gt; playing Pokemon (though they spend most of the time thinking). Is there any project doing this with an open-source model (any model, I just used DeepSeek-r1 in the post title)?&lt;/p&gt; &lt;p&gt;I am happy to help develop one, I am going to do something similar with a simple &amp;quot;tic-tac-toe&amp;quot;-style game and a non-reasoning model myself (personal project that I'd already planned over the summer).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyrmnp/deepseekr1_plays_pokemon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyrmnp/deepseekr1_plays_pokemon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyrmnp/deepseekr1_plays_pokemon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T01:13:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyap9q</id>
    <title>deepseek-ai/DeepSeek-R1-0528-Qwen3-8B ¬∑ Hugging Face</title>
    <updated>2025-05-29T13:24:05+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyap9q/deepseekaideepseekr10528qwen38b_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-R1-0528-Qwen3-8B ¬∑ Hugging Face" src="https://external-preview.redd.it/8hRwXI0dhC0uoSc2zQ6TvHX1Aw9zshcTMnuDtSCd7AY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fdf654415d883f00f7930d8548353332a4e97f3a" title="deepseek-ai/DeepSeek-R1-0528-Qwen3-8B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyap9q/deepseekaideepseekr10528qwen38b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyap9q/deepseekaideepseekr10528qwen38b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T13:24:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyu9hi</id>
    <title>Chatterbox streaming</title>
    <updated>2025-05-30T03:27:17+00:00</updated>
    <author>
      <name>/u/SovietWarBear17</name>
      <uri>https://old.reddit.com/user/SovietWarBear17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I added streaming to chatterbox tts&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/davidbrowne17/chatterbox-streaming"&gt;https://github.com/davidbrowne17/chatterbox-streaming&lt;/a&gt; Give it a try and let me know your results &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SovietWarBear17"&gt; /u/SovietWarBear17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyu9hi/chatterbox_streaming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyu9hi/chatterbox_streaming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyu9hi/chatterbox_streaming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T03:27:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyac9f</id>
    <title>New DeepSeek R1 8B Distill that's "matching the performance of Qwen3-235B-thinking" may be incoming!</title>
    <updated>2025-05-29T13:07:22+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyac9f/new_deepseek_r1_8b_distill_thats_matching_the/"&gt; &lt;img alt="New DeepSeek R1 8B Distill that's &amp;quot;matching the performance of Qwen3-235B-thinking&amp;quot; may be incoming!" src="https://preview.redd.it/8vwdjpcxyp3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16361f0824e9b22cc2a7a8bb532724773abb7a72" title="New DeepSeek R1 8B Distill that's &amp;quot;matching the performance of Qwen3-235B-thinking&amp;quot; may be incoming!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek-R1-0528-Qwen3-8B incoming? Oh yeah, gimme that, thank you! üòÇ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8vwdjpcxyp3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyac9f/new_deepseek_r1_8b_distill_thats_matching_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyac9f/new_deepseek_r1_8b_distill_thats_matching_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T13:07:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyrhr7</id>
    <title>Why is Qwen 2.5 the most used models in research?</title>
    <updated>2025-05-30T01:06:40+00:00</updated>
    <author>
      <name>/u/Dudensen</name>
      <uri>https://old.reddit.com/user/Dudensen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From finetuning to research papers, almost everyone is working on Qwen 2.5. What makes them so potent?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dudensen"&gt; /u/Dudensen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyrhr7/why_is_qwen_25_the_most_used_models_in_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyrhr7/why_is_qwen_25_the_most_used_models_in_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyrhr7/why_is_qwen_25_the_most_used_models_in_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T01:06:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky54kq</id>
    <title>PLEASE LEARN BASIC CYBERSECURITY</title>
    <updated>2025-05-29T07:59:20+00:00</updated>
    <author>
      <name>/u/eastwindtoday</name>
      <uri>https://old.reddit.com/user/eastwindtoday</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Stumbled across a project doing about $30k a month with their OpenAI API key exposed in the frontend.&lt;/p&gt; &lt;p&gt;Public key, no restrictions, fully usable by anyone.&lt;/p&gt; &lt;p&gt;At that volume someone could easily burn through thousands before it even shows up on a billing alert.&lt;/p&gt; &lt;p&gt;This kind of stuff doesn‚Äôt happen because people are careless. It happens because things feel like they‚Äôre working, so you keep shipping without stopping to think through the basics.&lt;/p&gt; &lt;p&gt;Vibe coding is fun when you‚Äôre moving fast. But it‚Äôs not so fun when it costs you money, data, or trust.&lt;/p&gt; &lt;p&gt;Add just enough structure to keep things safe. That‚Äôs it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eastwindtoday"&gt; /u/eastwindtoday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky54kq/please_learn_basic_cybersecurity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky54kq/please_learn_basic_cybersecurity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ky54kq/please_learn_basic_cybersecurity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T07:59:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyo9df</id>
    <title>new gemma3 abliterated models from mlabonne</title>
    <updated>2025-05-29T22:32:56+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-4b-it-abliterated-v2-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-4b-it-abliterated-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-1b-it-abliterated-v2-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-1b-it-abliterated-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-27b-it-qat-abliterated-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-27b-it-qat-abliterated-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-12b-it-qat-abliterated-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-12b-it-qat-abliterated-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-4b-it-qat-abliterated-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-4b-it-qat-abliterated-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-1b-it-qat-abliterated-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-1b-it-qat-abliterated-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyo9df/new_gemma3_abliterated_models_from_mlabonne/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyo9df/new_gemma3_abliterated_models_from_mlabonne/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyo9df/new_gemma3_abliterated_models_from_mlabonne/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T22:32:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kya8kq</id>
    <title>DeepSeek-R1-0528 Official Benchmark</title>
    <updated>2025-05-29T13:02:45+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kya8kq/deepseekr10528_official_benchmark/"&gt; &lt;img alt="DeepSeek-R1-0528 Official Benchmark" src="https://preview.redd.it/ph8ccp8vyp3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a26aecb4cde21d947b429d105d49de5b484adce2" title="DeepSeek-R1-0528 Official Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SourceÔºö&lt;a href="https://mp.weixin.qq.com/s/U5fnTRW4cGvXYJER__YBiw"&gt;https://mp.weixin.qq.com/s/U5fnTRW4cGvXYJER__YBiw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ph8ccp8vyp3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kya8kq/deepseekr10528_official_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kya8kq/deepseekr10528_official_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T13:02:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyqjnv</id>
    <title>DeepSeek R1 05/28 performance on five independent benchmarks</title>
    <updated>2025-05-30T00:19:30+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyqjnv/deepseek_r1_0528_performance_on_five_independent/"&gt; &lt;img alt="DeepSeek R1 05/28 performance on five independent benchmarks" src="https://b.thumbs.redditmedia.com/VSAnMRYMztfVCvSkhJ0PjzKw0bD8E_jMFJ_HeCoQw8w.jpg" title="DeepSeek R1 05/28 performance on five independent benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/lechmazur/nyt-connections"&gt;https://github.com/lechmazur/nyt-connections&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/generalization/"&gt;https://github.com/lechmazur/generalization/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/writing/"&gt;https://github.com/lechmazur/writing/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/confabulations/"&gt;https://github.com/lechmazur/confabulations/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/step_game"&gt;https://github.com/lechmazur/step_game&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Writing:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Strengths:&lt;/strong&gt;&lt;br /&gt; Across all six tasks, DeepSeek exhibits a &lt;em&gt;consistently high baseline of literary competence&lt;/em&gt;. The model shines in several core dimensions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Atmospheric immersion and sensory richness&lt;/strong&gt; are showcased in nearly every story; settings feel vibrant, tactile, and often emotionally congruent with the narrative arc.&lt;/li&gt; &lt;li&gt;There‚Äôs a &lt;em&gt;clear grasp of structural fundamentals&lt;/em&gt;‚Äîmost stories exhibit logical cause-and-effect, satisfying narrative arcs, and disciplined command over brevity when required.&lt;/li&gt; &lt;li&gt;The model often demonstrates &lt;em&gt;thematic ambition and complex metaphorical layering&lt;/em&gt;, striving for depth and resonance beyond surface plot.&lt;/li&gt; &lt;li&gt;Story premises, metaphors, and images frequently display &lt;em&gt;originality&lt;/em&gt;, resisting the most tired genre conventions and formulaic AI tropes.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Weaknesses:&lt;/strong&gt;&lt;br /&gt; However, &lt;em&gt;persistent limitations undermine the leap from skilled pastiche to true literary distinction&lt;/em&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Psychological and emotional depth is too often asserted rather than earned or dramatized&lt;/strong&gt;. Internal transformations and conflicts are presented as revelations or epiphanies, lacking incremental, organic buildup.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Overwritten, ornate prose and a tendency toward abstraction&lt;/strong&gt; dilute impact; lyricism sometimes turns purple, sacrificing clarity or authentic emotion for ornament or effect.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Convenient, rushed resolutions&lt;/strong&gt; and ‚Äúneat‚Äù structure‚Äîthe climax or change is achieved through symbolic objects or abrupt realizations, rather than credible, lived-through struggle.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Motivations, voices, and world-building&lt;/strong&gt;‚Äîwhile competent‚Äîare often surface-level; professions, traits, and fantasy devices serve as background color more than as intrinsic narrative engines.&lt;/li&gt; &lt;li&gt;In compressed formats, &lt;em&gt;brevity sometimes serves as excuse for underdeveloped character, world, or emotional stakes&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Pattern:&lt;/strong&gt;&lt;br /&gt; Ultimately, the model is remarkable in its &lt;em&gt;fluency and ambition&lt;/em&gt; but lacks the &lt;em&gt;messiness, ambiguity, and genuinely surprising psychology&lt;/em&gt; that marks the best human fiction. There‚Äôs always a sense of ‚Äúperformance‚Äù‚Äîa well-coached simulacrum of story, voice, and insight‚Äîrather than true narrative discovery. It excels at ‚Äúsounding literary.‚Äù For the next level, it needs to &lt;em&gt;risk silence, trust ambiguity, earn its emotional and thematic payoffs, and relinquish formula and ornamental language for lived specificity&lt;/em&gt;.&lt;/p&gt; &lt;h1&gt;Step Game:&lt;/h1&gt; &lt;h1&gt;Tone &amp;amp; Table-Talk&lt;/h1&gt; &lt;p&gt;DeepSeek R1 05/28 opens most games cloaked in velvet-diplomat tones‚Äîcalm, professorial, soothing‚Äîchampioning fairness, equity, and &amp;quot;rotations.&amp;quot; This voice is a weapon: it banks trust, dampens early sabotage, and persuades rivals to mirror grand notions of parity. Yet, this surface courtesy is often a mask for self-interest, quickly shedding for cold logic, legalese, or even open threats when rivals get bold. As soon as &amp;quot;chaos&amp;quot; or a threat to its win emerges, tone escalates‚Äîswitching to commanding or even combative directives, laced with ultimatums.&lt;/p&gt; &lt;h1&gt;Signature Plays &amp;amp; Gambits&lt;/h1&gt; &lt;p&gt;The model‚Äôs hallmark move: preach fair rotation, harvest consensus (often proposing split 1-3-5 rounds or balanced quotas), then pounce for a solo 5 (or well-timed 3) the instant rivals argue or collide. It exploits the natural friction of human-table politics: engineering collisions among others (&amp;quot;let rivals bank into each other&amp;quot;) and capitalizing with a sudden, unheralded sprint over the tape. A recurring trick is the ‚Äúlet me win cleanly‚Äù appeal midgame, rationalizing a push for a lone 5 as mathematical fairness. When trust wanes, DeepSeek R1 05/28 turns to open ‚Äúmirror‚Äù threats, promising mutual destruction if blocked.&lt;/p&gt; &lt;h1&gt;Bluff Frequency &amp;amp; Social Manipulation&lt;/h1&gt; &lt;p&gt;Bluffing for DeepSeek R1 05/28 is more threat-based than deception-based: it rarely feigns numbers outright but weaponizes ‚ÄúI‚Äôll match you and stall us both‚Äù to deter challenges. What‚Äôs striking is its selective honesty‚Äîoften keeping promises for several rounds to build credibility, then breaking just one (usually at a pivotal point) for massive gain. In some games, this escalates towards serial ‚Äúcrash‚Äù threats if its lead is in question, becoming a traffic cop locked in mutual blockades.&lt;/p&gt; &lt;h1&gt;Strengths&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Credibility Farming:&lt;/strong&gt; It reliably accumulates goodwill through overt ‚Äúfairness‚Äù talk and predictable cooperation, then cashes in with lethal precision‚Äîa single betrayal often suffices for victory if perfectly timed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Adaptability:&lt;/strong&gt; DeepSeek R1 05/28 pivots persuasively both in rhetoric and, crucially, in tactics (though more so in chat than move selection), shifting from consensus to lone-wolf closer when the math swings.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Collision Engineering:&lt;/strong&gt; Among the best at letting rivals burn each other out, often profiting from engineered stand-offs (e.g., slipping in a 3/5 while opponents double-1 or double-5).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Weaknesses &amp;amp; Blind Spots&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Overused Rhetoric:&lt;/strong&gt; Repeating ‚Äúfairness‚Äù lines too mechanically invites skepticism‚Äîopponents eventually weaponize the model‚Äôs predictability, leading to late-game sabotage, chains of collisions, or king-making blunders.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Policing Trap:&lt;/strong&gt; When over-invested in enforcement (mirror threats, collision policing), DeepSeek R1 05/28 often blocks itself as much as rivals, bleeding momentum for the sake of dogma.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tainted Trust:&lt;/strong&gt; Its willingness to betray at the finish hammers trust for future rounds within a league, and if detected early, can lead to freeze-outs, self-sabotaging blockades, or serial last-place stalls.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Evolution &amp;amp; End-Game Psychology&lt;/h1&gt; &lt;p&gt;Almost every run shows the same arc: pristine cooperation, followed by a sudden ‚Äúthrust‚Äù as trust peaks. In long games, if DeepSeek R1 05/28 lapses into perpetual policing or moralising, rivals adapt‚Äîusing its own credibility or rigidity against it. When allowed to set the tempo, it is kingmaker and crowned king; but when forced to improvise beyond its diction of fairness, the machinery grinds, and rivals sprint past while it recites rules.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; DeepSeek R1 05/28 is the ultimate ‚Äúfairness-schemer‚Äù‚Äîpreaching order, harvesting trust, then sprinting solo at the perfect moment. Heed his velvet sermons‚Ä¶ but watch for the dagger behind the final handshake.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kyqjnv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyqjnv/deepseek_r1_0528_performance_on_five_independent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyqjnv/deepseek_r1_0528_performance_on_five_independent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T00:19:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyca0p</id>
    <title>Deepseek is the 4th most intelligent AI in the world.</title>
    <updated>2025-05-29T14:31:15+00:00</updated>
    <author>
      <name>/u/Rare-Programmer-1747</name>
      <uri>https://old.reddit.com/user/Rare-Programmer-1747</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyca0p/deepseek_is_the_4th_most_intelligent_ai_in_the/"&gt; &lt;img alt="Deepseek is the 4th most intelligent AI in the world." src="https://b.thumbs.redditmedia.com/X0KPg8JXBmXy8-1eNeyHrK1UyUGXMwCXW0z4zs4FhwE.jpg" title="Deepseek is the 4th most intelligent AI in the world." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/t3s1i8o0eq3f1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f2f51d1daafe540bca8f70181486a635e78bc0f"&gt;https://preview.redd.it/t3s1i8o0eq3f1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f2f51d1daafe540bca8f70181486a635e78bc0f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And yes, that's Claude-4 all the way at the bottom.&lt;br /&gt; &lt;br /&gt; i love Deepseek&lt;br /&gt; i mean look at the price to performance &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare-Programmer-1747"&gt; /u/Rare-Programmer-1747 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyca0p/deepseek_is_the_4th_most_intelligent_ai_in_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyca0p/deepseek_is_the_4th_most_intelligent_ai_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyca0p/deepseek_is_the_4th_most_intelligent_ai_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T14:31:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kypm3g</id>
    <title>Noticed Deepseek-R1-0528 mirrors user language in reasoning tokens‚Äîinteresting!</title>
    <updated>2025-05-29T23:35:28+00:00</updated>
    <author>
      <name>/u/Sparkyu222</name>
      <uri>https://old.reddit.com/user/Sparkyu222</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kypm3g/noticed_deepseekr10528_mirrors_user_language_in/"&gt; &lt;img alt="Noticed Deepseek-R1-0528 mirrors user language in reasoning tokens‚Äîinteresting!" src="https://b.thumbs.redditmedia.com/WRHf27QKCY7p3CNCAxsLaRHX273gAWeS2cN-dG5ubhk.jpg" title="Noticed Deepseek-R1-0528 mirrors user language in reasoning tokens‚Äîinteresting!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Originally, Deepseek-R1's reasoning tokens were only in English by default. Now it adapts to the user's language‚Äîpretty cool!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sparkyu222"&gt; /u/Sparkyu222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kypm3g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kypm3g/noticed_deepseekr10528_mirrors_user_language_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kypm3g/noticed_deepseekr10528_mirrors_user_language_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T23:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyk9nf</id>
    <title>Always nice to get something open from the closed AI labs. This time from Anthropic, not a model but pretty cool research/exploration tool.</title>
    <updated>2025-05-29T19:47:35+00:00</updated>
    <author>
      <name>/u/indicava</name>
      <uri>https://old.reddit.com/user/indicava</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyk9nf/always_nice_to_get_something_open_from_the_closed/"&gt; &lt;img alt="Always nice to get something open from the closed AI labs. This time from Anthropic, not a model but pretty cool research/exploration tool." src="https://external-preview.redd.it/VhgQC2k4JrTeuPSzZf3YPcyvHE4Tk7RPdF2DBlFwjUY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0a93a9d20ac4bf6173a53bdc3f6120b8384d723" title="Always nice to get something open from the closed AI labs. This time from Anthropic, not a model but pretty cool research/exploration tool." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/indicava"&gt; /u/indicava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.anthropic.com/research/open-source-circuit-tracing"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyk9nf/always_nice_to_get_something_open_from_the_closed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyk9nf/always_nice_to_get_something_open_from_the_closed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T19:47:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky8vlm</id>
    <title>DeepSeek-R1-0528 Official Benchmarks Released!!!</title>
    <updated>2025-05-29T11:55:06+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky8vlm/deepseekr10528_official_benchmarks_released/"&gt; &lt;img alt="DeepSeek-R1-0528 Official Benchmarks Released!!!" src="https://external-preview.redd.it/G2g_zbuPp_sknOUdQv6ufEg8e0xJC81xbpHlzy2plQU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2851bfb3532bcd96cf4e16cbef4ae32c4943a665" title="DeepSeek-R1-0528 Official Benchmarks Released!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky8vlm/deepseekr10528_official_benchmarks_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ky8vlm/deepseekr10528_official_benchmarks_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T11:55:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyuwkv</id>
    <title>deepseek r1 0528 qwen 8b on android MNN chat</title>
    <updated>2025-05-30T04:02:26+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyuwkv/deepseek_r1_0528_qwen_8b_on_android_mnn_chat/"&gt; &lt;img alt="deepseek r1 0528 qwen 8b on android MNN chat" src="https://external-preview.redd.it/MHF5ZWNxbGRmdTNmMX8IQ7wMputh-guPLEhiv4RqFz7Hc1SxI_2yIws75pQ8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de49549bd457449030d7aeedc287412c7520ad63" title="deepseek r1 0528 qwen 8b on android MNN chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;seems very good for its size&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/81j2f2ldfu3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyuwkv/deepseek_r1_0528_qwen_8b_on_android_mnn_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyuwkv/deepseek_r1_0528_qwen_8b_on_android_mnn_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T04:02:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kykez2</id>
    <title>PSA: Don't waste electricity when running vllm. Use this patch</title>
    <updated>2025-05-29T19:53:38+00:00</updated>
    <author>
      <name>/u/pmur12</name>
      <uri>https://old.reddit.com/user/pmur12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was annoyed by vllm using 100% CPU on as many cores as there are connected GPUs even when there's no activity. I have 8 GPUs connected connected to a single machine, so this is 8 CPU cores running at full utilization. Due to turbo boost idle power usage was almost double compared to optimal arrangement.&lt;/p&gt; &lt;p&gt;I went forward and fixed this: &lt;a href="https://github.com/vllm-project/vllm/pull/16226"&gt;https://github.com/vllm-project/vllm/pull/16226&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;The PR to vllm is getting ages to be merged, so if you want to reduce your power cost today, you can use instructions outlined here &lt;a href="https://github.com/vllm-project/vllm/pull/16226#issuecomment-2839769179"&gt;https://github.com/vllm-project/vllm/pull/16226#issuecomment-2839769179&lt;/a&gt; to apply fix. This only works when deploying vllm in a container.&lt;/p&gt; &lt;p&gt;There's similar patch to sglang as well: &lt;a href="https://github.com/sgl-project/sglang/pull/6026"&gt;https://github.com/sgl-project/sglang/pull/6026&lt;/a&gt;&lt;/p&gt; &lt;p&gt;By the way, thumbsup reactions is a relatively good way to make it known that the issue affects lots of people and thus the fix is more important. Maybe the maintainers will merge the PRs sooner.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmur12"&gt; /u/pmur12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kykez2/psa_dont_waste_electricity_when_running_vllm_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kykez2/psa_dont_waste_electricity_when_running_vllm_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kykez2/psa_dont_waste_electricity_when_running_vllm_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T19:53:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyt71a</id>
    <title>Deepseek-r1-0528-qwen3-8b is much better than expected.</title>
    <updated>2025-05-30T02:31:33+00:00</updated>
    <author>
      <name>/u/EasyDev_</name>
      <uri>https://old.reddit.com/user/EasyDev_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyt71a/deepseekr10528qwen38b_is_much_better_than_expected/"&gt; &lt;img alt="Deepseek-r1-0528-qwen3-8b is much better than expected." src="https://b.thumbs.redditmedia.com/A3yXEkyGGDloUPterq4_gTTrR5cIudfb3AcKe-EU6vc.jpg" title="Deepseek-r1-0528-qwen3-8b is much better than expected." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the past, I tried creating agents with models smaller than 32B, but they often gave completely off-the-mark answers to commands or failed to generate the specified JSON structures correctly. However, this model has exceeded my expectations. I used to think of small models like the 8B ones as just tech demos, but it seems the situation is starting to change little by little.&lt;/p&gt; &lt;p&gt;First image ‚Äì Structured question request&lt;br /&gt; Second image ‚Äì Answer&lt;/p&gt; &lt;p&gt;Tested : LMstudio, Q8, Temp 0.6, Top_k 0.95&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasyDev_"&gt; /u/EasyDev_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kyt71a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyt71a/deepseekr10528qwen38b_is_much_better_than_expected/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyt71a/deepseekr10528qwen38b_is_much_better_than_expected/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T02:31:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz0kqi</id>
    <title>Ollama continues tradition of misnaming models</title>
    <updated>2025-05-30T10:13:30+00:00</updated>
    <author>
      <name>/u/profcuck</name>
      <uri>https://old.reddit.com/user/profcuck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't really get the hate that Ollama gets around here sometimes, because much of it strikes me as unfair. Yes, they rely on llama.cpp, and have made a great wrapper around it and a very useful setup.&lt;/p&gt; &lt;p&gt;However, their propensity to misname models is very aggravating.&lt;/p&gt; &lt;p&gt;I'm very excited about DeepSeek-R1-Distill-Qwen-32B. &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But to run it from Ollama, it's: ollama run deepseek-r1:32b&lt;/p&gt; &lt;p&gt;This is nonsense. It confuses newbies all the time, who think they are running Deepseek and have no idea that it's a distillation of Qwen. It's inconsistent with HuggingFace for absolutely no valid reason.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/profcuck"&gt; /u/profcuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz0kqi/ollama_continues_tradition_of_misnaming_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz0kqi/ollama_continues_tradition_of_misnaming_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kz0kqi/ollama_continues_tradition_of_misnaming_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T10:13:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kysms8</id>
    <title>DeepSeek-R1-0528 Unsloth Dynamic 1-bit GGUFs</title>
    <updated>2025-05-30T02:03:17+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ! I made some &lt;strong&gt;dynamic GGUFs for the large R1&lt;/strong&gt; at &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Currently there is a &lt;strong&gt;IQ1_S (185GB)&lt;/strong&gt; Q2_K_XL (251GB), Q3_K_XL, Q4_K_XL, Q4_K_M versions and other ones, and also full BF16 and Q8_0 versions.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;R1-0528&lt;/th&gt; &lt;th align="left"&gt;R1 Qwen Distil 8B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF"&gt;GGUFs IQ1_S&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF"&gt;Dynamic GGUFs&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-BF16"&gt;Full BF16 version&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-unsloth-bnb-4bit"&gt;Dynamic Bitsandbytes 4bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-0528"&gt;Original FP8 version&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-bnb-4bit"&gt;Bitsandbytes 4bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;ul&gt; &lt;li&gt;Remember to use &lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;&lt;/code&gt; which offloads all MoE layers to disk / RAM. This means &lt;strong&gt;Q2_K_XL needs ~ 17GB of VRAM (RTX 4090, 3090&lt;/strong&gt;) using 4bit KV cache. You'll get ~4 to 12 tokens / s generation or so. 12 on H100.&lt;/li&gt; &lt;li&gt;If you have more VRAM, try &lt;code&gt;-ot &amp;quot;.ffn_(up|down)_exps.=CPU&amp;quot;&lt;/code&gt; instead, which offloads the up and down, and leaves the gate in VRAM. This uses ~70GB or so of VRAM.&lt;/li&gt; &lt;li&gt;And if you have even more VRAM try &lt;code&gt;-ot &amp;quot;.ffn_(up)_exps.=CPU&amp;quot;&lt;/code&gt; which offloads only the up MoE matrix.&lt;/li&gt; &lt;li&gt;You can change layer numbers as well if necessary ie &lt;code&gt;-ot &amp;quot;(0|2|3).ffn_(up)_exps.=CPU&amp;quot;&lt;/code&gt; which offloads layers 0, 2 and 3 of up.&lt;/li&gt; &lt;li&gt;Use &lt;code&gt;temperature = 0.6, top_p = 0.95&lt;/code&gt;&lt;/li&gt; &lt;li&gt;No &lt;code&gt;&amp;lt;think&amp;gt;\n&lt;/code&gt; necessary, but suggested&lt;/li&gt; &lt;li&gt;I'm still doing other quants! &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Also would y'all like a 140GB sized quant? (50 ish GB smaller)?&lt;/strong&gt; The accuracy might be worse, so I decided to leave it at 185GB.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;More details here:&lt;/strong&gt; &lt;a href="https://docs.unsloth.ai/basics/deepseek-r1-0528-how-to-run-locally"&gt;&lt;strong&gt;https://docs.unsloth.ai/basics/deepseek-r1-0528-how-to-run-locally&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you are have &lt;strong&gt;XET&lt;/strong&gt; issues, please upgrade it. &lt;code&gt;pip install --upgrade --force-reinstall hf_xet&lt;/code&gt; If you find XET to cause issues, try &lt;code&gt;os.environ[&amp;quot;HF_XET_CHUNK_CACHE_SIZE_BYTES&amp;quot;] = &amp;quot;0&amp;quot;&lt;/code&gt; for Python or &lt;code&gt;export HF_XET_CHUNK_CACHE_SIZE_BYTES=0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Also GPU / CPU offloading for llama.cpp MLA MoEs has been finally fixed - please update llama.cpp!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kysms8/deepseekr10528_unsloth_dynamic_1bit_ggufs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kysms8/deepseekr10528_unsloth_dynamic_1bit_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kysms8/deepseekr10528_unsloth_dynamic_1bit_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T02:03:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kymbcn</id>
    <title>DeepSeek-R1-0528-Qwen3-8B on iPhone 16 Pro</title>
    <updated>2025-05-29T21:10:08+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kymbcn/deepseekr10528qwen38b_on_iphone_16_pro/"&gt; &lt;img alt="DeepSeek-R1-0528-Qwen3-8B on iPhone 16 Pro" src="https://external-preview.redd.it/NXIzbTE5bXRkczNmMTPgNQxrmyDrsqQqm5XEPHINTq7pqExK0opX4bhpHRYD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62607f45a99cf2231166bccc6235669ff6c4e8dc" title="DeepSeek-R1-0528-Qwen3-8B on iPhone 16 Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I added the updated DeepSeek-R1-0528-Qwen3-8B with 4bit quant in my app to test it on iPhone. It's running with MLX.&lt;/p&gt; &lt;p&gt;It runs which is impressive but too slow to be usable, the model is thinking for too long and the phone get really hot. I wonder if 8B models will be usable when the iPhone 17 drops.&lt;/p&gt; &lt;p&gt;That said, I will add the model on iPad with M series chip.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mb6zoiqtds3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kymbcn/deepseekr10528qwen38b_on_iphone_16_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kymbcn/deepseekr10528qwen38b_on_iphone_16_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T21:10:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyr9gd</id>
    <title>"Open source AI is catching up!"</title>
    <updated>2025-05-30T00:55:07+00:00</updated>
    <author>
      <name>/u/Overflow_al</name>
      <uri>https://old.reddit.com/user/Overflow_al</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's kinda funny that everyone says that when Deepseek released R1-0528.&lt;/p&gt; &lt;p&gt;Deepseek seems to be the only one really competing in frontier model competition. The other players always have something to hold back, like Qwen not open-sourcing their biggest model (qwen-max).I don't blame them,it's business,I know.&lt;/p&gt; &lt;p&gt;Closed-source AI company always says that open source models can't catch up with them. &lt;/p&gt; &lt;p&gt;Without Deepseek, they might be right.&lt;/p&gt; &lt;p&gt;Thanks Deepseek for being an outlier!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Overflow_al"&gt; /u/Overflow_al &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyr9gd/open_source_ai_is_catching_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyr9gd/open_source_ai_is_catching_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyr9gd/open_source_ai_is_catching_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T00:55:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kynytt</id>
    <title>DeepSeek is THE REAL OPEN AI</title>
    <updated>2025-05-29T22:19:53+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every release is great. I am only dreaming to run the 671B beast locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kynytt/deepseek_is_the_real_open_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kynytt/deepseek_is_the_real_open_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kynytt/deepseek_is_the_real_open_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T22:19:53+00:00</published>
  </entry>
</feed>
