<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-30T15:23:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jne5xo</id>
    <title>Low profile cpu cooler?</title>
    <updated>2025-03-30T14:55:05+00:00</updated>
    <author>
      <name>/u/Ok-Anxiety8313</name>
      <uri>https://old.reddit.com/user/Ok-Anxiety8313</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jne5xo/low_profile_cpu_cooler/"&gt; &lt;img alt="Low profile cpu cooler?" src="https://b.thumbs.redditmedia.com/VYOLoAT_3-L3CBi4yL8E-4dEBJvUTHE9v7BINgf9GVo.jpg" title="Low profile cpu cooler?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got an open frame to have more space between GPUs. I got the &lt;a href="https://www.ebay.com/itm/405431181041"&gt;Veddha T3 6-GPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unfortunately, my current CPU cooler (Dark Rock Pro 4) does not fit between the mobo level and &amp;quot;gpu tray&amp;quot; so I need to get a lower profile CPU cooler.&lt;/p&gt; &lt;p&gt;I am debating between a low profile air cooler and watercooling. A smaller air cooler should fit but then I am afraid the PCIe extenders might be too short to go around the cooler or will be too bended. On the other hand, a water cooler would use minimal vertical space but then I need to find a place for the tubes and radiator which I don't like and also I generally don't love AIO reliability/durability.&lt;/p&gt; &lt;p&gt;What kind of cooler should I get or avoid?&lt;/p&gt; &lt;p&gt;My CPU is a ryzen 7950X.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Anxiety8313"&gt; /u/Ok-Anxiety8313 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jne5xo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jne5xo/low_profile_cpu_cooler/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jne5xo/low_profile_cpu_cooler/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T14:55:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jner1g</id>
    <title>What is deep research to you?</title>
    <updated>2025-03-30T15:21:58+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm updating an old framework I have to seamlessly perform a simple online search in duckduckgo search (if the user activates that feature), retrieving the text results from the results only, but it only yields an overview of the text contents of the page, which is ok for quick search since the results are returned immediately.&lt;/p&gt; &lt;p&gt;The system recognizes complex inquiries intuitively and if the user requests a deep search, it proceeds to perform a systematic, agentic search online from the results, yielding 10 results, rather than simply parsing the overview text. I'm trying to get more ideas as to how to actually incorporate and expand deep search functionality to take a more broad, systematic, agentic approach. Here is what I have so far:&lt;/p&gt; &lt;p&gt;1 - Activate Deep Search when prompted, generating a query related to the user's inquiry, using the convo history as additional context.&lt;/p&gt; &lt;p&gt;2 - For each search result: check if the website respects robots.txt and if the text overview is related to the user's inquiry and if so, scrape the text inside webpage.&lt;/p&gt; &lt;p&gt;3 - If the webpage contains links, use the user's inquiry, convo history and the scraped text from the page itself (summarizing the text contents from context length-long chunks if the text is greater than the context length before achieving a final summary) to ask a list of questions related to the user's inquiry and the info gathered so far.&lt;/p&gt; &lt;p&gt;4 - After generating the list of questions, a list of links inside the search result is sent to the agent to see if any of the links may be related to the user's inquiry and the list of questions. If any link is detected as relevant, the agent selects that link and recursively performs step 2, but for links instead of search results. Keep in mind this is all done inside the same search result. If none of the links presented are related or there is an issue accessing the link, the agent stops digging and moves on to the next search result.&lt;/p&gt; &lt;p&gt;Once all of that is done, the agent will summarize each chunk of text gathered related to each search result, then provide a final summary before providing an answer to the user.&lt;/p&gt; &lt;p&gt;This actually works surprisingly well and is stable enough to keep going and gathering tons of accurate information. So once I deal with a number of issues (convo history chunking, handling pdf links, etc.) I want to expand the scope of the deep search further to reach even deeper conclusions. Here are some ideas:&lt;/p&gt; &lt;p&gt;1 - Scrape youtube videos - duckduckgo_search allows you to return youtube videos. I already have methods set up to perform the search and auto-download batches of youtube videos based on the search results and converting them to mp4. This is done with duckduckgo_search, yt-dlp and ffmpeg. All I would need to do afterwards is to break up the audio into 30-second temp audio clips and use local whisper to transcribe the audio and use the deep search agent to chunk/summarize them and include the information as part of the inquiry.&lt;/p&gt; &lt;p&gt;2 - That's it. Lmao.&lt;/p&gt; &lt;p&gt;If you read this far, you're probably thinking to yourself that this would take forever, and honestly, yes it does take a long time to generate an answer but when it does, it really does generate a goldmine of information that the agent worked so hard to gather, so my version of Deep Search is built for the patient in mind, who really need a lot of information or need to make sure you have incredibly precise information and are willing to wait for results. &lt;/p&gt; &lt;p&gt;I think its interesting to see the effects of scraping youtube videos alongside search results. I tried scraping related images from the links inside the search results but the agent kept correctly discarding the images as irrelevant, which means there usually isn't much valuable info to gather with images themselves.&lt;/p&gt; &lt;p&gt;That being said, I feel like even here I'm not doing enough to provide a satisfactory deep search. I feel like there should be additional functionality included (like RAG, etc.) and I'm personally not satisfied with this approach, even if it does yield valuable information. &lt;/p&gt; &lt;p&gt;So that begs the question: what is your interpretation of deep search and how would you approach it differently?&lt;/p&gt; &lt;p&gt;TL;DR: I have a bot with two versions of search: Shallow search for quick search results, and deep search, for in-depth, systematic, agentic approach to data gathering. Deep search may not be enough to really consider it &amp;quot;deep&amp;quot;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jner1g/what_is_deep_research_to_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jner1g/what_is_deep_research_to_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jner1g/what_is_deep_research_to_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T15:21:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmxdgg</id>
    <title>SplitQuantV2: Enhancing Low-Bit Quantization of LLMs Without GPUs</title>
    <updated>2025-03-29T21:58:54+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.07657"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmxdgg/splitquantv2_enhancing_lowbit_quantization_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmxdgg/splitquantv2_enhancing_lowbit_quantization_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T21:58:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn9206</id>
    <title>A good model to listen to me rant on niche topics?</title>
    <updated>2025-03-30T09:47:53+00:00</updated>
    <author>
      <name>/u/Mynameisjeff121</name>
      <uri>https://old.reddit.com/user/Mynameisjeff121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve had a good time with people‚Äôs suggestions in here when I was looking for models for different purposes, so I was hoping I could get help here again.&lt;/p&gt; &lt;p&gt;I‚Äôm looking for a model that‚Äôll hear me rant on niche video game/ fiction universes and ask questions about it. The few models I‚Äôve tested either derail too much or don‚Äôt really care about listening.&lt;/p&gt; &lt;p&gt;The searchbar on the huggingface site wasn‚Äôt that useful since models usually use tags on searches and I‚Äôm not that good on searching models. I‚Äôm kinda desperate now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mynameisjeff121"&gt; /u/Mynameisjeff121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn9206/a_good_model_to_listen_to_me_rant_on_niche_topics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn9206/a_good_model_to_listen_to_me_rant_on_niche_topics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn9206/a_good_model_to_listen_to_me_rant_on_niche_topics/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T09:47:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmpjeu</id>
    <title>SOTA 3d?</title>
    <updated>2025-03-29T16:03:22+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmpjeu/sota_3d/"&gt; &lt;img alt="SOTA 3d?" src="https://external-preview.redd.it/ErYaOL2J__P1a1nSZoN5VkFh-_pWwoLL-ogamC2v0BM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16974cbb9664a3f501eea6ca32995ed70308e190" title="SOTA 3d?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/VAST-AI/TripoSG"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmpjeu/sota_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmpjeu/sota_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T16:03:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnehr4</id>
    <title>When you prompt a non-thinking model to think, does it actually improve output?</title>
    <updated>2025-03-30T15:10:03+00:00</updated>
    <author>
      <name>/u/Kep0a</name>
      <uri>https://old.reddit.com/user/Kep0a</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For instance, Mistral 3 24b is not a reasoning model. However, when prompted correctly, I can have it generate &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; tags, and iteratively think through the problem.&lt;/p&gt; &lt;p&gt;In practice, I can get it to answer the &amp;quot;strawberry&amp;quot; test more often correctly, but I'm not sure if it's just due to actually thinking through the problem, or just because I asked it to &lt;strong&gt;think harder&lt;/strong&gt; that it just improves the chance of being correct. &lt;/p&gt; &lt;p&gt;Is this just mimicking reasoning, or actually helpful?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kep0a"&gt; /u/Kep0a &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnehr4/when_you_prompt_a_nonthinking_model_to_think_does/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnehr4/when_you_prompt_a_nonthinking_model_to_think_does/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnehr4/when_you_prompt_a_nonthinking_model_to_think_does/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T15:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmvsm3</id>
    <title>Local, GPU-Accelerated AI Characters with C#, ONNX &amp; Your LLM (Speech-to-Speech)</title>
    <updated>2025-03-29T20:44:56+00:00</updated>
    <author>
      <name>/u/fagenorn</name>
      <uri>https://old.reddit.com/user/fagenorn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing &lt;strong&gt;Persona Engine&lt;/strong&gt;, an open-source project I built for creating interactive AI characters. Think VTuber tech meets your local AI stack.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Voice Input:&lt;/strong&gt; Listens via mic (Whisper.net ASR).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Your LLM:&lt;/strong&gt; Connects to any &lt;strong&gt;OpenAI-compatible API&lt;/strong&gt; (perfect for Ollama, LM Studio, etc., via LiteLLM perhaps). Personality defined in personality.txt.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Voice Output:&lt;/strong&gt; Advanced TTS pipeline + optional &lt;strong&gt;Real-time Voice Cloning (RVC)&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live2D Avatar:&lt;/strong&gt; Animates your character.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Spout Output:&lt;/strong&gt; Direct feed to OBS/streaming software.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Tech Deep Dive:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Everything Runs Locally:&lt;/strong&gt; The ASR, TTS, RVC, and rendering are all done on your machine. Point it at your local LLM, and the whole loop stays offline.&lt;/li&gt; &lt;li&gt;C# &lt;strong&gt;Powered:&lt;/strong&gt; The entire engine is built in &lt;strong&gt;C# on .NET 9&lt;/strong&gt;. This involved rewriting a lot of common Python AI tooling/pipelines, but gives us great performance and lovely async/await patterns for managing all the concurrent tasks (listening, thinking, speaking, rendering).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ONNX Runtime Under the Hood:&lt;/strong&gt; I leverage ONNX for the AI models (Whisper, TTS components, RVC). &lt;strong&gt;Theoretically,&lt;/strong&gt; this means it could target different execution providers (DirectML for AMD/Intel, CoreML, CPU). &lt;strong&gt;However,&lt;/strong&gt; the current build and included dependencies are optimized and primarily tested for &lt;strong&gt;NVIDIA CUDA/cuDNN&lt;/strong&gt; for maximum performance, especially with RVC. Getting other backends working would require compiling/sourcing the appropriate ONNX Runtime builds and potentially some code adjustments.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross-Platform Potential:&lt;/strong&gt; Being C#/.NET means it could run on Linux/macOS, but you'd need to handle platform-specific native dependencies (like PortAudio, Spout alternatives e.g., Syphon) and compile things yourself. Windows is the main supported platform right now via the releases.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo (Code &amp;amp; Releases):&lt;/strong&gt; &lt;a href="https://github.com/fagenorn/handcrafted-persona-engine"&gt;https://github.com/fagenorn/handcrafted-persona-engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Short Demo Video:&lt;/strong&gt; &lt;a href="https://www.youtube.com/watch?v=4V2DgI7OtHE"&gt;https://www.youtube.com/watch?v=4V2DgI7OtHE&lt;/a&gt; (forgive the cheesiness, I was having a bit of fun with capcut)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick Heads-up:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For the pre-built releases: &lt;strong&gt;Requires NVIDIA GPU + correctly installed CUDA/cuDNN&lt;/strong&gt; for good performance. The README has a detailed guide for this.&lt;/li&gt; &lt;li&gt;Configure appsettings.json with your LLM endpoint/model.&lt;/li&gt; &lt;li&gt;Using standard LLMs? Grab personality_example.txt from the repo root as a starting point for personality.txt (requires prompt tuning!).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Excited to share this with a community that appreciates running things locally and diving into the tech! Let me know what you think or if you give it a spin. üòä&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fagenorn"&gt; /u/fagenorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmvsm3/local_gpuaccelerated_ai_characters_with_c_onnx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmvsm3/local_gpuaccelerated_ai_characters_with_c_onnx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmvsm3/local_gpuaccelerated_ai_characters_with_c_onnx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T20:44:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn10lx</id>
    <title>Gemini 2.5 Pro unusable for coding?</title>
    <updated>2025-03-30T00:56:48+00:00</updated>
    <author>
      <name>/u/hyperknot</name>
      <uri>https://old.reddit.com/user/hyperknot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Something really strange is going on with Gemini 2.5 Pro.&lt;/p&gt; &lt;p&gt;On one hand, it's supposedly the smartest coding model ever made. But on the other hand, I ask it to add one single parameter, and instead of a simple 2-line diff, it generates a 35-line one where it randomly changes logic, removes a time.sleep() from an API call pagination loop, and is generally just totally &amp;quot;drunk&amp;quot; about what I asked it to do. It's somehow both pedantic and drunk at the same time.&lt;/p&gt; &lt;p&gt;Every other model, even much smaller ones, can easily make the 2-line change and leave everything else alone.&lt;/p&gt; &lt;p&gt;I'm wondering how this thing beat the Aider leaderboard. Did something change since the launch?&lt;/p&gt; &lt;p&gt;Setting temp to 0.0 doesn't help either.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hyperknot"&gt; /u/hyperknot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn10lx/gemini_25_pro_unusable_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn10lx/gemini_25_pro_unusable_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn10lx/gemini_25_pro_unusable_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T00:56:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmqqxz</id>
    <title>First time testing: Qwen2.5:72b -&gt; Ollama Mac + open-webUI -&gt; M3 Ultra 512 gb</title>
    <updated>2025-03-29T16:57:54+00:00</updated>
    <author>
      <name>/u/Turbulent_Pin7635</name>
      <uri>https://old.reddit.com/user/Turbulent_Pin7635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqqxz/first_time_testing_qwen2572b_ollama_mac_openwebui/"&gt; &lt;img alt="First time testing: Qwen2.5:72b -&amp;gt; Ollama Mac + open-webUI -&amp;gt; M3 Ultra 512 gb" src="https://b.thumbs.redditmedia.com/GHJGnHixtYfi5hcwQIzYQveJXry9-u0b_5OgRRmDegc.jpg" title="First time testing: Qwen2.5:72b -&amp;gt; Ollama Mac + open-webUI -&amp;gt; M3 Ultra 512 gb" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First time using it. Tested with the qwen2.5:72b, I add in the gallery the results of the first run. I would appreciate any comment that could help me to improve it. I also, want to thanks the community for the patience answering some doubts I had before buying this machine. I'm just beginning. &lt;/p&gt; &lt;p&gt;Doggo is just a plus!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent_Pin7635"&gt; /u/Turbulent_Pin7635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jmqqxz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqqxz/first_time_testing_qwen2572b_ollama_mac_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqqxz/first_time_testing_qwen2572b_ollama_mac_openwebui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T16:57:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jndrns</id>
    <title>How do you integrate your LLM machine into the rest of your Homelab? Does it make sense to connect your LLM server to Kubernetes?</title>
    <updated>2025-03-30T14:35:52+00:00</updated>
    <author>
      <name>/u/Deep_Area_3790</name>
      <uri>https://old.reddit.com/user/Deep_Area_3790</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering if it does make sense to connect your LLM server to the rest of your homelab/kubernetes cluster and i am curious about how everyone here does it.&lt;/p&gt; &lt;p&gt;Do you run an hypervisor like proxmox or just an baremetal OS to dedicate the entire performance just to the LLM? &lt;/p&gt; &lt;p&gt;If you've got just one dedicated machine just for your LLM server, does the scheduling/orchestration part of Kubernetes actually provide any benefit? There is nowhere for the LLM server to reschedule and running directly on teh OS seems simpler.&lt;/p&gt; &lt;p&gt;For those of you using Kubernetes, I'm assuming you create taints to keep other apps from scheduling on your LLM node and potentially impacting performance, right?&lt;/p&gt; &lt;p&gt;Would Kubernetes still make sense just for easier integration into the already existing logging and monitoring stack, maybe ingress for the LLM API etc.?&lt;/p&gt; &lt;p&gt;How are you all handling this in your homelab?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deep_Area_3790"&gt; /u/Deep_Area_3790 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jndrns/how_do_you_integrate_your_llm_machine_into_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jndrns/how_do_you_integrate_your_llm_machine_into_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jndrns/how_do_you_integrate_your_llm_machine_into_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T14:35:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jndtf9</id>
    <title>Assessing facial recognition performance of vision-LLMs</title>
    <updated>2025-03-30T14:38:13+00:00</updated>
    <author>
      <name>/u/jordo45</name>
      <uri>https://old.reddit.com/user/jordo45</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jndtf9/assessing_facial_recognition_performance_of/"&gt; &lt;img alt="Assessing facial recognition performance of vision-LLMs" src="https://external-preview.redd.it/Q9B_F_rENth3JFM_pC3cxUL1Sc3Cm_CC0V2ft3_ILkE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=98656574f88b317c643d15328db5d2353fbf6309" title="Assessing facial recognition performance of vision-LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I thought it'd be interesting to assess face recognition performance of vision LLMs. Even though it wouldn't be wise to use a vision LLM to do face rec when there are dedicated models, I'll note that:&lt;/p&gt; &lt;p&gt;- it gives us a way to measure the gap between dedicated vision models and LLM approaches, to assess how close we are to 'vision is solved'.&lt;/p&gt; &lt;p&gt;- lots of jurisdictions have regulations around face rec system, so it is important to know if vision LLMs are becoming capable face rec systems.&lt;/p&gt; &lt;p&gt;I measured performance of multiple models on multiple datasets (AgeDB30, LFW, CFP). As a baseline, I used arface-resnet-100. Note that as there are 24,000 pair of images, I did not benchmark the more costly commercial APIs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8w6p0c239ure1.png?width=5363&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78b50ba32c2534ee297306c696748818549047da"&gt;https://preview.redd.it/8w6p0c239ure1.png?width=5363&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78b50ba32c2534ee297306c696748818549047da&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Samples&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lpab59z59ure1.png?width=1275&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d4edb554c4b0f17c415cdb304dd48adee4b3cd28"&gt;https://preview.redd.it/lpab59z59ure1.png?width=1275&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d4edb554c4b0f17c415cdb304dd48adee4b3cd28&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;- Most vision LLMs are very far from even a several year old resnet-100. &lt;/p&gt; &lt;p&gt;- All models perform better than random chance.&lt;/p&gt; &lt;p&gt;- The google models (Gemini, Gemma) perform best.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/yhenon/llm-face-vision"&gt;Repo here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jordo45"&gt; /u/jordo45 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jndtf9/assessing_facial_recognition_performance_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jndtf9/assessing_facial_recognition_performance_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jndtf9/assessing_facial_recognition_performance_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T14:38:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmjq5h</id>
    <title>Finally someone's making a GPU with expandable memory!</title>
    <updated>2025-03-29T10:54:13+00:00</updated>
    <author>
      <name>/u/Normal-Ad-7114</name>
      <uri>https://old.reddit.com/user/Normal-Ad-7114</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a RISC-V gpu with SO-DIMM slots, so don't get your hopes up just yet, but it's &lt;em&gt;something&lt;/em&gt;!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.servethehome.com/bolt-graphics-zeus-the-new-gpu-architecture-with-up-to-2-25tb-of-memory-and-800gbe/2/"&gt;https://www.servethehome.com/bolt-graphics-zeus-the-new-gpu-architecture-with-up-to-2-25tb-of-memory-and-800gbe/2/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://bolt.graphics/"&gt;https://bolt.graphics/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Normal-Ad-7114"&gt; /u/Normal-Ad-7114 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmjq5h/finally_someones_making_a_gpu_with_expandable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmjq5h/finally_someones_making_a_gpu_with_expandable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmjq5h/finally_someones_making_a_gpu_with_expandable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T10:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jncig2</id>
    <title>Any alternatives to the new 4o Multi-Modal Image capabilities?</title>
    <updated>2025-03-30T13:34:31+00:00</updated>
    <author>
      <name>/u/janusr</name>
      <uri>https://old.reddit.com/user/janusr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The new 4o native image capabilities are quite impressing. Are there any open alternatives which allow similar native image input and output?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/janusr"&gt; /u/janusr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jncig2/any_alternatives_to_the_new_4o_multimodal_image/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jncig2/any_alternatives_to_the_new_4o_multimodal_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jncig2/any_alternatives_to_the_new_4o_multimodal_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T13:34:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnb3cl</id>
    <title>MacBook M3, 24GB ram. What's best for LLM engine?</title>
    <updated>2025-03-30T12:12:33+00:00</updated>
    <author>
      <name>/u/Familyinalicante</name>
      <uri>https://old.reddit.com/user/Familyinalicante</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like in title. I am in process of moving from windows laptop to MacBook Air M3, 24GB ram. I use it for local development in vscode and need to connect to local LLM. I've installed Ollama and it works but of course it's slower than my 3080ti16GB in windows laptop. It's not real problem because for my purpose I can leave laptop for hours to see result (that's the main reason for transition because windows laptop crash after an hour or so and worked loudly like steam engine). My question is if Ollama is fist class citizen in Apple or there's much better solution. I dont do any bleeding edge thing and use standard models like llama, Gemma, deepseek for my purpose. I used to Ollama and use it in such manner that all my projects connect to Ollama server on localhost. I know about LMstudio but didn't use it a lot as Ollama was sufficient. So, is Ollama ok or there much faster solutions, like 30% faster or more? Or there's a special configuration for Ollama in Apple beside installing it actually?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Familyinalicante"&gt; /u/Familyinalicante &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnb3cl/macbook_m3_24gb_ram_whats_best_for_llm_engine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnb3cl/macbook_m3_24gb_ram_whats_best_for_llm_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnb3cl/macbook_m3_24gb_ram_whats_best_for_llm_engine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T12:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmttah</id>
    <title>Seen a lot of setups but I had to laugh at this one. Price isn't terrible but with how it looks to be maintained I'd be worried about springing a leak.</title>
    <updated>2025-03-29T19:13:59+00:00</updated>
    <author>
      <name>/u/sleepy_roger</name>
      <uri>https://old.reddit.com/user/sleepy_roger</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sleepy_roger"&gt; /u/sleepy_roger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rvhj7wnchore1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmttah/seen_a_lot_of_setups_but_i_had_to_laugh_at_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmttah/seen_a_lot_of_setups_but_i_had_to_laugh_at_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T19:13:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmx0ih</id>
    <title>Someone created a highly optimized RDNA3 kernel that outperforms RocBlas by 60% on 7900XTX. How can I implement this and would it significantly benefit LLM inference?</title>
    <updated>2025-03-29T21:41:45+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmx0ih/someone_created_a_highly_optimized_rdna3_kernel/"&gt; &lt;img alt="Someone created a highly optimized RDNA3 kernel that outperforms RocBlas by 60% on 7900XTX. How can I implement this and would it significantly benefit LLM inference?" src="https://external-preview.redd.it/1DvBQgPBbFWMlcok52huGfBv7vgJ1oQojIIBOC8IpDA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d50ef4e8c8eec3ec397e0751d55c871986cab02e" title="Someone created a highly optimized RDNA3 kernel that outperforms RocBlas by 60% on 7900XTX. How can I implement this and would it significantly benefit LLM inference?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://seb-v.github.io/optimization/update/2025/01/20/Fast-GPU-Matrix-multiplication.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmx0ih/someone_created_a_highly_optimized_rdna3_kernel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmx0ih/someone_created_a_highly_optimized_rdna3_kernel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T21:41:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmyvpd</id>
    <title>Moondream 2025-03-27 Release</title>
    <updated>2025-03-29T23:10:50+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmyvpd/moondream_20250327_release/"&gt; &lt;img alt="Moondream 2025-03-27 Release" src="https://external-preview.redd.it/GtrXq5esaL1vBtb6j5XRN12_1xTaHr3DjPq8-x_uFDM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=149650fb9eeb4a3684aaac7092e35ed112f39db9" title="Moondream 2025-03-27 Release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://moondream.ai/blog/moondream-2025-03-27-release"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmyvpd/moondream_20250327_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmyvpd/moondream_20250327_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T23:10:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmtkgo</id>
    <title>4x3090</title>
    <updated>2025-03-29T19:02:48+00:00</updated>
    <author>
      <name>/u/zetan2600</name>
      <uri>https://old.reddit.com/user/zetan2600</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmtkgo/4x3090/"&gt; &lt;img alt="4x3090" src="https://preview.redd.it/zi8ghi2ifore1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1eaa2ef7723a30f4134fa44b42f76a17aa5ba357" title="4x3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is the only benefit of multiple GPUs concurrency of requests? I have 4x3090 but still seem limited to small models because it needs to fit in 24G vram. &lt;/p&gt; &lt;p&gt;AMD threadripper pro 5965wx 128 PCIe lanes ASUS ws pro wrx80 256G ddr4 3200 8 channels Primary PSU Corsair i1600 watt Secondary PSU 750watt 4 gigabyte 3090 turbos Phanteks Enthoo Pro II case Noctua industrial fans Artic cpu cooler&lt;/p&gt; &lt;p&gt;I am using vllm with tensor parallism of 4. I see all 4 cards loaded up and utilized evenly but doesn't seem any faster than 2 GPUs. &lt;/p&gt; &lt;p&gt;Currently using Qwen/Qwen2.5-14B-Instruct-AWQ with good success paired with Cline. &lt;/p&gt; &lt;p&gt;Will a nvlink bridge help? How can I run larger models? &lt;/p&gt; &lt;p&gt;14b seems really dumb compared to Anthropic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zetan2600"&gt; /u/zetan2600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zi8ghi2ifore1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmtkgo/4x3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmtkgo/4x3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T19:02:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnegrp</id>
    <title>3 new Llama models inside LMArena (maybe LLama 4?)</title>
    <updated>2025-03-30T15:08:49+00:00</updated>
    <author>
      <name>/u/Straight-Worker-4327</name>
      <uri>https://old.reddit.com/user/Straight-Worker-4327</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnegrp/3_new_llama_models_inside_lmarena_maybe_llama_4/"&gt; &lt;img alt="3 new Llama models inside LMArena (maybe LLama 4?)" src="https://b.thumbs.redditmedia.com/dn-wlWwvH94ULQ168bBoDHch1sjJK-d3SZVT2HvBWwc.jpg" title="3 new Llama models inside LMArena (maybe LLama 4?)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Straight-Worker-4327"&gt; /u/Straight-Worker-4327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jnegrp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnegrp/3_new_llama_models_inside_lmarena_maybe_llama_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnegrp/3_new_llama_models_inside_lmarena_maybe_llama_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T15:08:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn9klk</id>
    <title>This is the Reason why I am Still Debating whether to buy RTX5090!</title>
    <updated>2025-03-30T10:27:32+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn9klk/this_is_the_reason_why_i_am_still_debating/"&gt; &lt;img alt="This is the Reason why I am Still Debating whether to buy RTX5090!" src="https://a.thumbs.redditmedia.com/fsn9OVlHRAb11iT2p_HWV3Lsw8YzibhfHmPiywKjW70.jpg" title="This is the Reason why I am Still Debating whether to buy RTX5090!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/23fu4zuc0tre1.png?width=1299&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09b1c89cef3582073f35174e47b52ffef612ee11"&gt;https://preview.redd.it/23fu4zuc0tre1.png?width=1299&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09b1c89cef3582073f35174e47b52ffef612ee11&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn9klk/this_is_the_reason_why_i_am_still_debating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn9klk/this_is_the_reason_why_i_am_still_debating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn9klk/this_is_the_reason_why_i_am_still_debating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T10:27:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jndsj5</id>
    <title>We built a website where you can vote on Minecraft structures generated by AI</title>
    <updated>2025-03-30T14:37:03+00:00</updated>
    <author>
      <name>/u/civilunhinged</name>
      <uri>https://old.reddit.com/user/civilunhinged</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/civilunhinged"&gt; /u/civilunhinged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://mcbench.ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jndsj5/we_built_a_website_where_you_can_vote_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jndsj5/we_built_a_website_where_you_can_vote_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T14:37:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnbhdl</id>
    <title>I think I found llama 4 - the "cybele" model on lmarena. It's very, very good and revealed it name ‚ò∫Ô∏è</title>
    <updated>2025-03-30T12:36:19+00:00</updated>
    <author>
      <name>/u/Salty-Garage7777</name>
      <uri>https://old.reddit.com/user/Salty-Garage7777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have you had similar experience with this model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salty-Garage7777"&gt; /u/Salty-Garage7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnbhdl/i_think_i_found_llama_4_the_cybele_model_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnbhdl/i_think_i_found_llama_4_the_cybele_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnbhdl/i_think_i_found_llama_4_the_cybele_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T12:36:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnd6px</id>
    <title>LLMs over torrent</title>
    <updated>2025-03-30T14:08:12+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnd6px/llms_over_torrent/"&gt; &lt;img alt="LLMs over torrent" src="https://preview.redd.it/8z6t2vvu3ure1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ade8fa1e4ff10e2d71461fdb60f942583a4d442f" title="LLMs over torrent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Just messing around with an idea - serving LLM models over torrent. I‚Äôve uploaded Qwen2.5-VL-3B-Instruct to a seedbox sitting in a neutral datacenter in the Netherlands (hosted via Feralhosting).&lt;/p&gt; &lt;p&gt;If you wanna try it out, grab the torrent file here and load it up in any torrent client:&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="http://sbnb.astraeus.feralhosting.com/Qwen2.5-VL-3B-Instruct.torrent"&gt;http://sbnb.astraeus.feralhosting.com/Qwen2.5-VL-3B-Instruct.torrent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is just an experiment - no promises about uptime, speed, or anything really. It might work, it might not ü§∑&lt;/p&gt; &lt;p&gt;‚∏ª&lt;/p&gt; &lt;p&gt;Some random thoughts / open questions: 1. Only models with redistribution-friendly licenses (like Apache-2.0) can be shared this way. Qwen is cool, Mistral too. Stuff from Meta or Google gets more legally fuzzy - might need a lawyer to be sure. 2. If we actually wanted to host a big chunk of available models, we‚Äôd need a ton of seedboxes. Huggingface claims they store 45PB of data üòÖ üìé &lt;a href="https://huggingface.co/docs/hub/storage-backends"&gt;https://huggingface.co/docs/hub/storage-backends&lt;/a&gt; 3. Binary deduplication would help save space. Bonus points if we can do OTA-style patch updates to avoid re-downloading full models every time. 4. Why bother? AI‚Äôs getting more important, and putting everything in one place feels a bit risky long term. Torrents could be a good backup layer or alt-distribution method.&lt;/p&gt; &lt;p&gt;‚∏ª&lt;/p&gt; &lt;p&gt;Anyway, curious what people think. If you‚Äôve got ideas, feedback, or even some storage/bandwidth to spare, feel free to join the fun. Let‚Äôs see what breaks üòÑ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8z6t2vvu3ure1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnd6px/llms_over_torrent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnd6px/llms_over_torrent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T14:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnc9rd</id>
    <title>It's not much, but its honest work! 4xRTX 3060 running 70b at 4x4x4x4x</title>
    <updated>2025-03-30T13:21:39+00:00</updated>
    <author>
      <name>/u/madaerodog</name>
      <uri>https://old.reddit.com/user/madaerodog</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnc9rd/its_not_much_but_its_honest_work_4xrtx_3060/"&gt; &lt;img alt="It's not much, but its honest work! 4xRTX 3060 running 70b at 4x4x4x4x" src="https://b.thumbs.redditmedia.com/xZgN0CnCg9_dkwL0g3ohDgCJu3nIHgZs9DZKGJ0a-FQ.jpg" title="It's not much, but its honest work! 4xRTX 3060 running 70b at 4x4x4x4x" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madaerodog"&gt; /u/madaerodog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jnc9rd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnc9rd/its_not_much_but_its_honest_work_4xrtx_3060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnc9rd/its_not_much_but_its_honest_work_4xrtx_3060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T13:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn5uto</id>
    <title>MacBook M4 Max isn't great for LLMs</title>
    <updated>2025-03-30T05:42:51+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had M1 Max and recently upgraded to M4 Max - inferance speed difference is huge improvement (~3x) but it's still much slower than 5 years old RTX 3090 you can get for 700$ USD. &lt;/p&gt; &lt;p&gt;While it's nice to be able to load large models, they're just not gonna be very usable on that machine. An example - pretty small 14b distilled Qwen 4bit quant runs pretty slow for coding (40tps, with diff frequently failing so needs to redo whole file), and quality is very low. 32b is pretty unusable via Roo Code and Cline because of low speed.&lt;/p&gt; &lt;p&gt;And this is the best a money can buy you as Apple laptop.&lt;/p&gt; &lt;p&gt;Those are very pricey machines and I don't see any mentions that they aren't practical for local AI. You likely better off getting 1-2 generations old Nvidia rig if really need it, or renting, or just paying for API, as quality/speed will be day and night without upfront cost. &lt;/p&gt; &lt;p&gt;If you're getting MBP - save yourselves thousands $ and just get minimal ram you need with a bit extra SSD, and use more specialized hardware for local AI. &lt;/p&gt; &lt;p&gt;It's an awesome machine, all I'm saying - it prob won't deliver if you have high AI expectations for it. &lt;/p&gt; &lt;p&gt;PS: to me, this is not about getting or not getting a MacBook. I've been getting them for 15 years now and think they are awesome. The top models might not be quite the AI beast you were hoping for dropping these kinda $$$$, this is all I'm saying. I've had M1 Max with 64GB for years, and after the initial euphoria of holy smokes I can run large stuff there - never did it again for the reasons mentioned above. M4 is much faster but does feel similar in that sense. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T05:42:51+00:00</published>
  </entry>
</feed>
