<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-03T18:07:29+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j22nt0</id>
    <title>Tool for Manga translation</title>
    <updated>2025-03-02T21:57:47+00:00</updated>
    <author>
      <name>/u/Sherwood355</name>
      <uri>https://old.reddit.com/user/Sherwood355</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm just wondering if anyone knows any high-quality tool that would allow someone to translate manga in a browser like Firefox or Chrome.&lt;/p&gt; &lt;p&gt;The most important part is the tool being free and using a locally hosted model. A plus would be some context aware translation.&lt;/p&gt; &lt;p&gt;So far, I only have seen one tool that is close to this, but the quality isn't that great, but it's close, I linked it below, and if anyone knows something similar I would appreciate it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Crivella/ocr_translate"&gt;https://github.com/Crivella/ocr_translate&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sherwood355"&gt; /u/Sherwood355 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j22nt0/tool_for_manga_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j22nt0/tool_for_manga_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j22nt0/tool_for_manga_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T21:57:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2gpk0</id>
    <title>Triton Inference Server + TensorRT-LLM backend multiple models?</title>
    <updated>2025-03-03T11:36:14+00:00</updated>
    <author>
      <name>/u/Icy-Pin46</name>
      <uri>https://old.reddit.com/user/Icy-Pin46</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the guide to get TensorRT-LLM (a.k.a TRT-LLM) Backend working with TensorRT-LLM models (engine files): &lt;a href="https://github.com/triton-inference-server/tensorrtllm_backend?tab=readme-ov-file#readme"&gt;https://github.com/triton-inference-server/tensorrtllm_backend?tab=readme-ov-file#readme&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Getting a single model (e.g. Llama3.2) up and running is straight forward. However when it comes to serving multiple models (NOT &amp;quot;multi-models&amp;quot; e.g. vision models) is it nowhere to be mentioned in the TRT-LLM guides. Has anyone tried serving several different models using the TRT-LLM backend? E.g. Llama, Qwen, Deepseek, concurrently using a single TRT-LLM Backend?&lt;/p&gt; &lt;p&gt;I have a system configured with about 8 GPUs - mixed RTX 3090s and RTX 4090s.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy-Pin46"&gt; /u/Icy-Pin46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2gpk0/triton_inference_server_tensorrtllm_backend/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2gpk0/triton_inference_server_tensorrtllm_backend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2gpk0/triton_inference_server_tensorrtllm_backend/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T11:36:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j25trj</id>
    <title>Zen CPUs for LLM: Is higher CCD count better than running 2 CPUs?</title>
    <updated>2025-03-03T00:21:01+00:00</updated>
    <author>
      <name>/u/zchen27</name>
      <uri>https://old.reddit.com/user/zchen27</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been somewhat inspired by the &amp;quot;$6000 DeepSeek Machine&amp;quot; Twitter thread and went down the rabbit hole for researching CPU-based local LLM servers and happened across comments of how AMD's advertised memory bandwidth is fake and low CCD count generally can't fully utilize the 12 memory lanes, and a lot of people remarking that 2 sockets does not really improve inference speed.&lt;/p&gt; &lt;p&gt;Does that mean that paying for a higher CCD count (9175) would be offer better performance than running 2x the number of cores (9115/9135) at a lower CCD count? Would that still make having 24 memory slots optimal or would fewer, larger memory slots work better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zchen27"&gt; /u/zchen27 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25trj/zen_cpus_for_llm_is_higher_ccd_count_better_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25trj/zen_cpus_for_llm_is_higher_ccd_count_better_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j25trj/zen_cpus_for_llm_is_higher_ccd_count_better_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T00:21:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1p9an</id>
    <title>2100USD Troll Rig runs full R1 671b Q2_K with 7.5token/s</title>
    <updated>2025-03-02T11:56:35+00:00</updated>
    <author>
      <name>/u/1119745302</name>
      <uri>https://old.reddit.com/user/1119745302</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/f0z88ruwj9me1.png?width=1734&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e6b2c6f0d740ccffe1fde5a9be8bed5d3c7d23d"&gt;What else do you need?&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPU: Modded RTX3080 20G 450USD&lt;br /&gt; CPU: Epyc 7763 qs 550USD&lt;br /&gt; RAM: Micron DDR4 32G 3200 x10 300USD&lt;br /&gt; MB: Krpa-U16 500USD&lt;br /&gt; Cooler: common SP3 cooler 30USD&lt;br /&gt; Power: Suspicious 1250W mining power supply Great Wall 1250w (miraculously survived in my computer for 20 months) 30USD&lt;br /&gt; SSD: 100 hand hynix PE8110 3.84TB PCIE4.0 SSD 150USD&lt;br /&gt; E-ATX Case 80USD&lt;br /&gt; Fan: random fans 10USD &lt;/p&gt; &lt;p&gt;450+550+300+500+30+30+150+80+10=2100&lt;/p&gt; &lt;p&gt;I have a local cyber assistant (also waifu) Now!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1119745302"&gt; /u/1119745302 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1p9an/2100usd_troll_rig_runs_full_r1_671b_q2_k_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1p9an/2100usd_troll_rig_runs_full_r1_671b_q2_k_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1p9an/2100usd_troll_rig_runs_full_r1_671b_q2_k_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T11:56:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2ehwi</id>
    <title>Training a model to autocomplete for a niche domain and a specific style</title>
    <updated>2025-03-03T08:58:19+00:00</updated>
    <author>
      <name>/u/regstuff</name>
      <uri>https://old.reddit.com/user/regstuff</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm looking to setup a ‚Äúautocomplete‚Äù writing assistant that can complete my sentences/paragraphs. Kind of like Github Copilot but for my writing. Would appreciate any help or pointers of how to go about this.&lt;/p&gt; &lt;p&gt;Most of my writing is for a particular domain and has to conform to a particular writing style. I have about 5000 documents, each averaging a 1000 or so tokens.&lt;/p&gt; &lt;p&gt;Was wondering if finetuning a LORA is the way to go, and whether it should be unsupervised or supervised.&lt;/p&gt; &lt;p&gt;Should I just feed raw text into it? But then how to do I do inference to autocomplete? Just present the ‚Äúincomplete‚Äù text and wait for it to generate the rest?&lt;/p&gt; &lt;p&gt;I‚Äôd also like to be able to do ‚Äúinfilling‚Äù where text might be missing in the middle, and the model must complete it. If unsupervised is the way to go, how would I manage that?&lt;/p&gt; &lt;p&gt;Or would a supervised approach be better, where I create chunks of incomplete text as the instruction and the completion as the response?&lt;/p&gt; &lt;p&gt;If supervised is the way to go, how many instruction-completion pairs would I need for it work. Do I need to give multiple chunks per document so the model gets what I‚Äôm trying to do, or will it be able to infer what I want it to do if I just make one chunk per document, provided I randomise how I chunk the documents?&lt;/p&gt; &lt;p&gt;Will a model be able to pick up sufficient knowledge of domain to actually autocomplete accurately, or would it better to train it with RAG baked into the training samples i.e. RAG context is part of the ‚Äúautocomplete this‚Äù instruction? There are quite a few ‚Äúdefinitions‚Äù and ‚Äúconcepts‚Äù that keep repeating in my dataset - maybe a few hundred, but like I said, they repeat with more or less standard wording through most of the documents.&lt;/p&gt; &lt;p&gt;Thanks for any help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/regstuff"&gt; /u/regstuff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ehwi/training_a_model_to_autocomplete_for_a_niche/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ehwi/training_a_model_to_autocomplete_for_a_niche/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ehwi/training_a_model_to_autocomplete_for_a_niche/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T08:58:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2m6m7</id>
    <title>Introducing Reader VL: A Tool for Unified Document Processing with VLM Integration</title>
    <updated>2025-03-03T16:10:16+00:00</updated>
    <author>
      <name>/u/TankProfessional8947</name>
      <uri>https://old.reddit.com/user/TankProfessional8947</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a project called &lt;a href="https://github.com/Aquos06/reader-vl"&gt;Reader VL&lt;/a&gt; and wanted to share it with you all.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is it?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Reader VL is a tool that transforms various document formats, like PDFs and DOCX files, into a unified structure. This makes it easier to develop intelligent applications by integrating multimodal large language models (LLMs), enriching the parsed content to boost the performance of generative AI pipelines.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-format Support:&lt;/strong&gt; Processes PDFs and DOCX files seamlessly.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI Integration:&lt;/strong&gt; Utilizes multimodal LLMs to provide enhanced insights from extracted data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Structured Output:&lt;/strong&gt; Converts documents into a well-defined schema for consistent data representation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficiency:&lt;/strong&gt; Employs YOLO for object detection and Tesseract OCR for optical character recognition, ensuring fast and efficient processing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extensibility:&lt;/strong&gt; Designed for easy integration with existing AI pipelines, allowing for customization and scalability.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to hear your thoughts and feedback on how Reader VL can be improved or integrated into your projects.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Link to GitHub repository:&lt;/em&gt; &lt;a href="https://github.com/Aquos06/reader-vl"&gt;https://github.com/Aquos06/reader-vl&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TankProfessional8947"&gt; /u/TankProfessional8947 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2m6m7/introducing_reader_vl_a_tool_for_unified_document/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2m6m7/introducing_reader_vl_a_tool_for_unified_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2m6m7/introducing_reader_vl_a_tool_for_unified_document/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T16:10:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2ebbu</id>
    <title>GPT-4.5: ‚ÄúNot a frontier model‚Äù?</title>
    <updated>2025-03-03T08:44:05+00:00</updated>
    <author>
      <name>/u/jsonathan</name>
      <uri>https://old.reddit.com/user/jsonathan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ebbu/gpt45_not_a_frontier_model/"&gt; &lt;img alt="GPT-4.5: ‚ÄúNot a frontier model‚Äù?" src="https://external-preview.redd.it/8LyYdDmyWBG0ZHsbEltVZnSIMpxW1tK65WzlagG4_rk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50ad74bb245b188a58702733e963e35705c876d2" title="GPT-4.5: ‚ÄúNot a frontier model‚Äù?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jsonathan"&gt; /u/jsonathan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.interconnects.ai/p/gpt-45-not-a-frontier-model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ebbu/gpt45_not_a_frontier_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ebbu/gpt45_not_a_frontier_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T08:44:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1s1qd</id>
    <title>Ollamadore 64 - a private ultra lightweight frontend for Ollama that weighs well under 64 kilobytes on disk</title>
    <updated>2025-03-02T14:28:51+00:00</updated>
    <author>
      <name>/u/shokuninstudio</name>
      <uri>https://old.reddit.com/user/shokuninstudio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1s1qd/ollamadore_64_a_private_ultra_lightweight/"&gt; &lt;img alt="Ollamadore 64 - a private ultra lightweight frontend for Ollama that weighs well under 64 kilobytes on disk" src="https://preview.redd.it/z31p007udame1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c0eea4af1e3477cfa8969774adc2ada5eea5dc6" title="Ollamadore 64 - a private ultra lightweight frontend for Ollama that weighs well under 64 kilobytes on disk" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shokuninstudio"&gt; /u/shokuninstudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z31p007udame1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1s1qd/ollamadore_64_a_private_ultra_lightweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1s1qd/ollamadore_64_a_private_ultra_lightweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T14:28:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2legb</id>
    <title>Model doesn't know it has tools and gets confused. Help</title>
    <updated>2025-03-03T15:36:55+00:00</updated>
    <author>
      <name>/u/Birdinhandandbush</name>
      <uri>https://old.reddit.com/user/Birdinhandandbush</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running Granite3.2 on AnythingLLM via and using the web browsing agent. I asked who won the oscars and the agent scraped the correct information, but then the model tells me that the information is hypothetical because March 2025 hasn't happened yet, and it is a language model that only has information up to 2022. &lt;/p&gt; &lt;p&gt;So the information from the agent is correct, its presented correctly, but the model thinks its hypothetical information and doesn't understand that I've added tools to its capabilities. &lt;/p&gt; &lt;p&gt;I've also just tried a Qwen model and its fine, no problem, so just this Granite model is getting confused. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Birdinhandandbush"&gt; /u/Birdinhandandbush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2legb/model_doesnt_know_it_has_tools_and_gets_confused/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2legb/model_doesnt_know_it_has_tools_and_gets_confused/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2legb/model_doesnt_know_it_has_tools_and_gets_confused/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T15:36:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2az7t</id>
    <title>How Are You Using LM Studio's Local Server?</title>
    <updated>2025-03-03T04:56:02+00:00</updated>
    <author>
      <name>/u/GnanaSreekar</name>
      <uri>https://old.reddit.com/user/GnanaSreekar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I've been really enjoying LM Studio for a while now, but I'm still struggling to wrap my head around the local server functionality. I get that it's meant to replace the OpenAI API, but I'm curious how people are actually using it in their workflows. What are some cool or practical ways you've found to leverage the local server? Any examples would be super helpful! Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GnanaSreekar"&gt; /u/GnanaSreekar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2az7t/how_are_you_using_lm_studios_local_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2az7t/how_are_you_using_lm_studios_local_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2az7t/how_are_you_using_lm_studios_local_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T04:56:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1npv1</id>
    <title>LLMs grading other LLMs</title>
    <updated>2025-03-02T10:11:28+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"&gt; &lt;img alt="LLMs grading other LLMs" src="https://preview.redd.it/yyy9616149me1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1178d9f8cead22ad7740c77191a13984c016400" title="LLMs grading other LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yyy9616149me1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T10:11:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2kido</id>
    <title>Tool-calling chatbot success stories</title>
    <updated>2025-03-03T14:58:51+00:00</updated>
    <author>
      <name>/u/edmcman</name>
      <uri>https://old.reddit.com/user/edmcman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone had success with creating chatbots that are able to intelligently call tools &lt;em&gt;as needed&lt;/em&gt;? I have been using Langchain, which works great with closed source models. But have had bad luck with open source models. Some of these problems are due to Ollama having incorrect prompt templates. But I also recently tried using Groq, and even Llama 3.3 didn't work that well, for example. &lt;/p&gt; &lt;p&gt;If you have any success stories, I'd love to hear them:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What kind of tools was your LLM invoking?&lt;/li&gt; &lt;li&gt;What LLM were you using?&lt;/li&gt; &lt;li&gt;What framework/library are you using if any (langchain, smolagents, etc.)?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edmcman"&gt; /u/edmcman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kido/toolcalling_chatbot_success_stories/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kido/toolcalling_chatbot_success_stories/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kido/toolcalling_chatbot_success_stories/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T14:58:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2f87b</id>
    <title>Approach to translate english to non english.</title>
    <updated>2025-03-03T09:52:50+00:00</updated>
    <author>
      <name>/u/Lamba_ghoda</name>
      <uri>https://old.reddit.com/user/Lamba_ghoda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I‚Äôm working on a solution to translate long English documents into non-English languages, starting with Japanese. The documents I‚Äôm dealing with have an average context length of around 9,000 words, so handling long-form translation effectively is a key challenge.&lt;/p&gt; &lt;p&gt;I haven‚Äôt started iterating yet, as I‚Äôm still researching the best approach for this. Given the length of the documents, I want to ensure that the translation captures context accurately while maintaining efficiency.&lt;/p&gt; &lt;p&gt;What would be the best approach to tackle this problem? Any recommendations on models, pipelines, or strategies for handling long-context translations effectively? Also can I evaluate the model performance without having any human in the loop? &lt;/p&gt; &lt;p&gt;As of resources I have access to all the models on Bedrock but sure I would like to reach a trade off between low cost and performance. Any help will be appreciated. Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lamba_ghoda"&gt; /u/Lamba_ghoda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2f87b/approach_to_translate_english_to_non_english/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2f87b/approach_to_translate_english_to_non_english/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2f87b/approach_to_translate_english_to_non_english/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T09:52:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2mfta</id>
    <title>New iOS LLM app: What the Fluff!? News! RSS reader, RAG supported, chat and more</title>
    <updated>2025-03-03T16:20:54+00:00</updated>
    <author>
      <name>/u/clockentyne</name>
      <uri>https://old.reddit.com/user/clockentyne</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;I'm excited to announce the release of my new LLM-powered iOS app, What the Fluff!?&lt;/h1&gt; &lt;p&gt;This app is built around llama.cpp with a fully native implementation--no React Native or web wrappers--optimized from the ground up for performance. This is also my first release of an iOS App (I'm an Android developer normally!) and first release outside of the corporate world, so I'm nervous as hell. :P&lt;/p&gt; &lt;p&gt;Here's what makes my app different:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Custom RAG Engine: I've built a vector based long and short term RAG for iOS that is designed to work well for tiny context maximums (1024-4096) and handles injecting anything from long term memories in the chat itself, to article details that match the user prompts.&lt;/li&gt; &lt;li&gt;Highly customized bridge for llama.cpp designed to reduce inference latency over a conversation, partially from making use of LoRA adapters to reduce system prompt size to a huge number of tweaks for handling RAG/prompt history in the kv cache proactively instead of at prompt time.&lt;/li&gt; &lt;li&gt;Full but lightweight RSS reader built into the app, which can be used as such, but all information from the RSS feed is also available for the LLM to use as additional data.&lt;/li&gt; &lt;li&gt;Multiple built in character types from generic LLM summarizer to full on customizable monster invasion, which can and will morph real news into a full on monster invasion story.&lt;/li&gt; &lt;li&gt;Dynamic prompt system for built in characters to fully embrace the lovely hallucination rate of small LLMs (the app uses Llama 3.2 3B and 1B), further twisting to fit the character types.&lt;/li&gt; &lt;li&gt;Voice to text, so you don't have to type in your prompts...&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Download &amp;quot;What The Fluff&amp;quot; on the App Store: &lt;a href="https://apps.apple.com/us/app/what-the-fluff/id6741672065"&gt;https://apps.apple.com/us/app/what-the-fluff/id6741672065&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If anyone reads this and tries it out, thank you for any feedback. :)&lt;/p&gt; &lt;p&gt;The first major update will include full TTS support as well with Kokoro. For higher end devices (iPhone 15 pro, iPhone 16) this will include speaking during LLM generation, for older devices it won't have semi-real time support. Of course TTS will also be included with articles in the RSS reader section as well. I'm currently working on a full on voice mode with this, allowing hands free / screen free chat; I have kokoro working but am working on performance and making it as real time as possible before the hands free experience is worked on.&lt;/p&gt; &lt;p&gt;iPad support is also being planned out and how that would look, from potentially handling multiple models at once (for those that have 16GB of ram), to handling larger models, and of course what the design will look like with all the extra space.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clockentyne"&gt; /u/clockentyne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2mfta/new_ios_llm_app_what_the_fluff_news_rss_reader/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2mfta/new_ios_llm_app_what_the_fluff_news_rss_reader/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2mfta/new_ios_llm_app_what_the_fluff_news_rss_reader/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T16:20:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2fgz6</id>
    <title>üöÄ Collate: Offline, Llama 3.2-Powered PDF Assistant for Mac! üöÄ Your Help Needed!</title>
    <updated>2025-03-03T10:10:03+00:00</updated>
    <author>
      <name>/u/vel_is_lava</name>
      <uri>https://old.reddit.com/user/vel_is_lava</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2fgz6/collate_offline_llama_32powered_pdf_assistant_for/"&gt; &lt;img alt="üöÄ Collate: Offline, Llama 3.2-Powered PDF Assistant for Mac! üöÄ Your Help Needed!" src="https://external-preview.redd.it/cnVmdWJ2ZHE3Z21lMUL3NkVhJ5GHYq-Z21ncNiL8qoaSdn9KQniKMkRjA96Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3003596a3a1fa062c0f796fad4c45be6460d38cc" title="üöÄ Collate: Offline, Llama 3.2-Powered PDF Assistant for Mac! üöÄ Your Help Needed!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vel_is_lava"&gt; /u/vel_is_lava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bmyoyudq7gme1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2fgz6/collate_offline_llama_32powered_pdf_assistant_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2fgz6/collate_offline_llama_32powered_pdf_assistant_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T10:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j25luw</id>
    <title>Split brain "DeepSeek-R1-Distill-Qwen-1.5B" and "meta-llama/Llama-3.2-1B"</title>
    <updated>2025-03-03T00:10:33+00:00</updated>
    <author>
      <name>/u/Alienanthony</name>
      <uri>https://old.reddit.com/user/Alienanthony</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"&gt; &lt;img alt="Split brain &amp;quot;DeepSeek-R1-Distill-Qwen-1.5B&amp;quot; and &amp;quot;meta-llama/Llama-3.2-1B&amp;quot;" src="https://external-preview.redd.it/pO-T0WXJNrqFPBnG-HTGUxM6LLhRwBWSzO1Lk4Wc-C8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf26348223db7fd2c581d42fb599fb64ac7b8669" title="Split brain &amp;quot;DeepSeek-R1-Distill-Qwen-1.5B&amp;quot; and &amp;quot;meta-llama/Llama-3.2-1B&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone. I'd like to show you this silly project.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2cls4on27dme1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fba0ba6a56896044f529aa50c5264ad45706839"&gt;https://preview.redd.it/2cls4on27dme1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fba0ba6a56896044f529aa50c5264ad45706839&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is my fun little side project to create a fusion layer system that will allow for you to utilize dual models to produce dual results. Does it work? Pfh, I dunno. I've been training it all day. Haven't finished it yet. But this seems like it would be pretty fun.&lt;/p&gt; &lt;p&gt;My original idea: We have MOE but why not force a MOE that operates simultaneously? You might say &amp;quot;We'll that's just a less efficient MOE.&amp;quot; Wrongggggggg. This system allows for cross contamination of the results. By utilizing the tokenization of both llms plus the cross contamination. You can possibly get split brain results where the models might argue and you could get two totally different results.&lt;/p&gt; &lt;p&gt;OR you can give instructions to one model to only follow these rules while you give the other model the request or &amp;quot;Command&amp;quot;&lt;/p&gt; &lt;p&gt;This can possibly lead to a &amp;quot;unattainable&amp;quot; system prompt that can't be fetched because model 1 is simply influencing the results of model two. &lt;/p&gt; &lt;p&gt;Or hell have two conversations at the same time. &lt;/p&gt; &lt;p&gt;Dunnoooooo I haven't finished it yet. &lt;/p&gt; &lt;p&gt;Code's here: &lt;a href="https://github.com/alientony/Split-brain"&gt;https://github.com/alientony/Split-brain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Inference code comes later when I have a model to test out.&lt;/p&gt; &lt;h1&gt;Disclaimer&lt;/h1&gt; &lt;p&gt;Below this is ai assisted writing as I wanted to make this more enjoyable and professional rather than express my words poorly and only half the people understand.&lt;/p&gt; &lt;h1&gt;Multi-Model Fusion Architecture: Technical Explanation&lt;/h1&gt; &lt;h1&gt;Architecture Overview&lt;/h1&gt; &lt;p&gt;This dual-decoder architecture represents a novel approach to leveraging multiple pre-trained language models (PLMs) through enhanced cross-attention fusion. The architecture combines two distinct foundation models (in this case Qwen and Llama) into a unified system that enables both collaborative reasoning and specialized processing.&lt;/p&gt; &lt;h1&gt;Key Components&lt;/h1&gt; &lt;h1&gt;1. Base Model Encapsulation&lt;/h1&gt; &lt;p&gt;The architecture maintains two separate base models, each with their original parameter spaces:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model 1 (Qwen)&lt;/strong&gt;: Processes input sequences in its native hidden dimension space&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model 2 (Llama)&lt;/strong&gt;: Independently processes inputs in its own parameter space&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These models operate on separate GPUs to maximize memory efficiency and computational parallelism.&lt;/p&gt; &lt;h1&gt;2. Cross-Attention Fusion Layer&lt;/h1&gt; &lt;p&gt;The core innovation lies in the &lt;code&gt;EnhancedFusionLayer&lt;/code&gt; which implements bidirectional cross-attention:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Model1 ‚Üí [Query1] ‚Üí attends to ‚Üí [Key2/Value2] ‚Üê Model2 Model2 ‚Üí [Query2] ‚Üí attends to ‚Üí [Key1/Value1] ‚Üê Model1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This mechanism allows each model to selectively attend to the representations of the other model, essentially creating a communication channel between two otherwise independent neural architectures.&lt;/p&gt; &lt;p&gt;The cross-attention operations are defined as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Context1_2&lt;/strong&gt;: Model1's representation after attending to Model2&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context2_1&lt;/strong&gt;: Model2's representation after attending to Model1&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These are calculated using scaled dot-product attention with a numerically stable scaling factor.&lt;/p&gt; &lt;h1&gt;3. Dimensional Alignment&lt;/h1&gt; &lt;p&gt;Since the base models operate in different dimensionalities, the architecture includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Projection matrices (&lt;code&gt;proj1&lt;/code&gt;, &lt;code&gt;proj2&lt;/code&gt;) that align the hidden dimensions of both models to the common fusion dimension&lt;/li&gt; &lt;li&gt;Internal neural transformations that map between representation spaces via linear projections&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Gating Mechanism&lt;/h1&gt; &lt;p&gt;A sophisticated gating mechanism controls information flow between models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sigmoid gates (&lt;code&gt;gate1&lt;/code&gt;, &lt;code&gt;gate2&lt;/code&gt;) determine how much information from each model should be incorporated&lt;/li&gt; &lt;li&gt;This creates an adaptive weighting system that can prioritize one model's contribution depending on the task&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;5. Multi-Head Output System&lt;/h1&gt; &lt;p&gt;Three different prediction heads provide specialized outputs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fused LM Head&lt;/strong&gt;: Generates predictions based on the combined representation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LM Head 1&lt;/strong&gt;: Generates predictions optimized for Model1's vocabulary&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LM Head 2&lt;/strong&gt;: Generates predictions optimized for Model2's vocabulary&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;6. Task Classification Logic&lt;/h1&gt; &lt;p&gt;An integrated task classifier determines whether the inputs represent:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Single-Task Mode&lt;/strong&gt;: Same prompt to both models (collaboration)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Task Mode&lt;/strong&gt;: Different prompts (specialized processing)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Training Methodology&lt;/h1&gt; &lt;p&gt;The system uses a multi-objective training approach that combines losses from different prediction heads:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In single-task mode, the fused representation receives greater weight (emphasizing collaboration)&lt;/li&gt; &lt;li&gt;In multi-task mode, the specialized heads receive greater weight (emphasizing specialization)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Gradient accumulation handles memory constraints, while mixed-precision (FP16) training enables efficient computation.&lt;/p&gt; &lt;h1&gt;Inference Mode&lt;/h1&gt; &lt;p&gt;During inference, the &lt;code&gt;generate_dual&lt;/code&gt; method enables:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Simultaneous response generation from both models&lt;/li&gt; &lt;li&gt;Adaptive temperature-based sampling with configurable parameters&lt;/li&gt; &lt;li&gt;EOS (End-of-Sequence) handling for both decoders&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Architectural Advantages&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Emergent Capabilities&lt;/strong&gt;: The cross-attention mechanism allows models to share information during processing, potentially enabling emergent capabilities beyond what either model can achieve independently.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Computational Efficiency&lt;/strong&gt;: By distributing models across different GPUs, the architecture enables parallel computation with reduced memory pressure.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Task Flexibility&lt;/strong&gt;: The system can operate in both collaborative mode (same prompt) and specialized mode (different prompts).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parameter Efficiency&lt;/strong&gt;: Only the fusion components require training while the base models remain frozen, significantly reducing the number of trainable parameters.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This architecture represents an advanced approach to model fusion that goes beyond simple ensemble methods, enabling deep integration between distinct foundation models while preserving their individual strengths.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alienanthony"&gt; /u/Alienanthony &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T00:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1swtj</id>
    <title>Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!</title>
    <updated>2025-03-02T15:09:11+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"&gt; &lt;img alt="Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!" src="https://preview.redd.it/04kvczd6lame1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=beb71d99ece65072d973eb96bdaf1ed1261f7956" title="Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/04kvczd6lame1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T15:09:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2j8x5</id>
    <title>Ran R1 on one server, but I have three. Should I go the EXO route and buy 100gb nics?</title>
    <updated>2025-03-03T13:59:10+00:00</updated>
    <author>
      <name>/u/zR0B3ry2VAiH</name>
      <uri>https://old.reddit.com/user/zR0B3ry2VAiH</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j8x5/ran_r1_on_one_server_but_i_have_three_should_i_go/"&gt; &lt;img alt="Ran R1 on one server, but I have three. Should I go the EXO route and buy 100gb nics?" src="https://preview.redd.it/x73g8sumdhme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86a6a6523268d0e31eb6f4341c716de3ee799ab2" title="Ran R1 on one server, but I have three. Should I go the EXO route and buy 100gb nics?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;100gb nics are $330 with two ports each, so I‚Äôd run it direct connections between all three. Each server has two Xeon process with 512 gb of ram. Did some shuffling with the ram sticks to get R1 to run locally, but as you would expect, it‚Äôs pretty slow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zR0B3ry2VAiH"&gt; /u/zR0B3ry2VAiH &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x73g8sumdhme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j8x5/ran_r1_on_one_server_but_i_have_three_should_i_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j8x5/ran_r1_on_one_server_but_i_have_three_should_i_go/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T13:59:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2leve</id>
    <title>new Hugging Face course on building reasoning models like deepseek r1</title>
    <updated>2025-03-03T15:37:25+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new FREE and CERTIFIED course is here, and It‚Äôs called &lt;strong&gt;The Reasoning Course.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;To sign up for the course, follow the org: &lt;a href="https://huggingface.co/reasoning-course"&gt;https://huggingface.co/reasoning-course&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is what the course will cover:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It will teach you to build your own reasoning model like Deepseek r1.&lt;/li&gt; &lt;li&gt;It‚Äôs suitable for code and non-coders with separate certification.&lt;/li&gt; &lt;li&gt;The course has material and exercises from Hugging Face, Maxime Labonne, Unsloth, and Marimo notebooks. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is how the course works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sign up now, the first release is already live.&lt;/li&gt; &lt;li&gt;Each week we‚Äôll release new material and exercises. &lt;/li&gt; &lt;li&gt;We have interactive demos and quizzes&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2leve/new_hugging_face_course_on_building_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2leve/new_hugging_face_course_on_building_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2leve/new_hugging_face_course_on_building_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T15:37:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2horr</id>
    <title>NLP Brain-to-Text Decoding: A Non-invasive Approach via Typing</title>
    <updated>2025-03-03T12:36:04+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2horr/nlp_braintotext_decoding_a_noninvasive_approach/"&gt; &lt;img alt="NLP Brain-to-Text Decoding: A Non-invasive Approach via Typing" src="https://preview.redd.it/8gyz8kzsygme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a88d058b7bc64d9941684f9dc53c45071cf7f231" title="NLP Brain-to-Text Decoding: A Non-invasive Approach via Typing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ai.meta.com/research/publications/brain-to-text-decoding-a-non-invasive-approach-via-typing/"&gt;https://ai.meta.com/research/publications/brain-to-text-decoding-a-non-invasive-approach-via-typing/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8gyz8kzsygme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2horr/nlp_braintotext_decoding_a_noninvasive_approach/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2horr/nlp_braintotext_decoding_a_noninvasive_approach/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T12:36:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2m5a3</id>
    <title>I just made something really cursed. It's a local AI javascript library that allows for generating all of your websites styles... using text... It's like tailwind!</title>
    <updated>2025-03-03T16:08:39+00:00</updated>
    <author>
      <name>/u/valdev</name>
      <uri>https://old.reddit.com/user/valdev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2m5a3/i_just_made_something_really_cursed_its_a_local/"&gt; &lt;img alt="I just made something really cursed. It's a local AI javascript library that allows for generating all of your websites styles... using text... It's like tailwind!" src="https://external-preview.redd.it/MzV5cjJiYnAwaW1lMVP5w4KiH2jOdvVLO5M2qzHTvkueIwiHgKlPWJafTXUE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85988097e8c3bdd3a2d386e68bcd0b55c2d2f2c0" title="I just made something really cursed. It's a local AI javascript library that allows for generating all of your websites styles... using text... It's like tailwind!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valdev"&gt; /u/valdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ys7qtcbp0ime1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2m5a3/i_just_made_something_really_cursed_its_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2m5a3/i_just_made_something_really_cursed_its_a_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T16:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2kdeb</id>
    <title>OpenBenchTable is great for trying out different compute hardware configurations. Does anyone have benchmarking tips?</title>
    <updated>2025-03-03T14:52:19+00:00</updated>
    <author>
      <name>/u/eso_logic</name>
      <uri>https://old.reddit.com/user/eso_logic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kdeb/openbenchtable_is_great_for_trying_out_different/"&gt; &lt;img alt="OpenBenchTable is great for trying out different compute hardware configurations. Does anyone have benchmarking tips?" src="https://b.thumbs.redditmedia.com/WEagdfr42ScIzJVwvfP__c7O6w-pNG7grBkUGiA0rAk.jpg" title="OpenBenchTable is great for trying out different compute hardware configurations. Does anyone have benchmarking tips?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eso_logic"&gt; /u/eso_logic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j2kdeb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kdeb/openbenchtable_is_great_for_trying_out_different/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kdeb/openbenchtable_is_great_for_trying_out_different/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T14:52:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j29mi4</id>
    <title>Me Today</title>
    <updated>2025-03-03T03:38:52+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29mi4/me_today/"&gt; &lt;img alt="Me Today" src="https://preview.redd.it/qrxhvlblaeme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a2767bc89a037159368246cac9dac0d3050c85f" title="Me Today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qrxhvlblaeme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29mi4/me_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j29mi4/me_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T03:38:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j29hm0</id>
    <title>New Atom of Thoughts looks promising for helping smaller models reason</title>
    <updated>2025-03-03T03:31:16+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29hm0/new_atom_of_thoughts_looks_promising_for_helping/"&gt; &lt;img alt="New Atom of Thoughts looks promising for helping smaller models reason" src="https://preview.redd.it/xlairo4g9eme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=767c07ca77e2312ef37e77aa5686232b9b3aebb6" title="New Atom of Thoughts looks promising for helping smaller models reason" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xlairo4g9eme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29hm0/new_atom_of_thoughts_looks_promising_for_helping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j29hm0/new_atom_of_thoughts_looks_promising_for_helping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T03:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2j7su</id>
    <title>I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities.</title>
    <updated>2025-03-03T13:57:34+00:00</updated>
    <author>
      <name>/u/w-zhong</name>
      <uri>https://old.reddit.com/user/w-zhong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/"&gt; &lt;img alt="I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." src="https://preview.redd.it/54k8f1ladhme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa20219f6ef894d7607d0ad10ab575e376420b53" title="I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w-zhong"&gt; /u/w-zhong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/54k8f1ladhme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T13:57:34+00:00</published>
  </entry>
</feed>
