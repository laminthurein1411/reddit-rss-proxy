<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-22T13:34:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jh5pxa</id>
    <title>Replacing sqlite with postgres in Open WebUI</title>
    <updated>2025-03-22T10:44:32+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have any of you switched from the default sqlite backend to postgres for Open WebUI? Did you notice any benefits. I already have a postgres DB for other things so wondered if it made sense to migrate (that way I can just backup the database and not worry about Open WebUI separately).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh5pxa/replacing_sqlite_with_postgres_in_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh5pxa/replacing_sqlite_with_postgres_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh5pxa/replacing_sqlite_with_postgres_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T10:44:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgmavv</id>
    <title>Llama 3.3 Nemotron 49B Super appears on LMSYS Arena</title>
    <updated>2025-03-21T17:25:23+00:00</updated>
    <author>
      <name>/u/jpydych</name>
      <uri>https://old.reddit.com/user/jpydych</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgmavv/llama_33_nemotron_49b_super_appears_on_lmsys_arena/"&gt; &lt;img alt="Llama 3.3 Nemotron 49B Super appears on LMSYS Arena" src="https://preview.redd.it/v330lkoru2qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e0578ce6d6bf5ebdcb8c012e8e2f02052435408" title="Llama 3.3 Nemotron 49B Super appears on LMSYS Arena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jpydych"&gt; /u/jpydych &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v330lkoru2qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgmavv/llama_33_nemotron_49b_super_appears_on_lmsys_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgmavv/llama_33_nemotron_49b_super_appears_on_lmsys_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T17:25:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgkqio</id>
    <title>New BitNet Model from Deepgrove</title>
    <updated>2025-03-21T16:20:41+00:00</updated>
    <author>
      <name>/u/Jake-Boggs</name>
      <uri>https://old.reddit.com/user/Jake-Boggs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgkqio/new_bitnet_model_from_deepgrove/"&gt; &lt;img alt="New BitNet Model from Deepgrove" src="https://external-preview.redd.it/75Zv26Tb9ec8ndEGYBOYu42vtVHBipVRmB1cGkts4ZM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd92282c47c9015d2d9452a76f2cbf3d52257025" title="New BitNet Model from Deepgrove" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jake-Boggs"&gt; /u/Jake-Boggs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/deepgrove-ai/Bonsai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgkqio/new_bitnet_model_from_deepgrove/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgkqio/new_bitnet_model_from_deepgrove/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T16:20:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgft94</id>
    <title>ByteDance released on HuggingFace an open image model that generates Photo While Preserving Your Identity</title>
    <updated>2025-03-21T12:37:24+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgft94/bytedance_released_on_huggingface_an_open_image/"&gt; &lt;img alt="ByteDance released on HuggingFace an open image model that generates Photo While Preserving Your Identity" src="https://preview.redd.it/efejft8gf1qe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e80345d3319d8c121635235946e7b1d2c0eb17a6" title="ByteDance released on HuggingFace an open image model that generates Photo While Preserving Your Identity" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Flexible Photo Recrafting While Preserving Your Identity&lt;/p&gt; &lt;p&gt;Project page: &lt;a href="https://bytedance.github.io/InfiniteYou/"&gt;https://bytedance.github.io/InfiniteYou/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/bytedance/InfiniteYou"&gt;https://github.com/bytedance/InfiniteYou&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/ByteDance/InfiniteYou"&gt;https://huggingface.co/ByteDance/InfiniteYou&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/efejft8gf1qe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgft94/bytedance_released_on_huggingface_an_open_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgft94/bytedance_released_on_huggingface_an_open_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T12:37:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh0n3q</id>
    <title>PSA: Get Flash Attention v2 on AMD 7900 (gfx1100)</title>
    <updated>2025-03-22T04:37:53+00:00</updated>
    <author>
      <name>/u/canesin</name>
      <uri>https://old.reddit.com/user/canesin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Considering you have installed ROCm, PyTorch (official website worked) git and uv:&lt;/p&gt; &lt;p&gt;&lt;code&gt;uv pip install pip triton==3.2.0&lt;/code&gt;&lt;br /&gt; &lt;code&gt;git clone --single-branch --branch main_perf&lt;/code&gt; &lt;a href="https://github.com/ROCm/flash-attention.git"&gt;&lt;code&gt;https://github.com/ROCm/flash-attention.git&lt;/code&gt;&lt;/a&gt;&lt;br /&gt; &lt;code&gt;cd flash-attention/&lt;/code&gt;&lt;br /&gt; &lt;code&gt;export FLASH_ATTENTION_TRITON_AMD_ENABLE=&amp;quot;TRUE&amp;quot;&lt;/code&gt;&lt;br /&gt; &lt;code&gt;export GPU_ARCHS=&amp;quot;gfx1100&amp;quot;&lt;/code&gt;&lt;br /&gt; &lt;code&gt;python&lt;/code&gt; &lt;a href="http://setup.py"&gt;&lt;code&gt;setup.py&lt;/code&gt;&lt;/a&gt; &lt;code&gt;install&lt;/code&gt;&lt;/p&gt; &lt;p&gt;:-)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/canesin"&gt; /u/canesin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh0n3q/psa_get_flash_attention_v2_on_amd_7900_gfx1100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh0n3q/psa_get_flash_attention_v2_on_amd_7900_gfx1100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh0n3q/psa_get_flash_attention_v2_on_amd_7900_gfx1100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T04:37:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgyj01</id>
    <title>RTX PRO 5000 Laptop 24GB GDDR7 10496 cores 175W</title>
    <updated>2025-03-22T02:38:52+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;256-bit 896GB/s bandwidth. 228TFLOPS Tensor Core F16 (60% faster than 3090).&lt;/p&gt; &lt;p&gt;Should have made a similar desktop card that would be a no-brainer upgrade for the 3090/4090 users.&lt;/p&gt; &lt;p&gt;&lt;a href="https://videocardz.com/newz/nvidia-announces-rtx-pro-blackwell-laptop-gpus-up-to-10496-cuda-cores-and-24gb-gddr7-memory"&gt;https://videocardz.com/newz/nvidia-announces-rtx-pro-blackwell-laptop-gpus-up-to-10496-cuda-cores-and-24gb-gddr7-memory&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgyj01/rtx_pro_5000_laptop_24gb_gddr7_10496_cores_175w/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgyj01/rtx_pro_5000_laptop_24gb_gddr7_10496_cores_175w/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgyj01/rtx_pro_5000_laptop_24gb_gddr7_10496_cores_175w/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T02:38:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgnye9</id>
    <title>RTX Pro Blackwell Pricing Listed</title>
    <updated>2025-03-21T18:33:48+00:00</updated>
    <author>
      <name>/u/AlohaGrassDragon</name>
      <uri>https://old.reddit.com/user/AlohaGrassDragon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RTX Pro Blackwell pricing is up on &lt;a href="http://connection.com"&gt;connection.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;6000 (24064 cores, 96GB, 1.8 TB/s, 600W, 2-slot flow through) - $8565&lt;/p&gt; &lt;p&gt;6000 Max-Q (24064 cores, 96GB, 1.8 TB/s, 300W, 2-slot blower) - $8565&lt;/p&gt; &lt;p&gt;5000 (14080 cores, 48GB, 1.3 TB/s, 300W, 2-slot blower) - $4569&lt;/p&gt; &lt;p&gt;4500 (10496 cores, 32GB, 896 GB/s, 200W, 2-slot blower) - $2623&lt;/p&gt; &lt;p&gt;4000 (8960 cores, 24GB, 672 GB/s, 140W, 1-slot blower) - $1481&lt;/p&gt; &lt;p&gt;I'm not sure if this is real or final pricing, but I could see some of these models being compelling for local LLM. The 5000 is competitive with current A6000 used pricing, the 4500 is not too far away price-wise from a 5090 with better power/thermals, and the 4000 with 24 GB in a single slot for ~$1500 at 140W is very competitive with a used 3090. It costs more than a 3090, but comes with a warranty and you can fit many more in a system because of the size and power without having to implement an expensive watercooling or dual power supply setup.&lt;/p&gt; &lt;p&gt;All-in-all, if this is real pricing, it looks to me that they are marketing to us directly and they see their biggest competitor as used nVidia cards.&lt;/p&gt; &lt;p&gt;*Edited to add per-card specs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlohaGrassDragon"&gt; /u/AlohaGrassDragon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgnye9/rtx_pro_blackwell_pricing_listed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgnye9/rtx_pro_blackwell_pricing_listed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgnye9/rtx_pro_blackwell_pricing_listed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T18:33:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgsw1e</id>
    <title>We built an open source mock interviews platform empowered by ollama</title>
    <updated>2025-03-21T22:04:49+00:00</updated>
    <author>
      <name>/u/Boring_Rabbit2275</name>
      <uri>https://old.reddit.com/user/Boring_Rabbit2275</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgsw1e/we_built_an_open_source_mock_interviews_platform/"&gt; &lt;img alt="We built an open source mock interviews platform empowered by ollama" src="https://preview.redd.it/4k3ec6po84qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24694323661f97c1dfe5bb63a0f6f9d073c2e8ef" title="We built an open source mock interviews platform empowered by ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Come practice your interviews for free using our project on GitHub here: &lt;a href="https://github.com/Azzedde/aiva_mock_interviews"&gt;https://github.com/Azzedde/aiva_mock_interviews&lt;/a&gt; We are two junior AI engineers, and we would really appreciate feedback on our work. Please star it if you like it.&lt;/p&gt; &lt;p&gt;We find that the junior era is full of uncertainty, and we want to know if we are doing good work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Boring_Rabbit2275"&gt; /u/Boring_Rabbit2275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4k3ec6po84qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgsw1e/we_built_an_open_source_mock_interviews_platform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgsw1e/we_built_an_open_source_mock_interviews_platform/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T22:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgzpdb</id>
    <title>What are you using local LLMs for? How do they compare to the big tech offerings?</title>
    <updated>2025-03-22T03:43:40+00:00</updated>
    <author>
      <name>/u/TedHoliday</name>
      <uri>https://old.reddit.com/user/TedHoliday</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m just curious what all people are using local LLMs for. For me personally, I use Claude daily at work I like the idea of running an LLM locally, but I know it would be less accurate on my single PC with one single RTX 4090. &lt;/p&gt; &lt;p&gt;I like the idea of not being subject to the constantly changing pricing models and worrying about how many tokens Iâ€™ve used up, but I feel like even like 5% more accurate code is worth it due to the time it can save.&lt;/p&gt; &lt;p&gt;So Iâ€™m just curious what people are using them for, and how are they now compared to the big players (and with what hardware)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TedHoliday"&gt; /u/TedHoliday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgzpdb/what_are_you_using_local_llms_for_how_do_they/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgzpdb/what_are_you_using_local_llms_for_how_do_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgzpdb/what_are_you_using_local_llms_for_how_do_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T03:43:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgfmn8</id>
    <title>Docker's response to Ollama</title>
    <updated>2025-03-21T12:27:37+00:00</updated>
    <author>
      <name>/u/Barry_Jumps</name>
      <uri>https://old.reddit.com/user/Barry_Jumps</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Am I the only one excited about this?&lt;/p&gt; &lt;p&gt;Soon we can &lt;code&gt;docker run model mistral/mistral-small&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.docker.com/llm/"&gt;https://www.docker.com/llm/&lt;/a&gt;&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=mk_2MIWxLI0&amp;amp;t=1544s"&gt;https://www.youtube.com/watch?v=mk_2MIWxLI0&amp;amp;t=1544s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most exciting for me is that docker desktop will finally allow container to access my Mac's GPU&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Barry_Jumps"&gt; /u/Barry_Jumps &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgfmn8/dockers_response_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgfmn8/dockers_response_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgfmn8/dockers_response_to_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T12:27:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgopeg</id>
    <title>Orpheus-FastAPI: Local TTS with 8 Voices &amp; Emotion Tags (OpenAI Endpoint Compatible)</title>
    <updated>2025-03-21T19:04:54+00:00</updated>
    <author>
      <name>/u/townofsalemfangay</name>
      <uri>https://old.reddit.com/user/townofsalemfangay</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ðŸ‘‹&lt;/p&gt; &lt;p&gt;I just released Orpheus-FastAPI, a high-performance Text-to-Speech server that connects to your local LLM inference server using Orpheus's latest release. You can hook it up to OpenWebui, SillyTavern, or just use the web interface to generate audio natively.&lt;/p&gt; &lt;p&gt;I'd very much recommend if you want to get the most out of it in terms of suprasegmental features (the modalities of human voice, ums, arrs, pauses, like Sesame has) you use a System prompt to make the model respond as such (including the Syntax baked into the model). I included examples on my git so you can see how close this is to Sesame's CSM.&lt;/p&gt; &lt;p&gt;It uses a quantised version of the Orpheus 3B model (I've also included a direct link to my Q8 GGUF) that can run on consumer hardware, and works with GPUStack (my favourite), LM Studio, or llama.cpp.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Lex-au/Orpheus-FastAPI"&gt;https://github.com/Lex-au/Orpheus-FastAPI&lt;/a&gt;&lt;br /&gt; Model: &lt;a href="https://huggingface.co/lex-au/Orpheus-3b-FT-Q8_0.gguf"&gt;https://huggingface.co/lex-au/Orpheus-3b-FT-Q8_0.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think or if you have questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/townofsalemfangay"&gt; /u/townofsalemfangay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgopeg/orpheusfastapi_local_tts_with_8_voices_emotion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgopeg/orpheusfastapi_local_tts_with_8_voices_emotion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgopeg/orpheusfastapi_local_tts_with_8_voices_emotion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T19:04:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgap0q</id>
    <title>SpatialLM: A large language model designed for spatial understanding</title>
    <updated>2025-03-21T06:43:28+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgap0q/spatiallm_a_large_language_model_designed_for/"&gt; &lt;img alt="SpatialLM: A large language model designed for spatial understanding" src="https://external-preview.redd.it/Z2F4NmRpYWFvenBlMV9xklPr-alq2N0OOZexCtU6lC7spKP7fvQP_oR6XFl8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94d9dd664854a15924490e41428c31c299e3851e" title="SpatialLM: A large language model designed for spatial understanding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9hvol38aozpe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgap0q/spatiallm_a_large_language_model_designed_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgap0q/spatiallm_a_large_language_model_designed_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T06:43:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgl41s</id>
    <title>Tencent introduces Hunyuan-T1, their large reasoning model. Competing with DeepSeek-R1!</title>
    <updated>2025-03-21T16:36:11+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgl41s/tencent_introduces_hunyuant1_their_large/"&gt; &lt;img alt="Tencent introduces Hunyuan-T1, their large reasoning model. Competing with DeepSeek-R1!" src="https://preview.redd.it/vcb57bt1m2qe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=374682829fe92002bc36926e45cf71896aada6ea" title="Tencent introduces Hunyuan-T1, their large reasoning model. Competing with DeepSeek-R1!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to their blog post here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vcb57bt1m2qe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgl41s/tencent_introduces_hunyuant1_their_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgl41s/tencent_introduces_hunyuant1_their_large/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T16:36:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh1m45</id>
    <title>Why Do I Feel Poor Each Time I Decide to Buy a New GPU Even Though I Make More Money?</title>
    <updated>2025-03-22T05:40:29+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean for God sake, this curse has been haunting me for decades now. The first time I bought a GPU with my own money, I had to dream for it for months, saving money every month for my scholarship. When I went to buy my dream GPU, prices increased and I ended up buying a mid-range NVIDIA card (I had to buy other PC component which were expensive). Then years later I got busy with work and had Playstation, so I didn't really need a good PC, couple with the fact that laptop prices were getting cheaper and performant, I just didn't need to build a new rig.&lt;/p&gt; &lt;p&gt;Fast forward a few year, and my old dream to create my own games came back strong, and I decided to learn (seriously this time) 3D modeling and rendering. There is just something satisfying fooling untrained (or trained) eyes looking at a CGI production and thinking it's real.&lt;br /&gt; That's when I decided to build a new PC. Alas, the new age of crypto reaches its peak and yeah.. shortage of GPUs. Then, I felt poor again even after my several years of work and money saving.&lt;/p&gt; &lt;p&gt;Then COVID hits, and an RTX3090 cost $4000, if you get your hand on one. I bought multiple parts from different countries just to minimize my spending, and I felt very poor.&lt;/p&gt; &lt;p&gt;Which brings me to today. I want to build a new rig from my new passion; tinkering with AI. Alas, I have the money to buy any GPU I want, but my damn rational brain isn't allowing me!!! It's too expensive.. Am I insane? An RTX5090 at a price equivalent to a second hand car is NOT A SMART PURCHASE. And, it only comes with 32GB of VRAM. I'd still run the same models my now old 3090 can run...&lt;/p&gt; &lt;p&gt;In short, no matter how much my income increases over the years, I will always feel poor when I want to buy an new GPU ðŸ˜­ðŸ˜­ðŸ˜­&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh1m45/why_do_i_feel_poor_each_time_i_decide_to_buy_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh1m45/why_do_i_feel_poor_each_time_i_decide_to_buy_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh1m45/why_do_i_feel_poor_each_time_i_decide_to_buy_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T05:40:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh01fi</id>
    <title>Can someone ELI5 what makes NVIDIA a monopoly in AI race?</title>
    <updated>2025-03-22T04:02:06+00:00</updated>
    <author>
      <name>/u/Trysem</name>
      <uri>https://old.reddit.com/user/Trysem</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I heard somewhere it's cuda,then why some other companies like AMD is not making something like cuda of their own?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trysem"&gt; /u/Trysem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh01fi/can_someone_eli5_what_makes_nvidia_a_monopoly_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh01fi/can_someone_eli5_what_makes_nvidia_a_monopoly_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh01fi/can_someone_eli5_what_makes_nvidia_a_monopoly_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T04:02:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgio2g</id>
    <title>Qwen 3 is coming soon!</title>
    <updated>2025-03-21T14:53:25+00:00</updated>
    <author>
      <name>/u/themrzmaster</name>
      <uri>https://old.reddit.com/user/themrzmaster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/36878"&gt;https://github.com/huggingface/transformers/pull/36878&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/themrzmaster"&gt; /u/themrzmaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgio2g/qwen_3_is_coming_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgio2g/qwen_3_is_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgio2g/qwen_3_is_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T14:53:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh6j47</id>
    <title>ðŸš€ Running vLLM with 2 GPUs on my home server - automated in minutes!</title>
    <updated>2025-03-22T11:39:27+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6j47/running_vllm_with_2_gpus_on_my_home_server/"&gt; &lt;img alt="ðŸš€ Running vLLM with 2 GPUs on my home server - automated in minutes!" src="https://b.thumbs.redditmedia.com/ScRkwidkM9zx6RUmuOk3MXdlYXgdVEQFsbGekaQlRYs.jpg" title="ðŸš€ Running vLLM with 2 GPUs on my home server - automated in minutes!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve got vLLM running on a dual-GPU home server, complete with my Sbnb Linux distro tailored for AI, Grafana GPU utilization dashboards, and automated benchmarking - all set up in just a few minutes thanks to Ansible.&lt;/p&gt; &lt;p&gt;If youâ€™re into LLMs, home labs, or automation, I put together a detailed how-to here: ðŸ”— &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-VLLM.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-VLLM.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to help if anyone wants to get started!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jh6j47"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6j47/running_vllm_with_2_gpus_on_my_home_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6j47/running_vllm_with_2_gpus_on_my_home_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T11:39:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh0ovc</id>
    <title>MoshiVis by kyutai - first open-source real-time speech model that can talk about images</title>
    <updated>2025-03-22T04:40:57+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh0ovc/moshivis_by_kyutai_first_opensource_realtime/"&gt; &lt;img alt="MoshiVis by kyutai - first open-source real-time speech model that can talk about images" src="https://external-preview.redd.it/Y3ptd2t6NGE3NnFlMax15wl1W2mX3SQ6hWixr4c-XUrnbjt3Ig1vm4pgUatm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53210f67e2cb9507383ccec4a8ff094869da2fcb" title="MoshiVis by kyutai - first open-source real-time speech model that can talk about images" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/v86w8w4a76qe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh0ovc/moshivis_by_kyutai_first_opensource_realtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh0ovc/moshivis_by_kyutai_first_opensource_realtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T04:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh7c6e</id>
    <title>My 4x3090 eGPU collection</title>
    <updated>2025-03-22T12:28:25+00:00</updated>
    <author>
      <name>/u/Threatening-Silence-</name>
      <uri>https://old.reddit.com/user/Threatening-Silence-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh7c6e/my_4x3090_egpu_collection/"&gt; &lt;img alt="My 4x3090 eGPU collection" src="https://b.thumbs.redditmedia.com/tuwbOdIfpLg-K_qo2ArzC4oMvVIIdECI4tmNxUjTuKA.jpg" title="My 4x3090 eGPU collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 3 more 3090s ready to hook up to the 2nd Thunderbolt port in the back when I get the UT4g docks in. &lt;/p&gt; &lt;p&gt;Will need to find an area with more room though ðŸ˜…&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Threatening-Silence-"&gt; /u/Threatening-Silence- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jh7c6e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh7c6e/my_4x3090_egpu_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh7c6e/my_4x3090_egpu_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T12:28:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgp6sw</id>
    <title>China modified 4090s with 48gb sold cheaper than RTX 5090 - water cooled around 3400 usd</title>
    <updated>2025-03-21T19:25:39+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgp6sw/china_modified_4090s_with_48gb_sold_cheaper_than/"&gt; &lt;img alt="China modified 4090s with 48gb sold cheaper than RTX 5090 - water cooled around 3400 usd" src="https://b.thumbs.redditmedia.com/Uv9b5l37Z3HDbLOdsI_RvWZEDLPnFUNe8L0-bY4NmCE.jpg" title="China modified 4090s with 48gb sold cheaper than RTX 5090 - water cooled around 3400 usd" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jgp6sw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgp6sw/china_modified_4090s_with_48gb_sold_cheaper_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgp6sw/china_modified_4090s_with_48gb_sold_cheaper_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T19:25:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh4r72</id>
    <title>Deepseek (the website) now has a optout like the others, earlier they didn't have.</title>
    <updated>2025-03-22T09:34:02+00:00</updated>
    <author>
      <name>/u/Yes_but_I_think</name>
      <uri>https://old.reddit.com/user/Yes_but_I_think</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4r72/deepseek_the_website_now_has_a_optout_like_the/"&gt; &lt;img alt="Deepseek (the website) now has a optout like the others, earlier they didn't have." src="https://b.thumbs.redditmedia.com/EtrziXc9-Pm757HjCIYTYkRO79iwGVPKkOldgIO-yAM.jpg" title="Deepseek (the website) now has a optout like the others, earlier they didn't have." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vooy8j4gn7qe1.png?width=1042&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a82a68facaa4f88b976ef720f67abb57811b25b7"&gt;https://preview.redd.it/vooy8j4gn7qe1.png?width=1042&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a82a68facaa4f88b976ef720f67abb57811b25b7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Yes_but_I_think"&gt; /u/Yes_but_I_think &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4r72/deepseek_the_website_now_has_a_optout_like_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4r72/deepseek_the_website_now_has_a_optout_like_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4r72/deepseek_the_website_now_has_a_optout_like_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T09:34:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh3i7k</id>
    <title>1.5B surprises o1-preview math benchmarks with this new finding</title>
    <updated>2025-03-22T07:59:05+00:00</updated>
    <author>
      <name>/u/Different-Olive-8745</name>
      <uri>https://old.reddit.com/user/Different-Olive-8745</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh3i7k/15b_surprises_o1preview_math_benchmarks_with_this/"&gt; &lt;img alt="1.5B surprises o1-preview math benchmarks with this new finding" src="https://external-preview.redd.it/v81uCWR00P0A7u3BP_mTIasdD33pY9M4769VjQUIiSw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff86f87a266a03096a160a58098c8ced38e6b00c" title="1.5B surprises o1-preview math benchmarks with this new finding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Olive-8745"&gt; /u/Different-Olive-8745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2503.16219"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh3i7k/15b_surprises_o1preview_math_benchmarks_with_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh3i7k/15b_surprises_o1preview_math_benchmarks_with_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T07:59:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh4s2h</id>
    <title>LLama.cpp smillar speed but in pure Rust, local LLM inference alternatives.</title>
    <updated>2025-03-22T09:35:49+00:00</updated>
    <author>
      <name>/u/LewisJin</name>
      <uri>https://old.reddit.com/user/LewisJin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a long time, every time I want to run a LLM locally, the only choice is llama.cpp or other tools with magical optimization. However, llama.cpp is not always easy to set up especially when it comes to a new model and new architecture. Without help from the community, you can hardly convert a new model into GGUF. Even if you can, it is still very hard to make it work in llama.cpp.&lt;/p&gt; &lt;p&gt;Now, we can have an alternative way to infer LLM locally with maximum speed. And it's in pure Rust! No C++ needed. With pyo3 you can still call it with python, but Rust is easy enough, right?&lt;/p&gt; &lt;p&gt;I made a minimal example the same as llama.cpp chat cli. It runs 6 times faster than using pytorch, based on the Candle framework.Check it out:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lucasjinreal/Crane"&gt;https://github.com/lucasjinreal/Crane&lt;/a&gt;&lt;/p&gt; &lt;p&gt;next I would adding Spark-TTS and &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;Orpheus-TTS&lt;/a&gt; support, if you interested in Rust and fast inference, please join to develop with rust!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LewisJin"&gt; /u/LewisJin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4s2h/llamacpp_smillar_speed_but_in_pure_rust_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4s2h/llamacpp_smillar_speed_but_in_pure_rust_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4s2h/llamacpp_smillar_speed_but_in_pure_rust_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T09:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgqmlr</id>
    <title>"If we confuse users enough, they will overpay"</title>
    <updated>2025-03-21T20:26:10+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqmlr/if_we_confuse_users_enough_they_will_overpay/"&gt; &lt;img alt="&amp;quot;If we confuse users enough, they will overpay&amp;quot;" src="https://preview.redd.it/epfkc4xxq3qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f18b9505527bc8ed40557544a084be28952fd9b" title="&amp;quot;If we confuse users enough, they will overpay&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/epfkc4xxq3qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqmlr/if_we_confuse_users_enough_they_will_overpay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqmlr/if_we_confuse_users_enough_they_will_overpay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T20:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh6lsx</id>
    <title>OpenAI released GPT-4.5 and O1 Pro via their API and it looks like a weird decision.</title>
    <updated>2025-03-22T11:44:18+00:00</updated>
    <author>
      <name>/u/lessis_amess</name>
      <uri>https://old.reddit.com/user/lessis_amess</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6lsx/openai_released_gpt45_and_o1_pro_via_their_api/"&gt; &lt;img alt="OpenAI released GPT-4.5 and O1 Pro via their API and it looks like a weird decision." src="https://preview.redd.it/x942twbra8qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79683f47809a02571ff90500acb5d28a046d6940" title="OpenAI released GPT-4.5 and O1 Pro via their API and it looks like a weird decision." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;O1 Pro costs 33 times more than Claude 3.7 Sonnet, yet in many cases delivers less capability. GPT-4.5 costs 25 times more and itâ€™s an old model with a cut-off date from November.&lt;/p&gt; &lt;p&gt;Why release old, overpriced models to developers who care most about cost efficiency?&lt;/p&gt; &lt;p&gt;This isn't an accident.&lt;/p&gt; &lt;p&gt;It's anchoring.&lt;/p&gt; &lt;p&gt;Anchoring works by establishing an initial reference point. Once that reference exists, subsequent judgments revolve around it.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Show something expensive.&lt;/li&gt; &lt;li&gt;Show something less expensive.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The second thing seems like a bargain.&lt;/p&gt; &lt;p&gt;The expensive API models reset our expectations. For years, AI got cheaper while getting smarter. OpenAI wants to break that pattern. They're saying high intelligence costs money. Big models cost money. They're claiming they don't even profit from these prices.&lt;/p&gt; &lt;p&gt;When they release their next frontier model at a &amp;quot;lower&amp;quot; price, you'll think it's reasonable. But it will still cost more than what we paid before this reset. The new &amp;quot;cheap&amp;quot; will be expensive by last year's standards.&lt;/p&gt; &lt;p&gt;OpenAI claims these models lose money. Maybe. But they're conditioning the market to accept higher prices for whatever comes next. The API release is just the first move in a longer game.&lt;/p&gt; &lt;p&gt;This was not a confused move. Itâ€™s smart business. (i'm VERY happy we have open-source)&lt;/p&gt; &lt;p&gt;&lt;a href="https://ivelinkozarev.substack.com/p/the-pricing-of-gpt-45-and-o1-pro"&gt;https://ivelinkozarev.substack.com/p/the-pricing-of-gpt-45-and-o1-pro&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lessis_amess"&gt; /u/lessis_amess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x942twbra8qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6lsx/openai_released_gpt45_and_o1_pro_via_their_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6lsx/openai_released_gpt45_and_o1_pro_via_their_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T11:44:18+00:00</published>
  </entry>
</feed>
