<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-25T21:49:05+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i9gjok</id>
    <title>I flipped the function-calling pattern on its head. More responsive and less boiler plate for common agentic scenarios.</title>
    <updated>2025-01-25T06:00:37+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9gjok/i_flipped_the_functioncalling_pattern_on_its_head/"&gt; &lt;img alt="I flipped the function-calling pattern on its head. More responsive and less boiler plate for common agentic scenarios." src="https://preview.redd.it/xy8y911jy2fe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ddfda590492ba4d1f38fdf8525f782b794bad4bd" title="I flipped the function-calling pattern on its head. More responsive and less boiler plate for common agentic scenarios." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I built Arch-Function LLM ( the #1 trending OSS function calling model on HuggingFace) and talked about it here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hr9ll1/i_built_a_small_function_calling_llm_that_packs_a/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1hr9ll1/i_built_a_small_function_calling_llm_that_packs_a/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But one interesting property of building a lean and powerful LLM was that we could flip the function calling pattern on its head if engineered the right way and improve developer velocity for a lot of common scenarios for an agentic app.&lt;/p&gt; &lt;p&gt;Rather than the laborious 1) the application send the prompt to the LLM with function definitions 2) LLM decides response or to use tool 3) responds with function details and arguments to call 4) your application parses the response and executes the function 5) your application calls the LLM again with the prompt and the result of the function call and 6) LLM responds back that is send to the user &lt;/p&gt; &lt;p&gt;Now - that complexity for many common agentic scenarios can be pushed upstream to the reverse proxy. Which calls into the API as/when necessary and defaults the message to a fallback endpoint if no clear intent was found. Simplifies a lot of the code, improves responsiveness, lowers token cost etc you can learn more about the project below &lt;/p&gt; &lt;p&gt;Of course for complex planning scenarios the gateway would simply forward that to an endpoint that is designed to handle those scenarios - but we are working on the most lean ‚Äúplanning‚Äù LLM too. Check it out and would be curious to hear your thoughts &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xy8y911jy2fe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9gjok/i_flipped_the_functioncalling_pattern_on_its_head/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9gjok/i_flipped_the_functioncalling_pattern_on_its_head/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T06:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9jre5</id>
    <title>The browser version of DeepSeek R1 allows for maximum of 50 messages, the API version has no such limitations, you buy the number of tokens you want to use. My issue is I cant find a nice application to hook the Deepseek API so I can run it somewhat similarly to the browser version.</title>
    <updated>2025-01-25T09:56:35+00:00</updated>
    <author>
      <name>/u/PurpleCartoonist3336</name>
      <uri>https://old.reddit.com/user/PurpleCartoonist3336</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fully explained in title.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PurpleCartoonist3336"&gt; /u/PurpleCartoonist3336 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9jre5/the_browser_version_of_deepseek_r1_allows_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9jre5/the_browser_version_of_deepseek_r1_allows_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9jre5/the_browser_version_of_deepseek_r1_allows_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T09:56:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9mv9q</id>
    <title>Memory makes computation universal, remember?</title>
    <updated>2025-01-25T13:24:35+00:00</updated>
    <author>
      <name>/u/waxbolt</name>
      <uri>https://old.reddit.com/user/waxbolt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IMO current ML/AI/deep learning has forgotten how important memory is for general intelligence. To the tune of removing &amp;quot;thinking tokens&amp;quot; of past iterations from the context from o1, even when that access to the history of thought is theoretically the thing giving these chain of thought models their power. WDYT?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waxbolt"&gt; /u/waxbolt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://thinks.lol/2025/01/memory-makes-computation-universal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9mv9q/memory_makes_computation_universal_remember/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9mv9q/memory_makes_computation_universal_remember/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T13:24:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8xy2e</id>
    <title>Llama 4 is going to be SOTA</title>
    <updated>2025-01-24T15:27:34+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8xy2e/llama_4_is_going_to_be_sota/"&gt; &lt;img alt="Llama 4 is going to be SOTA" src="https://b.thumbs.redditmedia.com/X8yVPkCmbce42iVqsBjAUoQEG4UG6S1VFSHlELRIGIA.jpg" title="Llama 4 is going to be SOTA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i8xy2e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8xy2e/llama_4_is_going_to_be_sota/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8xy2e/llama_4_is_going_to_be_sota/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-24T15:27:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9df4h</id>
    <title>Snowflake claims breakthrough can cut AI inferencing times by more than 50%</title>
    <updated>2025-01-25T02:57:24+00:00</updated>
    <author>
      <name>/u/naytres</name>
      <uri>https://old.reddit.com/user/naytres</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9df4h/snowflake_claims_breakthrough_can_cut_ai/"&gt; &lt;img alt="Snowflake claims breakthrough can cut AI inferencing times by more than 50%" src="https://external-preview.redd.it/rFow1BiIxlzLULRqkmp4MQmAOsr_RX2LigTLUndYEX0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a3a7651d8f8f1ff902a9f794c7c939c280a9067" title="Snowflake claims breakthrough can cut AI inferencing times by more than 50%" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/naytres"&gt; /u/naytres &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://siliconangle.com/2025/01/16/snowflake-claims-breakthrough-can-cut-ai-inferencing-times-50/?utm_source=tldrai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9df4h/snowflake_claims_breakthrough_can_cut_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9df4h/snowflake_claims_breakthrough_can_cut_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T02:57:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9x1cq</id>
    <title>Layla AI huge update, GPU &amp; NPU acceleration</title>
    <updated>2025-01-25T21:05:40+00:00</updated>
    <author>
      <name>/u/andyblakely</name>
      <uri>https://old.reddit.com/user/andyblakely</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been alpha testing this for months, and the public version has finally been released! Offline stable diffusion image generation in 10 seconds with the new NPU acceleration is crazy.&lt;/p&gt; &lt;p&gt;(Copied from release notes, which go into even more details: &lt;a href="https://www.layla-network.ai/post/layla-v5-1-0-has-been-published"&gt;https://www.layla-network.ai/post/layla-v5-1-0-has-been-published&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;New features: - Layla supports GPU inference! Supports Vulkan and OpenCL backends - Layla supports NPU inference for Stable Diffusion! - Layla supports reasoning models Deepseek R1 family!&lt;/p&gt; &lt;p&gt;Improvements: - redesigned Lorebook UI to handle lots of documents better - improved UI of model import - added timestamps to Long-term Memory table view - backup data now directly allows you to choose a folder to save to - added a Download Manager app to give the ability to view/cancel download tasks in case they get stuck - added Whisper Base and Whisper Base (English) models - added ability to configure the language Whisper models listen in - Q4_0 quants are now automatically converted on the fly to support your current architecture - allows saving TavernPNG directly to file system in character creation - supports sherpa-onnx TTS engine APK - redesigned chat message quick actions (copy button is now always visible, tap &amp;amp; hold the message to bring up a context menu with more action) - Create Character (AI) image generation now uses the default negative prompt configured in the SD mini-app&lt;/p&gt; &lt;p&gt;Bug fixes: - fixed bug when importing chat history - fixed bug in Layla Cloud when handling very long conversation histories - fixed bug where an error in one memory will stop ingestion of all LTM memories - fixed bug where too many quick actions take up all your screen in chat - fixed bug where chat accent colour was not being applied to character responses - fixed bug in default character image generation fallback phrase&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andyblakely"&gt; /u/andyblakely &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9x1cq/layla_ai_huge_update_gpu_npu_acceleration/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9x1cq/layla_ai_huge_update_gpu_npu_acceleration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9x1cq/layla_ai_huge_update_gpu_npu_acceleration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T21:05:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9ft5x</id>
    <title>Another sneak peek of OpenWebUI Artifacts overhaul (Canvas / Claude Artifacts)</title>
    <updated>2025-01-25T05:13:53+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9ft5x/another_sneak_peek_of_openwebui_artifacts/"&gt; &lt;img alt="Another sneak peek of OpenWebUI Artifacts overhaul (Canvas / Claude Artifacts)" src="https://a.thumbs.redditmedia.com/AN4txFKDtqXpsz69LEFppKlGNAqsJ193WmFLBMaMRj8.jpg" title="Another sneak peek of OpenWebUI Artifacts overhaul (Canvas / Claude Artifacts)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/z7wlunjmk2fe1.png?width=1483&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a14db1516ee46a23cb0c7dafbafb80d2d5413339"&gt;https://preview.redd.it/z7wlunjmk2fe1.png?width=1483&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a14db1516ee46a23cb0c7dafbafb80d2d5413339&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/99vtt93qn2fe1.gif"&gt;https://i.redd.it/99vtt93qn2fe1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ehtdjckkc6fe1.png?width=1246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=24c40a836a44e0845d2dd4fbe136c454563f891c"&gt;https://preview.redd.it/ehtdjckkc6fe1.png?width=1246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=24c40a836a44e0845d2dd4fbe136c454563f891c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Another update on what I'm working on! This has gotten a little bit bigger than I thought. It's almost done though!&lt;/p&gt; &lt;p&gt;Here are the main changes:&lt;/p&gt; &lt;p&gt;A working difference checker that shows you what changed since the last revision. This is easily toggleable with a single button click!&lt;/p&gt; &lt;p&gt;You can cycle between different code blocks in one message. The code viewer will also be able to have file names or code block titles on the top (determined by header text starting with two or three # symbols, Llama 3.3 70b sometimes does this and it works 100% of the time with a system prompt) If it can't find a header or title it will default to the coding language as normal&lt;/p&gt; &lt;p&gt;Code in the chat can be compacted and shown as files while the Artifacts window is open, this is also toggleable!&lt;/p&gt; &lt;p&gt;*Edit: Just added React component rendering! &lt;/p&gt; &lt;p&gt;I also added WAYYYY more coding languages, pretty much everything you guys mentioned on my last post.&lt;/p&gt; &lt;p&gt;Hope to share this with you guys soon!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9ft5x/another_sneak_peek_of_openwebui_artifacts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9ft5x/another_sneak_peek_of_openwebui_artifacts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9ft5x/another_sneak_peek_of_openwebui_artifacts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T05:13:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9vsal</id>
    <title>How to send multi image inputs to LM studio?</title>
    <updated>2025-01-25T20:10:25+00:00</updated>
    <author>
      <name>/u/GHOST--1</name>
      <uri>https://old.reddit.com/user/GHOST--1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using LM studio with Qwen2 VL 7B. I have to send a template image and a filled image and ask the llm to compare them and give me key-value pairs output.&lt;br /&gt; But, I also want to send a few examples in the prompt. How can I add few images side by side and answers, and then at the end of the prompt, add my test image? Has anyone tried this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GHOST--1"&gt; /u/GHOST--1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9vsal/how_to_send_multi_image_inputs_to_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9vsal/how_to_send_multi_image_inputs_to_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9vsal/how_to_send_multi_image_inputs_to_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T20:10:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8tx5z</id>
    <title>I benchmarked (almost) every model that can fit in 24GB VRAM (Qwens, R1 distils, Mistrals, even Llama 70b gguf)</title>
    <updated>2025-01-24T12:08:50+00:00</updated>
    <author>
      <name>/u/kyazoglu</name>
      <uri>https://old.reddit.com/user/kyazoglu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8tx5z/i_benchmarked_almost_every_model_that_can_fit_in/"&gt; &lt;img alt="I benchmarked (almost) every model that can fit in 24GB VRAM (Qwens, R1 distils, Mistrals, even Llama 70b gguf)" src="https://preview.redd.it/es9l38ezmxee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a66f2c3fda0b03915eea1c0a72185b32e17e660" title="I benchmarked (almost) every model that can fit in 24GB VRAM (Qwens, R1 distils, Mistrals, even Llama 70b gguf)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyazoglu"&gt; /u/kyazoglu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/es9l38ezmxee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8tx5z/i_benchmarked_almost_every_model_that_can_fit_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8tx5z/i_benchmarked_almost_every_model_that_can_fit_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-24T12:08:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9ukyq</id>
    <title>Hobbyist trainer but not user?</title>
    <updated>2025-01-25T19:17:37+00:00</updated>
    <author>
      <name>/u/diligentgrasshopper</name>
      <uri>https://old.reddit.com/user/diligentgrasshopper</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;I was just wondering if any of you are more of a hobbyist trainer than a local AI user.&lt;/p&gt; &lt;p&gt;All of my AI needs are directed to closed providers and I have no intention to move locally. However I'm really interested in making fully experimental, whacky and hacky models. I don't so much as care to make something &amp;quot;useful&amp;quot;, I just find the joy of making linear algebra do crazy shit extremely fun, especially building something from scratch. With the upcoming 50xx model I'm thinking of saving to buy a 5070, with 12gb it should fulfill a good deal of my neural network curiosity.&lt;/p&gt; &lt;p&gt;Are there any other folks like me here? Would love to hear your experiences and your fun stuffs--and also if any of you think my financial plan would be worth the fun lol. I'm thinking of outlining a bunch of projects I can do with it, such that it would stay on for at least 10 months non stop, the biggest model size I'd train would likely be around ~200M, it might get up to 1B but that depends on how long it would take to saturate the model with my data.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/diligentgrasshopper"&gt; /u/diligentgrasshopper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9ukyq/hobbyist_trainer_but_not_user/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9ukyq/hobbyist_trainer_but_not_user/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9ukyq/hobbyist_trainer_but_not_user/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T19:17:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9kpu4</id>
    <title>DeepSeek R1 vs o1 Pro</title>
    <updated>2025-01-25T11:06:09+00:00</updated>
    <author>
      <name>/u/YourAverageDev0</name>
      <uri>https://old.reddit.com/user/YourAverageDev0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious what people think of this. I personally have a ChatGPT Plus subscription which allows me to access o1 (not pro). I feel that R1 definitely beats o1, but there's lots of people claiming o1 Pro as just a completely different level of model. Curious about the people who has access to o1 Pro, how does it compare?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YourAverageDev0"&gt; /u/YourAverageDev0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9kpu4/deepseek_r1_vs_o1_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9kpu4/deepseek_r1_vs_o1_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9kpu4/deepseek_r1_vs_o1_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T11:06:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8vclf</id>
    <title>Depseek promises to open source agi</title>
    <updated>2025-01-24T13:27:12+00:00</updated>
    <author>
      <name>/u/Notdesciplined</name>
      <uri>https://old.reddit.com/user/Notdesciplined</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/victor207755822/status/1882757279436718454"&gt;https://x.com/victor207755822/status/1882757279436718454&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From Deli chen: ‚Äú All I know is we keep pushing forward to make open-source AGI a reality for everyone. ‚Äú&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Notdesciplined"&gt; /u/Notdesciplined &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8vclf/depseek_promises_to_open_source_agi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8vclf/depseek_promises_to_open_source_agi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8vclf/depseek_promises_to_open_source_agi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-24T13:27:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9aqo6</id>
    <title>Elara: a simple open-source tool for anonymizing LLM prompts</title>
    <updated>2025-01-25T00:39:31+00:00</updated>
    <author>
      <name>/u/tycho_brahes_nose_</name>
      <uri>https://old.reddit.com/user/tycho_brahes_nose_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9aqo6/elara_a_simple_opensource_tool_for_anonymizing/"&gt; &lt;img alt="Elara: a simple open-source tool for anonymizing LLM prompts" src="https://external-preview.redd.it/djY4NHhtdndjMWZlMYMng0KvYDOp_Dky5P0_JbnaIijN4OXuE6thGLl5uoU8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=950301c08b319036a0d41b37cbfe879cbd743451" title="Elara: a simple open-source tool for anonymizing LLM prompts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tycho_brahes_nose_"&gt; /u/tycho_brahes_nose_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ug5cymvwc1fe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9aqo6/elara_a_simple_opensource_tool_for_anonymizing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9aqo6/elara_a_simple_opensource_tool_for_anonymizing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T00:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9dmll</id>
    <title>Replicating DeepSeek-R3-Zero RL recipe on 3B LLM for &lt;30$, the model develops self-verification and search abilities all on its own</title>
    <updated>2025-01-25T03:08:14+00:00</updated>
    <author>
      <name>/u/Happysedits</name>
      <uri>https://old.reddit.com/user/Happysedits</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9dmll/replicating_deepseekr3zero_rl_recipe_on_3b_llm/"&gt; &lt;img alt="Replicating DeepSeek-R3-Zero RL recipe on 3B LLM for &amp;lt;30$, the model develops self-verification and search abilities all on its own" src="https://external-preview.redd.it/-U0AHUhVyO7_HDJ7f69uyMJA3j37LtkD4mztbS9f2x8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=311e0b7bf41c19947928451d0391722ceeb36c65" title="Replicating DeepSeek-R3-Zero RL recipe on 3B LLM for &amp;lt;30$, the model develops self-verification and search abilities all on its own" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Happysedits"&gt; /u/Happysedits &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/jiayi_pirate/status/1882839370505621655"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9dmll/replicating_deepseekr3zero_rl_recipe_on_3b_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9dmll/replicating_deepseekr3zero_rl_recipe_on_3b_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T03:08:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9mhlx</id>
    <title>What questions have you asked reasoning models to solve that you couldn't get done with non-reasoning models?</title>
    <updated>2025-01-25T13:02:52+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Aside from testing r1 for curiousity, I haven't had much cause to use reasoning models. I found that normal models could handle tasks that I wanted and for those tasks that it couldn't handle, the reasoning models were also unable to handle them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9mhlx/what_questions_have_you_asked_reasoning_models_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9mhlx/what_questions_have_you_asked_reasoning_models_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9mhlx/what_questions_have_you_asked_reasoning_models_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T13:02:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9kp43</id>
    <title>Improve a Small Language Model for a specific language (cheap recipe)</title>
    <updated>2025-01-25T11:04:40+00:00</updated>
    <author>
      <name>/u/anakin_87</name>
      <uri>https://old.reddit.com/user/anakin_87</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9kp43/improve_a_small_language_model_for_a_specific/"&gt; &lt;img alt="Improve a Small Language Model for a specific language (cheap recipe)" src="https://b.thumbs.redditmedia.com/W4nq1UfhyQ0enIgrZzBiLKJvDhS8bEl8oy1HDU1LPoo.jpg" title="Improve a Small Language Model for a specific language (cheap recipe)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üìì &lt;a href="https://www.kaggle.com/code/anakin87/post-training-gemma-for-italian-and-beyond"&gt;https://www.kaggle.com/code/anakin87/post-training-gemma-for-italian-and-beyond&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey! I recently took part in a Kaggle competition to fine-tune Gemma.&lt;/p&gt; &lt;p&gt;I fine-tuned the model to improve üáÆüáπ Italian performance, but I believe my recipe is adaptable to other languages and models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;In the attached notebook, you can find all code + datasets + models.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I hope it can be useful to someone.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Steps&lt;/strong&gt;&lt;br /&gt; üìä Choose reference metrics&lt;br /&gt; üßë‚Äçüî¨ Data curation for Instruction Fine Tuning: identify existing datasets + generate synthetic data&lt;br /&gt; üèãÔ∏è‚Äç‚ôÇÔ∏è Efficient Instruction Fine Tuning with Spectrum&lt;br /&gt; üßë‚Äçüî¨ Data curation for Preference Tuning: identify existing datasets + generate synthetic data&lt;br /&gt; üëçüëé Efficient Direct Preference Optimization with Spectrum&lt;br /&gt; üìà Evaluation&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mtyczkixg4fe1.jpg?width=1792&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ff0a5b0d894b56efa7f427cf326af5f01159bf73"&gt;Gemma Neogenesis - Improving Gemma 2 for a Specific Language on a Budget: Post-Training Recipe&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anakin_87"&gt; /u/anakin_87 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9kp43/improve_a_small_language_model_for_a_specific/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9kp43/improve_a_small_language_model_for_a_specific/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9kp43/improve_a_small_language_model_for_a_specific/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T11:04:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9txf3</id>
    <title>Deepseek is way better in Python code generation than ChatGPT (talking about the "free" versions of both)</title>
    <updated>2025-01-25T18:49:57+00:00</updated>
    <author>
      <name>/u/ThiccStorms</name>
      <uri>https://old.reddit.com/user/ThiccStorms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I haven't bought any subscriptions and im talking about the web based apps for both, and im just taking this opportunity to fanboy on deepseek because it produces super clean python code in one shot, whereas chat gpt generates a complex mess and i still had to specify some things again and again because it missed out on them in the initial prompt.&lt;br /&gt; I didn't generate a snippet out of scratch, i had an old function in python which i wanted to re-utilise for a similar use case, I wrote a detailed prompt to get what I need but ChatGPT still managed to screw up while deepseek nailed it in the first try. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThiccStorms"&gt; /u/ThiccStorms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9txf3/deepseek_is_way_better_in_python_code_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9txf3/deepseek_is_way_better_in_python_code_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9txf3/deepseek_is_way_better_in_python_code_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T18:49:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9ddj1</id>
    <title>Sky-T1-32B-Flash - Think Less, Achieve More: Cut Reasoning Costs by 50% Without Sacrificing Accuracy</title>
    <updated>2025-01-25T02:54:59+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/NovaSky-AI/Sky-T1-32B-Flash"&gt;https://huggingface.co/NovaSky-AI/Sky-T1-32B-Flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog post:&lt;/p&gt; &lt;p&gt;&lt;a href="https://novasky-ai.github.io/posts/reduce-overthinking/"&gt;https://novasky-ai.github.io/posts/reduce-overthinking/&lt;/a&gt;&lt;br /&gt; ---&lt;/p&gt; &lt;p&gt;GGUF:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/Sky-T1-32B-Flash-GGUF"&gt;https://huggingface.co/bartowski/Sky-T1-32B-Flash-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;FuseO1 Merge:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview"&gt;https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9ddj1/skyt132bflash_think_less_achieve_more_cut/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9ddj1/skyt132bflash_think_less_achieve_more_cut/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9ddj1/skyt132bflash_think_less_achieve_more_cut/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T02:54:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9k18t</id>
    <title>I am simply blown away by this 32B model. It's a Sky-T1 + Fuse-O1 + DeepseekR1 + Qwen32B fusion. Please read the full post</title>
    <updated>2025-01-25T10:16:36+00:00</updated>
    <author>
      <name>/u/Educational_Gap5867</name>
      <uri>https://old.reddit.com/user/Educational_Gap5867</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model available here: &lt;a href="https://huggingface.co/sm54/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview-Q4_K_M-GGUF"&gt;https://huggingface.co/sm54/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview-Q4_K_M-GGUF&lt;/a&gt;&lt;br /&gt; Original reddit post by &lt;a href="/u/AaronFeng47"&gt;u/AaronFeng47&lt;/a&gt; : &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i9ddj1/skyt132bflash_think_less_achieve_more_cut/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1i9ddj1/skyt132bflash_think_less_achieve_more_cut/&lt;/a&gt;&lt;br /&gt; Leetcode 1430. Prompt + Thought process here: &lt;a href="https://pastebin.com/hqWfbG7e"&gt;https://pastebin.com/hqWfbG7e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am simply blown away. This AI's thought process is so smooth. Every time I thought it was overthinking it wasn't it was judging the cases.&lt;br /&gt; Only caveat. 1: I think if the question would have been unknown it would've fucked up the final output I think it got distracted because it thought all the way through in Python but then suddenly started writing final code in Java and I am not sure what that was about. I had no pre-prompted it or anything like that.&lt;/p&gt; &lt;p&gt;Commands I ran&lt;br /&gt; `ollama pull &lt;a href="http://hf.co/sm54/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview-Q4_K_M-GGUF"&gt;hf.co/sm54/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview-Q4_K_M-GGUF&lt;/a&gt; `&lt;br /&gt; `Ollama run &lt;a href="http://hf.co/sm54/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview-Q4_K_M-GGUF%60"&gt;hf.co/sm54/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview-Q4_K_M-GGUF`&lt;/a&gt;&lt;br /&gt; `&amp;gt; PROMPT (check pastebin for Prompt) `&lt;/p&gt; &lt;p&gt;Final code here:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;```java public class Solution { public boolean doesPathExist(TreeNode root, int[] arr) { if (root == null || arr.length == 0) { return false; } Stack&amp;lt;Pair&amp;lt;TreeNode, Integer&amp;gt;&amp;gt; stack = new Stack&amp;lt;&amp;gt;(); stack.push(new Pair&amp;lt;&amp;gt;(root, 0)); while (!stack.isEmpty()) { Pair&amp;lt;TreeNode, Integer&amp;gt; currentPair = stack.pop(); TreeNode currentNode = currentPair.getKey(); int currentIndex = currentPair.getValue(); if (currentIndex &amp;gt;= arr.length) { continue; } if (currentNode.val != arr[currentIndex]) { continue; } // Check if this is the last element and a leaf if (currentIndex == arr.length - 1) { if (currentNode.left == null &amp;amp;&amp;amp; currentNode.right == null) { return true; } } else { // Push right first to process left later, maintaining order if (currentNode.right != null) { stack.push(new Pair&amp;lt;&amp;gt;(currentNode.right, currentIndex + 1)); } if (currentNode.left != null) { stack.push(new Pair&amp;lt;&amp;gt;(currentNode.left, currentIndex + 1)); } } } return false; } } // Helper class to store node and index class Pair&amp;lt;K, V&amp;gt; { private K key; private V value; public Pair(K key, V value) { this.key = key; this.value = value; } public K getKey() { return key; } public V getValue() { return value; } } ``` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is a dry run the model did to confirm all the different test cases. Simply immaculate.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;```plaintext Let me test it against some examples. Test case 1: Tree: 0 / \ 1 2 arr = [0,1] Expected output: True (path root -&amp;gt; left child) Code execution: stack starts with [(root,0)] pop root, i=0. value matches arr[0] =0. i is not last index, so push right and left children with i+1=1. Now stack has (2,1), then (1,1). Next iteration: pop (1,1). node 1's value is 1 == arr[1]=1. Check if it's last index: yes. Check if leaf. Node 1 has no left or right? In this tree, node 1 is a leaf (assuming original tree as in test case). Yes: return True. Test case passes. Another test case: Tree: 0 / \ 1 2 arr = [0,2] Expected output: True. In code: stack has root with i=0. matches. push right (2,1), then left(1,1). pop (1,1): node.val is 1 != arr[1]=2? So skip. then pop (2,1). val is 2 == arr[1]. Check if last index: yes, and node 2 is a leaf. So return True. Test passes. Another test case: Tree: 0 / \ 1 2 / 3 arr = [0,1,3] Expected output: True. Code: stack starts with (root,0). val matches. i is not last index. Push left (1,1) and right (2,1). Next pop (2,1): value 2 != arr[1]=1? So skip. pop (1,1): val 1 == arr[1]. Not last index (len(arr)=3). push its children: left (3,2), no right. stack now has (3,2). Pop (3,2): val is 3 == arr[2]=3. Check if i==2 (last) and node is leaf. Node 3's left and right are None? So yes: return True. Test passes. Another test case: Tree: 0 / \ 1 2 / 3 arr = [0,1,4] In code: At (3,2), val is 3 !=4. so continue. No other possibilities. stack becomes empty: return False. Yes. Another test case where arr is longer than any path. Tree: 0 / \ 1 2 arr = [0,1,3] So the tree has no node beyond left child's left (which doesn't exist). So len(arr)=3, but all root-to-leaf paths have length 2. So expected output is False. In code: stack pops (root,0): val matches, push children with i=1. pop (2,1): not arr[1]=1? So no. pop(1,1): match. since len(arr)=3&amp;gt;1+1=2? Wait: node 1 has left child 3? Wait in previous example, but in this test case's tree: node 1 doesn't have any children? Because the tree is: root 0 with left 1 (no children) and right 2. So in code: At node 1, i=1. since len(arr)-1 =2, which is greater than 1. so node 1's val matches arr[1] (1), then push its children: none. Thus, stack after popping (1,1): nothing pushed. So stack becomes empty. No other elements. Return False. Which is correct. Another edge case: root is None and arr is [0]: return False. Yes. root is a single node with val 5 and arr [5]. Return True. In code: yes. So the code seems to handle all these cases. Thus, I think this should be the correct approach. ``` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Wow.. just wow. I am really gonna use these think tags for my own leetcode prep.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Gap5867"&gt; /u/Educational_Gap5867 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9k18t/i_am_simply_blown_away_by_this_32b_model_its_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9k18t/i_am_simply_blown_away_by_this_32b_model_its_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9k18t/i_am_simply_blown_away_by_this_32b_model_its_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T10:16:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9dvrk</id>
    <title>Deepseek v3 will make MoE opensource models wayyy more common.</title>
    <updated>2025-01-25T03:22:08+00:00</updated>
    <author>
      <name>/u/tensorsgo</name>
      <uri>https://old.reddit.com/user/tensorsgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IDK why no one is talking about this but i just finished reading Deepseek v3's technical report, and how they‚Äôve found innovative and novel solution for one of the biggest challenges with training MoE architectures which is irregular loss spiking.&lt;/p&gt; &lt;p&gt;this issue was probably the major reason why we haven‚Äôt seen widespread adoption of MoE models before. But now, with their novel solutions laid out in this open report, it‚Äôs likely that other companies will start implementing similar approaches.&lt;/p&gt; &lt;p&gt;I can already imagine a MoE powered Qwen or Llama becoming flagship models in future, just like deepseek&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tensorsgo"&gt; /u/tensorsgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9dvrk/deepseek_v3_will_make_moe_opensource_models_wayyy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9dvrk/deepseek_v3_will_make_moe_opensource_models_wayyy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9dvrk/deepseek_v3_will_make_moe_opensource_models_wayyy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T03:22:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9wnfs</id>
    <title>Why do openai and meta etc plan to spend so much on data centers? how do they make the money back?</title>
    <updated>2025-01-25T20:48:52+00:00</updated>
    <author>
      <name>/u/lblblllb</name>
      <uri>https://old.reddit.com/user/lblblllb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chatgpt already has over 180mm users, which is over half of US population. With exception of limitation on o1, the service uptime seems mostly fine so far? why spend up to 500bln to build data centers for exclusive use of openai that will depreciate very quickly(due to GPU depreciation)? Same for meta spending 60bln on AI. how do they plan to make the money back? seems like they really have to be able to use AI to replace most of the knowledge workers in order to make a return.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reuters.com/business/media-telecom/stargate-artificial-intelligence-project-exclusively-serve-openai-ft-reports-2025-01-24/"&gt;https://www.reuters.com/business/media-telecom/stargate-artificial-intelligence-project-exclusively-serve-openai-ft-reports-2025-01-24/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lblblllb"&gt; /u/lblblllb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wnfs/why_do_openai_and_meta_etc_plan_to_spend_so_much/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wnfs/why_do_openai_and_meta_etc_plan_to_spend_so_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wnfs/why_do_openai_and_meta_etc_plan_to_spend_so_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T20:48:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9t0x2</id>
    <title>How Chinese AI Startup DeepSeek Made a Model that Rivals OpenAI</title>
    <updated>2025-01-25T18:10:15+00:00</updated>
    <author>
      <name>/u/CarbonTail</name>
      <uri>https://old.reddit.com/user/CarbonTail</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9t0x2/how_chinese_ai_startup_deepseek_made_a_model_that/"&gt; &lt;img alt="How Chinese AI Startup DeepSeek Made a Model that Rivals OpenAI" src="https://external-preview.redd.it/GaYe6FpTRtNr23ADdM65dvNw3TVMjwFcEfKfrHC4ukE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa1a135c85bd082bf94671971fb8ea8e80f02eb2" title="How Chinese AI Startup DeepSeek Made a Model that Rivals OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CarbonTail"&gt; /u/CarbonTail &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wired.com/story/deepseek-china-model-ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9t0x2/how_chinese_ai_startup_deepseek_made_a_model_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9t0x2/how_chinese_ai_startup_deepseek_made_a_model_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T18:10:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9wbya</id>
    <title>ByteDance announces Doubao-1.5-pro</title>
    <updated>2025-01-25T20:34:44+00:00</updated>
    <author>
      <name>/u/Outrageous-Win-3244</name>
      <uri>https://old.reddit.com/user/Outrageous-Win-3244</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wbya/bytedance_announces_doubao15pro/"&gt; &lt;img alt="ByteDance announces Doubao-1.5-pro" src="https://preview.redd.it/5pjykhaha7fe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0df07e6b549319488a93d42063d7e338ff3b8b7" title="ByteDance announces Doubao-1.5-pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ByteDance announces Doubao-1.5-pro&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Includes a &amp;quot;Deep Thinking&amp;quot; mode, surpassing O1-preview and O1 models on the AIME benchmark.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Outperforms deepseek-v3, gpt4o, and llama3.1-405B on popular benchmarks. &lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Built on a MoE architecture, with activated parameters far fewer than those in the above models. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Achieves a 7x MoE performance leverage‚Äîdelivering dense model performance with just 1/7 of the activated parameters (e.g., 20B activated params = 140B dense performance). &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Engineering-wise, features heterogeneous system design for prefill-decode and attn-fffn, maximizing throughput under low-latency requirements.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Win-3244"&gt; /u/Outrageous-Win-3244 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5pjykhaha7fe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wbya/bytedance_announces_doubao15pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wbya/bytedance_announces_doubao15pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T20:34:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9oqou</id>
    <title>Nvidia to wind down CUDA support for Maxwell and Pascal</title>
    <updated>2025-01-25T15:01:14+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Nvidia's release notes for CUDA 12.8 revealed that Maxwell, Pascal, and Volta GPUs will likely transition to the legacy driver branch. The document states that &amp;quot;architecture support for Maxwell, Pascal, and Volta is considered feature-complete and will be frozen in an upcoming release.&amp;quot; &lt;/p&gt; &lt;p&gt;I think most of us new this day was coming soon. I wouldn't fret too much about it though. This doesn't mean that the cards will stop working or any software built on CUDA will stop working anytime soon. Even if CUDA 12.8 is the last version to support Pascal, I think open source projects like Llama.cpp will continue supporting those cards for a few more years, given how widely used Pascal is in the community and the lack of any decently priced alternatives until now.&lt;/p&gt; &lt;p&gt;If anyone is considering buying a P40 for a new build, I don't think they should change their plans because of this announcement, especially if they find a good deal on the P40. &lt;/p&gt; &lt;p&gt;Personally, I have 10 P40s (just bought 5 last week at $180/card), 4 P100s, and 4 V100s and I'm not planning on retiring them anytime soon. They're great and work really well for my use cases. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpu-drivers/nvidia-starts-phasing-out-maxwell-pascal-and-volta-gpus-geforce-driver-support-status-unclear"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9oqou/nvidia_to_wind_down_cuda_support_for_maxwell_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9oqou/nvidia_to_wind_down_cuda_support_for_maxwell_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T15:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9nqj9</id>
    <title>Full open source reproduction of R1 in progress ‚è≥</title>
    <updated>2025-01-25T14:11:35+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9nqj9/full_open_source_reproduction_of_r1_in_progress/"&gt; &lt;img alt="Full open source reproduction of R1 in progress ‚è≥" src="https://preview.redd.it/s5rmvdhtd5fe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbf96bf7e9979be87994f66f0537b9e70492b54b" title="Full open source reproduction of R1 in progress ‚è≥" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s5rmvdhtd5fe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9nqj9/full_open_source_reproduction_of_r1_in_progress/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9nqj9/full_open_source_reproduction_of_r1_in_progress/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T14:11:35+00:00</published>
  </entry>
</feed>
