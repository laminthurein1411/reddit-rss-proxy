<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-05T11:48:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1l3lutf</id>
    <title>Anyone have any experience with Deepseek-R1-0528-Qwen3-8B?</title>
    <updated>2025-06-05T00:37:31+00:00</updated>
    <author>
      <name>/u/clduab11</name>
      <uri>https://old.reddit.com/user/clduab11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to download Unsloth's version on Msty (2021 iMac, 16GB), and per Unsloth's HuggingFace, they say to do the Q4_K_XL version because that's the version that's preconfigured with the prompt template and the settings and all that good jazz.&lt;/p&gt; &lt;p&gt;But I'm left scratching my head over here. It acts all bonkers. Spilling prompt tags (when they &lt;strong&gt;are&lt;/strong&gt; entered), never actually stops its output... regardless whether or not a prompt template is entered. Even in its reasoning it acts as if the user (me) is prompting it and engaging in its own schizophrenic conversation. Or it'll answer the query, then reason after the query like it's going to engage back in its own schizo convo.&lt;/p&gt; &lt;p&gt;And for the prompt templates? &lt;em&gt;Maaannnn&lt;/em&gt;...I've tried ChatML, Vicuna, Gemma Instruct, Alfred, a custom one combining a few of them, Jinja-format, non-Jinja format...wrapped text, non-wrapped text, nothing seems to work. I know it's something I'm doing wrong; it work's in HuggingFace's Open Playground just fine. Granite Instruct seemed to come the closest, but it still wrapped the answer and didn't &lt;strong&gt;stop&lt;/strong&gt; its answer, then it reasoned from its own output.&lt;/p&gt; &lt;p&gt;Quite a treat of a model; I just wonder if there's something I need to interrupt as far as how Msty prompts the LLM behind-the-scenes, or configure. Any advice? (inb4 switch to Open WebUI lol)&lt;/p&gt; &lt;p&gt;EDIT TO ADD: ChatML seems to throw the Think tags (even though the thinking is being done outside the think tags).&lt;/p&gt; &lt;p&gt;EDIT TO ADD 2: Even when copy/pasting the formatted Chat Template like‚Ä¶&lt;/p&gt; &lt;p&gt;EDIT TO ADD 3: SOLVED! Turns out I wasn‚Äôt auto connecting with sidecar correctly and it wasn‚Äôt correctly forwarding all the information. Further, the way you call the HF model in Msty matters. Works a treat now!‚Äô&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clduab11"&gt; /u/clduab11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3lutf/anyone_have_any_experience_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3lutf/anyone_have_any_experience_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3lutf/anyone_have_any_experience_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-05T00:37:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3ok95</id>
    <title>Local AI smart speaker</title>
    <updated>2025-06-05T02:53:13+00:00</updated>
    <author>
      <name>/u/Llamapants</name>
      <uri>https://old.reddit.com/user/Llamapants</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering if there were any low cost options for a Bluetooth speaker/microphone to connect to my server for voice chat with a local llm. Can an old echo or something be repurposed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Llamapants"&gt; /u/Llamapants &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3ok95/local_ai_smart_speaker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3ok95/local_ai_smart_speaker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3ok95/local_ai_smart_speaker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-05T02:53:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3vqut</id>
    <title>AI Linter VS Code suggestions</title>
    <updated>2025-06-05T10:26:43+00:00</updated>
    <author>
      <name>/u/DoggoChann</name>
      <uri>https://old.reddit.com/user/DoggoChann</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is a good extension to use a local model as a linter? I do not want AI generated code, I only want the AI to act as a linter and say, ‚Äúhey, you seem to be missing a zero in the integer here.‚Äù And obvious problems like that, but problems not so obvious a normal linter can find them. Ideally it would be able to trigger a warning at a line in the code and not open a big chat box for all problems which can be annoying to shuffle through &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DoggoChann"&gt; /u/DoggoChann &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3vqut/ai_linter_vs_code_suggestions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3vqut/ai_linter_vs_code_suggestions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3vqut/ai_linter_vs_code_suggestions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-05T10:26:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1l390xb</id>
    <title>Has anyone successfully built a coding assistant using local llama?</title>
    <updated>2025-06-04T15:49:06+00:00</updated>
    <author>
      <name>/u/rushblyatiful</name>
      <uri>https://old.reddit.com/user/rushblyatiful</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Something that's like Copilot, Kilocode, etc. &lt;/p&gt; &lt;p&gt;What model are you using? What pc specs do you have? How is the performance? &lt;/p&gt; &lt;p&gt;Lastly, is this even possible? &lt;/p&gt; &lt;p&gt;Edit: majority of the answers misunderstood my question. It literally says in the title about building an ai assistant. As in creating one from scratch or copy from existing ones, but code it nonetheless.&lt;/p&gt; &lt;p&gt;I should have phrased the question better.&lt;/p&gt; &lt;p&gt;Anyway, I guess reinventing the wheel is indeed a waste of time when I could just download a llama model and connect a popular ai assistant to it. &lt;/p&gt; &lt;p&gt;Silly me. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rushblyatiful"&gt; /u/rushblyatiful &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l390xb/has_anyone_successfully_built_a_coding_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l390xb/has_anyone_successfully_built_a_coding_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l390xb/has_anyone_successfully_built_a_coding_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T15:49:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3btj3</id>
    <title>How does gemma3:4b-it-qat fare against OpenAI models on MMLU-Pro benchmark? Try for yourself in Excel</title>
    <updated>2025-06-04T17:37:22+00:00</updated>
    <author>
      <name>/u/Kapperfar</name>
      <uri>https://old.reddit.com/user/Kapperfar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3btj3/how_does_gemma34bitqat_fare_against_openai_models/"&gt; &lt;img alt="How does gemma3:4b-it-qat fare against OpenAI models on MMLU-Pro benchmark? Try for yourself in Excel" src="https://external-preview.redd.it/YWJhMzhuazA1eTRmMe_FxokcQpBsD-M8wgi3hLI8PHPTr7rVnpmgkef-dH9o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc4432caac062358ceae7b73921dc23d1f01261e" title="How does gemma3:4b-it-qat fare against OpenAI models on MMLU-Pro benchmark? Try for yourself in Excel" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made an Excel add-in that lets you run a prompt on thousands of rows of tasks. Might be useful for some of you to quickly benchmark new models when they come out. In the video I ran gemma3:4b-it-qat, gpt-4.1-mini, and o4-mini on a (admittedly tiny) subset of the MMLU Pro benchmark. I think I understand now why OpenAI didn't include MMLU Pro in their gpt-4.1-mini announcement blog post :D&lt;/p&gt; &lt;p&gt;To try for yourself, clone the git repo at &lt;a href="https://github.com/getcellm/cellm/"&gt;https://github.com/getcellm/cellm/&lt;/a&gt;, build with Visual Studio, and run the installer Cellm-AddIn-Release-x64.msi in src\Cellm.Installers\bin\x64\Release\en-US. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kapperfar"&gt; /u/Kapperfar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ye3ahlk05y4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3btj3/how_does_gemma34bitqat_fare_against_openai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3btj3/how_does_gemma34bitqat_fare_against_openai_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T17:37:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3hys4</id>
    <title>Hardware considerations (5090 vs 2 x 3090). What AMD AM5 MOBO for dual GPU?</title>
    <updated>2025-06-04T21:41:24+00:00</updated>
    <author>
      <name>/u/Repsol_Honda_PL</name>
      <uri>https://old.reddit.com/user/Repsol_Honda_PL</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt; &lt;p&gt;I have an AM5 motherboard prepared for a single GPU card. I also have an MSI RTX 3090 Suprim.&lt;/p&gt; &lt;p&gt;I can also buy a second MSI RTX 3090 Suprim, used of course, but then I would have to change the motherboard (also case and PSU). The other option is to buy the used RTX 5090 instead of the 3090 (then the rest of the hardware remains the same). I have the possibility to buy a slightly used 5090 at a price almost same to two 3090s (because of case/PSU difference). I know 48 GB VRAM is more than 32 GB VRAM ;), but things get complicated with two cards (and the money is ultimately close).&lt;/p&gt; &lt;p&gt;If you persuade me to get two 3090 cards (it's almost a given on the LLM forums), then please suggest what AMD AM5 motherboard you recommend for two graphics cards (the MSI RTX 3090 Suprim are extremely large, heavy and power hungry - although the latter can be tamed by undervolting). What motherboards do you recommend? (They must be large, with a good power section so that I can install two 3090 cards without problems). I also need to make sure I have above-average cooling, although I won't go into water cooling.&lt;/p&gt; &lt;p&gt;I would have less problems with the 5090, but I know VRAM is so important. What works best for you guys and what do you recommend which direction to go?&lt;/p&gt; &lt;p&gt;The dual GPU board seems more future-proof, as you I will be able to replace the 3090s with two 5090s (Ti / Super) in the future (if you can talk about ‚Äòfuture-proof‚Äô solutions in the PC world ;) )&lt;/p&gt; &lt;p&gt;Thanks for your suggestions and help with the choice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Repsol_Honda_PL"&gt; /u/Repsol_Honda_PL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3hys4/hardware_considerations_5090_vs_2_x_3090_what_amd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3hys4/hardware_considerations_5090_vs_2_x_3090_what_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3hys4/hardware_considerations_5090_vs_2_x_3090_what_amd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T21:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3wlwy</id>
    <title>Best simple model for local fine tuning?</title>
    <updated>2025-06-05T11:17:53+00:00</updated>
    <author>
      <name>/u/Lucario1296</name>
      <uri>https://old.reddit.com/user/Lucario1296</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Back in the day I used to use gpt2 but tensorflow has moved on and it's not longer properly supported. Are there any good replacements? &lt;/p&gt; &lt;p&gt;I don't need an excellent model at all, something as simple and weak as gpt2 is ideal (I would much rather faster training). It'll be unlearning all its written language anyways: I'm tackling a similar project to the guy a while back that generated Pokemon sprites fine-tuning gpt2. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lucario1296"&gt; /u/Lucario1296 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3wlwy/best_simple_model_for_local_fine_tuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3wlwy/best_simple_model_for_local_fine_tuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3wlwy/best_simple_model_for_local_fine_tuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-05T11:17:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3qfhh</id>
    <title>RTX PRO 6000 machine for 12k?</title>
    <updated>2025-06-05T04:37:26+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Is there a company that sells a complete machine (cpu, ram, gpu, drive, motherboard, case, power supply, etc all wired up) with RTX 6000 Pro for 12k USD or less? &lt;/p&gt; &lt;p&gt;The card itself is around 7-8k I think, which leaves 4k for the other components. Is this economically possible?&lt;/p&gt; &lt;p&gt;Bonus point: The machine supports adding another rtx 6000 gpu in the future to get 2x96 GB of vram. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3qfhh/rtx_pro_6000_machine_for_12k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3qfhh/rtx_pro_6000_machine_for_12k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3qfhh/rtx_pro_6000_machine_for_12k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-05T04:37:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1l35rp1</id>
    <title>Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training</title>
    <updated>2025-06-04T13:37:13+00:00</updated>
    <author>
      <name>/u/Initial-Image-1015</name>
      <uri>https://old.reddit.com/user/Initial-Image-1015</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l35rp1/common_corpus_the_largest_collection_of_ethical/"&gt; &lt;img alt="Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training" src="https://preview.redd.it/l1wcpiqhyw4f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b09d23def47a360d64c26390174e009fd27fb722" title="Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Announcing the release of the official Common Corpus paper: a 20 page report detailing how we collected, processed and published 2 trillion tokens of reusable data for LLM pretraining.&amp;quot;&lt;/p&gt; &lt;p&gt;Thread by the first author: &lt;a href="https://x.com/Dorialexander/status/1930249894712717744"&gt;https://x.com/Dorialexander/status/1930249894712717744&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2506.01732"&gt;https://arxiv.org/abs/2506.01732&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Initial-Image-1015"&gt; /u/Initial-Image-1015 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l1wcpiqhyw4f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l35rp1/common_corpus_the_largest_collection_of_ethical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l35rp1/common_corpus_the_largest_collection_of_ethical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T13:37:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l318di</id>
    <title>Shisa V2 405B: The strongest model ever built in Japan! (JA/EN)</title>
    <updated>2025-06-04T09:32:34+00:00</updated>
    <author>
      <name>/u/randomfoo2</name>
      <uri>https://old.reddit.com/user/randomfoo2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l318di/shisa_v2_405b_the_strongest_model_ever_built_in/"&gt; &lt;img alt="Shisa V2 405B: The strongest model ever built in Japan! (JA/EN)" src="https://b.thumbs.redditmedia.com/72uFsuI12iQ2cSozyTM-SHrwJy10OoGPWLvGrCoRLbg.jpg" title="Shisa V2 405B: The strongest model ever built in Japan! (JA/EN)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, so we've released the latest member of our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jz2lll/shisa_v2_a_family_of_new_jaen_bilingual_models/"&gt;Shisa V2&lt;/a&gt; family of open bilingual (Japanes/English) models: &lt;a href="https://shisa.ai/posts/shisa-v2-405b/"&gt;Shisa V2 405B&lt;/a&gt;!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama 3.1 405B Fine Tune, inherits the Llama 3.1 license&lt;/li&gt; &lt;li&gt;Not just our JA mix but also additional KO + ZH-TW to augment 405B's native multilingual&lt;/li&gt; &lt;li&gt;Beats GPT-4 &amp;amp; GPT-4 Turbo in JA/EN, matches latest GPT-4o and DeepSeek-V3 in JA MT-Bench (it's not a reasoning or code model, but Êó•Êú¨Ë™û‰∏äÊâã!)&lt;/li&gt; &lt;li&gt;Based on our evals, it's is w/o a doubt the strongest model to ever be released from Japan, beating out the efforts of bigco's etc. Tiny teams can do great things leveraging open models!&lt;/li&gt; &lt;li&gt;Quants and end-point available for testing&lt;/li&gt; &lt;li&gt;Super cute doggos:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3suc49zzqv4f1.jpg?width=900&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=732f3e703e207d4d9a4b1e750e3b793f061a811f"&gt;Shisa V2 405B Êó•Êú¨Ë™û‰∏äÊâãÔºÅ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For the &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; crowd:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Of course full model weights at &lt;a href="https://huggingface.co/shisa-ai/shisa-v2-llama3.1-405b"&gt;shisa-ai/shisa-v2-llama-3.1-405b&lt;/a&gt; but also a range of GGUFs in a repo as well: &lt;a href="https://huggingface.co/shisa-ai/shisa-v2-llama3.1-405b-GGUF"&gt;shisa-ai/shisa-v2-llama3.1-405b-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;These GGUFs are all (except the Q8_0) imatrixed w/ a calibration set based on our (Apache 2.0, also available for download) core Shisa V2 SFT dataset. They range from 100GB for the IQ2_XXS to 402GB for the Q8_0. Thanks to ubergarm for the pointers for what the gguf quanting landscape looks like in 2025!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out our initially linked blog post for all the deets + a full set of overview slides in JA and EN versions. Explains how we did our testing, training, dataset creation, and all kinds of little fun tidbits like:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vp7we685rv4f1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ebbb9ad61d82ad55b9bceb9db3493e4bc038d80"&gt;Top Notch Japanese&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xatqzpz7rv4f1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b7a676b0479b34e2bca1af9d5d05d37b8cf32e7"&gt;When your model is significantly better than GPT 4 it just gives you 10s across the board üòÇ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;While I know these models are big and maybe not directly relevant to people here, we've now tested our dataset on a huge range of base models from 7B to 405B and can conclude it can basically make any model mo-betta' at Japanese (without negatively impacting English or other capabilities!).&lt;/p&gt; &lt;p&gt;This whole process has been basically my whole year, so happy to finally get it out there and of course, answer any questions anyone might have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomfoo2"&gt; /u/randomfoo2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l318di/shisa_v2_405b_the_strongest_model_ever_built_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l318di/shisa_v2_405b_the_strongest_model_ever_built_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l318di/shisa_v2_405b_the_strongest_model_ever_built_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T09:32:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3s5wh</id>
    <title>Interactive Results Browser for Misguided Attention Eval</title>
    <updated>2025-06-05T06:24:23+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thanks to Gemini 2.5 pro, there is now an&lt;a href="https://cpldcpu.github.io/MisguidedAttention/"&gt; interactive results browser&lt;/a&gt; for the &lt;a href="https://github.com/cpldcpu/MisguidedAttention"&gt;misguided attention eval&lt;/a&gt;. The matrix shows how each model fared for every prompt. You can click on a cell to see the actual responses.&lt;/p&gt; &lt;p&gt;The last wave of new models got significantly better at correctly responding to the prompts. Especially reasoning models.&lt;/p&gt; &lt;p&gt;Currently, DS-R1-0528 is leading the pack.&lt;/p&gt; &lt;p&gt;Claude Opus 4 is almost at the top of the chart even in non-thinking mode. I haven't run it in thinking mode yet (it's not available on openrouter), but I assume that it would jump ahead of R1. Likewise, O3 also remains untested.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3s5wh/interactive_results_browser_for_misguided/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3s5wh/interactive_results_browser_for_misguided/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3s5wh/interactive_results_browser_for_misguided/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-05T06:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3fdv3</id>
    <title>I made an LLM tool to let you search offline Wikipedia/StackExchange/DevDocs ZIM files (llm-tools-kiwix, works with Python &amp; LLM cli)</title>
    <updated>2025-06-04T19:57:22+00:00</updated>
    <author>
      <name>/u/mozanunal</name>
      <uri>https://old.reddit.com/user/mozanunal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just released &lt;a href="https://github.com/mozanunal/llm-tools-kiwix"&gt;&lt;code&gt;llm-tools-kiwix&lt;/code&gt;&lt;/a&gt;, a plugin for the &lt;a href="https://llm.datasette.io/"&gt;&lt;code&gt;llm&lt;/code&gt; CLI&lt;/a&gt; and Python that lets LLMs read and search offline ZIM archives (i.e., Wikipedia, DevDocs, StackExchange, and more) &lt;strong&gt;totally offline&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt;&lt;br /&gt; A lot of local LLM use cases could benefit from RAG using big knowledge bases, but most solutions require network calls. Kiwix makes it possible to have huge websites (Wikipedia, StackExchange, etc.) stored as &lt;code&gt;.zim&lt;/code&gt; files on your disk. Now you can let your LLM access those‚Äîno Internet needed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does it do?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Discovers your ZIM files&lt;/strong&gt; (in the cwd or a folder via &lt;code&gt;KIWIX_HOME&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Exposes tools so the LLM can search articles or read full content&lt;/li&gt; &lt;li&gt;Works on the command line or from Python (supports GPT-4o, ollama, Llama.cpp, etc via the &lt;code&gt;llm&lt;/code&gt; tool)&lt;/li&gt; &lt;li&gt;No cloud or browser needed, just pure local retrieval&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example use-case:&lt;/strong&gt;&lt;br /&gt; Say you have &lt;code&gt;wikipedia_en_all_nopic_2023-10.zim&lt;/code&gt; downloaded and want your LLM to answer questions using it:&lt;/p&gt; &lt;p&gt;&lt;code&gt; llm install llm-tools-kiwix # (one-time setup) llm -m ollama:llama3 --tool kiwix_search_and_collect \ &amp;quot;Summarize notable attempts at human-powered flight from Wikipedia.&amp;quot; \ --tools-debug &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Or use the Docker/DevDocs ZIMs for local developer documentation search.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to try:&lt;/strong&gt; 1. Download some ZIM files from &lt;a href="https://download.kiwix.org/zim/"&gt;https://download.kiwix.org/zim/&lt;/a&gt; 2. Put them in your project dir, or set &lt;code&gt;KIWIX_HOME&lt;/code&gt; 3. &lt;code&gt;llm install llm-tools-kiwix&lt;/code&gt; 4. Use tool mode as above!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open source, Apache 2.0.&lt;/strong&gt;&lt;br /&gt; Repo + docs: &lt;a href="https://github.com/mozanunal/llm-tools-kiwix"&gt;https://github.com/mozanunal/llm-tools-kiwix&lt;/a&gt;&lt;br /&gt; PyPI: &lt;a href="https://pypi.org/project/llm-tools-kiwix/"&gt;https://pypi.org/project/llm-tools-kiwix/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think! Would love feedback, bug reports, or ideas for more offline tools.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mozanunal"&gt; /u/mozanunal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3fdv3/i_made_an_llm_tool_to_let_you_search_offline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3fdv3/i_made_an_llm_tool_to_let_you_search_offline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3fdv3/i_made_an_llm_tool_to_let_you_search_offline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T19:57:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3wloi</id>
    <title>Check out this new VSCode Extension! Query multiple BitNet servers from within GitHub Copilot via the Model Context Protocol all locally!</title>
    <updated>2025-06-05T11:17:31+00:00</updated>
    <author>
      <name>/u/ufos1111</name>
      <uri>https://old.reddit.com/user/ufos1111</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://marketplace.visualstudio.com/items?itemName=nftea-gallery.bitnet-vscode-extension"&gt;https://marketplace.visualstudio.com/items?itemName=nftea-gallery.bitnet-vscode-extension&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/grctest/BitNet-VSCode-Extension"&gt;https://github.com/grctest/BitNet-VSCode-Extension&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/grctest/FastAPI-BitNet"&gt;https://github.com/grctest/FastAPI-BitNet&lt;/a&gt; (updated to support llama's server executables &amp;amp; uses fastapi-mcp package to expose its endpoints to copilot)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ufos1111"&gt; /u/ufos1111 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3wloi/check_out_this_new_vscode_extension_query/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3wloi/check_out_this_new_vscode_extension_query/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3wloi/check_out_this_new_vscode_extension_query/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-05T11:17:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1l39ea3</id>
    <title>Drummer's Cydonia 24B v3 - A Mistral 24B 2503 finetune!</title>
    <updated>2025-06-04T16:03:35+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l39ea3/drummers_cydonia_24b_v3_a_mistral_24b_2503/"&gt; &lt;img alt="Drummer's Cydonia 24B v3 - A Mistral 24B 2503 finetune!" src="https://external-preview.redd.it/v0smBaFAfIOYhWsjTXmZvmibfthD29DfOmGvXCsBLOk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=21f1147df642a26ce4a38f94f61704c291faa086" title="Drummer's Cydonia 24B v3 - A Mistral 24B 2503 finetune!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Survey Time: I'm working on Skyfall v3 but need opinions on the upscale size. 31B sounds comfy for a 24GB setup? Do you have an upper/lower bound in mind for that range?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l39ea3/drummers_cydonia_24b_v3_a_mistral_24b_2503/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l39ea3/drummers_cydonia_24b_v3_a_mistral_24b_2503/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T16:03:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3c8is</id>
    <title>GRMR-V3: A set of models for reliable grammar correction.</title>
    <updated>2025-06-04T17:53:41+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let's face it: You don't need big models like 32B, or medium sized models like 8B for grammar correction. Smaller models, like &amp;lt;1B parameters, usually miss some grammatical nuances that require more context. So I've created a set of 1B-4B fine-tuned models specialized in just doing that: fixing grammar.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/qingy2024/grmr-v3-models-683e6a27b42e4eb0e950fbdd"&gt;Models&lt;/a&gt;: GRMR-V3 (1B, 1.2B, 1.7B, 3B, 4B, and 4.3B)&lt;br /&gt; &lt;a href="https://huggingface.co/collections/qingy2024/grmr-v3-ggufs-684083beb5be4b136e5fbc68"&gt;GGUFs here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Models don't really work with multiple messages, it just looks at your first message.&lt;br /&gt; - It works in llama.cpp, vllm, basically any inference engine.&lt;br /&gt; - Make sure you use the sampler settings in the model card, I know Open WebUI has different defaults.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example Input/Output:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Original Text&lt;/th&gt; &lt;th align="left"&gt;Corrected Text&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;i dont know weather to bring a umbrella today&lt;/td&gt; &lt;td align="left"&gt;I don't know whether to bring an umbrella today.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3c8is/grmrv3_a_set_of_models_for_reliable_grammar/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3c8is/grmrv3_a_set_of_models_for_reliable_grammar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3c8is/grmrv3_a_set_of_models_for_reliable_grammar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T17:53:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3i78l</id>
    <title>UPDATE: Inference needs nontrivial amount of PCIe bandwidth (8x RTX 3090 rig, tensor parallelism)</title>
    <updated>2025-06-04T21:51:11+00:00</updated>
    <author>
      <name>/u/pmur12</name>
      <uri>https://old.reddit.com/user/pmur12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A month ago I &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kds51e/inference_needs_nontrivial_amount_of_pcie/"&gt;complained&lt;/a&gt; that connecting 8 RTX 3090 with PCIe 3.0 x4 links is bad idea. I have upgraded my rig with better PCIe links and have an update with some numbers.&lt;/p&gt; &lt;p&gt;The upgrade: PCIe 3.0 -&amp;gt; 4.0, x4 width to x8 width. Used H12SSL with 16-core EPYC 7302. I didn't try the p2p nvidia drivers yet.&lt;/p&gt; &lt;p&gt;The numbers:&lt;/p&gt; &lt;p&gt;Bandwidth (p2pBandwidthLatencyTest, read):&lt;/p&gt; &lt;p&gt;Before: 1.6GB/s single direction&lt;/p&gt; &lt;p&gt;After: 6.1GB/s single direction&lt;/p&gt; &lt;p&gt;LLM:&lt;/p&gt; &lt;p&gt;Model: TechxGenus/Mistral-Large-Instruct-2411-AWQ&lt;/p&gt; &lt;p&gt;Before: ~25 t/s generation and ~100 t/s prefill on 80k context.&lt;/p&gt; &lt;p&gt;After: ~33 t/s generation and ~250 t/s prefill on 80k context.&lt;/p&gt; &lt;p&gt;Both of these were achieved running docker.io/lmsysorg/sglang:v0.4.6.post2-cu124&lt;/p&gt; &lt;p&gt;250t/s prefill makes me very happy. The LLM is finally fast enough to not choke on adding extra files to context when coding.&lt;/p&gt; &lt;p&gt;Options:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;environment: - TORCHINDUCTOR_CACHE_DIR=/root/cache/torchinductor_cache - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True command: - python3 - -m - sglang.launch_server - --host - 0.0.0.0 - --port - &amp;quot;8000&amp;quot; - --model-path - TechxGenus/Mistral-Large-Instruct-2411-AWQ - --sleep-on-idle - --tensor-parallel-size - &amp;quot;8&amp;quot; - --mem-fraction-static - &amp;quot;0.90&amp;quot; - --chunked-prefill-size - &amp;quot;2048&amp;quot; - --context-length - &amp;quot;128000&amp;quot; - --cuda-graph-max-bs - &amp;quot;8&amp;quot; - --enable-torch-compile - --json-model-override-args - '{ &amp;quot;rope_scaling&amp;quot;: {&amp;quot;factor&amp;quot;: 4.0, &amp;quot;original_max_position_embeddings&amp;quot;: 32768, &amp;quot;type&amp;quot;: &amp;quot;yarn&amp;quot; }}' &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmur12"&gt; /u/pmur12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3i78l/update_inference_needs_nontrivial_amount_of_pcie/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3i78l/update_inference_needs_nontrivial_amount_of_pcie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3i78l/update_inference_needs_nontrivial_amount_of_pcie/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T21:51:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3lrdq</id>
    <title>My former go-to misguided attention prompt in shambles (DS-V3-0528)</title>
    <updated>2025-06-05T00:32:47+00:00</updated>
    <author>
      <name>/u/nomorebuttsplz</name>
      <uri>https://old.reddit.com/user/nomorebuttsplz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3lrdq/my_former_goto_misguided_attention_prompt_in/"&gt; &lt;img alt="My former go-to misguided attention prompt in shambles (DS-V3-0528)" src="https://preview.redd.it/8uil7xc0705f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9ce87f679d955eba51793d99329aa97280deb77" title="My former go-to misguided attention prompt in shambles (DS-V3-0528)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last year, &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1h8g8v3/a_test_prompt_the_new_llama_33_70b_struggles_with/"&gt;this prompt&lt;/a&gt; was useful to differentiate the smartest models from the rest. This year, the AI not only doesn't fall for it but realizes it's being tested and how it's being tested.&lt;/p&gt; &lt;p&gt;I'm liking 0528's new chain of thought where it tries to read the user's intentions. Makes collaboration easier when you can track its &amp;quot;intentions&amp;quot; and it can track yours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomorebuttsplz"&gt; /u/nomorebuttsplz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8uil7xc0705f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3lrdq/my_former_goto_misguided_attention_prompt_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3lrdq/my_former_goto_misguided_attention_prompt_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-05T00:32:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3tby7</id>
    <title>VLLM with 4x7900xtx with Qwen3-235B-A22B-UD-Q2_K_XL</title>
    <updated>2025-06-05T07:41:40+00:00</updated>
    <author>
      <name>/u/djdeniro</name>
      <uri>https://old.reddit.com/user/djdeniro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Reddit!&lt;/p&gt; &lt;p&gt;Our &amp;quot;AI&amp;quot; computer now has 4x 7900 XTX and 1x 7800 XT.&lt;/p&gt; &lt;p&gt;Llama-server works well, and we successfully launched Qwen3-235B-A22B-UD-Q2_K_XL with a 40,960 context length.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;GPU&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;Input&lt;/th&gt; &lt;th align="left"&gt;OutPut&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;4x7900 xtx&lt;/td&gt; &lt;td align="left"&gt;HIP llama-server, -fa&lt;/td&gt; &lt;td align="left"&gt;160 t/s (356 tokens)&lt;/td&gt; &lt;td align="left"&gt;20 t/s (328 tokens)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4x7900 xtx&lt;/td&gt; &lt;td align="left"&gt;HIP llama-server, -fa --parallel 2 for 2 request in one time&lt;/td&gt; &lt;td align="left"&gt;130 t/s (58t/s + 72t//s)&lt;/td&gt; &lt;td align="left"&gt;13.5 t/s (7t/s + 6.5t/s)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3x7900 xtx + 1x7800xt&lt;/td&gt; &lt;td align="left"&gt;HIP llama-server, -fa&lt;/td&gt; &lt;td align="left"&gt;...&lt;/td&gt; &lt;td align="left"&gt;16-18 token/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Question to discuss:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Is it possible to run this model from Unsloth AI faster using VLLM on amd or no ways to launch GGUF?&lt;/p&gt; &lt;p&gt;Can we offload layers to each GPU in a smarter way?&lt;/p&gt; &lt;p&gt;If you've run a similar model (even on different GPUs), please share your results.&lt;/p&gt; &lt;p&gt;If you're considering setting up a test (perhaps even on AMD hardware), feel free to ask any relevant questions here.&lt;/p&gt; &lt;p&gt;___&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-swap config models: &amp;quot;qwen3-235b-a22b:Q2_K_XL&amp;quot;: env: - &amp;quot;HSA_OVERRIDE_GFX_VERSION=11.0.0&amp;quot; - &amp;quot;CUDA_VISIBLE_DEVICES=0,1,2,3,4&amp;quot; - &amp;quot;HIP_VISIBLE_DEVICES=0,1,2,3,4&amp;quot; - &amp;quot;AMD_DIRECT_DISPATCH=1&amp;quot; aliases: - Qwen3-235B-A22B-Thinking cmd: &amp;gt; /opt/llama-cpp/llama-hip/build/bin/llama-server --model /mnt/tb_disk/llm/models/235B-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf --main-gpu 0 --temp 0.6 --top-k 20 --min-p 0.0 --top-p 0.95 --gpu-layers 99 --tensor-split 22.5,22,22,22,0 --ctx-size 40960 --host 0.0.0.0 --port ${PORT} --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn --device ROCm0,ROCm1,ROCm2,ROCm3,ROCm4 --parallel 2 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/djdeniro"&gt; /u/djdeniro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3tby7/vllm_with_4x7900xtx_with_qwen3235ba22budq2_k_xl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3tby7/vllm_with_4x7900xtx_with_qwen3235ba22budq2_k_xl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3tby7/vllm_with_4x7900xtx_with_qwen3235ba22budq2_k_xl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-05T07:41:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3op8b</id>
    <title>why isn‚Äôt anyone building legit tools with local LLMs?</title>
    <updated>2025-06-05T03:00:37+00:00</updated>
    <author>
      <name>/u/mindfulbyte</name>
      <uri>https://old.reddit.com/user/mindfulbyte</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;asked this in a recent comment but curious what others think.&lt;/p&gt; &lt;p&gt;i could be missing it, but why aren‚Äôt more niche on device products being built? not talking wrappers or playgrounds, i mean real, useful tools powered by local LLMs.&lt;/p&gt; &lt;p&gt;models are getting small enough, 3B and below is workable for a lot of tasks.&lt;/p&gt; &lt;p&gt;the potential upside is clear to me, so what‚Äôs the blocker? compute? distribution? user experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mindfulbyte"&gt; /u/mindfulbyte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3op8b/why_isnt_anyone_building_legit_tools_with_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3op8b/why_isnt_anyone_building_legit_tools_with_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3op8b/why_isnt_anyone_building_legit_tools_with_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-05T03:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1l352wk</id>
    <title>AMA ‚Äì I‚Äôve built 7 commercial RAG projects. Got tired of copy-pasting boilerplate, so we open-sourced our internal stack.</title>
    <updated>2025-06-04T13:06:03+00:00</updated>
    <author>
      <name>/u/Loud_Picture_1877</name>
      <uri>https://old.reddit.com/user/Loud_Picture_1877</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I‚Äôm a senior tech lead with 8+ years of experience, and for the last ~3 I‚Äôve been knee-deep in building LLM-powered systems ‚Äî RAG pipelines, agentic apps, text2SQL engines. We‚Äôve shipped real products in manufacturing, sports analytics, NGOs, legal‚Ä¶ you name it.&lt;/p&gt; &lt;p&gt;After doing this &lt;em&gt;again and again&lt;/em&gt;, I got tired of the same story: building ingestion from scratch, duct-taping vector DBs, dealing with prompt spaghetti, and debugging hallucinations without proper logs.&lt;/p&gt; &lt;p&gt;So we built &lt;a href="https://github.com/deepsense-ai/ragbits"&gt;&lt;strong&gt;ragbits&lt;/strong&gt;&lt;/a&gt; ‚Äî a toolbox of reliable, type-safe, modular building blocks for GenAI apps. What started as an internal accelerator is now &lt;strong&gt;fully open-sourced (v1.0.0)&lt;/strong&gt; and ready to use.&lt;/p&gt; &lt;p&gt;Why we built it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We wanted &lt;em&gt;repeatability&lt;/em&gt;. RAG isn‚Äôt magic ‚Äî but building it cleanly every time takes effort.&lt;/li&gt; &lt;li&gt;We needed to &lt;em&gt;move fast&lt;/em&gt; for PoCs, without sacrificing structure.&lt;/li&gt; &lt;li&gt;We hated black boxes ‚Äî ragbits integrates easily with your observability stack (OpenTelemetry, CLI debugging, prompt testing).&lt;/li&gt; &lt;li&gt;And most importantly, we wanted to scale apps without turning the codebase into a dumpster fire.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm happy to answer questions about RAG, our approach, gotchas from real deployments, or the internals of ragbits. No fluff ‚Äî just real lessons from shipping LLM systems in production.&lt;/p&gt; &lt;p&gt;We‚Äôre looking for feedback, contributors, and people who want to build better GenAI apps. If that sounds like you, take &lt;a href="https://github.com/deepsense-ai/ragbits"&gt;ragbits&lt;/a&gt; for a spin. &lt;/p&gt; &lt;p&gt;Let‚Äôs talk üëá&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud_Picture_1877"&gt; /u/Loud_Picture_1877 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l352wk/ama_ive_built_7_commercial_rag_projects_got_tired/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l352wk/ama_ive_built_7_commercial_rag_projects_got_tired/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l352wk/ama_ive_built_7_commercial_rag_projects_got_tired/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T13:06:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3p1f0</id>
    <title>OpenAI should open source GPT3.5 turbo</title>
    <updated>2025-06-05T03:18:48+00:00</updated>
    <author>
      <name>/u/Expensive-Apricot-25</name>
      <uri>https://old.reddit.com/user/Expensive-Apricot-25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dont have a real point here, just the title, food for thought.&lt;/p&gt; &lt;p&gt;I think it would be a pretty cool thing to do. at this point it's extremely out of date, so they wouldn't be loosing any &amp;quot;edge&amp;quot;, it would just be a cool thing to do/have and would be a nice throwback.&lt;/p&gt; &lt;p&gt;openAI's 10th year anniversary is coming up in december, would be a pretty cool thing to do, just sayin.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expensive-Apricot-25"&gt; /u/Expensive-Apricot-25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3p1f0/openai_should_open_source_gpt35_turbo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3p1f0/openai_should_open_source_gpt35_turbo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3p1f0/openai_should_open_source_gpt35_turbo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-05T03:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3u7e9</id>
    <title>I organized a 100-game Town of Salem competition featuring best models as players. Game logs are available too.</title>
    <updated>2025-06-05T08:43:52+00:00</updated>
    <author>
      <name>/u/kyazoglu</name>
      <uri>https://old.reddit.com/user/kyazoglu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3u7e9/i_organized_a_100game_town_of_salem_competition/"&gt; &lt;img alt="I organized a 100-game Town of Salem competition featuring best models as players. Game logs are available too." src="https://b.thumbs.redditmedia.com/tsdMiaeHEwu-rxDpQaHC_byhirYy_QZP1vzWdIvOaqs.jpg" title="I organized a 100-game Town of Salem competition featuring best models as players. Game logs are available too." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As many of you probably know, Town of Salem is a popular game. If you don't know what I'm talking about, you can read the game_rules.yaml in the repo. My personal preference has always been to moderate rather than play among friends. Two weeks ago, I had the idea to make LLMs play this game to have fun and see who is the best. Imo, this is a great way to measure LLM capabilities across several crucial areas: contextual understanding, managing information privacy, developing sophisticated strategies, employing deception, and demonstrating persuasive skills. I'll be sharing charts based on a simulation of 100 games. For a deeper dive into the methodology, more detailed results and more charts, please visit the repo &lt;a href="https://github.com/summersonnn/Town-Of-Salem-with-LLMs"&gt;https://github.com/summersonnn/Town-Of-Salem-with-LLMs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Total dollars spent: ~60$ - half of which spent on new Claude models. Looking at the results, I see those 30$ spent for nothing :D&lt;/p&gt; &lt;p&gt;Vampire points are calculated as follows :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If vampires win and a vampire is alive at the end, that vampire earns 1 point&lt;/li&gt; &lt;li&gt;If vampires win but the vampire is dead, they receive 0.5 points&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Peasant survival rate is calculated as follows: sum the total number of rounds survived across all games that this model/player has participated in and divide by the total number of rounds played in those same games. Win Ratios are self-explanatory.&lt;/p&gt; &lt;p&gt;Quick observations: - New Deepseek, even the distilled Qwen is very good at this game. - Claude models and Grok are worst - GPT 4.1 is also very successful. - Gemini models are average in general but performs best when peasant&lt;/p&gt; &lt;p&gt;Overall win ratios: - Vampires win ratio: 34/100 : 34% - Peasants win ratio: 45/100 : 45% - Clown win ratio: 21/100 : 21%&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyazoglu"&gt; /u/kyazoglu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1l3u7e9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3u7e9/i_organized_a_100game_town_of_salem_competition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3u7e9/i_organized_a_100game_town_of_salem_competition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-05T08:43:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3dhjx</id>
    <title>Real-time conversational AI running 100% locally in-browser on WebGPU</title>
    <updated>2025-06-04T18:42:30+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3dhjx/realtime_conversational_ai_running_100_locally/"&gt; &lt;img alt="Real-time conversational AI running 100% locally in-browser on WebGPU" src="https://external-preview.redd.it/MmRtc2I4c3JneTRmMb-Z1L0lQHsk-1t-PBURRQBQD36a7CaPOYYP63NLiwlg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8486b1316a63e3a8c818e1c9ffa5b1b35e9abcfe" title="Real-time conversational AI running 100% locally in-browser on WebGPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t419j8srgy4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3dhjx/realtime_conversational_ai_running_100_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3dhjx/realtime_conversational_ai_running_100_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T18:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3vt95</id>
    <title>New embedding model "Qwen3-Embedding-0.6B-GGUF" just dropped.</title>
    <updated>2025-06-05T10:30:53+00:00</updated>
    <author>
      <name>/u/Proto_Particle</name>
      <uri>https://old.reddit.com/user/Proto_Particle</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3vt95/new_embedding_model_qwen3embedding06bgguf_just/"&gt; &lt;img alt="New embedding model &amp;quot;Qwen3-Embedding-0.6B-GGUF&amp;quot; just dropped." src="https://external-preview.redd.it/nCFCX9SJ8G9lwL3THBeDPCNNzee25aFLCHH5cPLrrSM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7073b11dd8c3ebaa999dbf1000e83f46f243a01e" title="New embedding model &amp;quot;Qwen3-Embedding-0.6B-GGUF&amp;quot; just dropped." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone tested it yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proto_Particle"&gt; /u/Proto_Particle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Embedding-0.6B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3vt95/new_embedding_model_qwen3embedding06bgguf_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3vt95/new_embedding_model_qwen3embedding06bgguf_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-05T10:30:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3niws</id>
    <title>After court order, OpenAI is now preserving all ChatGPT and API logs</title>
    <updated>2025-06-05T02:00:22+00:00</updated>
    <author>
      <name>/u/iGermanProd</name>
      <uri>https://old.reddit.com/user/iGermanProd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3niws/after_court_order_openai_is_now_preserving_all/"&gt; &lt;img alt="After court order, OpenAI is now preserving all ChatGPT and API logs" src="https://external-preview.redd.it/X_hEWYjElFi1bBhWlW8lpN7Rp7cf6NXMmqc3u_L3ogI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f187439e139ab8a2daded4a0c65829ac68188bda" title="After court order, OpenAI is now preserving all ChatGPT and API logs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;OpenAI could have taken steps to anonymize the chat logs but chose not to, only making an argument for why it &amp;quot;would not&amp;quot; be able to segregate data, rather than explaining why it &amp;quot;can‚Äôt.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Surprising absolutely nobody, except maybe ChatGPT users, OpenAI and the United States own your data and can do whatever they want with it. ClosedAI have the audacity to pretend they're the good guys, despite not doing anything tech-wise to prevent this from being possible. My personal opinion is that Gemini, Claude, et al. are next. Yet another win for open weights. Own your tech, own your data.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iGermanProd"&gt; /u/iGermanProd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arstechnica.com/tech-policy/2025/06/openai-says-court-forcing-it-to-save-all-chatgpt-logs-is-a-privacy-nightmare/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3niws/after_court_order_openai_is_now_preserving_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3niws/after_court_order_openai_is_now_preserving_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-05T02:00:22+00:00</published>
  </entry>
</feed>
