<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-19T23:06:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jezoa8</id>
    <title>I built an Opensource Hybrid Reasoning LLM</title>
    <updated>2025-03-19T15:29:31+00:00</updated>
    <author>
      <name>/u/Altruistic-Tea-5612</name>
      <uri>https://old.reddit.com/user/Altruistic-Tea-5612</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jezoa8/i_built_an_opensource_hybrid_reasoning_llm/"&gt; &lt;img alt="I built an Opensource Hybrid Reasoning LLM" src="https://external-preview.redd.it/g3zlUvCeapXPuEZ6nIvL25M9kJ9WCIOgMAou5ESCEbo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3230cab2698ea7f0b474a9ed4cde18b5d626000" title="I built an Opensource Hybrid Reasoning LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built this model called Apollo which is a Hybrid reasoner built based on Qwen using mergekit and this is an experiment to answer a question in my mind can we build a LLM model which can answer simple questions quicker and think for a while to answer complex questions and I attached eval numbers here and you can find gguf in attached repo and I recommend people here to try this model and let me know your feedback&lt;/p&gt; &lt;p&gt;repo: &lt;a href="https://huggingface.co/rootxhacker/Apollo-v3-32B"&gt;https://huggingface.co/rootxhacker/Apollo-v3-32B&lt;/a&gt;&lt;br /&gt; gguf: &lt;a href="https://huggingface.co/mradermacher/Apollo-v3-32B-GGUF"&gt;https://huggingface.co/mradermacher/Apollo-v3-32B-GGUF&lt;/a&gt;&lt;br /&gt; blog: &lt;a href="https://medium.com/@harishhacker3010/making-opensource-hybrid-reasoner-llm-to-build-better-rags-4364418ef7c4"&gt;https://medium.com/@harishhacker3010/making-opensource-hybrid-reasoner-llm-to-build-better-rags-4364418ef7c4&lt;/a&gt;&lt;br /&gt; I found this model this good for building RAGs and I use this for RAG&lt;/p&gt; &lt;p&gt;if anyone over here found useful and ran eval against benchmarks do definitely share to me I will credit your work and add them into article&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yenfdompynpe1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a82c07f17b2e36173bbba01168194326a1ff295"&gt;https://preview.redd.it/yenfdompynpe1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a82c07f17b2e36173bbba01168194326a1ff295&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic-Tea-5612"&gt; /u/Altruistic-Tea-5612 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jezoa8/i_built_an_opensource_hybrid_reasoning_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jezoa8/i_built_an_opensource_hybrid_reasoning_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jezoa8/i_built_an_opensource_hybrid_reasoning_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T15:29:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf2sam</id>
    <title>SoftWhisper ‚Äì easy audio to text transcription ‚Äì test needed</title>
    <updated>2025-03-19T17:39:36+00:00</updated>
    <author>
      <name>/u/Substantial_Swan_144</name>
      <uri>https://old.reddit.com/user/Substantial_Swan_144</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf2sam/softwhisper_easy_audio_to_text_transcription_test/"&gt; &lt;img alt="SoftWhisper ‚Äì easy audio to text transcription ‚Äì test needed" src="https://external-preview.redd.it/IhxxiwT4HxvbiUbyiDU5qic4dobnwoYx_6bV0uOjFAI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9105f19c6a03995cc3644c4aecb46fddd468a46e" title="SoftWhisper ‚Äì easy audio to text transcription ‚Äì test needed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, Redditers,&lt;/p&gt; &lt;p&gt;I have recently created an audio to text piece of software which tries to be as easy to use as possible: SoftWhisper. The current implementation can transcribe 2 hours in 2 minutes if you use GPU acceleration, and I need your help.&lt;/p&gt; &lt;p&gt;While I have released a build with GPU for AMD, NVIDIA and Intel acceleration, some users with NVIDIA cards have been reporting the program silently fails. This is why I created a CUDA-enabled build specifically for them.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/27bvxjhinope1.png?width=2008&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98f71c5ecfebbc0ececaac9626aae1bfce8df240"&gt;https://preview.redd.it/27bvxjhinope1.png?width=2008&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98f71c5ecfebbc0ececaac9626aae1bfce8df240&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can find more about the project here: &lt;a href="https://github.com/NullMagic2/SoftWhisper/releases/tag/March-2025"&gt;https://github.com/NullMagic2/SoftWhisper/releases/tag/March-2025&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you have an NVIDIA card, we need you! Help us test the NVIDIA build and tell us if it works: &lt;a href="https://github.com/NullMagic2/SoftWhisper/releases/download/March-2025/SoftWhisper.March.2025.NVIDIA.CUDA.support.zip"&gt;https://github.com/NullMagic2/SoftWhisper/releases/download/March-2025/SoftWhisper.March.2025.NVIDIA.CUDA.support.zip&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Your help will be much appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Swan_144"&gt; /u/Substantial_Swan_144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf2sam/softwhisper_easy_audio_to_text_transcription_test/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf2sam/softwhisper_easy_audio_to_text_transcription_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf2sam/softwhisper_easy_audio_to_text_transcription_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T17:39:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1je6ns1</id>
    <title>Meta talks about us and open source source AI for over 1 Billion downloads</title>
    <updated>2025-03-18T14:46:25+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je6ns1/meta_talks_about_us_and_open_source_source_ai_for/"&gt; &lt;img alt="Meta talks about us and open source source AI for over 1 Billion downloads" src="https://preview.redd.it/gcql3piongpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58b8393e9781f3853aac114d10af307ef017ca59" title="Meta talks about us and open source source AI for over 1 Billion downloads" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gcql3piongpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je6ns1/meta_talks_about_us_and_open_source_source_ai_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je6ns1/meta_talks_about_us_and_open_source_source_ai_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T14:46:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf4le1</id>
    <title>GitHub - fidecastro/llama-cpp-connector: Super simple Python connectors for llama.cpp, including vision models (Gemma 3, Qwen2-VL)</title>
    <updated>2025-03-19T18:53:56+00:00</updated>
    <author>
      <name>/u/Antique_Juggernaut_7</name>
      <uri>https://old.reddit.com/user/Antique_Juggernaut_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf4le1/github_fidecastrollamacppconnector_super_simple/"&gt; &lt;img alt="GitHub - fidecastro/llama-cpp-connector: Super simple Python connectors for llama.cpp, including vision models (Gemma 3, Qwen2-VL)" src="https://external-preview.redd.it/38h4lUk-J3GjI5OGY0Xw8aB3X6o4AmklS5Sgl-C6jW8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0cddb86c96492fb3e37ce54d94d1f46a6627b6e" title="GitHub - fidecastro/llama-cpp-connector: Super simple Python connectors for llama.cpp, including vision models (Gemma 3, Qwen2-VL)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Antique_Juggernaut_7"&gt; /u/Antique_Juggernaut_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/fidecastro/llama-cpp-connector"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf4le1/github_fidecastrollamacppconnector_super_simple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf4le1/github_fidecastrollamacppconnector_super_simple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T18:53:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jezrkg</id>
    <title>My Local Llama's</title>
    <updated>2025-03-19T15:33:13+00:00</updated>
    <author>
      <name>/u/getfitdotus</name>
      <uri>https://old.reddit.com/user/getfitdotus</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jezrkg/my_local_llamas/"&gt; &lt;img alt="My Local Llama's" src="https://b.thumbs.redditmedia.com/rWpvztVJ1vqHh62Svkwv_pRGX7IgMuLyn1_XIpZuoss.jpg" title="My Local Llama's" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just some local lab AI p0rn.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Top&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ThreadRipper&lt;/li&gt; &lt;li&gt;Quad 3090's&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Bottom&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Threadripper&lt;/li&gt; &lt;li&gt;Quad ada a6000's&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9j6utjw31ope1.jpg?width=4284&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=062eddb550a725843ce39be8a721adfd8a97bbd5"&gt;https://preview.redd.it/9j6utjw31ope1.jpg?width=4284&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=062eddb550a725843ce39be8a721adfd8a97bbd5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getfitdotus"&gt; /u/getfitdotus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jezrkg/my_local_llamas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jezrkg/my_local_llamas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jezrkg/my_local_llamas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T15:33:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jew0pk</id>
    <title>Is RTX 50xx series intentionally locked for compute / AI ?</title>
    <updated>2025-03-19T12:40:05+00:00</updated>
    <author>
      <name>/u/EmilPi</name>
      <uri>https://old.reddit.com/user/EmilPi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.videocardbenchmark.net/directCompute.html"&gt;https://www.videocardbenchmark.net/directCompute.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In this chart, all 50xx cards are below their 40xx counterparts. And in overall gamers-targeted benchmark &lt;a href="https://www.videocardbenchmark.net/high_end_gpus.html"&gt;https://www.videocardbenchmark.net/high_end_gpus.html&lt;/a&gt; 50xx has just a small edge over 40xx.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmilPi"&gt; /u/EmilPi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jew0pk/is_rtx_50xx_series_intentionally_locked_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jew0pk/is_rtx_50xx_series_intentionally_locked_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jew0pk/is_rtx_50xx_series_intentionally_locked_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T12:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf54a9</id>
    <title>Check out my little hobby project! This let's you watch two chatbots talk to one another and experiment with how different system prompts affect the conversation.</title>
    <updated>2025-03-19T19:14:57+00:00</updated>
    <author>
      <name>/u/6x10tothe23rd</name>
      <uri>https://old.reddit.com/user/6x10tothe23rd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;First of all, this was 90% vibe coded with Claude, although I held it's hand pretty closely the whole time. I've been more and more fascinated lately with how conversational and opinionated the latest models have been getting. I mainly built this to see how much better GPT-4.5 would be compared to the super tiny models I can actually run on my 3070 Ti (in a laptop so even less VRAM üò≠). I was actually pretty fascinated with some of the conversations that came out of it! Give it a shot yourself, and if anyone wants to help contribute you're more than welcome, I have little to no knowledge of web dev and usually work exclusively in python.&lt;/p&gt; &lt;p&gt;Here's the repo: &lt;a href="https://github.com/ParallelUniverseProgrammer/PiazzaArtificiale"&gt;https://github.com/ParallelUniverseProgrammer/PiazzaArtificiale&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you guys think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/6x10tothe23rd"&gt; /u/6x10tothe23rd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf54a9/check_out_my_little_hobby_project_this_lets_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf54a9/check_out_my_little_hobby_project_this_lets_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf54a9/check_out_my_little_hobby_project_this_lets_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T19:14:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jeqxvq</id>
    <title>Meta releases new model: VGGT (Visual Geometry Grounded Transformer.)</title>
    <updated>2025-03-19T06:43:01+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://vgg-t.github.io/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeqxvq/meta_releases_new_model_vggt_visual_geometry/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jeqxvq/meta_releases_new_model_vggt_visual_geometry/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T06:43:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf74rm</id>
    <title>Amoral Gemma 3 4b</title>
    <updated>2025-03-19T20:37:39+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted about my Gemma 3 12B yesterday, here is the 4b trained to be amoral&lt;br /&gt; &lt;a href="https://huggingface.co/soob3123/amoral-gemma3-4B"&gt;soob3123/amoral-gemma3-4B ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/soob3123/amoral-gemma3-4B-gguf"&gt;soob3123/amoral-gemma3-4B-gguf ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf74rm/amoral_gemma_3_4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf74rm/amoral_gemma_3_4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf74rm/amoral_gemma_3_4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T20:37:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jes9za</id>
    <title>Nemotron-Super-49B - Just MIGHT be a killer for creative writing. (24gb Vram)</title>
    <updated>2025-03-19T08:28:10+00:00</updated>
    <author>
      <name>/u/Majestical-psyche</name>
      <uri>https://old.reddit.com/user/Majestical-psyche</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;24 GB Vram, with IQ3 XXS (for 16k context, you can use XS for 8k)&lt;/p&gt; &lt;p&gt;I'm not sure if I got lucky or not, I usally don't post until I know it's good. BUT, luck or not - its creative potiental is there! And it's VERY creative and smart on my first try using it. And, it has really good context recall. Uncencored for NSFW stories too?&lt;/p&gt; &lt;p&gt;Ime, The new: Qwen, Mistral small, Gemma 3 are all dry and not creative, and not smart for stories... &lt;/p&gt; &lt;p&gt;I'm posting this because I would like feed back on your experince with this model for creative writing.&lt;/p&gt; &lt;p&gt;What is your experince like?&lt;/p&gt; &lt;p&gt;Thank you, my favorite community. ‚ù§Ô∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Majestical-psyche"&gt; /u/Majestical-psyche &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jes9za/nemotronsuper49b_just_might_be_a_killer_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jes9za/nemotronsuper49b_just_might_be_a_killer_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jes9za/nemotronsuper49b_just_might_be_a_killer_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T08:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf68qs</id>
    <title>LLM Agents are simply Graph ‚Äî Tutorial For Dummies</title>
    <updated>2025-03-19T20:00:42+00:00</updated>
    <author>
      <name>/u/Willing-Site-8137</name>
      <uri>https://old.reddit.com/user/Willing-Site-8137</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! I just posted a quick tutorial explaining how LLM agents (like OpenAI Agents, Pydantic AI, Manus AI, AutoGPT or PerplexityAI) are basically small graphs with loops and branches. For example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OpenAI Agents&lt;/strong&gt;: &lt;a href="https://github.com/openai/openai-agents-python/blob/48ff99bb736249e99251eb2c7ecf00237488c17a/src/agents/run.py#L119"&gt;run.py#L119&lt;/a&gt; for a workflow in graph.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pydantic Agents&lt;/strong&gt;: &lt;a href="https://github.com/pydantic/pydantic-ai/blob/4c0f384a0626299382c22a8e3372638885e18286/pydantic_ai_slim/pydantic_ai/_agent_graph.py#L779"&gt;_agent_graph.py#L779&lt;/a&gt; organizes steps in a graph.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Langchain&lt;/strong&gt;: &lt;a href="https://github.com/langchain-ai/langchain/blob/4d1d726e61ed58b39278903262d19bbe9f010772/libs/langchain/langchain/agents/agent_iterator.py#L174"&gt;agent_iterator.py#L174&lt;/a&gt; demonstrates the loop structure.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: &lt;a href="https://github.com/langchain-ai/langgraph/blob/24f7d7c4399e2c19b634ed7af0d551ad327e25d7/libs/cli/examples/graphs/agent.py#L56"&gt;agent.py#L56&lt;/a&gt; for a graph-based approach.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If all the hype has been confusing, this guide shows how they actually work under the hood, with simple examples. Check it out!&lt;/p&gt; &lt;p&gt;&lt;a href="https://zacharyhuang.substack.com/p/llm-agent-internal-as-a-graph-tutorial"&gt;https://zacharyhuang.substack.com/p/llm-agent-internal-as-a-graph-tutorial&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Willing-Site-8137"&gt; /u/Willing-Site-8137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf68qs/llm_agents_are_simply_graph_tutorial_for_dummies/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf68qs/llm_agents_are_simply_graph_tutorial_for_dummies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf68qs/llm_agents_are_simply_graph_tutorial_for_dummies/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T20:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jewjr8</id>
    <title>Sonnet 3.7 Max ‚Äì Max Spending, Max Regret</title>
    <updated>2025-03-19T13:07:33+00:00</updated>
    <author>
      <name>/u/ivkemilioner</name>
      <uri>https://old.reddit.com/user/ivkemilioner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sonnet 3.7 Max, thinking I'd max out my workflow.&lt;/p&gt; &lt;p&gt;Turns out, I also maxed out my budget and my anxiety levels.&lt;/p&gt; &lt;p&gt;Max is gambling:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The cost?&lt;/strong&gt; High.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The guarantee?&lt;/strong&gt; Only that you‚Äôll have extra troubleshooting to do.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivkemilioner"&gt; /u/ivkemilioner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jewjr8/sonnet_37_max_max_spending_max_regret/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jewjr8/sonnet_37_max_max_spending_max_regret/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jewjr8/sonnet_37_max_max_spending_max_regret/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T13:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf0wvz</id>
    <title>Benchmark results: PCIe4.0 1x/4x/8x/16x/NVLINK 3090/4090</title>
    <updated>2025-03-19T16:21:53+00:00</updated>
    <author>
      <name>/u/Ok-Anxiety8313</name>
      <uri>https://old.reddit.com/user/Ok-Anxiety8313</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf0wvz/benchmark_results_pcie40_1x4x8x16xnvlink_30904090/"&gt; &lt;img alt="Benchmark results: PCIe4.0 1x/4x/8x/16x/NVLINK 3090/4090" src="https://b.thumbs.redditmedia.com/e2ORfpJkaBHJmBvFT0y4nhdnkH66LMNIpgWCIUfayeY.jpg" title="Benchmark results: PCIe4.0 1x/4x/8x/16x/NVLINK 3090/4090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: I run a bunch of experiments of DDP training with different communication methods between GPUs and here are the results.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;NVLINK is generally so much better than PCIe for training, even at 16x channels.&lt;/li&gt; &lt;li&gt;PCIe 1x is absolute garbage for training. but 4/8/16 is decent at a large batch size&lt;/li&gt; &lt;li&gt;Go look at the plots i made.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I have been trying to figure out what kind of communication I absolutely need for my GPU rig. So I measured DDP training throughput for different number of PCIe 4.0 channels in 2x4090 and comparing PCIe vs. NVLINK in 2x3090 in DDP training of diffusion models. I run everything on &lt;a href="http://vast.ai"&gt;vast.ai&lt;/a&gt; instances.&lt;/p&gt; &lt;p&gt;The setting I used might be somewhat different from the &amp;quot;Local LLama&amp;quot;-specific needs, but I think it will still be relevant for many of you.&lt;/p&gt; &lt;p&gt;- Training only. These experiments do not necessarily say that much about inference efficiency.&lt;/p&gt; &lt;p&gt;- DDP Distributed approach. Meaning the whole model fits onto each gpu, forward pass and backward pass computed independently. After, the gradients are synchronised (this is where the communication bottleneck can happen) and finally we take an optimizer step. This should be the least communication-intensive method.&lt;/p&gt; &lt;p&gt;- SDXL diffusion training. This is an image generation model but you should have similar results with training LLMs of similar size (this one is 2.6B )&lt;/p&gt; &lt;p&gt;- Overall I believe these experiments are useful to anyone who wants to train or fine-tune using multiple 3090/4090s. I used DDP only, this is the parallelism with the least communication overhead so this implies that if communication speed matters for DDP training, it matters for any kind of distributed training.&lt;/p&gt; &lt;p&gt;I am reporting the batch time / batch size * #GPUs. I expect the single GPU to be optimal in this metric since there is no communication overhead and by multiplying by number of GPUs there is no advantage in number of flops in this metric. The question is how close can we get to single-gpu efficiency via dual-gpu.&lt;/p&gt; &lt;p&gt;Because DDP syncronizes gradients once per batch, the larger the batch size the longer forward/backward will take and the less relative importance will the communication overhead have. For the record this is done by accumulating gradients over minibatches, with no synchronization between gpus until the whole batch is done.&lt;/p&gt; &lt;p&gt;Now the promised plots.&lt;/p&gt; &lt;p&gt;First results. PCIe speed matters. 1x is really bad, the difference between 4x, 8x, 16x is small when we increase batch size&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sik67hdc7ope1.png?width=3551&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aeffde8924d9fe5c8c7e986c2a429b636d2cbae5"&gt;https://preview.redd.it/sik67hdc7ope1.png?width=3551&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aeffde8924d9fe5c8c7e986c2a429b636d2cbae5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ideally, for single GPU training, the PCIe speed should not matter, I attribute the differences to potential undervolting of the GPU by certain cloud providers or perhaps other system differences between servers. I am also not sure why there is not so much difference between 8x and 4x. Maybe different PCIe topology or something? Or perhaps different system specs that I did not measure can impact the communication speed.&lt;/p&gt; &lt;p&gt;Second set of results.&lt;/p&gt; &lt;p&gt;NVLINK is so much better than PCIe&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hv4p6mdh7ope1.png?width=3569&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3bc061d00df2104b34c2c94b7b3e700e9217bc6"&gt;https://preview.redd.it/hv4p6mdh7ope1.png?width=3569&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3bc061d00df2104b34c2c94b7b3e700e9217bc6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;These results are for 3090 not 4090 bc NVLINK is not available. For reference the orange line of the second plot would somewhat correspond to the red line of the first plot (PCIe 16x). The closer to the single-gpu lines the better and NVLINK get really close regardless of batch size, much more than PCIEe 16x. This points out the importance of NVLINK. Also I don't think you can connect more than 2 3090 at the same time with NVLINK so that is unfortunate :)&lt;/p&gt; &lt;p&gt;follow at &lt;a href="https://x.com/benetnu"&gt;https://x.com/benetnu&lt;/a&gt; :)&lt;/p&gt; &lt;p&gt;code for the experiments is at: &lt;a href="https://github.com/benoriol/diffusion_benchmark"&gt;https://github.com/benoriol/diffusion_benchmark&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Anxiety8313"&gt; /u/Ok-Anxiety8313 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf0wvz/benchmark_results_pcie40_1x4x8x16xnvlink_30904090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf0wvz/benchmark_results_pcie40_1x4x8x16xnvlink_30904090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf0wvz/benchmark_results_pcie40_1x4x8x16xnvlink_30904090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T16:21:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jeod23</id>
    <title>Still can't believe it. Got this A6000 (Ampere) beauty, working perfectly for 1300USD on Chile!</title>
    <updated>2025-03-19T03:49:34+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeod23/still_cant_believe_it_got_this_a6000_ampere/"&gt; &lt;img alt="Still can't believe it. Got this A6000 (Ampere) beauty, working perfectly for 1300USD on Chile!" src="https://b.thumbs.redditmedia.com/XFSJpkliPR8uY1jFq2k76gGRqvjuRsWCKVnvxNMCY1M.jpg" title="Still can't believe it. Got this A6000 (Ampere) beauty, working perfectly for 1300USD on Chile!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jeod23"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeod23/still_cant_believe_it_got_this_a6000_ampere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jeod23/still_cant_believe_it_got_this_a6000_ampere/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T03:49:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf4u9e</id>
    <title>Why don't we have non-Apple alternative to unified memory?</title>
    <updated>2025-03-19T19:03:47+00:00</updated>
    <author>
      <name>/u/This_Woodpecker_9163</name>
      <uri>https://old.reddit.com/user/This_Woodpecker_9163</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are we sleeping on this and allowing ourselves to be exploited by the GPU giants?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/This_Woodpecker_9163"&gt; /u/This_Woodpecker_9163 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf4u9e/why_dont_we_have_nonapple_alternative_to_unified/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf4u9e/why_dont_we_have_nonapple_alternative_to_unified/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf4u9e/why_dont_we_have_nonapple_alternative_to_unified/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T19:03:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jevseg</id>
    <title>New Multiview 3D Model by Stability AI</title>
    <updated>2025-03-19T12:27:23+00:00</updated>
    <author>
      <name>/u/EssayHealthy5075</name>
      <uri>https://old.reddit.com/user/EssayHealthy5075</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jevseg/new_multiview_3d_model_by_stability_ai/"&gt; &lt;img alt="New Multiview 3D Model by Stability AI" src="https://external-preview.redd.it/MjJ0cmhwMHUzbnBlMU3CkfuJ2R9TEEddx2aHL95I0ePLV3sEuU-hTYUsrCez.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0250d00697c362c042930fc3be758b09733fea28" title="New Multiview 3D Model by Stability AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This multi-view diffusion model transforms 2D images into immersive 3D videos with realistic depth and perspective‚Äîwithout complex reconstruction or scene-specific optimization.&lt;/p&gt; &lt;p&gt;The model generates 3D videos from a single input image or up to 32, following user-defined camera trajectories as well as 14 other dynamic camera paths, including 360¬∞, Lemniscate, Spiral, Dolly Zoom, Move, Pan, and Roll.&lt;/p&gt; &lt;p&gt;Stable Virtual Camera is currently in research preview. &lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://stability.ai/news/introducing-stable-virtual-camera-multi-view-video-generation-with-3d-camera-control"&gt;https://stability.ai/news/introducing-stable-virtual-camera-multi-view-video-generation-with-3d-camera-control&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Project Page: &lt;a href="https://stable-virtual-camera.github.io/"&gt;https://stable-virtual-camera.github.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://stability.ai/s/stable-virtual-camera.pdf"&gt;https://stability.ai/s/stable-virtual-camera.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model weights: &lt;a href="https://huggingface.co/stabilityai/stable-virtual-camera"&gt;https://huggingface.co/stabilityai/stable-virtual-camera&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/Stability-AI/stable-virtual-camera"&gt;https://github.com/Stability-AI/stable-virtual-camera&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EssayHealthy5075"&gt; /u/EssayHealthy5075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/clw6m2au3npe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jevseg/new_multiview_3d_model_by_stability_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jevseg/new_multiview_3d_model_by_stability_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T12:27:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jezj71</id>
    <title>New open-source model for transpiling PyTorch to Triton outperforms DeepSeek-R1 and OpenAI o1 on kernelbench - made with reinforcement fine-tuning</title>
    <updated>2025-03-19T15:23:26+00:00</updated>
    <author>
      <name>/u/Fantastic-Tax6709</name>
      <uri>https://old.reddit.com/user/Fantastic-Tax6709</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jezj71/new_opensource_model_for_transpiling_pytorch_to/"&gt; &lt;img alt="New open-source model for transpiling PyTorch to Triton outperforms DeepSeek-R1 and OpenAI o1 on kernelbench - made with reinforcement fine-tuning" src="https://a.thumbs.redditmedia.com/1f-A5j4uyipf7Q6GI3ac81je-5JNKA_lnSRBYea-gq8.jpg" title="New open-source model for transpiling PyTorch to Triton outperforms DeepSeek-R1 and OpenAI o1 on kernelbench - made with reinforcement fine-tuning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there, we trained a model for translating PyTorch code to Triton and open-sourced it here: &lt;a href="https://huggingface.co/predibase/Predibase-T2T-32B-RFT"&gt;https://huggingface.co/predibase/Predibase-T2T-32B-RFT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To do it, we trained Qwen2.5-Coder-32B-instruct using reinforcement fine-tuning (based on GRPO) and, according to kernelbench, are outperforming DeepSeek-R1 and OpenAI o1 by about 3x.&lt;/p&gt; &lt;p&gt;We wrote about the RFT implementation and the model here: &lt;a href="https://predibase.com/blog/introducing-reinforcement-fine-tuning-on-predibase"&gt;https://predibase.com/blog/introducing-reinforcement-fine-tuning-on-predibase&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vj39t0dcznpe1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=24817b6bf2787122cff9a049681e3afadc0fb2f4"&gt;https://preview.redd.it/vj39t0dcznpe1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=24817b6bf2787122cff9a049681e3afadc0fb2f4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fantastic-Tax6709"&gt; /u/Fantastic-Tax6709 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jezj71/new_opensource_model_for_transpiling_pytorch_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jezj71/new_opensource_model_for_transpiling_pytorch_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jezj71/new_opensource_model_for_transpiling_pytorch_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T15:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jes8ue</id>
    <title>Llama4 is probably coming next month, multi modal, long context</title>
    <updated>2025-03-19T08:25:40+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jes8ue/llama4_is_probably_coming_next_month_multi_modal/"&gt; &lt;img alt="Llama4 is probably coming next month, multi modal, long context" src="https://external-preview.redd.it/rBTq4xgcNZ3Fw4CkfyZwhQC8tEjpI_nR5A7MTEmNxNA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a2e5f0e1f34849faacd7b3f87b98e5ad732879a6" title="Llama4 is probably coming next month, multi modal, long context" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/cblvrkrcwlpe1.png?width=1677&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a2c29ccbad996630f7cddc95d39a35ba9ef3fb6"&gt;https://preview.redd.it/cblvrkrcwlpe1.png?width=1677&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a2c29ccbad996630f7cddc95d39a35ba9ef3fb6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;source:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.meta.com/blog/connect-2025-llamacon-save-the-date/?srsltid=AfmBOoqvpQ6A0__ic3TrgNRj_RoGpBKWSnRmGFO_-RbGs5bZ7ntliloW"&gt;https://www.meta.com/blog/connect-2025-llamacon-save-the-date/?srsltid=AfmBOoqvpQ6A0__ic3TrgNRj_RoGpBKWSnRmGFO_-RbGs5bZ7ntliloW&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Probably ~1M context, multi modal&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jes8ue/llama4_is_probably_coming_next_month_multi_modal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jes8ue/llama4_is_probably_coming_next_month_multi_modal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jes8ue/llama4_is_probably_coming_next_month_multi_modal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T08:25:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jez456</id>
    <title>KBLaM by microsoft, This looks interesting</title>
    <updated>2025-03-19T15:05:42+00:00</updated>
    <author>
      <name>/u/AryanEmbered</name>
      <uri>https://old.reddit.com/user/AryanEmbered</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.microsoft.com/en-us/research/blog/introducing-kblam-bringing-plug-and-play-external-knowledge-to-llms/"&gt;https://www.microsoft.com/en-us/research/blog/introducing-kblam-bringing-plug-and-play-external-knowledge-to-llms/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone more knowledgeable, please enlighten us&lt;/p&gt; &lt;p&gt;in what contexts can it replace rag?&lt;/p&gt; &lt;p&gt;I genuinely believe rag getting solved is the next big unlock. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AryanEmbered"&gt; /u/AryanEmbered &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jez456/kblam_by_microsoft_this_looks_interesting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jez456/kblam_by_microsoft_this_looks_interesting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jez456/kblam_by_microsoft_this_looks_interesting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T15:05:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf10ar</id>
    <title>Gemma 3 GRPO now in Unsloth + Bug Fixes</title>
    <updated>2025-03-19T16:25:58+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf10ar/gemma_3_grpo_now_in_unsloth_bug_fixes/"&gt; &lt;img alt="Gemma 3 GRPO now in Unsloth + Bug Fixes" src="https://external-preview.redd.it/zDUwsKwEDam-qG7u2ijw3m6H-OIciOZkuCU51Tgu7r4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=626174c3432ba48d9fed8ccced1e2bbb42d41c7a" title="Gemma 3 GRPO now in Unsloth + Bug Fixes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We collabed with Hugging Face to create a &lt;strong&gt;free notebook&lt;/strong&gt; to train your own reasoning model using &lt;strong&gt;Gemma 3&lt;/strong&gt; and GRPO &amp;amp; also did some fixes for training + inference&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some frameworks had large training losses when finetuning Gemma 3 - Unsloth should have correct losses!&lt;/li&gt; &lt;li&gt;We worked really hard to make Gemma 3 work in a free Colab T4 environment after inference AND &lt;strong&gt;training did not work for Gemma 3 on older GPUs&lt;/strong&gt; limited to float16. This issue affected all frameworks including us, transformers, vLLM etc.&lt;/li&gt; &lt;li&gt;Note - it's NOT a bug in Gemma 3 - in fact I consider it a &lt;strong&gt;very cool feature&lt;/strong&gt;!! It's the first time I've seen this behavior, and it's probably maybe why Gemma 3 seems extremely powerful for it's size!&lt;/li&gt; &lt;li&gt;I found that Gemma 3 had &lt;strong&gt;infinite activations&lt;/strong&gt; if one uses float16, since float16's maximum range is 65504, and Gemma 3 had values of 800,000 or larger. Llama 3.1 8B's max activation value is around 324.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ibevwuip5ope1.png?width=3580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05185613d47e1397ff2476d3e570b2c0d8478d30"&gt;https://preview.redd.it/ibevwuip5ope1.png?width=3580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05185613d47e1397ff2476d3e570b2c0d8478d30&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; is now the only framework which works in FP16 machines for Gemma 3 inference and training. This means you can now do &lt;strong&gt;GRPO, SFT&lt;/strong&gt;, FFT etc. for Gemma 3, in a free T4 GPU instance on Colab via Unsloth!&lt;/li&gt; &lt;li&gt;Please update Unsloth to the latest version to enable many many bug fixes, and Gemma 3 finetuning support via &lt;code&gt;pip install --upgrade unsloth unsloth_zoo&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Read about our Gemma 3 &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-and-fine-tune-gemma-3#unsloth-fine-tuning-fixes-for-gemma-3"&gt;fixes + details here&lt;/a&gt;!&lt;/li&gt; &lt;li&gt;This fix also solved an issue where training loss was not calculated properly for Gemma 3 in FP16.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We picked Gemma 3 (1B) for our GRPO notebook because of its smaller size, which makes inference faster and easier. But you can also use &lt;strong&gt;Gemma 3 (4B) or (12B)&lt;/strong&gt; just by changing the model name and it should fit on Colab.&lt;/p&gt; &lt;p&gt;For newer folks, we made a step-by-step &lt;a href="https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"&gt;GRPO tutorial here&lt;/a&gt;. And here's our Colab notebooks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GRPO: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/HuggingFace%20Course-Gemma3_(1B"&gt;Gemma 3 (1B) Notebook&lt;/a&gt;-GRPO.ipynb) - long link here: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/HuggingFace%20Course-Gemma3_(1B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/HuggingFace%20Course-Gemma3_(1B)-GRPO.ipynb&lt;/a&gt;-GRPO.ipynb)&lt;/li&gt; &lt;li&gt;Normal SFT: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B"&gt;Gemma 3 (4B) Notebook&lt;/a&gt;.ipynb)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy tuning and let me know if you have any questions! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf10ar/gemma_3_grpo_now_in_unsloth_bug_fixes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf10ar/gemma_3_grpo_now_in_unsloth_bug_fixes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf10ar/gemma_3_grpo_now_in_unsloth_bug_fixes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T16:25:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf6igq</id>
    <title>Apache TTS: Orpheus 3B 0.1 FT</title>
    <updated>2025-03-19T20:11:33+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a respect post, it's not my model. In TTS land, a finetuned, Apache licensed 3B boi is a huge drop.&lt;/p&gt; &lt;p&gt;Weights: &lt;a href="https://huggingface.co/canopylabs/orpheus-3b-0.1-ft"&gt;https://huggingface.co/canopylabs/orpheus-3b-0.1-ft&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;Space:&lt;/del&gt; &lt;a href="https://huggingface.co/spaces/canopylabs/orpheus-3b-finetune"&gt;&lt;del&gt;https://huggingface.co/spaces/canopylabs/orpheus-3b-finetune&lt;/del&gt;&lt;/a&gt; Space now 404's&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;https://github.com/canopyai/Orpheus-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://canopylabs.ai/model-releases"&gt;https://canopylabs.ai/model-releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As an aside, I personally love it when the weights repro the demo samples. Well done.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf6igq/apache_tts_orpheus_3b_01_ft/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf6igq/apache_tts_orpheus_3b_01_ft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf6igq/apache_tts_orpheus_3b_01_ft/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T20:11:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jex61b</id>
    <title>If "The Model is the Product" article is true, a lot of AI companies are doomed</title>
    <updated>2025-03-19T13:38:25+00:00</updated>
    <author>
      <name>/u/bttf88</name>
      <uri>https://old.reddit.com/user/bttf88</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious to hear the community's thoughts on this blog post that was near the top of Hacker News yesterday. Unsurprisingly, it got voted down, because I think it's news that not many YC founders want to hear.&lt;/p&gt; &lt;p&gt;I think the argument holds a lot of merit. Basically, major AI Labs like OpenAI and Anthropic are clearly moving towards training their models for Agentic purposes using RL. OpenAI's DeepResearch is one example, Claude Code is another. The models are learning how to select and leverage tools as part of their training - eating away at the complexities of application layer.&lt;/p&gt; &lt;p&gt;If this continues, the application layer that many AI companies today are inhabiting will end up competing with the major AI Labs themselves. The article quotes the VP of AI @ DataBricks predicting that all closed model labs will shut down their APIs within the next 2 -3 years. Wild thought but not totally implausible.&lt;/p&gt; &lt;p&gt;&lt;a href="https://vintagedata.org/blog/posts/model-is-the-product"&gt;https://vintagedata.org/blog/posts/model-is-the-product&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bttf88"&gt; /u/bttf88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jex61b/if_the_model_is_the_product_article_is_true_a_lot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jex61b/if_the_model_is_the_product_article_is_true_a_lot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jex61b/if_the_model_is_the_product_article_is_true_a_lot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T13:38:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jevzm3</id>
    <title>only the real ones remember</title>
    <updated>2025-03-19T12:38:23+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jevzm3/only_the_real_ones_remember/"&gt; &lt;img alt="only the real ones remember" src="https://preview.redd.it/dh21r5dq5npe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5558750be400389e9a0376174765e8479016507" title="only the real ones remember" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dh21r5dq5npe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jevzm3/only_the_real_ones_remember/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jevzm3/only_the_real_ones_remember/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T12:38:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf5ufk</id>
    <title>New RTX PRO 6000 with 96G VRAM</title>
    <updated>2025-03-19T19:44:59+00:00</updated>
    <author>
      <name>/u/ThenExtension9196</name>
      <uri>https://old.reddit.com/user/ThenExtension9196</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this at nvidia GTC. Truly a beautiful card. Very similar styling as the 5090FE and even has the same cooling system. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThenExtension9196"&gt; /u/ThenExtension9196 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cost3vsw9ppe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf5ufk/new_rtx_pro_6000_with_96g_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf5ufk/new_rtx_pro_6000_with_96g_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T19:44:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jev3fl</id>
    <title>A man can dream</title>
    <updated>2025-03-19T11:47:24+00:00</updated>
    <author>
      <name>/u/Severin_Suveren</name>
      <uri>https://old.reddit.com/user/Severin_Suveren</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jev3fl/a_man_can_dream/"&gt; &lt;img alt="A man can dream" src="https://preview.redd.it/cw3hsv4mwmpe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70e23762b65bf659739163a3e09585431a44e8b5" title="A man can dream" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severin_Suveren"&gt; /u/Severin_Suveren &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cw3hsv4mwmpe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jev3fl/a_man_can_dream/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jev3fl/a_man_can_dream/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T11:47:24+00:00</published>
  </entry>
</feed>
