<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-02T17:07:02+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1l0p3et</id>
    <title>Let's build a production level Small Language Model (SLM) from scratch | 3 hour workshop</title>
    <updated>2025-06-01T13:34:12+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0p3et/lets_build_a_production_level_small_language/"&gt; &lt;img alt="Let's build a production level Small Language Model (SLM) from scratch | 3 hour workshop" src="https://external-preview.redd.it/3QugVQO6P_Q3v0881CbP7ispW7LV5z9hQhVFGV8ZV58.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64639bca07382b454fb4ec613939209217564782" title="Let's build a production level Small Language Model (SLM) from scratch | 3 hour workshop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/z6y0w621jb4f1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8491a9d59f58f3082afceaa6c737005c740ad38b"&gt;https://preview.redd.it/z6y0w621jb4f1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8491a9d59f58f3082afceaa6c737005c740ad38b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I made a 3 hour workshop showing how to build an SLM from scratch. &lt;/p&gt; &lt;p&gt;Watch it here: &lt;a href="https://youtu.be/pOFcwcwtv3k?si=1UI4uCdw_HLbdQgX"&gt;https://youtu.be/pOFcwcwtv3k?si=1UI4uCdw_HLbdQgX&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here is what I cover in the workshop: &lt;/p&gt; &lt;p&gt;(a) Download a dataset with 1million+ samples&lt;/p&gt; &lt;p&gt;(b) Pre-process and tokenize the dataset&lt;/p&gt; &lt;p&gt;(c) Divide the dataset into input-target pairs&lt;/p&gt; &lt;p&gt;(d) Assemble the SLM architecture: tokenization layer, attention layer, transformer block, output layer and everything in between&lt;/p&gt; &lt;p&gt;(e) Pre-train the entire SLM&lt;/p&gt; &lt;p&gt;(f) Run inference and generate new text from your trained SLM!&lt;/p&gt; &lt;p&gt;This is not a toy project. &lt;/p&gt; &lt;p&gt;It's a production-level project with an extensive dataset. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0p3et/lets_build_a_production_level_small_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0p3et/lets_build_a_production_level_small_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0p3et/lets_build_a_production_level_small_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T13:34:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1l17m9g</id>
    <title>SAGA Update: Autonomous Novel Writing with Deep KG &amp; Semantic Context - Now Even More Advanced!</title>
    <updated>2025-06-02T03:12:03+00:00</updated>
    <author>
      <name>/u/MariusNocturnum</name>
      <uri>https://old.reddit.com/user/MariusNocturnum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A couple of weeks ago, I shared an early version of SAGA (Semantic And Graph-enhanced Authoring), my project for autonomous novel generation. Thanks to some great initial feedback and a lot of focused development, I'm excited to share a significantly advanced version!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is SAGA?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SAGA, powered by its NANA (Next-gen Autonomous Narrative Architecture) engine, is designed to write entire novels. It's not just about stringing words together; it employs a team of specialized AI agents that handle planning, drafting, comprehensive evaluation, continuity checking, and intelligent revision. The core idea is to combine the creative power of local LLMs with the structured knowledge of a Neo4j graph database and the coherence provided by semantic embeddings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's New &amp;amp; Improved Since Last Time?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SAGA has undergone substantial enhancements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Deep Neo4j Integration:&lt;/strong&gt; Moved from a simpler DB to a full Neo4j backend. This allows for much richer tracking of characters, world-building, plot points, and dynamic relationships. It includes a robust schema with constraints and a vector index for semantic searches.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Hybrid Context Generation:&lt;/strong&gt; For each chapter, SAGA now generates a &amp;quot;hybrid context&amp;quot; by: &lt;ul&gt; &lt;li&gt; Performing &lt;strong&gt;semantic similarity searches&lt;/strong&gt; (via Ollama embeddings) on past chapter content stored in Neo4j to maintain narrative flow and tone.&lt;/li&gt; &lt;li&gt; Extracting &lt;strong&gt;key reliable facts&lt;/strong&gt; directly from the Neo4j knowledge graph to ensure the LLM adheres to established canon.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Advanced Revision Logic:&lt;/strong&gt; The revision process is now more sophisticated, capable of &lt;strong&gt;patch-based revisions&lt;/strong&gt; for targeted fixes or full chapter rewrites when necessary.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Sophisticated Evaluation &amp;amp; Continuity:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt; The &lt;code&gt;ComprehensiveEvaluatorAgent&lt;/code&gt; assesses drafts on multiple axes (plot, theme, depth, consistency).&lt;/li&gt; &lt;li&gt; A dedicated &lt;code&gt;WorldContinuityAgent&lt;/code&gt; performs focused checks against the KG and world-building data to catch inconsistencies.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Provisional Data Handling:&lt;/strong&gt; The system now explicitly tracks whether data is &amp;quot;provisional&amp;quot; (e.g., from an unrevised draft), allowing for better canon management.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Markdown for User Input:&lt;/strong&gt; You can now seed your story using a &lt;code&gt;user_story_elements.md&lt;/code&gt; file with &lt;code&gt;[Fill-in]&lt;/code&gt; placeholders, making initial setup more intuitive.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Text De-duplication:&lt;/strong&gt; Added a step to help reduce repetitive phrasing or content in generated drafts.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Performance &amp;amp; Stability:&lt;/strong&gt; Lots of under-the-hood improvements. SAGA can now generate a batch of 3 chapters (each ~13K+ tokens of narrative) in about 11 minutes on my setup, including all the planning, evaluation, and KG updates.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Core Architecture Still Intact:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The agentic pipeline remains central:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;strong&gt;Initial Setup:&lt;/strong&gt; Parses user markdown or generates plot, characters, and world-building; pre-populates Neo4j.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Chapter Loop:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Plan:&lt;/strong&gt; &lt;code&gt;PlannerAgent&lt;/code&gt; details scenes.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Context:&lt;/strong&gt; Hybrid semantic &amp;amp; KG context is built.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Draft:&lt;/strong&gt; &lt;code&gt;DraftingAgent&lt;/code&gt; writes the chapter.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Evaluate:&lt;/strong&gt; &lt;code&gt;ComprehensiveEvaluatorAgent&lt;/code&gt; &amp;amp; &lt;code&gt;WorldContinuityAgent&lt;/code&gt; scrutinize the draft.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Revise:&lt;/strong&gt; &lt;code&gt;ChapterRevisionLogic&lt;/code&gt; applies fixes.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Finalize &amp;amp; Update KG:&lt;/strong&gt; &lt;code&gt;KGMaintainerAgent&lt;/code&gt; summarizes, embeds, saves the chapter to Neo4j, and extracts/merges new knowledge back into the graph and agent state.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why This Approach?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The goal is to create narratives that are not only creative but also &lt;em&gt;coherent&lt;/em&gt; and &lt;em&gt;consistent&lt;/em&gt; over tens of thousands of tokens. The graph database acts as the story's long-term memory and source of truth, while semantic embeddings help maintain flow and relevance.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current Performance Example:&lt;/strong&gt; Using local GGUF models (Qwen3 14B for narration/planning, smaller Qwen3s for other tasks), SAGA generates: * &lt;strong&gt;3 chapters&lt;/strong&gt; (each ~13,000+ tokens of narrative) * In approximately &lt;strong&gt;11 minutes&lt;/strong&gt; * This includes all planning, context generation, evaluation, and knowledge graph updates.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Check it out &amp;amp; Get Involved:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/Lanerra/saga"&gt;https://github.com/Lanerra/saga&lt;/a&gt; (The README has been updated with detailed setup instructions!)&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Setup:&lt;/strong&gt; You'll need Python, Ollama (for embeddings), an OpenAI-API compatible LLM server, and Neo4j (Docker setup provided).&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Reset Script:&lt;/strong&gt; &lt;code&gt;reset_neo4j.py&lt;/code&gt; is still there to easily clear the database and start fresh.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Inspect KG:&lt;/strong&gt; The &lt;code&gt;inspect_kg.py&lt;/code&gt; script mentioned previously has been replaced by direct Neo4j browser interaction (which is much more powerful for visualization).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm really proud of how far SAGA has come and believe it's pushing into some interesting territory for AI-assisted storytelling. I'd love for you all to try it out, see what kind of sagas NANA can spin up for you, and share your thoughts, feedback, or any issues you encounter.&lt;/p&gt; &lt;p&gt;What kind of stories will you create?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MariusNocturnum"&gt; /u/MariusNocturnum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l17m9g/saga_update_autonomous_novel_writing_with_deep_kg/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l17m9g/saga_update_autonomous_novel_writing_with_deep_kg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l17m9g/saga_update_autonomous_novel_writing_with_deep_kg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T03:12:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1h5dq</id>
    <title>Best Open source LLMs for tool call / structured output</title>
    <updated>2025-06-02T12:48:12+00:00</updated>
    <author>
      <name>/u/Initial_Track6190</name>
      <uri>https://old.reddit.com/user/Initial_Track6190</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have tried Qwen models (both 2.5 and 3) but it they still get the output wrong. (using vLLM). At least Qwen 32B (thinking and non thinking both) struggle with the output I specify. I have tried guided decoding too but no luck, they sometime work, but it's super unstable in terms out output. Llama 4 is nice but sometimes it stucks in the loop of calling tools, or not adhering to what I asked. Would appreciate your recommendations. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Initial_Track6190"&gt; /u/Initial_Track6190 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1h5dq/best_open_source_llms_for_tool_call_structured/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1h5dq/best_open_source_llms_for_tool_call_structured/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1h5dq/best_open_source_llms_for_tool_call_structured/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T12:48:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0q2zk</id>
    <title>DeepSeek-R1-0528-UD-Q6-K-XL on 10 Year Old Hardware</title>
    <updated>2025-06-01T14:19:51+00:00</updated>
    <author>
      <name>/u/Simusid</name>
      <uri>https://old.reddit.com/user/Simusid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't expect anything useful in this post. I did it just to see if it was possible. This was on a 10+ year old system with a 6th generation i5 with 12gb of RAM. My ssd is nearly full so I had to mount an external 8TB USB drive to store the 560GB model. At least it is USB-3.&lt;/p&gt; &lt;p&gt;I made an 800GB swap file and enabled it, then launched llama-cli with a simple prompt and went to bed. I half expected that the model might not even have fully loaded when I got up but it was already part way through the response.&lt;/p&gt; &lt;p&gt;With no GPU, it seems to be about seven minutes per token.&lt;/p&gt; &lt;p&gt;Edit - I've named this system TreeBeard&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Simusid"&gt; /u/Simusid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0q2zk/deepseekr10528udq6kxl_on_10_year_old_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0q2zk/deepseekr10528udq6kxl_on_10_year_old_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0q2zk/deepseekr10528udq6kxl_on_10_year_old_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T14:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1lgvi</id>
    <title>Multiturn causes additional output Quality?</title>
    <updated>2025-06-02T15:47:42+00:00</updated>
    <author>
      <name>/u/Federal_Order4324</name>
      <uri>https://old.reddit.com/user/Federal_Order4324</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So recently while just testing some things, I tried to change how I process the user assistant chat messages. &lt;/p&gt; &lt;p&gt;Instead of having alternating user and assistant messages be sent, I passed the entire chat as raw text with a user: and assistant: prefixed in the user message. System prompt was kept the same.&lt;/p&gt; &lt;p&gt;The post processing looked like this:&lt;/p&gt; &lt;p&gt;Please fulfill users request taking the previous chat history into account. &amp;lt;Chat_History&amp;gt; .... &amp;lt;/Chat_History&amp;gt;&lt;/p&gt; &lt;p&gt;Here is users next message. user:&lt;/p&gt; &lt;p&gt;Has anyone else seen this behavior? It seems like while higher context requests degrade model output, instruction following etc., the multi round seem to create some additional degradation. Would it better to just use single turn instead?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal_Order4324"&gt; /u/Federal_Order4324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1lgvi/multiturn_causes_additional_output_quality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1lgvi/multiturn_causes_additional_output_quality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1lgvi/multiturn_causes_additional_output_quality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T15:47:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1lqdm</id>
    <title>Which LLM is best at understanding information in spreadsheets?</title>
    <updated>2025-06-02T15:58:12+00:00</updated>
    <author>
      <name>/u/ColoradoCyclist</name>
      <uri>https://old.reddit.com/user/ColoradoCyclist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been having trouble finding an LLM that can properly process spreadsheet data. I've tried Gemma 8b and the latest deepseek. Yet both struggle to even do simple matching. I haven't tried Gemma 27b yet but I'm just not sure what I'm missing here. ChatGPT has no issues for me so it's not the data or what I'm requesting. &lt;/p&gt; &lt;p&gt;I'm running on a 4090 and i9 with 64gb. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ColoradoCyclist"&gt; /u/ColoradoCyclist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1lqdm/which_llm_is_best_at_understanding_information_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1lqdm/which_llm_is_best_at_understanding_information_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1lqdm/which_llm_is_best_at_understanding_information_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T15:58:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1l13fqa</id>
    <title>How are people running dual GPU these days?</title>
    <updated>2025-06-01T23:42:00+00:00</updated>
    <author>
      <name>/u/admiralamott</name>
      <uri>https://old.reddit.com/user/admiralamott</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 4080 but was considering getting a 3090 for LLM models. I've never ran a dual set up before because I read like 6 years ago that it isn't used anymore. But clearly people are doing it so is that still going on? How does it work? Will it only offload to 1 gpu and then to the RAM, or can it offload to one GPU and then to the second one if it needs more? How do I know if my PC can do it? It's down to the motherboard right? (Sorry I am so behind rn) I'm also using ollama with OpenWebUI if that helps.&lt;/p&gt; &lt;p&gt;Thank you for your time :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/admiralamott"&gt; /u/admiralamott &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l13fqa/how_are_people_running_dual_gpu_these_days/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l13fqa/how_are_people_running_dual_gpu_these_days/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l13fqa/how_are_people_running_dual_gpu_these_days/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T23:42:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1n6h4</id>
    <title>Best uncensored multi language LLM up to 12B, still Mistral Nemo?</title>
    <updated>2025-06-02T16:54:30+00:00</updated>
    <author>
      <name>/u/Blizado</name>
      <uri>https://old.reddit.com/user/Blizado</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to use a fixed model for my private none commercial AI project because I want to finetune it later (LoRAs) for it's specific tasks. For that I need:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A up to 12B text to text model - need to match into 12GB VRAM inclusive 8K context window.&lt;/li&gt; &lt;li&gt;As uncensored as possible in it's core.&lt;/li&gt; &lt;li&gt;Official support for main languages (At least EN/FR/DE).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Actually I have Mistral Nemo Instruct on my list, nothing else. It is the only model from that I know that match all three points without a &amp;quot;however&amp;quot;.&lt;/p&gt; &lt;p&gt;12B at max because I set me a limit of 16GB VRAM for my AI project usage in total and that must be enough for the LLM with 8K context, Whisper and a TTS. 16GB because I want to open source my project later and don't want that it is limited to users with at least 24GB VRAM. 16GB are more and more common on actual graphic cards (don't by 8GB versions anymore!).&lt;/p&gt; &lt;p&gt;I know you can uncensor models, BUT abliterated models are mostly only uncensored for English language. I always noticed more worse performance on other languages with such models and don't want to deal with that. And Mistral Nemo is known to be very uncensored so no extra uncensoring needed.&lt;/p&gt; &lt;p&gt;Because the most finetuned models are only done for one or two languages, finetuned models fall out as options. I want to support at least EN/FR/DE languages. I'm myself a nativ German speaker and don't want to talk to AI all the time in English only. So I know very good how annoying it is that many AI projects only support English.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Blizado"&gt; /u/Blizado &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1n6h4/best_uncensored_multi_language_llm_up_to_12b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1n6h4/best_uncensored_multi_language_llm_up_to_12b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1n6h4/best_uncensored_multi_language_llm_up_to_12b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T16:54:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1drru</id>
    <title>Best Video captioning model</title>
    <updated>2025-06-02T09:40:05+00:00</updated>
    <author>
      <name>/u/VihmaVillu</name>
      <uri>https://old.reddit.com/user/VihmaVillu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Need to generate text captions from small video clips that later i can use to do semantic scene search. What are the best models for VRAM 12-32GB.&lt;/p&gt; &lt;p&gt;Maybe i can train/fine tune so i can do embeded search?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VihmaVillu"&gt; /u/VihmaVillu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1drru/best_video_captioning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1drru/best_video_captioning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1drru/best_video_captioning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T09:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1mliw</id>
    <title>Has anyone had success implementing a local FIM model?</title>
    <updated>2025-06-02T16:31:49+00:00</updated>
    <author>
      <name>/u/m_abdelfattah</name>
      <uri>https://old.reddit.com/user/m_abdelfattah</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed that the auto-completion features in my current IDE can be sluggish. As I rely heavily on auto-completion during coding, I strongly prefer accurate autocomplete suggestions like those offered by &amp;quot;Cursor&amp;quot; over automated code generation(Chat/Agent tabs). Therefore, I'm seeking a local alternative that incorporates an intelligent agent capable of analyzing my entire codebase. Is this request overly ambitious ðŸ™ˆ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m_abdelfattah"&gt; /u/m_abdelfattah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1mliw/has_anyone_had_success_implementing_a_local_fim/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1mliw/has_anyone_had_success_implementing_a_local_fim/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1mliw/has_anyone_had_success_implementing_a_local_fim/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T16:31:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1imus</id>
    <title>MedGemma on Android</title>
    <updated>2025-06-02T13:54:16+00:00</updated>
    <author>
      <name>/u/caiporadomato</name>
      <uri>https://old.reddit.com/user/caiporadomato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any way to use the multimodal capabilities of MedGemma on android? Tried with both Layla and Crosstalk apps but the model cant read images using them&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/caiporadomato"&gt; /u/caiporadomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1imus/medgemma_on_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1imus/medgemma_on_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1imus/medgemma_on_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T13:54:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1b801</id>
    <title>What LLM libraries/frameworks are worthwhile and what is better to roll your own from scratch?</title>
    <updated>2025-06-02T06:47:29+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Maybe I'm suffering from NIH, but the core of systems can be quite simple to roll out using just python.&lt;/p&gt; &lt;p&gt;What libraries/frameworks do you find most valuable to use instead of rolling your own?&lt;/p&gt; &lt;p&gt;EDIT: Sorry. I was unclear. When implementing an application which calls on LLM functionality (via API) do you roll everything by hand or do you use frameworks such as Langchain, Pocket Flow or Burr etc. e.g. when you build pipelines/workflows for gathering data to put into context (RAG) or use multiple calls to generate context and have different flows/branches.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1b801/what_llm_librariesframeworks_are_worthwhile_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1b801/what_llm_librariesframeworks_are_worthwhile_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1b801/what_llm_librariesframeworks_are_worthwhile_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T06:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0zsv7</id>
    <title>25L Portable NV-linked Dual 3090 LLM Rig</title>
    <updated>2025-06-01T21:01:58+00:00</updated>
    <author>
      <name>/u/Special-Wolverine</name>
      <uri>https://old.reddit.com/user/Special-Wolverine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0zsv7/25l_portable_nvlinked_dual_3090_llm_rig/"&gt; &lt;img alt="25L Portable NV-linked Dual 3090 LLM Rig" src="https://b.thumbs.redditmedia.com/SCLEVQyCptjUTsrbRVb6jCIJUk1CuEOO-Ud355bse9Q.jpg" title="25L Portable NV-linked Dual 3090 LLM Rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Main point of portability is because The workplace of the coworker I built this for is truly offline, with no potential for LAN or wifi, so to download new models and update the system periodically I need to go pick it up from him and take it home. &lt;/p&gt; &lt;p&gt;WARNING - these components don't fit if you try to copy this build. The bottom GPU is resting on the Arctic p12 slim fans at the bottom of the case and pushing up on the GPU. Also the top arctic p14 Max fans don't have mounting points for half of their screw holes, and are in place by being very tightly wedged against the motherboard, case, and PSU. Also, there 's probably way too much pressure on the pcie cables coming off the gpus when you close the glass. Also I had to daisy chain the PCIE cables because the Corsair RM 1200e only has four available on the PSU side and these particular EVGA 3090s require 3x 8pin power. Allegedly it just enforces a hardware power limit to 300 w but you should make it a little bit more safe by also enforcing the 300W power limit in Nvidia -SMI To make sure that the cards don't try to pull 450W through 300W pipes. Could have fit a bigger PSU, but then I wouldn't get that front fan which is probably crucial.&lt;/p&gt; &lt;p&gt;All that being said, with a 300w power limit applied to both gpus in a silent fan profile, this rig has surprisingly good temperatures and noise levels considering how compact it is. &lt;/p&gt; &lt;p&gt;During Cinebench 24 with both gpus being 100% utilized, the CPU runs at 63 C and both gpus at 67 Celsius somehow with almost zero gap between them and the glass closed. All the while running at about 37 to 40 decibels from 1 meter away. &lt;/p&gt; &lt;p&gt;Prompt processing and inference - the gpus run at about 63 C, CPU at 55 C, and decibels at 34. &lt;/p&gt; &lt;p&gt;Again, I don't understand why the temperatures for both are almost the same, when logically the top GPU should be much hotter. The only gap between the two gpus is the size of one of those little silicone rubber DisplayPort caps wedged into the end, right between where the pcie power cables connect to force the GPUs apart a little.&lt;/p&gt; &lt;p&gt;Everything but the case, CPU cooler, and PSU was bought used on Facebook Marketplace&lt;/p&gt; &lt;p&gt;&lt;a href="https://pcpartpicker.com/list/nQXzgn"&gt;PCPartPicker Part List&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Item&lt;/th&gt; &lt;th align="left"&gt;Price&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/qtvqqs/amd-ryzen-7-5800x-38-ghz-8-core-processor-100-100000063wof"&gt;AMD Ryzen 7 5800X 3.8 GHz 8-Core Processor&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$160.54 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU Cooler&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/HbDQzy/id-cooling-frozn-a720-black-986-cfm-cpu-cooler-frozn-a720-black"&gt;ID-COOLING FROZN A720 BLACK 98.6 CFM CPU Cooler&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$69.98 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/CLkgXL/asus-rog-strix-x570-e-gaming-atx-am4-motherboard-rog-strix-x570-e-gaming"&gt;Asus ROG Strix X570-E Gaming ATX AM4 Motherboard&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$559.00 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/6rrcCJ/corsair-memory-cmk32gx4m2b3200c16"&gt;Corsair Vengeance LPX 32 GB (2 x 16 GB) DDR4-3200 CL16 Memory&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$81.96 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/DDWBD3/samsung-980-pro-1-tb-m2-2280-nvme-solid-state-drive-mz-v8p1t0bam"&gt;Samsung 980 Pro 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$149.99 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Video Card&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/PG848d/evga-geforce-rtx-3090-24-gb-ftw3-ultra-gaming-video-card-24g-p5-3987-kr"&gt;EVGA FTW3 ULTRA GAMING GeForce RTX 3090 24 GB Video Card&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$750.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Video Card&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/PG848d/evga-geforce-rtx-3090-24-gb-ftw3-ultra-gaming-video-card-24g-p5-3987-kr"&gt;EVGA FTW3 ULTRA GAMING GeForce RTX 3090 24 GB Video Card&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$750.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Custom&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;NVlink SLI bridge&lt;/td&gt; &lt;td align="left"&gt;$90.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Custom&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Mechanic Master c34plus&lt;/td&gt; &lt;td align="left"&gt;$200.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Custom&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Corsair RM1200e&lt;/td&gt; &lt;td align="left"&gt;$210.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Custom&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2x Arctic p14 max, 3x p12, 3x p12 slim&lt;/td&gt; &lt;td align="left"&gt;$60.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Prices include shipping, taxes, rebates, and discounts&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;$3081.47&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Generated by &lt;a href="https://pcpartpicker.com"&gt;PCPartPicker&lt;/a&gt; 2025-06-01 16:48 EDT-0400&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special-Wolverine"&gt; /u/Special-Wolverine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1l0zsv7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0zsv7/25l_portable_nvlinked_dual_3090_llm_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0zsv7/25l_portable_nvlinked_dual_3090_llm_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T21:01:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1bjhm</id>
    <title>System Prompt Learning: Teaching your local LLMs to learn problem-solving strategies from experience (optillm plugin)</title>
    <updated>2025-06-02T07:08:14+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I wanted to share something we've been working on that might interest folks running local LLMs - &lt;strong&gt;System Prompt Learning (SPL)&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;You know how ChatGPT, Claude, etc. perform so well partly because they have incredibly detailed system prompts with sophisticated reasoning strategies? Most of us running local models just use basic prompts and miss out on those performance gains.&lt;/p&gt; &lt;h1&gt;What is SPL?&lt;/h1&gt; &lt;p&gt;SPL implements what Andrej Karpathy called the &amp;quot;third paradigm&amp;quot; for LLM learning - instead of just pretraining and fine-tuning, models can now learn problem-solving strategies from their own experience.&lt;/p&gt; &lt;h1&gt;How it works:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Automatically classifies problems into 16 types (math, coding, word problems, etc.)&lt;/li&gt; &lt;li&gt;Builds a persistent database of effective solving strategies&lt;/li&gt; &lt;li&gt;Selects the best strategies for each query&lt;/li&gt; &lt;li&gt;Evaluates how well strategies worked and refines them over time&lt;/li&gt; &lt;li&gt;All strategies are human-readable JSON - you can inspect and edit them&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results:&lt;/h1&gt; &lt;p&gt;Tested with gemini-2.0-flash-lite across math benchmarks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Arena Hard&lt;/strong&gt;: 29% â†’ 37.6% (+8.6%)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AIME24&lt;/strong&gt;: 23.33% â†’ 30% (+6.67%)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OptiLLMBench&lt;/strong&gt;: 61% â†’ 65% (+4%)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MATH-500&lt;/strong&gt;: 85% â†’ 85.6% (+0.6%)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;After 500 queries, the system developed 129 strategies, refined 97 of them, and achieved much better problem-solving.&lt;/p&gt; &lt;h1&gt;For Local LLM Users:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Works with &lt;strong&gt;any OpenAI-compatible API&lt;/strong&gt; (so llama.cpp, Ollama, vLLM, etc.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Runs completely locally&lt;/strong&gt; - strategies stored in local JSON files&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Two modes&lt;/strong&gt;: inference-only (default) or learning mode&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Minimal overhead&lt;/strong&gt; - just augments your system prompt&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open source&lt;/strong&gt; and easy to inspect/modify&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Setup:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;pip install optillm # Point to your local LLM endpoint python optillm.py --base_url http://localhost:8080/v1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then just add &lt;code&gt;spl-&lt;/code&gt; prefix to your model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model=&amp;quot;spl-llama-3.2-3b&amp;quot; # or whatever your model is &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Enable learning mode to create new strategies:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;extra_body={&amp;quot;spl_learning&amp;quot;: True} &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Example Strategy Learned:&lt;/h1&gt; &lt;p&gt;The system automatically learned this strategy for word problems:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Understand&lt;/strong&gt;: Read carefully, identify unknowns&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plan&lt;/strong&gt;: Define variables, write equations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Solve&lt;/strong&gt;: Step-by-step with units&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verify&lt;/strong&gt;: Check reasonableness&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;All strategies are stored in &lt;code&gt;~/.optillm/spl/data/strategies.json&lt;/code&gt; so you can back them up, share them, or manually edit them.&lt;/p&gt; &lt;h1&gt;Why This Matters for Local LLMs:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Your model gets &lt;strong&gt;progressively better&lt;/strong&gt; at problem types you use frequently&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transparent learning&lt;/strong&gt; - you can see exactly what strategies it develops&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No external dependencies&lt;/strong&gt; - everything runs locally&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transferable knowledge&lt;/strong&gt; - you can share strategy files between deployments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This feels like a step toward local models that actually improve through use, rather than being static after training.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/codelion/optillm"&gt;https://github.com/codelion/optillm&lt;/a&gt;&lt;/li&gt; &lt;li&gt;SPL Plugin: &lt;a href="https://github.com/codelion/optillm/tree/main/optillm/plugins/spl"&gt;https://github.com/codelion/optillm/tree/main/optillm/plugins/spl&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Technical article: &lt;a href="https://huggingface.co/blog/codelion/system-prompt-learning"&gt;https://huggingface.co/blog/codelion/system-prompt-learning&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Andrej's original tweet: &lt;a href="https://x.com/karpathy/status/1921368644069765486"&gt;https://x.com/karpathy/status/1921368644069765486&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Anyone tried this yet? Would love to hear how it works with different local models!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; Works great with reasoning models like DeepSeek-R1, QwQ, etc. The strategies help guide their thinking process.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1bjhm/system_prompt_learning_teaching_your_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1bjhm/system_prompt_learning_teaching_your_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1bjhm/system_prompt_learning_teaching_your_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T07:08:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0y0wp</id>
    <title>Allowing LLM to ponder in Open WebUI</title>
    <updated>2025-06-01T19:47:52+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y0wp/allowing_llm_to_ponder_in_open_webui/"&gt; &lt;img alt="Allowing LLM to ponder in Open WebUI" src="https://external-preview.redd.it/dHd6NjY5c2JkZDRmMbDY_eAdKP8QUXyZwc-4j2cel9Olwb9ejqufCbXqijwB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d96e6747cff63170125fef17cdbcf53af47bbb3f" title="Allowing LLM to ponder in Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What is this?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A completely superficial way of letting LLM to ponder a bit before making its conversation turn. The process is streamed to an artifact within Open WebUI.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/av/harbor/blob/main/boost/src/modules/ponder.py"&gt;Code&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uoeptbsbdd4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y0wp/allowing_llm_to_ponder_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y0wp/allowing_llm_to_ponder_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T19:47:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1ggkp</id>
    <title>[DEMO] I created a coding agent that can do dynamic, runtime debugging.</title>
    <updated>2025-06-02T12:14:32+00:00</updated>
    <author>
      <name>/u/bn_from_zentara</name>
      <uri>https://old.reddit.com/user/bn_from_zentara</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1ggkp/demo_i_created_a_coding_agent_that_can_do_dynamic/"&gt; &lt;img alt="[DEMO] I created a coding agent that can do dynamic, runtime debugging." src="https://external-preview.redd.it/MnJzMXR6MGg4aTRmMSD9styMP4r1TEey8oyCG95ikT3wdTqvY8nF4QJV9K9T.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c16192412336aa8a18b501474cd7b09cd1c3407" title="[DEMO] I created a coding agent that can do dynamic, runtime debugging." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm just annoyed with inability of current coding agents creating buggy code and can not fix it. It is said that current LLM have Ph.D level and cannot fix some obvious bugs, just loop around and around and offer the same wrong solution for the bug. At the same time they look very smart, much knowledgeable than me. Why is that? My explanation is that they do not have access to the information as I do. When I do debugging, I can look at variable values, can go up and down the stack to figure out where the wrong variables values get it.&lt;br /&gt; It seems to me that this can be fixed easily if we give a coding agent the rich context as we do when debugging by given them all the debugging tools. This approach has been pioneered previously by several posts such as :&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1inqb6n/letting_llms_using_an_ides_debugger/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1inqb6n/letting_llms_using_an_ides_debugger/&lt;/a&gt; , and &lt;a href="https://www.reddit.com/r/ClaudeAI/comments/1i3axh1/enable_claude_to_interactively_debug_for_you_via/"&gt;https://www.reddit.com/r/ClaudeAI/comments/1i3axh1/enable_claude_to_interactively_debug_for_you_via/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Those posts really provided the proof of concept of exactly what I am looking for . Also recently Microsoft published a paper about their Debug-gym, &lt;a href="https://www.microsoft.com/en-us/research/blog/debug-gym-an-environment-for-ai-coding-tools-to-learn-how-to-debug-code-like-programmers/"&gt;https://www.microsoft.com/en-us/research/blog/debug-gym-an-environment-for-ai-coding-tools-to-learn-how-to-debug-code-like-programmers/&lt;/a&gt; , saying that by leveraging the runtime state knowledge, LLM can increase pretty substantially on coding accuracy.&lt;/p&gt; &lt;p&gt;One of the previous work uses MCP server approach. While MCP server provides the flexibility to quickly change the coding agent, I could not make it work robustly, stable in my setting. Maybe the sse transport layer of MCP server does not work well. Also current solutions only provide limited debugging functions. Inspired by those previous works, here I expanded the debugging toolset, made it directly integrated with my favorite coding agent - Roo -Code, skipping the MCP communication. Although this way, I lost the plug and play flexibility of MCP server, what I gain is more stable, robust performance.&lt;br /&gt; Included is the demo of my coding agent - a fork from the wonderful coding agent Roo-Code. Besides writing code , it can set breakpoints, inspect stack variable, go up and down the stack, evaluate expression, run statements, etc. , have access to most debugger function tools. As Zentara Code - my forked coding agent communicate with debugger through VSCode DAP, it is language agnostic, can work with any language that has VSCode debugger extention. I have tested it with Python, TypeScript and Javascript.&lt;/p&gt; &lt;p&gt;I mostly code in Python. I usually ask Zentara Code write a code for me, and then write pytest tests for the code it write. Pytest by default captures all the assertion errors to make it own analysis, do not bubble up the exception. I was able to make Zentara code to capture those pytest exceptions. Now Zentara code can run those pytest tests, see the exception messages, use runtime state to interactively debug the exceptions smartly.&lt;br /&gt; The code will be released soon after I finishing up final touch. The demo attached is an illustration of how Zentara code struggles and successfully debugs a buggy quicksort implementation using dynamic runtime info.&lt;/p&gt; &lt;p&gt;I just would like to share with you the preliminary result and get your initial impressions and feedbacks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bn_from_zentara"&gt; /u/bn_from_zentara &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qic49y0h8i4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1ggkp/demo_i_created_a_coding_agent_that_can_do_dynamic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1ggkp/demo_i_created_a_coding_agent_that_can_do_dynamic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T12:14:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1l13tv3</id>
    <title>Who is getting paid to work doing this rather than just hobby dabbling..what was your path?</title>
    <updated>2025-06-02T00:00:47+00:00</updated>
    <author>
      <name>/u/bornfree4ever</name>
      <uri>https://old.reddit.com/user/bornfree4ever</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really enjoy hacking together LLM scripts and ideas. but how do I get paid doing it??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bornfree4ever"&gt; /u/bornfree4ever &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l13tv3/who_is_getting_paid_to_work_doing_this_rather/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l13tv3/who_is_getting_paid_to_work_doing_this_rather/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l13tv3/who_is_getting_paid_to_work_doing_this_rather/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T00:00:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1a944</id>
    <title>Snapdragon 8 Elite gets 5.5 t/s on Qwen3 30B A3B</title>
    <updated>2025-06-02T05:44:39+00:00</updated>
    <author>
      <name>/u/1ncehost</name>
      <uri>https://old.reddit.com/user/1ncehost</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1a944/snapdragon_8_elite_gets_55_ts_on_qwen3_30b_a3b/"&gt; &lt;img alt="Snapdragon 8 Elite gets 5.5 t/s on Qwen3 30B A3B" src="https://preview.redd.it/jagac0yccg4f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6e0ede5a0ed01ba1b9b850ec415585886e5bad1" title="Snapdragon 8 Elite gets 5.5 t/s on Qwen3 30B A3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Phone is a Razr Ultra 2025&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1ncehost"&gt; /u/1ncehost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jagac0yccg4f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1a944/snapdragon_8_elite_gets_55_ts_on_qwen3_30b_a3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1a944/snapdragon_8_elite_gets_55_ts_on_qwen3_30b_a3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T05:44:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1581z</id>
    <title>Which model are you using? June'25 edition</title>
    <updated>2025-06-02T01:09:13+00:00</updated>
    <author>
      <name>/u/Ok_Influence505</name>
      <uri>https://old.reddit.com/user/Ok_Influence505</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As proposed previously from this &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jxu0f7/we_should_have_a_monthly_which_models_are_you/"&gt;post&lt;/a&gt;, it's time for another monthly check-in on the latest models and their applications. The goal is to keep everyone updated on recent releases and discover hidden gems that might be flying under the radar.&lt;/p&gt; &lt;p&gt;With new models like DeepSeek-R1-0528, Claude 4 dropping recently, I'm curious to see how these stack up against established options. Have you tested any of the latest releases? How do they compare to what you were using before?&lt;/p&gt; &lt;p&gt;So, let start a discussion on what models (both proprietary and open-weights) are use using (or stop using ;) ) for different purposes (coding, writing, creative writing etc.).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Influence505"&gt; /u/Ok_Influence505 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1581z/which_model_are_you_using_june25_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1581z/which_model_are_you_using_june25_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1581z/which_model_are_you_using_june25_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T01:09:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1glmq</id>
    <title>Anyone tried this? - Self improving AI agents</title>
    <updated>2025-06-02T12:21:26+00:00</updated>
    <author>
      <name>/u/davesmith001</name>
      <uri>https://old.reddit.com/user/davesmith001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Repository for &lt;strong&gt;Darwin GÃ¶del Machine (DGM)&lt;/strong&gt;, a novel self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jennyzzt/dgm"&gt;https://github.com/jennyzzt/dgm&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davesmith001"&gt; /u/davesmith001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1glmq/anyone_tried_this_self_improving_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1glmq/anyone_tried_this_self_improving_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1glmq/anyone_tried_this_self_improving_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T12:21:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1mb6y</id>
    <title>PlayAI's Latest Diffusion-based Speech Editing Model: PlayDiffusion</title>
    <updated>2025-06-02T16:20:20+00:00</updated>
    <author>
      <name>/u/SandSalt8370</name>
      <uri>https://old.reddit.com/user/SandSalt8370</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1mb6y/playais_latest_diffusionbased_speech_editing/"&gt; &lt;img alt="PlayAI's Latest Diffusion-based Speech Editing Model: PlayDiffusion" src="https://external-preview.redd.it/w-YzJC8yYFljokN1sVgB95jsZmNJotgMItgN5CbyhjY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d65ddc626651c2d4f886345c5637afbe5263791e" title="PlayAI's Latest Diffusion-based Speech Editing Model: PlayDiffusion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;PlayAI open-sourced a new Speech Editing model today that allows for precise &amp;amp; clean speech editing. A huge step up from traditional autoregressive models that aren't designed for this task.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SandSalt8370"&gt; /u/SandSalt8370 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/playht/playdiffusion"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1mb6y/playais_latest_diffusionbased_speech_editing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1mb6y/playais_latest_diffusionbased_speech_editing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T16:20:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1jyld</id>
    <title>Smallest LLM you tried that's legit</title>
    <updated>2025-06-02T14:48:26+00:00</updated>
    <author>
      <name>/u/Remarkable-Law9287</name>
      <uri>https://old.reddit.com/user/Remarkable-Law9287</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;what's the smallest LLM you've used that gives proper text, not just random gibberish? &lt;/p&gt; &lt;p&gt;I've tried qwen2.5:0.5B.it works pretty well for me, actually quite good&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Law9287"&gt; /u/Remarkable-Law9287 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1jyld/smallest_llm_you_tried_thats_legit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1jyld/smallest_llm_you_tried_thats_legit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1jyld/smallest_llm_you_tried_thats_legit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T14:48:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1j94p</id>
    <title>NVIDIA RTX PRO 6000 Unlocks GB202's Full Performance In Gaming: Beats GeForce RTX 5090 Convincingly</title>
    <updated>2025-06-02T14:20:16+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1j94p/nvidia_rtx_pro_6000_unlocks_gb202s_full/"&gt; &lt;img alt="NVIDIA RTX PRO 6000 Unlocks GB202's Full Performance In Gaming: Beats GeForce RTX 5090 Convincingly" src="https://external-preview.redd.it/CZ499DlxtUi8-a0hH-i2iuvuqGABLEdCAAN2p00rlA0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64ca9b6f5516529ad31d61311b7ab6293c4d549c" title="NVIDIA RTX PRO 6000 Unlocks GB202's Full Performance In Gaming: Beats GeForce RTX 5090 Convincingly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-rtx-pro-6000-beats-geforce-rtx-5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1j94p/nvidia_rtx_pro_6000_unlocks_gb202s_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1j94p/nvidia_rtx_pro_6000_unlocks_gb202s_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T14:20:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1l19yud</id>
    <title>IQ1_Smol_Boi</title>
    <updated>2025-06-02T05:26:51+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l19yud/iq1_smol_boi/"&gt; &lt;img alt="IQ1_Smol_Boi" src="https://preview.redd.it/9u1teeqt4g4f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bab7030e08d75ad0063051230aa543c0b2a3ac7f" title="IQ1_Smol_Boi" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some folks asked me for an R1-0528 quant that might fit on 128GiB RAM + 24GB VRAM. I didn't think it was possible, but turns out my new smol boi &lt;code&gt;IQ1_S_R4&lt;/code&gt; is 131GiB and actually runs okay (ik_llama.cpp fork only), and has perplexity lower &amp;quot;better&amp;quot; than &lt;code&gt;Qwen3-235B-A22B-Q8_0&lt;/code&gt; which is almost twice the size! Not sure that means it is better, but kinda surprising to me.&lt;/p&gt; &lt;p&gt;Unsloth's newest smol boi is an odd &lt;code&gt;UD-TQ1_0&lt;/code&gt; weighing in at 151GiB. The &lt;code&gt;TQ1_0&lt;/code&gt; quant is a 1.6875 bpw quant types for TriLMs and BitNet b1.58 models. However, if you open up the side-bar on the modelcard it doesn't actually have any TQ1_0 layers/tensors and is mostly a mix of IQN_S and such. So not sure what is going on there or if it was a mistake. It does at least run from what I can tell, though I didn't try inferencing with it. They do have an &lt;code&gt;IQ1_S&lt;/code&gt; as well, but it seems rather larger given their recipe though I've heard folks have had success with it.&lt;/p&gt; &lt;p&gt;Bartowski's smol boi &lt;code&gt;IQ1_M&lt;/code&gt; is the next smallest I've seen at about 138GiB and seems to work okay in my limited testing. Surprising how these quants can still run at such low bit rates!&lt;/p&gt; &lt;p&gt;Anyway, I wouldn't recommend these smol bois if you have enough RAM+VRAM to fit a more optimized larger quant, but if at least there are some options &amp;quot;For the desperate&amp;quot; haha...&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9u1teeqt4g4f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l19yud/iq1_smol_boi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l19yud/iq1_smol_boi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T05:26:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1e6ic</id>
    <title>Ignore the hype - AI companies still have no moat</title>
    <updated>2025-06-02T10:06:26+00:00</updated>
    <author>
      <name>/u/No_Tea2273</name>
      <uri>https://old.reddit.com/user/No_Tea2273</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1e6ic/ignore_the_hype_ai_companies_still_have_no_moat/"&gt; &lt;img alt="Ignore the hype - AI companies still have no moat" src="https://external-preview.redd.it/TawOSRI4o3WDthoH5zp4cL7vlpQPtqKfMqXniUZMdX0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc884e61e4fbae7ba821197e1b5320440aaab413" title="Ignore the hype - AI companies still have no moat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An article I wrote a while back, I think &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; still wins&lt;/p&gt; &lt;p&gt;The basis of it is that Every single AI tool â€“ has an open source alternative, every. single. one â€“ so programming wise, for a new company to implement these features is not a matter of development complexity but a matter of getting the biggest audience &lt;/p&gt; &lt;p&gt;Everything has an open source versioned alternative right now&lt;/p&gt; &lt;p&gt;Take for example&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Tea2273"&gt; /u/No_Tea2273 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://river.berlin/blog/there-is-still-no-moat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1e6ic/ignore_the_hype_ai_companies_still_have_no_moat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1e6ic/ignore_the_hype_ai_companies_still_have_no_moat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T10:06:26+00:00</published>
  </entry>
</feed>
