<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-31T07:34:57+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jnqmsg</id>
    <title>Am I the only one using LLMs with greedy decoding for coding?</title>
    <updated>2025-03-31T00:15:43+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using greedy decoding (i.e. always choose the most probable token by setting top_k=0 or temperature=0) for coding tasks. Are there better decoding / sampling params that will give me better results?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnqmsg/am_i_the_only_one_using_llms_with_greedy_decoding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnqmsg/am_i_the_only_one_using_llms_with_greedy_decoding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnqmsg/am_i_the_only_one_using_llms_with_greedy_decoding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T00:15:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jngswb</id>
    <title>Dou (ÈÅì) - Visual Knowledge Organization and Analysis Tool</title>
    <updated>2025-03-30T16:53:31+00:00</updated>
    <author>
      <name>/u/shokuninstudio</name>
      <uri>https://old.reddit.com/user/shokuninstudio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jngswb/dou_ÈÅì_visual_knowledge_organization_and_analysis/"&gt; &lt;img alt="Dou (ÈÅì) - Visual Knowledge Organization and Analysis Tool" src="https://external-preview.redd.it/GOPYVA45L4Hs0UVngiY3mnW7r51k7UY71hhV_hEB_XU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a2a61ea030a8d438936d0ede1ad351db268caa9" title="Dou (ÈÅì) - Visual Knowledge Organization and Analysis Tool" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shokuninstudio"&gt; /u/shokuninstudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/shokuninstudio/Dou"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jngswb/dou_ÈÅì_visual_knowledge_organization_and_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jngswb/dou_ÈÅì_visual_knowledge_organization_and_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T16:53:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnskpx</id>
    <title>What's the best middle-sized open weight model for python and JavaScript coding?</title>
    <updated>2025-03-31T02:00:22+00:00</updated>
    <author>
      <name>/u/Gerdel</name>
      <uri>https://old.reddit.com/user/Gerdel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building my own front end designed for dual GPUs using llamacpp with react and it is called GingerGUI. It's named after my favorite chess grandmaster FYI.&lt;/p&gt; &lt;p&gt;I find Gemini deeply unreliable. GPT even 4.5 also hallucinates and just delete code half the time. &lt;/p&gt; &lt;p&gt;Claude 3.7 has built most of it It is absolutely incredible but I run out of quota so damn quickly. I've got two GPUs, a 3090 and a 4060ti 16gb. I'm wondering if anything from Mistral small three upwards to command r 34b with various Qwen models in between might be helpful for this project, So I'm asking for advice here instead of testing them one at a time because that will just take forever. Sorry if this is a bit of a repeat post and people talk about this all the time. Things get updated so quickly though, maybe it's a good time to go over this again! Thanks in advance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gerdel"&gt; /u/Gerdel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnskpx/whats_the_best_middlesized_open_weight_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnskpx/whats_the_best_middlesized_open_weight_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnskpx/whats_the_best_middlesized_open_weight_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T02:00:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnld6i</id>
    <title>We experimented with developing cross language voice cloning TTS for Indic Languages</title>
    <updated>2025-03-30T20:11:03+00:00</updated>
    <author>
      <name>/u/Aquaaa3539</name>
      <uri>https://old.reddit.com/user/Aquaaa3539</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnld6i/we_experimented_with_developing_cross_language/"&gt; &lt;img alt="We experimented with developing cross language voice cloning TTS for Indic Languages" src="https://external-preview.redd.it/ZXB1a3Y5ZGl3dnJlMXyf8-rvm1C__Q4bDL3gJBkjO_bjkyMUPsobX80FiZpA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=039712a29c370d05620c2cf8ca6c56b1d00c0c18" title="We experimented with developing cross language voice cloning TTS for Indic Languages" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We at our startup FuturixAI experimented with developing cross language voice cloning TTS models for Indic Languages&lt;br /&gt; Here is the result&lt;/p&gt; &lt;p&gt;Currently developed for Hindi, Telegu and Marathi&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aquaaa3539"&gt; /u/Aquaaa3539 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h9sc3kdiwvre1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnld6i/we_experimented_with_developing_cross_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnld6i/we_experimented_with_developing_cross_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T20:11:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnr4i2</id>
    <title>How could I help improve llama.cpp?</title>
    <updated>2025-03-31T00:41:40+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm a Computer Engineering student. I have some experience with C and C++, but I've never worked on open-source projects as large as llama.cpp.&lt;br /&gt; I'd like to know how I could contribute and what would be the best way to get started.&lt;/p&gt; &lt;p&gt;Thank you for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnr4i2/how_could_i_help_improve_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnr4i2/how_could_i_help_improve_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnr4i2/how_could_i_help_improve_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T00:41:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnlzb6</id>
    <title>Free Search: Updates and Improvements.</title>
    <updated>2025-03-30T20:37:15+00:00</updated>
    <author>
      <name>/u/Far-Celebration-470</name>
      <uri>https://old.reddit.com/user/Far-Celebration-470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, &lt;/p&gt; &lt;p&gt;Last week, I open sourced Free Search API. It allows sourcing results from top search engines (including google, bing) for free. It uses searxng instances for this purpose. &lt;/p&gt; &lt;p&gt;I was overwhelmed by community's response and I am glad for all the support and suggestions. Today, I have pushed several improvements that make this API more stable. These improvements include &lt;/p&gt; &lt;p&gt;1) Parallel scrapping of search results for faster response&lt;br /&gt; 2) Markdown formatting of search results&lt;br /&gt; 3) Prioritizing SearXNG instances that have faster google response time&lt;br /&gt; 4) Update/Get endpoints for searxng instances. &lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/HanzlaJavaid/Free-Search/tree/main"&gt;https://github.com/HanzlaJavaid/Free-Search/tree/main&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Try the deployed version: &lt;a href="https://freesearch.replit.app/docs"&gt;https://freesearch.replit.app/docs&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I highly appreciate PRs, issues, stars, and any kind of feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Celebration-470"&gt; /u/Far-Celebration-470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnlzb6/free_search_updates_and_improvements/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnlzb6/free_search_updates_and_improvements/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnlzb6/free_search_updates_and_improvements/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T20:37:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnrfpp</id>
    <title>New llama model "themis" on lmarena</title>
    <updated>2025-03-31T00:58:08+00:00</updated>
    <author>
      <name>/u/Shyvadi</name>
      <uri>https://old.reddit.com/user/Shyvadi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its hidden and only available in battle but it said it was llama could this be llama 4?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shyvadi"&gt; /u/Shyvadi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnrfpp/new_llama_model_themis_on_lmarena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnrfpp/new_llama_model_themis_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnrfpp/new_llama_model_themis_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T00:58:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jndsj5</id>
    <title>We built a website where you can vote on Minecraft structures generated by AI</title>
    <updated>2025-03-30T14:37:03+00:00</updated>
    <author>
      <name>/u/civilunhinged</name>
      <uri>https://old.reddit.com/user/civilunhinged</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/civilunhinged"&gt; /u/civilunhinged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://mcbench.ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jndsj5/we_built_a_website_where_you_can_vote_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jndsj5/we_built_a_website_where_you_can_vote_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T14:37:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnbhdl</id>
    <title>I think I found llama 4 - the "cybele" model on lmarena. It's very, very good and revealed it name ‚ò∫Ô∏è</title>
    <updated>2025-03-30T12:36:19+00:00</updated>
    <author>
      <name>/u/Salty-Garage7777</name>
      <uri>https://old.reddit.com/user/Salty-Garage7777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have you had similar experience with this model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salty-Garage7777"&gt; /u/Salty-Garage7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnbhdl/i_think_i_found_llama_4_the_cybele_model_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnbhdl/i_think_i_found_llama_4_the_cybele_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnbhdl/i_think_i_found_llama_4_the_cybele_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T12:36:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnhuy3</id>
    <title>Llama 3.2 going insane on Facebook</title>
    <updated>2025-03-30T17:39:40+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnhuy3/llama_32_going_insane_on_facebook/"&gt; &lt;img alt="Llama 3.2 going insane on Facebook" src="https://b.thumbs.redditmedia.com/NqDMwH6Bz4GQgIvs8QbIfAzCKcWDnZaM3TyMCLpxkoc.jpg" title="Llama 3.2 going insane on Facebook" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It kept going like this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jnhuy3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnhuy3/llama_32_going_insane_on_facebook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnhuy3/llama_32_going_insane_on_facebook/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T17:39:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn5uto</id>
    <title>MacBook M4 Max isn't great for LLMs</title>
    <updated>2025-03-30T05:42:51+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had M1 Max and recently upgraded to M4 Max - inferance speed difference is huge improvement (~3x) but it's still much slower than 5 years old RTX 3090 you can get for 700$ USD. &lt;/p&gt; &lt;p&gt;While it's nice to be able to load large models, they're just not gonna be very usable on that machine. An example - pretty small 14b distilled Qwen 4bit quant runs pretty slow for coding (40tps, with diff frequently failing so needs to redo whole file), and quality is very low. 32b is pretty unusable via Roo Code and Cline because of low speed.&lt;/p&gt; &lt;p&gt;And this is the best a money can buy you as Apple laptop.&lt;/p&gt; &lt;p&gt;Those are very pricey machines and I don't see any mentions that they aren't practical for local AI. You likely better off getting 1-2 generations old Nvidia rig if really need it, or renting, or just paying for API, as quality/speed will be day and night without upfront cost. &lt;/p&gt; &lt;p&gt;If you're getting MBP - save yourselves thousands $ and just get minimal ram you need with a bit extra SSD, and use more specialized hardware for local AI. &lt;/p&gt; &lt;p&gt;It's an awesome machine, all I'm saying - it prob won't deliver if you have high AI expectations for it. &lt;/p&gt; &lt;p&gt;PS: to me, this is not about getting or not getting a MacBook. I've been getting them for 15 years now and think they are awesome. The top models might not be quite the AI beast you were hoping for dropping these kinda $$$$, this is all I'm saying. I've had M1 Max with 64GB for years, and after the initial euphoria of holy smokes I can run large stuff there - never did it again for the reasons mentioned above. M4 is much faster but does feel similar in that sense. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T05:42:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnwxw3</id>
    <title>I had Claude and Gemini Pro collaborate on a game. The result? 2048 Ultimate Edition</title>
    <updated>2025-03-31T06:30:54+00:00</updated>
    <author>
      <name>/u/eposnix</name>
      <uri>https://old.reddit.com/user/eposnix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I like both Claude and Gemini for coding, but for different reasons, so I had the idea to just put them in a loop and let them work with each other on a project. The prompt: &amp;quot;Make an amazing version of 2048.&amp;quot; They deliberated for about 10 minutes straight, bouncing ideas back and forth, and 2900+ lines of code later, output &lt;strong&gt;2048 Ultimate Edition&lt;/strong&gt; (they named it themselves).&lt;/p&gt; &lt;p&gt;The final version of their 2048 game boasted these features (none of which I asked for):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smooth animations&lt;/li&gt; &lt;li&gt;Difficulty settings&lt;/li&gt; &lt;li&gt;Adjustable grid sizes&lt;/li&gt; &lt;li&gt;In-game stats tracking (total moves, average score, etc.)&lt;/li&gt; &lt;li&gt;Save/load feature&lt;/li&gt; &lt;li&gt;Achievements system&lt;/li&gt; &lt;li&gt;Clean UI with keyboard &lt;em&gt;and&lt;/em&gt; swipe controls&lt;/li&gt; &lt;li&gt;Light/Dark mode toggle&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feel free to try it out here: &lt;a href="https://www.eposnix.com/AI/2048.html"&gt;https://www.eposnix.com/AI/2048.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, you can read their collaboration here: &lt;a href="https://pastebin.com/yqch19yy"&gt;https://pastebin.com/yqch19yy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;While this doesn't necessarily involve local models, this method can easily be adapted to use local models instead.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eposnix"&gt; /u/eposnix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnwxw3/i_had_claude_and_gemini_pro_collaborate_on_a_game/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnwxw3/i_had_claude_and_gemini_pro_collaborate_on_a_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnwxw3/i_had_claude_and_gemini_pro_collaborate_on_a_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T06:30:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnxlbn</id>
    <title>Warning: Fake deepseek v3.1 blog post</title>
    <updated>2025-03-31T07:20:42+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There has been this blog post recently circulating about the release of an alleged &amp;quot;Deepseek V3.1&amp;quot;, and after looking into the website, it seems like it is totally fake. Remember, deepseek does not have any official blog.&lt;/p&gt; &lt;p&gt;blog post: &lt;a href="https://deepseek.ai/blog/deepseek-v31"&gt;https://deepseek.ai/blog/deepseek-v31&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnxlbn/warning_fake_deepseek_v31_blog_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnxlbn/warning_fake_deepseek_v31_blog_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnxlbn/warning_fake_deepseek_v31_blog_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T07:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jngj5u</id>
    <title>I built a coding agent that allows qwen2.5-coder to use tools</title>
    <updated>2025-03-30T16:41:24+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jngj5u/i_built_a_coding_agent_that_allows_qwen25coder_to/"&gt; &lt;img alt="I built a coding agent that allows qwen2.5-coder to use tools" src="https://preview.redd.it/1erih6euuure1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5447b0990d64ea3d82b01889605650baf3b6948d" title="I built a coding agent that allows qwen2.5-coder to use tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1erih6euuure1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jngj5u/i_built_a_coding_agent_that_allows_qwen25coder_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jngj5u/i_built_a_coding_agent_that_allows_qwen25coder_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T16:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnegrp</id>
    <title>3 new Llama models inside LMArena (maybe LLama 4?)</title>
    <updated>2025-03-30T15:08:49+00:00</updated>
    <author>
      <name>/u/Straight-Worker-4327</name>
      <uri>https://old.reddit.com/user/Straight-Worker-4327</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnegrp/3_new_llama_models_inside_lmarena_maybe_llama_4/"&gt; &lt;img alt="3 new Llama models inside LMArena (maybe LLama 4?)" src="https://b.thumbs.redditmedia.com/dn-wlWwvH94ULQ168bBoDHch1sjJK-d3SZVT2HvBWwc.jpg" title="3 new Llama models inside LMArena (maybe LLama 4?)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Straight-Worker-4327"&gt; /u/Straight-Worker-4327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jnegrp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnegrp/3_new_llama_models_inside_lmarena_maybe_llama_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnegrp/3_new_llama_models_inside_lmarena_maybe_llama_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T15:08:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnc9rd</id>
    <title>It's not much, but its honest work! 4xRTX 3060 running 70b at 4x4x4x4x</title>
    <updated>2025-03-30T13:21:39+00:00</updated>
    <author>
      <name>/u/madaerodog</name>
      <uri>https://old.reddit.com/user/madaerodog</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnc9rd/its_not_much_but_its_honest_work_4xrtx_3060/"&gt; &lt;img alt="It's not much, but its honest work! 4xRTX 3060 running 70b at 4x4x4x4x" src="https://b.thumbs.redditmedia.com/xZgN0CnCg9_dkwL0g3ohDgCJu3nIHgZs9DZKGJ0a-FQ.jpg" title="It's not much, but its honest work! 4xRTX 3060 running 70b at 4x4x4x4x" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madaerodog"&gt; /u/madaerodog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jnc9rd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnc9rd/its_not_much_but_its_honest_work_4xrtx_3060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnc9rd/its_not_much_but_its_honest_work_4xrtx_3060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T13:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnsfb3</id>
    <title>Bailing Moe is now supported in llama.cpp</title>
    <updated>2025-03-31T01:51:45+00:00</updated>
    <author>
      <name>/u/MaruluVR</name>
      <uri>https://old.reddit.com/user/MaruluVR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been looking forward to this one, finally a new small MOE model. &lt;/p&gt; &lt;p&gt;Ling comes in 3 variants Lite (16.8B total 2.75B active), Lite Coder (16.8B total 2.75B active) and Plus (290B total 28.8B active).&lt;/p&gt; &lt;p&gt;With the small size they are perfectly suited for CPU inference.&lt;/p&gt; &lt;p&gt;It will be interesting to see how these compare to Qwen 3 MOE once that releases. &lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/collections/inclusionAI/ling-67c51c85b34a7ea0aba94c32"&gt;https://huggingface.co/collections/inclusionAI/ling-67c51c85b34a7ea0aba94c32&lt;/a&gt;&lt;/p&gt; &lt;p&gt;info about model: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jk96ei/ling_a_new_moe_model_series_including_linglite/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jk96ei/ling_a_new_moe_model_series_including_linglite/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pull request: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/12634#pullrequestreview-2727983571"&gt;https://github.com/ggml-org/llama.cpp/pull/12634#pullrequestreview-2727983571&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaruluVR"&gt; /u/MaruluVR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnsfb3/bailing_moe_is_now_supported_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnsfb3/bailing_moe_is_now_supported_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnsfb3/bailing_moe_is_now_supported_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T01:51:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnjrdk</id>
    <title>Benchmark: RTX 3090, 4090, and even 4080 are surprisingly strong for 1-person QwQ-32B inference. (but 5090 not yet)</title>
    <updated>2025-03-30T19:01:34+00:00</updated>
    <author>
      <name>/u/fxtentacle</name>
      <uri>https://old.reddit.com/user/fxtentacle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't want to send all of my code to any outside company, but I still want to use AI code completion. Accordingly, I was curious how fast various GPUs would be for hosting when there's only 1 user: me. I used vLLM and &lt;code&gt;QwQ-32B-Q4_K_M&lt;/code&gt; for benchmarking.&lt;/p&gt; &lt;p&gt;&lt;code&gt;median_ttft_ms&lt;/code&gt; measures how long it takes for the GPU to handle the context and parse my request. And then &lt;code&gt;median_otps&lt;/code&gt; is how many output tokens the GPU can generate per second. (OTPS = Output Tokens Per Second) Overall, the &lt;code&gt;median_ttft_ms&lt;/code&gt; values were all &amp;lt;1s unless the card was overloaded and I think they will rarely matter in practice. That means the race is on for the highest OTPS.&lt;/p&gt; &lt;p&gt;As expected, a H200 is fast with 334ms + 30 OTPS. The H100 NVL is still fast with 426ms + 23 OTPS. The &amp;quot;old&amp;quot; H100 with HBM3 is similar at 310ms + 22 OTPS.&lt;/p&gt; &lt;p&gt;But I did not expect 2x RTX 4080 to score 383ms + 33 OTPS, which is really close to the H200 and that's somewhat insane if you consider that I'm comparing a 34000‚Ç¨ datacenter product with a 1800‚Ç¨ home setup. An old pair of 2x RTX 3090 is also still pleasant at 564ms + 28 OTPS. And a (watercooled and gently overclocked) RTX 3090 TI rocked the ranking with 558ms + 36 OTPS. You can also clearly see that vLLM is not fully optimized for the RTX 5090 yet, because there the official docker image did not work (yet) and I had to compile from source and, still, the results were somewhat meh with 517ms + 18 TOPS, which is slightly slower than a single 4090.&lt;/p&gt; &lt;p&gt;You'll notice that the consumer GPUs are slower in the initial context and request parsing. That makes sense because that task is highly parallel, i.e. what datacenter products were optimized for. But due to higher clock speeds and more aggressive cooling, consumer GPUs outcompete both H100 and H200 at output token generation, which is the sequential part of the task.&lt;/p&gt; &lt;p&gt;Here's my raw result JSONs from &lt;code&gt;vllm/benchmarks/benchmark_serving.py&lt;/code&gt; and a table with even more hardware variations: &lt;a href="https://github.com/DeutscheKI/llm-performance-tests"&gt;https://github.com/DeutscheKI/llm-performance-tests&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyway, my take-aways from this would be:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;RAM clock dominates everything. OC for the win!&lt;/li&gt; &lt;li&gt;Go with 2x 4080 over a single 4090 or 5090.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fxtentacle"&gt; /u/fxtentacle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnjrdk/benchmark_rtx_3090_4090_and_even_4080_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnjrdk/benchmark_rtx_3090_4090_and_even_4080_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnjrdk/benchmark_rtx_3090_4090_and_even_4080_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T19:01:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnd6px</id>
    <title>LLMs over torrent</title>
    <updated>2025-03-30T14:08:12+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnd6px/llms_over_torrent/"&gt; &lt;img alt="LLMs over torrent" src="https://preview.redd.it/8z6t2vvu3ure1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ade8fa1e4ff10e2d71461fdb60f942583a4d442f" title="LLMs over torrent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Just messing around with an idea - serving LLM models over torrent. I‚Äôve uploaded Qwen2.5-VL-3B-Instruct to a seedbox sitting in a neutral datacenter in the Netherlands (hosted via Feralhosting).&lt;/p&gt; &lt;p&gt;If you wanna try it out, grab the torrent file here and load it up in any torrent client:&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="http://sbnb.astraeus.feralhosting.com/Qwen2.5-VL-3B-Instruct.torrent"&gt;http://sbnb.astraeus.feralhosting.com/Qwen2.5-VL-3B-Instruct.torrent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is just an experiment - no promises about uptime, speed, or anything really. It might work, it might not ü§∑&lt;/p&gt; &lt;p&gt;‚∏ª&lt;/p&gt; &lt;p&gt;Some random thoughts / open questions: 1. Only models with redistribution-friendly licenses (like Apache-2.0) can be shared this way. Qwen is cool, Mistral too. Stuff from Meta or Google gets more legally fuzzy - might need a lawyer to be sure. 2. If we actually wanted to host a big chunk of available models, we‚Äôd need a ton of seedboxes. Huggingface claims they store 45PB of data üòÖ üìé &lt;a href="https://huggingface.co/docs/hub/storage-backends"&gt;https://huggingface.co/docs/hub/storage-backends&lt;/a&gt; 3. Binary deduplication would help save space. Bonus points if we can do OTA-style patch updates to avoid re-downloading full models every time. 4. Why bother? AI‚Äôs getting more important, and putting everything in one place feels a bit risky long term. Torrents could be a good backup layer or alt-distribution method.&lt;/p&gt; &lt;p&gt;‚∏ª&lt;/p&gt; &lt;p&gt;Anyway, curious what people think. If you‚Äôve got ideas, feedback, or even some storage/bandwidth to spare, feel free to join the fun. Let‚Äôs see what breaks üòÑ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8z6t2vvu3ure1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnd6px/llms_over_torrent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnd6px/llms_over_torrent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T14:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnplb1</id>
    <title>MLX fork with speculative decoding in server</title>
    <updated>2025-03-30T23:22:58+00:00</updated>
    <author>
      <name>/u/LocoMod</name>
      <uri>https://old.reddit.com/user/LocoMod</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I forked mlx-lm and ported the speculative decoding from the generate command to the server command, so now we can launch an OpenAI compatible completions endpoint with it enabled. I‚Äôm working on tidying the tests up to submit PR to upstream but wanted to announce here in case anyone wanted this capability now. I get a 90% speed increase when using qwen coder 0.5 as draft model and 32b as main model.&lt;/p&gt; &lt;p&gt;&lt;code&gt; mlx_lm.server --host localhost --port 8080 --model ./Qwen2.5-Coder-32B-Instruct-8bit --draft-model ./Qwen2.5-Coder-0.5B-8bit &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/intelligencedev/mlx-lm/tree/add-server-draft-model-support/mlx_lm"&gt;https://github.com/intelligencedev/mlx-lm/tree/add-server-draft-model-support/mlx_lm&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LocoMod"&gt; /u/LocoMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnplb1/mlx_fork_with_speculative_decoding_in_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnplb1/mlx_fork_with_speculative_decoding_in_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnplb1/mlx_fork_with_speculative_decoding_in_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T23:22:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnvhkd</id>
    <title>The diminishing returns of larger models, perhaps you don't need to spend big on hardware for inference</title>
    <updated>2025-03-31T04:50:11+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been tracking the recent performance of models like Gemma 27B, QwQ 32B, and Mistral Small, and I'm starting to believe we're hitting a point of diminishing returns with the really large (70B+) LLMs. For a while, scaling to larger parameters was the path to better overall performance. But the gap is shrinking ‚Äì and shrinking fast.&lt;/p&gt; &lt;p&gt;Gemma3 27B consistently punches above its weight, often rivaling or exceeding Llama 3.3 70B on many benchmarks, especially when considering cost/performance. QwQ 32B is another excellent example. These aren't just &amp;quot;good for their size&amp;quot; ‚Äì they're legitimately competitive.&lt;/p&gt; &lt;p&gt;Why is this happening? A few factors:&lt;/p&gt; &lt;p&gt;- Distillation: We're getting really good at distilling knowledge from larger models into smaller ones.&lt;/p&gt; &lt;p&gt;- Architecture Improvements: Innovations in attention mechanisms, routing, and other architectural details are making smaller models more efficient.&lt;/p&gt; &lt;p&gt;- Data Quality: Better curated and more focused training datasets are allowing smaller models to learn more effectively.&lt;/p&gt; &lt;p&gt;- Diminishing Returns: Each doubling in parameter count yields a smaller and smaller improvement in performance. Going from 7B to 30B is a bigger leap than going from 30B to 70B and from 70 to 400B.&lt;/p&gt; &lt;p&gt;What does this mean for inference?&lt;/p&gt; &lt;p&gt;If you‚Äôre currently shelling out for expensive GPU time to run 70B+ models, consider this: the performance gap is closing. Investing in a ton of hardware today might only give you a marginal advantage that disappears in a few months.&lt;/p&gt; &lt;p&gt;If you can be patient, the advances happening in the 30B-50B range will likely deliver a lot of the benefits of larger models without the massive hardware requirements. What requires an H100 today may happily run on an RTX 4090 , or even more modest GPU, in the near future.&lt;/p&gt; &lt;p&gt;What are your thoughts?&lt;/p&gt; &lt;p&gt;TL;DR: Gemma, QwQ, and others are showing that smaller LLMs can be surprisingly competitive with larger ones. Don't overspend on hardware now ‚Äì the benefits of bigger models are rapidly becoming accessible in smaller packages.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnvhkd/the_diminishing_returns_of_larger_models_perhaps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnvhkd/the_diminishing_returns_of_larger_models_perhaps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnvhkd/the_diminishing_returns_of_larger_models_perhaps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T04:50:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnwh90</id>
    <title>We used AlphaMaze idea to train a robotics control model!</title>
    <updated>2025-03-31T05:57:54+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnwh90/we_used_alphamaze_idea_to_train_a_robotics/"&gt; &lt;img alt="We used AlphaMaze idea to train a robotics control model!" src="https://external-preview.redd.it/YXg2dnh6d3VzeXJlMdEk5BYEIzAqHbVyhNzLaxnyvsN1SHVgxmelOVR9PzS5.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9536f905923cbcb5516b647645a0f579b0ee3023" title="We used AlphaMaze idea to train a robotics control model!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, it‚Äôs me again, from Menlo Research (aka homebrew aka Jan)! We just launched a new experiment: AlphaSpace ‚Äì a robotics model that operates purely on semantic tokens, with no hardcoded rules or modality encoding!&lt;/p&gt; &lt;p&gt;In the previous release, AlphaSpace demonstrated spatial reasoning in a 2D (5x5) maze. The model's reasoning improved when applying GRPO. More importantly, the entire project was built by representing the maze using semantic tokens‚Äîwithout relying on modality encoding or encoders!&lt;/p&gt; &lt;p&gt;However, this experiment raises some key questions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How far can semantic tokens take us?&lt;/li&gt; &lt;li&gt;If 5x5 is too small, can this tokenization method scale to 100x100, or even 1000x1000?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To explore this, we conducted a new experiment called AlphaSpace, building on some ideas from AlphaMaze but with significant changes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Larger reasoning space: From 2D 5x5 to 3D 100x100x30.&lt;/li&gt; &lt;li&gt;No traditional visual representation‚Äîinstead, we generate synthetic reasoning data more systematically.&lt;/li&gt; &lt;li&gt;Testing the model on a robotics benchmark.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What makes AlphaSpace exciting?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Represents space purely through semantic tokens, without step-by-step planning.&lt;/li&gt; &lt;li&gt;No dependence on a modality encoder, making it easier to integrate into various systems without end-to-end training.&lt;/li&gt; &lt;li&gt;100% synthetic dataset.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out more details here:&lt;br /&gt; Paper: &lt;a href="https://arxiv.org/abs/2503.18769"&gt;https://arxiv.org/abs/2503.18769&lt;/a&gt;&lt;br /&gt; Model: &lt;a href="https://huggingface.co/homebrewltd/AlphaSpace-1.5B"&gt;https://huggingface.co/homebrewltd/AlphaSpace-1.5B&lt;/a&gt;&lt;br /&gt; Dataset: &lt;a href="https://huggingface.co/datasets/Menlo/Pick-Place-Table-Reasoning-local-pos-v0.2"&gt;https://huggingface.co/datasets/Menlo/Pick-Place-Table-Reasoning-local-pos-v0.2&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/menloresearch/space-thinker"&gt;https://github.com/menloresearch/space-thinker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://alphaspace.menlo.ai/"&gt;https://alphaspace.menlo.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SPOILER:&lt;br /&gt; - As much as we want to this model development has been halted a bit early and there are still many things we didn't account for when training the model, so just treat it as a small and fun experiment&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yw0asvwusyre1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnwh90/we_used_alphamaze_idea_to_train_a_robotics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnwh90/we_used_alphamaze_idea_to_train_a_robotics/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T05:57:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnfpnr</id>
    <title>It‚Äôs been 1000 releases and 5000 commits in llama.cpp</title>
    <updated>2025-03-30T16:04:30+00:00</updated>
    <author>
      <name>/u/Yes_but_I_think</name>
      <uri>https://old.reddit.com/user/Yes_but_I_think</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnfpnr/its_been_1000_releases_and_5000_commits_in/"&gt; &lt;img alt="It‚Äôs been 1000 releases and 5000 commits in llama.cpp" src="https://external-preview.redd.it/wyCM1fHzTa-IIqHgS1QTxdSYNXn668elDj0WmYMPf_k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0d58c9a49c1e9ce629e5b31dce17b727d8c6ab8" title="It‚Äôs been 1000 releases and 5000 commits in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;1000th release of llama.cpp&lt;/p&gt; &lt;p&gt;Almost 5000 commits. (4998)&lt;/p&gt; &lt;p&gt;It all started with llama 1 leak.&lt;/p&gt; &lt;p&gt;Thanks you team. Someone tag ‚Äòem if you know their handle.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Yes_but_I_think"&gt; /u/Yes_but_I_think &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnfpnr/its_been_1000_releases_and_5000_commits_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnfpnr/its_been_1000_releases_and_5000_commits_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T16:04:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnvqsg</id>
    <title>why is no one talking about Qwen 2.5 omni?</title>
    <updated>2025-03-31T05:06:57+00:00</updated>
    <author>
      <name>/u/brocolongo</name>
      <uri>https://old.reddit.com/user/brocolongo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems crazy to me the first multimodal with voice, image, and text gen open sourced and no one is talking about it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brocolongo"&gt; /u/brocolongo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnvqsg/why_is_no_one_talking_about_qwen_25_omni/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnvqsg/why_is_no_one_talking_about_qwen_25_omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnvqsg/why_is_no_one_talking_about_qwen_25_omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T05:06:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnuhwm</id>
    <title>Anthropic expiring paid credits - anyone successfully prevented this from happening? Feels like Anthropic is penalising customers who preload more money (for convenience) than just the bare minimum required every week/month</title>
    <updated>2025-03-31T03:48:33+00:00</updated>
    <author>
      <name>/u/superloser48</name>
      <uri>https://old.reddit.com/user/superloser48</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnuhwm/anthropic_expiring_paid_credits_anyone/"&gt; &lt;img alt="Anthropic expiring paid credits - anyone successfully prevented this from happening? Feels like Anthropic is penalising customers who preload more money (for convenience) than just the bare minimum required every week/month" src="https://preview.redd.it/vobhuow16yre1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6471885b98f091f1d6e2c29d73292e98f1c33229" title="Anthropic expiring paid credits - anyone successfully prevented this from happening? Feels like Anthropic is penalising customers who preload more money (for convenience) than just the bare minimum required every week/month" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/superloser48"&gt; /u/superloser48 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vobhuow16yre1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnuhwm/anthropic_expiring_paid_credits_anyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnuhwm/anthropic_expiring_paid_credits_anyone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T03:48:33+00:00</published>
  </entry>
</feed>
