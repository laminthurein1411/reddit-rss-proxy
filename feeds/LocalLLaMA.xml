<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-14T15:37:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kmhwah</id>
    <title>Open source robust LLM extractor for HTML/Markdown in Typescript</title>
    <updated>2025-05-14T15:21:05+00:00</updated>
    <author>
      <name>/u/Visual-Librarian6601</name>
      <uri>https://old.reddit.com/user/Visual-Librarian6601</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While working with LLMs for structured web data extraction, I kept running into issues with invalid JSON and broken links in the output. This led me to build a library focused on robust extraction and enrichment:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Clean HTML conversion&lt;/strong&gt;: transforms HTML into LLM-friendly markdown with an option to extract just the main content&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM structured output&lt;/strong&gt;: Uses Gemini 2.5 flash or GPT-4o mini to balance accuracy and cost. Can also also use custom prompt&lt;/li&gt; &lt;li&gt;&lt;strong&gt;JSON sanitization&lt;/strong&gt;: If the LLM structured output fails or doesn't fully match your schema, a sanitization process attempts to recover and fix the data, especially useful for deeply nested objects and arrays&lt;/li&gt; &lt;li&gt;&lt;strong&gt;URL validation&lt;/strong&gt;: all extracted URLs are validated - handling relative URLs, removing invalid ones, and repairing markdown-escaped links&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Github: &lt;a href="https://github.com/lightfeed/lightfeed-extract"&gt;https://github.com/lightfeed/lightfeed-extract&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love to hear if anyone else has experimented with LLMs for data extraction or if you have any questions about this approach!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Visual-Librarian6601"&gt; /u/Visual-Librarian6601 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmhwah/open_source_robust_llm_extractor_for_htmlmarkdown/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmhwah/open_source_robust_llm_extractor_for_htmlmarkdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmhwah/open_source_robust_llm_extractor_for_htmlmarkdown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T15:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmi6vl</id>
    <title>I updated the SmolVLM llama.cpp webcam demo to run locally in-browser on WebGPU.</title>
    <updated>2025-05-14T15:33:15+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi6vl/i_updated_the_smolvlm_llamacpp_webcam_demo_to_run/"&gt; &lt;img alt="I updated the SmolVLM llama.cpp webcam demo to run locally in-browser on WebGPU." src="https://external-preview.redd.it/Z3l2NXpmczhucjBmMUwcvEt1gWTYtmZHqUwsIc9aRH3JKfTLJ5UHo4J1H4An.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61d172895901d0b35dab0f76eb10b4c4648b8f5c" title="I updated the SmolVLM llama.cpp webcam demo to run locally in-browser on WebGPU." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/&lt;/a&gt;, I decided to update the llama.cpp server demo so that it runs 100% locally in-browser on WebGPU, using Transformers.js. This means you can simply visit the link and run the demo, without needing to install anything locally. &lt;/p&gt; &lt;p&gt;I hope you like it! &lt;a href="https://huggingface.co/spaces/webml-community/smolvlm-realtime-webgpu"&gt;https://huggingface.co/spaces/webml-community/smolvlm-realtime-webgpu&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PS: The source code is a single index.html file you can find in the &amp;quot;Files&amp;quot; section on the demo page.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/or5b3ks8nr0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi6vl/i_updated_the_smolvlm_llamacpp_webcam_demo_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi6vl/i_updated_the_smolvlm_llamacpp_webcam_demo_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T15:33:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1klrony</id>
    <title>The Scariest Thing In LLMs/AI Isn't the Models or the Math... It's the Names.</title>
    <updated>2025-05-13T17:19:33+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klrony/the_scariest_thing_in_llmsai_isnt_the_models_or/"&gt; &lt;img alt="The Scariest Thing In LLMs/AI Isn't the Models or the Math... It's the Names." src="https://preview.redd.it/p5s9pcsd1l0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=938c01763e5e47522657359535bc0c0b28ee9579" title="The Scariest Thing In LLMs/AI Isn't the Models or the Math... It's the Names." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p5s9pcsd1l0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klrony/the_scariest_thing_in_llmsai_isnt_the_models_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klrony/the_scariest_thing_in_llmsai_isnt_the_models_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T17:19:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1klqir8</id>
    <title>WizardLM Team has joined Tencent</title>
    <updated>2025-05-13T16:34:03+00:00</updated>
    <author>
      <name>/u/GTT444</name>
      <uri>https://old.reddit.com/user/GTT444</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klqir8/wizardlm_team_has_joined_tencent/"&gt; &lt;img alt="WizardLM Team has joined Tencent" src="https://external-preview.redd.it/ILHoDHQUFu7tKCNSAM9UVMgUHxifQhr_Q9wIcfRI8lA.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b01c639188d58a880692f842b9d003ae1c11a2f7" title="WizardLM Team has joined Tencent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See attached post, looks like they are training Tencent's Hunyuan Turbo Model's now? But I guess these models aren't open source or even available via API outside of China?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GTT444"&gt; /u/GTT444 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/CanXu20/status/1922303283890397264"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klqir8/wizardlm_team_has_joined_tencent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klqir8/wizardlm_team_has_joined_tencent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T16:34:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1km2jyz</id>
    <title>Gemini 2.5 exp death.</title>
    <updated>2025-05-14T00:57:56+00:00</updated>
    <author>
      <name>/u/brocolongo</name>
      <uri>https://old.reddit.com/user/brocolongo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now that 2.5 exp free it's dead, what alternatives are you guys using for coding ?üòû (Free alternatives) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brocolongo"&gt; /u/brocolongo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km2jyz/gemini_25_exp_death/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km2jyz/gemini_25_exp_death/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1km2jyz/gemini_25_exp_death/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T00:57:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmi3ra</id>
    <title>AMD Strix Halo (Ryzen AI Max+ 395) GPU LLM Performance</title>
    <updated>2025-05-14T15:29:44+00:00</updated>
    <author>
      <name>/u/randomfoo2</name>
      <uri>https://old.reddit.com/user/randomfoo2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been doing some (ongoing) testing on a Strix Halo system recently and with a bunch of desktop systems coming out, and very few advanced/serious GPU-based LLM performance reviews out there, I figured it might be worth sharing a few notes I've made on the current performance and state of software.&lt;/p&gt; &lt;p&gt;This post will primarily focus on LLM inference with the Strix Halo GPU on Linux (but the llama.cpp testing should be pretty relevant for Windows as well).&lt;/p&gt; &lt;p&gt;This post gets rejected with too many links so I'll just leave a single link for those that want to dive deeper: &lt;a href="https://llm-tracker.info/_TOORG/Strix-Halo"&gt;https://llm-tracker.info/_TOORG/Strix-Halo&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Raw Performance&lt;/h1&gt; &lt;p&gt;In terms of raw compute specs, the Ryzen AI Max 395's Radeon 8060S has 40 RDNA3.5 CUs. At a max clock of 2.9GHz this should have a peak of &lt;strong&gt;59.4 FP16/BF16 TFLOPS&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;code&gt; 512 ops/clock/CU * 40 CU * 2.9e9 clock / 1e12 = 59.392 FP16 TFLOPS &lt;/code&gt;&lt;/p&gt; &lt;p&gt;This peak value requires either WMMA or wave32 VOPD otherwise the max is halved.&lt;/p&gt; &lt;p&gt;Using mamf-finder to test, without hipBLASLt, it takes about 35 hours to test and only gets to &lt;strong&gt;5.1 BF16 TFLOPS&lt;/strong&gt; (&lt;strong&gt;&amp;lt;9%&lt;/strong&gt; max theoretical).&lt;/p&gt; &lt;p&gt;However, when run with hipBLASLt, this goes up to &lt;strong&gt;36.9 TFLOPS&lt;/strong&gt; (&lt;strong&gt;&amp;gt;60%&lt;/strong&gt; max theoretical) which is comparable to MI300X efficiency numbers.&lt;/p&gt; &lt;p&gt;On the memory bandwidth (MBW) front, &lt;code&gt;rocm_bandwidth_test&lt;/code&gt; gives about &lt;strong&gt;212 GB/s&lt;/strong&gt; peak bandwidth (DDR5-8000 on a 256-bit bus gives a theoretical peak MBW of &lt;strong&gt;256 GB/s&lt;/strong&gt;). This is roughly in line with the max MBW tested by ThePhawx, jack stone, and others on various Strix Halo systems.&lt;/p&gt; &lt;p&gt;One thing &lt;code&gt;rocm_bandwidth_test&lt;/code&gt; gives you is also CPU to GPU speed, which is &lt;em&gt;~84 GB/s&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;The system I am using is set to almost all of its memory dedicated to GPU - 8GB GART and 110 GB GTT and has a very high (&amp;gt;100W TDP).&lt;/p&gt; &lt;h1&gt;llama.cpp&lt;/h1&gt; &lt;p&gt;What most people probably want to know is how these chips perform with llama.cpp for bs=1 inference. &lt;/p&gt; &lt;p&gt;First I'll test with the standard TheBloke/Llama-2-7B-GGUF Q4_0 so you can easily compare to other tests like my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ghvwsj/llamacpp_compute_and_memory_bandwidth_efficiency/"&gt;previous compute and memory bandwidth efficiency tests across architectures&lt;/a&gt; or the official llama.cpp Apple Silicon M-series performance thread.&lt;/p&gt; &lt;p&gt;I ran with a number of different backends, and the results were actually pretty surprising:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Run&lt;/th&gt; &lt;th align="left"&gt;pp512 (t/s)&lt;/th&gt; &lt;th align="left"&gt;tg128 (t/s)&lt;/th&gt; &lt;th align="left"&gt;Max Mem (MiB)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU&lt;/td&gt; &lt;td align="left"&gt;294.64 ¬± 0.58&lt;/td&gt; &lt;td align="left"&gt;28.94 ¬± 0.04&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU + FA&lt;/td&gt; &lt;td align="left"&gt;294.36 ¬± 3.13&lt;/td&gt; &lt;td align="left"&gt;29.42 ¬± 0.03&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP&lt;/td&gt; &lt;td align="left"&gt;348.96 ¬± 0.31&lt;/td&gt; &lt;td align="left"&gt;48.72 ¬± 0.01&lt;/td&gt; &lt;td align="left"&gt;4219&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP + FA&lt;/td&gt; &lt;td align="left"&gt;331.96 ¬± 0.41&lt;/td&gt; &lt;td align="left"&gt;45.78 ¬± 0.02&lt;/td&gt; &lt;td align="left"&gt;4245&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP + WMMA&lt;/td&gt; &lt;td align="left"&gt;322.63 ¬± 1.34&lt;/td&gt; &lt;td align="left"&gt;48.40 ¬± 0.02&lt;/td&gt; &lt;td align="left"&gt;4218&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP + WMMA + FA&lt;/td&gt; &lt;td align="left"&gt;343.91 ¬± 0.60&lt;/td&gt; &lt;td align="left"&gt;50.88 ¬± 0.01&lt;/td&gt; &lt;td align="left"&gt;4218&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;881.71 ¬± 1.71&lt;/td&gt; &lt;td align="left"&gt;52.22 ¬± 0.05&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3923&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan + FA&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;884.20 ¬± 6.23&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;52.73 ¬± 0.07&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3923&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The HIP version performs &lt;strong&gt;far&lt;/strong&gt; below what you'd expect in terms of tok/TFLOP efficiency for prompt processing even vs other RDNA3 architectures:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;gfx1103&lt;/code&gt; Radeon 780M iGPU gets 14.51 tok/TFLOP. At that efficiency you'd expect the about 850 tok/s that the Vulkan backend delivers.&lt;/li&gt; &lt;li&gt;&lt;code&gt;gfx1100&lt;/code&gt; Radeon 7900 XTX gets 25.12 tok/TFLOP. At that efficiency you'd expect almost 1500 tok/s, almost double what the Vulkan backend delivers, and &amp;gt;4X what the current HIP backend delivers.&lt;/li&gt; &lt;li&gt;HIP pp512 barely beats out CPU backend numbers. I don't have an explanation for this.&lt;/li&gt; &lt;li&gt;Just for a reference of how bad the HIP performance is, an 18CU M3 Pro has ~12.8 FP16 TFLOPS (4.6X less compute than Strix Halo) and delivers about the same pp512. Lunar Lake Arc 140V has 32 FP16 TFLOPS (almost 1/2 Strix Halo) and has a pp512 of 657 tok/s (1.9X faster)&lt;/li&gt; &lt;li&gt;With the Vulkan backend pp512 is about the same as an M4 Max and tg128 is about equivalent to an M4 Pro&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Testing a similar system with Linux 6.14 vs 6.15 showed a 15% performance difference so it's possible future driver/platform updates will improve/fix Strix Halo's ROCm/HIP compute efficiency problems.&lt;/p&gt; &lt;p&gt;So that's a bit grim, but I did want to point out one silver lining. With the recent fixes for Flash Attention with the llama.cpp Vulkan backend, I did some higher context testing, and here, the HIP + rocWMMA backend actually shows some strength. It has basically &lt;strong&gt;no decrease in either pp or tg performance at 8K context&lt;/strong&gt; and uses the least memory to boot:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Run&lt;/th&gt; &lt;th align="left"&gt;pp8192 (t/s)&lt;/th&gt; &lt;th align="left"&gt;tg8192 (t/s)&lt;/th&gt; &lt;th align="left"&gt;Max Mem (MiB)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP&lt;/td&gt; &lt;td align="left"&gt;245.59 ¬± 0.10&lt;/td&gt; &lt;td align="left"&gt;12.43 ¬± 0.00&lt;/td&gt; &lt;td align="left"&gt;6+10591&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP + FA&lt;/td&gt; &lt;td align="left"&gt;190.86 ¬± 0.49&lt;/td&gt; &lt;td align="left"&gt;30.01 ¬± 0.00&lt;/td&gt; &lt;td align="left"&gt;7+8089&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP + WMMA&lt;/td&gt; &lt;td align="left"&gt;230.10 ¬± 0.70&lt;/td&gt; &lt;td align="left"&gt;12.37 ¬± 0.00&lt;/td&gt; &lt;td align="left"&gt;6+10590&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP + WMMA + FA&lt;/td&gt; &lt;td align="left"&gt;368.77 ¬± 1.22&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;50.97 ¬± 0.00&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;7+8062&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;487.69 ¬± 0.83&lt;/td&gt; &lt;td align="left"&gt;7.54 ¬± 0.02&lt;/td&gt; &lt;td align="left"&gt;7761+1180&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan + FA&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;490.18 ¬± 4.89&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;32.03 ¬± 0.01&lt;/td&gt; &lt;td align="left"&gt;7767+1180&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;ul&gt; &lt;li&gt;You need to have &lt;code&gt;rocmwmma&lt;/code&gt; installed - many distros have packages but you need gfx1151 support is very new (#PR 538) from last week) so you will probably need to build your own rocWMMA from source&lt;/li&gt; &lt;li&gt;You should then rebuild llama.cpp with &lt;code&gt;-DGGML_HIP_ROCWMMA_FATTN=ON&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you mostly do 1-shot inference, then the Vulkan + FA backend is actually probably the best and is the most cross-platform/easy option. If you frequently have longer conversations then HIP + WMMA + FA is probalby the way to go, even if prompt processing is much slower than it should be right now.&lt;/p&gt; &lt;p&gt;I also ran some tests with Qwen3-30B-A3B UD-Q4_K_XL. Larger MoEs is where these large unified memory APUs really shine. &lt;/p&gt; &lt;p&gt;Here are Vulkan results. One thing worth noting, and this is particular to the Qwen3 MoE and Vulkan backend, but using &lt;code&gt;-b 256&lt;/code&gt; significantly improves the pp512 performance:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Run&lt;/th&gt; &lt;th align="left"&gt;pp512 (t/s)&lt;/th&gt; &lt;th align="left"&gt;tg128 (t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;70.03 ¬± 0.18&lt;/td&gt; &lt;td align="left"&gt;75.32 ¬± 0.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan b256&lt;/td&gt; &lt;td align="left"&gt;118.78 ¬± 0.64&lt;/td&gt; &lt;td align="left"&gt;74.76 ¬± 0.07&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;While the pp512 is slow, tg128 is as speedy as you'd expect for 3B activations.&lt;/p&gt; &lt;p&gt;This is still only a 16.5 GB model though, so let's go bigger. Llama 4 Scout is 109B parameters and 17B activations and the UD-Q4_K_XL is 57.93 GiB.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Run&lt;/th&gt; &lt;th align="left"&gt;pp512 (t/s)&lt;/th&gt; &lt;th align="left"&gt;tg128 (t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;102.61 ¬± 1.02&lt;/td&gt; &lt;td align="left"&gt;20.23 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP&lt;/td&gt; &lt;td align="left"&gt;GPU Hang&lt;/td&gt; &lt;td align="left"&gt;GPU Hang&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;While Llama 4 has had a rocky launch, this is a model that performs about as well as Llama 3.3 70B, but tg is 4X faster, and has SOTA vision as well, so having this speed for tg is a real win.&lt;/p&gt; &lt;p&gt;I've also been able to successfully RPC llama.cpp to test some truly massive (Llama 4 Maverick, Qwen 235B-A22B models, but I'll leave that for a future followup).&lt;/p&gt; &lt;p&gt;Besides romWMMA, I was able to build a ROCm 6.4 image for Strix Halo (gfx1151) using &lt;a href="/u/scottt"&gt;u/scottt&lt;/a&gt;'s dockerfiles. These docker images have hipBLASLt built with gfx1151 support.&lt;/p&gt; &lt;p&gt;I was also able to build AOTriton without too much hassle (it takes about 1h wall time on Strix Halo if you restrict to just the gfx1151 GPU_TARGET).&lt;/p&gt; &lt;p&gt;Composable Kernel (CK) has gfx1151 support now as well and builds in about 15 minutes.&lt;/p&gt; &lt;p&gt;PyTorch was a huge PITA to build, but with a fair amount of elbow grease, I was able to get HEAD (2.8.0a0) compiling, however it still has problems with Flash Attention not working even with &lt;code&gt;TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL&lt;/code&gt; set.&lt;/p&gt; &lt;p&gt;There's a lot of active work ongoing for PyTorch. For those interested, I'd recommend checking out my linked docs.&lt;/p&gt; &lt;p&gt;I won't bother testing training or batch inference engines until at least PyTorch FA is sorted. Current testing shows fwd/bwd pass to be in the &lt;strong&gt;~1 TFLOPS&lt;/strong&gt; ballpark (very bad)...&lt;/p&gt; &lt;p&gt;This testing obviously isn't very comprehensive, but since there's very little out there, I figure I'd at least share some of the results, especially with the various Chinese Strix Halo mini PCs beginning to ship and with Computex around the corner.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomfoo2"&gt; /u/randomfoo2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T15:29:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmcdyt</id>
    <title>LLM - better chunking method</title>
    <updated>2025-05-14T11:07:03+00:00</updated>
    <author>
      <name>/u/Phoenix2990</name>
      <uri>https://old.reddit.com/user/Phoenix2990</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Problems with using an LLM to chunk:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Time/latency -&amp;gt; it takes time for the LLM to output all the chunks.&lt;/li&gt; &lt;li&gt;Hitting output context window cap -&amp;gt; since you‚Äôre essentially re-creating entire documents but in chunks, then you‚Äôll often hit the token capacity of the output window.&lt;/li&gt; &lt;li&gt;Cost - since your essentially outputting entire documents again, you r costs go up.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The method below helps all 3.&lt;/p&gt; &lt;p&gt;Method:&lt;/p&gt; &lt;p&gt;Step 1: assign an identification number to each and every sentence or paragraph in your document.&lt;/p&gt; &lt;p&gt;a) Use a standard python library to parse the document into chunks of paragraphs or sentences. b) assign an identification number to each, and every sentence.&lt;/p&gt; &lt;p&gt;Example sentence: Red Riding Hood went to the shops. She did not like the food that they had there.&lt;/p&gt; &lt;p&gt;Example output: &amp;lt;1&amp;gt; Red Riding Hood went to the shops.&amp;lt;/1&amp;gt;&amp;lt;2&amp;gt;She did not like the food that they had there.&amp;lt;/2&amp;gt;&lt;/p&gt; &lt;p&gt;Note: this can easily be done with very standard python libraries that identify sentences. It‚Äôs very fast.&lt;/p&gt; &lt;p&gt;You now have a method to identify sentences using a single digit. The LLM will now take advantage of this.&lt;/p&gt; &lt;p&gt;Step 2. a) Send the entire document WITH the identification numbers associated to each sentence. b) tell the LLM ‚Äúhow‚Äùyou would like it to chunk the material I.e: ‚Äúplease keep semantic similar content together‚Äù c) tell the LLM that you have provided an I.d number for each sentence and that you want it to output only the i.d numbers e.g: chunk 1: 1,2,3 chunk 2: 4,5,6,7,8,9 chunk 3: 10,11,12,13&lt;/p&gt; &lt;p&gt;etc&lt;/p&gt; &lt;p&gt;Step 3: Reconstruct your chunks locally based on the LLM response. The LLM will provide you with the chunks and the sentence i.d‚Äôs that go into each chunk. All you need to do in your script is to re-construct it locally.&lt;/p&gt; &lt;p&gt;Notes:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I did this method a couple years ago using ORIGINAL Haiku. It never messed up the chunking method. So it will definitely work for new models.&lt;/li&gt; &lt;li&gt;although I only provide 2 sentences in my example, in reality I used this with many, many, many chunks. For example, I chunked large court cases using this method.&lt;/li&gt; &lt;li&gt;It‚Äôs actually a massive time and token save. Suddenly a 50 token sentence becomes ‚Äú1‚Äù token‚Ä¶.&lt;/li&gt; &lt;li&gt;If someone else already identified this method then please ignore this post :)&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Phoenix2990"&gt; /u/Phoenix2990 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmcdyt/llm_better_chunking_method/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmcdyt/llm_better_chunking_method/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmcdyt/llm_better_chunking_method/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T11:07:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1klvja8</id>
    <title>Local Benchmark on local models</title>
    <updated>2025-05-13T19:50:55+00:00</updated>
    <author>
      <name>/u/Expensive-Apricot-25</name>
      <uri>https://old.reddit.com/user/Expensive-Apricot-25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klvja8/local_benchmark_on_local_models/"&gt; &lt;img alt="Local Benchmark on local models" src="https://preview.redd.it/rrkggcovrl0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd6c517847054aeb3ac1cd91751e04d6c36c8c67" title="Local Benchmark on local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here are the results of the local models I have been testing over the last year. The test is a modified version of the HumanEval dataset. I picked this data set because there is no answer key to train on, and smaller models didn't seem to overfit it, so it seemed like a good enough benchmark.&lt;/p&gt; &lt;p&gt;I have been running this benchmark over the last year, and qwen 3 made HUGE strides on this benchmark, both reasoning and non-reasoning, very impressive. Most notably, qwen3:4b scores in the top 3 within margin of error.&lt;/p&gt; &lt;p&gt;I ran the benchmarks using ollama, all models are Q4 with the exception of gemma3 4b 16fp, which scored extremely low, and the reason is due to gemma3 arcitecture bugs when gemma3 was first released, and I just never re-tested it. I tried testing qwen3:30b reasoning, but I just dont have the proper hardware, and it would have taken a week.&lt;/p&gt; &lt;p&gt;Anyways, thought it was interesting so I thought I'd share. Hope you guys find it interesting/helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expensive-Apricot-25"&gt; /u/Expensive-Apricot-25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rrkggcovrl0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klvja8/local_benchmark_on_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klvja8/local_benchmark_on_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T19:50:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1klkmah</id>
    <title>Qwen3 Technical Report</title>
    <updated>2025-05-13T12:26:42+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klkmah/qwen3_technical_report/"&gt; &lt;img alt="Qwen3 Technical Report" src="https://preview.redd.it/kku7lzsulj0f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d8d566f0f7c92d2b0575c613f30a76aafba7a29" title="Qwen3 Technical Report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 Technical Report released.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf"&gt;https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kku7lzsulj0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klkmah/qwen3_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klkmah/qwen3_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T12:26:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmhb0c</id>
    <title>SWE-rebench: A continuously updated benchmark for SWE LLMs</title>
    <updated>2025-05-14T14:57:47+00:00</updated>
    <author>
      <name>/u/Fabulous_Pollution10</name>
      <uri>https://old.reddit.com/user/Fabulous_Pollution10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmhb0c/swerebench_a_continuously_updated_benchmark_for/"&gt; &lt;img alt="SWE-rebench: A continuously updated benchmark for SWE LLMs" src="https://external-preview.redd.it/qiTXPrfyonyQjbl3SeR1ri8_ePNvw_vqoI1O6pcB2Ho.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93143daeda204e9289a0f20f090a660db25d1840" title="SWE-rebench: A continuously updated benchmark for SWE LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! We present &lt;a href="https://swe-rebench.com/"&gt;SWE-rebench&lt;/a&gt; ‚Äî a new benchmark for evaluating agentic LLMs on a continuously updated and decontaminated set of real-world software engineering tasks, mined from active GitHub repositories.&lt;/p&gt; &lt;p&gt;SWE-rebench combines the methodologies of SWE-bench and LiveCodeBench: we collect new issues from a wide range of repositories and evaluate how agents powered by different models solve them. The leaderboard will be continuously updated with new issues and models!&lt;/p&gt; &lt;p&gt;Let us know which models you'd like us to evaluate.&lt;br /&gt; Stay tuned!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gc3mvvuzfr0f1.png?width=2250&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f465a5b37ef1a75db08982762c37f4c19ddfe33d"&gt;https://preview.redd.it/gc3mvvuzfr0f1.png?width=2250&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f465a5b37ef1a75db08982762c37f4c19ddfe33d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Pollution10"&gt; /u/Fabulous_Pollution10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmhb0c/swerebench_a_continuously_updated_benchmark_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmhb0c/swerebench_a_continuously_updated_benchmark_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmhb0c/swerebench_a_continuously_updated_benchmark_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T14:57:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmi59x</id>
    <title>Stable Audio Open Small - new fast audio generation model</title>
    <updated>2025-05-14T15:31:25+00:00</updated>
    <author>
      <name>/u/iGermanProd</name>
      <uri>https://old.reddit.com/user/iGermanProd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Weights&lt;/strong&gt;: &lt;a href="https://huggingface.co/stabilityai/stable-audio-open-small"&gt;https://huggingface.co/stabilityai/stable-audio-open-small&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2505.08175"&gt;https://arxiv.org/abs/2505.08175&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Arm learning path&lt;/strong&gt;: &lt;a href="https://learn.arm.com/learning-paths/mobile-graphics-and-gaming/run-stable-audio-open-small-with-lite-rt"&gt;https://learn.arm.com/learning-paths/mobile-graphics-and-gaming/run-stable-audio-open-small-with-lite-rt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The last link has some demos, they claim 30% faster than realtime!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iGermanProd"&gt; /u/iGermanProd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi59x/stable_audio_open_small_new_fast_audio_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi59x/stable_audio_open_small_new_fast_audio_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi59x/stable_audio_open_small_new_fast_audio_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T15:31:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1km5p7a</id>
    <title>Aya Vision: Advancing the Frontier of Multilingual Multimodality</title>
    <updated>2025-05-14T03:41:24+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates highquality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Aya-Vision-8B: &lt;a href="https://huggingface.co/CohereLabs/aya-vision-8B"&gt;https://huggingface.co/CohereLabs/aya-vision-8B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Aya-Vision-32B: &lt;a href="https://huggingface.co/CohereLabs/aya-vision-32B"&gt;https://huggingface.co/CohereLabs/aya-vision-32B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;AyaVisionBench: &lt;a href="https://huggingface.co/datasets/CohereLabs/AyaVisionBench"&gt;https://huggingface.co/datasets/CohereLabs/AyaVisionBench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/pdf/2505.08751"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km5p7a/aya_vision_advancing_the_frontier_of_multilingual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1km5p7a/aya_vision_advancing_the_frontier_of_multilingual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T03:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kma6tm</id>
    <title>Found a pretty good cline-compatible Qwen3 MoE for Apple Silicon</title>
    <updated>2025-05-14T08:36:49+00:00</updated>
    <author>
      <name>/u/FluffyGoatNerder</name>
      <uri>https://old.reddit.com/user/FluffyGoatNerder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I regularly test new models appearing on ollama's directory for use on my Mac M2 Ultra. Sparse models load tokens faster on Silicon so MoEs are models I target. &lt;a href="https://www.ollama.com/mychen76/qwen3_cline_roocode:30b"&gt;mychen76/qwen3_cline_roocode:30b &lt;/a&gt;is a MoE of qwen3 and so far, it has performed very well. The same user has also produced a 128k context window version (non-MoE) but this does not (yet) load on ollama. Just FYI since I often use stuff from here and often forget to feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FluffyGoatNerder"&gt; /u/FluffyGoatNerder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kma6tm/found_a_pretty_good_clinecompatible_qwen3_moe_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kma6tm/found_a_pretty_good_clinecompatible_qwen3_moe_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kma6tm/found_a_pretty_good_clinecompatible_qwen3_moe_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T08:36:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1klrio8</id>
    <title>LLM trained to gaslight people</title>
    <updated>2025-05-13T17:13:04+00:00</updated>
    <author>
      <name>/u/LividResearcher7818</name>
      <uri>https://old.reddit.com/user/LividResearcher7818</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I finetuned gemma 3 12b using RL to be an expert at gaslighting and demeaning it‚Äôs users. I‚Äôve been training LLMs using RL with soft rewards for a while now, and seeing OpenAI‚Äôs experiments with sycophancy I wanted to see if we can apply it to make the model behave on the other end of the spectrum..&lt;/p&gt; &lt;p&gt;It is not perfect (i guess no eval exists for measuring this), but can be really good in some situations.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.gaslight-gpt.com/"&gt;https://www.gaslight-gpt.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(A lot of people using the website at once, way more than my single gpu machine can handle so i will share weights on hf)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LividResearcher7818"&gt; /u/LividResearcher7818 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klrio8/llm_trained_to_gaslight_people/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klrio8/llm_trained_to_gaslight_people/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klrio8/llm_trained_to_gaslight_people/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T17:13:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1klxlbx</id>
    <title>BitNet Finetunes of R1 Distills</title>
    <updated>2025-05-13T21:12:14+00:00</updated>
    <author>
      <name>/u/codys12</name>
      <uri>https://old.reddit.com/user/codys12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klxlbx/bitnet_finetunes_of_r1_distills/"&gt; &lt;img alt="BitNet Finetunes of R1 Distills" src="https://external-preview.redd.it/DkDKsAS_zzAadrbN0EABgGOgbPBi8t0wwT1ePj0VWZI.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7e8dd284821c06c24c1fb34c18947d53d363c4e" title="BitNet Finetunes of R1 Distills" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My group recently discovered that you can finetune directly to ternary ({-1, 0, 1}) BitNet if you add an extra RMS Norm to the intput of linear layers. We are releasing the preview of two models - bitnet-r1-llama-8b and bitnet-r1-qwen-32b. These models are &amp;lt;3GB and &amp;lt;10GB respectively.&lt;/p&gt; &lt;p&gt;We also have a PR out in HF transformers so that anyone can load these models with an extra RMS norm by changing the quant_config, and finetune themselves&lt;/p&gt; &lt;p&gt;Try these out and see if they are good for a BitNet model!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codys12"&gt; /u/codys12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/0xCodyS/status/1922077684948996229"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klxlbx/bitnet_finetunes_of_r1_distills/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klxlbx/bitnet_finetunes_of_r1_distills/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T21:12:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1km889x</id>
    <title>On-Device AgentCPM-GUI is Now Open-Source</title>
    <updated>2025-05-14T06:17:31+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km889x/ondevice_agentcpmgui_is_now_opensource/"&gt; &lt;img alt="On-Device AgentCPM-GUI is Now Open-Source" src="https://external-preview.redd.it/aDMycXVkdG93bzBmMdd4vZsHqnodJB44bgTX0N7YjbnpSNGmYM_uAYq-hEK7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e33aa65a2cab5ffebdbd994ff8d9266e282b88c0" title="On-Device AgentCPM-GUI is Now Open-Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Key Features: &lt;/p&gt; &lt;p&gt;- 1st open-source GUI agent finely tuned for Chinese apps&lt;/p&gt; &lt;p&gt;- RFT-enhanced reasoning abilities&lt;/p&gt; &lt;p&gt;- Compact action-space design&lt;/p&gt; &lt;p&gt;- High-quality GUI grounding&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9k8szctowo0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km889x/ondevice_agentcpmgui_is_now_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1km889x/ondevice_agentcpmgui_is_now_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T06:17:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmdzv0</id>
    <title>best small language model? around 2-10b parameters</title>
    <updated>2025-05-14T12:33:08+00:00</updated>
    <author>
      <name>/u/ThatIsNotIllegal</name>
      <uri>https://old.reddit.com/user/ThatIsNotIllegal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;whats the best small language model for chatting in english only, no need for any type of coding, math or multilingual capabilities, i've seen gemma and the smaller qwen models but are there any better alternatives that focus just on chatting/emotional intelligence? &lt;/p&gt; &lt;p&gt;sorry if my question seems stupid i'm still new to this :P&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThatIsNotIllegal"&gt; /u/ThatIsNotIllegal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmdzv0/best_small_language_model_around_210b_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmdzv0/best_small_language_model_around_210b_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmdzv0/best_small_language_model_around_210b_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T12:33:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmhr87</id>
    <title>Drummer's Snowpiercer 15B v1 - Trudge through the winter with a finetune of Nemotron 15B Thinker!</title>
    <updated>2025-05-14T15:15:31+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmhr87/drummers_snowpiercer_15b_v1_trudge_through_the/"&gt; &lt;img alt="Drummer's Snowpiercer 15B v1 - Trudge through the winter with a finetune of Nemotron 15B Thinker!" src="https://external-preview.redd.it/vaSJWfDvrVhyb2X2lFu4a2nMHg68l5zMzNqYLj2vNZ8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=059aaa73054b57a14497284f4a9f9a7d64c69435" title="Drummer's Snowpiercer 15B v1 - Trudge through the winter with a finetune of Nemotron 15B Thinker!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Snowpiercer-15B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmhr87/drummers_snowpiercer_15b_v1_trudge_through_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmhr87/drummers_snowpiercer_15b_v1_trudge_through_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T15:15:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kme2c4</id>
    <title>Build DeepSeek architecture from scratch | 20 high quality video lectures</title>
    <updated>2025-05-14T12:36:36+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kme2c4/build_deepseek_architecture_from_scratch_20_high/"&gt; &lt;img alt="Build DeepSeek architecture from scratch | 20 high quality video lectures" src="https://external-preview.redd.it/KAbXE4K5sDdk4MosCKTIZy94mD_n03QyKwLpBwLHH7s.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66d99eb54310442088ed7f01364f74ed3363b88f" title="Build DeepSeek architecture from scratch | 20 high quality video lectures" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/of6lxo00sq0f1.gif"&gt;A few notes I made as part of this playlist&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are the 20 lectures covering everything from Multi-Head Latent Attention to Mixture of Experts. &lt;/p&gt; &lt;p&gt;It took me 2 months to finish recording these lectures. &lt;/p&gt; &lt;p&gt;One of the most challenging (and also rewarding) thing I have done this year. &lt;/p&gt; &lt;p&gt;Until now, we have uploaded 20 lectures in this playlist: &lt;/p&gt; &lt;p&gt;(1) DeepSeek series introduction: &lt;a href="https://youtu.be/QWNxQIq0hMo"&gt;https://youtu.be/QWNxQIq0hMo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(2) DeepSeek basics: &lt;a href="https://youtu.be/WjhDDeZ7DvM"&gt;https://youtu.be/WjhDDeZ7DvM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(3) Journey of a token into the LLM architecture: &lt;a href="https://youtu.be/rkEYwH4UGa4"&gt;https://youtu.be/rkEYwH4UGa4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(4) Attention mechanism explained in 1 hour: &lt;a href="https://youtu.be/K45ze9Yd5UE"&gt;https://youtu.be/K45ze9Yd5UE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(5) Self Attention Mechanism - Handwritten from scratch: &lt;a href="https://youtu.be/s8mskq-nzec"&gt;https://youtu.be/s8mskq-nzec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(6) Causal Attention Explained: Don't Peek into the Future: &lt;a href="https://youtu.be/c6Kkj6iLeBg"&gt;https://youtu.be/c6Kkj6iLeBg&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(7) Multi-Head Attention Visually Explained: &lt;a href="https://youtu.be/qbN4ulK-bZA"&gt;https://youtu.be/qbN4ulK-bZA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(8) Multi-Head Attention Handwritten from Scratch: &lt;a href="https://youtu.be/rvsEW-EsD-Y"&gt;https://youtu.be/rvsEW-EsD-Y&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(9) Key Value Cache from Scratch: &lt;a href="https://youtu.be/IDwTiS4_bKo"&gt;https://youtu.be/IDwTiS4_bKo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(10) Multi-Query Attention Explained: &lt;a href="https://youtu.be/Z6B51Odtn-Y"&gt;https://youtu.be/Z6B51Odtn-Y&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(11) Understand Grouped Query Attention (GQA): &lt;a href="https://youtu.be/kx3rETIxo4Q"&gt;https://youtu.be/kx3rETIxo4Q&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(12) Multi-Head Latent Attention From Scratch: &lt;a href="https://youtu.be/NlDQUj1olXM"&gt;https://youtu.be/NlDQUj1olXM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(13) Multi-Head Latent Attention Coded from Scratch in Python: &lt;a href="https://youtu.be/mIaWmJVrMpc"&gt;https://youtu.be/mIaWmJVrMpc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(14) Integer and Binary Positional Encodings: &lt;a href="https://youtu.be/rP0CoTxe5gU"&gt;https://youtu.be/rP0CoTxe5gU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(15) All about Sinusoidal Positional Encodings: &lt;a href="https://youtu.be/bQCQ7VO-TWU"&gt;https://youtu.be/bQCQ7VO-TWU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(16) Rotary Positional Encodings: &lt;a href="https://youtu.be/a17DlNxkv2k"&gt;https://youtu.be/a17DlNxkv2k&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(17) How DeepSeek exactly implemented Latent Attention | MLA + RoPE: &lt;a href="https://youtu.be/m1x8vA_Tscc"&gt;https://youtu.be/m1x8vA_Tscc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(18) Mixture of Experts (MoE) Introduction: &lt;a href="https://youtu.be/v7U21meXd6Y"&gt;https://youtu.be/v7U21meXd6Y&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(19) Mixture of Experts Hands on Demonstration: &lt;a href="https://youtu.be/yw6fpYPJ7PI"&gt;https://youtu.be/yw6fpYPJ7PI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(20) Mixture of Experts Balancing Techniques: &lt;a href="https://youtu.be/nRadcspta_8"&gt;https://youtu.be/nRadcspta_8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Next up: Multi-Token Prediction (MTP) and Fine-grained quantization.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kme2c4/build_deepseek_architecture_from_scratch_20_high/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kme2c4/build_deepseek_architecture_from_scratch_20_high/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kme2c4/build_deepseek_architecture_from_scratch_20_high/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T12:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmetlw</id>
    <title>GitHub - ByteDance-Seed/Seed1.5-VL: Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning, achieving state-of-the-art performance on 38 out of 60 public benchmarks.</title>
    <updated>2025-05-14T13:12:54+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmetlw/github_bytedanceseedseed15vl_seed15vl_a/"&gt; &lt;img alt="GitHub - ByteDance-Seed/Seed1.5-VL: Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning, achieving state-of-the-art performance on 38 out of 60 public benchmarks." src="https://external-preview.redd.it/0Gwi4j4952nP4TJd3fepu6BYEfG11JFAepo3FpZAd4E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08030f75958c411f48f1551b1ab776c4bb0ca72a" title="GitHub - ByteDance-Seed/Seed1.5-VL: Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning, achieving state-of-the-art performance on 38 out of 60 public benchmarks." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let's wait for the weights.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ByteDance-Seed/Seed1.5-VL"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmetlw/github_bytedanceseedseed15vl_seed15vl_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmetlw/github_bytedanceseedseed15vl_seed15vl_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T13:12:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1km81fb</id>
    <title>Embrace the jank (2x5090)</title>
    <updated>2025-05-14T06:04:35+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km81fb/embrace_the_jank_2x5090/"&gt; &lt;img alt="Embrace the jank (2x5090)" src="https://external-preview.redd.it/E_uF8bYPAY2RyGg_EbX05IxfyM8iqcKYDZnPrNcsqUo.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3550e05f24088bc37f358744f2b8d324c6207068" title="Embrace the jank (2x5090)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got a second 5090 to add to my 4x3090 setup as they have come down in price and have availability in my country now. Only to notice the Gigabyte model is way to long for this mining rig. ROPs are good luckily, this seem like later batches. Cable temps look good but I have the 5090 power limited to 400w and the 3090 to 250w&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1km81fb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km81fb/embrace_the_jank_2x5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1km81fb/embrace_the_jank_2x5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T06:04:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1km7azf</id>
    <title>US issues worldwide restriction on using Huawei AI chips</title>
    <updated>2025-05-14T05:17:14+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km7azf/us_issues_worldwide_restriction_on_using_huawei/"&gt; &lt;img alt="US issues worldwide restriction on using Huawei AI chips" src="https://external-preview.redd.it/soYDsx1CxZzYVuQCW5jcyDs7LrLivdc870--Rv91s1Y.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84f437471c1c3dd9791d2e3dd486dbfff8b54094" title="US issues worldwide restriction on using Huawei AI chips" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://asia.nikkei.com/Spotlight/Huawei-crackdown/US-issues-worldwide-restriction-on-using-Huawei-AI-chips"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km7azf/us_issues_worldwide_restriction_on_using_huawei/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1km7azf/us_issues_worldwide_restriction_on_using_huawei/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T05:17:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmaztr</id>
    <title>Announcing MAESTRO: A Local-First AI Research App! (Plus some benchmarks)</title>
    <updated>2025-05-14T09:35:43+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmaztr/announcing_maestro_a_localfirst_ai_research_app/"&gt; &lt;img alt="Announcing MAESTRO: A Local-First AI Research App! (Plus some benchmarks)" src="https://external-preview.redd.it/bCy94p-09BFETAgyXqeKanaFoQ80U0YpxsXUwomgXJQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=665b9b97c2e20b7fb7b2b64d0b32539fb52568c4" title="Announcing MAESTRO: A Local-First AI Research App! (Plus some benchmarks)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I'm excited to introduce &lt;strong&gt;MAESTRO&lt;/strong&gt; (Multi-Agent Execution System &amp;amp; Tool-driven Research Orchestrator), an AI-powered research application designed for deep research tasks, with a strong focus on local control and capabilities. You can set it up locally to conduct comprehensive research using your own document collections and your choice of local or API-based LLMs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/murtaza-nasir/maestro"&gt;MAESTRO on GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MAESTRO offers a modular framework with document ingestion, a powerful Retrieval-Augmented Generation (RAG) pipeline, and a multi-agent system (Planning, Research, Reflection, Writing) to tackle complex research questions. You can interact with it via a Streamlit Web UI or a command-line interface.&lt;/p&gt; &lt;h1&gt;Key Highlights:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local Deep Research:&lt;/strong&gt; Run it on your own machine.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Your LLMs:&lt;/strong&gt; Configure and use local LLM providers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Powerful RAG:&lt;/strong&gt; Ingest your PDFs into a local, queryable knowledge base with hybrid search.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Agent System:&lt;/strong&gt; Let AI agents collaborate on planning, information gathering, analysis, and report synthesis.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Batch Processing:&lt;/strong&gt; Create batch jobs with multiple research questions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transparency:&lt;/strong&gt; Track costs and resource usage.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;LLM Performance &amp;amp; Benchmarks:&lt;/h1&gt; &lt;p&gt;We've put a lot of effort into evaluating LLMs to ensure MAESTRO produces high-quality, factual reports. We used a panel of &amp;quot;verifier&amp;quot; LLMs to assess the performance of various models (including popular local options) in key research and writing tasks.&lt;/p&gt; &lt;p&gt;These benchmarks helped us identify strong candidates for different agent roles within MAESTRO, balancing performance on tasks like note generation and writing synthesis. While our evaluations included a mix of API-based and self-hostable models, we've provided specific recommendations and considerations for local setups in our documentation.&lt;/p&gt; &lt;p&gt;You can find all the details on our evaluation methodology, the full benchmark results (including performance heatmaps), and our model recommendations in the &lt;code&gt;VERIFIER_AND_MODEL_FINDINGS.md&lt;/code&gt; file within the repository.&lt;/p&gt; &lt;p&gt;For the future, we plan to improve the UI to move away from streamlit and create better documentation, in addition to improvements and additions in the agentic research framework itself.&lt;/p&gt; &lt;p&gt;We'd love for you to check out the &lt;a href="https://github.com/murtaza-nasir/maestro"&gt;project on GitHub&lt;/a&gt;, try it out, and share your feedback! We're especially interested in hearing from the LocalLLaMA community on how we can make it even better for local setups.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kmaztr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmaztr/announcing_maestro_a_localfirst_ai_research_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmaztr/announcing_maestro_a_localfirst_ai_research_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T09:35:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmg1ht</id>
    <title>Wan-AI/Wan2.1-VACE-14B ¬∑ Hugging Face (Apache-2.0)</title>
    <updated>2025-05-14T14:06:06+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmg1ht/wanaiwan21vace14b_hugging_face_apache20/"&gt; &lt;img alt="Wan-AI/Wan2.1-VACE-14B ¬∑ Hugging Face (Apache-2.0)" src="https://external-preview.redd.it/TmzIWNNChRov_gA4HjoE6PO2tsdMf2f2ESAHN00wPOY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c439fad5f75361eaa3eb176a04dfd9733c3e274" title="Wan-AI/Wan2.1-VACE-14B ¬∑ Hugging Face (Apache-2.0)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Wan2.1&lt;/strong&gt; &lt;a href="https://github.com/ali-vilab/VACE"&gt;VACE&lt;/a&gt;, an all-in-one model for video creation and editing&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Wan-AI/Wan2.1-VACE-14B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmg1ht/wanaiwan21vace14b_hugging_face_apache20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmg1ht/wanaiwan21vace14b_hugging_face_apache20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T14:06:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1klx9q2</id>
    <title>Real-time webcam demo with SmolVLM using llama.cpp</title>
    <updated>2025-05-13T20:59:50+00:00</updated>
    <author>
      <name>/u/dionisioalcaraz</name>
      <uri>https://old.reddit.com/user/dionisioalcaraz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"&gt; &lt;img alt="Real-time webcam demo with SmolVLM using llama.cpp" src="https://external-preview.redd.it/OHg0YjZidWQ0bTBmMduXqqISYSTmhZJt9j6zzJp3o5OEqUQPvF7tZjxvn6li.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dbb3d3b1a7db42b1a83c7e14926531c1ab78b9f" title="Real-time webcam demo with SmolVLM using llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dionisioalcaraz"&gt; /u/dionisioalcaraz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/81evi7ud4m0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T20:59:50+00:00</published>
  </entry>
</feed>
