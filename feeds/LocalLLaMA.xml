<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-13T13:35:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1io1txa</id>
    <title>OpenAI's plans for GPT4.5/GPT-5 - ETA weeks / months according to Sam</title>
    <updated>2025-02-12T21:09:09+00:00</updated>
    <author>
      <name>/u/rajwanur</name>
      <uri>https://old.reddit.com/user/rajwanur</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io1txa/openais_plans_for_gpt45gpt5_eta_weeks_months/"&gt; &lt;img alt="OpenAI's plans for GPT4.5/GPT-5 - ETA weeks / months according to Sam" src="https://external-preview.redd.it/C21I1UZsCNPoAR2CpLpEnL-d9RF9Rx4gseKID9bem40.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92adc0a85147e7c6ef3687d2dd3114dd7c01753f" title="OpenAI's plans for GPT4.5/GPT-5 - ETA weeks / months according to Sam" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/2skvct3pwrie1.png?width=602&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4d2bdff65bcb8e941840064badc168abfdc6db9"&gt;https://preview.redd.it/2skvct3pwrie1.png?width=602&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4d2bdff65bcb8e941840064badc168abfdc6db9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3k86ernrwrie1.png?width=594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b1690aa3128a046ca97dda9c08b1a4f5df72cf2"&gt;https://preview.redd.it/3k86ernrwrie1.png?width=594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b1690aa3128a046ca97dda9c08b1a4f5df72cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Link to the post: &lt;a href="https://x.com/sama/status/1889755723078443244"&gt;https://x.com/sama/status/1889755723078443244&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rajwanur"&gt; /u/rajwanur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io1txa/openais_plans_for_gpt45gpt5_eta_weeks_months/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io1txa/openais_plans_for_gpt45gpt5_eta_weeks_months/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io1txa/openais_plans_for_gpt45gpt5_eta_weeks_months/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T21:09:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioj04s</id>
    <title>Implementing a Forward Pass of DeepSeek-R1-Distill-Qwen-1.5B in Pure Python?</title>
    <updated>2025-02-13T13:27:17+00:00</updated>
    <author>
      <name>/u/Dxbson</name>
      <uri>https://old.reddit.com/user/Dxbson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm a developer with plenty of coding experience, but completely new to deep learning and machine learning. To better understand how LLMs work, I thought what better way than to implement a forward pass from scratch in pure Python‚Äîno dependencies, just standard Python libraries. I know this won‚Äôt be efficient, but speed isn‚Äôt my goal. I just want to grasp the inner workings of transformers through hands-on implementation.&lt;/p&gt; &lt;p&gt;I got this idea after discovering &lt;a href="https://github.com/karpathy/llama2.c/tree/master"&gt;Andrej Karpathy's llama2.c&lt;/a&gt;, which is an inference implementation for Llama-2 in pure, which I'm sure you guys are familiar with around here. In that repo, he manually implements the matrix multiplications, softmax, tokenization, sampling, decoding etc. I‚Äôd love to do the same but in Python (C might come later), and I figured a good model to try this on would be &lt;strong&gt;DeepSeek-R1-Distill-Qwen-1.5B&lt;/strong&gt; since it‚Äôs a distilled model and more manageable than larger LLMs.&lt;/p&gt; &lt;p&gt;I found &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/tree/main"&gt;this Hugging Face repo&lt;/a&gt;, which contains config files, tokenizer info and safetensors, but I‚Äôm struggling to determine the actual architecture of the model. Should I be referencing &lt;strong&gt;DeepSeek-V3&lt;/strong&gt; or &lt;strong&gt;Qwen-1.5B&lt;/strong&gt;? Where can I find the exact model architecture, including layer details, attention mechanisms?&lt;/p&gt; &lt;p&gt;I know they have some demo code in the &lt;a href="https://github.com/deepseek-ai/DeepSeek-V3/tree/main/inference"&gt;DeepSeek-V3 repo&lt;/a&gt;, but I'm unsure if I can utilize this same architecture here on this distilled model. They do say &amp;quot;&lt;em&gt;DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1. We slightly change their configs and tokenizers. Please use our setting to run these models.&lt;/em&gt;, but I'm not really sure what to make of this, and which setting they are referring to. &lt;/p&gt; &lt;p&gt;If anyone has pointers on where to start (e.g, resources on interpreting these config files, implementing a transformer forward pass from scratch, or whether I should be using Qwen‚Äôs or DeepSeek‚Äôs model architecture), I‚Äôd really appreciate it :)&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dxbson"&gt; /u/Dxbson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioj04s/implementing_a_forward_pass_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioj04s/implementing_a_forward_pass_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioj04s/implementing_a_forward_pass_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T13:27:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioewvw</id>
    <title>WebRover 2.0 - AI Copilot for Browser Automation and Research Workflows</title>
    <updated>2025-02-13T08:53:08+00:00</updated>
    <author>
      <name>/u/Elegant_Fish_3822</name>
      <uri>https://old.reddit.com/user/Elegant_Fish_3822</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioewvw/webrover_20_ai_copilot_for_browser_automation_and/"&gt; &lt;img alt="WebRover 2.0 - AI Copilot for Browser Automation and Research Workflows" src="https://external-preview.redd.it/PMbSHk0WW6PoDIccKf_6k0rFhzH7cvXADJSNQbeOQeM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5df5a740fb78975f904e1d12f013d08df810dc2" title="WebRover 2.0 - AI Copilot for Browser Automation and Research Workflows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever wondered if AI could autonomously navigate the web to perform complex research tasks‚Äîtasks that might take you hours or even days‚Äîwithout stumbling over context limitations like existing large language models?&lt;/p&gt; &lt;p&gt;Introducing WebRover 2.0, an open-source web automation agent that efficiently orchestrates complex research tasks using Langchains's agentic framework, LangGraph, and retrieval-augmented generation (RAG) pipelines. Simply provide the agent with a topic, and watch as it takes control of your browser to conduct human-like research.&lt;/p&gt; &lt;p&gt;I welcome your feedback, suggestions, and contributions to enhance WebRover further. Let's collaborate to push the boundaries of autonomous AI agents! üöÄ&lt;/p&gt; &lt;p&gt;Explore the the project on Github : &lt;a href="https://github.com/hrithikkoduri/WebRover"&gt;https://github.com/hrithikkoduri/WebRover&lt;/a&gt;&lt;/p&gt; &lt;p&gt;[Curious to see it in action? üé• In the demo video below, I prompted the deep research agent to write a detailed report on AI systems in healthcare. It autonomously browses the web, opens links, reads through webpages, self-reflects, and infers to build a comprehensive report with references. Additionally, it also opens Google Docs and types down the entire report for you to use later.]&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ioewvw/video/jzfc8ncjevie1/player"&gt;https://reddit.com/link/1ioewvw/video/jzfc8ncjevie1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Elegant_Fish_3822"&gt; /u/Elegant_Fish_3822 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioewvw/webrover_20_ai_copilot_for_browser_automation_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioewvw/webrover_20_ai_copilot_for_browser_automation_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioewvw/webrover_20_ai_copilot_for_browser_automation_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T08:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1iohaj5</id>
    <title>Correct my prompts for text summaries (Llama, but suggestions are welcome)</title>
    <updated>2025-02-13T11:46:44+00:00</updated>
    <author>
      <name>/u/brian-the-porpoise</name>
      <uri>https://old.reddit.com/user/brian-the-porpoise</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to build a little handy assistant that summarizes my digital notes for me. &lt;/p&gt; &lt;p&gt;I am using llama.cpp with llama-3.2-3b-instruct-q8_0. &lt;/p&gt; &lt;p&gt;No matter what I try as prompts, the result is not adhering to the requirements I set.&lt;br /&gt; E.g. I want it to narrate from the first person perspective, but it never does it. I specifically asked it to refer to me as &amp;quot;you&amp;quot;, doesnt do it either. &lt;/p&gt; &lt;p&gt;I have found that after going back and forth with it, it will eventually do as I require, but I would need a one-shot prompt so I can automate the whole thing. &lt;/p&gt; &lt;p&gt;The core issue is that it does not narrate from the desired POV, and continues to refer to the subject as &amp;quot;the speaker&amp;quot;, and that it keeps summarizing in bullet points. &lt;/p&gt; &lt;p&gt;Here are things I have tried: &lt;/p&gt; &lt;p&gt;&lt;code&gt;the following is a diary entry written by me. summarize it in 200 words. No bullet points. First person narration:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Assume you wrote the following text, but now need to make it more concise, 200 words or less.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Trying to assign a role, as suggested by other posts:&lt;/p&gt; &lt;p&gt;&lt;code&gt;You are an expert psychologist who needs to summarize your own diary entry in 200 words. This is the entry:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Turn the following text into a diary entry of max 200 words using &amp;quot;I&amp;quot; language/point of view:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Turn the following text into a diary entry narrated from YOUR perspective. Max 200 words:&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brian-the-porpoise"&gt; /u/brian-the-porpoise &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohaj5/correct_my_prompts_for_text_summaries_llama_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohaj5/correct_my_prompts_for_text_summaries_llama_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iohaj5/correct_my_prompts_for_text_summaries_llama_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T11:46:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioho0e</id>
    <title>stripping unecessary comments or anotations from results</title>
    <updated>2025-02-13T12:10:36+00:00</updated>
    <author>
      <name>/u/ben74940x</name>
      <uri>https://old.reddit.com/user/ben74940x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, given theses prompts for llava:7b I'm stuck with having various responses with comments, introductions, anotations, and repetition of the prompt itself, and I wanna get rid of theses in order to exploit raw responses ...&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;- Give the 20 main keywords without introduction in a raw response, separated by commas with no special characters, without any comment or anotations&lt;/p&gt; &lt;p&gt;- Describe me this image without comments or anotations for someone who cant see&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;=&amp;gt; where I get : &lt;/p&gt; &lt;p&gt;&lt;code&gt;- here's a list of keywords&lt;/code&gt;&lt;br /&gt; &lt;code&gt;- sure, here's the description ...&lt;/code&gt; &lt;/p&gt; &lt;p&gt;I'm also facing the same problem with llama3.2:3b on translation tasks:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;-Traduis moi ce texte en francais sans commentaire ni anotations: the quick fox jumps over the white rabbit&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;=&amp;gt; Where i get most of the time&lt;/p&gt; &lt;pre&gt;&lt;code&gt;- Ceci est une liste de mots - un texte √† traduire. Je vais donc simplement traduire les mots : - Voici la traduction en fran√ßais : - Remarque : Je suis d√©sol√©, mais je n'ai pas trouv√© de traduction parfaite &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;My question is hence simple, how can I get rid of these ?&lt;/p&gt; &lt;p&gt;Thanks in advance for any clue, notice, comment, enlightenment ;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ben74940x"&gt; /u/ben74940x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioho0e/stripping_unecessary_comments_or_anotations_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioho0e/stripping_unecessary_comments_or_anotations_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioho0e/stripping_unecessary_comments_or_anotations_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T12:10:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1io8qe0</id>
    <title>AceInstruct 1.5B / 7B / 72B by Nvidia</title>
    <updated>2025-02-13T02:24:12+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io8qe0/aceinstruct_15b_7b_72b_by_nvidia/"&gt; &lt;img alt="AceInstruct 1.5B / 7B / 72B by Nvidia" src="https://external-preview.redd.it/AW9WUUjiULOHbAfYY66Sx6D3OmGPFlGm47TagKzBqgo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94eb08024b4ddeaf5f136dca632fc922d506f5fb" title="AceInstruct 1.5B / 7B / 72B by Nvidia" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/nvidia/AceInstruct-1.5B"&gt;https://huggingface.co/nvidia/AceInstruct-1.5B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/AceInstruct-7B"&gt;https://huggingface.co/nvidia/AceInstruct-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/AceInstruct-72B"&gt;https://huggingface.co/nvidia/AceInstruct-72B&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We introduce AceInstruct, a family of advanced SFT models for coding, mathematics, and general-purpose tasks. The AceInstruct family, which includes AceInstruct-1.5B, 7B, and 72B, is &lt;strong&gt;Improved using Qwen&lt;/strong&gt;. These models are fine-tuned on Qwen2.5-Base using &lt;a href="https://huggingface.co/datasets/nvidia/AceMath-Instruct-Training-Data"&gt;general SFT datasets&lt;/a&gt;. These same datasets are also used in the training of &lt;a href="https://huggingface.co/nvidia/AceMath-72B-Instruct"&gt;AceMath-Instruct&lt;/a&gt;. Different from AceMath-Instruct which is specialized for math questions, AceInstruct is versatile and can be applied to a wide range of domains. Benchmark evaluations across coding, mathematics, and general knowledge tasks demonstrate that AceInstruct delivers performance comparable to Qwen2.5-Instruct.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5v30ob7mgtie1.png?width=708&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c419909e48136207192ee44705b79c037068d73"&gt;https://preview.redd.it/5v30ob7mgtie1.png?width=708&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c419909e48136207192ee44705b79c037068d73&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Bruh, from 1.5b to 7b and then straight up to 72b, it's the same disappointing release strategy as Meta Llama. I guess I'll keep using Qwen 2.5 32b until Qwen 3.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io8qe0/aceinstruct_15b_7b_72b_by_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io8qe0/aceinstruct_15b_7b_72b_by_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io8qe0/aceinstruct_15b_7b_72b_by_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T02:24:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioiwlv</id>
    <title>Lessons learned while deploying Deepseek R1 for multiple enterprises</title>
    <updated>2025-02-13T13:21:55+00:00</updated>
    <author>
      <name>/u/tempNull</name>
      <uri>https://old.reddit.com/user/tempNull</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the past few weeks, at &lt;a href="https://tensorfuse.io/"&gt;Tensorfuse&lt;/a&gt;, we have been busy deploying Deepseek R1 in all its forms (&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B"&gt;distilled&lt;/a&gt;, &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF"&gt;quantised&lt;/a&gt;, etc.) for many enterprises. This entire experience made us aware of the fact that there is very little awareness among enterprise engineers about how to serve an LLM and the metrics/systems around it. This post is a &amp;quot;things to remember&amp;quot; list around serving LLMs in the enterprise.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Don't fall into these Myths&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Myth 1: ‚ÄúHigher Precision = Higher Accuracy‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;FP16 matches FP32 accuracy for most LLMs.&lt;/li&gt; &lt;li&gt;AWQ 4-bit quantization achieves ~99% of FP16 quality.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Myth 2: ‚ÄúBatching = Higher Latency‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Modern frameworks batch without increasing latency via continuous in-flight batching.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Myth 3: ‚ÄúAdding More GPUs = Faster Inference‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;More GPUs only help for large models (e.g., Llama2-70B, GPT-3) by providing more memory for weights (minor speedups are noticed when moving from Ampere to Hopper, nothing significant)&lt;/li&gt; &lt;li&gt;Smaller models run best on a single optimised GPU.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. Always use an inference server like vLLM / SGLang. Avoid wrapping models in web frameworks without any optimisations.&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;FastAPI (and other web frameworks) is great for APIs but struggles with high-throughput inference because Python‚Äôs GIL introduces latency, manual batching is inefficient, and there is no built-in GPU scheduling.&lt;/li&gt; &lt;li&gt;vLLM, TGI, or Triton Server optimize inference with native batching, efficient memory management, and lower latency.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. Which inference engine is the Fastest ?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;When serving an LLM, throughput (measured in tokens per second, TPS) is a key metric. Below is a comparative analysis of top inference frameworks running deepseek-7B-distill variant on an Nvidia A100 (FP16):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Framework&lt;/th&gt; &lt;th&gt;TPS (7B Model)&lt;/th&gt; &lt;th&gt;Key Features &amp;amp; Strengths&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;vLLM&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;130-1800&lt;/td&gt; &lt;td&gt;PagedAttention, dynamic batching&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;SGLang&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;~180-5000&lt;/td&gt; &lt;td&gt;RadixAttention, prefix-sharing&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;TensorRT-LLM&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;220-743&lt;/td&gt; &lt;td&gt;Nvidia-optimized, FP8 support&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Triton Server&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;160-200&lt;/td&gt; &lt;td&gt;Dynamic batching, multi-framework&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Llama.cpp&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;20-90&lt;/td&gt; &lt;td&gt;CPU support, lightweight&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="http://mistral.rs"&gt;&lt;strong&gt;mistral.rs&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td&gt;~150-200&lt;/td&gt; &lt;td&gt;Rust-based, CPU/GPU efficiency&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;TGI (HF)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;180-220&lt;/td&gt; &lt;td&gt;Hugging Face integration, multi-GPU&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Lower limit is for long text single-stream and upper limit is for many concurrent streams**.**&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Heuristics for choosing your inference engine?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;High throughput needs? &lt;em&gt;vLLM&lt;/em&gt;, &lt;em&gt;SGLang&lt;/em&gt;, and &lt;em&gt;TensorRT-LLM&lt;/em&gt; are top choices.&lt;/li&gt; &lt;li&gt;CPU-based inference? &lt;em&gt;Llama.cpp&lt;/em&gt; or &lt;a href="http://mistral.rs"&gt;&lt;em&gt;mistral.rs&lt;/em&gt;&lt;/a&gt; are strong contenders.&lt;/li&gt; &lt;li&gt;Seamless Hugging Face model serving? &lt;em&gt;TGI&lt;/em&gt; is well-integrated.&lt;/li&gt; &lt;li&gt;Enterprise-scale NVIDIA deployments? &lt;em&gt;TensorRT-LLM + Triton&lt;/em&gt; offers peak performance with enterprise security guarantees via their enterprise containers.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;5. Always use `hf-transfer`: Speed Up Model Downloads (3-5x Faster)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hugging Face downloads are limited to Python‚Äôs single-threaded requests, causing bottlenecks.&lt;/li&gt; &lt;li&gt;&lt;code&gt;hf-transfer&lt;/code&gt; leverages Rust-based parallel downloads, achieving 500MB/s+ speeds on high-bandwidth networks. This doesn‚Äôt directly improve inference speed, but it improves &lt;em&gt;deployment agility&lt;/em&gt; ‚Äì spinning up new instances or switching models becomes less painful. For anyone frequently downloading from the Hub, it‚Äôs a useful trick.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;6. Follow these optimisations -&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;In-Flight Continuous Batching -&lt;/strong&gt; In-flight continuous batching merges new requests into a batch &lt;em&gt;mid-generation&lt;/em&gt;, maximizing GPU efficiency. It prevents GPUs from sitting idle between requests. This results in up to 3.5x throughput increase over naive request processing. This is enabled by default in almost every inference engine.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;KV Cache Optimization -&lt;/strong&gt; If your use case involves repetitive prompts or multi-turn interactions with a shared context, consider prefix-caching strategies. If multiple queries have the &lt;strong&gt;same prefix&lt;/strong&gt;, you can do the heavy computation for that prefix once and reuse it for subsequent requests. This is all about the &lt;strong&gt;KV cache&lt;/strong&gt; ‚Äì the key/value tensors that store the model‚Äôs attention history. SGLang wins, hands down, in this.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quantization: More Speed, Minimal Accuracy Loss -&lt;/strong&gt; If you are deploying a model and concerned about latency, definitely consider using a quantised 4-bit version (if available) or running the AWQ tooling on it. &lt;ul&gt; &lt;li&gt;Reducing precision (e.g., 4-bit instead of 16-bit) shrinks memory usage and boosts speed.&lt;/li&gt; &lt;li&gt;AWQ (Activation-Aware Weight Quantization) achieves up to 1.7x speedup vs. GPTQ, with &amp;lt;1% accuracy loss.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;7. Heuristics for the Right Optimisation Strategy&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Size &amp;amp; Hardware Constraints -&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;‚â§7B:&lt;/strong&gt; &lt;em&gt;Can run on a single GPU (or even CPU with llama.cpp)&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;13B-30B:&lt;/strong&gt; &lt;em&gt;Requires 8-bit or 4-bit quantization for a single GPU.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;65B+:&lt;/strong&gt; &lt;em&gt;Requires multi-GPU or aggressive quantization (AWQ 4-bit).&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Based on Use Case&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Use case&lt;/th&gt; &lt;th&gt;Strategy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;High Throughput (API, chatbot serving)&lt;/td&gt; &lt;td&gt;Use vLLM/TGI with batching and quantization&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Long Contexts (Document QA, Code analysis)&lt;/td&gt; &lt;td&gt;Use frameworks like vLLM that handle large KV cache efficiently&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Latency-Critical (Autocompletion, low-latency responses)&lt;/td&gt; &lt;td&gt;Prioritise single-GPU optimisation and avoid excessive batching.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;TLDR -&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Optimizing LLM inference is a &lt;strong&gt;balancing act&lt;/strong&gt; between speed, memory, and accuracy. The best approach depends on &lt;strong&gt;your use case, hardware, and performance needs.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Key Takeaways:&lt;/h1&gt; &lt;p&gt;‚úÖ Use in-flight batching (&lt;em&gt;vLLM, TGI&lt;/em&gt;) for maximum TPS.&lt;/p&gt; &lt;p&gt;‚úÖ Quantize models (AWQ, GPTQ) for 4-bit gains without accuracy loss.&lt;/p&gt; &lt;p&gt;‚úÖ Optimize KV cache (&lt;em&gt;cut latency by ~40%&lt;/em&gt;).&lt;/p&gt; &lt;p&gt;‚úÖ Avoid naive FastAPI deployment ‚Äì use inference-optimized servers.&lt;/p&gt; &lt;p&gt;‚úÖ Choose the right strategy based on model size and use case.&lt;/p&gt; &lt;p&gt;By applying these strategies, you can achieve fast inference speeds while keeping your costs low.&lt;/p&gt; &lt;p&gt;P.S. - This is a modified form of our blog originally posted here - &lt;a href="https://tensorfuse.io/blog/llm-throughput-vllm-vs-sglang"&gt;https://tensorfuse.io/blog/llm-throughput-vllm-vs-sglang&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you found this informative, please show your support by upvoting and sharing this post.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 1&lt;/strong&gt; - Formatting fixes around table.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tempNull"&gt; /u/tempNull &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioiwlv/lessons_learned_while_deploying_deepseek_r1_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioiwlv/lessons_learned_while_deploying_deepseek_r1_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioiwlv/lessons_learned_while_deploying_deepseek_r1_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T13:21:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1iohbij</id>
    <title>I built a knowledge management system that enables you to connect knowledge to any RAG</title>
    <updated>2025-02-13T11:48:33+00:00</updated>
    <author>
      <name>/u/Outside-Project-1451</name>
      <uri>https://old.reddit.com/user/Outside-Project-1451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm excited to introduce Simba ‚Äì an open-source solution I developed to simplify managing and leveraging knowledge in Retrieval-Augmented Generation (RAG) systems.&lt;/p&gt; &lt;p&gt;In simple terms, Simba enables you to structure and connect a knowledge base (Word, PDF, PowerPoint documents, etc.) to any chatbot.&lt;/p&gt; &lt;p&gt;üîç Why Simba?&lt;/p&gt; &lt;p&gt;While working on AI projects, I frequently encountered challenges such as:&lt;/p&gt; &lt;p&gt;üìÇ Handling long, complex documents (including tables, images, multiple sections‚Ä¶)&lt;/p&gt; &lt;p&gt;üîé Indexing and structuring information for effective retrieval&lt;/p&gt; &lt;p&gt;üõ†Ô∏è Controlling the sources that a chatbot uses&lt;/p&gt; &lt;p&gt;Simba addresses these issues with:&lt;/p&gt; &lt;p&gt;‚úÖ Advanced parsing that automatically structures documents using state-of-the-art algorithms&lt;/p&gt; &lt;p&gt;‚úÖ An intuitive interface to visualize, modify, and organize data chunks&lt;/p&gt; &lt;p&gt;‚úÖ Precise knowledge control to include or exclude sources as needed&lt;/p&gt; &lt;p&gt;‚úÖ A flexible architecture allowing you to choose your LLMs, vector databases, chunking strategies, and parsers&lt;/p&gt; &lt;p&gt;üìå When to Use Simba?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For long and complex documents (tables, images, multiple sections‚Ä¶)&lt;/li&gt; &lt;li&gt;When you need granular control over which sources are included during conversations&lt;/li&gt; &lt;li&gt;When managing data access is critical (permissions and roles ‚Äì a feature coming soon)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üéØ Who Is Simba For?&lt;/p&gt; &lt;p&gt;Simba is crafted for developers aiming to integrate a structured knowledge base into their RAG systems.&lt;/p&gt; &lt;p&gt;üõ†Ô∏è Although the project is still evolving and doesn‚Äôt yet cover every planned feature, it‚Äôs on track to become a powerful tool for the community.&lt;/p&gt; &lt;p&gt;üí° Feedback Is a Gift!&lt;/p&gt; &lt;p&gt;The magic of open source lies in collaboration. If you encounter bugs, unclear areas, or simply have suggestions, please share your feedback. You can propose improvements, bug fixes, or new features directly on GitHub.&lt;/p&gt; &lt;p&gt;Check out the repository here: &lt;a href="https://github.com/GitHamza0206/simba"&gt;https://github.com/GitHamza0206/simba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚≠ê Simba is nearing 100 stars on GitHub, and the goal is to reach 1000 stars within the next 2 months! If you appreciate the project, please give it a star ‚≠ê ‚Äì your support means a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outside-Project-1451"&gt; /u/Outside-Project-1451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohbij/i_built_a_knowledge_management_system_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohbij/i_built_a_knowledge_management_system_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iohbij/i_built_a_knowledge_management_system_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T11:48:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1io2vq5</id>
    <title>Promptable object tracking robots with Moondream VLM &amp; OpenCV Optical Flow (open source)</title>
    <updated>2025-02-12T21:53:10+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2vq5/promptable_object_tracking_robots_with_moondream/"&gt; &lt;img alt="Promptable object tracking robots with Moondream VLM &amp;amp; OpenCV Optical Flow (open source)" src="https://external-preview.redd.it/N2xjdjR4MG80c2llMTEDy-zmwY-2zxEHn6L-Fnq1X838PMp4mnmxIFCi0bu_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35f82c0e1ecceb7465617120ea97715cdb5a48e9" title="Promptable object tracking robots with Moondream VLM &amp;amp; OpenCV Optical Flow (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/z5buym0o4sie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2vq5/promptable_object_tracking_robots_with_moondream/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io2vq5/promptable_object_tracking_robots_with_moondream/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T21:53:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioi4lm</id>
    <title>Benchmark Paper: Vision-Language Models vs Traditional OCR in Videos</title>
    <updated>2025-02-13T12:38:18+00:00</updated>
    <author>
      <name>/u/ashutrv</name>
      <uri>https://old.reddit.com/user/ashutrv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new benchmark paper just dropped evaluating how well &lt;strong&gt;Vision-Language Models (VLMs)&lt;/strong&gt; perform compared to &lt;strong&gt;traditional OCR&lt;/strong&gt; tools in dynamic video environments. &lt;/p&gt; &lt;p&gt;The study, led by the team at &lt;strong&gt;VideoDB&lt;/strong&gt;, introduces a &lt;strong&gt;curated dataset of 1,477 manually annotated frames&lt;/strong&gt; spanning diverse domains‚Äîcode editors, news broadcasts, YouTube videos, and advertisements.&lt;/p&gt; &lt;p&gt;üîó &lt;strong&gt;Read the paper&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2502.06445"&gt;https://arxiv.org/abs/2502.06445&lt;/a&gt;&lt;br /&gt; üîó &lt;strong&gt;Explore the dataset &amp;amp; repo&lt;/strong&gt;: &lt;a href="https://github.com/video-db/ocr-benchmark"&gt;https://github.com/video-db/ocr-benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Three state-of-the-art VLMs ‚Äì Claude-3, Gemini-1.5, and GPT-4o ‚Äì were benchmarked against traditional OCR tools like EasyOCR and RapidOCR, using metrics such as Word Error Rate (WER), Character Error Rate (CER), and Accuracy.&lt;/p&gt; &lt;h1&gt;üîç Key Findings:&lt;/h1&gt; &lt;p&gt;‚úÖ &lt;strong&gt;VLMs outperformed&lt;/strong&gt; traditional OCR in many cases, demonstrating robustness across varied video contexts.&lt;br /&gt; ‚ö†Ô∏è &lt;strong&gt;Challenges persist&lt;/strong&gt; ‚Äì hallucinated text, security policy triggers, and difficulty with occluded/stylized text.&lt;br /&gt; üìÇ &lt;strong&gt;Public Dataset Available&lt;/strong&gt; ‚Äì The full dataset and benchmarking framework are open for research &amp;amp; collaboration.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashutrv"&gt; /u/ashutrv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioi4lm/benchmark_paper_visionlanguage_models_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioi4lm/benchmark_paper_visionlanguage_models_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioi4lm/benchmark_paper_visionlanguage_models_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T12:38:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1inoui5</id>
    <title>AMD reportedly working on gaming Radeon RX 9070 XT GPU with 32GB memory</title>
    <updated>2025-02-12T11:36:29+00:00</updated>
    <author>
      <name>/u/noiserr</name>
      <uri>https://old.reddit.com/user/noiserr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoui5/amd_reportedly_working_on_gaming_radeon_rx_9070/"&gt; &lt;img alt="AMD reportedly working on gaming Radeon RX 9070 XT GPU with 32GB memory" src="https://external-preview.redd.it/qxSKCWeduksNqEDRWvwQaww7R41JuTdE_uY1z8NDX_M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b97591e394a959b1d54b453c3148692e6cab6ca" title="AMD reportedly working on gaming Radeon RX 9070 XT GPU with 32GB memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noiserr"&gt; /u/noiserr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/amd-reportedly-working-on-gaming-radeon-rx-9000-gpu-with-32gb-memory"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoui5/amd_reportedly_working_on_gaming_radeon_rx_9070/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inoui5/amd_reportedly_working_on_gaming_radeon_rx_9070/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T11:36:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iofdch</id>
    <title>VRAM Requirements for Training a 70B Model with GRPO &amp; ZeRO-3?</title>
    <updated>2025-02-13T09:28:43+00:00</updated>
    <author>
      <name>/u/thanhdouwu</name>
      <uri>https://old.reddit.com/user/thanhdouwu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to estimate VRAM usage when training a 70B parameter model with GRPO. If I use ZeRO-3, set the context length to 8k or 16k, and use a rule-based reward model, how much VRAM would I need?&lt;/p&gt; &lt;p&gt;Additionally, if you've trained with LoRA or different model sizes, I'd love to hear about your experience‚ÄîVRAM consumption, setup details, and any optimizations you found helpful. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thanhdouwu"&gt; /u/thanhdouwu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iofdch/vram_requirements_for_training_a_70b_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iofdch/vram_requirements_for_training_a_70b_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iofdch/vram_requirements_for_training_a_70b_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T09:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1io4x5c</id>
    <title>OpenThinker-32B &amp; 7B</title>
    <updated>2025-02-12T23:21:11+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/open-thoughts/OpenThinker-32B"&gt;https://huggingface.co/open-thoughts/OpenThinker-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/open-thoughts/OpenThinker-7B"&gt;https://huggingface.co/open-thoughts/OpenThinker-7B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io4x5c/openthinker32b_7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io4x5c/openthinker32b_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io4x5c/openthinker32b_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T23:21:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1io655d</id>
    <title>The endgame of Tool-Use, toolmaking</title>
    <updated>2025-02-13T00:17:33+00:00</updated>
    <author>
      <name>/u/fractalcrust</name>
      <uri>https://old.reddit.com/user/fractalcrust</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Stop making bespoke tools for every usecase. What we need is a tool-making tool, enabling LLMs to create their own tools to solve their tasks. Nothing could possibly go wrong and I'm 100% comfortable leaving my LLM unsupervised &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fractalcrust"&gt; /u/fractalcrust &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io655d/the_endgame_of_tooluse_toolmaking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io655d/the_endgame_of_tooluse_toolmaking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io655d/the_endgame_of_tooluse_toolmaking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T00:17:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1io811j</id>
    <title>Who builds PCs that can handle 70B local LLMs?</title>
    <updated>2025-02-13T01:48:52+00:00</updated>
    <author>
      <name>/u/Moist-Mongoose4467</name>
      <uri>https://old.reddit.com/user/Moist-Mongoose4467</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are only a few videos on YouTube that show folks buying old server hardware and cobbling together affordable PCs with a bunch of cores, RAM, and CPU RAM. Is there a company or person that does that for a living (or side hustle)? I don't have $10,000 to $50,000 for a home server with multiple high-end GPUs. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moist-Mongoose4467"&gt; /u/Moist-Mongoose4467 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io811j/who_builds_pcs_that_can_handle_70b_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io811j/who_builds_pcs_that_can_handle_70b_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io811j/who_builds_pcs_that_can_handle_70b_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T01:48:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1io9lfc</id>
    <title>DeepSeek Distilled Qwen 1.5B on NPU for Windows on Snapdragon</title>
    <updated>2025-02-13T03:09:14+00:00</updated>
    <author>
      <name>/u/SkyFeistyLlama8</name>
      <uri>https://old.reddit.com/user/SkyFeistyLlama8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft just released a Qwen 1.5B DeepSeek Distilled local model that targets the Hexagon NPU on Snapdragon X Plus/Elite laptops. Finally, we have an LLM that officially runs on the NPU for prompt eval (inference runs on CPU). &lt;/p&gt; &lt;p&gt;To run it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;run VS Code under Windows on ARM&lt;/li&gt; &lt;li&gt;download the AI Toolkit extension&lt;/li&gt; &lt;li&gt;Ctrl-Shift-P to load the command palette, type &amp;quot;Load Model Catalog&amp;quot;&lt;/li&gt; &lt;li&gt;scroll down to the DeepSeek (NPU Optimized) card, click +Add. The extension then downloads a bunch of ONNX files.&lt;/li&gt; &lt;li&gt;to run inference, Ctrl-Shift-P to load the command palette, then type &amp;quot;Focus on my models view&amp;quot; to load, then have fun in the chat playground&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Task Manager shows NPU usage at 50% and CPU at 25% during inference so it's working as intended. Larger Qwen and Llama models are coming so we finally have multiple performant inference stacks on Snapdragon.&lt;/p&gt; &lt;p&gt;The actual executable is in the &amp;quot;ai-studio&amp;quot; directory under VS Code's extensions directory. There's an ONNX runtime .exe along with a bunch of QnnHtp DLLs. It might be interesting to code up a PowerShell workflow for this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SkyFeistyLlama8"&gt; /u/SkyFeistyLlama8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io9lfc/deepseek_distilled_qwen_15b_on_npu_for_windows_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io9lfc/deepseek_distilled_qwen_15b_on_npu_for_windows_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io9lfc/deepseek_distilled_qwen_15b_on_npu_for_windows_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T03:09:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioclzg</id>
    <title>InternVideo2.5 releasedÔºÅHas anyone tried it out? How well does it perform?</title>
    <updated>2025-02-13T06:04:38+00:00</updated>
    <author>
      <name>/u/vansinhu</name>
      <uri>https://old.reddit.com/user/vansinhu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioclzg/internvideo25_releasedhas_anyone_tried_it_out_how/"&gt; &lt;img alt="InternVideo2.5 releasedÔºÅHas anyone tried it out? How well does it perform?" src="https://external-preview.redd.it/Y7gp2ezADJTiI3oUU3P5TMgIEsAjig-29MVwWQpiG_c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de07f828e6653fd6667798fac5614b252c311381" title="InternVideo2.5 releasedÔºÅHas anyone tried it out? How well does it perform?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/47l93seekuie1.png?width=592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5b0206424f31cc8412405cdc02e780ce5763b9b"&gt;https://preview.redd.it/47l93seekuie1.png?width=592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5b0206424f31cc8412405cdc02e780ce5763b9b&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Handles videos 6x longer than predecessors&lt;/li&gt; &lt;li&gt;Pinpoints objects/actions with surgical precision&lt;/li&gt; &lt;li&gt;Trained on 300K+ hours of diverse video data&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Outperforms SOTA on multiple benchmarks &amp;amp; unlocks possibilities for Autonomous Driving, VR, and more!Code: &lt;a href="https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5"&gt;https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2501.12386"&gt;https://arxiv.org/abs/2501.12386&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://huggingface.co/OpenGVLab/InternVideo2_5_Chat_8B"&gt;https://huggingface.co/OpenGVLab/InternVideo2_5_Chat_8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xieqwfmhkuie1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c9e39c7d538478ba3387fa5077d06c0017df073"&gt;https://preview.redd.it/xieqwfmhkuie1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c9e39c7d538478ba3387fa5077d06c0017df073&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vansinhu"&gt; /u/vansinhu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioclzg/internvideo25_releasedhas_anyone_tried_it_out_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioclzg/internvideo25_releasedhas_anyone_tried_it_out_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioclzg/internvideo25_releasedhas_anyone_tried_it_out_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T06:04:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1iofe4w</id>
    <title>[update] aiaio: simple, lightweight ui with more features now</title>
    <updated>2025-02-13T09:30:25+00:00</updated>
    <author>
      <name>/u/abhi1thakur</name>
      <uri>https://old.reddit.com/user/abhi1thakur</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iofe4w/update_aiaio_simple_lightweight_ui_with_more/"&gt; &lt;img alt="[update] aiaio: simple, lightweight ui with more features now" src="https://external-preview.redd.it/eWJ1dWZtaTlsdmllMTTMNvywGLfHKtiMdeeDDuKKJ-xtwCq_lpvrE6nUhuq6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f5f1683ed07dbea3c137ac3ecb29bfaf68079ce" title="[update] aiaio: simple, lightweight ui with more features now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abhi1thakur"&gt; /u/abhi1thakur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1bduxmi9lvie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iofe4w/update_aiaio_simple_lightweight_ui_with_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iofe4w/update_aiaio_simple_lightweight_ui_with_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T09:30:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioikl0</id>
    <title>Gemini beats everyone is OCR benchmarking tasks in videos. Full Paper : https://arxiv.org/abs/2502.06445</title>
    <updated>2025-02-13T13:03:22+00:00</updated>
    <author>
      <name>/u/ashutrv</name>
      <uri>https://old.reddit.com/user/ashutrv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioikl0/gemini_beats_everyone_is_ocr_benchmarking_tasks/"&gt; &lt;img alt="Gemini beats everyone is OCR benchmarking tasks in videos. Full Paper : https://arxiv.org/abs/2502.06445" src="https://preview.redd.it/8u7jixwzmwie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f7ab3742f3fd3c2245bf8eadfbaad2fecacd6ac" title="Gemini beats everyone is OCR benchmarking tasks in videos. Full Paper : https://arxiv.org/abs/2502.06445" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashutrv"&gt; /u/ashutrv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8u7jixwzmwie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioikl0/gemini_beats_everyone_is_ocr_benchmarking_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioikl0/gemini_beats_everyone_is_ocr_benchmarking_tasks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T13:03:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1iof0r2</id>
    <title>When it comes to fine-tuning LLMs, the training dataset isn‚Äôt just a factor‚Äîit‚Äôs the kingmaker.</title>
    <updated>2025-02-13T09:01:31+00:00</updated>
    <author>
      <name>/u/Excellent_Delay_3701</name>
      <uri>https://old.reddit.com/user/Excellent_Delay_3701</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let‚Äôs take a look at the current SOTA models‚ÄîLlama 3.x, DeepSeek, Mistral, and others. (Don't forget I am talking about fine-tune for specific tasks, not pre-train)&lt;/p&gt; &lt;p&gt;The real kingmaker for top performance? A meticulously cleaned, balanced, and well-structured dataset. Even if a ‚Äúperfect‚Äù dataset doesn‚Äôt exist, getting as close as possible makes all the difference.&lt;/p&gt; &lt;p&gt;Sure, training variables and hyperparameters impact an LLM‚Äôs performance. But in the end, isn‚Äôt the dataset everything?&lt;/p&gt; &lt;p&gt;If you‚Äôre fine-tuning an LLM or SLM for a specific task and not seeing the results you want after a few iterations, the first place you should look is the dataset. &lt;/p&gt; &lt;p&gt;How many of you changes model architectures, apply something new?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Delay_3701"&gt; /u/Excellent_Delay_3701 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iof0r2/when_it_comes_to_finetuning_llms_the_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iof0r2/when_it_comes_to_finetuning_llms_the_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iof0r2/when_it_comes_to_finetuning_llms_the_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T09:01:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1io4s4s</id>
    <title>This paper might be a breakthrough Google doesn't know they have</title>
    <updated>2025-02-12T23:14:52+00:00</updated>
    <author>
      <name>/u/Ok-Possibility-5586</name>
      <uri>https://old.reddit.com/user/Ok-Possibility-5586</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2105.03824"&gt;2105.03824&lt;/a&gt;&lt;/p&gt; &lt;p&gt;FNet: Mixing Tokens with Fourier Transforms&lt;/p&gt; &lt;p&gt;^^^ this paper is from 2022 before LLMs blew up in the public imagination.&lt;/p&gt; &lt;p&gt;If someone is able to replicate this, maybe by training a smaller model and cutting out the layers and splicing into a bigger model (or something else, I'm winging it here) then maybe we get some big speedups. According to the paper (from Google) it's looking at a 90% speedup and memory reduction.&lt;/p&gt; &lt;p&gt;&lt;a href="/u/danielhanchen"&gt;u/danielhanchen&lt;/a&gt; have you seen this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Possibility-5586"&gt; /u/Ok-Possibility-5586 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io4s4s/this_paper_might_be_a_breakthrough_google_doesnt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io4s4s/this_paper_might_be_a_breakthrough_google_doesnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io4s4s/this_paper_might_be_a_breakthrough_google_doesnt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T23:14:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1io3hn2</id>
    <title>NoLiMa: Long-Context Evaluation Beyond Literal Matching - Finally a good benchmark that shows just how bad LLM performance is at long context. Massive drop at just 32k context for all models.</title>
    <updated>2025-02-12T22:19:00+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io3hn2/nolima_longcontext_evaluation_beyond_literal/"&gt; &lt;img alt="NoLiMa: Long-Context Evaluation Beyond Literal Matching - Finally a good benchmark that shows just how bad LLM performance is at long context. Massive drop at just 32k context for all models." src="https://preview.redd.it/95ysyjzs8sie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=846630231480ed6a71d97aeaed4938ab9b5cc355" title="NoLiMa: Long-Context Evaluation Beyond Literal Matching - Finally a good benchmark that shows just how bad LLM performance is at long context. Massive drop at just 32k context for all models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/95ysyjzs8sie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io3hn2/nolima_longcontext_evaluation_beyond_literal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io3hn2/nolima_longcontext_evaluation_beyond_literal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T22:19:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1iohk4o</id>
    <title>Let's build DeepSeek from Scratch | Taught by MIT PhD graduate</title>
    <updated>2025-02-13T12:03:45+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"&gt; &lt;img alt="Let's build DeepSeek from Scratch | Taught by MIT PhD graduate" src="https://external-preview.redd.it/pAa68GpmjnpZeahm_YMGQkYTs9KtW9HemhGbAYHU02s.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=555355166a247eb92939344c89b96ed48dd7655a" title="Let's build DeepSeek from Scratch | Taught by MIT PhD graduate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/vjwhw6ticwie1.gif"&gt;https://i.redd.it/vjwhw6ticwie1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Join us for the 6pm Youtube premier here: &lt;a href="https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ"&gt;https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ever since DeepSeek was launched, everyone is focused on: &lt;/p&gt; &lt;p&gt;- Flashy headlines&lt;/p&gt; &lt;p&gt;- Company wars&lt;/p&gt; &lt;p&gt;- Building LLM applications powered by DeepSeek&lt;/p&gt; &lt;p&gt;I very strongly think that students, researchers, engineers and working professionals should focus on the foundations. &lt;/p&gt; &lt;p&gt;The real question we should ask ourselves is: &lt;/p&gt; &lt;p&gt;‚ÄúCan I build the DeepSeek architecture and model myself, from scratch?‚Äù&lt;/p&gt; &lt;p&gt;If you ask this question, you will discover that to make DeepSeek work, there are a number of key ingredients which play a role:&lt;/p&gt; &lt;p&gt;(1) Mixture of Experts (MoE)&lt;/p&gt; &lt;p&gt;(2) Multi-head Latent Attention (MLA)&lt;/p&gt; &lt;p&gt;(3) Rotary Positional Encodings (RoPE)&lt;/p&gt; &lt;p&gt;(4) Multi-token prediction (MTP)&lt;/p&gt; &lt;p&gt;(5) Supervised Fine-Tuning (SFT)&lt;/p&gt; &lt;p&gt;(6) Group Relative Policy Optimisation (GRPO)&lt;/p&gt; &lt;p&gt;My aim with the ‚ÄúBuild DeepSeek from Scratch‚Äù playlist is: &lt;/p&gt; &lt;p&gt;- To teach you the mathematical foundations behind all the 6 ingredients above.&lt;/p&gt; &lt;p&gt;- To code all 6 ingredients above, from scratch.&lt;/p&gt; &lt;p&gt;- To assemble these ingredients and to run a ‚Äúmini Deep-Seek‚Äù on your own.&lt;/p&gt; &lt;p&gt;After this, you will among the top 0.1%. of ML/LLM engineers who can build DeepSeek ingredients on their own.&lt;/p&gt; &lt;p&gt;This playlist won‚Äôt be a 1 hour or 2 hour video. This will be a mega playlist of 35-40 videos with a duration of 40+ hours. &lt;/p&gt; &lt;p&gt;It will be in-depth. No fluff. Solid content. &lt;/p&gt; &lt;p&gt;Join us for the 6pm premier here: &lt;a href="https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ"&gt;https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S: Attached is a small GIF showing the notes we have made. This is just 5-10% of the total amount of notes and material we have prepared for this series!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T12:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1io5o9a</id>
    <title>How do LLMs actually do this?</title>
    <updated>2025-02-12T23:56:05+00:00</updated>
    <author>
      <name>/u/No-Conference-8133</name>
      <uri>https://old.reddit.com/user/No-Conference-8133</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io5o9a/how_do_llms_actually_do_this/"&gt; &lt;img alt="How do LLMs actually do this?" src="https://preview.redd.it/m6rfcv5tqsie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97e3e2e816211a62c38e8c3c60368cca7c8d38d4" title="How do LLMs actually do this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The LLM can‚Äôt actually see or look close. It can‚Äôt zoom in the picture and count the fingers carefully or slower.&lt;/p&gt; &lt;p&gt;My guess is that when I say &amp;quot;look very close&amp;quot; it just adds a finger and assumes a different answer. Because LLMs are all about matching patterns. When I tell someone to look very close, the answer usually changes.&lt;/p&gt; &lt;p&gt;Is this accurate or am I totally off?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Conference-8133"&gt; /u/No-Conference-8133 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m6rfcv5tqsie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io5o9a/how_do_llms_actually_do_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io5o9a/how_do_llms_actually_do_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T23:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1io2ija</id>
    <title>Is Mistral's Le Chat truly the FASTEST?</title>
    <updated>2025-02-12T21:37:41+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2ija/is_mistrals_le_chat_truly_the_fastest/"&gt; &lt;img alt="Is Mistral's Le Chat truly the FASTEST?" src="https://preview.redd.it/zk2uyy142sie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abb4eab5a990f54584b5bb28366386e39bb58419" title="Is Mistral's Le Chat truly the FASTEST?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zk2uyy142sie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2ija/is_mistrals_le_chat_truly_the_fastest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io2ija/is_mistrals_le_chat_truly_the_fastest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T21:37:41+00:00</published>
  </entry>
</feed>
