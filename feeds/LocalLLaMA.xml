<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-19T16:36:06+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i4qrcx</id>
    <title>Training tiny LLM on local machine</title>
    <updated>2025-01-19T04:54:17+00:00</updated>
    <author>
      <name>/u/Dark_Knight003</name>
      <uri>https://old.reddit.com/user/Dark_Knight003</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a ML newbie and want to get into core AI. I plan to learn how to build LLMs and hopefully train a tiny LLM on local machine to understand the concepts in depth. However I understand that training LLMs requires massive GPU resources. &lt;/p&gt; &lt;p&gt;I want to know if it is possible to train a tiny LLM from scratch on local Mac with 32gb gpu (unified memory) that gives coherent and related output? I am planning to buy a new Mac hence wanted to understand this. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Knight003"&gt; /u/Dark_Knight003 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4qrcx/training_tiny_llm_on_local_machine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4qrcx/training_tiny_llm_on_local_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4qrcx/training_tiny_llm_on_local_machine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T04:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4hyn9</id>
    <title>The Case for Open Models</title>
    <updated>2025-01-18T21:31:18+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hyn9/the_case_for_open_models/"&gt; &lt;img alt="The Case for Open Models" src="https://external-preview.redd.it/9jCHWcqHBL_Q8CbrEzDT6EMKWVd07U_SmrG2Z0RXzj8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf06e18f281752c7c009c87125ee559793b0b851" title="The Case for Open Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://amgadhasan.substack.com/p/the-case-for-open-models"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hyn9/the_case_for_open_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hyn9/the_case_for_open_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T21:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i51kov</id>
    <title>Google API error</title>
    <updated>2025-01-19T15:56:28+00:00</updated>
    <author>
      <name>/u/xmmr</name>
      <uri>https://old.reddit.com/user/xmmr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I get 100% error on &lt;/p&gt; &lt;p&gt;`google.ai.generativelanguage.v1beta.GenerativeService.GenerateContent` and &lt;/p&gt; &lt;p&gt;`google.ai.generativelanguage.v1beta.GenerativeService.StreamGenerateContent`&lt;/p&gt; &lt;p&gt;How to get actual output tokens?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xmmr"&gt; /u/xmmr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i51kov/google_api_error/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i51kov/google_api_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i51kov/google_api_error/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T15:56:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4fmvy</id>
    <title>4080 16gb and my old 3070 8gb</title>
    <updated>2025-01-18T19:45:36+00:00</updated>
    <author>
      <name>/u/Glooves</name>
      <uri>https://old.reddit.com/user/Glooves</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4fmvy/4080_16gb_and_my_old_3070_8gb/"&gt; &lt;img alt="4080 16gb and my old 3070 8gb" src="https://b.thumbs.redditmedia.com/_MB00WAkuuUTDJAJzS8IGuj3FCZdE56GfbQP0ShAflY.jpg" title="4080 16gb and my old 3070 8gb" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Decided to throw my old 3070 in and an old set of ddr4 to see what happens. Now up to 24 gb of vram and 64 gb of dram with a 12700kf. I was worried about my 750 watt psu but itâ€™s pulling under 400 watts at load and Iâ€™ll set some limits just in case. Got 22 tok/sec on gwen 2.5 32b q4_0. Iâ€™ll try a 70b later. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glooves"&gt; /u/Glooves &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i4fmvy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4fmvy/4080_16gb_and_my_old_3070_8gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4fmvy/4080_16gb_and_my_old_3070_8gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T19:45:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4h3pp</id>
    <title>What would you do with free access to a 4x H100 server?</title>
    <updated>2025-01-18T20:52:42+00:00</updated>
    <author>
      <name>/u/SquareJordan</name>
      <uri>https://old.reddit.com/user/SquareJordan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Long story short I have one in the lab and all thatâ€™s being run on it thus far are benchmarks. What should I do with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SquareJordan"&gt; /u/SquareJordan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4h3pp/what_would_you_do_with_free_access_to_a_4x_h100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4h3pp/what_would_you_do_with_free_access_to_a_4x_h100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4h3pp/what_would_you_do_with_free_access_to_a_4x_h100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T20:52:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4cfpz</id>
    <title>Llama 3.2 1B Instruct â€“ What Are the Best Use Cases for Small LLMs?</title>
    <updated>2025-01-18T17:23:32+00:00</updated>
    <author>
      <name>/u/ThetaCursed</name>
      <uri>https://old.reddit.com/user/ThetaCursed</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4cfpz/llama_32_1b_instruct_what_are_the_best_use_cases/"&gt; &lt;img alt="Llama 3.2 1B Instruct â€“ What Are the Best Use Cases for Small LLMs?" src="https://preview.redd.it/tr0h9qvkdsde1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=237cce46c19ab8ed30310b8c79fcf688f233dcf2" title="Llama 3.2 1B Instruct â€“ What Are the Best Use Cases for Small LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThetaCursed"&gt; /u/ThetaCursed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tr0h9qvkdsde1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4cfpz/llama_32_1b_instruct_what_are_the_best_use_cases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4cfpz/llama_32_1b_instruct_what_are_the_best_use_cases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T17:23:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4r1ig</id>
    <title>Why reasoning models might be a huge breakthrough for local inference / training</title>
    <updated>2025-01-19T05:09:52+00:00</updated>
    <author>
      <name>/u/valewolf</name>
      <uri>https://old.reddit.com/user/valewolf</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently all the speculation I have seen out there suggests that OpenAI's O series of reasoning models still uses a relatively large expensive to train base model. Given the compute resources OpenAI has that is probably unlikely to change.&lt;/p&gt; &lt;p&gt;For those hoping to train and use models locally however it may be possible to extend the same concepts with a very small ~8B open source model that could easily by trained or run on almost any consumer GPU. Probably the amount of test time compute needed / length of reasoning traces needed to achieve good performance may be extreme but still you could probably get there. Because of the low GPU memory requirements of the base model getting a setup running would likely be easy even if its quite slow relatively speaking. &lt;/p&gt; &lt;p&gt;How far could this approach be scaled? could you use a 1B base model? Obviously there is a point where this may stop working but where is that point I wonder.&lt;/p&gt; &lt;p&gt;Anyways, is anyone aware of open source projects that have explored this idea? and assuming its feasible what would some of the major pros / cons be?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valewolf"&gt; /u/valewolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4r1ig/why_reasoning_models_might_be_a_huge_breakthrough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4r1ig/why_reasoning_models_might_be_a_huge_breakthrough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4r1ig/why_reasoning_models_might_be_a_huge_breakthrough/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T05:09:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4w5oe</id>
    <title>Which LLM would is good at analyzing content?</title>
    <updated>2025-01-19T11:06:03+00:00</updated>
    <author>
      <name>/u/Zundrium</name>
      <uri>https://old.reddit.com/user/Zundrium</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to find an LLM that works well with instructions like:&lt;/p&gt; &lt;p&gt;&lt;em&gt;&amp;quot;List all &amp;lt;section&amp;gt; elements from this HTML page&amp;quot;&lt;/em&gt; or &lt;em&gt;&amp;quot;Count the number of times the word 'test' has been used in this text&amp;quot;.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I know I can use regex for this kind of parsing, but I just want to understand why Claude is perfect at this and I have yet to find a local LLM that comes close to accurately counting or listing items.&lt;/p&gt; &lt;p&gt;I've tried Gemma 2, Phi 4 and Llama 3.3 with no success.&lt;/p&gt; &lt;p&gt;Maybe I'm missing something, so if you know anything I'd love to hear it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zundrium"&gt; /u/Zundrium &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4w5oe/which_llm_would_is_good_at_analyzing_content/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4w5oe/which_llm_would_is_good_at_analyzing_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4w5oe/which_llm_would_is_good_at_analyzing_content/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T11:06:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1i457gp</id>
    <title>Intel should release a 24GB version of the Arc B580</title>
    <updated>2025-01-18T10:58:44+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i457gp/intel_should_release_a_24gb_version_of_the_arc/"&gt; &lt;img alt="Intel should release a 24GB version of the Arc B580" src="https://external-preview.redd.it/KNNit46prWlA2v7rjsUV6TaIPMXvtB72RAGA4ZyQjNE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ddd3f42144ca0c2a05d54cf349b57f74c2e13f0f" title="Intel should release a 24GB version of the Arc B580" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The B580 is already showing impressive performance for LLM inference, matching the RTX 3060 in Vulkan benchmarks (~36 tokens/sec on Qwen2 7B) while being more power efficient and $50 cheaper. But VRAM is the real bottleneck for running larger models locally.&lt;/p&gt; &lt;p&gt;With Intel's strong XMX matrix performance and the existing clamshell memory design validated in shipping docs, a 24GB variant is technically feasible. This would enable running 13B models quantized to 8-bit (most 13B models need ~14GB), existing models with larger context, etc.&lt;/p&gt; &lt;p&gt;It would have way better price/performance than RTX 4060 Ti 16GB, native Vulkan support without CUDA lock-in and more performance potential if OpenVINO is further optimized.&lt;/p&gt; &lt;p&gt;The regular B580's stellar price/performance ratio shows Intel can be aggressive on pricing. A ~$329 24GB variant would hit a sweet spot for local LLM enthusiasts building inference rigs.&lt;/p&gt; &lt;p&gt;This is Intel's chance to build mind- and marketshare among AI developers and enthusiasts who are tired of CUDA lock-in. They can grow a community around OpenVINO and their AI tooling. Every developer who builds with Intel's stack today helps their ecosystem forward. The MLPerf results show they have the performance - now they just need to get the hardware into developers' hands.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dec 16 '24: &lt;a href="https://www.pcgamer.com/hardware/graphics-cards/shipping-document-suggests-that-a-24-gb-version-of-intels-arc-b580-graphics-card-could-be-heading-to-market-though-not-for-gaming/"&gt;Shipping document suggests that a 24 GB version of Intel's Arc B580 graphics card could be heading to market, though not for gaming&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xaydqqjygqde1.png?width=691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d57bc47d8936ed555b725e7733a88541d20f6d8"&gt;https://preview.redd.it/xaydqqjygqde1.png?width=691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d57bc47d8936ed555b725e7733a88541d20f6d8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i457gp/intel_should_release_a_24gb_version_of_the_arc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i457gp/intel_should_release_a_24gb_version_of_the_arc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i457gp/intel_should_release_a_24gb_version_of_the_arc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T10:58:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i51nw6</id>
    <title>New Thinking Model: Art (Auto Regressive Thinker)</title>
    <updated>2025-01-19T16:00:20+00:00</updated>
    <author>
      <name>/u/GuiltyBookkeeper4849</name>
      <uri>https://old.reddit.com/user/GuiltyBookkeeper4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; Today, we are releasing a new model: &lt;strong&gt;Art&lt;/strong&gt;.&lt;br /&gt; We finetuned &lt;strong&gt;Qwen 3B Instruct&lt;/strong&gt; on &lt;strong&gt;Gemini Flash Thinking&lt;/strong&gt; data.&lt;/p&gt; &lt;p&gt;ðŸ”¹ &lt;strong&gt;Model card&lt;/strong&gt;: &lt;a href="https://huggingface.co/AGI-0/Art-v0-3B"&gt;https://huggingface.co/AGI-0/Art-v0-3B&lt;/a&gt;&lt;br /&gt; ðŸ”¹ &lt;strong&gt;Demo&lt;/strong&gt;: &lt;a href="https://huggingface.co/spaces/freeCS-dot-org/Art3B-chat"&gt;https://huggingface.co/spaces/freeCS-dot-org/Art3B-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuiltyBookkeeper4849"&gt; /u/GuiltyBookkeeper4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i51nw6/new_thinking_model_art_auto_regressive_thinker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i51nw6/new_thinking_model_art_auto_regressive_thinker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i51nw6/new_thinking_model_art_auto_regressive_thinker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T16:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4znfj</id>
    <title>New Open Source Writing Tools for macOS with support for Intel Macs</title>
    <updated>2025-01-19T14:30:03+00:00</updated>
    <author>
      <name>/u/AryaMR2679</name>
      <uri>https://old.reddit.com/user/AryaMR2679</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4znfj/new_open_source_writing_tools_for_macos_with/"&gt; &lt;img alt="New Open Source Writing Tools for macOS with support for Intel Macs" src="https://external-preview.redd.it/NXN0a3Zhc2lueWRlMTElqiJuMUTDrlTUcvyAvOchCruMz7n8w62BU9IpwUsV.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1762b9386e0ecfeb42a5c09f46c49320fe72be4c" title="New Open Source Writing Tools for macOS with support for Intel Macs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AryaMR2679"&gt; /u/AryaMR2679 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lpkrf6sinyde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4znfj/new_open_source_writing_tools_for_macos_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4znfj/new_open_source_writing_tools_for_macos_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T14:30:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4h1qo</id>
    <title>Kokoro TTS and GLaDOS make a low latency, realistic AI voice assistant</title>
    <updated>2025-01-18T20:50:12+00:00</updated>
    <author>
      <name>/u/Voidmesmer</name>
      <uri>https://old.reddit.com/user/Voidmesmer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4h1qo/kokoro_tts_and_glados_make_a_low_latency/"&gt; &lt;img alt="Kokoro TTS and GLaDOS make a low latency, realistic AI voice assistant" src="https://external-preview.redd.it/dqmU2zN05DBQIRTAk9vSzRwo09lhayQQe9rfQzGoyS8.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3af3bcb4c5c2426d61848d63048912843cb91350" title="Kokoro TTS and GLaDOS make a low latency, realistic AI voice assistant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Voidmesmer"&gt; /u/Voidmesmer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=EBHwOCoGMKQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4h1qo/kokoro_tts_and_glados_make_a_low_latency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4h1qo/kokoro_tts_and_glados_make_a_low_latency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T20:50:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4em80</id>
    <title>Interesting article on how DeepSeek has improved the architecture in DeepSeek V2 and V3.</title>
    <updated>2025-01-18T19:00:14+00:00</updated>
    <author>
      <name>/u/jpydych</name>
      <uri>https://old.reddit.com/user/jpydych</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="http://epoch.ai"&gt;epoch.ai&lt;/a&gt; has published an interesting article: &lt;a href="https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture"&gt;https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It talks about MLA, MoE innovations and Multi-Token Prediction.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jpydych"&gt; /u/jpydych &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4em80/interesting_article_on_how_deepseek_has_improved/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4em80/interesting_article_on_how_deepseek_has_improved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4em80/interesting_article_on_how_deepseek_has_improved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T19:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4awir</id>
    <title>Have you truly replaced paid models(chatgpt, Claude etc) with self hosted ollama or hugging face ?</title>
    <updated>2025-01-18T16:14:58+00:00</updated>
    <author>
      <name>/u/Economy-Fact-8362</name>
      <uri>https://old.reddit.com/user/Economy-Fact-8362</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve been experimenting with locally hosted setups, but I keep finding myself coming back to ChatGPT for the ease and performance. For those of you whoâ€™ve managed to fully switch, do you still use services like ChatGPT occasionally? Do you use both? &lt;/p&gt; &lt;p&gt;Also, what kind of GPU setup is really needed to get that kind of seamless experience? My 16GB VRAM feels pretty inadequate in comparison to what these paid models offer. Would love to hear your thoughts and setups...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy-Fact-8362"&gt; /u/Economy-Fact-8362 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4awir/have_you_truly_replaced_paid_modelschatgpt_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4awir/have_you_truly_replaced_paid_modelschatgpt_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4awir/have_you_truly_replaced_paid_modelschatgpt_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T16:14:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4kzgp</id>
    <title>We need to be able to train models on consumer-grade hardware</title>
    <updated>2025-01-18T23:55:33+00:00</updated>
    <author>
      <name>/u/yoop001</name>
      <uri>https://old.reddit.com/user/yoop001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The number of papers being published is off the charts, and thereâ€™s no way the big players can implement everything. That means some game-changing ideas might slip through the cracks. But if everyday folks could test out these so-called breakthroughs, weâ€™d be in a position to help the big players spot the real gems worth scaling up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoop001"&gt; /u/yoop001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4kzgp/we_need_to_be_able_to_train_models_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4kzgp/we_need_to_be_able_to_train_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4kzgp/we_need_to_be_able_to_train_models_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T23:55:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4rsnr</id>
    <title>Theory: How AI agents talk is more important than what they do.</title>
    <updated>2025-01-19T05:55:41+00:00</updated>
    <author>
      <name>/u/ExaminationNo8522</name>
      <uri>https://old.reddit.com/user/ExaminationNo8522</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My day job is building AI agents(among other things), but I was talking to a friend this weekend about AI agents, and we realized that: Why are we building graphs and tests like they're traditional program, when AI is much closer to a human than a program? Just a person, they're super unreliable, but creative. However, society is built from people, and actually manages to keep things ticking over to a large extent. Therefore shouldn't we take more inspiration from real life bureaucracies in how we build AI agent systems, and pay a lot more attention to how agents communicate among themselves to achieve tasks. Imagine an AI system which has hierarchies, change requests and mini forms that agents would have to fill out to keep other agents informed about what they're doing - wouldn't that be more effective at doing the thing they want to do? What do people think?&lt;/p&gt; &lt;p&gt;Relevant blogpost: &lt;a href="https://2084.substack.com/p/bureaucracy-is-all-you-need"&gt;Bureaucracy is all you need. - by Lukas Nel - 2084&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExaminationNo8522"&gt; /u/ExaminationNo8522 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4rsnr/theory_how_ai_agents_talk_is_more_important_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4rsnr/theory_how_ai_agents_talk_is_more_important_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4rsnr/theory_how_ai_agents_talk_is_more_important_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T05:55:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4wv1h</id>
    <title>Overview Article of China's Six Leading AI Companies</title>
    <updated>2025-01-19T11:54:00+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4wv1h/overview_article_of_chinas_six_leading_ai/"&gt; &lt;img alt="Overview Article of China's Six Leading AI Companies" src="https://external-preview.redd.it/d_RUfW4sqqyugkU3RZhTPXuPGvIixVBC-QSNDEEWxGA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=076aad9895b4e1d05fec0419738232279ab56dec" title="Overview Article of China's Six Leading AI Companies" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://technode.com/2025/01/09/meet-chinas-top-six-ai-unicorns-who-are-leading-the-wave-of-ai-in-china/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4wv1h/overview_article_of_chinas_six_leading_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4wv1h/overview_article_of_chinas_six_leading_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T11:54:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4hb2l</id>
    <title>Theory: trying to use newer and more powerful LLMs to sound more human is likely moving in the wrong direction</title>
    <updated>2025-01-18T21:01:51+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like using more powerful LLMs to try to achieve human like speech is probably moving AWAY from the solution rather than towards it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;EDIT:&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;tl;dr-&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Newer models are more powerful and have larger context, but are heavily trained with outputs from other LLMs. This results in modern models responding far more intelligently than Llama 1 era models, but also loaded with inescapable &amp;quot;slop&amp;quot; and &amp;quot;GPTisms&amp;quot;. My proposal is that by using older Llama 1 era models/fine-tunes like Guanaco 65b (which is primarily human data trained) as a final step &amp;quot;editor&amp;quot; to rewrite what modern LLMs put out, you can get output that sounds more human and has more natural speech patterns. This could be good for articles/emails/papers/etc.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;My thought process is this: what's the difference between Llama 3.3 and Llama 1? Besides technology, I mean. Lets go back in time, to the days when we only had a 2048 token context windows; before things like NTK RoPE scaling, when your only choices were linear RoPE scaling and/or a superHoT model. Forget all the technological differences, and think: what differentiated the models of those days to the models of today?&lt;/p&gt; &lt;p&gt;Synthetic data in the training, right? The models back then were trained on minimal, if any, synthetic data. Even most of the chat models were barely trained on user/assistant type interactions. The training data was mostly human text.&lt;/p&gt; &lt;p&gt;But today? Modern models have tons and tons of &amp;quot;GPTisms&amp;quot; baked into the training data, not even speaking about how bad the instruct tune can be. So of course when you talk to the thing, it's going to sound like what we now expect an AI Assistant to sound like. And the more powerful models we get, the more we're going to see this synthetic data further baked in there.&lt;/p&gt; &lt;p&gt;In my opinion, it makes a lot of sense to go back and try some of these older models for this purpose, such as using a workflow to have the final step use an old Llama 1 model to rewrite the output of the modern LLMs.&lt;/p&gt; &lt;p&gt;Just to test it, I updated one of my Wilmer workflows to use Mistral Small as the thinker and Guanaco 65b as the responder. I had mistral write the initial response to me trying to sound human, and gave Guanaco very simple instructions to take that response and rewrite it to also sound more human. I then did the same using ChatGPT and Llama 3.3 70b.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; &lt;em&gt;This was a very small and simple POC test, and the quality of the output could be far higher with proper prompting, but I was just curious what the difference would be.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I asked the question: &lt;strong&gt;&lt;em&gt;&amp;quot;Which do you like better: Java or C#?&amp;quot;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mistral Small response:&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Well, it depends on what you're looking to do. Both Java and C# have their strengths. For example:&lt;/p&gt; &lt;p&gt;- **Java**: Itâ€™s great for building web applications, Android apps, and large-scale enterprise systems. It has a larger community and more libraries available.&lt;/p&gt; &lt;p&gt;- **C#**: It's fantastic for game development (thanks to Unity), building Windows applications, and integrating with other Microsoft services.&lt;/p&gt; &lt;p&gt;Personally, I think they both have their use cases. Java might be more versatile for certain web projects, but C# can be incredibly efficient for desktop applications and games.&lt;/p&gt; &lt;p&gt;Do you have a specific type of project in mind that you want to work on?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Obviously a very complete and well written answer, but also very indicative that an AI wrote it. Something about the way it responds is familiar to folks who would be using chatgpt and the like.&lt;/p&gt; &lt;p&gt;Now, look at &lt;strong&gt;Guanaco 65b's rewrite&lt;/strong&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I'd say Java and C# both have their pros and cons. Java is great for building websites, Android apps, and huge enterprise systems. But C# is amazing for making games with Unity, building Windows programs, and hooking into other Microsoft services. Personally, I'd use Java for web stuff and C# for desktop apps and games, but it depends on your project.&lt;/p&gt; &lt;p&gt;So, what're you trying to build? I can give you some pointers.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The difference is subtle, but see how it feels a bit more... human?&lt;/p&gt; &lt;p&gt;Here is &lt;strong&gt;ChatGPT o1's rewrite&lt;/strong&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Honestly, it really depends on what youâ€™re trying to accomplish. Both Java and C# have their own strengths:&lt;/p&gt; &lt;p&gt;* Java: Great for web applications, Android apps, and large-scale enterprise systems. It also has a huge community and tons of libraries.&lt;/p&gt; &lt;p&gt;* C#: Awesome for game development (especially with Unity), building Windows apps, and integrating with Microsoft services.&lt;/p&gt; &lt;p&gt;Personally, I think both are solid choices. Java might be more versatile for certain web-based projects, while C# shines when it comes to desktop applications and gaming.&lt;/p&gt; &lt;p&gt;Any specific type of project youâ€™re looking to work on?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This one remains sounding like an AI assistant. Same instructions and prompt I gave Guanaco.&lt;/p&gt; &lt;p&gt;Here is &lt;strong&gt;Llama 3.3's rewrite&lt;/strong&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;so you're wondering about c# and java, right? they both have their own things goin for 'em, and honestly it kinda comes down to what you're workin on and what you like. they're actually pretty similar, they're both object-oriented, they both run on virtual machines, c# uses .net and java uses the jvm, and they both got a ton of libraries and a huge community behind 'em.&lt;/p&gt; &lt;p&gt;so like, here's some differences, ok? c# is made by microsoft, so it works super well with windows and all that, and java's open-source and can run on pretty much anything that's got a jvm, so that's linux, mac, android, all that. and syntax-wise, they're a little different too, like c# uses these curly braces to define a block of code and java uses semicolons to end statements and all that.&lt;/p&gt; &lt;p&gt;personally, i'm cool with both, i mean c# is awesome for windows stuff, but java's got that whole cross-platform thing goin on, so yeahâ€¦ what about you, got a preference or whatever?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Its... well, about what you'd expect lol. Llama also deciding rewriting like a human means no capitalization =D Also, it added some details of its own to the response, but that's ok.&lt;/p&gt; &lt;p&gt;Again, this is a very simplistic POC just to show what I mean, and I'm probably going to spend more time delving into the idea, but I really think that there's still value in these old models, especially when trying to write things that sound more &amp;quot;human&amp;quot;, like maybe article or email writing.&lt;/p&gt; &lt;p&gt;Anyhow, just a thought I'd figure might spark some conversation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hb2l/theory_trying_to_use_newer_and_more_powerful_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hb2l/theory_trying_to_use_newer_and_more_powerful_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hb2l/theory_trying_to_use_newer_and_more_powerful_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T21:01:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4l5hb</id>
    <title>What LLM benchmarks actually measure (explained intuitively)</title>
    <updated>2025-01-19T00:03:18+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;1. GPQA (Graduate-Level Google-Proof Q&amp;amp;A Benchmark)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: GPQA evaluates LLMs on their ability to answer highly challenging, graduate-level questions in biology, physics, and chemistry. These questions are designed to be &amp;quot;Google-proof,&amp;quot; meaning they require deep, specialized understanding and reasoning that cannot be easily found through a simple internet search.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Difficulty&lt;/strong&gt;: Questions are crafted to be extremely difficult, with experts achieving around 65% accuracy.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Domain Expertise&lt;/strong&gt;: Tests the model's ability to handle complex, domain-specific questions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Application&lt;/strong&gt;: Useful for scalable oversight experiments where AI systems need to provide reliable information beyond human capabilities.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. MMLU (Massive Multitask Language Understanding)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: MMLU assesses the general knowledge and problem-solving abilities of LLMs across 57 subjects, ranging from elementary mathematics to professional fields like law and ethics. It tests both world knowledge and reasoning skills.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Breadth&lt;/strong&gt;: Covers a wide array of topics, making it a comprehensive test of an LLM's understanding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Granularity&lt;/strong&gt;: Evaluates models in zero-shot and few-shot settings, mimicking real-world scenarios where models must perform with minimal context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scoring&lt;/strong&gt;: Models are scored based on their accuracy in answering multiple-choice questions.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. MMLU-Pro&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: An enhanced version of MMLU, MMLU-Pro introduces more challenging, reasoning-focused questions and increases the number of answer choices from four to ten, making the tasks more complex.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Increased Complexity&lt;/strong&gt;: More reasoning-intensive questions, reducing the chance of correct answers by random guessing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stability&lt;/strong&gt;: Demonstrates greater stability under varying prompts, with less sensitivity to prompt variations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance Drop&lt;/strong&gt;: Causes a significant drop in accuracy compared to MMLU, highlighting its increased difficulty.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. MATH&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: The MATH benchmark evaluates LLMs on their ability to solve complex mathematical problems, ranging from high school to competition-level mathematics.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Problem Types&lt;/strong&gt;: Includes algebra, geometry, probability, and calculus problems.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Step-by-Step Solutions&lt;/strong&gt;: Each problem comes with a detailed solution, allowing for evaluation of reasoning steps.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Application&lt;/strong&gt;: Useful for educational applications where accurate and efficient problem-solving is crucial.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;5. HumanEval&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: HumanEval focuses on the functional correctness of code generated by LLMs. It consists of programming challenges where models must generate code that passes provided unit tests.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Code Generation&lt;/strong&gt;: Tests the model's ability to understand and produce functional code from docstrings.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evaluation Metric&lt;/strong&gt;: Uses the pass@k metric, where 'k' different solutions are generated, and the model is considered successful if any solution passes all tests.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Coding&lt;/strong&gt;: Simulates real-world coding scenarios where multiple attempts might be made to solve a problem.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;6. MMMU (Massive Multi-discipline Multimodal Understanding and Reasoning)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: MMMU evaluates multimodal models on tasks requiring college-level subject knowledge and deliberate reasoning across various disciplines, including visual understanding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multimodal&lt;/strong&gt;: Incorporates text and images, testing models on tasks like understanding diagrams, charts, and other visual formats.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Expert-Level&lt;/strong&gt;: Questions are sourced from university-level materials, ensuring high difficulty.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Comprehensive&lt;/strong&gt;: Covers six core disciplines with over 183 subfields, providing a broad assessment.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;7. MathVista&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: MathVista assesses mathematical reasoning in visual contexts, combining challenges from diverse mathematical and graphical tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Visual Context&lt;/strong&gt;: Requires models to understand and reason with visual information alongside mathematical problems.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmark Composition&lt;/strong&gt;: Derived from existing datasets and includes new datasets for specific visual reasoning tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance Gap&lt;/strong&gt;: Highlights the gap between LLM capabilities and human performance in visually intensive mathematical reasoning.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;8. DocVQA (Document Visual Question Answering)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: DocVQA evaluates models on their ability to answer questions based on document images, testing both textual and visual comprehension.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Document Understanding&lt;/strong&gt;: Assesses the model's ability to interpret various document elements like text, tables, and figures.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Scenarios&lt;/strong&gt;: Mimics real-world document analysis tasks where understanding context and layout is crucial.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evaluation Metric&lt;/strong&gt;: Uses metrics like Average Normalized Levenshtein Similarity (ANLS) to measure performance.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;9. HELM (Holistic Evaluation of Language Models)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: HELM evaluates LLMs from multiple angles, offering a comprehensive view of their performance. It assesses accuracy, performance across various tasks, and integrates qualitative reviews to capture subtleties in model responses.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Holistic Approach&lt;/strong&gt;: Uses established datasets to assess accuracy and performance, alongside qualitative reviews for a nuanced understanding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Error Analysis&lt;/strong&gt;: Conducts detailed error analysis to identify specific areas where models struggle.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Task Diversity&lt;/strong&gt;: Covers a wide range of tasks, from text classification to machine translation, providing a broad assessment of model capabilities.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;10. GLUE (General Language Understanding Evaluation)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: GLUE provides a baseline for evaluating general language understanding capabilities of LLMs. It includes tasks like sentiment analysis, question answering, and textual entailment.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Comprehensive&lt;/strong&gt;: Encompasses a variety of NLP tasks, making it a robust benchmark for general language understanding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Publicly Available&lt;/strong&gt;: Datasets are publicly available, allowing for widespread use and comparison.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Leaderboard&lt;/strong&gt;: GLUE maintains a leaderboard where models are ranked based on their performance across its tasks.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;11. BIG-Bench Hard (BBH)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: BBH focuses on the limitations and failure modes of LLMs by selecting particularly challenging tasks from the larger BIG-Bench benchmark.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Difficulty&lt;/strong&gt;: Consists of 23 tasks where no prior model outperformed average human-rater scores, highlighting areas where models fall short.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Focused Evaluation&lt;/strong&gt;: Aims to push the boundaries of model capabilities by concentrating on tasks that are difficult for current models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Relevance&lt;/strong&gt;: Tasks are designed to reflect real-world challenges where models need to demonstrate advanced reasoning and understanding.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;12. MT-Bench&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: MT-Bench evaluates models' ability to engage in coherent, informative, and engaging conversations, focusing on conversation flow and instruction-following capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-Turn&lt;/strong&gt;: Contains 80 questions with follow-up questions, simulating real-world conversational scenarios.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM-as-a-Judge&lt;/strong&gt;: Uses strong LLMs like GPT-4 to assess the quality of model responses, providing an objective evaluation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Human Preferences&lt;/strong&gt;: Responses are annotated by graduate students with domain expertise, ensuring relevance and quality.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;13. FinBen&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: FinBen is designed to evaluate LLMs in the financial domain, covering tasks like information extraction, text analysis, question answering, and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Domain-Specific&lt;/strong&gt;: Focuses on financial tasks, providing a specialized benchmark for financial applications.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Broad Task Coverage&lt;/strong&gt;: Includes 36 datasets covering 24 tasks in seven financial domains, offering a comprehensive evaluation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Application&lt;/strong&gt;: Evaluates models on practical financial tasks, including stock trading, highlighting their utility in financial services.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;14. LegalBench&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: LegalBench assesses LLMs' legal reasoning capabilities, using datasets from various legal domains.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Legal Reasoning&lt;/strong&gt;: Tests models on tasks requiring legal knowledge and reasoning, crucial for legal applications.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Collaborative Development&lt;/strong&gt;: Developed through collaboration, ensuring a wide range of legal tasks are covered.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Scenarios&lt;/strong&gt;: Mimics real-world legal scenarios where models must interpret and apply legal principles.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4l5hb/what_llm_benchmarks_actually_measure_explained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4l5hb/what_llm_benchmarks_actually_measure_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4l5hb/what_llm_benchmarks_actually_measure_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T00:03:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i51xp7</id>
    <title>What is a decent local GPU setup for full finetuning a large BERT or ModernBERT model (~300 million parameters)?</title>
    <updated>2025-01-19T16:11:42+00:00</updated>
    <author>
      <name>/u/Lazy_Wedding_1383</name>
      <uri>https://old.reddit.com/user/Lazy_Wedding_1383</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same as above. Budget is 5000 USD&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lazy_Wedding_1383"&gt; /u/Lazy_Wedding_1383 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i51xp7/what_is_a_decent_local_gpu_setup_for_full/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i51xp7/what_is_a_decent_local_gpu_setup_for_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i51xp7/what_is_a_decent_local_gpu_setup_for_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T16:11:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4oeg9</id>
    <title>yek -- serialize your code repo (or part of it) to feed into any LLM using a fast Rust based program</title>
    <updated>2025-01-19T02:50:53+00:00</updated>
    <author>
      <name>/u/mehyay76</name>
      <uri>https://old.reddit.com/user/mehyay76</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4oeg9/yek_serialize_your_code_repo_or_part_of_it_to/"&gt; &lt;img alt="yek -- serialize your code repo (or part of it) to feed into any LLM using a fast Rust based program" src="https://external-preview.redd.it/hezSolCpDQfrkKm4K9-XqPSYxr_22M4CNnAVkJ9xh7w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f2bbfcd7007496d377206d2d8a40c8e2afedc2f" title="yek -- serialize your code repo (or part of it) to feed into any LLM using a fast Rust based program" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehyay76"&gt; /u/mehyay76 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/bodo-run/yek"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4oeg9/yek_serialize_your_code_repo_or_part_of_it_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4oeg9/yek_serialize_your_code_repo_or_part_of_it_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T02:50:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4xck6</id>
    <title>Why is OpenRouter trusted?</title>
    <updated>2025-01-19T12:26:04+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys. I'm curious what makes it a trusted proxy?&lt;/p&gt; &lt;p&gt;I investigated a bit and top contributor of the openrouter runner package: &lt;a href="https://github.com/OpenRouterTeam/openrouter-runner/graphs/contributors"&gt;https://github.com/OpenRouterTeam/openrouter-runner/graphs/contributors&lt;/a&gt;&lt;/p&gt; &lt;p&gt;tweets crypto non stop &lt;a href="https://x.com/litbid"&gt;https://x.com/litbid&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is not clear how they cover infra costs for proxying so much real-time data as they sell tokens on their base price. I understand they receive discounts for so much usage from providers like Anthropic? Is it possible they have agreements with all the other providers like DeepSeek?&lt;/p&gt; &lt;p&gt;In a scenario they don't have agreement with anyone at all, they must hoard all this data and handle it unclearly to the end user, don't you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4xck6/why_is_openrouter_trusted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4xck6/why_is_openrouter_trusted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4xck6/why_is_openrouter_trusted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T12:26:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4w47k</id>
    <title>A summary of Qwen Models!</title>
    <updated>2025-01-19T11:03:14+00:00</updated>
    <author>
      <name>/u/rbgo404</name>
      <uri>https://old.reddit.com/user/rbgo404</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4w47k/a_summary_of_qwen_models/"&gt; &lt;img alt="A summary of Qwen Models!" src="https://preview.redd.it/bvg95yewmxde1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f3b57fffcd0c406cfbf23fc038343779f95f470" title="A summary of Qwen Models!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rbgo404"&gt; /u/rbgo404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bvg95yewmxde1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4w47k/a_summary_of_qwen_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4w47k/a_summary_of_qwen_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T11:03:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i50lxx</id>
    <title>OpenAI has access to the FrontierMath dataset; the mathematicians involved in creating it were unaware of this</title>
    <updated>2025-01-19T15:13:21+00:00</updated>
    <author>
      <name>/u/LLMtwink</name>
      <uri>https://old.reddit.com/user/LLMtwink</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/JacquesThibs/status/1880770081132810283?s=19"&gt;https://x.com/JacquesThibs/status/1880770081132810283?s=19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The holdout set that the Lesswrong post &lt;em&gt;implies&lt;/em&gt; exists hasn't been developed yet&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/georgejrjrjr/status/1880972666385101231?s=19"&gt;https://x.com/georgejrjrjr/status/1880972666385101231?s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LLMtwink"&gt; /u/LLMtwink &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i50lxx/openai_has_access_to_the_frontiermath_dataset_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i50lxx/openai_has_access_to_the_frontiermath_dataset_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i50lxx/openai_has_access_to_the_frontiermath_dataset_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T15:13:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4vwm7</id>
    <title>Iâ€™m starting to think ai benchmarks are useless</title>
    <updated>2025-01-19T10:48:32+00:00</updated>
    <author>
      <name>/u/getpodapp</name>
      <uri>https://old.reddit.com/user/getpodapp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Across every possible task I can think of Claude beats all other models by a wide margin IMO. &lt;/p&gt; &lt;p&gt;I have three ai agents that I've built that are tasked with researching, writing and outreaching to clients.&lt;/p&gt; &lt;p&gt;Claude absolutely wipes the floor with every other model, yet Claude is usually beat in benchmarks by OpenAI and Google models.&lt;/p&gt; &lt;p&gt;When I ask the question, how do we know these labs aren't benchmarks by just overfitting their models to perform well on the benchmark the answer is always &amp;quot;yeah we don't really know that&amp;quot;. Not only can we never be sure but they are absolutely incentivised to do it. &lt;/p&gt; &lt;p&gt;I remember only a few months ago, whenever a new model would be released that would do 0.5% or whatever better on MMLU pro, I'd switch my agents to use that new model assuming the pricing was similar. (Thanks to openrouter this is really easy)&lt;/p&gt; &lt;p&gt;At this point I'm just stuck with running the models and seeing which one of the outputs perform best at their task (mine and coworkers opinions)&lt;/p&gt; &lt;p&gt;How do you go about evaluating model performance? Benchmarks seem highly biased towards labs that want to win the ai benchmarks, fortunately not Anthropic.&lt;/p&gt; &lt;p&gt;Looking forward to responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getpodapp"&gt; /u/getpodapp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4vwm7/im_starting_to_think_ai_benchmarks_are_useless/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4vwm7/im_starting_to_think_ai_benchmarks_are_useless/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4vwm7/im_starting_to_think_ai_benchmarks_are_useless/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T10:48:32+00:00</published>
  </entry>
</feed>
