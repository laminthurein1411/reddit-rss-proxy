<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-24T04:25:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jiilot</id>
    <title>jukofyork/DeepSeek-R1-DRAFT-0.5B-GGUF Â· Hugging Face</title>
    <updated>2025-03-24T04:24:30+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jiilot/jukofyorkdeepseekr1draft05bgguf_hugging_face/"&gt; &lt;img alt="jukofyork/DeepSeek-R1-DRAFT-0.5B-GGUF Â· Hugging Face" src="https://external-preview.redd.it/s10KkW_R4qMdLDisGcqbfEBZS2Ye7-xQcXtqJrlwkdQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e1d39d630b0703ef51c2c41d065cb21b81b2ceb" title="jukofyork/DeepSeek-R1-DRAFT-0.5B-GGUF Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/jukofyork/DeepSeek-R1-DRAFT-0.5B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jiilot/jukofyorkdeepseekr1draft05bgguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jiilot/jukofyorkdeepseekr1draft05bgguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T04:24:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhyg3l</id>
    <title>Nvidia Jetson Thor AGX specs</title>
    <updated>2025-03-23T12:45:16+00:00</updated>
    <author>
      <name>/u/Temporary-Size7310</name>
      <uri>https://old.reddit.com/user/Temporary-Size7310</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;@SureshotM6 who attend to GTC &amp;quot;An Introduction to Building Humanoid Robots&amp;quot; reported Jetson Thor AGX specs:&lt;/p&gt; &lt;p&gt;â€¢ Available in June 2025&lt;/p&gt; &lt;p&gt;â€¢ 2560 CUDA cores, 96 Tensor cores (+25% from Orin AGX)&lt;/p&gt; &lt;p&gt;â€¢ 7.8 FP32 TFLOPS (47% faster than Jetson Orin AGX at 5.32 FP32 TFLOPS)&lt;/p&gt; &lt;p&gt;â€¢ 2000 FP4 TOPS&lt;/p&gt; &lt;p&gt;â€¢ 1000 FP8 TOPS (Orin AGX is 275 INT8 TOPS; Blackwell has same INT8/FP8 performance)&lt;/p&gt; &lt;p&gt;â€¢ 14 ARMv9 cores at 2.6x performance of Orin cores (Orin has 12 cores)&lt;/p&gt; &lt;p&gt;â€¢ 128GB of RAM (Orin AGX is 64GB)&lt;/p&gt; &lt;p&gt;â€¢ 273GB/s RAM bandwidth (33% faster than Orin AGX at 204.8GB/s)&lt;/p&gt; &lt;p&gt;â€¢ 120W max power (double Orin AGX at 60W)&lt;/p&gt; &lt;p&gt;â€¢ 4x 25GbE&lt;/p&gt; &lt;p&gt;â€¢ 1x 5GbE (at least present on devkit)&lt;/p&gt; &lt;p&gt;â€¢ 12 lanes PCle Gen5 (32GT/s per lane).&lt;/p&gt; &lt;p&gt;â€¢ 100mm x 87mm (same as existing AGX)&lt;/p&gt; &lt;p&gt;â€¢ All 1/O interfaces for devkit &amp;quot;on one side of board&amp;quot;&lt;/p&gt; &lt;p&gt;â€¢ Integrated 1TB NVMe storage on devkit&lt;/p&gt; &lt;p&gt;As I told in my post on DGX Sparks, it is really similar to Jetson, while one is designed for on premise, jetson are made for embedded&lt;/p&gt; &lt;p&gt;The number of Cuda core and tensor core could give us some hints on the DGX Sparks number that's still not release&lt;/p&gt; &lt;p&gt;The OS is not specified but it will be probably Jetpack (Jetson Linux/Ubuntu based with librairies for AI)&lt;/p&gt; &lt;p&gt;Note: With enhancement on Nvidia arm based hardware we should see more aarch64 and wheel software&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary-Size7310"&gt; /u/Temporary-Size7310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhyg3l/nvidia_jetson_thor_agx_specs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhyg3l/nvidia_jetson_thor_agx_specs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhyg3l/nvidia_jetson_thor_agx_specs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T12:45:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhsqlr</id>
    <title>How does Groq.com do it? (Groq not Elon's grok)</title>
    <updated>2025-03-23T06:05:49+00:00</updated>
    <author>
      <name>/u/AlgorithmicKing</name>
      <uri>https://old.reddit.com/user/AlgorithmicKing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How does groq run llms so fast? Is it just very high power or they use some technique?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlgorithmicKing"&gt; /u/AlgorithmicKing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhsqlr/how_does_groqcom_do_it_groq_not_elons_grok/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhsqlr/how_does_groqcom_do_it_groq_not_elons_grok/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhsqlr/how_does_groqcom_do_it_groq_not_elons_grok/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T06:05:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhl6jp</id>
    <title>Gemma3 is outperforming a ton of models on fine-tuning / world knowledge</title>
    <updated>2025-03-22T23:03:02+00:00</updated>
    <author>
      <name>/u/fluxwave</name>
      <uri>https://old.reddit.com/user/fluxwave</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6jp/gemma3_is_outperforming_a_ton_of_models_on/"&gt; &lt;img alt="Gemma3 is outperforming a ton of models on fine-tuning / world knowledge" src="https://external-preview.redd.it/XifaOkXuUsJa3iGAsHuitV7h5kD9H1PhRfgSOnPUnbc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1663259cbca9e5b87b6c2d99b3d23f12f5a2118" title="Gemma3 is outperforming a ton of models on fine-tuning / world knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yid0t6cxmbqe1.png?width=556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2cacf5b4f6c0d99f9902bf7e3a5e4da5c50d41ea"&gt;https://preview.redd.it/yid0t6cxmbqe1.png?width=556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2cacf5b4f6c0d99f9902bf7e3a5e4da5c50d41ea&lt;/a&gt;&lt;/p&gt; &lt;p&gt;At fine-tuning they seem to be smashing evals -- see this tweet above from OpenPipe. &lt;/p&gt; &lt;p&gt;Then in world-knowledge (or at least this smaller task of identifying the gender of scholars across history) a 12B model beat OpenAI's gpt-4o-mini. This is using no fine-tuning. &lt;a href="https://thedataquarry.com/blog/using-llms-to-enrich-datasets/"&gt;https://thedataquarry.com/blog/using-llms-to-enrich-datasets/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p11ujen8nbqe1.png?width=1187&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=897f8506ee01cffcbad459d11da436a2e1521501"&gt;Written by Prashanth Rao&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(disclaimer: Prashanth is a member of the BAML community -- our prompting DSL / toolchain &lt;a href="https://github.com/BoundaryML/baml"&gt;https://github.com/BoundaryML/baml&lt;/a&gt; , but he works at KuzuDB).&lt;/p&gt; &lt;p&gt;Has anyone else seen amazing results with Gemma3? Curious to see if people have tried it more.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fluxwave"&gt; /u/fluxwave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6jp/gemma3_is_outperforming_a_ton_of_models_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6jp/gemma3_is_outperforming_a_ton_of_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6jp/gemma3_is_outperforming_a_ton_of_models_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T23:03:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji8u8k</id>
    <title>Does any other official release match Gemma 3's natural language style?</title>
    <updated>2025-03-23T20:26:49+00:00</updated>
    <author>
      <name>/u/redditisunproductive</name>
      <uri>https://old.reddit.com/user/redditisunproductive</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma 3 is the only non-slop official release (not counting independent finetunes) I have tried. They must have a completely independent dataset or something. Even 4o is forced in comparison, like you can tell they aligned away the slop but the syntax and such is still there.&lt;/p&gt; &lt;p&gt;Is there any other official open source release like this, or is Gemma really that unique?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redditisunproductive"&gt; /u/redditisunproductive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji8u8k/does_any_other_official_release_match_gemma_3s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji8u8k/does_any_other_official_release_match_gemma_3s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji8u8k/does_any_other_official_release_match_gemma_3s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T20:26:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji8ypl</id>
    <title>Current best practice on local voice cloning?</title>
    <updated>2025-03-23T20:32:03+00:00</updated>
    <author>
      <name>/u/Straight-Worker-4327</name>
      <uri>https://old.reddit.com/user/Straight-Worker-4327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are the current best practices for creating a TTS model from my own voice.&lt;br /&gt; I have a lot of audio material of me talking. &lt;/p&gt; &lt;p&gt;Which method would you recommend sounds most natural? Is there something that can also do emotional speech. I would like to finetune it locally but I can also do it in the cloud? Do you maybe now a cloud service which offers voice cloning which you can then download and use local?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Straight-Worker-4327"&gt; /u/Straight-Worker-4327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji8ypl/current_best_practice_on_local_voice_cloning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji8ypl/current_best_practice_on_local_voice_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji8ypl/current_best_practice_on_local_voice_cloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T20:32:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji6mp3</id>
    <title>Testing Groq's Speculative Decoding version of Meta Llama 3.3 70 B</title>
    <updated>2025-03-23T18:53:36+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all - just wanted to share this video - my kid has been buggin me to let her make youtube videos of our cat. Dont ask how, but I managed to convince her to help me make AI videos instead - so presenting, our first collaboration - Testing out LLAMA spec dec. &lt;/p&gt; &lt;p&gt;TLDR - We want to test if speculative decoding impacts quality, and what kind of speedups we get. Conclusion - no impact on quality, between 2-4 x speed ups on groq :-) &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=1ojrDaxExLY"&gt;https://www.youtube.com/watch?v=1ojrDaxExLY&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji6mp3/testing_groqs_speculative_decoding_version_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji6mp3/testing_groqs_speculative_decoding_version_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji6mp3/testing_groqs_speculative_decoding_version_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T18:53:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhzd0x</id>
    <title>Accomplished Agentic AI by DDD (Document Driven Development) and CDD (Compiler Driven Development)</title>
    <updated>2025-03-23T13:33:25+00:00</updated>
    <author>
      <name>/u/SamchonFramework</name>
      <uri>https://old.reddit.com/user/SamchonFramework</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhzd0x/accomplished_agentic_ai_by_ddd_document_driven/"&gt; &lt;img alt="Accomplished Agentic AI by DDD (Document Driven Development) and CDD (Compiler Driven Development)" src="https://external-preview.redd.it/z2FVrkSNjmQYb0AhBKdMT7w4IJXCJkDclbDm6XF3Sl0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96ff6f86ee0f4f38f43271a3071e071d123fc574" title="Accomplished Agentic AI by DDD (Document Driven Development) and CDD (Compiler Driven Development)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SamchonFramework"&gt; /u/SamchonFramework &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wrtnlabs.io/agentica/docs/concepts/document-driven-development/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhzd0x/accomplished_agentic_ai_by_ddd_document_driven/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhzd0x/accomplished_agentic_ai_by_ddd_document_driven/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T13:33:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhv57y</id>
    <title>Finally some good news for older hardware pricing</title>
    <updated>2025-03-23T09:04:21+00:00</updated>
    <author>
      <name>/u/xlrz28xd</name>
      <uri>https://old.reddit.com/user/xlrz28xd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.businessinsider.com/nvidia-ceo-jensen-huang-joke-blackwell-hopper-gpu-customers-2025-3"&gt;https://www.businessinsider.com/nvidia-ceo-jensen-huang-joke-blackwell-hopper-gpu-customers-2025-3&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;I said before that when Blackwell starts shipping in volume, you couldn't give Hoppers away,&amp;quot; he said at Nvidia's big AI conference Tuesday.&lt;/p&gt; &lt;p&gt;&amp;quot;There are circumstances where Hopper is fine,&amp;quot; he added. &amp;quot;Not many.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And then:&lt;/p&gt; &lt;p&gt;CFO Brian Olsavsky said on Amazon's earnings call last month that the company &amp;quot;observed an increased pace of technology development, particularly in the area of artificial intelligence and machine learning.&amp;quot;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;As a result, we're decreasing the useful life for a subset of our servers and networking equipment from 6 years to 5 years, beginning in January 2025,&amp;quot; Olsavsky said, adding that this will cut operating income this year by about $700 million.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Then, more bad news: Amazon &amp;quot;early-retired&amp;quot; some of its servers and network equipment, Olsavsky said, adding that this &amp;quot;accelerated depreciation&amp;quot; cost about $920 million and that the company expects it will decrease operating income in 2025 by about $600 million.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xlrz28xd"&gt; /u/xlrz28xd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhv57y/finally_some_good_news_for_older_hardware_pricing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhv57y/finally_some_good_news_for_older_hardware_pricing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhv57y/finally_some_good_news_for_older_hardware_pricing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T09:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jia4bo</id>
    <title>LLM-Tournament - Have 4 Frontier Models Duke It Out over 5 Rounds to Solve Your Problem</title>
    <updated>2025-03-23T21:21:47+00:00</updated>
    <author>
      <name>/u/dicklesworth</name>
      <uri>https://old.reddit.com/user/dicklesworth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jia4bo/llmtournament_have_4_frontier_models_duke_it_out/"&gt; &lt;img alt="LLM-Tournament - Have 4 Frontier Models Duke It Out over 5 Rounds to Solve Your Problem" src="https://external-preview.redd.it/ibJyVkbD40D0g_HKQl_AwGobyLDOplTncuvnZPCWTBk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc6923b6aae868e63860532f5d2e1a3b02641bf4" title="LLM-Tournament - Have 4 Frontier Models Duke It Out over 5 Rounds to Solve Your Problem" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had this idea yesterday and wrote this article. In the process, I decided to automate the entire method, and the project that does that is linked at the end of the article.&lt;/p&gt; &lt;p&gt;Right now, itâ€™s set up to use LLM APls, but it would be trivially easy to switch it to use local LLMs, and I'll probably add that soon as an option. The more interesting part is the method itself and how well it works in practice.&lt;/p&gt; &lt;p&gt;Iâ€™m really excited about this and think Iâ€™m going to be using this very intensively for my own development work, for any code that has to solve messy, ill-defined problems that admit a lot of possible approaches and solutions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dicklesworth"&gt; /u/dicklesworth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Dicklesworthstone/llm_multi_round_coding_tournament/blob/main/README.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jia4bo/llmtournament_have_4_frontier_models_duke_it_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jia4bo/llmtournament_have_4_frontier_models_duke_it_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T21:21:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji2grb</id>
    <title>A770 vs 9070XT benchmarks</title>
    <updated>2025-03-23T15:57:27+00:00</updated>
    <author>
      <name>/u/DurianyDo</name>
      <uri>https://old.reddit.com/user/DurianyDo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;9900X, X870, 96GB 5200MHz CL40, Sparkle Titan OC edition, Gigabyte Gaming OC.&lt;/p&gt; &lt;p&gt;Ubuntu 24.10 default drivers for AMD and Intel&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks with Flash Attention:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;./llama-bench -ngl 100 -fa 1 -t 24 -m &amp;quot;~/Mistral-Small-24B-Instruct-2501-Q4_K_L.gguf&amp;quot; &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;type&lt;/th&gt; &lt;th align="left"&gt;A770&lt;/th&gt; &lt;th align="left"&gt;9070XT&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;30.83&lt;/td&gt; &lt;td align="left"&gt;248.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;5.48&lt;/td&gt; &lt;td align="left"&gt;19.28&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;./llama-bench -ngl 100 -fa 1 -t 24 -m &amp;quot;~/Meta-Llama-3.1-8B-Instruct-Q5_K_S.gguf&amp;quot;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;type&lt;/th&gt; &lt;th align="left"&gt;A770&lt;/th&gt; &lt;th align="left"&gt;9070XT&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;93.08&lt;/td&gt; &lt;td align="left"&gt;412.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;16.59&lt;/td&gt; &lt;td align="left"&gt;30.44&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;...and then during benchmarking I found that there's more performance without FA :)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;9070XT Without Flash Attention:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;./llama-bench -m &amp;quot;Mistral-Small-24B-Instruct-2501-Q4_K_L.gguf&amp;quot; and ./llama-bench -m &amp;quot;Meta-Llama-3.1-8B-Instruct-Q5_K_S.gguf&amp;quot; &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;9070XT&lt;/th&gt; &lt;th align="left"&gt;Mistral-Small-24B-I-Q4KL&lt;/th&gt; &lt;th align="left"&gt;Llama-3.1-8B-I-Q5KS&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;No FA&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;451.34&lt;/td&gt; &lt;td align="left"&gt;1268.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;33.55&lt;/td&gt; &lt;td align="left"&gt;84.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;With FA&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;248.07&lt;/td&gt; &lt;td align="left"&gt;412.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;19.28&lt;/td&gt; &lt;td align="left"&gt;30.44&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DurianyDo"&gt; /u/DurianyDo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji2grb/a770_vs_9070xt_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji2grb/a770_vs_9070xt_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji2grb/a770_vs_9070xt_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T15:57:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jicibv</id>
    <title>Local AI Voice Assistant with Ollama + gTTS, would love some feedback!</title>
    <updated>2025-03-23T23:07:33+00:00</updated>
    <author>
      <name>/u/typhoon90</name>
      <uri>https://old.reddit.com/user/typhoon90</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jicibv/local_ai_voice_assistant_with_ollama_gtts_would/"&gt; &lt;img alt="Local AI Voice Assistant with Ollama + gTTS, would love some feedback!" src="https://external-preview.redd.it/TQ2fGDRsMwYWjW0KZrmUIsOmwQ7oZISnsLRQppMhYvk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5ebab71cac0d3ea15e7e3563017501fcd0f9627" title="Local AI Voice Assistant with Ollama + gTTS, would love some feedback!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/typhoon90"&gt; /u/typhoon90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ExoFi-Labs/OllamaGTTS"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jicibv/local_ai_voice_assistant_with_ollama_gtts_would/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jicibv/local_ai_voice_assistant_with_ollama_gtts_would/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T23:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jif6oa</id>
    <title>Second Me: Local trained Open-source alternative to centralized AI that preserves your autonomy</title>
    <updated>2025-03-24T01:17:15+00:00</updated>
    <author>
      <name>/u/DontPlayMeLikeAFool</name>
      <uri>https://old.reddit.com/user/DontPlayMeLikeAFool</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,I wanted to share our Python-based open-source project &lt;a href="https://github.com/Mindverse/Second-Me"&gt;Second Me&lt;/a&gt;. We've created a framework that lets you build and train a personalized AI representation of yourself.Technical highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hierarchical Memory Modeling with three-layer structure (L0-L2)&lt;/li&gt; &lt;li&gt;Me-alignment system using reinforcement learning&lt;/li&gt; &lt;li&gt;Outperforms leading RAG systems by 37% in personalization tests&lt;/li&gt; &lt;li&gt;Decentralized architecture for AI-to-AI interaction&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Python codebase is well-documented and contributions are welcome! We're particularly interested in expanding the role-play capabilities and improving the memory modeling system.If you're interested in AI, identity, or decentralized AI systems, we'd love your feedback and stars!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DontPlayMeLikeAFool"&gt; /u/DontPlayMeLikeAFool &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jif6oa/second_me_local_trained_opensource_alternative_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jif6oa/second_me_local_trained_opensource_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jif6oa/second_me_local_trained_opensource_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T01:17:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji32vh</id>
    <title>Understanding R1-Zero-Like Training - Deepseek v3 and Qwen can reason without RL, GRPO has a bug, and introducing Dr. GRPO</title>
    <updated>2025-03-23T16:23:49+00:00</updated>
    <author>
      <name>/u/KTibow</name>
      <uri>https://old.reddit.com/user/KTibow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji32vh/understanding_r1zerolike_training_deepseek_v3_and/"&gt; &lt;img alt="Understanding R1-Zero-Like Training - Deepseek v3 and Qwen can reason without RL, GRPO has a bug, and introducing Dr. GRPO" src="https://external-preview.redd.it/TAZjkkE0Ie6I8uHJeC_LAchU08G_51bEcBfU7K5RFMM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d788038bc811df56653c5f2bd40a0746e3f2efe" title="Understanding R1-Zero-Like Training - Deepseek v3 and Qwen can reason without RL, GRPO has a bug, and introducing Dr. GRPO" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KTibow"&gt; /u/KTibow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/sail-sg/understand-r1-zero"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji32vh/understanding_r1zerolike_training_deepseek_v3_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji32vh/understanding_r1zerolike_training_deepseek_v3_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T16:23:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji8o7p</id>
    <title>Quantization Method Matters: MLX Q2 vs GGUF Q2_K: MLX ruins the model performance whereas GGUF keeps it useable</title>
    <updated>2025-03-23T20:19:33+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji8o7p/quantization_method_matters_mlx_q2_vs_gguf_q2_k/"&gt; &lt;img alt="Quantization Method Matters: MLX Q2 vs GGUF Q2_K: MLX ruins the model performance whereas GGUF keeps it useable" src="https://external-preview.redd.it/ZHI0enNwOW16aHFlMU0viwrkIp2qoHODjO1xeyQmexy85eAy-zLVHbokf46f.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81e60a85697fe81f6d46a7a028cc13e36dea4682" title="Quantization Method Matters: MLX Q2 vs GGUF Q2_K: MLX ruins the model performance whereas GGUF keeps it useable" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3ijpyp9mzhqe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji8o7p/quantization_method_matters_mlx_q2_vs_gguf_q2_k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji8o7p/quantization_method_matters_mlx_q2_vs_gguf_q2_k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T20:19:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji75t5</id>
    <title>Mistral 24b</title>
    <updated>2025-03-23T19:15:30+00:00</updated>
    <author>
      <name>/u/Illustrious-Dot-6888</name>
      <uri>https://old.reddit.com/user/Illustrious-Dot-6888</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First time using Mistral 24b today. Man, how good this thing is! And fast too!Finally a model that translates perfectly. This is a keeper.ðŸ¤—&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Dot-6888"&gt; /u/Illustrious-Dot-6888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji75t5/mistral_24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji75t5/mistral_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji75t5/mistral_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T19:15:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji5mbg</id>
    <title>Are there any attempts at CPU-only LLM architectures? I know Nvidia doesn't like it, but the biggest threat to their monopoly is AI models that don't need that much GPU compute</title>
    <updated>2025-03-23T18:10:50+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically the title. I know of this post &lt;a href="https://github.com/flawedmatrix/mamba-ssm"&gt;https://github.com/flawedmatrix/mamba-ssm&lt;/a&gt; that optimizes MAMBA for CPU-only devices, but other than that, I don't know of any other effort.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji5mbg/are_there_any_attempts_at_cpuonly_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji5mbg/are_there_any_attempts_at_cpuonly_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji5mbg/are_there_any_attempts_at_cpuonly_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T18:10:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhwr2p</id>
    <title>Next Gemma versions wishlist</title>
    <updated>2025-03-23T11:00:25+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I'm Omar from the Gemma team. Few months ago, we &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hchoyy/open_models_wishlist/"&gt;asked for user feedback &lt;/a&gt;and incorporated it into Gemma 3: longer context, a smaller model, vision input, multilinguality, and so on, while doing a nice lmsys jump! We also made sure to collaborate with OS maintainers to have decent support at day-0 in your favorite tools, including vision in llama.cpp!&lt;/p&gt; &lt;p&gt;Now, it's time to look into the future. What would you like to see for future Gemma versions? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhwr2p/next_gemma_versions_wishlist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhwr2p/next_gemma_versions_wishlist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhwr2p/next_gemma_versions_wishlist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T11:00:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jie6oo</id>
    <title>Mistral small draft model</title>
    <updated>2025-03-24T00:27:09+00:00</updated>
    <author>
      <name>/u/frivolousfidget</name>
      <uri>https://old.reddit.com/user/frivolousfidget</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jie6oo/mistral_small_draft_model/"&gt; &lt;img alt="Mistral small draft model" src="https://external-preview.redd.it/fGQq4SyEYUq9b_wFHpxbLnoYjtgYQA70SDrLvYPMmkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c1cb6b0de0e00a235e6f56fd99854ec7f7e0180" title="Mistral small draft model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was browsing hugging face and found this model, made a 4bit mlx quants and it actually seems to work really well! 60.7% accepted tokens in a coding test!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frivolousfidget"&gt; /u/frivolousfidget &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/alamios/Mistral-Small-3.1-DRAFT-0.5B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jie6oo/mistral_small_draft_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jie6oo/mistral_small_draft_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T00:27:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jiewjn</id>
    <title>Possible Llama 4 prototypes on Chatbot Arena</title>
    <updated>2025-03-24T01:02:40+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There currently is an unusually large number of anonymous Llama/Meta models randomly appearing on &lt;a href="https://lmarena.ai/"&gt;Chatbot Arena&lt;/a&gt; Battle and it's fair to assume assuming that all or most of them are test versions of Llama 4. Most appear to have image input capabilities and some have a different feel than others. Anybody tested them?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;aurora&lt;/code&gt; -&amp;gt; Developed by MetaAI, image-enabled.&lt;/li&gt; &lt;li&gt;&lt;code&gt;ertiga&lt;/code&gt; -&amp;gt; Llama, developed by MetaAI, image-enabled.&lt;/li&gt; &lt;li&gt;&lt;code&gt;pinnacle&lt;/code&gt; -&amp;gt; Llama, developed by MetaAI, image-enabled.&lt;/li&gt; &lt;li&gt;&lt;code&gt;rhea&lt;/code&gt; -&amp;gt; Claims to be Llama 3, a friendly assistant created by Meta AI.&lt;/li&gt; &lt;li&gt;&lt;code&gt;solaris&lt;/code&gt; -&amp;gt; Llama model, image-enabled.&lt;/li&gt; &lt;li&gt;&lt;code&gt;sparrow&lt;/code&gt; -&amp;gt; LLaMA (Large Language Model Application), made by Meta&lt;/li&gt; &lt;li&gt;&lt;code&gt;spectra&lt;/code&gt; -&amp;gt; No name disclosed, but created by MetaAI. Image-enabled.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jiewjn/possible_llama_4_prototypes_on_chatbot_arena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jiewjn/possible_llama_4_prototypes_on_chatbot_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jiewjn/possible_llama_4_prototypes_on_chatbot_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T01:02:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji0fwh</id>
    <title>Qwq gets bad reviews because it's used wrong</title>
    <updated>2025-03-23T14:25:50+00:00</updated>
    <author>
      <name>/u/Far_Buyer_7281</name>
      <uri>https://old.reddit.com/user/Far_Buyer_7281</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title says it all, Loaded up with these parameters in ollama:&lt;/p&gt; &lt;p&gt;temperature 0.6&lt;br /&gt; top_p 0.95&lt;br /&gt; top_k 40&lt;br /&gt; repeat_penalty 1&lt;br /&gt; num_ctx 16,384 &lt;/p&gt; &lt;p&gt;Using a logic that does &lt;strong&gt;&lt;em&gt;not&lt;/em&gt;&lt;/strong&gt; feed the thinking proces into the context,&lt;br /&gt; Its the best local modal available right now, &lt;strong&gt;&lt;em&gt;I think I will die on this hill.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;But you can proof me wrong, tell me about a task or prompt another model can do better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far_Buyer_7281"&gt; /u/Far_Buyer_7281 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji0fwh/qwq_gets_bad_reviews_because_its_used_wrong/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji0fwh/qwq_gets_bad_reviews_because_its_used_wrong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji0fwh/qwq_gets_bad_reviews_because_its_used_wrong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T14:25:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji7oh6</id>
    <title>Q2 models are utterly useless. Q4 is the minimum quantization level that doesn't ruin the model (at least for MLX). Example with Mistral Small 24B at Q2 â†“</title>
    <updated>2025-03-23T19:37:18+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji7oh6/q2_models_are_utterly_useless_q4_is_the_minimum/"&gt; &lt;img alt="Q2 models are utterly useless. Q4 is the minimum quantization level that doesn't ruin the model (at least for MLX). Example with Mistral Small 24B at Q2 â†“" src="https://external-preview.redd.it/MGNib2hqNDBzaHFlMduLIsGKeKAIqIhMG9hPFCUJocOn0o4QCk_kODmlK36c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=606d07cb0b1252363afb0bafccfd8b42f1c705d6" title="Q2 models are utterly useless. Q4 is the minimum quantization level that doesn't ruin the model (at least for MLX). Example with Mistral Small 24B at Q2 â†“" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ns6gqa40shqe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji7oh6/q2_models_are_utterly_useless_q4_is_the_minimum/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji7oh6/q2_models_are_utterly_useless_q4_is_the_minimum/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T19:37:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji525h</id>
    <title>Since its release I've gone through all three phases of QwQ acceptance</title>
    <updated>2025-03-23T17:47:23+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji525h/since_its_release_ive_gone_through_all_three/"&gt; &lt;img alt="Since its release I've gone through all three phases of QwQ acceptance" src="https://preview.redd.it/8qv1c0xd8hqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61fdacdcc23fd66a96010f24d3c0bec601ad7eed" title="Since its release I've gone through all three phases of QwQ acceptance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qv1c0xd8hqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji525h/since_its_release_ive_gone_through_all_three/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji525h/since_its_release_ive_gone_through_all_three/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T17:47:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jifvny</id>
    <title>I made a diagram and explanation of how transformers work</title>
    <updated>2025-03-24T01:52:51+00:00</updated>
    <author>
      <name>/u/Cromulent123</name>
      <uri>https://old.reddit.com/user/Cromulent123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jifvny/i_made_a_diagram_and_explanation_of_how/"&gt; &lt;img alt="I made a diagram and explanation of how transformers work" src="https://b.thumbs.redditmedia.com/fprSteMLUKc-khfg243EAMYIKDnFkQHn_2xMr_C2w7o.jpg" title="I made a diagram and explanation of how transformers work" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cromulent123"&gt; /u/Cromulent123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jifvny"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jifvny/i_made_a_diagram_and_explanation_of_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jifvny/i_made_a_diagram_and_explanation_of_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T01:52:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jig5re</id>
    <title>Meta released a paper last month that seems to have gone under the radar. ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization. This is a better solution than BitNet and means if Meta wanted (for 10% extra compute) they could give us extremely performant 2-bit models.</title>
    <updated>2025-03-24T02:07:26+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jig5re/meta_released_a_paper_last_month_that_seems_to/"&gt; &lt;img alt="Meta released a paper last month that seems to have gone under the radar. ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization. This is a better solution than BitNet and means if Meta wanted (for 10% extra compute) they could give us extremely performant 2-bit models." src="https://b.thumbs.redditmedia.com/9hRP5bjRzlFUKNIF0QROoq6Vx5TN7YGbabV11IZeJ3M.jpg" title="Meta released a paper last month that seems to have gone under the radar. ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization. This is a better solution than BitNet and means if Meta wanted (for 10% extra compute) they could give us extremely performant 2-bit models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/pdf/2502.02631"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jig5re/meta_released_a_paper_last_month_that_seems_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jig5re/meta_released_a_paper_last_month_that_seems_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T02:07:26+00:00</published>
  </entry>
</feed>
