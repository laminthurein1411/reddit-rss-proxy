<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-10T14:05:58+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ilh46m</id>
    <title>Training a non-English reasoning model using GRPO and Unsloth</title>
    <updated>2025-02-09T15:26:28+00:00</updated>
    <author>
      <name>/u/emanuilov</name>
      <uri>https://old.reddit.com/user/emanuilov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with training reasoning models in languages other than English/Chinese using the GRPO trainer and Unsloth.AI.&lt;/p&gt; &lt;p&gt;While most reasoning models (like DeepSeek-R1) &amp;quot;think&amp;quot; on English/Chinese, I wanted to validate if we could get decent results in other languages without massive compute.&lt;/p&gt; &lt;p&gt;Using Llama 3.1 8B as the base model, the GRPO trainer from trl, and Unsloth, I managed to get a working prototype in Bulgarian after ~5 hours of training on an L40S GPU.&lt;/p&gt; &lt;p&gt;The approach should work for any language where the base model has some pre-training coverage.&lt;/p&gt; &lt;p&gt;Link to the model: &lt;a href="https://huggingface.co/s-emanuilov/LLMBG-Llama-3.1-8B-BG-Reasoning-v0.1"&gt;https://huggingface.co/s-emanuilov/LLMBG-Llama-3.1-8B-BG-Reasoning-v0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog post about the training, dataset, etc: &lt;a href="https://unfoldai.com/reasoning-in-a-non-english-language/"&gt;https://unfoldai.com/reasoning-in-a-non-english-language/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Notebooks and training logs: &lt;a href="https://github.com/s-emanuilov/LLMBG-Llama-3.1-8B-BG-Reasoning-v0.1"&gt;https://github.com/s-emanuilov/LLMBG-Llama-3.1-8B-BG-Reasoning-v0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I hope this helps others working on multilingual reasoning models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emanuilov"&gt; /u/emanuilov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilh46m/training_a_nonenglish_reasoning_model_using_grpo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilh46m/training_a_nonenglish_reasoning_model_using_grpo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilh46m/training_a_nonenglish_reasoning_model_using_grpo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T15:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1im0d55</id>
    <title>Best FOSS LLM Coding framework?</title>
    <updated>2025-02-10T07:04:58+00:00</updated>
    <author>
      <name>/u/noellarkin</name>
      <uri>https://old.reddit.com/user/noellarkin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been copying and pasting things into Claude. This was fine when I was only intermittently using LLMs for coding, but it has become a part of my workflow now and copy/pasting seems so inefficient. What's the best FOSS LLM coding framework that has some form of short and long term memory, can load different projects, do some degree of RAG on large codebases spanning multiple files and directories etc...?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noellarkin"&gt; /u/noellarkin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im0d55/best_foss_llm_coding_framework/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im0d55/best_foss_llm_coding_framework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im0d55/best_foss_llm_coding_framework/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T07:04:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ill18f</id>
    <title>Great Models Think Alike and this Undermines AI Oversight</title>
    <updated>2025-02-09T18:12:42+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ill18f/great_models_think_alike_and_this_undermines_ai/"&gt; &lt;img alt="Great Models Think Alike and this Undermines AI Oversight" src="https://external-preview.redd.it/BvPrweB4u2rzXxGIWqF_D8vwdPandRjeXx7kZVQLVZc.jpg?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3074d0e0dcf27a6d3d02265c9d1155726b988eea" title="Great Models Think Alike and this Undermines AI Oversight" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://paperswithcode.com/paper/great-models-think-alike-and-this-undermines"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ill18f/great_models_think_alike_and_this_undermines_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ill18f/great_models_think_alike_and_this_undermines_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T18:12:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilyu1c</id>
    <title>Do reasoning LLMs suffer more from Quantization?</title>
    <updated>2025-02-10T05:25:42+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen this posted a few times without real evidence. But I'm kind of starting to see it myself.&lt;/p&gt; &lt;p&gt;Q5 is my go to for coding and general knowledge models. &lt;/p&gt; &lt;p&gt;For R1 distills though (all of them) my own testing suggests that Q5 quants introduce way more chaos and second guessing which throws off the end result, and Q6 suddenly seems to be the floor for what's acceptable.&lt;/p&gt; &lt;p&gt;Has anyone else noticed this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilyu1c/do_reasoning_llms_suffer_more_from_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilyu1c/do_reasoning_llms_suffer_more_from_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilyu1c/do_reasoning_llms_suffer_more_from_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T05:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilfhyl</id>
    <title>Is Nvidia Becoming a Bottleneck for AI Advancement?</title>
    <updated>2025-02-09T14:10:15+00:00</updated>
    <author>
      <name>/u/TheArchivist314</name>
      <uri>https://old.reddit.com/user/TheArchivist314</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was thinking about this this morning and wondering if Nvidia might be a bottleneck on AI advancement which led to me reading about recent developments and debates around AI and gpu hardware‚Äîand with Nvidia being at the center of it all. Given its dominant role in powering both the training and inference of AI models, I‚Äôm curious about whether Nvidia‚Äôs current position might actually be holding back AI progress in some ways.&lt;/p&gt; &lt;p&gt;Here are a few points that have caught my attention:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Supply Constraints:&lt;/strong&gt;&lt;br /&gt; Recent reports indicate that there are serious concerns about the supply of Nvidia‚Äôs AI chips. For example, EU competition chief Margrethe Vestager recently warned about a ‚Äúhuge bottleneck‚Äù in Nvidia‚Äôs chip supply, suggesting that shortages might slow down the rollout of AI technologies across industries Ó®Å0Ó®Ç.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Scaling Challenges:&lt;/strong&gt;&lt;br /&gt; There‚Äôs also discussion around the ‚Äúscaling law‚Äù in AI. Nvidia‚Äôs GPUs have been the workhorse behind the rapid advances in large language models and other AI systems. However, as models get larger and inference demands increase, some argue that relying heavily on Nvidia‚Äôs architecture (even with innovations like the Blackwell and Hopper series) might hit physical and economic limits. The Financial Times recently discussed how these scaling challenges might be a limiting factor, implying that more chips (and perhaps different chip architectures) will be needed to sustain AI progress Ó®Å1Ó®Ç.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Emerging Alternatives:&lt;/strong&gt;&lt;br /&gt; On the flip side, a number of new players‚Äîlike Cerebras, Groq, and even competitors from AMD and Intel‚Äîare developing specialized hardware for AI inference. These alternatives could potentially ease the pressure on Nvidia if they prove to be more efficient or cost-effective for certain tasks. This makes me wonder: Is the industry‚Äôs heavy reliance on Nvidia‚Äôs GPUs really sustainable in the long run, or will these emerging solutions shift the balance?&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Given all this, I‚Äôm trying to figure out: - Are Nvidia‚Äôs supply and architectural limitations currently acting as a bottleneck to further AI innovation?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Or is the situation more about a temporary growing pain in a rapidly evolving market, where Nvidia‚Äôs advancements (and their ability to innovate continuously) will keep pace with demand?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôd love to hear your thoughts &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheArchivist314"&gt; /u/TheArchivist314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilfhyl/is_nvidia_becoming_a_bottleneck_for_ai_advancement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilfhyl/is_nvidia_becoming_a_bottleneck_for_ai_advancement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilfhyl/is_nvidia_becoming_a_bottleneck_for_ai_advancement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T14:10:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilkosp</id>
    <title>Local Deep Research - A local LLM research assistant that generates follow-up questions and uses DuckDuckGo for web searches</title>
    <updated>2025-02-09T17:58:30+00:00</updated>
    <author>
      <name>/u/ComplexIt</name>
      <uri>https://old.reddit.com/user/ComplexIt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;- Runs 100% locally with Ollama (only search queries go to DuckDuckGo)&lt;/p&gt; &lt;p&gt;- Works with Mistral 7B or DeepSeek 14B&lt;/p&gt; &lt;p&gt;- Generates structured research reports with sources&lt;/p&gt; &lt;p&gt;Quick install:&lt;/p&gt; &lt;p&gt;&lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/LearningCircuit/local-deep-research"&gt;&lt;code&gt;https://github.com/LearningCircuit/local-deep-research&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama pull deepseek-r1:14b&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;python&lt;/code&gt; &lt;a href="http://main.py"&gt;&lt;code&gt;main.py&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/LearningCircuit/local-deep-research"&gt;https://github.com/LearningCircuit/local-deep-research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexIt"&gt; /u/ComplexIt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilkosp/local_deep_research_a_local_llm_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilkosp/local_deep_research_a_local_llm_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilkosp/local_deep_research_a_local_llm_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T17:58:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilt4r7</id>
    <title>FPGA LLM inference server with super efficient watts/token</title>
    <updated>2025-02-10T00:04:47+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilt4r7/fpga_llm_inference_server_with_super_efficient/"&gt; &lt;img alt="FPGA LLM inference server with super efficient watts/token" src="https://external-preview.redd.it/qzjctx5TUKN9ENKtDwE3WZ073fcu9uouZLK6NAEg5pg.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f25cd0c5b96251518d1082b6b20b4041107dbc42" title="FPGA LLM inference server with super efficient watts/token" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=hbm3ewrfQ9I"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilt4r7/fpga_llm_inference_server_with_super_efficient/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilt4r7/fpga_llm_inference_server_with_super_efficient/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T00:04:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilza8m</id>
    <title>My experience trying out coding agents -- Qwen2.5-coder-tools/Sonnet 3.5 on Cline and Github Copilot agent mode</title>
    <updated>2025-02-10T05:53:56+00:00</updated>
    <author>
      <name>/u/Hoak-em</name>
      <uri>https://old.reddit.com/user/Hoak-em</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To start, here's the Qwen2.5 model I've been testing out: &lt;a href="https://ollama.com/hhao/qwen2.5-coder-tools:14b"&gt;https://ollama.com/hhao/qwen2.5-coder-tools:14b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd like to just make a few quick notes about my experience over the past few days trying out the preview copilot agent feature against cline using both a specialized version of Qwen2.5 through Cline and the Sonnet 3.5 (copilot API) through Cline and copilot:&lt;/p&gt; &lt;p&gt;To start, the bad things:&lt;br /&gt; - Qwen 2.5 coder-tools still seems to run very slowly on my 7900xt, as even though it shouldnt push over the VRAM limit on its own, I'm also running the monitor and IDE on my machine and it kinda runs through the rest. A Q6 quant could be helpful here to get me just a bit extra VRAM.&lt;br /&gt; - Sonnet 3.5 (from copilot API) appears to have the same issues that Sonnet had with my pro chat subscription before -- it's almost like there are two different versions of it that I have access to at different times -- one that is really good at following rules and one that has a 50/50 chance of doing so. Direct access to the API might remedy this but it's expensive, so I'd rather not do that.&lt;br /&gt; - Cline just seems to be really bad at figuring out when it should continue or stop, whichever model I choose and whatever instructions I give it. In comparison to using sonnet pro chat directly with javascript, I've just repeatedly felt like I can trust it to run on its own, and some of the interfaces are so buggy that they're not reliable, such as the history/checkpoints interface. The really irritating thing is that in a controlled environment, Cline should be able to continue until it reaches a solution -- but it never keeps the exit conditions in memory, and thus says it &amp;quot;completed the task&amp;quot; after completing a piece of the task (usually not correctly)&lt;br /&gt; - Both Cline and Copilot are &lt;strong&gt;terrible at atypical environments.&lt;/strong&gt; I can fully define the quirks of the unique environment that the tools are running in -- such as with ROCM vs. CUDA or a heavily restricted Docker Engine, but both are unable to keep this information whithin the model's context -- since the model will break out of it -- such as recommending changing the base image to a CUDA image for a docker container that's meant for ROCM, or getting stuck in a circle of trying the same debugging/fix steps over and over if the problem isn't one that has been solved online before (to be fair, I had difficulty solving this problem directly as well and it was with dev container instances in vscode with the crippled docker engine)&lt;/p&gt; &lt;p&gt;Gonna be honest, not too many good things, but they show some room for growth:&lt;br /&gt; - Qwen 2.5 can do very simple tasks without using up my rate limits and seems to be really good at using tools at this point -- reaching near the tool-use of error rate of Sonnet 3.5 in my short sessions with it. A slight quant to reduce size and speed things up (without losing this efficacy) would make it my go-to if I could solve the exit-condition problem of Cline (and possibly even spawn multiple Cline agents or have them work under a super-agent).&lt;br /&gt; - Sonnet 3.5 agents can manage complex tasks &lt;strong&gt;as long as they match existing patterns and expectations perfectly --&lt;/strong&gt; otherwise it just requires me to spend more time in agent mode than I would with the chat on the side and autocomplete in the editor.&lt;/p&gt; &lt;p&gt;So far, this agent coding thing really is showing me that Software Engineers aren't gonna be out of a job any time soon, and in fact that the current uses for even the most powerful existing coding agents (Sonnet 3.5 + agent frameworks) do not mesh well with the actual proprietariness and limitations of academic/work systems that require accomodations and use irregular architectures. It appears that getting agents to perform really well at the standard/average coding tasks and environments makes them perform extrodinarily poorly in irregular/real-world hard engineering tasks.&lt;/p&gt; &lt;p&gt;Out of this, I have a few questions for the further development of these kinds of systems:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Am I just using Cline wrong? Is the default prompt used as part of Cline just not very performant with the models I'm using? (And what prompts should I try?)&lt;/li&gt; &lt;li&gt;Given that we have fine-tunes for specific tasks of models, such as qwen2.5-coder, the tool-use version that I'm using, and tool-use versions of R1 (and distilled) models, should the fine-tunes become even more specific so that a specific &amp;quot;irregular&amp;quot; model can be assigned to the specific &amp;quot;irregular&amp;quot; task? For example, a super-agent would assign a coding model fine-tuned on AI coding using ROCM or OneAPI rather than the typical model which will default to CUDA?&lt;/li&gt; &lt;li&gt;Given that I have access to Sonnet 3.5 through the copilot API as a powerful model but frequently run into rate limits when using the agent mode, are there any existing tools that allow powerful agents through the copilot api to leverage cheap (but focused) local llms?&lt;/li&gt; &lt;li&gt;And finally, any interesting coding/tool-use/planning models that fit coding/software engineering usecases that fit nicely into 20GB VRAM with room to spare?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hoak-em"&gt; /u/Hoak-em &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilza8m/my_experience_trying_out_coding_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilza8m/my_experience_trying_out_coding_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilza8m/my_experience_trying_out_coding_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T05:53:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1im6f94</id>
    <title>Downclocking dual 5090s?</title>
    <updated>2025-02-10T13:45:23+00:00</updated>
    <author>
      <name>/u/michaelmalak</name>
      <uri>https://old.reddit.com/user/michaelmalak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are people planning on downclocking 5090s in order to run two cards in a system plugged in to a standard U.S. 110v 15A circuit?&lt;/p&gt; &lt;p&gt;If I were to go dual 5090s, personally, I'd have an electrician install a 20A circuit. Just wondering if people are planning ahead.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/michaelmalak"&gt; /u/michaelmalak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im6f94/downclocking_dual_5090s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im6f94/downclocking_dual_5090s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im6f94/downclocking_dual_5090s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T13:45:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilkamr</id>
    <title>A comprehensive overview of everything I know about fine-tuning.</title>
    <updated>2025-02-09T17:41:46+00:00</updated>
    <author>
      <name>/u/The-Silvervein</name>
      <uri>https://old.reddit.com/user/The-Silvervein</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I‚Äôve been working on fine-tuning LLMs a bit later than everyone else (among the ones I know), and I‚Äôve struggled to understand why I‚Äôm doing what I‚Äôm doing. I‚Äôve compiled a small collection of everything I know about fine-tuning LLMs or transformer models for specific use cases. I‚Äôd like to hear your thoughts on these things!&lt;/p&gt; &lt;p&gt;Also, please share your experiences too! I'd love to hear those even more.&lt;/p&gt; &lt;p&gt;---------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;When you shouldn't fine-tune:&lt;/strong&gt;&lt;br /&gt; - When wanting the model to respond in a &amp;quot;specific&amp;quot; way in rare circumstances. That's what prompt engineering is for! Don't use a bulldozer to kill a fly.&lt;br /&gt; - For the model to learn &amp;quot;new knowledge&amp;quot;&lt;br /&gt; - When you have too little data. (Though it's being disproven that low data performs better than high data for mathematical reasoning! Still in research!)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Choosing the right data&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You want the model to learn the patterns, not the words. You need enough diverse samples, not large data of the same kind.&lt;/li&gt; &lt;li&gt;More data isn't always better. Don't dump all the data you have onto the model.&lt;/li&gt; &lt;li&gt;Every training example needs a clear input and a clear output. And optionally, context text to add additional information.&lt;/li&gt; &lt;li&gt;The dataset must have enough cases, edge cases and everything in between. You can also augment the dataset by using data from a Larger LLM.&lt;/li&gt; &lt;li&gt;Pack your datasets! They help!&lt;/li&gt; &lt;li&gt;Determine if you're performing open-ended, Instruction or chat-based text generation**.**&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Choosing the right model:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You don't need a 100B model for every task you have. For real-world applications, 1-13B models are more practical.&lt;/li&gt; &lt;li&gt;You must check the licensing to see if you use the model for commercial use cases. Some have very strict licensing.&lt;/li&gt; &lt;li&gt;A good starting point? Llama-3.1-8B.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;General fine-tuning:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An 8B model needs ~16GB of memory to load up. So, mixed precision and quantisations are used to initialise a model in case of memory restrictions.&lt;/li&gt; &lt;li&gt;If the batch size can't be increased, use the Gradient-accumulation approach. General accumulations are done for overall batch sizes of 16,32,128.&lt;/li&gt; &lt;li&gt;Save checkpoints regularly, and use &lt;code&gt;resume_from_checkpoint=True&lt;/code&gt; when needed.&lt;/li&gt; &lt;li&gt;Consider using Model-parallelism or Data-parallelism techniques to work across multiple devices for large-scale training.&lt;/li&gt; &lt;li&gt;Documentation will help in surprisingly weird situations. Maintain it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;LoRA finetuning:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Don't use QLoRA for everything. Use it only if you realise that the model couldn't fit your device. Using QLoRA roughly comes with 39% more training time while saving roughly a third of the memory needed.&lt;/li&gt; &lt;li&gt;SGD+Learning rate schedulers are useful. But using LR Schedulers with other optimizers like AdamW/Adam seems to give diminishing returns. &lt;em&gt;(need to check&lt;/em&gt; &lt;code&gt;sophia&lt;/code&gt; &lt;em&gt;optimiser.)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;A high number of training epochs doesn't bode well for LoRA finetuning.&lt;/li&gt; &lt;li&gt;Despite the general understanding of lora_alpha ~2*lora_rank, it's sometimes better to check with other values too! These two parameters need meticulous adjustments.&lt;/li&gt; &lt;li&gt;The training times found outside might be confusing. It would take too long on your PC, but it seems very fast on the reported sites. Well, your choice of GPU would also be implicating the speed. So keep that in mind.&lt;/li&gt; &lt;li&gt;LoRA is actively changing. Don't forget to check and test its different versions, such as LoRA-plus, DoRA, LoFTQ, AdaLoRA, DyLoRA, LoRA-FA etc. (still need to check many of these...)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Choosing the finetuning strategy:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Determine the right task:&lt;/strong&gt; &lt;ol&gt; &lt;li&gt;You must &amp;quot;adapt&amp;quot; the model for task-specific finetuning, such as code generation, document summarisation, and question answering.&lt;/li&gt; &lt;li&gt;For domain-specific needs like medical, financial, legal, etc., you need to push the model to update its knowledge =&amp;gt; Use RAG when applicable or fine-tune the entire model. (EDIT: This is supposed to be re-training, not fine-tuning.)&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Utilise pruning depending on the kind of task you're trying to perform. Generally, in production environments, the faster the inference, the better the performance. In this case, pruning+finetuning helps. We need to keep that in mind.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The-Silvervein"&gt; /u/The-Silvervein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilkamr/a_comprehensive_overview_of_everything_i_know/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilkamr/a_comprehensive_overview_of_everything_i_know/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilkamr/a_comprehensive_overview_of_everything_i_know/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T17:41:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1im180h</id>
    <title>Speculation of the local large model after 2 years</title>
    <updated>2025-02-10T08:08:36+00:00</updated>
    <author>
      <name>/u/Still_Potato_415</name>
      <uri>https://old.reddit.com/user/Still_Potato_415</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently, a local model with a size of 7B-9B has an intelligence level that is roughly equivalent to gpt-3.5 from two years ago. OpenAI has not disclosed the parameter size of gpt-3.5. I searched online comments, and there is no exact data, but it is estimated to be in the order of 100B parameters, which means that in 2 years, we have been able to use the most advanced large model from 2 years ago locally. &lt;/p&gt; &lt;p&gt;Considering that machine performance will also have a considerable improvement in 2 years, this linear extrapolation suggests that in 2 years, we can expect to run a MoE large model with a size of 50B-70B on par with DeepSeek R1 locally. &lt;/p&gt; &lt;p&gt;This is an optimistic estimate.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Still_Potato_415"&gt; /u/Still_Potato_415 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im180h/speculation_of_the_local_large_model_after_2_years/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im180h/speculation_of_the_local_large_model_after_2_years/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im180h/speculation_of_the_local_large_model_after_2_years/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T08:08:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilhmjh</id>
    <title>Are o1 and r1 like models "pure" llms?</title>
    <updated>2025-02-09T15:49:01+00:00</updated>
    <author>
      <name>/u/Independent_Key1940</name>
      <uri>https://old.reddit.com/user/Independent_Key1940</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilhmjh/are_o1_and_r1_like_models_pure_llms/"&gt; &lt;img alt="Are o1 and r1 like models &amp;quot;pure&amp;quot; llms?" src="https://preview.redd.it/5bo3l9c6x4ie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c02869ca10b8a6483466c006c5295d4c533b8587" title="Are o1 and r1 like models &amp;quot;pure&amp;quot; llms?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ofcourse they are! RL has been used in LLM since gpt 3.5 it's just now we've scaled the RL to play a larger part but that doesn't mean the core architecture of llm is changed.&lt;/p&gt; &lt;p&gt;What do you all think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent_Key1940"&gt; /u/Independent_Key1940 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5bo3l9c6x4ie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilhmjh/are_o1_and_r1_like_models_pure_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilhmjh/are_o1_and_r1_like_models_pure_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T15:49:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1im001e</id>
    <title>I built an open source library to perform Knowledge Distillation</title>
    <updated>2025-02-10T06:40:09+00:00</updated>
    <author>
      <name>/u/darkItachi94</name>
      <uri>https://old.reddit.com/user/darkItachi94</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;br /&gt; I recently dove deep into the weeds of knowledge distillation. &lt;a href="https://www.horusailabs.com/blogs/a-primer-to-distillation"&gt;Here&lt;/a&gt; is a blog post I wrote which gives a high level introduction to Distillation.&lt;/p&gt; &lt;p&gt;I conducted several experiments on Distillation, here is a snippet of the results:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Dataset&lt;/th&gt; &lt;th align="left"&gt;Qwen2 Model Family&lt;/th&gt; &lt;th align="left"&gt;MMLU (Reasoning)&lt;/th&gt; &lt;th align="left"&gt;GSM8k (Math)&lt;/th&gt; &lt;th align="left"&gt;WikiSQL (Coding)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Pretrained - 7B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.598&lt;/td&gt; &lt;td align="left"&gt;0.724&lt;/td&gt; &lt;td align="left"&gt;0.536&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Pretrained - 1.5B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.486&lt;/td&gt; &lt;td align="left"&gt;0.431&lt;/td&gt; &lt;td align="left"&gt;0.518&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Finetuned - 1.5B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.494&lt;/td&gt; &lt;td align="left"&gt;0.441&lt;/td&gt; &lt;td align="left"&gt;0.849&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Distilled - 1.5B, Logits Distillation&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.531&lt;/td&gt; &lt;td align="left"&gt;0.489&lt;/td&gt; &lt;td align="left"&gt;0.862&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Distilled - 1.5B, Layers Distillation&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.527&lt;/td&gt; &lt;td align="left"&gt;0.481&lt;/td&gt; &lt;td align="left"&gt;0.841&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;For a detailed analysis, you can read &lt;a href="https://www.horusailabs.com/blogs/distillation-benchmarking"&gt;this&lt;/a&gt; report.&lt;/p&gt; &lt;p&gt;I created an open source library to facilitate its adoption. You can try it &lt;a href="https://github.com/horus-ai-labs/DistillFlow/"&gt;here&lt;/a&gt;.&lt;br /&gt; My conclusion: Prefer distillation over fine-tuning when there is a substantial gap between the larger and smaller model on the target dataset. In such cases, distillation can effectively transfer knowledge, leading to significantly better performance than standard fine-tuning alone.&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkItachi94"&gt; /u/darkItachi94 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im001e/i_built_an_open_source_library_to_perform/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im001e/i_built_an_open_source_library_to_perform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im001e/i_built_an_open_source_library_to_perform/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T06:40:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilpkke</id>
    <title>I built NanoSage, a deep research local assistant that runs on your laptop</title>
    <updated>2025-02-09T21:21:14+00:00</updated>
    <author>
      <name>/u/predatar</name>
      <uri>https://old.reddit.com/user/predatar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilpkke/i_built_nanosage_a_deep_research_local_assistant/"&gt; &lt;img alt="I built NanoSage, a deep research local assistant that runs on your laptop" src="https://external-preview.redd.it/of9AG5repkF7wLAuYopFh5XHJ1Z4LNyiRkviw1g0y34.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b088f14e0a1d39a73a2151a8f50dbce2402979f9" title="I built NanoSage, a deep research local assistant that runs on your laptop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically, Given a query, NanoSage looks through the internet for relevant information, builds a tree structure of the relevant chunk of information as it finds it, summarize it, and backtracks and builds the final reports from the most relevant chunks, and all you need is just a tiny LLM that can runs on CPU.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/masterFoad/NanoSage"&gt;https://github.com/masterFoad/NanoSage&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cool Concepts I implemented and wanted to explore&lt;/p&gt; &lt;p&gt;üîπ Recursive Search with Table of Content Tracking üîπ Retrieval-Augmented Generation üîπ Supports Local &amp;amp; Web Data Sources üîπ Configurable Depth &amp;amp; Monte Carlo Exploration üîπCustomize retrieval model (colpali or all-minilm) üîπOptional Monte Carlo tree search for the given query and its subqueries. üîπCustomize your knowledge base by dumping files in the directory.&lt;/p&gt; &lt;p&gt;All with simple gemma 2 2b using ollama Takes about 2 - 10 minutes depending on the query&lt;/p&gt; &lt;p&gt;See first comment for a sample report&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/predatar"&gt; /u/predatar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/masterFoad/NanoSage"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilpkke/i_built_nanosage_a_deep_research_local_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilpkke/i_built_nanosage_a_deep_research_local_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T21:21:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1im561m</id>
    <title>Gylphstral-24B: v1 Released! (MLX)</title>
    <updated>2025-02-10T12:38:59+00:00</updated>
    <author>
      <name>/u/vesudeva</name>
      <uri>https://old.reddit.com/user/vesudeva</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay, everyone, the time is here - Glyphstral v1 is officially RELEASED!&lt;/p&gt; &lt;p&gt;Following up on my preview post from last week (&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1ikn5fg%2Fglyphstral24b_symbolic_deductive_reasoning_model%2F"&gt;link to original Reddit post here&lt;/a&gt;), I've finally got the repo all setup and the first version of Glyphstral-24b is now live on Hugging Face: &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fhuggingface.co%2FSeverian%2FGlyphstral-24b-v1"&gt;https://huggingface.co/Severian/Glyphstral-24b-v1&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;As you know, I've been diving deep into symbolic AI and really trying to see if we can push LLMs to be better at actual reasoning and multi-dimensional thought. Glyphstral is the result of that deep dive, trained to work with my &amp;quot;Glyph Code Logic Flow&amp;quot; framework. It's all about getting models to use structured, deductive symbolic logic, which you can read all about over here: &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Fseverian42%2FComputational-Model-for-Symbolic-Representations%2Ftree%2Fmain"&gt;https://github.com/severian42/Computational-Model-for-Symbolic-Representations/tree/main&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I have been very low on time so I haven't been able to make the GGUF's, as I know most of you will need those instead of the MLX version, so apologies for the delay. &lt;/p&gt; &lt;p&gt;A benchmark is also in the works! I honestly just didn't feel like holding off on the release so that some people could start testing it right away. More updates coming this week, just think of this as a soft launch.&lt;/p&gt; &lt;p&gt;This is very much a first step, and there's definitely tons more to do, but I'm genuinely excited about where this is heading. Check out the Hugging Face repo, give it a spin, and let me know what you think! Docs and more info are up there too. &lt;/p&gt; &lt;p&gt;Huge thanks for all the initial interest and encouragement on the first post. Let's see what Glyphstral can do. &lt;/p&gt; &lt;p&gt;Tell me if it works well, tell me if it sucks. All feedback is welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vesudeva"&gt; /u/vesudeva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im561m/gylphstral24b_v1_released_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im561m/gylphstral24b_v1_released_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im561m/gylphstral24b_v1_released_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T12:38:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1im4d3y</id>
    <title>How is it that Google's Gemini Pro 2.0 Experimental 02-05 Tops the LLM Arena Charts, but seems to perform badly in real world testing?</title>
    <updated>2025-02-10T11:51:55+00:00</updated>
    <author>
      <name>/u/RMCPhoto</name>
      <uri>https://old.reddit.com/user/RMCPhoto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm curious if anyone can shed some light on the recently released Gemini Pro 2.0 model's performance on LLM Arena vs real world experimentation.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard"&gt;https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have tried Gemini Pro 2.0 for many tasks and found that it hallucinated more than any other SOTA model. This was coding tasks, basic logic tasks, tasks where it presumed that it had search results when it did not and just made up information. Other tasks where it did not have the information in the model and instead provided completely made up data.&lt;/p&gt; &lt;p&gt;I understand that LLM arena does not require this sort of validation, but I worry that the confidence with which it provides incorrect answers is polluting the responses.&lt;/p&gt; &lt;p&gt;Even in Coding on LLMA, 2.0 pro experimental seemingly tops the charts, yet in any basic testing it is nowhere close to claude, which simply provides better code solutions with fewer errors.&lt;/p&gt; &lt;p&gt;The 95% CLI is +15/-13, which is quite high meaning that certainty of the score has not been established, but still, has anyone found it to be reliable?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RMCPhoto"&gt; /u/RMCPhoto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im4d3y/how_is_it_that_googles_gemini_pro_20_experimental/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im4d3y/how_is_it_that_googles_gemini_pro_20_experimental/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im4d3y/how_is_it_that_googles_gemini_pro_20_experimental/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T11:51:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1im0qlx</id>
    <title>I found out today that deepseek already had their own alphageometry model which they also realized open source, and nobody seemed to talk about it? They used lean4 and reinforcement learning to make models learn how to prove theorems, this was a 7b model however.</title>
    <updated>2025-02-10T07:32:14+00:00</updated>
    <author>
      <name>/u/Sudden-Lingonberry-8</name>
      <uri>https://old.reddit.com/user/Sudden-Lingonberry-8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im0qlx/i_found_out_today_that_deepseek_already_had_their/"&gt; &lt;img alt="I found out today that deepseek already had their own alphageometry model which they also realized open source, and nobody seemed to talk about it? They used lean4 and reinforcement learning to make models learn how to prove theorems, this was a 7b model however." src="https://external-preview.redd.it/T3eZL03WWFsyqSUvySEnk1tSTyP6Z6NoRcxQhBfEagA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ca261b0e40f7df500ec6c66b94ca2519d9d6805" title="I found out today that deepseek already had their own alphageometry model which they also realized open source, and nobody seemed to talk about it? They used lean4 and reinforcement learning to make models learn how to prove theorems, this was a 7b model however." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sudden-Lingonberry-8"&gt; /u/Sudden-Lingonberry-8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://bdtechtalks.com/2024/06/03/deepseek-prover/?noamp=mobile"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im0qlx/i_found_out_today_that_deepseek_already_had_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im0qlx/i_found_out_today_that_deepseek_already_had_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T07:32:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilsd9g</id>
    <title>Deepseek‚Äôs AI model is ‚Äòthe best work‚Äô out of China but the hype is 'exaggerated,' Google Deepmind CEO says. ‚ÄúDespite the hype, there‚Äôs no actual new scientific advance.‚Äù</title>
    <updated>2025-02-09T23:25:46+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilsd9g/deepseeks_ai_model_is_the_best_work_out_of_china/"&gt; &lt;img alt="Deepseek‚Äôs AI model is ‚Äòthe best work‚Äô out of China but the hype is 'exaggerated,' Google Deepmind CEO says. ‚ÄúDespite the hype, there‚Äôs no actual new scientific advance.‚Äù" src="https://external-preview.redd.it/D5cH9r3dkBcSJGjeKeJ46CPgSVrjJ9bdZXxiph4I2fA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d50f50547706f5f05c2fe9a32fd3a8706a7eabf" title="Deepseek‚Äôs AI model is ‚Äòthe best work‚Äô out of China but the hype is 'exaggerated,' Google Deepmind CEO says. ‚ÄúDespite the hype, there‚Äôs no actual new scientific advance.‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/02/09/deepseeks-ai-model-the-best-work-out-of-china-google-deepmind-ceo.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilsd9g/deepseeks_ai_model_is_the_best_work_out_of_china/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilsd9g/deepseeks_ai_model_is_the_best_work_out_of_china/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T23:25:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1im24eg</id>
    <title>super-lightweight local chat ui: aiaio</title>
    <updated>2025-02-10T09:17:59+00:00</updated>
    <author>
      <name>/u/abhi1thakur</name>
      <uri>https://old.reddit.com/user/abhi1thakur</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im24eg/superlightweight_local_chat_ui_aiaio/"&gt; &lt;img alt="super-lightweight local chat ui: aiaio" src="https://external-preview.redd.it/a3Y4M202ajk0YWllMYF_wLtPRaZvIjUu74nHzfLN9YPkgJJ1_RAlJtFmiyzb.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a5aae413453bdad7e387e9bc23c767d6884ef001" title="super-lightweight local chat ui: aiaio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abhi1thakur"&gt; /u/abhi1thakur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1btqc6j94aie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im24eg/superlightweight_local_chat_ui_aiaio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im24eg/superlightweight_local_chat_ui_aiaio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T09:17:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilsfb1</id>
    <title>TL;DR of Andrej Karpathy‚Äôs Latest Deep Dive on LLMs</title>
    <updated>2025-02-09T23:28:38+00:00</updated>
    <author>
      <name>/u/i_am_exception</name>
      <uri>https://old.reddit.com/user/i_am_exception</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Andrej Karpathy just dropped a &lt;strong&gt;3-hour, 31-minute&lt;/strong&gt; deep dive on LLMs like ChatGPT‚Äî&lt;strong&gt;a goldmine of information&lt;/strong&gt;. I watched the whole thing, took notes, and turned them into an article that summarizes the key takeaways in &lt;strong&gt;just 15 minutes&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;If you don‚Äôt have time to watch the full video, this breakdown covers everything you need. &lt;strong&gt;That said, if you can, watch the entire thing‚Äîit‚Äôs absolutely worth it.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;üëâ &lt;strong&gt;Read the full summary here&lt;/strong&gt;: &lt;a href="https://anfalmushtaq.com/articles/deep-dive-into-llms-like-chatgpt-tldr"&gt;https://anfalmushtaq.com/articles/deep-dive-into-llms-like-chatgpt-tldr&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i_am_exception"&gt; /u/i_am_exception &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilsfb1/tldr_of_andrej_karpathys_latest_deep_dive_on_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilsfb1/tldr_of_andrej_karpathys_latest_deep_dive_on_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilsfb1/tldr_of_andrej_karpathys_latest_deep_dive_on_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T23:28:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilw2p9</id>
    <title>Talk me out of buying this 512GB/s Gen 5 NVMe RAID card + 4 drives to try to run 1.58bit DeepSeek-R1:671b on (in place of more RAM)</title>
    <updated>2025-02-10T02:49:33+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilw2p9/talk_me_out_of_buying_this_512gbs_gen_5_nvme_raid/"&gt; &lt;img alt="Talk me out of buying this 512GB/s Gen 5 NVMe RAID card + 4 drives to try to run 1.58bit DeepSeek-R1:671b on (in place of more RAM)" src="https://preview.redd.it/7nmoiav078ie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcd54168953e1da724e565f76e2a9f5de82e49ba" title="Talk me out of buying this 512GB/s Gen 5 NVMe RAID card + 4 drives to try to run 1.58bit DeepSeek-R1:671b on (in place of more RAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know it‚Äôs probably a dumb idea, but the theoretical bandwidth of 512GB per second using a PCIE Gen 5 RAID seems appealing when you stuff it full of Gen 5 NVME drives.&lt;/p&gt; &lt;p&gt;For reference, I‚Äôm running a AERO TRX50 motherboard with a Threadripper 7960 with 64GB DDR5 and a 3090 (borrowed). &lt;/p&gt; &lt;p&gt;I know VRAM is the best option, followed by system RAM, but would this 4 channel RAID running at 512GB/s with the fastest drives I could find have any hope of running an offloaded 1.58 bit DeepSeek-R1 model at like maybe 2 tokens per second?&lt;/p&gt; &lt;p&gt;Like I said, please talk me out of it if it‚Äôs going to be a waste of money vs. just buying more DDR5&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7nmoiav078ie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilw2p9/talk_me_out_of_buying_this_512gbs_gen_5_nvme_raid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilw2p9/talk_me_out_of_buying_this_512gbs_gen_5_nvme_raid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T02:49:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1im35yl</id>
    <title>How to scale RAG to 20 million documents ?</title>
    <updated>2025-02-10T10:33:39+00:00</updated>
    <author>
      <name>/u/Sarcinismo</name>
      <uri>https://old.reddit.com/user/Sarcinismo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;Curious to hear if you worked on RAG use cases with 20+ million documents and how you handled such scale from latency, embedding and indexing perspectives.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sarcinismo"&gt; /u/Sarcinismo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im35yl/how_to_scale_rag_to_20_million_documents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im35yl/how_to_scale_rag_to_20_million_documents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im35yl/how_to_scale_rag_to_20_million_documents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T10:33:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1im141p</id>
    <title>Orange Pi AI Studio Pro mini PC with 408GB/s bandwidth</title>
    <updated>2025-02-10T08:00:39+00:00</updated>
    <author>
      <name>/u/michaeljchou</name>
      <uri>https://old.reddit.com/user/michaeljchou</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im141p/orange_pi_ai_studio_pro_mini_pc_with_408gbs/"&gt; &lt;img alt="Orange Pi AI Studio Pro mini PC with 408GB/s bandwidth" src="https://b.thumbs.redditmedia.com/4-1ai0qkO5Xa7dtRGbvrlqdk94A166Fd8-qidSmu1eY.jpg" title="Orange Pi AI Studio Pro mini PC with 408GB/s bandwidth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/michaeljchou"&gt; /u/michaeljchou &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1im141p"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im141p/orange_pi_ai_studio_pro_mini_pc_with_408gbs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im141p/orange_pi_ai_studio_pro_mini_pc_with_408gbs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T08:00:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilzcwm</id>
    <title>671B DeepSeek-R1/V3-q4 on a Single Machine (2√ó Xeon + 24GB GPU) ‚Äì Up to 286 tokens/s Prefill &amp; 14 tokens/s Decode</title>
    <updated>2025-02-10T05:58:47+00:00</updated>
    <author>
      <name>/u/CombinationNo780</name>
      <uri>https://old.reddit.com/user/CombinationNo780</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilzcwm/671b_deepseekr1v3q4_on_a_single_machine_2_xeon/"&gt; &lt;img alt="671B DeepSeek-R1/V3-q4 on a Single Machine (2√ó Xeon + 24GB GPU) ‚Äì Up to 286 tokens/s Prefill &amp;amp; 14 tokens/s Decode" src="https://external-preview.redd.it/HTxOk-pm59cMNSNVg9McK5hbABS7kz3K65hC8Z_V08I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f46113aaee323590b36c46395dcf62eb9140f297" title="671B DeepSeek-R1/V3-q4 on a Single Machine (2√ó Xeon + 24GB GPU) ‚Äì Up to 286 tokens/s Prefill &amp;amp; 14 tokens/s Decode" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, we're the KTransformers team (formerly known for &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1edbue3/local_deepseekv2_inference_120_ts_for_prefill_and/"&gt;our local CPU/GPU hybrid inference open source project with DeepSeek-V2&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;We've heard your requests for DeepSeek-R1/V3 support‚Äîand we're excited to finally deliver!&lt;/p&gt; &lt;p&gt;Apologies for the wait, but we've been cooking up something truly amazing.&lt;/p&gt; &lt;p&gt;Today, we're proud to announce that we not only support DeepSeek-R1/V3, as showcased in the video at &lt;a href="https://github.com/kvcache-ai/ktransformers"&gt;https://github.com/kvcache-ai/ktransformers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o0x777ie49ie1.jpg?width=2560&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=23cd7dad22d211277f1787bd1f993c7c22200401"&gt;https://preview.redd.it/o0x777ie49ie1.jpg?width=2560&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=23cd7dad22d211277f1787bd1f993c7c22200401&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But we're also previewing our upcoming optimizations, including an Intel AMX-accelerated kernel and a selective expert activation method, which will significantly enhance performance.&lt;/p&gt; &lt;p&gt;With v0.3-preview, &lt;strong&gt;we achieve up to 286 tokens/s for prefill, making it up to 28√ó faster than llama.cpp&lt;/strong&gt; for local inference.&lt;/p&gt; &lt;p&gt;The binary distribution is available now and the source code will come ASAP! Check out the details here: &lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md"&gt;https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some rationale behind this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Why CPU/GPU Hybrid Inference?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;DeepSeek's MLA operators are highly computationally intensive. While running everything on CPU is possible, offloading the heavy computations to the GPU results in a massive performance boost.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Where Does the Speedup Come From?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- Expert Offload: Unlike traditional layer-based or KVCache offloading (as seen in llama.cpp), we offload the expert computation to the CPU and MLA/KVCache to GPU, aligning perfectly with DeepSeek‚Äôs architecture for optimal efficiency.&lt;/p&gt; &lt;p&gt;- Intel AMX Optimization ‚Äì Our AMX-accelerated kernel is meticulously tuned, running several times faster than existing llama.cpp implementations. We plan to open-source this kernel after cleansing and are considering upstream contributions to llama.cpp.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Why Intel CPUs?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Intel is currently the only CPU vendor that supports AMX-like instructions, which delivers significantly better performance compared to AVX-only alternatives. BUT, we also support AMD CPUs and due to the Expert Offload it will also be faster than the current llama.cpp&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombinationNo780"&gt; /u/CombinationNo780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilzcwm/671b_deepseekr1v3q4_on_a_single_machine_2_xeon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilzcwm/671b_deepseekr1v3q4_on_a_single_machine_2_xeon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilzcwm/671b_deepseekr1v3q4_on_a_single_machine_2_xeon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T05:58:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1im4wxs</id>
    <title>They got the scent now..</title>
    <updated>2025-02-10T12:23:58+00:00</updated>
    <author>
      <name>/u/linkcharger</name>
      <uri>https://old.reddit.com/user/linkcharger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im4wxs/they_got_the_scent_now/"&gt; &lt;img alt="They got the scent now.." src="https://preview.redd.it/euoub08i1bie1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4309ce55a1a79e872658fda4004c32f51846e045" title="They got the scent now.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/linkcharger"&gt; /u/linkcharger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/euoub08i1bie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im4wxs/they_got_the_scent_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im4wxs/they_got_the_scent_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T12:23:58+00:00</published>
  </entry>
</feed>
