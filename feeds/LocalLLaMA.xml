<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-25T17:05:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jjfww5</id>
    <title>My personal benchmark</title>
    <updated>2025-03-25T10:07:24+00:00</updated>
    <author>
      <name>/u/olddoglearnsnewtrick</name>
      <uri>https://old.reddit.com/user/olddoglearnsnewtrick</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjfww5/my_personal_benchmark/"&gt; &lt;img alt="My personal benchmark" src="https://b.thumbs.redditmedia.com/pc4gIbezsyQH1A3_qHqSevY2dISe-6VXml6A7oD_iIg.jpg" title="My personal benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am tasked to do several tasks of knowledge extraction from Italian language news articles. The following is the comparison of several LLMs against a human curated gold set of entities:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ka58n7d1euqe1.png?width=2842&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51f820fc6d76a893970de4df3e00ca6fa06d588c"&gt;https://preview.redd.it/ka58n7d1euqe1.png?width=2842&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51f820fc6d76a893970de4df3e00ca6fa06d588c&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Overall Top Performer.&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;google/gemini‚Äê2.0‚Äêflash‚Äê001&lt;/strong&gt; achieves by far the highest F1 score (0.8638), driven by a very strong precision (0.9448).&lt;/li&gt; &lt;li&gt;It also posts a high recall (0.7957) relative to its peers, so it is excelling at both correctly identifying entities and minimizing false positives.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Precision‚ÄìRecall Trade‚Äêoffs.&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Most of the other models have lower recall, suggesting they are missing more true mentions (FN).&lt;/li&gt; &lt;li&gt;The precision‚Äìrecall balance for google/gemini‚Äê2.0‚Äêflash‚Äê001 stands out as the best overall compromise, whereas others (e.g., &lt;strong&gt;qwen/qwen2.5‚Äê32b‚Äêinstruct&lt;/strong&gt;) sacrifice quite a bit of recall for higher precision.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speed Considerations.&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;qwen/qwen2.5‚Äê32b‚Äêinstruct&lt;/strong&gt; is the fastest at &lt;strong&gt;2.86 s/article&lt;/strong&gt; but underperforms in F1 (0.6516).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;google/gemini‚Äê2.0‚Äêflash‚Äê001&lt;/strong&gt; is both highly accurate (top F1) and still quite fast at &lt;strong&gt;3.74 s/article&lt;/strong&gt;, which is among the better speeds in the table.&lt;/li&gt; &lt;li&gt;By contrast, &lt;strong&gt;qwen/qwq‚Äê32b&lt;/strong&gt; takes over &lt;strong&gt;70 s/article&lt;/strong&gt;‚Äîmuch slower‚Äîyet still only achieves an F1 of 0.7339.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Secondary Tier of Performance.&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Several models cluster around the mid‚Äêto‚Äêhigh 0.70s in F1 (e.g., &lt;strong&gt;mistralai/mistral‚Äêsmall&lt;/strong&gt;, &lt;strong&gt;meta‚Äêllama/Llama‚Äê3.3‚Äê70B&lt;/strong&gt;, &lt;strong&gt;deepseek/deepseek‚Äêchat&lt;/strong&gt;), which are respectable but noticeably lower than google/gemini‚Äê2.0‚Äôs 0.86.&lt;/li&gt; &lt;li&gt;Within this cluster, &lt;strong&gt;mistralai/mistral‚Äêsmall&lt;/strong&gt; gets slightly above 0.77 in F1, and &lt;strong&gt;meta‚Äêllama&lt;/strong&gt; is at 0.7688, indicating close but still clearly behind the leader.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;False Positives vs. False Negatives.&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Looking at the ‚ÄúFP‚Äù and ‚ÄúFN‚Äù columns shows how each model‚Äôs mistakes break down. For example: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;google/gemini‚Äê2.0&lt;/strong&gt; has only 69 FPs but 303 FNs, indicating it errs more by missing entities (as do most NER systems).&lt;/li&gt; &lt;li&gt;Models with lower recall (higher FN counts) pay the F1 penalty more sharply, as can be seen with openai/gpt‚Äê40‚Äêmini (FN=470) and qwen2.5‚Äê32b (FN=528).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Implications for Deployment.&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;If maximum accuracy is the priority, &lt;strong&gt;google/gemini‚Äê2.0‚Äêflash‚Äê001&lt;/strong&gt; is the clear choice.&lt;/li&gt; &lt;li&gt;If extremely tight inference speed is needed and some accuracy can be sacrificed, &lt;strong&gt;qwen/qwen2.5‚Äê32b&lt;/strong&gt; might be appealing.&lt;/li&gt; &lt;li&gt;For general use, models in the 0.75‚Äì0.77 F1 range represent a middle ground but do not match the best combination of speed and accuracy offered by google/gemini‚Äê2.0.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In summary, &lt;strong&gt;google/gemini‚Äê2.0‚Äêflash‚Äê001&lt;/strong&gt; stands out both for its top‚Äêtier F1 and low inference time, making it the leader in these NER evaluations. Several other models do reasonably well but either trail on accuracy, speed, or both.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/olddoglearnsnewtrick"&gt; /u/olddoglearnsnewtrick &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjfww5/my_personal_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjfww5/my_personal_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjfww5/my_personal_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T10:07:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jisuq4</id>
    <title>DeepSeek V3-0324 has caught up to Sonnet 3.7 in my code creativity benchmark - "Write a raytracer that renders an interesting scene with many colourful lightsources in python."</title>
    <updated>2025-03-24T15:06:11+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jisuq4/deepseek_v30324_has_caught_up_to_sonnet_37_in_my/"&gt; &lt;img alt="DeepSeek V3-0324 has caught up to Sonnet 3.7 in my code creativity benchmark - &amp;quot;Write a raytracer that renders an interesting scene with many colourful lightsources in python.&amp;quot;" src="https://external-preview.redd.it/UVEiW2axvGQT_-A-QrSCYJfIlHr0MZzVYvUIPeOZnEI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a8ffe19911392c13473b9c66e3fd7b50efa3454" title="DeepSeek V3-0324 has caught up to Sonnet 3.7 in my code creativity benchmark - &amp;quot;Write a raytracer that renders an interesting scene with many colourful lightsources in python.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A while ago I set up a code creativity benchmark by asking various LLMs a very simple prompt:&lt;/p&gt; &lt;p&gt;&amp;gt; &lt;code&gt;Write a raytracer that renders an interesting scene with many colourful lightsources in python. Output a 800x600 image as a png&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I only allowed one shot, no iterative prompting to solve broken code. What is interesting is that most LLMs generated code that created a very simple scene with a red, green and blue sphere, often also not aligned properly. Assumingly, the simple RGB example is something that is often represented in pretraining data.&lt;/p&gt; &lt;p&gt;Yet, somehow Sonnet 3.5 and especially Sonnet 3.7 created programs that generated more complex and varied scenes, using nicer colors. At the same time the filesize also increased. Anthropic had found some way to get the model to increase the creativity in coding and create more asthetic outcomes - no idea how to measure this other than looking at the images. (Speculation about how they did it and more ideas how to measure this are welcome in the comments)&lt;/p&gt; &lt;p&gt;Today I tested DeepSeek V3 0324 and it has definitely caught up to 3.7, a huge improvement over V3!&lt;/p&gt; &lt;p&gt;Benchmark data and more information &lt;a href="https://github.com/cpldcpu/llmbenchmark/blob/master/raytracer/Readme.md"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4nsm9rbaknqe1.png?width=1293&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=da86b326644f473a6814fc444b1d4b67a17941ee"&gt;Variance test where every LLM is prompted 4 times&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9acauhqcknqe1.png?width=1302&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dcbebb2bfb5ce33f53e31e9e1c0facb013a5b9a8"&gt;Summary of all tested LLMs&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jisuq4/deepseek_v30324_has_caught_up_to_sonnet_37_in_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jisuq4/deepseek_v30324_has_caught_up_to_sonnet_37_in_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jisuq4/deepseek_v30324_has_caught_up_to_sonnet_37_in_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T15:06:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jip611</id>
    <title>Deepseek releases new V3 checkpoint (V3-0324)</title>
    <updated>2025-03-24T12:12:24+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jip611/deepseek_releases_new_v3_checkpoint_v30324/"&gt; &lt;img alt="Deepseek releases new V3 checkpoint (V3-0324)" src="https://external-preview.redd.it/L_MDAztp6gi49dQUv9vk2IeXw1OjSoBT_ooENnggvOg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94d961b0b48a76bd398ef8e9a387f6a5087e577d" title="Deepseek releases new V3 checkpoint (V3-0324)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-0324"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jip611/deepseek_releases_new_v3_checkpoint_v30324/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jip611/deepseek_releases_new_v3_checkpoint_v30324/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T12:12:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jj11ls</id>
    <title>Misguided Attention Eval - DeepSeek V3-0324 significantly improved over V3 to become best non-reasoning model</title>
    <updated>2025-03-24T20:29:48+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jj11ls/misguided_attention_eval_deepseek_v30324/"&gt; &lt;img alt="Misguided Attention Eval - DeepSeek V3-0324 significantly improved over V3 to become best non-reasoning model" src="https://external-preview.redd.it/hCI99i_TREo1DsPSTlaYs3damdb5lZgT6asEOzNq1xk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=36a13f3b4390ccbdfc8cb861939ea3a10c7fb6ed" title="Misguided Attention Eval - DeepSeek V3-0324 significantly improved over V3 to become best non-reasoning model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The original DeepSeek V3 did not perform that well on the Misguided Attention eval, however the update scaled up the ranks to be the best non-reasoning model, ahead of Sonnet-3.7 (non-thinking). &lt;/p&gt; &lt;p&gt;It's quite astonishing that it is solving some prompts that were previously only solved by reasoning models (e.g. jugs 4 liters). It seems that V3-0324 has learned to detect reasoning loops and break out of them. This is a capability that also many reasoning models lack. It is not clear whether there has been data contamination or this is a general ability. I will post some examples in the comments.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/39yfh9f64pqe1.png?width=1205&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=511425a7b173b6bdd87c0473febc3ca1941d47cb"&gt;https://preview.redd.it/39yfh9f64pqe1.png?width=1205&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=511425a7b173b6bdd87c0473febc3ca1941d47cb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2421xqtl4pqe1.png?width=4170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=58c7aec9277a736ccfe4163be86c565c7ab74857"&gt;Darker = higher number of correct responses for that specific prompt.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/cpldcpu/MisguidedAttention"&gt;Misguided Attention&lt;/a&gt; is a collection of prompts to challenge the reasoning abilities of large language models in presence of misguiding information.&lt;/p&gt; &lt;p&gt;Thanks to numerous community contributions I was able to to increase the number of prompts to 52. Thanks a lot to all contributors! More contributions are always valuable to fight saturation of the benchmark.&lt;/p&gt; &lt;p&gt;In addition, I improved the automatic evaluation so that fewer manual interventions ware required.&lt;/p&gt; &lt;p&gt;Below, you can see the first results from the long dataset evaluation - more will be added over time. R1 took the lead here and we can also see the impressive improvement that finetuning llama-3.3 with deepseek traces brought. I expect that o1 would beat r1 based on the results from the small eval. Currently no o1 long eval is planned due to excessive API costs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jj11ls/misguided_attention_eval_deepseek_v30324/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jj11ls/misguided_attention_eval_deepseek_v30324/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jj11ls/misguided_attention_eval_deepseek_v30324/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T20:29:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjmog5</id>
    <title>pre-trainined small MoE model from scratch, but why its good?</title>
    <updated>2025-03-25T15:54:17+00:00</updated>
    <author>
      <name>/u/V1rgin_</name>
      <uri>https://old.reddit.com/user/V1rgin_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share my experience pre-training a small MoE model from scratch. I have created a tutorial with code and checkpoints if you would be interested (with a beautiful explanation of RoPE, btw):&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@bogdan.su/in-this-article-we-will-build-our-llm-which-i-called-lightlm-from-scratch-choose-the-optimal-c1e1839668db"&gt;https://medium.com/@bogdan.su/in-this-article-we-will-build-our-llm-which-i-called-lightlm-from-scratch-choose-the-optimal-c1e1839668db&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd like to tell you about a little find:&lt;br /&gt; In brief, I trained 1 MoE model that uses 100% of active parameters (2 routed experts and 1 shared expert) and 2 default-Transformer models (with different number of parameters for Attention and FFN) and it was surprising to me that the MoE model performed better and more stable than the other two. I was sure it shouldn't work that way, but the MoE model was better even using only half of the training dataset.&lt;br /&gt; I was 100% sure that a larger number of dimensions in the hidden layers of FFN should show a better result than distributing ‚Äúknowledge‚Äù among experts. Apparently this is not the case(?)&lt;/p&gt; &lt;p&gt;If you have some intuitive/mathematical explanation for this, I'd really like to read it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/V1rgin_"&gt; /u/V1rgin_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjmog5/pretrainined_small_moe_model_from_scratch_but_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjmog5/pretrainined_small_moe_model_from_scratch_but_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjmog5/pretrainined_small_moe_model_from_scratch_but_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T15:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjenu4</id>
    <title>Arc-AGI-2 new benchmark</title>
    <updated>2025-03-25T08:30:43+00:00</updated>
    <author>
      <name>/u/tim_Andromeda</name>
      <uri>https://old.reddit.com/user/tim_Andromeda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjenu4/arcagi2_new_benchmark/"&gt; &lt;img alt="Arc-AGI-2 new benchmark" src="https://external-preview.redd.it/EgcNoTp8CXkUREswLOJFaRCpJzlzLh4JgeugjVVm_00.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12fe1cebb1de52a5010be00250423f821bd40d0a" title="Arc-AGI-2 new benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is great. A lot of thought was put into how to measure AGI. A thing that confuses me, there‚Äôs a training data set. Seeing as this was just released, I assume models have not ingested the public training data yet (is that how it works?) o3 (not mini) scored nearly 80% on ARC-AGI-1, but used an exorbitant amount of compute. Arc2 aims to control for this. Efficiency is considered. We could hypothetically build a system that uses all the compute in the world and solves these, but what would that really prove?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tim_Andromeda"&gt; /u/tim_Andromeda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arcprize.org/blog/announcing-arc-agi-2-and-arc-prize-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjenu4/arcagi2_new_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjenu4/arcagi2_new_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T08:30:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjaall</id>
    <title>One shot website (DeepSeek V3.1)</title>
    <updated>2025-03-25T03:30:57+00:00</updated>
    <author>
      <name>/u/AlgorithmicKing</name>
      <uri>https://old.reddit.com/user/AlgorithmicKing</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjaall/one_shot_website_deepseek_v31/"&gt; &lt;img alt="One shot website (DeepSeek V3.1)" src="https://b.thumbs.redditmedia.com/R6zgrxZkRNRgKaT4490xfHc8TelVjx6-Zdu1BIxxatk.jpg" title="One shot website (DeepSeek V3.1)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1jjaall/video/pn6ffizc9rqe1/player"&gt;https://reddit.com/link/1jjaall/video/pn6ffizc9rqe1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Wanted to compare it to claude 3.7 but....&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xs17iopf9rqe1.png?width=395&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b49874b1b6362c4ba2ed4e7f8cfaedd6a00d71ae"&gt;https://preview.redd.it/xs17iopf9rqe1.png?width=395&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b49874b1b6362c4ba2ed4e7f8cfaedd6a00d71ae&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Prompt:&lt;/p&gt; &lt;p&gt;create a homepage for a branding agency and make sure to add 100% of your creativity in it (I mean it: particles gradients, glows vfx etc.) in html&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlgorithmicKing"&gt; /u/AlgorithmicKing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjaall/one_shot_website_deepseek_v31/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjaall/one_shot_website_deepseek_v31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjaall/one_shot_website_deepseek_v31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T03:30:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjk13v</id>
    <title>Qwen2.5-VL-32B-Instruct EXL2</title>
    <updated>2025-03-25T14:00:37+00:00</updated>
    <author>
      <name>/u/TheActualStudy</name>
      <uri>https://old.reddit.com/user/TheActualStudy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been experimenting with getting Qwen2.5-VL-32B-Instruct working in TabbyAPI and wanted to share some changes to its config files that seem to be working. Specifically, adding in vision and rope config from the 7B version and adjusting the image_processor_type name in preprocessor_config.json. Quant available &lt;a href="https://huggingface.co/christopherthompson81/Qwen2.5-VL-32B-Instruct-exl2-4_25bpw"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I'm using my own interface, &lt;a href="https://github.com/christopherthompson81/tabbyUI"&gt;TabbyUI&lt;/a&gt;, to test. If you try it out, I would appreciate any feedback (issues, PRs, comments). Hopefully the vision model changes work for others.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheActualStudy"&gt; /u/TheActualStudy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjk13v/qwen25vl32binstruct_exl2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjk13v/qwen25vl32binstruct_exl2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjk13v/qwen25vl32binstruct_exl2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T14:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjlk7c</id>
    <title>Build Your Own AI Memory ‚Äì Tutorial For Dummies</title>
    <updated>2025-03-25T15:07:50+00:00</updated>
    <author>
      <name>/u/Willing-Site-8137</name>
      <uri>https://old.reddit.com/user/Willing-Site-8137</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! I just published a quick, beginner friendly tutorial showing how to build an AI memory system from scratch. It walks through:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Short-term vs. long-term memory&lt;/li&gt; &lt;li&gt;How to store and retrieve older chats&lt;/li&gt; &lt;li&gt;A minimal implementation with a simple self-loop you can test yourself&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No fancy jargon or complex abstractions‚Äîjust a friendly explanation with sample code using &lt;a href="https://github.com/The-Pocket/PocketFlow"&gt;PocketFlow&lt;/a&gt;, a 100-line framework. If you‚Äôve ever wondered how a chatbot remembers details, check it out!&lt;/p&gt; &lt;p&gt;&lt;a href="https://zacharyhuang.substack.com/p/build-ai-agent-memory-from-scratch"&gt;https://zacharyhuang.substack.com/p/build-ai-agent-memory-from-scratch&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Willing-Site-8137"&gt; /u/Willing-Site-8137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjlk7c/build_your_own_ai_memory_tutorial_for_dummies/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjlk7c/build_your_own_ai_memory_tutorial_for_dummies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjlk7c/build_your_own_ai_memory_tutorial_for_dummies/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T15:07:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjl8x6</id>
    <title>Tired of seeing evaluations using pygame/three.js</title>
    <updated>2025-03-25T14:54:16+00:00</updated>
    <author>
      <name>/u/Hv_V</name>
      <uri>https://old.reddit.com/user/Hv_V</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everytime a new model gets released people quickly start posting results of rotating-hexagon-bouncing ball, platformer games, 3d three.js environments, minecraft clones and shit. This all looks cool but currently pygame and three.js are not industry standard to make full real world production grade software . I think better evaluation can be to let llms create scripts for unreal/unity engine to generate game logic, assets, textures etc procedurally using add-ons, create frontends in web and native SDKs(react native, flutter, kotlin, swift etc), backend code(node js), database schemas(sql or nosql) and audit the results to see if the results are industry standard or not in terms of performance, security and functionality. We need people to priorities real world benchmarks like SWE and HLE.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hv_V"&gt; /u/Hv_V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl8x6/tired_of_seeing_evaluations_using_pygamethreejs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl8x6/tired_of_seeing_evaluations_using_pygamethreejs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl8x6/tired_of_seeing_evaluations_using_pygamethreejs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T14:54:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jj9d5c</id>
    <title>Change log of DeepSeek-V3-0324</title>
    <updated>2025-03-25T02:40:34+00:00</updated>
    <author>
      <name>/u/OedoSoldier</name>
      <uri>https://old.reddit.com/user/OedoSoldier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jj9d5c/change_log_of_deepseekv30324/"&gt; &lt;img alt="Change log of DeepSeek-V3-0324" src="https://external-preview.redd.it/AO2sAF0_c_2mBe6UautksfrJRPPX3sFbs0Fu0kPn0C0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa0a8cd368da789c05b75a810cf0a1e21413b8f2" title="Change log of DeepSeek-V3-0324" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/75pe0hzi0rqe1.png?width=1283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=48b2d3b67c5b490814a18c0ab7f6e8a7bba71841"&gt;https://preview.redd.it/75pe0hzi0rqe1.png?width=1283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=48b2d3b67c5b490814a18c0ab7f6e8a7bba71841&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://api-docs.deepseek.com/updates"&gt;https://api-docs.deepseek.com/updates&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OedoSoldier"&gt; /u/OedoSoldier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jj9d5c/change_log_of_deepseekv30324/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jj9d5c/change_log_of_deepseekv30324/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jj9d5c/change_log_of_deepseekv30324/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T02:40:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjl45h</id>
    <title>Compared performance of vLLM vs SGLang on 2 Nvidia GPUs - SGLang crushes it with Data Parallelism</title>
    <updated>2025-03-25T14:48:25+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl45h/compared_performance_of_vllm_vs_sglang_on_2/"&gt; &lt;img alt="Compared performance of vLLM vs SGLang on 2 Nvidia GPUs - SGLang crushes it with Data Parallelism" src="https://a.thumbs.redditmedia.com/_IlnldFF_EGoXSBruEJQcoCcpBzX8koRU3W3RBstwu4.jpg" title="Compared performance of vLLM vs SGLang on 2 Nvidia GPUs - SGLang crushes it with Data Parallelism" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wrapped up a head-to-head benchmark of vLLM and SGLang on a 2x Nvidia GPU setup, and the results were pretty telling.&lt;/p&gt; &lt;p&gt;Running SGLang with data parallelism (--dp 2) yielded ~150% more requests and tokens generated compared to vLLM using tensor parallelism (--tensor-parallel-size 2). Not entirely surprising, given the architectural differences between data and tensor parallelism, but nice to see it quantified.&lt;/p&gt; &lt;p&gt;SGLang:&lt;/p&gt; &lt;p&gt;&lt;code&gt; ============ Serving Benchmark Result ============ Successful requests: 10000 Benchmark duration (s): 640.00 Total input tokens: 10240000 Total generated tokens: 1255483 Request throughput (req/s): 15.63 Output token throughput (tok/s): 1961.70 Total Token throughput (tok/s): 17961.80 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;vLLM:&lt;/p&gt; &lt;p&gt;&lt;code&gt; ============ Serving Benchmark Result ============ Successful requests: 10000 Benchmark duration (s): 1628.80 Total input tokens: 10240000 Total generated tokens: 1254908 Request throughput (req/s): 6.14 Output token throughput (tok/s): 770.45 Total Token throughput (tok/s): 7057.28 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;For anyone curious or wanting to reproduce: I‚Äôve documented the full setup and benchmark steps for both stacks. Everything is codified with Ansible for fast, reproducible testing: ‚Ä¢ SGLang: &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-SGLANG.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-SGLANG.md&lt;/a&gt; ‚Ä¢ vLLM: &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-VLLM.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-VLLM.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts or see if others have similar results across different models or GPU configs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jjl45h"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl45h/compared_performance_of_vllm_vs_sglang_on_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl45h/compared_performance_of_vllm_vs_sglang_on_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T14:48:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jj3w03</id>
    <title>New DeepSeek benchmark scores</title>
    <updated>2025-03-24T22:25:46+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jj3w03/new_deepseek_benchmark_scores/"&gt; &lt;img alt="New DeepSeek benchmark scores" src="https://preview.redd.it/smu0dyp3rpqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a68417184573d77643f9e46e4be7a04e47760398" title="New DeepSeek benchmark scores" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/smu0dyp3rpqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jj3w03/new_deepseek_benchmark_scores/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jj3w03/new_deepseek_benchmark_scores/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-24T22:25:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjddzl</id>
    <title>$150 Phi-4 Q4 server</title>
    <updated>2025-03-25T06:52:31+00:00</updated>
    <author>
      <name>/u/EuphoricPenguin22</name>
      <uri>https://old.reddit.com/user/EuphoricPenguin22</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjddzl/150_phi4_q4_server/"&gt; &lt;img alt="$150 Phi-4 Q4 server" src="https://b.thumbs.redditmedia.com/C-3gG-UQbMZQE1ffkpsUf_pMl7uj8ZLqO3JKvheB2_w.jpg" title="$150 Phi-4 Q4 server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to build a local LLM server to run smaller models away from my main 3090 rig. I didn't want to spend a lot, though, so I did some digging and caught wind of the P102-100 cards. I found one on eBay that apparently worked for $42 after shipping. This computer (i7-10700 HP prebuilt) was one we put out of service and had sitting around, so I purchased a $65 500W proprietary HP PSU and a new fans and thermal pads for the GPU for $40-ish. &lt;/p&gt; &lt;p&gt;The GPU was in pretty rough shape: it was caked in thick dust, the fans were squeaking, and the old paste was crumbling. I did my best to clean it up as shown, and I did install new fans. I'm sure my thermal pad application job leaves something to be desired. Anyway, a hacked BIOS (for 10GB VRAM) and driver later, I have a new 10GB CUDA box that can run a 8.5GB Q4 quant of Phi-4 at 10-20 tokens per second. Temps look to be sitting around 60¬∞C-70¬∞C while under load from inference. &lt;/p&gt; &lt;p&gt;My next goal is to get OpenHands running; it works great on my other machines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EuphoricPenguin22"&gt; /u/EuphoricPenguin22 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jjddzl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjddzl/150_phi4_q4_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjddzl/150_phi4_q4_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T06:52:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjhtln</id>
    <title>mOrpheus: Using Whisper STT + Orpheus TTS + Gemma 3 using LM Studio to create a virtual assistant.</title>
    <updated>2025-03-25T12:09:17+00:00</updated>
    <author>
      <name>/u/NighthawkXL</name>
      <uri>https://old.reddit.com/user/NighthawkXL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjhtln/morpheus_using_whisper_stt_orpheus_tts_gemma_3/"&gt; &lt;img alt="mOrpheus: Using Whisper STT + Orpheus TTS + Gemma 3 using LM Studio to create a virtual assistant." src="https://external-preview.redd.it/P03UNyUnfLZkVgRKVkoK6RfK1RFxjBeZhspoe9bSuRw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10722e11edbe2a033b67017ad1d4380c7abb1ab2" title="mOrpheus: Using Whisper STT + Orpheus TTS + Gemma 3 using LM Studio to create a virtual assistant." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NighthawkXL"&gt; /u/NighthawkXL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Nighthawk42/mOrpheus"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjhtln/morpheus_using_whisper_stt_orpheus_tts_gemma_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjhtln/morpheus_using_whisper_stt_orpheus_tts_gemma_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T12:09:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjdv9n</id>
    <title>DeepSeek-V3-0324 HF Model Card Updated With Benchmarks</title>
    <updated>2025-03-25T07:28:57+00:00</updated>
    <author>
      <name>/u/Few_Butterfly_4834</name>
      <uri>https://old.reddit.com/user/Few_Butterfly_4834</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-0324/blob/main/README.md"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3-0324/blob/main/README.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Butterfly_4834"&gt; /u/Few_Butterfly_4834 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjdv9n/deepseekv30324_hf_model_card_updated_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjdv9n/deepseekv30324_hf_model_card_updated_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjdv9n/deepseekv30324_hf_model_card_updated_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T07:28:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjblbt</id>
    <title>Implications for local LLM scene if Trump does a full Nvidia ban in China</title>
    <updated>2025-03-25T04:46:12+00:00</updated>
    <author>
      <name>/u/auradragon1</name>
      <uri>https://old.reddit.com/user/auradragon1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Edit: Getting downvoted. If you'd like to have interesting discussions here, upvote this post. Otherwise, I will delete this post soon and post it somewhere else.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I think this post should belong here because it's very much related to local LLMs. At this point, Chinese LLMs are by far, the biggest contributors to open source LLMs.&lt;/p&gt; &lt;p&gt;DeepSeek and Qwen, and other Chinese models are getting too good despite not having the latest Nvidia hardware. They have to use gimped Nvidia hopper GPUs with limited bandwidth. Or they're using lesser AI chips from Huawei that wasn't made using the latest TSMC node. Chinese companies have been banned from using TSMC N5, N3, and N2 nodes since late 2024. &lt;/p&gt; &lt;p&gt;I'm certain that Sam Altman, Elon, Bezos, Google founders, Zuckerberg are all lobbying Trump to do a fun Nvidia ban in China. Every single one of them showed up at Trump's inauguration and donated to his fund. This likely means not even gimped Nvidia GPUs can be sold in China. &lt;/p&gt; &lt;p&gt;US big tech companies can't get a high ROI if free/low cost Chinese LLMs are killing their profit margins.&lt;/p&gt; &lt;p&gt;When Deepseek R1 destroyed Nvidia's stock price, it wasn't because people thought the efficiency would lead to less Nvidia demand. No, it'd increase Nvidia demand. Instead, I believe Wall Street was worried that tech bros would lobby Trump to do a fun Nvidia ban in China. Tech bros have way more influence on Trump than Nvidia.&lt;/p&gt; &lt;p&gt;A full ban on Nvidia in China would benefit US tech bros in a few ways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Slow down competition from China. Blackwell US models vs gimped Hopper Chinese models in late 2025.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Easier and faster access to Nvidia's GPUs for US companies. I estimate that 30% of Nvidia's GPU sales end up in China.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Lower Nvidia GPU prices all around because of the reduced demand.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auradragon1"&gt; /u/auradragon1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjblbt/implications_for_local_llm_scene_if_trump_does_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjblbt/implications_for_local_llm_scene_if_trump_does_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjblbt/implications_for_local_llm_scene_if_trump_does_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T04:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjalaj</id>
    <title>Deepseek-v3-0324 on Aider</title>
    <updated>2025-03-25T03:47:28+00:00</updated>
    <author>
      <name>/u/Harrycognito</name>
      <uri>https://old.reddit.com/user/Harrycognito</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjalaj/deepseekv30324_on_aider/"&gt; &lt;img alt="Deepseek-v3-0324 on Aider" src="https://preview.redd.it/ssol9q8ecrqe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24a68d0ca221d1deace8a0c0efcbb4bc1ad3d0a5" title="Deepseek-v3-0324 on Aider" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Harrycognito"&gt; /u/Harrycognito &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ssol9q8ecrqe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjalaj/deepseekv30324_on_aider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjalaj/deepseekv30324_on_aider/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T03:47:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjis2u</id>
    <title>Recent models really make me think attention is all we need</title>
    <updated>2025-03-25T13:00:19+00:00</updated>
    <author>
      <name>/u/ludosudowudo</name>
      <uri>https://old.reddit.com/user/ludosudowudo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The new sonnet 3.7 and Deepseek v3 are really a step up reasoning wise from older models. A lot of people at first also agreed there seemed to be no walls left for reasoning when the inference time reinforcement learning paradigm shift happened a couple of months ago with O1. That's until very recently, when they saw how a Claude 3.7 Agent playing pokemon really childishly struggles with the game. Since then I feel like people are switching again to the opinion that a new breakthrough or architectural solution is needed to solve the better memory and context problem.&lt;/p&gt; &lt;p&gt;However, the more time I spent thinking about it, the more it feels like this context/memory problem is also a solvable problem with reinforcement learning. The problem of memory and context is not the lack of memory, these models have a huge amount of context window. It seems to be a problem related to the management of memory and context. And as we can see with the simple framework the agent playing the game is currently using to manage memory, it seems validating and summarizing context helps. In essence, the problem of memory management and orchestration seems to be climbable with reinforcement learning.&lt;/p&gt; &lt;p&gt;My prediction is that reinforcement learning on memory/context management will cause models to climb their search algorithm to spend more tokens on higher-order context management. Just like with the Deepseek &amp;quot;aha&amp;quot; moment and the &amp;lt;think&amp;gt; tokens, I predict that with reinforcement learning on agentic tasks fairly quickly a &amp;quot;reassess&amp;quot; moment will emerge and a &amp;lt;recontextualize&amp;gt; token will naturally follow. This higher-order context management, just like reasoning, is bound to already be present in the huge amount of pretraining data, and probably can be unlocked with a small reinforcement learning run with the right dataset.&lt;/p&gt; &lt;p&gt;I really think attention, scale and reinforcement learning is all we need to get to human level agent performance.&lt;/p&gt; &lt;p&gt;edit: As to what is valuable data for this kind of ability training, my guess is that the most valuable problems for this kind of climbing will be long simple hierarchical tasks where a lot of diverse subtasks each with a lot of memory/context need to be continuously juggled over long thinking process. The subtasks also need temporal dependencies between them. In essence subtask A can only be solvable up to a certain point x, after which subtask B can only be solvable to a certain point y, after which subtask A is solvable again to point z, etc. Without these temporal dependencies in the problems subtasks, reinforcement learning will optimize probably to fully solving subtask A instead of recontextualizing to subtask B during its long thinking stage.&lt;/p&gt; &lt;p&gt;edit2: A farfetched possible example is the class of problems that are better solvable with breath first search instead of depth first search.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ludosudowudo"&gt; /u/ludosudowudo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjis2u/recent_models_really_make_me_think_attention_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjis2u/recent_models_really_make_me_think_attention_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjis2u/recent_models_really_make_me_think_attention_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T13:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjl49l</id>
    <title>Qwen?! üëÄ</title>
    <updated>2025-03-25T14:48:34+00:00</updated>
    <author>
      <name>/u/Lowkey_LokiSN</name>
      <uri>https://old.reddit.com/user/Lowkey_LokiSN</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl49l/qwen/"&gt; &lt;img alt="Qwen?! üëÄ" src="https://external-preview.redd.it/pSzn5luA5bEL814Ul3JN__zTnOX5m5uc7UJYJ7zNQ_k.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa05eb3479835a6a18730e50d61faa6a62ffce2b" title="Qwen?! üëÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/gcu4thlvluqe1.png?width=1168&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bc2bbb1a4b8d74ca2d65572c25e1ed2ae19b4db"&gt;Is it what I think it is?!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This was posted as a reply shortly after Qwen2.5-VL-32B-Instruct's announcement&lt;br /&gt; &lt;a href="https://x.com/JustinLin610/status/1904231553183744020"&gt;https://x.com/JustinLin610/status/1904231553183744020&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lowkey_LokiSN"&gt; /u/Lowkey_LokiSN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl49l/qwen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl49l/qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl49l/qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T14:48:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jj6i4m</id>
    <title>Deepseek v3</title>
    <updated>2025-03-25T00:19:31+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jj6i4m/deepseek_v3/"&gt; &lt;img alt="Deepseek v3" src="https://preview.redd.it/xaic503gbqqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=616bfd3de239ef7db7a2416bc9be3a95051f9c0b" title="Deepseek v3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xaic503gbqqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jj6i4m/deepseek_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jj6i4m/deepseek_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T00:19:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jji2da</id>
    <title>DeepSeek-V3-0324 GGUF - Unsloth</title>
    <updated>2025-03-25T12:22:51+00:00</updated>
    <author>
      <name>/u/Co0k1eGal3xy</name>
      <uri>https://old.reddit.com/user/Co0k1eGal3xy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Available Formats so far;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;UD-Q2_K_XL (226.6GB)&lt;/li&gt; &lt;li&gt;Q2_K (244.0GB)&lt;/li&gt; &lt;li&gt;Q3_K_M (319.2GB)&lt;/li&gt; &lt;li&gt;UD-Q3_K_XL (320.7GB)&lt;/li&gt; &lt;li&gt;Q4_K_M (404.3GB)&lt;/li&gt; &lt;li&gt;UD-Q4_K_XL (404.9GB)&lt;/li&gt; &lt;li&gt;Q5_K_M (475.4GB)&lt;/li&gt; &lt;li&gt;Q6_K (550.5GB)&lt;/li&gt; &lt;li&gt;Q8_0 (712.9GB)&lt;/li&gt; &lt;li&gt;BF16 (1765.3GB)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jji2da/comment/mjnbeh9/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;Hey thanks for posting! We haven't finished uploading the rest but currently we're in the process of testing them.&lt;/a&gt; - u/&lt;a href="https://www.reddit.com/user/yoracale/"&gt;yoracale&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Co0k1eGal3xy"&gt; /u/Co0k1eGal3xy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jji2da/deepseekv30324_gguf_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jji2da/deepseekv30324_gguf_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jji2da/deepseekv30324_gguf_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T12:22:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjjv8k</id>
    <title>DeepSeek official communication on X: DeepSeek-V3-0324 is out now!</title>
    <updated>2025-03-25T13:53:11+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjjv8k/deepseek_official_communication_on_x/"&gt; &lt;img alt="DeepSeek official communication on X: DeepSeek-V3-0324 is out now!" src="https://b.thumbs.redditmedia.com/__aOAn3RMb1pB4WQ7nZaRtP8KJ2vjbYZROoq35jWyoc.jpg" title="DeepSeek official communication on X: DeepSeek-V3-0324 is out now!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jjjv8k"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjjv8k/deepseek_official_communication_on_x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjjv8k/deepseek_official_communication_on_x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T13:53:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjgje5</id>
    <title>We got competition</title>
    <updated>2025-03-25T10:50:03+00:00</updated>
    <author>
      <name>/u/BlueeWaater</name>
      <uri>https://old.reddit.com/user/BlueeWaater</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgje5/we_got_competition/"&gt; &lt;img alt="We got competition" src="https://preview.redd.it/bamkfj1yftqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c43810bb7e5d8ea7891aeebc79e47a801d562c8" title="We got competition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlueeWaater"&gt; /u/BlueeWaater &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bamkfj1yftqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgje5/we_got_competition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgje5/we_got_competition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T10:50:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjgi8y</id>
    <title>Deepseek V3 0324 is now the best non-reasoning model (across both open and closed source) according to Artificial Analisys.</title>
    <updated>2025-03-25T10:47:48+00:00</updated>
    <author>
      <name>/u/kristaller486</name>
      <uri>https://old.reddit.com/user/kristaller486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgi8y/deepseek_v3_0324_is_now_the_best_nonreasoning/"&gt; &lt;img alt="Deepseek V3 0324 is now the best non-reasoning model (across both open and closed source) according to Artificial Analisys." src="https://preview.redd.it/4hh6ys9gftqe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e01ab49fd276d31a93696fb2a9a9f51d5ad35c7" title="Deepseek V3 0324 is now the best non-reasoning model (across both open and closed source) according to Artificial Analisys." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kristaller486"&gt; /u/kristaller486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4hh6ys9gftqe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgi8y/deepseek_v3_0324_is_now_the_best_nonreasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgi8y/deepseek_v3_0324_is_now_the_best_nonreasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T10:47:48+00:00</published>
  </entry>
</feed>
