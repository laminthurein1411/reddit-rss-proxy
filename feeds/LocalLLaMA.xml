<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-05T12:10:56+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kf0x6n</id>
    <title>What local models are actually good at generating UI‚Äôs?</title>
    <updated>2025-05-05T02:23:55+00:00</updated>
    <author>
      <name>/u/Capable-Ad-7494</name>
      <uri>https://old.reddit.com/user/Capable-Ad-7494</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve looked into UIGEN and while it does have a good look to some examples, and it seems worst than qwen 8b oddly enough?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Capable-Ad-7494"&gt; /u/Capable-Ad-7494 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf0x6n/what_local_models_are_actually_good_at_generating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf0x6n/what_local_models_are_actually_good_at_generating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf0x6n/what_local_models_are_actually_good_at_generating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T02:23:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kemt2m</id>
    <title>Which coding model is best for 48GB VRAM</title>
    <updated>2025-05-04T15:42:29+00:00</updated>
    <author>
      <name>/u/Su1tz</name>
      <uri>https://old.reddit.com/user/Su1tz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is for data science, mostly excel data manipulation in python. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Su1tz"&gt; /u/Su1tz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T15:42:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf3od0</id>
    <title>I have spent 7+ hours trying to get WSL2 to work with Multi-GPU training - is it basically impossible on windows? lol</title>
    <updated>2025-05-05T05:05:09+00:00</updated>
    <author>
      <name>/u/RoyalCities</name>
      <uri>https://old.reddit.com/user/RoyalCities</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First time running / attempting distributed training via Windows using WSL2 and I'm getting constant issues regarding NCCL.&lt;/p&gt; &lt;p&gt;Is Linux essentially the only game in town for training if you plan on training with multiple GPUs via NVLink (and the pipeline specifically uses NCCL)?&lt;/p&gt; &lt;p&gt;Jensen was out here hyping up WSL2 in January like it was the best thing since sliced bread but I have hit a wall trying to get it to work.&lt;/p&gt; &lt;p&gt;&amp;quot;Windows WSL2...basically it's two operating systems within one - it works perfectly...&amp;quot;&lt;br /&gt; &lt;a href="https://www.youtube.com/live/k82RwXqZHY8?si=xbF7ZLrkBDI6Irzr&amp;amp;t=2940"&gt;https://www.youtube.com/live/k82RwXqZHY8?si=xbF7ZLrkBDI6Irzr&amp;amp;t=2940&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RoyalCities"&gt; /u/RoyalCities &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf3od0/i_have_spent_7_hours_trying_to_get_wsl2_to_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf3od0/i_have_spent_7_hours_trying_to_get_wsl2_to_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf3od0/i_have_spent_7_hours_trying_to_get_wsl2_to_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T05:05:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ken4uk</id>
    <title>I made a fake phone to text fake people with llamacpp</title>
    <updated>2025-05-04T15:56:28+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's useless and stupid, but also kinda fun. You create and add characters to a pretend phone, and then message them.&lt;/p&gt; &lt;p&gt;Does not work with &amp;quot;thinking&amp;quot; models as it isn't set to parse out the thinking tags.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/openconstruct/llamaphone"&gt;LLamaPhone&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ken4uk/i_made_a_fake_phone_to_text_fake_people_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ken4uk/i_made_a_fake_phone_to_text_fake_people_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ken4uk/i_made_a_fake_phone_to_text_fake_people_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T15:56:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1keyvqs</id>
    <title>Is it possible to system prompt Qwen 3 models to have "reasoning effort"?</title>
    <updated>2025-05-05T00:36:01+00:00</updated>
    <author>
      <name>/u/wunnsen</name>
      <uri>https://old.reddit.com/user/wunnsen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm wondering if I can prompt Qwen 3 models to output shorter / longer / more concise think tags.&lt;br /&gt; Has anyone attempted this yet for Qwen or a similar model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wunnsen"&gt; /u/wunnsen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keyvqs/is_it_possible_to_system_prompt_qwen_3_models_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keyvqs/is_it_possible_to_system_prompt_qwen_3_models_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1keyvqs/is_it_possible_to_system_prompt_qwen_3_models_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T00:36:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf9zxu</id>
    <title>I want to deepen my understanding and knowledge of ai.</title>
    <updated>2025-05-05T12:08:39+00:00</updated>
    <author>
      <name>/u/Fair_Mission4349</name>
      <uri>https://old.reddit.com/user/Fair_Mission4349</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am currently working as an ai full stack dev, but I want to deepen my understanding and knowledge of ai. I have mainly worked in stable diffusion and agent style chatbots, which are connected to your database. But It's mostly just prompting and using the various apis. I want to further deepen my understanding and have a widespread knowledge of ai. I have mostly done udemy courses and am self learnt ( was guided by a senior / my mentor ). Can someone suggest a path or roadmap and resources ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fair_Mission4349"&gt; /u/Fair_Mission4349 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf9zxu/i_want_to_deepen_my_understanding_and_knowledge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf9zxu/i_want_to_deepen_my_understanding_and_knowledge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf9zxu/i_want_to_deepen_my_understanding_and_knowledge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T12:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kewr1q</id>
    <title>Jetbrains Coding model</title>
    <updated>2025-05-04T22:51:43+00:00</updated>
    <author>
      <name>/u/SpeedyBrowser45</name>
      <uri>https://old.reddit.com/user/SpeedyBrowser45</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Jetbrains just released a coding model. has anyone tried it?&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/JetBrains/mellum-68120b4ae1423c86a2da007a"&gt;https://huggingface.co/collections/JetBrains/mellum-68120b4ae1423c86a2da007a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpeedyBrowser45"&gt; /u/SpeedyBrowser45 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kewr1q/jetbrains_coding_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kewr1q/jetbrains_coding_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kewr1q/jetbrains_coding_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T22:51:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf86st</id>
    <title>Whisper Transcription Workflow: Home Server vs. Android Phone? Seeking Advice!</title>
    <updated>2025-05-05T10:23:35+00:00</updated>
    <author>
      <name>/u/CtrlAltDelve</name>
      <uri>https://old.reddit.com/user/CtrlAltDelve</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been doing a lot with the Whisper models lately. I find myself making voice recordings while I'm out, and then later I use something like MacWhisper at home to transcribe them using the best available Whisper model. After that, I take the content and process it using a local LLM.&lt;/p&gt; &lt;p&gt;This workflow has been &lt;em&gt;really&lt;/em&gt; helpful for me.&lt;/p&gt; &lt;p&gt;One inconvenience is having to wait until I get home to use MacWhisper. I also prefer not to use any hosted transcription services. So, I've been considering a couple of ideas:&lt;/p&gt; &lt;p&gt;First, seeing if I can get Whisper to run properly on my Android phone (an S25 Ultra). This...is pretty involved and I'm not much of an Android developer. I've tried to do some reading on transformers.js but I think this is a little beyond my ability right now. &lt;/p&gt; &lt;p&gt;Second, having Whisper running on my home server continuously. This server is a Mac Mini M4 with 16 GB of RAM. I could set up a watch directory so that any audio file placed there gets automatically transcribed. Then, I could use something like Blip to send the files over to the server and have it automatically accept them. &lt;/p&gt; &lt;p&gt;Does anyone have any suggestions on either of these? Or any other thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CtrlAltDelve"&gt; /u/CtrlAltDelve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf86st/whisper_transcription_workflow_home_server_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf86st/whisper_transcription_workflow_home_server_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf86st/whisper_transcription_workflow_home_server_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T10:23:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf2ezy</id>
    <title>Computer-Use Model Capabilities</title>
    <updated>2025-05-05T03:47:52+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf2ezy/computeruse_model_capabilities/"&gt; &lt;img alt="Computer-Use Model Capabilities" src="https://preview.redd.it/kuxk8trzxvye1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93a6922dbf1de371175b17445c82c9c4f62314bf" title="Computer-Use Model Capabilities" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.trycua.com/blog/build-your-own-operator-on-macos-2#computer-use-model-capabilities"&gt;https://www.trycua.com/blog/build-your-own-operator-on-macos-2#computer-use-model-capabilities&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kuxk8trzxvye1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf2ezy/computeruse_model_capabilities/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf2ezy/computeruse_model_capabilities/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T03:47:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1keoint</id>
    <title>LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!</title>
    <updated>2025-05-04T16:55:10+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/"&gt; &lt;img alt="LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!" src="https://a.thumbs.redditmedia.com/bM9LC8PSLdBmtmFQOjBwlZthNsiYL5J4IXaOzEPqwY4.jpg" title="LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/3bwwfd4epsye1.png?width=3404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=adbb0bce2c13bc560499b0d3459329d16d0a3291"&gt;You can't go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B and 30B)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m4x5z2sposye1.png?width=3404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=26b2ff50d960dd957e86feb04a8c21030ef0195c"&gt;mainline llama.cpp just got a boost for fully offloaded Qwen3 MoE (single expert)&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;tl;dr;&lt;/h1&gt; &lt;p&gt;I highly recommend doing a &lt;code&gt;git pull&lt;/code&gt; and re-building your &lt;code&gt;ik_llama.cpp&lt;/code&gt; or &lt;code&gt;llama.cpp&lt;/code&gt; repo to take advantage of recent major performance improvements just released.&lt;/p&gt; &lt;p&gt;The friendly competition between these amazing projects is producing delicious fruit for the whole GGUF loving &lt;code&gt;r/LocalLLaMA&lt;/code&gt; community!&lt;/p&gt; &lt;p&gt;If you have enough VRAM to fully offload and already have an existing &amp;quot;normal&amp;quot; quant of Qwen3 MoE then you'll get a little more speed out of mainline llama.cpp. If you are doing hybrid CPU+GPU offload or want to take advantage of the new SotA iqN_k quants, then check out ik_llama.cpp fork!&lt;/p&gt; &lt;h1&gt;Details&lt;/h1&gt; &lt;p&gt;I spent yesterday compiling and running benhmarks on the newest versions of both &lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;ik_llama.cpp&lt;/a&gt; and mainline &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For those that don't know, ikawrakow was an early contributor to mainline llama.cpp working on important features that have since trickled down into ollama, lmstudio, koboldcpp etc. At some point (presumably for reasons beyond my understanding) the &lt;code&gt;ik_llama.cpp&lt;/code&gt; fork was built and has a number of interesting features including SotA &lt;code&gt;iqN_k&lt;/code&gt; quantizations that pack in a lot of quality for the size while retaining good speed performance. (These new quants are &lt;em&gt;not&lt;/em&gt; available in ollma, lmstudio, koboldcpp, etc.)&lt;/p&gt; &lt;p&gt;A few recent PRs made by ikawrakow to &lt;code&gt;ik_llama.cpp&lt;/code&gt; and by JohannesGaessler to mainline have &lt;em&gt;boosted performance across the board&lt;/em&gt; and especially on CUDA with Flash Attention implementations for Grouped Query Attention (GQA) models and also Mixutre of Experts (MoEs) like the recent and amazing Qwen3 235B and 30B releases!&lt;/p&gt; &lt;h1&gt;References&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp/pull/370"&gt;ikawrakow/ik_llama.cpp/pull/370&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T16:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf49i4</id>
    <title>Running Dia-1.6B TTS on My Mac with M Chip</title>
    <updated>2025-05-05T05:44:00+00:00</updated>
    <author>
      <name>/u/Own_Connection_8018</name>
      <uri>https://old.reddit.com/user/Own_Connection_8018</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf49i4/running_dia16b_tts_on_my_mac_with_m_chip/"&gt; &lt;img alt="Running Dia-1.6B TTS on My Mac with M Chip" src="https://external-preview.redd.it/aSK-pqev9BF3YhBcrUotRcsKL0gCyLQvXMU9-ld9doI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=daae9f894126ec73ec9f6849eb9c7ccce06bc5a3" title="Running Dia-1.6B TTS on My Mac with M Chip" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I made a small project to run the Dia-1.6B text-to-speech model on my Mac with an M chip. It‚Äôs a cool TTS model that makes realistic voices, supports multiple speakers, and can even do stuff like voice cloning or add emotions. I set it up as a simple server using FastAPI, and it works great on M1/M2/M3 Macs.&lt;/p&gt; &lt;p&gt;Check it out here: &lt;a href="https://github.com/zhaopengme/mac-dia-server"&gt;mac-dia-server&lt;/a&gt;. The README has easy steps to get it running with Python 3.9+. It‚Äôs not too hard to set up, and you can test it with some example commands I included.&lt;/p&gt; &lt;p&gt;Let me know what you think! If you have questions, hit me up on X at . &lt;a href="https://x.com/zhaopengme"&gt;https://x.com/zhaopengme&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own_Connection_8018"&gt; /u/Own_Connection_8018 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/zhaopengme/mac-dia-server"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf49i4/running_dia16b_tts_on_my_mac_with_m_chip/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf49i4/running_dia16b_tts_on_my_mac_with_m_chip/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T05:44:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kenk4f</id>
    <title>QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison.</title>
    <updated>2025-05-04T16:14:12+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kenk4f/qwq_32b_vs_qwen_3_32b_vs_glm432b_html_coding_only/"&gt; &lt;img alt="QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison." src="https://external-preview.redd.it/rvBe2sSMWUb2BiTsxft299oO0IRh9G0lMoWcfjP8v_w.png?width=140&amp;amp;height=70&amp;amp;crop=140:70,smart&amp;amp;auto=webp&amp;amp;s=725aced25c191e436bfd44f4619541673facf020" title="QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All models are from Bartowski - q4km version&lt;/p&gt; &lt;p&gt;Test only HTML frontend.&lt;/p&gt; &lt;p&gt;My assessment lauout quality from 0 to 10&lt;/p&gt; &lt;p&gt;Prompt&lt;/p&gt; &lt;p&gt;&amp;quot;Generate a beautiful website for Steve's pc repair using a single html script.&amp;quot;&lt;/p&gt; &lt;p&gt;QwQ 32b - 3/10&lt;/p&gt; &lt;p&gt;- poor layout but ..works , very basic&lt;/p&gt; &lt;p&gt;- 250 line of code&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6rol9pc6hsye1.png?width=2461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b"&gt;https://preview.redd.it/6rol9pc6hsye1.png?width=2461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen 3 32b - 6/10&lt;/p&gt; &lt;p&gt;- much better looks but still not too complex layout&lt;/p&gt; &lt;p&gt;- 310 lines of the code&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z9qixbh8hsye1.png?width=2461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29e6bb4b272399ba8140785feb429a196ecc5173"&gt;https://preview.redd.it/z9qixbh8hsye1.png?width=2461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29e6bb4b272399ba8140785feb429a196ecc5173&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM-4-32b 9/10&lt;/p&gt; &lt;p&gt;- looks insanely good , quality layout like sonnet 3.7 easily&lt;/p&gt; &lt;p&gt;- 1500+ code lines&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3zj2lr2ahsye1.png?width=2469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964"&gt;https://preview.redd.it/3zj2lr2ahsye1.png?width=2469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM-4-32b is insanely good for html code frontend.&lt;/p&gt; &lt;p&gt;I say that model is VERY GOOD ONLY IN THIS FIELD and JavaScript at most.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Other coding language like python , c , c++ or any other quality of the code will be on the level of qwen 2.5 32b coder, reasoning and math also is on the seme level but for html and JavaScript ... is GREAT.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kenk4f/qwq_32b_vs_qwen_3_32b_vs_glm432b_html_coding_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kenk4f/qwq_32b_vs_qwen_3_32b_vs_glm432b_html_coding_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kenk4f/qwq_32b_vs_qwen_3_32b_vs_glm432b_html_coding_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T16:14:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf76z8</id>
    <title>Fine tuning Qwen3</title>
    <updated>2025-05-05T09:16:01+00:00</updated>
    <author>
      <name>/u/Basic-Pay-9535</name>
      <uri>https://old.reddit.com/user/Basic-Pay-9535</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to finetune Qwen 3 reasoning. But I need to generate think tags for my dataset . Which model / method would u recommend best in order to create these think tags ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Basic-Pay-9535"&gt; /u/Basic-Pay-9535 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf76z8/fine_tuning_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf76z8/fine_tuning_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf76z8/fine_tuning_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T09:16:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kepuli</id>
    <title>Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)</title>
    <updated>2025-05-04T17:51:10+00:00</updated>
    <author>
      <name>/u/intofuture</name>
      <uri>https://old.reddit.com/user/intofuture</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/"&gt; &lt;img alt="Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)" src="https://external-preview.redd.it/JTz5zGA4pu0kVgU5PcVKCkISLorm6EJbAz9pvaqpZXQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b29e8ff81b9a05957e855ea181b9201334e650a3" title="Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLlama!&lt;/p&gt; &lt;p&gt;We've started publishing open-source model performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS, Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+ soon.&lt;/p&gt; &lt;p&gt;We‚Äôre doing this because perf metrics determine the viability of shipping models in apps to users (no end-user wants crashing/slow AI features that hog up their specific device).&lt;/p&gt; &lt;p&gt;Although benchmarks get posted in threads here and there, we feel like a more consolidated and standardized hub should probably exist.&lt;/p&gt; &lt;p&gt;We figured we'd kickstart this since we already maintain this benchmarking infra/tooling at RunLocal for our enterprise customers. Note: We‚Äôve mostly focused on supporting model formats like Core ML, ONNX and TFLite to date, so a few things are still WIP for GGUF support. &lt;/p&gt; &lt;p&gt;Thought it would be cool to start with benchmarks for Qwen3 (Num Prefill Tokens=512, Num Generation Tokens=128). &lt;a href="https://huggingface.co/collections/unsloth/qwen3-680edabfb790c8c34a242f95"&gt;GGUFs are from Unsloth&lt;/a&gt; üêê&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l59qu1gxysye1.png?width=961&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=381abad7b25e1d719265826441b51aa50177d143"&gt;Qwen3 GGUF benchmarks on laptops&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z5qxhpc1zsye1.png?width=913&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c48aa6c5c753c7dc74c4397aac34f92383d17afe"&gt;Qwen3 GGUF benchmarks on phones&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can see more of the benchmark data for Qwen3 &lt;a href="https://edgemeter.runlocal.ai/public/pipelines/a240f768-2847-4e06-8df9-156ea3c2c321"&gt;here&lt;/a&gt;. We realize there are so many variables (devices, backends, etc.) that interpreting the data is currently harder than it should be. We'll work on that!&lt;/p&gt; &lt;p&gt;You can also see benchmarks for a few other models &lt;a href="https://edgemeter.runlocal.ai/public/pipelines"&gt;here&lt;/a&gt;. If you want to see benchmarks for any others, feel free to request them and we‚Äôll try to publish ASAP!&lt;/p&gt; &lt;p&gt;Lastly, you can run your own benchmarks on our devices for free (limited to some degree to avoid our devices melting!).&lt;/p&gt; &lt;p&gt;This free/public version is a bit of a frankenstein fork of our enterprise product, so any benchmarks you run would be private to your account. But if there's interest, we can add a way for you to also publish them so that the public benchmarks aren‚Äôt bottlenecked by us. &lt;/p&gt; &lt;p&gt;It‚Äôs still very early days for us with this, so please let us know what would make it better/cooler for the community: &lt;a href="https://edgemeter.runlocal.ai/public/pipelines"&gt;https://edgemeter.runlocal.ai/public/pipelines&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To more on-device AI in production! üí™&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/aev5rgjazsye1.gif"&gt;https://i.redd.it/aev5rgjazsye1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/intofuture"&gt; /u/intofuture &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T17:51:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1keo3te</id>
    <title>UI-Tars-1.5 reasoning never fails to entertain me.</title>
    <updated>2025-05-04T16:37:31+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keo3te/uitars15_reasoning_never_fails_to_entertain_me/"&gt; &lt;img alt="UI-Tars-1.5 reasoning never fails to entertain me." src="https://preview.redd.it/627wnr5emsye1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b896f5165e878160c1e104137518ab1d80b3addc" title="UI-Tars-1.5 reasoning never fails to entertain me." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;7B parameter computer use agent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/627wnr5emsye1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keo3te/uitars15_reasoning_never_fails_to_entertain_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1keo3te/uitars15_reasoning_never_fails_to_entertain_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T16:37:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kewkno</id>
    <title>Qwen 30B A3B performance degradation with KV quantization</title>
    <updated>2025-05-04T22:43:11+00:00</updated>
    <author>
      <name>/u/fakezeta</name>
      <uri>https://old.reddit.com/user/fakezeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I came across this gist &lt;a href="https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4"&gt;https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4&lt;/a&gt; that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M quantization.&lt;/p&gt; &lt;p&gt;I tried to replicate locally but could I was not able, model sometimes entered in a repetition loop even with dry sampling or came to wrong conclusion after generating lots of thinking tokens.&lt;/p&gt; &lt;p&gt;I was using Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested Bartowski Q5_K_S but it had no improvement. The model didn't entered in any repetition loop but generated lots of thinking tokens without finding any solution.&lt;/p&gt; &lt;p&gt;Then I saw that sunpazed didn't used KV quantization and tried the same: boom! First time right.&lt;/p&gt; &lt;p&gt;It worked with Q5_K_S and also with Q4_K_XL&lt;/p&gt; &lt;p&gt;For who wants more details I leave here a gist &lt;a href="https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef"&gt;https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do you have any report of performance degradation with long generations on Qwen3 30B A3B and KV quantization?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fakezeta"&gt; /u/fakezeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T22:43:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1keyy4k</id>
    <title>Well, that's just, like‚Ä¶ your benchmark, man.</title>
    <updated>2025-05-05T00:39:27+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keyy4k/well_thats_just_like_your_benchmark_man/"&gt; &lt;img alt="Well, that's just, like‚Ä¶ your benchmark, man." src="https://preview.redd.it/mdy01ntgwuye1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d618c2e8de6cb32e8937776f2dfc048d10d61fc" title="Well, that's just, like‚Ä¶ your benchmark, man." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Especially as teams put AI into production, we need to start treating evaluation like a first-class discipline: versioned, interpretable, reproducible, and aligned to outcomes and improved UX.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Without some kind of ExperimentOps, you‚Äôre one false positive away from months of shipping the wrong thing.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mdy01ntgwuye1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keyy4k/well_thats_just_like_your_benchmark_man/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1keyy4k/well_thats_just_like_your_benchmark_man/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T00:39:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1keolh9</id>
    <title>Visa is looking for vibe coders - thoughts?</title>
    <updated>2025-05-04T16:58:36+00:00</updated>
    <author>
      <name>/u/eastwindtoday</name>
      <uri>https://old.reddit.com/user/eastwindtoday</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keolh9/visa_is_looking_for_vibe_coders_thoughts/"&gt; &lt;img alt="Visa is looking for vibe coders - thoughts?" src="https://preview.redd.it/gefvhv84qsye1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=235b3e1de1b7df4bd1bc1f7519f84b5259303d05" title="Visa is looking for vibe coders - thoughts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eastwindtoday"&gt; /u/eastwindtoday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gefvhv84qsye1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keolh9/visa_is_looking_for_vibe_coders_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1keolh9/visa_is_looking_for_vibe_coders_thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T16:58:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf62ck</id>
    <title>Absolute best performer for 48 Gb vram</title>
    <updated>2025-05-05T07:53:25+00:00</updated>
    <author>
      <name>/u/TacGibs</name>
      <uri>https://old.reddit.com/user/TacGibs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I was wondering if there's a better model than Deepcogito 70B (a fined-tuned thinking version of Llama 3.3 70B for those who don't know) for 48Gb vram today ?&lt;/p&gt; &lt;p&gt;I'm not talking about pure speed, just about a usable model (so no CPU/Ram offloading) with decent speed (more than 10t/s) and great knowledge.&lt;/p&gt; &lt;p&gt;Sadly it seems that the 70B size isn't a thing anymore :(&lt;/p&gt; &lt;p&gt;And yes Qwen3 32B is very nice and a bit faster, but you can feel that it's a smaller model (even if it's incredibly good for it's size).&lt;/p&gt; &lt;p&gt;Thanks !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TacGibs"&gt; /u/TacGibs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf62ck/absolute_best_performer_for_48_gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf62ck/absolute_best_performer_for_48_gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf62ck/absolute_best_performer_for_48_gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T07:53:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kezq68</id>
    <title>Speed metrics running DeepSeekV3 0324/Qwen3 235B and other models, on 128GB VRAM (5090+4090x2+A6000) + 192GB RAM on Consumer motherboard/CPU (llamacpp/ikllamacpp)</title>
    <updated>2025-05-05T01:19:58+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there guys, hope is all going good.&lt;/p&gt; &lt;p&gt;I have been testing some bigger models on this setup and wanted to share some metrics if it helps someone!&lt;/p&gt; &lt;p&gt;Setup is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 7 7800X3D&lt;/li&gt; &lt;li&gt;192GB DDR5 6000Mhz at CL30 (overclocked and adjusted resistances to make it stable)&lt;/li&gt; &lt;li&gt;RTX 5090 MSI Vanguard LE SOC, flashed to Gigabyte Aorus Master VBIOS.&lt;/li&gt; &lt;li&gt;RTX 4090 ASUS TUF, flashed to Galax HoF VBIOS.&lt;/li&gt; &lt;li&gt;RTX 4090 Gigabyte Gaming OC, flashed to Galax HoF VBIOS.&lt;/li&gt; &lt;li&gt;RTX A6000 (Ampere)&lt;/li&gt; &lt;li&gt;AM5 MSI Carbon X670E&lt;/li&gt; &lt;li&gt;Running at X8 5.0 (5090) / X8 4.0 (4090) / X4 4.0 (4090) / X4 4.0 (A6000), all from CPU lanes (using M2 to PCI-E adapters)&lt;/li&gt; &lt;li&gt;Fedora 41-42 (believe me, I tried these on Windows and multiGPU is just borked there)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The models I have tested are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DeepSeek V3 0324 at Q2_K_XL (233GB), from &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD"&gt;https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qwen3 235B at Q3_K_XL, Q4_K_L, Q6_K from &lt;a href="https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Llama-3.1-Nemotron-Ultra-253B at Q3_K_XL from &lt;a href="https://huggingface.co/unsloth/Llama-3_1-Nemotron-Ultra-253B-v1-GGUF"&gt;https://huggingface.co/unsloth/Llama-3_1-Nemotron-Ultra-253B-v1-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;c4ai-command-a-03-2025 111B at Q6_K_XL from &lt;a href="https://huggingface.co/bartowski/CohereForAI_c4ai-command-a-03-2025-GGUF"&gt;https://huggingface.co/bartowski/CohereForAI_c4ai-command-a-03-2025-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mistral-Large-Instruct-2411 123B at Q4_K_M from &lt;a href="https://huggingface.co/bartowski/Mistral-Large-Instruct-2411-GGUF"&gt;https://huggingface.co/bartowski/Mistral-Large-Instruct-2411-GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All on llamacpp, for offloading mostly on the case of bigger models. command a and Mistral Large run faster on EXL2.&lt;/p&gt; &lt;p&gt;I have also used llamacpp (&lt;a href="https://github.com/ggml-org/llama.cpp"&gt;https://github.com/ggml-org/llama.cpp&lt;/a&gt;) and ikllamacpp (&lt;a href="https://github.com/ikawrakow/ik%5C_llama.cpp"&gt;https://github.com/ikawrakow/ik\_llama.cpp&lt;/a&gt;), so I will note where I use which.&lt;/p&gt; &lt;p&gt;All of these models were loaded with 32K, without flash attention or cache quantization, except in the case of Nemotron, mostly to give some VRAM usages. FA when avaialble reduces VRAM usage with cache/buffer size heavily.&lt;/p&gt; &lt;p&gt;Also, when running -ot, I did use each layer instead of regex. This is because when using the regex I got issues with VRAM usage.&lt;/p&gt; &lt;p&gt;They were compiled from source with:&lt;/p&gt; &lt;p&gt;&lt;code&gt;CC=gcc-14 CXX=g++-14 CUDAHOSTCXX=g++-14 cmake -B build_linux \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-DGGML_CUDA=ON \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-DGGML_CUDA_FA_ALL_QUANTS=ON \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-DGGML_BLAS=OFF \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-DCMAKE_CUDA_ARCHITECTURES=&amp;quot;86;89;120&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-DCMAKE_CUDA_FLAGS=&amp;quot;-allow-unsupported-compiler -ccbin=g++-14&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;(Had to force CC and CXX 14, as CUDA doesn't support GCC15 yet, which is what Fedora ships)&lt;/p&gt; &lt;h1&gt;DeepSeek V3 0324 (Q2_K_XL, llamacpp)&lt;/h1&gt; &lt;p&gt;For this model, MLA was added recently, which let me to use more tensors on GPU.&lt;/p&gt; &lt;p&gt;Command to run it was&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m '/GGUFs/DeepSeek-V3-0324-UD-Q2_K_XL-merged.gguf' -c 32768 --no-mmap --no-warmup -ngl 999 -ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; -ot &amp;quot;blk.(7|8|9|10).ffn.=CUDA1&amp;quot; -ot &amp;quot;blk.(11|12|13|14|15).ffn.=CUDA2&amp;quot; -ot &amp;quot;blk.(16|17|18|19|20|21|22|23|24|25).ffn.=CUDA3&amp;quot; -ot &amp;quot;ffn.*=CPU&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 38919.92 ms / 1528 tokens ( 25.47 ms per token, 39.26 tokens per second)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;eval time = 57175.47 ms / 471 tokens ( 121.39 ms per token, 8.24 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This makes it pretty usable. The important part is setting the experts to be only on CPU, and active params + other experts on GPU. With MLA, it uses ~4GB for 32K and ~8GB for 64K. Without MLA, 16K uses 80GB of VRAM.&lt;/p&gt; &lt;h1&gt;Qwen3 235B (Q3_K_XL, llamacpp)&lt;/h1&gt; &lt;p&gt;For this model and size, we're able to load the model entirely on VRAM. Note: When using only GPU, on my case, llamacpp is faster than ik llamacpp.&lt;/p&gt; &lt;p&gt;Command to run it was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m '/GGUFs/Qwen3-235B-A22B-128K-UD-Q3_K_XL-00001-of-00003.gguf' -c 32768 --no-mmap --no-warmup -ngl 999 -ts 0.8,0.8,1.2,2&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 6532.37 ms / 3358 tokens ( 1.95 ms per token, 514.06 tokens per second)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;eval time = 53259.78 ms / 1359 tokens ( 39.19 ms per token, 25.52 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Pretty good model but I would try to use at least Q4_K_S/M. Cache size at 32K is 6GB, and 12GB at 64K. This cache size is the same for all Qwen3 235B quants&lt;/p&gt; &lt;h1&gt;Qwen3 235B (Q4_K_XL, llamacpp)&lt;/h1&gt; &lt;p&gt;For this model, we're using ~20GB of RAM and the rest on GPU.&lt;/p&gt; &lt;p&gt;Command to run it was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m '/GGUFs/Qwen3-235B-A22B-128K-UD-Q4_K_XL-00001-of-00003.gguf' -c 32768 --no-mmap --no-warmup -ngl 999 -ot &amp;quot;blk\.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|13)\.ffn.*=CUDA0&amp;quot; -ot &amp;quot;blk\.(14|15|16|17|18|19|20|21|22|23|24|25|26|27)\.ffn.*=CUDA1&amp;quot; -ot &amp;quot;blk\.(28|29|30|31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|)\.ffn.*=CUDA2&amp;quot; -ot &amp;quot;blk\.(47|48|49|50|51|52|53|54|55|56|57|58|59|60|61|62|63|64|65|66|67|68|69|70|71|72|73|74|75|76|77|78)\.ffn.*=CUDA3&amp;quot; -ot &amp;quot;ffn.*=CPU&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 17405.76 ms / 3358 tokens ( 5.18 ms per token, 192.92 tokens per second)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;eval time = 92420.55 ms / 1549 tokens ( 59.66 ms per token, 16.76 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Model is pretty good at this point, and speeds are still acceptable. But on this case is where ik llamacpp shines.&lt;/p&gt; &lt;h1&gt;Qwen3 235B (Q4_K_XL, ik llamacpp)&lt;/h1&gt; &lt;p&gt;ik llamacpp with some extra parameters makes the models run faster when offloading. If you're wondering why this isn't the case or I didn't post with DeepSeek V3 0324, it is because quants of main llamacpp have MLA which are incompatible with MLA from ikllamacpp, which was implemented before via another method.&lt;/p&gt; &lt;p&gt;Command to run it was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m '/GGUFs/Qwen3-235B-A22B-128K-UD-Q4_K_XL-00001-of-00003.gguf' -c 32768 --no-mmap --no-warmup -ngl 999 -ot &amp;quot;blk\.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|13)\.ffn.*=CUDA0&amp;quot; -ot &amp;quot;blk\.(14|15|16|17|18|19|20|21|22|23|24|25|26|27)\.ffn.*=CUDA1&amp;quot; -ot &amp;quot;blk\.(28|29|30|31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|)\.ffn.*=CUDA2&amp;quot; -ot &amp;quot;blk\.(47|48|49|50|51|52|53|54|55|56|57|58|59|60|61|62|63|64|65|66|67|68|69|70|71|72|73|74|75|76|77|78)\.ffn.*=CUDA3&amp;quot; -ot &amp;quot;ffn.*=CPU&amp;quot; -fmoe -amb 1024 -rtr&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;INFO [ print_timings] prompt eval time = 15739.89 ms / 3358 tokens ( 4.69 ms per token, 213.34 tokens per second) | tid=&amp;quot;140438394236928&amp;quot; ti&lt;/code&gt;&lt;br /&gt; &lt;code&gt;mestamp=1746406901 id_slot=0 id_task=0 t_prompt_processing=15739.888 n_prompt_tokens_processed=3358 t_token=4.687280524121501 n_tokens_second=213.34332239212884&lt;/code&gt;&lt;br /&gt; &lt;code&gt;INFO [ print_timings] generation eval time = 66275.69 ms / 1067 runs ( 62.11 ms per token, 16.10 tokens per second) | tid=&amp;quot;140438394236928&amp;quot; ti&lt;/code&gt;&lt;br /&gt; &lt;code&gt;mestamp=1746406901 id_slot=0 id_task=0 t_token_generation=66275.693 n_decoded=1067 t_token=62.11405154639175 n_tokens_second=16.099416719791975&lt;/code&gt;&lt;/p&gt; &lt;p&gt;So basically 10% more speed in PP and similar generation t/s.&lt;/p&gt; &lt;h1&gt;Qwen3 235B (Q6_K, llamacpp)&lt;/h1&gt; &lt;p&gt;This is the point where models are really close to Q8 and then to F16. This was more for test porpouses, but still is very usable.&lt;/p&gt; &lt;p&gt;This uses about 70GB RAM and rest on VRAM.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Command to run was:&lt;/code&gt;&lt;br /&gt; &lt;code&gt;./llama-server -m '/models_llm/Qwen3-235B-A22B-128K-Q6_K-00001-of-00004.gguf' -c 32768 --no-mmap --no-warmup -ngl 999 -ot &amp;quot;blk\.(0|1|2|3|4|5|6|7|8)\.ffn.*=CUDA0&amp;quot; -ot &amp;quot;blk\.(9|10|11|12|13|14|15|16|17)\.ffn.*=CUDA1&amp;quot; -ot &amp;quot;blk\.(18|19|20|21|22|23|24|25|26|27|28|29|30)\.ffn.*=CUDA2&amp;quot; -ot &amp;quot;blk\.(31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|47|48|49|50|51|52)\.ffn.*=CUDA3&amp;quot; -ot &amp;quot;ffn.*=CPU&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speed are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 57152.69 ms / 3877 tokens ( 14.74 ms per token, 67.84 tokens per second) eval time = 38705.90 ms / 318 tokens ( 121.72 ms per token, 8.22 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;h1&gt;Qwen3 235B (Q6_K, ik llamacpp)&lt;/h1&gt; &lt;p&gt;ik llamacpp makes a huge increase in PP performance.&lt;/p&gt; &lt;p&gt;Command to run was:&lt;/p&gt; &lt;p&gt;./llama-server -m '/models_llm/Qwen3-235B-A22B-128K-Q6_K-00001-of-00004.gguf' -c 32768 --no-mmap --no-warmup -ngl 999 -ot &amp;quot;blk\.(0|1|2|3|4|5|6|7|8)\.ffn.*=CUDA0&amp;quot; -ot &amp;quot;blk\.(9|10|11|12|13|14|15|16|17)\.ffn.*=CUDA1&amp;quot; -ot &amp;quot;blk\.(18|19|20|21|22|23|24|25|26|27|28|29|30)\.ffn.*=CUDA2&amp;quot; -ot &amp;quot;blk\.(31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|47|48|49|50|51|52)\.ffn.*=CUDA3&amp;quot; -ot &amp;quot;ffn.*=CPU&amp;quot; -fmoe -amb 512 -rtr&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;INFO [ print_timings] prompt eval time = 36897.66 ms / 3877 tokens ( 9.52 ms per token, 105.07 tokens per second) | tid=&amp;quot;140095757803520&amp;quot; timestamp=1746307138 id_slot=0 id_task=0 t_prompt_processing=36897.659 n_prompt_tokens_processed=3877 t_token=9.517064482847562 n_tokens_second=105.07441678075024&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;INFO [ print_timings] generation eval time = 143560.31 ms / 1197 runs ( 119.93 ms per token, 8.34 tokens per second) | tid=&amp;quot;140095757803520&amp;quot; timestamp=1746307138 id_slot=0 id_task=0 t_token_generation=143560.31 n_decoded=1197 t_token=119.93342522974102 n_tokens_second=8.337959147622348&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Basically 40-50% more PP performance and similar generation speed.&lt;/p&gt; &lt;h1&gt;Llama 3.1 Nemotron 253B (Q3_K_XL, llamacpp)&lt;/h1&gt; &lt;p&gt;This model was PAINFUL to make it work fully on GPU, as layers are uneven. Some layers near the end are 8B each.&lt;/p&gt; &lt;p&gt;This is also the only model I had to use CTK8/CTV4, else it doesn't fit. &lt;/p&gt; &lt;p&gt;The commands to run it were:&lt;/p&gt; &lt;p&gt;&lt;code&gt;export CUDA_VISIBLE_DEVICES=0,1,3,2&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m /run/media/pancho/08329F4A329F3B9E/models_llm/Llama-3_1-Nemotron-Ultra-253B-v1-UD-Q3_K_XL-00001-of-00003.gguf -c 32768 -ngl 163 -ts 6.5,6,10,4 --no-warmup -fa -ctk q8_0 -ctv q4_0 -mg 2 --prio 3&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I don't have the specific speeds at the moment (as to run this model I have to close any application of my desktop), but they are, from a picture I got some days ago:&lt;/p&gt; &lt;p&gt;&lt;code&gt;PP: 130 t/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Generation speed: 7.5 t/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Cache size is 5GB for 32K and 10GB for 64K.&lt;/p&gt; &lt;h1&gt;c4ai-command-a-03-2025 111B (Q6_K, llamacpp)&lt;/h1&gt; &lt;p&gt;I particullay have liked command a models, and I also feel this model is great. Ran on GPU only.&lt;/p&gt; &lt;p&gt;Command to run it was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m '/GGUFs/CohereForAI_c4ai-command-a-03-2025-Q6_K-merged.gguf' -c 32768 -ngl 99 -ts 10,11,17,20 --no-warmup&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 4101.94 ms / 3403 tokens ( 1.21 ms per token, 829.61 tokens per second)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;eval time = 46452.40 ms / 472 tokens ( 98.42 ms per token, 10.16 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For reference: EXL2 with the same quant size gets ~12 t/s.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Cache size is 8GB for 32K and 16GB for 64K.&lt;/p&gt; &lt;h1&gt;Mistral Large 2411 123B (Q4_K_M, llamacpp)&lt;/h1&gt; &lt;p&gt;Also have been a fan of Mistral Large models, as they work pretty good!&lt;/p&gt; &lt;p&gt;Command to run it was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m '/run/media/pancho/DE1652041651DDD9/HuggingFaceModelDownload&lt;/code&gt;&lt;br /&gt; &lt;code&gt;er/Storage/GGUFs/Mistral-Large-Instruct-2411-Q4_K_M-merged.gguf' -c 32768 -ngl 99 -ts 7,7,10,5 --no-warmup&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 4427.90 ms / 3956 tokens ( 1.12 ms per token, 893.43 tokens per second)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;eval time = 30739.23 ms / 387 tokens ( 79.43 ms per token, 12.59 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Cache size is quite big, 12GB for 32K and 24GB for 64K. In fact it is so big that if I want to load it on 3 GPUs (since size is 68GB) I need to use flash attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For reference: EXL2 with this same size gets 25 t/s with Tensor Parallel enabled. And 16-20 t/s on 6.5bpw EXL2 (EXL2 lets you to use TP with uneven VRAM)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;That's all the tests I have been running lately! I have been testing for both coding (python, C, C++) and RP. Not sure if you guys are interested in which one I prefer for each task or rank them.&lt;/p&gt; &lt;p&gt;Any question is welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T01:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf5lq4</id>
    <title>Does the Pareto principle apply to MoE models in practice?</title>
    <updated>2025-05-05T07:18:48+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf5lq4/does_the_pareto_principle_apply_to_moe_models_in/"&gt; &lt;img alt="Does the Pareto principle apply to MoE models in practice?" src="https://preview.redd.it/meqwqkomzwye1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=90b7b7164d1e5b4f5e2f7e1fe2d3294442e22401" title="Does the Pareto principle apply to MoE models in practice?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pareto Effect: In practice, a small number of experts (e.g., 2 or 3) may end up handling a majority of the traffic for many types of inputs. This aligns with the Pareto observation that a small set of experts could be responsible for most of the work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/meqwqkomzwye1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf5lq4/does_the_pareto_principle_apply_to_moe_models_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf5lq4/does_the_pareto_principle_apply_to_moe_models_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T07:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf1yg9</id>
    <title>Qwen3-32B-IQ4_XS GGUFs - MMLU-PRO benchmark comparison</title>
    <updated>2025-05-05T03:21:49+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/"&gt; &lt;img alt="Qwen3-32B-IQ4_XS GGUFs - MMLU-PRO benchmark comparison" src="https://external-preview.redd.it/jJ4wm0NIfgUy0MSOkw2YI6r-EjpVW_Y_SPR-xICfNk4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bf4c693cb7ebd3ae7a7b3eb2dc65cfbfc6e1d6d" title="Qwen3-32B-IQ4_XS GGUFs - MMLU-PRO benchmark comparison" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since IQ4_XS is my favorite quant for 32B models, I decided to run some benchmarks to compare IQ4_XS GGUFs from different sources.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MMLU-PRO 0.25 subset(3003 questions), 0 temp, No Think, IQ4_XS, Q8 KV Cache&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The entire benchmark took &lt;strong&gt;&lt;em&gt;11 hours, 37 minutes, and 30 seconds.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9ptc0cl2svye1.png?width=2475&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06a3b551fba60a33877f8e67af9932e381a15cc6"&gt;https://preview.redd.it/9ptc0cl2svye1.png?width=2475&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06a3b551fba60a33877f8e67af9932e381a15cc6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The difference is apparently minimum, so just keep using whatever iq4 quant you already downloaded. &lt;/p&gt; &lt;p&gt;&lt;em&gt;The official MMLU-PRO leaderboard is listing the score of Qwen3 base model instead of instruct, that's why these iq4 quants score higher than the one on MMLU-PRO leaderboard.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;gguf source:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-32B-GGUF/blob/main/Qwen3-32B-IQ4_XS.gguf"&gt;https://huggingface.co/unsloth/Qwen3-32B-GGUF/blob/main/Qwen3-32B-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-32B-128K-GGUF/blob/main/Qwen3-32B-128K-IQ4_XS.gguf"&gt;https://huggingface.co/unsloth/Qwen3-32B-128K-GGUF/blob/main/Qwen3-32B-128K-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/Qwen_Qwen3-32B-GGUF/blob/main/Qwen_Qwen3-32B-IQ4_XS.gguf"&gt;https://huggingface.co/bartowski/Qwen_Qwen3-32B-GGUF/blob/main/Qwen_Qwen3-32B-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/Qwen3-32B-i1-GGUF/blob/main/Qwen3-32B.i1-IQ4_XS.gguf"&gt;https://huggingface.co/mradermacher/Qwen3-32B-i1-GGUF/blob/main/Qwen3-32B.i1-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T03:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf9i52</id>
    <title>RTX 5060 Ti 16GB sucks for gaming, but seems like a diamond in the rough for AI</title>
    <updated>2025-05-05T11:42:02+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf9i52/rtx_5060_ti_16gb_sucks_for_gaming_but_seems_like/"&gt; &lt;img alt="RTX 5060 Ti 16GB sucks for gaming, but seems like a diamond in the rough for AI" src="https://a.thumbs.redditmedia.com/ZP_Pe9inInMfd_-rxiw6xfzYHLp5iOazS6Ztzjzk5U4.jpg" title="RTX 5060 Ti 16GB sucks for gaming, but seems like a diamond in the rough for AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I recently grabbed an RTX 5060 Ti 16GB for ‚Äújust‚Äù $499 - while it‚Äôs no one‚Äôs first choice for gaming (reviews are pretty harsh), for AI workloads? This card might be a hidden gem.&lt;/p&gt; &lt;p&gt;I mainly wanted those 16GB of VRAM to fit bigger models, and it actually worked out. Ran LightRAG to ingest this beefy PDF: &lt;a href="https://www.fiscal.treasury.gov/files/reports-statements/financial-report/2024/executive-summary-2024.pdf"&gt;https://www.fiscal.treasury.gov/files/reports-statements/financial-report/2024/executive-summary-2024.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Compared it with a 12GB GPU (RTX 3060 Ti 12GB) - and I‚Äôve attached Grafana charts showing GPU utilization for both runs.&lt;/p&gt; &lt;p&gt;üü¢ 16GB card: finished in 3 min 29 sec (green line) üü° 12GB card: took 8 min 52 sec (yellow line)&lt;/p&gt; &lt;p&gt;Logs showed the 16GB card could load all 41 layers, while the 12GB one only managed 31. The rest had to be constantly swapped in and out - crushing performance by 2x and leading to underutilizing the GPU (as clearly seen in the Grafana metrics).&lt;/p&gt; &lt;p&gt;LightRAG uses ‚ÄúMistral Nemo Instruct 12B‚Äù, served via Ollama, if you‚Äôre curious.&lt;/p&gt; &lt;p&gt;TL;DR: 16GB+ VRAM saves serious time.&lt;/p&gt; &lt;p&gt;Bonus: the card is noticeably shorter than others ‚Äî it has 2 coolers instead of the usual 3, thanks to using PCIe x8 instead of x16. Great for small form factor builds or neat home AI setups. I‚Äôm planning one myself (please share yours if you‚Äôre building something similar!).&lt;/p&gt; &lt;p&gt;And yep - I had written a full guide earlier on how to go from clean bare metal to fully functional LightRAG setup in minutes. Fully automated, just follow the steps: üëâ &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-LightRAG.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-LightRAG.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you try this setup or run into issues - happy to help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kf9i52"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf9i52/rtx_5060_ti_16gb_sucks_for_gaming_but_seems_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf9i52/rtx_5060_ti_16gb_sucks_for_gaming_but_seems_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T11:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kexdgy</id>
    <title>What do I test out / run first?</title>
    <updated>2025-05-04T23:21:02+00:00</updated>
    <author>
      <name>/u/Recurrents</name>
      <uri>https://old.reddit.com/user/Recurrents</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kexdgy/what_do_i_test_out_run_first/"&gt; &lt;img alt="What do I test out / run first?" src="https://external-preview.redd.it/Gj8FzKNPTvVSOxJwgeuufUJzmZ6BR-6YWri04zLtxfs.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=99bf755df82e7e9bb2bc2cafc9271bdc27217ed4" title="What do I test out / run first?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got her in the mail. Haven't had a chance to put her in yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recurrents"&gt; /u/Recurrents &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kexdgy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kexdgy/what_do_i_test_out_run_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kexdgy/what_do_i_test_out_run_first/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T23:21:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf5ry6</id>
    <title>JOSIEFIED Qwen3 8B is amazing! Uncensored, Useful, and great personality.</title>
    <updated>2025-05-05T07:31:25+00:00</updated>
    <author>
      <name>/u/My_Unbiased_Opinion</name>
      <uri>https://old.reddit.com/user/My_Unbiased_Opinion</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf5ry6/josiefied_qwen3_8b_is_amazing_uncensored_useful/"&gt; &lt;img alt="JOSIEFIED Qwen3 8B is amazing! Uncensored, Useful, and great personality." src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="JOSIEFIED Qwen3 8B is amazing! Uncensored, Useful, and great personality." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Primary link is for Ollama but here is the creator's model card on HF:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1"&gt;https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just wanna say this model has replaced my older Abliterated models. I genuinely think this Josie model is better than the stock model. It adhears to instructions better and is not dry in its responses at all. Running at Q8 myself and it definitely punches above its weight class. Using it primarily in a online RAG system. &lt;/p&gt; &lt;p&gt;Hoping for a 30B A3B Josie finetune in the future! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/My_Unbiased_Opinion"&gt; /u/My_Unbiased_Opinion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/goekdenizguelmez/JOSIEFIED-Qwen3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf5ry6/josiefied_qwen3_8b_is_amazing_uncensored_useful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf5ry6/josiefied_qwen3_8b_is_amazing_uncensored_useful/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T07:31:25+00:00</published>
  </entry>
</feed>
