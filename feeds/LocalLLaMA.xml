<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-25T23:23:37+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k6zn5h</id>
    <title>New reasoning benchmark got released. Gemini is SOTA, but what's going on with Qwen?</title>
    <updated>2025-04-24T18:31:34+00:00</updated>
    <author>
      <name>/u/Additional-Hour6038</name>
      <uri>https://old.reddit.com/user/Additional-Hour6038</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6zn5h/new_reasoning_benchmark_got_released_gemini_is/"&gt; &lt;img alt="New reasoning benchmark got released. Gemini is SOTA, but what's going on with Qwen?" src="https://preview.redd.it/a6awqhrhmtwe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a0c258afc7e096b062e3e8afff59d5e57504b75" title="New reasoning benchmark got released. Gemini is SOTA, but what's going on with Qwen?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No benchmaxxing on this one! &lt;a href="http://alphaxiv.org/abs/2504.16074"&gt;http://alphaxiv.org/abs/2504.16074&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Additional-Hour6038"&gt; /u/Additional-Hour6038 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a6awqhrhmtwe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6zn5h/new_reasoning_benchmark_got_released_gemini_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6zn5h/new_reasoning_benchmark_got_released_gemini_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T18:31:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7pei4</id>
    <title>MarOS a simple UI wrapper for ollama to easily chat with models on a local network</title>
    <updated>2025-04-25T16:35:45+00:00</updated>
    <author>
      <name>/u/Radiant_Dog1937</name>
      <uri>https://old.reddit.com/user/Radiant_Dog1937</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7pei4/maros_a_simple_ui_wrapper_for_ollama_to_easily/"&gt; &lt;img alt="MarOS a simple UI wrapper for ollama to easily chat with models on a local network" src="https://b.thumbs.redditmedia.com/7vA_0bo7zdX884HdbzCSeIYNAAd-7-KSV475xQu04pk.jpg" title="MarOS a simple UI wrapper for ollama to easily chat with models on a local network" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is &lt;a href="https://chatgames.itch.io/maros-ai-chat"&gt;MarOs&lt;/a&gt;, the current UI I'm using for my chat models. It has straightforward features, save/load chats, create custom system prompts and profiles, and easy model selection from your library of ollama models. Its UI is meant to be phone friendly so you can use any device on your local network to chat.&lt;/p&gt; &lt;p&gt;It works with ollama so a very small number of concurrent users should work with responses being queued, depending on your hardware of course.&lt;/p&gt; &lt;p&gt;It also automatically handles images, switching between an image and text model when you provide an image. &lt;/p&gt; &lt;p&gt;The UI space is crowded, so here's another one. &lt;a href="https://chatgames.itch.io/maros-ai-chat"&gt;MarOs AI Chat by ChatGames&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Radiant_Dog1937"&gt; /u/Radiant_Dog1937 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k7pei4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7pei4/maros_a_simple_ui_wrapper_for_ollama_to_easily/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7pei4/maros_a_simple_ui_wrapper_for_ollama_to_easily/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T16:35:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7x201</id>
    <title>Cheapest build for 4 x PCI 3.0 and 1TB RAM?</title>
    <updated>2025-04-25T21:56:36+00:00</updated>
    <author>
      <name>/u/wawawawatikkatikkati</name>
      <uri>https://old.reddit.com/user/wawawawatikkatikkati</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are the best options here? I am considering buying 4 x 3090 with power limited to 250w each, on a mobo with up to 1TB RAM, for running deepseek in memory, stable diffusion flux, and whatever else... having this setup seems possibly achievable financially and the power draw should be below 1600w - any suggestions? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wawawawatikkatikkati"&gt; /u/wawawawatikkatikkati &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7x201/cheapest_build_for_4_x_pci_30_and_1tb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7x201/cheapest_build_for_4_x_pci_30_and_1tb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7x201/cheapest_build_for_4_x_pci_30_and_1tb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T21:56:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1k76ztc</id>
    <title>I built a free, local open-source alternative to lovable/v0/bolt... now supporting local models!</title>
    <updated>2025-04-24T23:48:13+00:00</updated>
    <author>
      <name>/u/wwwillchen</name>
      <uri>https://old.reddit.com/user/wwwillchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k76ztc/i_built_a_free_local_opensource_alternative_to/"&gt; &lt;img alt="I built a free, local open-source alternative to lovable/v0/bolt... now supporting local models!" src="https://external-preview.redd.it/ZWg3ODc5bHFjdndlMdAf36ezY_hex0Hwu237_4wVe3-ifn3RUf3HJXpttA9U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=510833b73d6e9075fc9b3ec4be11e243ba72c9e2" title="I built a free, local open-source alternative to lovable/v0/bolt... now supporting local models!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi localLlama&lt;/p&gt; &lt;p&gt;Iâ€™m excited to share an early release of &lt;a href="http://dyad.sh/"&gt;&lt;strong&gt;Dyad&lt;/strong&gt;&lt;/a&gt; â€” a free, local, open-source AI app builder. It's designed as an alternative to v0, Lovable, and Bolt, but without the lock-in or limitations.&lt;/p&gt; &lt;p&gt;Hereâ€™s what makes Dyad different:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Runs locally&lt;/strong&gt; - Dyad runs entirely on your computer, making it fast and frictionless. Because your code lives locally, you can easily switch back and forth between Dyad and your IDE like Cursor, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Run local models&lt;/strong&gt; - I've just added &lt;a href="https://www.dyad.sh/docs/guides/ai-models/local-models"&gt;Ollama integration&lt;/a&gt;, letting you build with your favorite local LLMs!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Free&lt;/strong&gt; - Dyad is free and bring-your-own API key. This means you can use your free Gemini API key and get 25 free messages/day with Gemini Pro 2.5!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can download it &lt;a href="http://dyad.sh/"&gt;here&lt;/a&gt;. Itâ€™s totally free and works on Mac &amp;amp; Windows.&lt;/p&gt; &lt;p&gt;Iâ€™d love your feedback. Feel free to comment here or join &lt;a href="https://www.reddit.com/r/dyadbuilders/"&gt;r/dyadbuilders&lt;/a&gt; â€” Iâ€™m building based on community input!&lt;/p&gt; &lt;p&gt;P.S. I &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jpa1ep/i_got_tired_of_guessing_what_blackbox_ai_coding/"&gt;shared&lt;/a&gt; an earlier version a few weeks back - appreciate everyone's feedback, based on that I rewrote Dyad and made it much simpler to use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wwwillchen"&gt; /u/wwwillchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/krhz58lqcvwe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k76ztc/i_built_a_free_local_opensource_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k76ztc/i_built_a_free_local_opensource_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T23:48:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7ictf</id>
    <title>olmOCR-7B-faithful by TNG, a fine-tuned version of olmOCR-7B-0225-preview</title>
    <updated>2025-04-25T11:14:56+00:00</updated>
    <author>
      <name>/u/hdmcndog</name>
      <uri>https://old.reddit.com/user/hdmcndog</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7ictf/olmocr7bfaithful_by_tng_a_finetuned_version_of/"&gt; &lt;img alt="olmOCR-7B-faithful by TNG, a fine-tuned version of olmOCR-7B-0225-preview" src="https://external-preview.redd.it/4RZYg5749xg-1AffmqgsNSXXr7Iuj60lXffvU2kpcPo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c7c740e1d09161f2481a17c5befe38bca7b30de" title="olmOCR-7B-faithful by TNG, a fine-tuned version of olmOCR-7B-0225-preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A fine-tuned version of olmOCR-7B-0225-preview that aims to extract &lt;em&gt;all&lt;/em&gt; information from documents, including header and footer information.&lt;/p&gt; &lt;p&gt;Release article: &lt;a href="https://huggingface.co/blog/tngtech/finetuning-olmocr-to-be-a-faithful-ocr-engine"&gt;https://huggingface.co/blog/tngtech/finetuning-olmocr-to-be-a-faithful-ocr-engine&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hdmcndog"&gt; /u/hdmcndog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tngtech/olmOCR-7B-faithful"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7ictf/olmocr7bfaithful_by_tng_a_finetuned_version_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7ictf/olmocr7bfaithful_by_tng_a_finetuned_version_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T11:14:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7seqn</id>
    <title>Whatâ€™s Meta hinting at with this cryptic post? We need Bindy to decode this for us:</title>
    <updated>2025-04-25T18:38:23+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7seqn/whats_meta_hinting_at_with_this_cryptic_post_we/"&gt; &lt;img alt="Whatâ€™s Meta hinting at with this cryptic post? We need Bindy to decode this for us:" src="https://preview.redd.it/w1t0tdarz0xe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c52aab51f1bacd76acfaa9ceb42eb78619be9fdf" title="Whatâ€™s Meta hinting at with this cryptic post? We need Bindy to decode this for us:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w1t0tdarz0xe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7seqn/whats_meta_hinting_at_with_this_cryptic_post_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7seqn/whats_meta_hinting_at_with_this_cryptic_post_we/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T18:38:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7j2h5</id>
    <title>Modular have come a long way in just 3 years</title>
    <updated>2025-04-25T11:55:06+00:00</updated>
    <author>
      <name>/u/Cane_P</name>
      <uri>https://old.reddit.com/user/Cane_P</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In their latest presentation, they talk about how they now have support for CPU (x86 &amp;amp; ARM since 2023) and NVIDIA &amp;amp; AMD GPU's (I believe that it is currently optimized for A100, H100 &amp;amp; MI300X. There might be more, but those are the models that I have seen mentioned).&lt;/p&gt; &lt;p&gt;They have already open sourced some of their code and will soon release ~250k lines of GPU kernel code, and we will soon get to know how the Python operability is getting along to.&lt;/p&gt; &lt;p&gt;They have a new simpler license for Mojo and MAX.&lt;/p&gt; &lt;p&gt;Presentation (unfortunately bad audio): &lt;a href="https://www.youtube.com/live/uul6hZ5NXC8"&gt;https://www.youtube.com/live/uul6hZ5NXC8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Article from EE Times: &lt;a href="https://www.eetimes.com/after-three-years-modulars-cuda-alternative-is-ready/"&gt;https://www.eetimes.com/after-three-years-modulars-cuda-alternative-is-ready/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cane_P"&gt; /u/Cane_P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7j2h5/modular_have_come_a_long_way_in_just_3_years/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7j2h5/modular_have_come_a_long_way_in_just_3_years/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7j2h5/modular_have_come_a_long_way_in_just_3_years/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T11:55:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7sxko</id>
    <title>Latest ExecuTorch release includes windows support, packages for iOS and Android and a number of new models</title>
    <updated>2025-04-25T19:00:26+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/pytorch/executorch"&gt;ExecuTorch&lt;/a&gt; still appears to have the best performance on mobile and todays release comes with drop in packages for &lt;a href="https://pytorch.org/executorch/main/using-executorch-ios.html#integration"&gt;iOS&lt;/a&gt; and &lt;a href="https://pytorch.org/executorch/0.6/using-executorch-android.html#using-aar-from-maven-central"&gt;Android&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Also includes Ph14, Qwen 2.5 and SmolLm2&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7sxko/latest_executorch_release_includes_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7sxko/latest_executorch_release_includes_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7sxko/latest_executorch_release_includes_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T19:00:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7stfg</id>
    <title>I built a debugging MCP server that saves me ~2 programming hours a day</title>
    <updated>2025-04-25T18:55:29+00:00</updated>
    <author>
      <name>/u/klawisnotwashed</name>
      <uri>https://old.reddit.com/user/klawisnotwashed</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7stfg/i_built_a_debugging_mcp_server_that_saves_me_2/"&gt; &lt;img alt="I built a debugging MCP server that saves me ~2 programming hours a day" src="https://external-preview.redd.it/O7iRdpSAOTXAp8ugLZRXApZZDYusuN_fEGs3eP-yzAo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7b677b4629df76504b2adac01e7283245681d54" title="I built a debugging MCP server that saves me ~2 programming hours a day" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Deebo is an agentic debugging system wrapped in an MCP server, so it acts as a copilot for your coding agent.&lt;/p&gt; &lt;p&gt;Think of your main coding agent as a single threaded process. Deebo introduces multi threadedness to AI-assisted coding. You can have your agent delegate tricky bugs, context heavy tasks, validate theories, run simulations, etc.&lt;/p&gt; &lt;p&gt;The cool thing is the agents inside the deebo mcp server USE mcp themselves! They use git and file system MCP tools in order to actually read and edit code. They also do their work in separate git branches which provides natural process isolation. &lt;/p&gt; &lt;p&gt;Deebo scales to production codebases, too. I took on a tinygrad bug bounty with me + Cline + Deebo with no previous experience with the tinygrad codebase. Deebo spawned 17 scenario agents over multiple OODA loops, and synthesized 2 valid fixes! You can read the &lt;a href="https://github.com/snagasuri/deebo-prototype/tree/master/memory-bank/9bd38e9840d3/sessions/session-1744006973678"&gt;session logs here&lt;/a&gt; and see &lt;a href="https://github.com/snagasuri/deebo-prototype/blob/master/memory-bank/9bd38e9840d3/progress.md"&gt;the final fix here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If youâ€™ve ever gotten frustrated with your coding agent for looping endlessly on a seemingly simple task, you can install Deebo with a one line npx &lt;a href="mailto:deebo-setup@latest"&gt;deebo-setup@latest&lt;/a&gt;. The code is fully open source! Take a look at the code! &lt;a href="https://github.com/snagasuri/deebo-prototype"&gt;https://github.com/snagasuri/deebo-prototype&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I came up with all the system design, implementation, etc. myself so if anyone wants to chat about how Deebo works/has any questions I'd love to talk! Would highly appreciate your guys feedback! Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klawisnotwashed"&gt; /u/klawisnotwashed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/snagasuri/deebo-prototype"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7stfg/i_built_a_debugging_mcp_server_that_saves_me_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7stfg/i_built_a_debugging_mcp_server_that_saves_me_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T18:55:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7ue47</id>
    <title>Trained the tiny stories dataset on a 12M parameter model.</title>
    <updated>2025-04-25T20:02:05+00:00</updated>
    <author>
      <name>/u/Slaghton</name>
      <uri>https://old.reddit.com/user/Slaghton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7ue47/trained_the_tiny_stories_dataset_on_a_12m/"&gt; &lt;img alt="Trained the tiny stories dataset on a 12M parameter model." src="https://preview.redd.it/qnx9gqc671xe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fdcb4160b1d2416d1a24263a7cd97dc785946e9f" title="Trained the tiny stories dataset on a 12M parameter model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trained a 12M Parameter model on the tiny stories dataset. &lt;/p&gt; &lt;p&gt;**GPU used is an Nvidia 4080** &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/roneneldan/TinyStories"&gt;https://huggingface.co/datasets/roneneldan/TinyStories&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I played some video games while it was running off and on so it probably would've finished a bit earlier around 45 hours or so. &lt;/p&gt; &lt;p&gt;I think for smaller models, if you go past the Chinchilla Scaling Law of using 20 tokens per parameter, you can see improvements. This becomes less and less as the model is scaled up though I believe. &lt;/p&gt; &lt;p&gt;(Though maybe bigger models would actually benefit to but the compute becomes ridiculous and gains might be much lower than smaller models)&lt;/p&gt; &lt;p&gt;P.S. The stories aren't the best (lol), but they are pretty coherent. &lt;/p&gt; &lt;p&gt;Configuration info below.&lt;/p&gt; &lt;p&gt;config = LlamaConfig(&lt;/p&gt; &lt;p&gt;vocab_size=vocab_size,&lt;/p&gt; &lt;p&gt;hidden_size=384,&lt;/p&gt; &lt;p&gt;intermediate_size=768, &lt;/p&gt; &lt;p&gt;num_hidden_layers=8, &lt;/p&gt; &lt;p&gt;num_attention_heads=8, &lt;/p&gt; &lt;p&gt;max_position_embeddings=6000,&lt;/p&gt; &lt;p&gt;rms_norm_eps=1e-5,&lt;/p&gt; &lt;p&gt;initializer_range=0.02,&lt;/p&gt; &lt;p&gt;use_cache=True,&lt;/p&gt; &lt;p&gt;tie_word_embeddings=False,&lt;/p&gt; &lt;p&gt;attention_dropout=0.1,&lt;/p&gt; &lt;p&gt;hidden_dropout=0.1,&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;training_args = TrainingArguments(&lt;/p&gt; &lt;p&gt;output_dir=output_dir,&lt;/p&gt; &lt;p&gt;overwrite_output_dir=False,&lt;/p&gt; &lt;p&gt;num_train_epochs=1, &lt;/p&gt; &lt;p&gt;per_device_train_batch_size=8,&lt;/p&gt; &lt;p&gt;gradient_accumulation_steps=1,&lt;/p&gt; &lt;p&gt;save_strategy=&amp;quot;steps&amp;quot;, # Use steps for saving&lt;/p&gt; &lt;p&gt;save_steps=5000,&lt;/p&gt; &lt;p&gt;logging_strategy=&amp;quot;steps&amp;quot;, # Use steps for logging&lt;/p&gt; &lt;p&gt;logging_steps=100, # Log training loss frequently for the scheduler&lt;/p&gt; &lt;p&gt;save_total_limit=10,&lt;/p&gt; &lt;p&gt;prediction_loss_only=True, # Often True for Causal LM if not evaluating metrics like perplexity&lt;/p&gt; &lt;p&gt;learning_rate=.0008, # Initial learning rate for AdamW&lt;/p&gt; &lt;p&gt;weight_decay=.05,&lt;/p&gt; &lt;p&gt;fp16=True,&lt;/p&gt; &lt;p&gt;gradient_checkpointing=True,&lt;/p&gt; &lt;p&gt;max_grad_norm=1.0,&lt;/p&gt; &lt;p&gt;# Evaluation settings (important if using eval_loss with scheduler later)&lt;/p&gt; &lt;p&gt;evaluation_strategy=&amp;quot;steps&amp;quot; if not disable_eval else &amp;quot;no&amp;quot;,&lt;/p&gt; &lt;p&gt;eval_steps=5000 if not disable_eval else None,&lt;/p&gt; &lt;p&gt;report_to=&amp;quot;wandb&amp;quot;, # Log to W&amp;amp;B&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;Training stats below.&lt;/p&gt; &lt;p&gt;{'train_runtime': 180146.524, 'train_samples_per_second': 35.091, 'train_steps_per_second': 4.386, 'train_loss': 0.23441845736255604, 'epoch': 3.0}&lt;/p&gt; &lt;p&gt;100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 790191/790191 [50:02:26&amp;lt;00:00, 4.39it/s]&lt;/p&gt; &lt;p&gt;2025-04-25 13:32:42,894 - INFO - Saving final model and training state...&lt;/p&gt; &lt;p&gt;***** train metrics *****&lt;/p&gt; &lt;p&gt;epoch = 3.0&lt;/p&gt; &lt;p&gt;total_flos = 711039651GF&lt;/p&gt; &lt;p&gt;train_loss = 0.2344&lt;/p&gt; &lt;p&gt;train_runtime = 2 days, 2:02:26.52&lt;/p&gt; &lt;p&gt;train_samples_per_second = 35.091&lt;/p&gt; &lt;p&gt;train_steps_per_second = 4.386&lt;/p&gt; &lt;p&gt;2025-04-25 13:32:43,067 - INFO - Training completed successfully!&lt;/p&gt; &lt;p&gt;2025-04-25 13:32:43,068 - INFO - Final model saved to: ./llama_model_test\final&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;wandb: Run summary:&lt;/p&gt; &lt;p&gt;wandb: eval/loss 0.19124&lt;/p&gt; &lt;p&gt;wandb: eval/runtime 47.0576&lt;/p&gt; &lt;p&gt;wandb: eval/samples_per_second 225.022&lt;/p&gt; &lt;p&gt;wandb: eval/steps_per_second 28.136&lt;/p&gt; &lt;p&gt;wandb: lr 0.0&lt;/p&gt; &lt;p&gt;wandb: total_flos 7.634730128676549e+17&lt;/p&gt; &lt;p&gt;wandb: train/epoch 3&lt;/p&gt; &lt;p&gt;wandb: train/global_step 790191&lt;/p&gt; &lt;p&gt;wandb: train/grad_norm 0.22934&lt;/p&gt; &lt;p&gt;wandb: train/learning_rate 0.0&lt;/p&gt; &lt;p&gt;wandb: train/loss 0.1965&lt;/p&gt; &lt;p&gt;wandb: train_loss 0.23442&lt;/p&gt; &lt;p&gt;wandb: train_runtime 180146.524&lt;/p&gt; &lt;p&gt;wandb: train_samples_per_second 35.091&lt;/p&gt; &lt;p&gt;wandb: train_steps_per_second 4.386&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slaghton"&gt; /u/Slaghton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qnx9gqc671xe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7ue47/trained_the_tiny_stories_dataset_on_a_12m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7ue47/trained_the_tiny_stories_dataset_on_a_12m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T20:02:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7o884</id>
    <title>Android AI agent based on object detection and LLMs</title>
    <updated>2025-04-25T15:47:26+00:00</updated>
    <author>
      <name>/u/saccharineboi</name>
      <uri>https://old.reddit.com/user/saccharineboi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7o884/android_ai_agent_based_on_object_detection_and/"&gt; &lt;img alt="Android AI agent based on object detection and LLMs" src="https://external-preview.redd.it/cDg0M2JqZnE0MHhlMXhgiTVNaxVoQRD1C2MXVA7X3PPwZtUbpbDgJ5BuncDZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3752d41933290108d1701e5c4033d05f0c2b4001" title="Android AI agent based on object detection and LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My friend has open-sourced deki, an AI agent for Android OS.&lt;/p&gt; &lt;p&gt;It is an Android AI agent powered by ML model, which is fully open-sourced.&lt;/p&gt; &lt;p&gt;It understands whatâ€™s on your screen and can perform tasks based on your voice or text commands.&lt;/p&gt; &lt;p&gt;Some examples:&lt;br /&gt; * &amp;quot;Write my friend &amp;quot;some_name&amp;quot; in WhatsApp that I'll be 15 minutes late&amp;quot;&lt;br /&gt; * &amp;quot;Open Twitter in the browser and write a post about something&amp;quot;&lt;br /&gt; * &amp;quot;Read my latest notifications&amp;quot;&lt;br /&gt; * &amp;quot;Write a linkedin post about something&amp;quot;&lt;/p&gt; &lt;p&gt;Currently, it works only on Android â€” but support for other OS is planned.&lt;/p&gt; &lt;p&gt;The ML and backend codes were also fully open-sourced.&lt;/p&gt; &lt;p&gt;Video prompt example:&lt;/p&gt; &lt;p&gt;&amp;quot;Open linkedin, tap post and write: hi, it is deki, and now I am open sourced. But don't send, just return&amp;quot;&lt;/p&gt; &lt;p&gt;You can find other AI agent demos and usage examples, like, code generation or object detection on github.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/RasulOs/deki"&gt;https://github.com/RasulOs/deki&lt;/a&gt;&lt;/p&gt; &lt;p&gt;License: GPLv3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/saccharineboi"&gt; /u/saccharineboi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/isn6vhfq40xe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7o884/android_ai_agent_based_on_object_detection_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7o884/android_ai_agent_based_on_object_detection_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T15:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7e542</id>
    <title>7B Reasoning Rust Coding Model with Open Dataset</title>
    <updated>2025-04-25T06:22:07+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7e542/7b_reasoning_rust_coding_model_with_open_dataset/"&gt; &lt;img alt="7B Reasoning Rust Coding Model with Open Dataset" src="https://external-preview.redd.it/p7L3vw8UA3QYYsIQPN70mTI04OM5s45JyiPaERXOxBg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=314df762ea670ace7afe4fd1f6277bc8c4f4c048" title="7B Reasoning Rust Coding Model with Open Dataset" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Tesslate/Tessa-Rust-T1-7B-Q8_0-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7e542/7b_reasoning_rust_coding_model_with_open_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7e542/7b_reasoning_rust_coding_model_with_open_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T06:22:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7t089</id>
    <title>Any possibility for Small size models of Llama 3.3 &amp; 4 in future?</title>
    <updated>2025-04-25T19:03:18+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm part of No/Poor GPU club. My old laptop doesn't have GPU at all. Friend's laptop has 8GB VRAM. Time to time I use his laptop only for LLM stuff.&lt;/p&gt; &lt;p&gt;I use small size models till 3.2 version. Then both later versions came with large models. (Frankly expected 10-15B models from 3.3 or 4 Versions).&lt;/p&gt; &lt;p&gt;I know Meta won't touch 3.3 version anymore &amp;amp; hereafter won't release small model for 4 version. I don't think in future we'll get small models from Meta.&lt;/p&gt; &lt;p&gt;So any possibility of small size models from 3.3 or 4 versions models by some other way? Hope someday some legends do this &amp;amp; uploads small models to HuggingFace for same.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Llama&lt;/th&gt; &lt;th align="left"&gt;Parameters&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Llama 3&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;8B&lt;/strong&gt; 70.6B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Llama 3.1&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;8B&lt;/strong&gt; 70.6B 405B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Llama 3.2&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1B 3B 11B&lt;/strong&gt; 90B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.3&lt;/td&gt; &lt;td align="left"&gt;70B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 4&lt;/td&gt; &lt;td align="left"&gt;109B 400B 2T&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7t089/any_possibility_for_small_size_models_of_llama_33/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7t089/any_possibility_for_small_size_models_of_llama_33/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7t089/any_possibility_for_small_size_models_of_llama_33/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T19:03:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7rnu9</id>
    <title>How far can we take quantization aware training (QAT)?</title>
    <updated>2025-04-25T18:07:46+00:00</updated>
    <author>
      <name>/u/gofiend</name>
      <uri>https://old.reddit.com/user/gofiend</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;TLDR: Why can't we train quantization aware models to optimally use the lowest bit quantization it can for every layer / block of parameters?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;There was a recent post here on a very clever new 11 bit float &amp;quot;format&amp;quot; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"&gt;DF11&lt;/a&gt; that has interesting inferencing time vs. memory tradeoffs compared to BF16. It got me thinking further along a fun topic - what does (smallish) model training look like in ~2 years?&lt;/p&gt; &lt;p&gt;We already have frontier (for their size ğŸ˜…) quantization-aware trained models from &lt;a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/"&gt;Google&lt;/a&gt;, and I suspect most labs will release something similar. But I think we're going to go further:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It's obvious that there is value from BF16/INT8 parameters in some blocks and not in others, and a lot of value in clustering parameters that need dynamic range together&lt;/li&gt; &lt;li&gt;A smaller model (all else being equal) is better for inferencing because memory bandwidth (not compute) is the speed contraint&lt;/li&gt; &lt;li&gt;Model parameters almost seem like a legacy concept at this point. We would all prefer to spend 17GB of VRAM on &lt;a href="https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf"&gt;gemma-3-27b-it-qat-q4_0-gguf&lt;/a&gt; vs. ~24GB of VRAM on &lt;a href="https://huggingface.co/google/gemma-3-12b-it"&gt;gemma-3-12b-it&lt;/a&gt; at BF16&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So: can we train models with their memory footprint and estimated token generation rate (targeting a reference architecture) as part of the objective function?&lt;/p&gt; &lt;p&gt;My &lt;em&gt;naive&lt;/em&gt; proposal:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Add memory footprint and a function that approximates token generation rate to the training loss function&lt;/li&gt; &lt;li&gt;Add a differentiable &amp;quot;quantization&amp;quot; parameter for every ~4K of parameters (activation, weights etc.)&lt;/li&gt; &lt;li&gt;During each batch of the forward pass, use the quantization parameter to drop the block of parameters from BF16 to DF11 to INT8 to INT4 probabilistically based on value i.e. &lt;ul&gt; &lt;li&gt;A high value would mostly do the forward pass in BF16, a little in DF11 and very little in INT8/4&lt;/li&gt; &lt;li&gt;A middle value would be mostly INT8 with a little DF11 and INT4&lt;/li&gt; &lt;li&gt;A low value would be mostly INT4&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Calculate the average memory footprint and tokens/second rate (again an approximate reference model is fine) and incorporate into the loss, then run the backward pass &lt;ul&gt; &lt;li&gt;This should make the quantization parameter nicely differentiable and trainable (?)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;At the end of training freeze blocks of parameters at the quantization level that reflects the final values of the quantization parameter (i.e. a mid value would freeze at INT8) &lt;ul&gt; &lt;li&gt;In theory the model would have learnt to cluster its use of high dynamic range parameters to minimize the use of BF16 and maximize the use of INT8/4&lt;/li&gt; &lt;li&gt;You can imagine training multiple sizes of the same model almost in parallel by varying the cost function&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll poke at the literature, but I'd appreciate pointers to anything similar that folks have done already (and of course your thoughts on why this naive approach is ... naive).&lt;/p&gt; &lt;p&gt;A really simple first step might be running an optimization exercise like this on an existing model ... but &lt;a href="/u/danielhanchen"&gt;u/danielhanchen&lt;/a&gt; might just be all over &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k71mab/unsloth_dynamic_v20_ggufs_llama_4_bug_fixes_kl/"&gt;that already&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gofiend"&gt; /u/gofiend &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7rnu9/how_far_can_we_take_quantization_aware_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7rnu9/how_far_can_we_take_quantization_aware_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7rnu9/how_far_can_we_take_quantization_aware_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T18:07:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7kv9a</id>
    <title>Intel Updates Its PyTorch Extension With DeepSeek-R1 Support, New Optimizations</title>
    <updated>2025-04-25T13:25:15+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7kv9a/intel_updates_its_pytorch_extension_with/"&gt; &lt;img alt="Intel Updates Its PyTorch Extension With DeepSeek-R1 Support, New Optimizations" src="https://external-preview.redd.it/yTiUURrBkqcGYJGBhqzC01YOstzVvXfVd3FxAo3YWYU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73fdf48b98f8c4bbf03db30badce672add745943" title="Intel Updates Its PyTorch Extension With DeepSeek-R1 Support, New Optimizations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/Intel-PyTorch-Extension-2.7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7kv9a/intel_updates_its_pytorch_extension_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7kv9a/intel_updates_its_pytorch_extension_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T13:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7r8qu</id>
    <title>SOTA Spatial Reasoning in 2025</title>
    <updated>2025-04-25T17:51:12+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7r8qu/sota_spatial_reasoning_in_2025/"&gt; &lt;img alt="SOTA Spatial Reasoning in 2025" src="https://b.thumbs.redditmedia.com/AasNY8iky2mDi_qpsR-CfGk7uE_qPRprH6Ci0ul1Isg.jpg" title="SOTA Spatial Reasoning in 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The ability to accurately estimate distances from RGB image input is just at the ğ—³ğ—¿ğ—¼ğ—»ğ˜ğ—¶ğ—²ğ—¿ ğ—¼ğ—³ ğ—°ğ˜‚ğ—¿ğ—¿ğ—²ğ—»ğ˜ ğ—”ğ—œ ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—°ğ—®ğ—½ğ—®ğ—¯ğ—¶ğ—¹ğ—¶ğ˜ğ—¶ğ—²ğ˜€.&lt;/p&gt; &lt;p&gt;Nonetheless, distance estimation is a ğ—°ğ—¿ğ—¶ğ˜ğ—¶ğ—°ğ—®ğ—¹ ğ—³ğ—¼ğ—¿ ğ—½ğ—²ğ—¿ğ—°ğ—²ğ—½ğ˜ğ—¶ğ—¼ğ—» ğ—®ğ—»ğ—± ğ—½ğ—¹ğ—®ğ—»ğ—»ğ—¶ğ—»ğ—´ ğ—¶ğ—» ğ—²ğ—ºğ—¯ğ—¼ğ—±ğ—¶ğ—²ğ—± ğ—”ğ—œ ğ—®ğ—½ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€ ğ—¹ğ—¶ğ—¸ğ—² ğ—¿ğ—¼ğ—¯ğ—¼ğ˜ğ—¶ğ—°ğ˜€ which must navigate around our 3D world.&lt;/p&gt; &lt;p&gt;Making a ğ—¼ğ—½ğ—²ğ—»-ğ˜„ğ—²ğ—¶ğ—´ğ—µğ˜ model ğ˜€ğ—ºğ—®ğ—¹ğ—¹ and ğ—³ğ—®ğ˜€ğ˜ enough to run ğ—¼ğ—»-ğ—±ğ—²ğ˜ƒğ—¶ğ—°ğ—², using ğ—¼ğ—½ğ—²ğ—»-ğ˜€ğ—¼ğ˜‚ğ—¿ğ—°ğ—² ğ—°ğ—¼ğ—±ğ—² and ğ—±ğ—®ğ˜ğ—®, we aim to democratize embodied AI.&lt;/p&gt; &lt;p&gt;I've updated the comparison among closed APIs with SOTA performance in &lt;strong&gt;quantitative spatial reasoning&lt;/strong&gt; tasks like distance/size estimation from RGB inputs and our 3B open-weight model: SpaceThinker&lt;/p&gt; &lt;p&gt;The performance for the the 3B SpaceThinker lies between gpt-4o and gemini-2.5-pro in estimating distances using the QSpatial++ split of Q-Spatial-Bench.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Evaluation Results:&lt;/strong&gt; &lt;a href="https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B#qspatial-comparison-table-42525"&gt;https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B#qspatial-comparison-table-42525&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interesting finding:&lt;/strong&gt; By switching model name in &lt;a href="https://colab.research.google.com/drive/1buEe2QC4_pnrJwQ9XyRAH7RfaIa6pbex?usp=sharing"&gt;this colab&lt;/a&gt;, using the non-reasoning variant &lt;a href="https://huggingface.co/remyxai/SpaceQwen2.5-VL-3B-Instruct"&gt;SpaceQwen&lt;/a&gt;, you'll find using the &lt;a href="https://github.com/andrewliao11/Q-Spatial-Bench-code/blob/main/prompt_templates/spatial_prompt_steps.txt"&gt;step-by-step reasoning prompt&lt;/a&gt; actually hurts performance, challenging the convention that reasoning models &lt;a href="https://huggingface.co/blog/NormalUhr/deepseek-r1-explained#74-prompt-engineering-sensitivities"&gt;don't benefit&lt;/a&gt; from complex instructions the way non-reasoning models do.&lt;/p&gt; &lt;p&gt;Modifying the above colab, you can also compare SpaceThinker to it's base model to assess the performance impact due to SFT by LoRA using the SpaceThinker dataset: &lt;a href="https://huggingface.co/datasets/remyxai/SpaceThinker"&gt;https://huggingface.co/datasets/remyxai/SpaceThinker&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k7r8qu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7r8qu/sota_spatial_reasoning_in_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7r8qu/sota_spatial_reasoning_in_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T17:51:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7k1ck</id>
    <title>No thinking, is the right way to think?</title>
    <updated>2025-04-25T12:45:32+00:00</updated>
    <author>
      <name>/u/Eralyon</name>
      <uri>https://old.reddit.com/user/Eralyon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2504.09858"&gt;https://arxiv.org/abs/2504.09858&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR:&lt;br /&gt; Bypassing the thinking process, forcing the beginning of the answer by &amp;quot;Thinking: Okay, I think I have finished thinking&amp;quot; (lol), they get similar/better inference results !!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eralyon"&gt; /u/Eralyon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7k1ck/no_thinking_is_the_right_way_to_think/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7k1ck/no_thinking_is_the_right_way_to_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7k1ck/no_thinking_is_the_right_way_to_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T12:45:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7uxxk</id>
    <title>LM Studio 0.3.15 with support for GLM-4 models and NVIDIA RTX50-series just got released</title>
    <updated>2025-04-25T20:25:39+00:00</updated>
    <author>
      <name>/u/ispolin</name>
      <uri>https://old.reddit.com/user/ispolin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uxxk/lm_studio_0315_with_support_for_glm4_models_and/"&gt; &lt;img alt="LM Studio 0.3.15 with support for GLM-4 models and NVIDIA RTX50-series just got released" src="https://b.thumbs.redditmedia.com/zPNTVgAISYkoCxdv3xYABX-74BRXUsT6QERGjZPbVto.jpg" title="LM Studio 0.3.15 with support for GLM-4 models and NVIDIA RTX50-series just got released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/mxja601ei1xe1.png?width=2102&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31ca8d6f8f7b767e7379e5b00878cc43622b19c1"&gt;https://preview.redd.it/mxja601ei1xe1.png?width=2102&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31ca8d6f8f7b767e7379e5b00878cc43622b19c1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ispolin"&gt; /u/ispolin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uxxk/lm_studio_0315_with_support_for_glm4_models_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uxxk/lm_studio_0315_with_support_for_glm4_models_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uxxk/lm_studio_0315_with_support_for_glm4_models_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T20:25:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7t6dm</id>
    <title>Deepseek r2 when?</title>
    <updated>2025-04-25T19:10:33+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hope it comes out this month, i saw a post that said it was gonna come out before May..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7t6dm/deepseek_r2_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7t6dm/deepseek_r2_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7t6dm/deepseek_r2_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T19:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7tg8n</id>
    <title>GLM-4-9B(Q5_K_L) Heptagon Balls sim (multi-prompt)</title>
    <updated>2025-04-25T19:22:16+00:00</updated>
    <author>
      <name>/u/danihend</name>
      <uri>https://old.reddit.com/user/danihend</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7tg8n/glm49bq5_k_l_heptagon_balls_sim_multiprompt/"&gt; &lt;img alt="GLM-4-9B(Q5_K_L) Heptagon Balls sim (multi-prompt)" src="https://external-preview.redd.it/M3Z4eDhhdmU3MXhlMYg3hh2y6NN7WC_nAJWjhF3jltCetUE7ORI41iUNIAJC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45713b47f9ba53d37e0b87a41e271222a2364c40" title="GLM-4-9B(Q5_K_L) Heptagon Balls sim (multi-prompt)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title pretty much says it but just to clarify - it wasn't one-shot. It was prompt-&amp;gt;response-&amp;gt;error, then this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Here is an error after running the sim: &amp;lt;error&amp;gt; Exception in Tkinter callback Traceback (most recent call last): File &amp;quot;C:\Users\username\anaconda3\Lib\tkinter_init_.py&amp;quot;, line 1967, in call return self.func(*args) ^^^^^^^^^^^^^^^^ File &amp;quot;C:\Users\username\anaconda3\Lib\tkinter_init_.py&amp;quot;, line 861, in callit func(*args) File &amp;quot;c:\Users\username\VSCodeProjects\model_tests\balls\GLM49B_Q5KL_balls.py&amp;quot;, line 140, in update current_time_ms = float(current_time) ^^^^^^^^^^^^^^^^^^^ ValueError: could not convert string to float: 'after#2' &amp;lt;/error&amp;gt; Now think as hard as you can about why this is happening. Look at the entire script and consider how the parts work together. You are free to think as long as you need if you use thinking tags like this: &amp;lt;think&amp;gt;thoughts here&amp;lt;/think&amp;gt;. Once finished thinking, just provide the patch to the code. No need to rewrite it all. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then I applied the fix, got another error, replaced the original Assistant code block with the new code and presented the new error as if it were the 1st error by editing my message. I think that resulted in the working version.&lt;/p&gt; &lt;p&gt;So TL;DR - couple of prompts to get it working.&lt;/p&gt; &lt;p&gt;Simply pasting error after error did not work, but structured prompting with a bit of thinking seems to bring out some more potential.&lt;/p&gt; &lt;p&gt;Just thought I'd share in case it helps people with prompting it and just to show that it is not a bad model for it's size. The result is very similar to the 32B version.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danihend"&gt; /u/danihend &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zrjvo8ve71xe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7tg8n/glm49bq5_k_l_heptagon_balls_sim_multiprompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7tg8n/glm49bq5_k_l_heptagon_balls_sim_multiprompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T19:22:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7rgyv</id>
    <title>Tiny Agents: a MCP-powered agent in 50 lines of code</title>
    <updated>2025-04-25T18:00:23+00:00</updated>
    <author>
      <name>/u/julien_c</name>
      <uri>https://old.reddit.com/user/julien_c</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7rgyv/tiny_agents_a_mcppowered_agent_in_50_lines_of_code/"&gt; &lt;img alt="Tiny Agents: a MCP-powered agent in 50 lines of code" src="https://external-preview.redd.it/fCTs8gI7KvvOKk5o8AQ0g6EQWi7h5KkDI0MBs8uNyiw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09d0dce63a6cd60077b6242bb1e5e6a8b6411b5f" title="Tiny Agents: a MCP-powered agent in 50 lines of code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I'm a co-founder of HuggingFace and a big &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; fan.&lt;/p&gt; &lt;p&gt;Today I'm dropping Tiny Agents, a 50 lines-of-code Agent in Javascript ğŸ”¥&lt;/p&gt; &lt;p&gt;I spent the last few weeks diving into MCP (Model Context Protocol) to understand what the hype was about.&lt;/p&gt; &lt;p&gt;It is fairly simple, but still quite useful as a standard API to expose sets of Tools that can be hooked to LLMs.&lt;/p&gt; &lt;p&gt;But while implementing it I came to my second realization:&lt;/p&gt; &lt;p&gt;Once you have a MCP Client, an Agent is literally just a while loop on top of it. ğŸ¤¯&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/blog/tiny-agents"&gt;https://huggingface.co/blog/tiny-agents&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v0acl2n6t0xe1.png?width=1846&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8cecc5f62c6e05855d5ea1b67cceb56e2ccddbf5"&gt;https://preview.redd.it/v0acl2n6t0xe1.png?width=1846&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8cecc5f62c6e05855d5ea1b67cceb56e2ccddbf5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/julien_c"&gt; /u/julien_c &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7rgyv/tiny_agents_a_mcppowered_agent_in_50_lines_of_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7rgyv/tiny_agents_a_mcppowered_agent_in_50_lines_of_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7rgyv/tiny_agents_a_mcppowered_agent_in_50_lines_of_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T18:00:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7quqt</id>
    <title>Do people trying to squeeze every last GB out of their GPU use their IGPU to display to their monitor?</title>
    <updated>2025-04-25T17:34:59+00:00</updated>
    <author>
      <name>/u/Golfclubwar</name>
      <uri>https://old.reddit.com/user/Golfclubwar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;By default, just for basic display, Linux can eat 500MB, windows can eat 1.1GB. I imagine for someone with like an 8-12GB card trying to barely squeeze the biggest model they can onto the gpu by tweaking context size and quant etc., this is a highly nontrivial cost. &lt;/p&gt; &lt;p&gt;Unless for some reason you needed the dgpu for something else, why wouldnâ€™t they just display using their IGPU instead? Obviously thereâ€™s still a fixed driver overhead, but youâ€™d save nearly a gigabyte, and in terms of simply using an IDE and a browser itâ€™s hard to think of any drawbacks.&lt;/p&gt; &lt;p&gt;Am I stupid and this wouldnâ€™t work the way I think it would or something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Golfclubwar"&gt; /u/Golfclubwar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7quqt/do_people_trying_to_squeeze_every_last_gb_out_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7quqt/do_people_trying_to_squeeze_every_last_gb_out_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7quqt/do_people_trying_to_squeeze_every_last_gb_out_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T17:34:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7uvpm</id>
    <title>Qwen introduces their mobile app</title>
    <updated>2025-04-25T20:22:54+00:00</updated>
    <author>
      <name>/u/Vegetable-Practice85</name>
      <uri>https://old.reddit.com/user/Vegetable-Practice85</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uvpm/qwen_introduces_their_mobile_app/"&gt; &lt;img alt="Qwen introduces their mobile app" src="https://preview.redd.it/ewjq8s2ei1xe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fe3b4f5cf5c69932dece355c02addb1e439cdd0" title="Qwen introduces their mobile app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable-Practice85"&gt; /u/Vegetable-Practice85 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ewjq8s2ei1xe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uvpm/qwen_introduces_their_mobile_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uvpm/qwen_introduces_their_mobile_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T20:22:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7krlm</id>
    <title>Gemma 3 fakes (and ignores) the system prompt</title>
    <updated>2025-04-25T13:20:27+00:00</updated>
    <author>
      <name>/u/WolframRavenwolf</name>
      <uri>https://old.reddit.com/user/WolframRavenwolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7krlm/gemma_3_fakes_and_ignores_the_system_prompt/"&gt; &lt;img alt="Gemma 3 fakes (and ignores) the system prompt" src="https://preview.redd.it/xuycbwnk4zwe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fba119d92fca9059223ac136a22602c0f3b43b8" title="Gemma 3 fakes (and ignores) the system prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The screenshot shows what Gemma 3 said when I pointed out that it wasn't following its system prompt properly. &amp;quot;Who reads the fine print? ğŸ˜‰&amp;quot; - really, seriously, WTF?&lt;/p&gt; &lt;p&gt;At first I thought it may be an issue with the format/quant, an inference engine bug or just my settings or prompt. But digging deeper, I realized I had been fooled: While the [Gemma 3 chat template](&lt;a href="https://huggingface.co/google/gemma-3-27b-it/blob/main/chat%5C_template.json"&gt;https://huggingface.co/google/gemma-3-27b-it/blob/main/chat\_template.json&lt;/a&gt;) *does* support a system role, all it *really* does is dump the system prompt into the first user message. That's both ugly *and* unreliable - doesn't even use any special tokens, so there's no way for the model to differentiate between what the system (platform/dev) specified as general instructions and what the (possibly untrusted) user said. ğŸ™ˆ&lt;/p&gt; &lt;p&gt;Sure, the model still follows instructions like any other user input - but it never learned to treat them as higher-level system rules, so they're basically &amp;quot;optional&amp;quot;, which is why it ignored mine like &amp;quot;fine print&amp;quot;. That makes Gemma 3 utterly unreliable - so I'm switching to Mistral Small 3.1 24B Instruct 2503 which has proper system prompt support.&lt;/p&gt; &lt;p&gt;Hopefully Google will provide *real* system prompt support in Gemma 4 - or the community will deliver a better finetune in the meantime. For now, I'm hoping Mistral's vision capability gets wider support, since that's one feature I'll miss from Gemma.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WolframRavenwolf"&gt; /u/WolframRavenwolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xuycbwnk4zwe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7krlm/gemma_3_fakes_and_ignores_the_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7krlm/gemma_3_fakes_and_ignores_the_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T13:20:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7o89n</id>
    <title>We compress any BF16 model to ~70% size during inference, while keeping the output LOSSLESS so that you can fit in more ERP context or run larger models.</title>
    <updated>2025-04-25T15:47:29+00:00</updated>
    <author>
      <name>/u/choHZ</name>
      <uri>https://old.reddit.com/user/choHZ</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Glad to share another interesting piece of work from us: &lt;a href="https://arxiv.org/abs/2504.11651"&gt;&lt;strong&gt;70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float (DF11)&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The tl;dr of this work is super simple. We â€” and several prior works â€” noticed that while &lt;strong&gt;BF16&lt;/strong&gt; is often promoted as a â€œmore range, less precisionâ€ alternative to FP16 (especially to avoid value overflow/underflow during training), &lt;strong&gt;its range part (exponent bits) ends up being pretty redundant once the model is trained.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In other words, although BF16 as a data format can represent a wide range of numbers, most trained models' exponents are plenty sparse. In practice, the exponent bits carry around 2.6 bits of actual information on average â€” far from the full 8 bits they're assigned.&lt;/p&gt; &lt;p&gt;This opens the door for classic Huffman coding â€” where shorter bit sequences are assigned to more frequent values â€” to &lt;strong&gt;compress the model weights&lt;/strong&gt; into a new data format we call &lt;strong&gt;DFloat11/DF11&lt;/strong&gt;, resulting in a &lt;strong&gt;LOSSLESS compression down to ~11 bits&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;But isnâ€™t this just Zip?&lt;/h1&gt; &lt;p&gt;Not exactly. It is true that tools like Zip also leverage Huffman coding, but the tricky part here is &lt;strong&gt;making it memory efficient during inference&lt;/strong&gt;, as end users are probably not gonna be too trilled if it just makes model checkpoint downloads a bit faster (in all fairness, smaller chekpoints means a lot when training at scale, but that's not a problem for everyday users).&lt;/p&gt; &lt;p&gt;What does matter to everyday users is &lt;strong&gt;making the memory footprint smaller during GPU inference, which requires nontrivial efforts.&lt;/strong&gt; But we have figured it out, and weâ€™ve open-sourced the code.&lt;/p&gt; &lt;p&gt;So now you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Run models that previously didnâ€™t fit into your GPU memory.&lt;/li&gt; &lt;li&gt;Or run the same model with &lt;strong&gt;larger batch sizes and/or longer sequences&lt;/strong&gt; (very handy for those lengthy ERPs, or so I have heard).&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;GPU Type&lt;/th&gt; &lt;th align="left"&gt;Method&lt;/th&gt; &lt;th align="left"&gt;Successfully Run?&lt;/th&gt; &lt;th align="left"&gt;Required Memory&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3.1-405B-Instruct&lt;/td&gt; &lt;td align="left"&gt;8Ã—H100-80G&lt;/td&gt; &lt;td align="left"&gt;BF16&lt;/td&gt; &lt;td align="left"&gt;âŒ&lt;/td&gt; &lt;td align="left"&gt;811.71 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;DF11 (Ours)&lt;/td&gt; &lt;td align="left"&gt;âœ…&lt;/td&gt; &lt;td align="left"&gt;551.22 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3.3-70B-Instruct&lt;/td&gt; &lt;td align="left"&gt;1Ã—H200-141G&lt;/td&gt; &lt;td align="left"&gt;BF16&lt;/td&gt; &lt;td align="left"&gt;âŒ&lt;/td&gt; &lt;td align="left"&gt;141.11 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;DF11 (Ours)&lt;/td&gt; &lt;td align="left"&gt;âœ…&lt;/td&gt; &lt;td align="left"&gt;96.14 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5-32B-Instruct&lt;/td&gt; &lt;td align="left"&gt;1Ã—A6000-48G&lt;/td&gt; &lt;td align="left"&gt;BF16&lt;/td&gt; &lt;td align="left"&gt;âŒ&lt;/td&gt; &lt;td align="left"&gt;65.53 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;DF11 (Ours)&lt;/td&gt; &lt;td align="left"&gt;âœ…&lt;/td&gt; &lt;td align="left"&gt;45.53 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1-Distill-Llama-8B&lt;/td&gt; &lt;td align="left"&gt;1Ã—RTX 5080-16G&lt;/td&gt; &lt;td align="left"&gt;BF16&lt;/td&gt; &lt;td align="left"&gt;âŒ&lt;/td&gt; &lt;td align="left"&gt;16.06 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;DF11 (Ours)&lt;/td&gt; &lt;td align="left"&gt;âœ…&lt;/td&gt; &lt;td align="left"&gt;11.23 GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Some research promo posts try to surgercoat their weakness or tradeoff, thats not us. So here's are some honest FAQs:&lt;/p&gt; &lt;h1&gt;Whatâ€™s the catch?&lt;/h1&gt; &lt;p&gt;Like all compression work, thereâ€™s a cost to decompressing. And here are some efficiency reports.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;On an A100 with batch size 128, DF11 is &lt;strong&gt;basically just as fast&lt;/strong&gt; as BF16 (1.02x difference, assuming both version fits in the GPUs with the same batch size). See Figure 9.&lt;/li&gt; &lt;li&gt;It is up to &lt;strong&gt;38.8x faster&lt;/strong&gt; than CPU offloading, so if you have a model that can't be run on your GPU in BF16, but can in DF11, there are plenty sweet performance gains over CPU offloading â€” one of the other popular way to run larger-than-capacity models. See Figure 3.&lt;/li&gt; &lt;li&gt;With the model weight being compressed, you can use the saved real estate for larger batch size or longer context length. This is expecially significant if the model is already tightly fitted in GPU. See Figure 4.&lt;/li&gt; &lt;li&gt;What about batch size 1 latency when both versions (DF11 &amp;amp; BF16) can fit in a single GPU? This is where DF11 is the weakest â€” we observe &lt;strong&gt;~40% slower&lt;/strong&gt; (2k/100 tokens for in/out). So there is not much motivation in using DF11 if you are not trying to run larger model/bigger batch size/longer sequence length.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why not just (lossy) quantize to 8-bit?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;The short answer is you should totally do that if you are satisfied with the output lossy 8-bit quantization with respect to your task. But how do you really know it is always good?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Many benchmark literature suggest that compressing a model (weight-only or otherwise) to 8-bit-ish is typically a safe operation, even though it's technically lossy. What we found, however, is that while this claim is often made in quantization papers, their benchmarks tend to focus on general tasks like MMLU and Commonsense Reasoning; which do not present a comprehensive picture of model capability.&lt;/p&gt; &lt;p&gt;More challenging benchmarks â€” such as those involving complex reasoning â€” and real-world user preferences often reveal noticeable differences. One good example is Chatbot Arena indicates the 8-bit (though it is W8A8 where DF11 is weight only, so it is not 100% apple-to-apple) and 16-bit Llama 3.1 405b tend to behave quite differently on some categories of tasks (e.g., Math and Coding).&lt;/p&gt; &lt;p&gt;Although the broader question: &lt;em&gt;â€œWhich specific task, on which model, using which quantization technique, under what conditions, will lead to a noticeable drop compared to FP16/BF16?â€&lt;/em&gt; is likely to remain open-ended simply due to the sheer amount of potential combinations and definition of â€œnoticable.â€ &lt;strong&gt;It is fair to say that lossy quantization introduces complexities that some end-users would prefer to avoid, since it creates uncontrolled variables that must be empirically stress-tested for each deployment scenario.&lt;/strong&gt; DF11 offeres an alternative that avoids this concern 100%.&lt;/p&gt; &lt;h1&gt;What about finetuning?&lt;/h1&gt; &lt;p&gt;Our method could potentially pair well with PEFT methods like LoRA, where the base weights are frozen. But since we compress block-wise, we canâ€™t just apply it naively without breaking gradients. We're actively exploring this direction. If it works, if would potentially become a QLoRA alternative where you can lossly LoRA finetune a model with reduced memory footprint.&lt;/p&gt; &lt;p&gt;(As always, happy to answer questions or chat until my advisor notices Iâ€™m doomscrolling socials during work hours :&amp;gt; )&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2504.11651"&gt;https://arxiv.org/abs/2504.11651&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/LeanModels/DFloat11"&gt;https://github.com/LeanModels/DFloat11&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/choHZ"&gt; /u/choHZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T15:47:29+00:00</published>
  </entry>
</feed>
