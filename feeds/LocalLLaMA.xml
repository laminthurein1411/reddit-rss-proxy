<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-24T17:48:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ktpz29</id>
    <title>Anyone on Oahu want to let me borrow an RTX 6000 Pro to benchmark against this dual 5090 rig?</title>
    <updated>2025-05-23T17:52:15+00:00</updated>
    <author>
      <name>/u/Special-Wolverine</name>
      <uri>https://old.reddit.com/user/Special-Wolverine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktpz29/anyone_on_oahu_want_to_let_me_borrow_an_rtx_6000/"&gt; &lt;img alt="Anyone on Oahu want to let me borrow an RTX 6000 Pro to benchmark against this dual 5090 rig?" src="https://b.thumbs.redditmedia.com/n5NQk6PSosGh3XCgL-CRJ01dhOkMxqKYXWlOm95gUJw.jpg" title="Anyone on Oahu want to let me borrow an RTX 6000 Pro to benchmark against this dual 5090 rig?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sits on my office desk for running very large context prompts (50K words) with QwQ 32B. Gotta be offline because they have a lot of P.I.I.&lt;/p&gt; &lt;p&gt;Had it in a Mechanic Master c34plus (25L) but CPU fans (Scythe Grand Tornado 3,000rpm) kept ramping up because two 5090s were blasting the radiator in a confined space, and could only fit a 1300W PSU in that tiny case which meant heavy power limiting for the CPU and GPUs.&lt;/p&gt; &lt;p&gt;Paid $3,200 each for the 5090 FE's and would have paid more. Couldn't be happier and this rig turns what used to take me 8 hours into 5 minutes of prompt processing and inference + 15 minutes of editing to output complicated 15 page reports.&lt;/p&gt; &lt;p&gt;Anytime I show a coworker what it can do, they immediately throw money at me and tell me to build them a rig, so I tell them I'll get them 80% of the performance for about $2,200 and I've built two dual 3090 local Al rigs for such coworkers so far.&lt;/p&gt; &lt;p&gt;Frame is a 3D printed one from Etsy by ArcadeAdamsParts. There were some minor issues with it, but Adam was eager to address them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special-Wolverine"&gt; /u/Special-Wolverine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ktpz29"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktpz29/anyone_on_oahu_want_to_let_me_borrow_an_rtx_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktpz29/anyone_on_oahu_want_to_let_me_borrow_an_rtx_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T17:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kucgs2</id>
    <title>LLM help for recovering deleted data?</title>
    <updated>2025-05-24T14:07:41+00:00</updated>
    <author>
      <name>/u/dreamyrhodes</name>
      <uri>https://old.reddit.com/user/dreamyrhodes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So recently I had a mishap and lost most of my /home. I am currently in the process of restoring data. Images are simple, I will just browse through them, delete the thumbnail cache crap and move what I wanna keep. MP3s I can rename with a script analyzing their metadata. But the recovery process also collected a few hundred thousand text files. That is everything from local config files, jsons, saved passwords (encrypted), browser bookmarks and settings, lots of doubles or outdated stuff.&lt;/p&gt; &lt;p&gt;I thought about getting help from a LLM to analyze the content and suggest categorization or maybe even possible merges (of different versions of jsons).&lt;/p&gt; &lt;p&gt;But I am unsure how where I would start with something like this... I have koboldcpp installed, I need a model and a way to interact with it that it can read text files and analyze / summarize them like &amp;quot;f15649040.txt looks like saved browser history ranging from date to date, I will move it to mozilla_rescue folder&amp;quot;. Something like that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dreamyrhodes"&gt; /u/dreamyrhodes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kucgs2/llm_help_for_recovering_deleted_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kucgs2/llm_help_for_recovering_deleted_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kucgs2/llm_help_for_recovering_deleted_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T14:07:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1kua0su</id>
    <title>RL Based Sales Conversion - I Just built a PyPI package</title>
    <updated>2025-05-24T12:04:13+00:00</updated>
    <author>
      <name>/u/Nandakishor_ml</name>
      <uri>https://old.reddit.com/user/Nandakishor_ml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kua0su/rl_based_sales_conversion_i_just_built_a_pypi/"&gt; &lt;img alt="RL Based Sales Conversion - I Just built a PyPI package" src="https://preview.redd.it/mkd4apfqyp2f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df2889c3284406632eca4be22465645ca4c1f500" title="RL Based Sales Conversion - I Just built a PyPI package" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My idea is to create pure Reinforcement learning that understand the infinite branches of sales conversations. Then predict the conversion probability of each conversation turns, as it progress indefinetly, then use these probabilities to guide the LLM to move towards those branches that leads to conversion.&lt;/p&gt; &lt;p&gt;The pipeline is simple. When user starts conversation, it first passed to an LLM like llama or Qwen, then it will generate customer engagement and sales effectiveness score as metrics, along with that the embedding model will generate embeddings, then combine this to create the state space vectors, using this the PPO generate final probabilities of conversion, as the turn goes on, the state vectors are added with previous conversation conversion probabilities to improve more. &lt;/p&gt; &lt;p&gt;Simple usage given below&lt;/p&gt; &lt;p&gt;PyPI: &lt;a href="https://pypi.org/project/deepmost/"&gt;https://pypi.org/project/deepmost/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/DeepMostInnovations/deepmost"&gt;https://github.com/DeepMostInnovations/deepmost&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from deepmost import sales conversation = [ &amp;quot;Hello, I'm looking for information on your new AI-powered CRM&amp;quot;, &amp;quot;You've come to the right place! Our AI CRM helps increase sales efficiency. What challenges are you facing?&amp;quot;, &amp;quot;We struggle with lead prioritization and follow-up timing&amp;quot;, &amp;quot;Excellent! Our AI automatically analyzes leads and suggests optimal follow-up times. Would you like to see a demo?&amp;quot;, &amp;quot;That sounds interesting. What's the pricing like?&amp;quot; ] # Analyze conversation progression (prints results automatically) results = sales.analyze_progression(conversation, llm_model=&amp;quot;unsloth/Qwen3-4B-GGUF&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nandakishor_ml"&gt; /u/Nandakishor_ml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mkd4apfqyp2f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kua0su/rl_based_sales_conversion_i_just_built_a_pypi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kua0su/rl_based_sales_conversion_i_just_built_a_pypi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T12:04:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktklo5</id>
    <title>Unmute by Kyutai: Make LLMs listen and speak</title>
    <updated>2025-05-23T14:12:46+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems nicely polished and apparently works with any LLM. Open-source in the coming weeks.&lt;/p&gt; &lt;p&gt;Demo uses Gemma 3 12B as base LLM (demo link in the blog post, reddit seems to auto-delete my post if I include it here).&lt;/p&gt; &lt;p&gt;If any Kyutai dev happens to lurk here, would love to hear about the memory requirements of the TTS &amp;amp; STT models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://kyutai.org/2025/05/22/unmute.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktklo5/unmute_by_kyutai_make_llms_listen_and_speak/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktklo5/unmute_by_kyutai_make_llms_listen_and_speak/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T14:12:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktudaj</id>
    <title>Best local coding model right now?</title>
    <updated>2025-05-23T20:57:09+00:00</updated>
    <author>
      <name>/u/Combinatorilliance</name>
      <uri>https://old.reddit.com/user/Combinatorilliance</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I was very active here about a year ago, but I've been using Claude a lot the past few months.&lt;/p&gt; &lt;p&gt;I do like claude a lot, but it's not magic and smaller models are actually quite a lot nicer in the sense that I have far, far more control over &lt;/p&gt; &lt;p&gt;I have a 7900xtx, and I was eyeing gemma 27b for local coding support?&lt;/p&gt; &lt;p&gt;Are there any other models I should be looking at? Qwen 3 maybe?&lt;/p&gt; &lt;p&gt;Perhaps a model specifically for coding? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Combinatorilliance"&gt; /u/Combinatorilliance &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktudaj/best_local_coding_model_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktudaj/best_local_coding_model_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktudaj/best_local_coding_model_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T20:57:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktoh78</id>
    <title>LLMI system I (not my money) got for our group</title>
    <updated>2025-05-23T16:52:23+00:00</updated>
    <author>
      <name>/u/SandboChang</name>
      <uri>https://old.reddit.com/user/SandboChang</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktoh78/llmi_system_i_not_my_money_got_for_our_group/"&gt; &lt;img alt="LLMI system I (not my money) got for our group" src="https://preview.redd.it/lgjexuw8ak2f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3260ccc53dd2f7cca5692637366920fd7a9928ec" title="LLMI system I (not my money) got for our group" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SandboChang"&gt; /u/SandboChang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lgjexuw8ak2f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktoh78/llmi_system_i_not_my_money_got_for_our_group/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktoh78/llmi_system_i_not_my_money_got_for_our_group/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T16:52:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kugp9h</id>
    <title>Qwen3 30B A3B unsloth GGUF vs MLX generation speed difference</title>
    <updated>2025-05-24T17:14:24+00:00</updated>
    <author>
      <name>/u/ahmetegesel</name>
      <uri>https://old.reddit.com/user/ahmetegesel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks. Is it just me or unsloth quants got slower with Qwen3 models? I can almost swear that there was 5-10t/s difference between these two quants before. I was getting 60-75t/s with GGUF and 80t/s with MLX. And I am pretty sure that both were 8bit quants. In fact, I was using UD 8_K_XL from unsloth, which is supposed to be a bit bigger and maybe slightly slower. All I did was to update the models since I heard there were more fixes from unsloth. But for some reason, I am getting 13t/s from 8_K_XL and 75t/s from MLX 8 bit. &lt;/p&gt; &lt;p&gt;Setup:&lt;br /&gt; -Mac M4 Max 128GB&lt;br /&gt; -LM Studio latest version&lt;br /&gt; -400/40k context used&lt;br /&gt; -thinking enabled &lt;/p&gt; &lt;p&gt;I tried with and without flash attention to see if there is bug in that feature now as I was using that when first tried weeks ago and got 75t/s speed back then, but still the same result&lt;/p&gt; &lt;p&gt;Anyone experiencing this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ahmetegesel"&gt; /u/ahmetegesel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kugp9h/qwen3_30b_a3b_unsloth_gguf_vs_mlx_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kugp9h/qwen3_30b_a3b_unsloth_gguf_vs_mlx_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kugp9h/qwen3_30b_a3b_unsloth_gguf_vs_mlx_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T17:14:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktiq99</id>
    <title>I accidentally too many P100</title>
    <updated>2025-05-23T12:48:51+00:00</updated>
    <author>
      <name>/u/TooManyPascals</name>
      <uri>https://old.reddit.com/user/TooManyPascals</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/"&gt; &lt;img alt="I accidentally too many P100" src="https://b.thumbs.redditmedia.com/IdF4SU4XHKp-_JI6o-Y6kol8-cLrv94jdBxKlq9CTYI.jpg" title="I accidentally too many P100" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I had quite positive results with a P100 last summer, so when R1 came out, I decided to try if I could put 16 of them in a single pc... and I could.&lt;/p&gt; &lt;p&gt;Not the fastest think in the universe, and I am not getting awesome PCIE speed (2@4x). But it works, is still cheaper than a 5090, and I hope I can run stuff with large contexts.&lt;/p&gt; &lt;p&gt;I hoped to run llama4 with large context sizes, and scout runs almost ok, but llama4 as a model is abysmal. I tried to run Qwen3-235B-A22B, but the performance with llama.cpp is pretty terrible, and I haven't been able to get it working with the vllm-pascal (ghcr.io/sasha0552/vllm:latest).&lt;/p&gt; &lt;p&gt;If you have any pointers on getting Qwen3-235B to run with any sort of parallelism, or want me to benchmark any model, just say so!&lt;/p&gt; &lt;p&gt;The MB is a 2014 intel S2600CW with dual 8-core xeons, so CPU performance is rather low. I also tried to use MB with an EPYC, but it doesn't manage to allocate the resources to all PCIe devices.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TooManyPascals"&gt; /u/TooManyPascals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ktiq99"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T12:48:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ku7qe6</id>
    <title>AMD GPU support</title>
    <updated>2025-05-24T09:36:49+00:00</updated>
    <author>
      <name>/u/Fade_Yeti</name>
      <uri>https://old.reddit.com/user/Fade_Yeti</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all. &lt;/p&gt; &lt;p&gt;I am looking to upgrade the GPU in my server with something with more than 8GB VRAM. How is AMD in the space at the moment in regards to support on linux? &lt;/p&gt; &lt;p&gt;Here are the 3 options:&lt;/p&gt; &lt;p&gt;Radeon RX 7800 XT 16GB&lt;/p&gt; &lt;p&gt;GeForce RTX 4060 Ti 16GB&lt;/p&gt; &lt;p&gt;GeForce RTX 5060 Ti OC 16G&lt;/p&gt; &lt;p&gt;Any advice would be greatly appreciated&lt;/p&gt; &lt;p&gt;EDIT: Thanks for all the advice. I picked up a 4060 Ti 16GB for $370ish&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fade_Yeti"&gt; /u/Fade_Yeti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku7qe6/amd_gpu_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku7qe6/amd_gpu_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ku7qe6/amd_gpu_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T09:36:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktsqit</id>
    <title>Best Vibe Code tools (like Cursor) but are free and use your own local LLM?</title>
    <updated>2025-05-23T19:46:58+00:00</updated>
    <author>
      <name>/u/StartupTim</name>
      <uri>https://old.reddit.com/user/StartupTim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen Cursor and how it works, and it looks pretty cool, but I rather use my own local hosted LLMs and not pay a usage fee to a 3rd party company.&lt;/p&gt; &lt;p&gt;Does anybody know of any good Vibe Coding tools, as good or better than Cursor, that run on your own local LLMs?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;p&gt;EDIT: Especially tools that integrate with ollama's API.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StartupTim"&gt; /u/StartupTim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktsqit/best_vibe_code_tools_like_cursor_but_are_free_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktsqit/best_vibe_code_tools_like_cursor_but_are_free_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktsqit/best_vibe_code_tools_like_cursor_but_are_free_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T19:46:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kufdow</id>
    <title>Best model for captioning?</title>
    <updated>2025-05-24T16:17:39+00:00</updated>
    <author>
      <name>/u/thetobesgeorge</name>
      <uri>https://old.reddit.com/user/thetobesgeorge</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Whatâ€™s the best model right now for captioning pictures?&lt;br /&gt; Iâ€™m just interested in playing around and captioning individual pictures on a one by one basis&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thetobesgeorge"&gt; /u/thetobesgeorge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kufdow/best_model_for_captioning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kufdow/best_model_for_captioning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kufdow/best_model_for_captioning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T16:17:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ku5cfe</id>
    <title>What Models for C/C++?</title>
    <updated>2025-05-24T06:48:07+00:00</updated>
    <author>
      <name>/u/Aroochacha</name>
      <uri>https://old.reddit.com/user/Aroochacha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using unsloth/Qwen2.5-Coder-32B-Instruct-128K-GGUF (int 8.) Worked great for small stuff (one header/.c implementation) moreover it hallucinated when I had it evaluate a kernel api I wrote. (6 files.)&lt;/p&gt; &lt;p&gt;What are people using? I am curious about any model that are good at C. Bonus if they are good at shader code.&lt;/p&gt; &lt;p&gt;I am running a RTX A6000 PRO 96GB card in a Razer Core X. Replaced my 3090 in the TB enclosure. Have a 4090 in the gaming rig.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aroochacha"&gt; /u/Aroochacha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku5cfe/what_models_for_cc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku5cfe/what_models_for_cc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ku5cfe/what_models_for_cc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T06:48:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ku1444</id>
    <title>A Privacy-Focused Perplexity That Runs Locally on Your Phone</title>
    <updated>2025-05-24T02:28:43+00:00</updated>
    <author>
      <name>/u/Ssjultrainstnict</name>
      <uri>https://old.reddit.com/user/Ssjultrainstnict</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku1444/a_privacyfocused_perplexity_that_runs_locally_on/"&gt; &lt;img alt="A Privacy-Focused Perplexity That Runs Locally on Your Phone" src="https://external-preview.redd.it/H2OOCv1bv050E4CBNwcheCR0p5galvx3UpT4d2t0NLs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0dc808e4d075ada1aefee047e33becc1859e20d5" title="A Privacy-Focused Perplexity That Runs Locally on Your Phone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1ku1444/video/e80rh7mb5n2f1/player"&gt;https://reddit.com/link/1ku1444/video/e80rh7mb5n2f1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! ðŸ‘‹&lt;/p&gt; &lt;p&gt;I wanted to share &lt;strong&gt;MyDeviceAI&lt;/strong&gt; - a completely private alternative to Perplexity that runs entirely on your device. If you're tired of your search queries being sent to external servers and want the power of AI search without the privacy trade-offs, this might be exactly what you're looking for.&lt;/p&gt; &lt;h1&gt;What Makes This Different&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Complete Privacy&lt;/strong&gt;: Unlike Perplexity or other AI search tools, MyDeviceAI keeps everything local. Your search queries, the results, and all processing happen on your device. No data leaves your phone, period.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SearXNG Integration&lt;/strong&gt;: The app now comes with built-in SearXNG search - no configuration needed. You get comprehensive search results with image previews, all while maintaining complete privacy. SearXNG aggregates results from multiple search engines without tracking you.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Local AI Processing&lt;/strong&gt;: Powered by Qwen 3, the AI model runs entirely on your device. Modern iPhones get lightning-fast responses, and even older models are fully supported (just a bit slower).&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;100% Free &amp;amp; Open Source&lt;/strong&gt;: Check out the code at &lt;a href="http://github.com/navedmerchant/MyDeviceAI"&gt;MyDeviceAI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Web Search + AI&lt;/strong&gt;: Get the best of both worlds - current information from the web processed by local AI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat History&lt;/strong&gt;: 30+ days of conversation history, all stored locally&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Thinking Mode&lt;/strong&gt;: Complex reasoning capabilities for challenging problems&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero Wait Time&lt;/strong&gt;: Model loads asynchronously in the background&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Personalization&lt;/strong&gt;: Beta feature for custom user contexts&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Recent Updates&lt;/h1&gt; &lt;p&gt;The latest release includes a prettier UI, out-of-the-box SearXNG integration, image previews with search results, and tons of bug fixes.&lt;/p&gt; &lt;p&gt;This app has completely replaced ChatGPT for me, I am a very curious person and keep using it for looking up things that come to my mind, and its always spot on. I also compared it with Perplexity and while Perplexity has a slight edge in some cases, MyDeviceAI generally gives me the correct information and completely to the point. Download at: &lt;a href="https://apps.apple.com/us/app/mydeviceai/id6736578281"&gt;MyDeviceAI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking forward to your feedback. Please leave a review on the AppStore if this worked for you and solved a problem, and if you like to support further development of this App!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ssjultrainstnict"&gt; /u/Ssjultrainstnict &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku1444/a_privacyfocused_perplexity_that_runs_locally_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku1444/a_privacyfocused_perplexity_that_runs_locally_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ku1444/a_privacyfocused_perplexity_that_runs_locally_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T02:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuf20u</id>
    <title>Best small model for code auto-completion?</title>
    <updated>2025-05-24T16:03:36+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am currently using the &lt;a href="http://continue.dev"&gt;continue.dev&lt;/a&gt; extension for VS Code. I want to use a small model for code autocompletion, something that is 3B or less as I intend to run it locally using llama.cpp (no gpu).&lt;/p&gt; &lt;p&gt;What would be a good model for such a use case?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuf20u/best_small_model_for_code_autocompletion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuf20u/best_small_model_for_code_autocompletion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuf20u/best_small_model_for_code_autocompletion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T16:03:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kty4mh</id>
    <title>Anyone else prefering non thinking models ?</title>
    <updated>2025-05-23T23:50:26+00:00</updated>
    <author>
      <name>/u/StandardLovers</name>
      <uri>https://old.reddit.com/user/StandardLovers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So far Ive experienced non CoT models to have more curiosity and asking follow up questions. Like gemma3 or qwen2.5 72b. Tell them about something and they ask follow up questions, i think CoT models ask them selves all the questions and end up very confident. I also understand the strength of CoT models for problem solving, and perhaps thats where their strength is.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StandardLovers"&gt; /u/StandardLovers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kty4mh/anyone_else_prefering_non_thinking_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kty4mh/anyone_else_prefering_non_thinking_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kty4mh/anyone_else_prefering_non_thinking_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T23:50:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kudhxg</id>
    <title>Cosmos-Reason1: Physical AI Common Sense and Embodied Reasoning Models</title>
    <updated>2025-05-24T14:55:18+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Cosmos-Reason1-7B"&gt;https://huggingface.co/nvidia/Cosmos-Reason1-7B&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Description:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Cosmos-Reason1 Models&lt;/strong&gt;: Physical AI models understand physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.&lt;/p&gt; &lt;p&gt;The Cosmos-Reason1 models are post-trained with physical common sense and embodied reasoning data with supervised fine-tuning and reinforcement learning. These are Physical AI models that can understand space, time, and fundamental physics, and can serve as planning models to reason about the next steps of an embodied agent.&lt;/p&gt; &lt;p&gt;The models are ready for commercial use.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It's based on Qwen2.5 VL&lt;/p&gt; &lt;p&gt;ggufs already available:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/models?other=base_model:quantized:nvidia/Cosmos-Reason1-7B"&gt;https://huggingface.co/models?other=base_model:quantized:nvidia/Cosmos-Reason1-7B&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kudhxg/cosmosreason1_physical_ai_common_sense_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kudhxg/cosmosreason1_physical_ai_common_sense_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kudhxg/cosmosreason1_physical_ai_common_sense_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T14:55:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktlz3w</id>
    <title>96GB VRAM! What should run first?</title>
    <updated>2025-05-23T15:10:20+00:00</updated>
    <author>
      <name>/u/Mother_Occasion_8076</name>
      <uri>https://old.reddit.com/user/Mother_Occasion_8076</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktlz3w/96gb_vram_what_should_run_first/"&gt; &lt;img alt="96GB VRAM! What should run first?" src="https://preview.redd.it/co0zhh06sj2f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64b43f0124c5d5b397b2efd848e6e83c1dcfcfdc" title="96GB VRAM! What should run first?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had to make a fake company domain name to order this from a supplier. They wouldnâ€™t even give me a quote with my Gmail address. I got the card though!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mother_Occasion_8076"&gt; /u/Mother_Occasion_8076 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/co0zhh06sj2f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktlz3w/96gb_vram_what_should_run_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktlz3w/96gb_vram_what_should_run_first/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T15:10:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuejfp</id>
    <title>New gemma 3n is amazing, wish they suported pc gpu inference</title>
    <updated>2025-05-24T15:41:07+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there at least a workaround to run .task models on pc? Works great on my android phone but id love to play around and deploy it on a local server&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuejfp/new_gemma_3n_is_amazing_wish_they_suported_pc_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuejfp/new_gemma_3n_is_amazing_wish_they_suported_pc_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuejfp/new_gemma_3n_is_amazing_wish_they_suported_pc_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T15:41:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kua2u0</id>
    <title>On the go native GPU inference and chatting with Gemma 3n E4B on an old S21 Ultra Snapdragon!</title>
    <updated>2025-05-24T12:07:15+00:00</updated>
    <author>
      <name>/u/lets_theorize</name>
      <uri>https://old.reddit.com/user/lets_theorize</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kua2u0/on_the_go_native_gpu_inference_and_chatting_with/"&gt; &lt;img alt="On the go native GPU inference and chatting with Gemma 3n E4B on an old S21 Ultra Snapdragon!" src="https://preview.redd.it/elvym2oe0q2f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0b066e579f5d26e7e58dbeb2bdf0effb4c91efc" title="On the go native GPU inference and chatting with Gemma 3n E4B on an old S21 Ultra Snapdragon!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lets_theorize"&gt; /u/lets_theorize &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/elvym2oe0q2f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kua2u0/on_the_go_native_gpu_inference_and_chatting_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kua2u0/on_the_go_native_gpu_inference_and_chatting_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T12:07:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ku8861</id>
    <title>MCP server to connect LLM agents to any database</title>
    <updated>2025-05-24T10:10:30+00:00</updated>
    <author>
      <name>/u/RaeudigerRaffi</name>
      <uri>https://old.reddit.com/user/RaeudigerRaffi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, my startup sadly failed, so I decided to convert it to an open source project since we actually built alot of internal tools. The result is todays release &lt;a href="https://github.com/raeudigerRaeffi/turbular"&gt;Turbular&lt;/a&gt;. Turbular is an MCP server under the MIT license that allows you to connect your LLM agent to any database. Additional features are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Schema normalizes: translates schemas into proper naming conventions (LLMs perform very poorly on non standard schema naming conventions)&lt;/li&gt; &lt;li&gt;Query optimization: optimizes your LLM generated queries and renormalizes them&lt;/li&gt; &lt;li&gt;Security: All your queries (except for Bigquery) are run with autocommit off meaning your LLM agent can not wreak havoc on your database&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know what you think and I would be happy about any suggestions in which direction to move this project&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RaeudigerRaffi"&gt; /u/RaeudigerRaffi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku8861/mcp_server_to_connect_llm_agents_to_any_database/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku8861/mcp_server_to_connect_llm_agents_to_any_database/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ku8861/mcp_server_to_connect_llm_agents_to_any_database/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T10:10:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ku95nk</id>
    <title>LLM long-term memory improvement.</title>
    <updated>2025-05-24T11:12:20+00:00</updated>
    <author>
      <name>/u/Dem0lari</name>
      <uri>https://old.reddit.com/user/Dem0lari</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working on a concept for a node-based memory architecture for LLMs, inspired by cognitive maps, biological memory networks, and graph-based data storage.&lt;/p&gt; &lt;p&gt;Instead of treating memory as a flat log or embedding space, this system stores contextual knowledge as a web of tagged nodes, connected semantically. Each node contains small, modular pieces of memory (like past conversation fragments, facts, or concepts) and metadata like topic, source, or character reference (in case of storytelling use). This structure allows LLMs to selectively retrieve relevant context without scanning the entire conversation history, potentially saving tokens and improving relevance.&lt;/p&gt; &lt;p&gt;I've documented the concept and included an example in this repo:&lt;/p&gt; &lt;p&gt;ðŸ”— &lt;a href="https://github.com/Demolari/node-memory-system"&gt;https://github.com/Demolari/node-memory-system&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love to hear feedback, criticism, or any related ideas. Do you think something like this could enhance the memory capabilities of current or future LLMs?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dem0lari"&gt; /u/Dem0lari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku95nk/llm_longterm_memory_improvement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku95nk/llm_longterm_memory_improvement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ku95nk/llm_longterm_memory_improvement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T11:12:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ku6wol</id>
    <title>How much VRAM would even a smaller model take to get 1 million context model like Gemini 2.5 flash/pro?</title>
    <updated>2025-05-24T08:38:08+00:00</updated>
    <author>
      <name>/u/TumbleweedDeep825</name>
      <uri>https://old.reddit.com/user/TumbleweedDeep825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to convince myself not to waste money on a localLLM setup that I don't need since gemini 2.5 flash is cheaper and probably faster than anything I could build.&lt;/p&gt; &lt;p&gt;Let's say 1 million context is impossible. What about 200k context?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TumbleweedDeep825"&gt; /u/TumbleweedDeep825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku6wol/how_much_vram_would_even_a_smaller_model_take_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku6wol/how_much_vram_would_even_a_smaller_model_take_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ku6wol/how_much_vram_would_even_a_smaller_model_take_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T08:38:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kug045</id>
    <title>Cua : Docker Container for Computer Use Agents</title>
    <updated>2025-05-24T16:44:45+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kug045/cua_docker_container_for_computer_use_agents/"&gt; &lt;img alt="Cua : Docker Container for Computer Use Agents" src="https://external-preview.redd.it/bnFrNDRtOXdkcjJmMcsvHa0C_XuOSkhUSfxPH2wNUS_IzERNrp7qS2qcV3Nx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15606b7fbeffdaf14d86c1b274034d173ab85280" title="Cua : Docker Container for Computer Use Agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cua is the Docker for Computer-Use Agent, an open-source framework that enables AI agents to control full operating systems within high-performance, lightweight virtual containers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2ibhpziwdr2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kug045/cua_docker_container_for_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kug045/cua_docker_container_for_computer_use_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T16:44:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktzwgq</id>
    <title>Ollama finally acknowledged llama.cpp officially</title>
    <updated>2025-05-24T01:22:35+00:00</updated>
    <author>
      <name>/u/simracerman</name>
      <uri>https://old.reddit.com/user/simracerman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the 0.7.1 release, they introduce the capabilities of their multimodal engine. At the end in the &lt;a href="https://imgur.com/a/zKMizcr"&gt;acknowledgments section&lt;/a&gt; they thanked the GGML project.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/blog/multimodal-models"&gt;https://ollama.com/blog/multimodal-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simracerman"&gt; /u/simracerman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktzwgq/ollama_finally_acknowledged_llamacpp_officially/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktzwgq/ollama_finally_acknowledged_llamacpp_officially/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktzwgq/ollama_finally_acknowledged_llamacpp_officially/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T01:22:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktx15j</id>
    <title>Guys! I managed to build a 100% fully local voice AI with Ollama that can have full conversations, control all my smart devices AND now has both short term + long term memory. ðŸ¤˜</title>
    <updated>2025-05-23T22:56:42+00:00</updated>
    <author>
      <name>/u/RoyalCities</name>
      <uri>https://old.reddit.com/user/RoyalCities</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktx15j/guys_i_managed_to_build_a_100_fully_local_voice/"&gt; &lt;img alt="Guys! I managed to build a 100% fully local voice AI with Ollama that can have full conversations, control all my smart devices AND now has both short term + long term memory. ðŸ¤˜" src="https://external-preview.redd.it/b3A3aWt5dmIzbTJmMSKAZduYkWK-j-eA22aXbm6vzflALmDerWrgdNPvGQZJ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=179ccde64a277eb295ce8f54c6f88facef7ddb65" title="Guys! I managed to build a 100% fully local voice AI with Ollama that can have full conversations, control all my smart devices AND now has both short term + long term memory. ðŸ¤˜" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found out recently that Amazon/Alexa is going to use ALL users vocal data with ZERO opt outs for their new Alexa+ service so I decided to build my own that is 1000x better and runs fully local. &lt;/p&gt; &lt;p&gt;The stack uses Home Assistant directly tied into Ollama. The long and short term memory is a custom automation design that I'll be documenting soon and providing for others. &lt;/p&gt; &lt;p&gt;This entire set up runs 100% local and you could probably get away with the whole thing working within / under 16 gigs of VRAM. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RoyalCities"&gt; /u/RoyalCities &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/iigum5tb3m2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktx15j/guys_i_managed_to_build_a_100_fully_local_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktx15j/guys_i_managed_to_build_a_100_fully_local_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T22:56:42+00:00</published>
  </entry>
</feed>
