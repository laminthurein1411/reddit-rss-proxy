<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-29T12:45:33+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ibzmef</id>
    <title>New bomb dropped from asian researchers: YuE: Open Music Foundation Models for Full-Song Generation</title>
    <updated>2025-01-28T11:37:51+00:00</updated>
    <author>
      <name>/u/wayl</name>
      <uri>https://old.reddit.com/user/wayl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only few days ago a &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; user was going to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ia40om/would_give_up_a_kidney_for_a_local_audio_model/"&gt;give away a kidney&lt;/a&gt; for this.&lt;/p&gt; &lt;p&gt;YuE is an open-source project by HKUST tackling the challenge of generating full-length songs from lyrics (lyrics2song). Unlike existing models limited to short clips, YuE can produce 5-minute songs with coherent vocals and accompaniment. Key innovations include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A semantically enhanced audio tokenizer for efficient training.&lt;/li&gt; &lt;li&gt;Dual-token technique for synced vocal-instrumental modeling.&lt;/li&gt; &lt;li&gt;Lyrics-chain-of-thoughts for progressive song generation.&lt;/li&gt; &lt;li&gt;Support for diverse genres, languages, and advanced vocal techniques (e.g., scatting, death growl).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out the &lt;a href="https://github.com/multimodal-art-projection/YuE"&gt;GitHub repo&lt;/a&gt; for demos and model checkpoints.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wayl"&gt; /u/wayl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibzmef/new_bomb_dropped_from_asian_researchers_yue_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibzmef/new_bomb_dropped_from_asian_researchers_yue_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibzmef/new_bomb_dropped_from_asian_researchers_yue_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T11:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1icphqa</id>
    <title>How to run deepseek r1 on 4xH100</title>
    <updated>2025-01-29T08:45:26+00:00</updated>
    <author>
      <name>/u/sanjay920</name>
      <uri>https://old.reddit.com/user/sanjay920</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/sanjay920/run_deepseek_r1"&gt;https://github.com/sanjay920/run_deepseek_r1&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Throughput Achieved&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;DeepSeek R1 running on a 4√óH100 setup reached a generation rate of &lt;strong&gt;25 tokens/second&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Over an hour, that amounts to &lt;strong&gt;90,000 output tokens&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Compute Costs on Lambda Cloud&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Running 4√óH100 GPUs on Lambda Cloud costs &lt;strong&gt;$12.36 per hour&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Generating 90k tokens in one hour results in an estimated &lt;strong&gt;$137 per 1 million tokens&lt;/strong&gt; (based on 11.1 hours needed to generate 1M tokens).&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Comparison to OpenAI O1 Pricing&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;OpenAI O1 charges &lt;strong&gt;$60 per 1 million output tokens&lt;/strong&gt;, making it roughly &lt;strong&gt;2√ó to 2.5√ó cheaper&lt;/strong&gt; than this self-hosted setup at the current throughput.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sanjay920"&gt; /u/sanjay920 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icphqa/how_to_run_deepseek_r1_on_4xh100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icphqa/how_to_run_deepseek_r1_on_4xh100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icphqa/how_to_run_deepseek_r1_on_4xh100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T08:45:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic8kpf</id>
    <title>What if releasing R1 is a 4D chess move by a Quant firm?</title>
    <updated>2025-01-28T18:34:23+00:00</updated>
    <author>
      <name>/u/Dull_Art6802</name>
      <uri>https://old.reddit.com/user/Dull_Art6802</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hear me out, Quant firms are know to analyze every detail possible in order to predict the future stock prices. I am talking using satellites to observe how many cars come of from Tesla factories, divorce rates, container ship movements, how many times you flush your toilet etc ridiculous details. &lt;/p&gt; &lt;p&gt;Now DeepSeek is owned by such a Quant firm and I find it impossible to believe that they at least did not have some idea what R1's release could cause on the market, so what if before releasing R1 they bought a lot of put options on NVDA and then by releasing the model they crashed Nvidia stock netting in a couple billion USD.&lt;/p&gt; &lt;p&gt;These people might be 4 parallel dimensions ahead of us lol.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dull_Art6802"&gt; /u/Dull_Art6802 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8kpf/what_if_releasing_r1_is_a_4d_chess_move_by_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8kpf/what_if_releasing_r1_is_a_4d_chess_move_by_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8kpf/what_if_releasing_r1_is_a_4d_chess_move_by_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T18:34:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic03lx</id>
    <title>DeepSeek is running inference on the new home Chinese chips made by Huawei, the 910C</title>
    <updated>2025-01-28T12:08:07+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic03lx/deepseek_is_running_inference_on_the_new_home/"&gt; &lt;img alt="DeepSeek is running inference on the new home Chinese chips made by Huawei, the 910C" src="https://b.thumbs.redditmedia.com/Squ2MR8UElQKTlUEoWhmDAJq100Xox0Tn99gOS2k4QM.jpg" title="DeepSeek is running inference on the new home Chinese chips made by Huawei, the 910C" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Alexander Doria on X: &lt;em&gt;I feel this should be a much bigger story: DeepSeek has trained on Nvidia H800 but is running inference on the new home Chinese chips made by Huawei, the 910C.&lt;/em&gt;: &lt;a href="https://x.com/Dorialexander/status/1884167945280278857"&gt;https://x.com/Dorialexander/status/1884167945280278857&lt;/a&gt;&lt;br /&gt; Original source: Zephyr: &lt;em&gt;HUAWEI&lt;/em&gt;: &lt;a href="https://x.com/angelusm0rt1s/status/1884154694123298904"&gt;https://x.com/angelusm0rt1s/status/1884154694123298904&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sfzjno0q6qfe1.jpg?width=506&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ae0c800b9fffea55bc7861f583160795e935c07d"&gt;https://preview.redd.it/sfzjno0q6qfe1.jpg?width=506&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ae0c800b9fffea55bc7861f583160795e935c07d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Partial translation:&lt;br /&gt; &lt;em&gt;In Huawei Cloud&lt;/em&gt;&lt;br /&gt; &lt;em&gt;ModelArts Studio (MaaS) Model-as-a-Service Platform&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Ascend-Adapted New Model is Here!&lt;/em&gt;&lt;br /&gt; &lt;em&gt;DeepSeek-R1-Distill&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Qwen-14B, Qwen-32B, and Llama-8B have been launched.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;More models coming soon.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic03lx/deepseek_is_running_inference_on_the_new_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic03lx/deepseek_is_running_inference_on_the_new_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic03lx/deepseek_is_running_inference_on_the_new_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T12:08:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic9wi6</id>
    <title>Block released a new open source AI agent called Goose. It can do more than coding for engineers üëÄ</title>
    <updated>2025-01-28T19:27:42+00:00</updated>
    <author>
      <name>/u/emreloperr</name>
      <uri>https://old.reddit.com/user/emreloperr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic9wi6/block_released_a_new_open_source_ai_agent_called/"&gt; &lt;img alt="Block released a new open source AI agent called Goose. It can do more than coding for engineers üëÄ" src="https://external-preview.redd.it/7ekVDB698j31gkjO7h0OU3gtvIyHbMD1tb8PRnYlUV8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a32c6085097dca9063e7efe673aa54d3a944b2c2" title="Block released a new open source AI agent called Goose. It can do more than coding for engineers üëÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emreloperr"&gt; /u/emreloperr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://block.github.io/goose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic9wi6/block_released_a_new_open_source_ai_agent_called/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic9wi6/block_released_a_new_open_source_ai_agent_called/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T19:27:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1icsk3w</id>
    <title>3 new reasoning datasets using R1 - High-quality CoTs (from Maxime Labonne on X)</title>
    <updated>2025-01-29T12:23:04+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icsk3w/3_new_reasoning_datasets_using_r1_highquality/"&gt; &lt;img alt="3 new reasoning datasets using R1 - High-quality CoTs (from Maxime Labonne on X)" src="https://external-preview.redd.it/pXALc_V-tAIQTXDrrHGn0OLNGuLlNPdMvDRZSfsMWeQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25de674d750578ef139c2374aa784ae1110fd4b9" title="3 new reasoning datasets using R1 - High-quality CoTs (from Maxime Labonne on X)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bespoke-Stratos-17k: &lt;a href="https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k"&gt;https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k&lt;/a&gt;&lt;br /&gt; OpenThoughts-114k: &lt;a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k"&gt;https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k&lt;/a&gt;&lt;br /&gt; R1-Distill-SFT (1.8M samples): &lt;a href="https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT"&gt;https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT&lt;/a&gt;&lt;br /&gt; Maxime Labonne: &lt;em&gt;Dataset review ?? Lots of new reasoning datasets but very few use R1 yet&lt;/em&gt;.: &lt;a href="https://x.com/maximelabonne/status/1884565062708543572"&gt;https://x.com/maximelabonne/status/1884565062708543572&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/t5oftpp9exfe1.png?width=950&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=994e0594e9860fedec383b1acde4cc4e6a248b7b"&gt;https://preview.redd.it/t5oftpp9exfe1.png?width=950&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=994e0594e9860fedec383b1acde4cc4e6a248b7b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icsk3w/3_new_reasoning_datasets_using_r1_highquality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icsk3w/3_new_reasoning_datasets_using_r1_highquality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icsk3w/3_new_reasoning_datasets_using_r1_highquality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T12:23:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1icc5hq</id>
    <title>DeepSeek R1 671B running on 2 M2 Ultras faster than reading speed</title>
    <updated>2025-01-28T20:59:13+00:00</updated>
    <author>
      <name>/u/noblex33</name>
      <uri>https://old.reddit.com/user/noblex33</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icc5hq/deepseek_r1_671b_running_on_2_m2_ultras_faster/"&gt; &lt;img alt="DeepSeek R1 671B running on 2 M2 Ultras faster than reading speed" src="https://external-preview.redd.it/T3dwFxBeTh14I0gRGwTgWSW_ZOjtsyWdThn4yOyUB0o.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3e486673640592fed5e6e7beea3c8482611b687" title="DeepSeek R1 671B running on 2 M2 Ultras faster than reading speed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noblex33"&gt; /u/noblex33 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/awnihannun/status/1881412271236346233"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icc5hq/deepseek_r1_671b_running_on_2_m2_ultras_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icc5hq/deepseek_r1_671b_running_on_2_m2_ultras_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T20:59:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic4czy</id>
    <title>Qwen2.5-Max</title>
    <updated>2025-01-28T15:41:07+00:00</updated>
    <author>
      <name>/u/Either-Job-341</name>
      <uri>https://old.reddit.com/user/Either-Job-341</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Another chinese model release, lol. They say it's on par with DeepSeek V3.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/Qwen/Qwen2.5-Max-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen2.5-Max-Demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Either-Job-341"&gt; /u/Either-Job-341 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic4czy/qwen25max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic4czy/qwen25max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic4czy/qwen25max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T15:41:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1icq2mf</id>
    <title>I have a budget of 40k USD I need to setup machine to host deepseek r1 - what options do I have</title>
    <updated>2025-01-29T09:30:39+00:00</updated>
    <author>
      <name>/u/zibenmoka</name>
      <uri>https://old.reddit.com/user/zibenmoka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, &lt;/p&gt; &lt;p&gt;looking for some tips/directions on hardware choice to host deepseek r1 locally (my budget is up to 40k) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zibenmoka"&gt; /u/zibenmoka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icq2mf/i_have_a_budget_of_40k_usd_i_need_to_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icq2mf/i_have_a_budget_of_40k_usd_i_need_to_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icq2mf/i_have_a_budget_of_40k_usd_i_need_to_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T09:30:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1icku59</id>
    <title>Hugging Face wants to reverse-engineer DeepSeek‚Äôs R1</title>
    <updated>2025-01-29T03:39:59+00:00</updated>
    <author>
      <name>/u/etherd0t</name>
      <uri>https://old.reddit.com/user/etherd0t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icku59/hugging_face_wants_to_reverseengineer_deepseeks_r1/"&gt; &lt;img alt="Hugging Face wants to reverse-engineer DeepSeek‚Äôs R1" src="https://external-preview.redd.it/ZliqvWOhcshla4bACNsdeiO3AvnKoD4VRcBkZC2miYE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bfe10219195ebf4681d234a1326d7b71fa1f5efd" title="Hugging Face wants to reverse-engineer DeepSeek‚Äôs R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/etherd0t"&gt; /u/etherd0t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://siliconangle.com/2025/01/28/hugging-face-wants-reverse-engineer-deepseeks-r1-reasoning-model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icku59/hugging_face_wants_to_reverseengineer_deepseeks_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icku59/hugging_face_wants_to_reverseengineer_deepseeks_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T03:39:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic3k3b</id>
    <title>No censorship when running Deepseek locally.</title>
    <updated>2025-01-28T15:05:21+00:00</updated>
    <author>
      <name>/u/ISNT_A_ROBOT</name>
      <uri>https://old.reddit.com/user/ISNT_A_ROBOT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic3k3b/no_censorship_when_running_deepseek_locally/"&gt; &lt;img alt="No censorship when running Deepseek locally." src="https://preview.redd.it/95fhiv1e2rfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c23d5ab8c71a12862078dfeeb51d4785b2a9bb58" title="No censorship when running Deepseek locally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ISNT_A_ROBOT"&gt; /u/ISNT_A_ROBOT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/95fhiv1e2rfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic3k3b/no_censorship_when_running_deepseek_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic3k3b/no_censorship_when_running_deepseek_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T15:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic61zb</id>
    <title>"Sir, China just released another model"</title>
    <updated>2025-01-28T16:52:39+00:00</updated>
    <author>
      <name>/u/danilofs</name>
      <uri>https://old.reddit.com/user/danilofs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic61zb/sir_china_just_released_another_model/"&gt; &lt;img alt="&amp;quot;Sir, China just released another model&amp;quot;" src="https://a.thumbs.redditmedia.com/hlgnuSLD8D8wJmgHtUGXn38QzQ7a2xkFvpX65Gj6yM0.jpg" title="&amp;quot;Sir, China just released another model&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The burst of DeepSeek V3 has attracted attention from the whole AI community to large-scale MoE models. Concurrently, they have built Qwen2.5-Max, a large MoE LLM pretrained on massive data and post-trained with curated SFT and RLHF recipes. It achieves competitive performance against the top-tier models, and outcompetes DeepSeek V3 in benchmarks like Arena Hard, LiveBench, LiveCodeBench, GPQA-Diamond. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6f0byi66lrfe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e94fa7c2356d596c2d472e3f13adf2a792368255"&gt;https://preview.redd.it/6f0byi66lrfe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e94fa7c2356d596c2d472e3f13adf2a792368255&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danilofs"&gt; /u/danilofs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic61zb/sir_china_just_released_another_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic61zb/sir_china_just_released_another_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic61zb/sir_china_just_released_another_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T16:52:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1icr6md</id>
    <title>How come we dont see many people spinning up R1 671b in the cloud, selling access and making bank?</title>
    <updated>2025-01-29T10:53:59+00:00</updated>
    <author>
      <name>/u/linkcharger</name>
      <uri>https://old.reddit.com/user/linkcharger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What am I missing? I'm not too knowledgeable about deploying big models like these, but for people that are, shouldn't it be quite easy to deploy it in the cloud? &lt;/p&gt; &lt;p&gt;That's the cool thing about open weights, no? If you have the hardware (which is nothing crazy if you're already using VPS), you can run and scale it dynamically.&lt;/p&gt; &lt;p&gt;And since it's so efficient, it should be quite cheap when spread out over several users. Why aren't we seeing everyone and their grandma selling us a subscription to their website?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/linkcharger"&gt; /u/linkcharger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icr6md/how_come_we_dont_see_many_people_spinning_up_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icr6md/how_come_we_dont_see_many_people_spinning_up_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icr6md/how_come_we_dont_see_many_people_spinning_up_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T10:53:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibxj3a</id>
    <title>Trump to impose 25% to 100% tariffs on Taiwan-made chips, impacting TSMC</title>
    <updated>2025-01-28T09:04:57+00:00</updated>
    <author>
      <name>/u/noblex33</name>
      <uri>https://old.reddit.com/user/noblex33</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxj3a/trump_to_impose_25_to_100_tariffs_on_taiwanmade/"&gt; &lt;img alt="Trump to impose 25% to 100% tariffs on Taiwan-made chips, impacting TSMC" src="https://external-preview.redd.it/AH_s6Lnngj4fg7u4p7ikli1G9UIpzFPfjMk_755j9_E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93f0138e6b1b669eee32d0888eddee9317da1a1b" title="Trump to impose 25% to 100% tariffs on Taiwan-made chips, impacting TSMC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noblex33"&gt; /u/noblex33 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/tech-industry/trump-to-impose-25-percent-100-percent-tariffs-on-taiwan-made-chips-impacting-tsmc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxj3a/trump_to_impose_25_to_100_tariffs_on_taiwanmade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxj3a/trump_to_impose_25_to_100_tariffs_on_taiwanmade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T09:04:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic8cjf</id>
    <title>$6,000 computer to run Deepseek R1 670B Q8 locally at 6-8 tokens/sec</title>
    <updated>2025-01-28T18:25:13+00:00</updated>
    <author>
      <name>/u/MoltenBoron</name>
      <uri>https://old.reddit.com/user/MoltenBoron</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just saw this on X/Twitter: Tower PC with 2 AMD EPYC CPUs and 24 x 32GB DDR5-RDIMM. No GPUs. 400 W power consumption.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Complete hardware + software setup for running Deepseek-R1 locally. The actual model, no distillations, and Q8 quantization for full quality. Total cost, $6,000. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://x.com/carrigmat/status/1884244369907278106"&gt;https://x.com/carrigmat/status/1884244369907278106&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Alternative link (no login):&lt;/p&gt; &lt;p&gt;&lt;a href="https://threadreaderapp.com/thread/1884244369907278106.html"&gt;https://threadreaderapp.com/thread/1884244369907278106.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoltenBoron"&gt; /u/MoltenBoron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8cjf/6000_computer_to_run_deepseek_r1_670b_q8_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8cjf/6000_computer_to_run_deepseek_r1_670b_q8_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8cjf/6000_computer_to_run_deepseek_r1_670b_q8_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T18:25:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic7hts</id>
    <title>Everyone and their mother knows about DeepSeek</title>
    <updated>2025-01-28T17:50:32+00:00</updated>
    <author>
      <name>/u/siegevjorn</name>
      <uri>https://old.reddit.com/user/siegevjorn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone I interact talks about deepseek now. How it's scary, how it's better than Chatgpt, how it's open-source...&lt;/p&gt; &lt;p&gt;But the fact is, 99.9% of these people (including myself) have no way to run 670b model (which actually is the model in hype) in manner that benefit from open-source. I mean just using their front end is no different than using chatGPT. And chatGPT and cluade have, free versions, which evidently are better!&lt;/p&gt; &lt;p&gt;Heck, I hear news reporters talking about how great it is because it works freakishly well and it is an open-source. But in reality, its just open weight, no one have yet to replicate what they did. &lt;/p&gt; &lt;p&gt;But why all the hype? Don't you feel this is too much? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/siegevjorn"&gt; /u/siegevjorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic7hts/everyone_and_their_mother_knows_about_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic7hts/everyone_and_their_mother_knows_about_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic7hts/everyone_and_their_mother_knows_about_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T17:50:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1icmkmn</id>
    <title>Deepseek banned in my company server (major MBB)</title>
    <updated>2025-01-29T05:18:52+00:00</updated>
    <author>
      <name>/u/Purple_War_837</name>
      <uri>https://old.reddit.com/user/Purple_War_837</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was happily using deepseek web interface along with the dirt cheap api calls. But suddenly I can not use it today. The hype since last couple of days alerted the assholes deciding which llms to use.&lt;br /&gt; I think this trend is going to continue for other big companies as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Purple_War_837"&gt; /u/Purple_War_837 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icmkmn/deepseek_banned_in_my_company_server_major_mbb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icmkmn/deepseek_banned_in_my_company_server_major_mbb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icmkmn/deepseek_banned_in_my_company_server_major_mbb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T05:18:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ichohj</id>
    <title>DeepSeek API: Every Request Is A Timeout :(</title>
    <updated>2025-01-29T01:00:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ichohj/deepseek_api_every_request_is_a_timeout/"&gt; &lt;img alt="DeepSeek API: Every Request Is A Timeout :(" src="https://preview.redd.it/wpmv3ibe0ufe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=449243d847b49583ee6497956bd77db9ec221af7" title="DeepSeek API: Every Request Is A Timeout :(" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wpmv3ibe0ufe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ichohj/deepseek_api_every_request_is_a_timeout/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ichohj/deepseek_api_every_request_is_a_timeout/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T01:00:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1icqzcz</id>
    <title>DeepSeek-R1 evolving a Game of Life pattern really feels like a breakthrough</title>
    <updated>2025-01-29T10:39:01+00:00</updated>
    <author>
      <name>/u/IrisColt</name>
      <uri>https://old.reddit.com/user/IrisColt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icqzcz/deepseekr1_evolving_a_game_of_life_pattern_really/"&gt; &lt;img alt="DeepSeek-R1 evolving a Game of Life pattern really feels like a breakthrough" src="https://external-preview.redd.it/P8lS0kk6BFe2IEo6TxCZd1LVwksc34IkzGTVx_SCc8w.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d74dbe4f1d67cc8b587db9aa01762f26e269bcf" title="DeepSeek-R1 evolving a Game of Life pattern really feels like a breakthrough" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm truly amazed. I've just discovered that DeepSeek-R1 has managed to correctly compute one generation of Conway's Game of Life (starting from a simple five-cell row pattern)‚Äîa first for any LLM I've tested. While it required a significant amount of reasoning (749.31 seconds of thought), the model got it right on the first try. It felt just like using a bazooka to kill a fly (5596 tokens at 7 tk/s).&lt;/p&gt; &lt;p&gt;While this might sound modest, I‚Äôve long viewed this challenge as the ‚Äústrawberry problem‚Äù but on steroids. DeepSeek-R1 had to understand cellular automata rules, visualize a grid, track multiple cells simultaneously, and apply specific survival and birth rules to each position‚Äîall while maintaining spatial reasoning.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vup8iom0vwfe1.png?width=138&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=61bcf0740f9a0b8f6bb64525ce64e293e6253fe4"&gt;Pattern at gen 0.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zgzeawc2vwfe1.png?width=138&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5886ae4cefba04201dd1a847800f0004333f3bbb"&gt;Pattern at gen 1.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Simulate one generation of Conway's Game of Life starting from the following initial configuration: ....... ....... ....... .OOOOO. ....... ....... ....... Use a 7x7 grid for the simulation. Represent alive cells with &amp;quot;O&amp;quot; and dead cells with &amp;quot;.&amp;quot;. Apply the rules of Conway's Game of Life to calculate each generation. Provide diagrams of the initial state, and first generation, in the same format as shown above.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/JTveEkXg"&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; and answer (Pastebin)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Initial state: ....... ....... ....... .OOOOO. ....... ....... .......&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;First generation: ....... ....... ..OOO.. ..OOO.. ..OOO.. ....... .......&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IrisColt"&gt; /u/IrisColt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icqzcz/deepseekr1_evolving_a_game_of_life_pattern_really/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icqzcz/deepseekr1_evolving_a_game_of_life_pattern_really/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icqzcz/deepseekr1_evolving_a_game_of_life_pattern_really/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T10:39:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1icjg39</id>
    <title>Some evidence of DeepSeek being attacked by DDoS has been released!</title>
    <updated>2025-01-29T02:27:21+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icjg39/some_evidence_of_deepseek_being_attacked_by_ddos/"&gt; &lt;img alt="Some evidence of DeepSeek being attacked by DDoS has been released!" src="https://b.thumbs.redditmedia.com/Lszyle7jiuM1pNHqVGd7qcP6bBSMrdPUgCiUZt1sZRY.jpg" title="Some evidence of DeepSeek being attacked by DDoS has been released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/jppuddrxfufe1.png?width=1035&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=156acff91c3d3055b63a4ba7cc0e90ac54db4216"&gt;In the first phase, on January 3, 4, 6, 7, and 13, there were suspected HTTP proxy attacks.During this period, Xlab could see a large number of proxy requests to link DeepSeek through proxies, which were likely HTTP proxy attacks.In the second phase, on January 20, 22-26, the attack method changed to SSDP and NTP reflection amplification.During this period, the main attack methods detected by XLab were SSDP and NTP reflection amplification, and a small number of HTTP proxy attacks. Usually, the defense of SSDP and NTP reflection amplification attacks is simple and easy to clean up.In the third phase, on January 27 and 28, the number of attacks increased sharply, and the means changed to application layer attacks.Starting from the 27th, the main attack method discovered by XLab changed to HTTP proxy attacks. Attacking such application layer attacks simulates normal user behavior, which is significantly more difficult to defend than classic SSDP and NTP reflection amplification attacks, so it is more effective.XLab also found that the peak of the attack on January 28 occurred between 03:00-04:00 Beijing time (UTC+8), which corresponds to 14:00-15:00 Eastern Standard Time (UTC-5) in North America. This time window selection shows that the attack has border characteristics, and it does not rule out the purpose of targeted attacks on overseas service providers.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sbm25zfyfufe1.png?width=1035&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b70b8ecd569c5f7c95f8f9ca2a802b8871c1f0c6"&gt;this DDoS attack was accompanied by a large number of brute force attacks. All the brute force attack IPs came from the United States. XLab's data can identify that half of these IPs are VPN exits, and it is speculated that this may be caused by DeepSeek's overseas restrictions on mobile phone users.03DeepSeek responded promptly and minimized the impactFaced with the sudden escalation of large-scale DDoS attacks late at night on the 27th and 28th, DeepSeek responded and handled it immediately. Based on the passivedns data of the big network, XLab saw that DeepSeek switched IP at 00:58 on the morning of the 28th when the attacker launched an effective and destructive HTTP proxy attack. This switching time is consistent with Deepseek's own announcement time in the screenshot above, which should be for better security defense. This also further proves XLab's own judgment on this DDoS attack.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Starting at 03:00 on January 28, the DDoS attack was accompanied by a large number of brute force attacks. All brute force attack IPs come from the United States.&lt;/p&gt; &lt;p&gt;source: &lt;a href="https://club.6parkbbs.com/military/index.php?app=forum&amp;amp;act=threadview&amp;amp;tid=18616721"&gt;https://club.6parkbbs.com/military/index.php?app=forum&amp;amp;act=threadview&amp;amp;tid=18616721&lt;/a&gt; (only Chinese text)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icjg39/some_evidence_of_deepseek_being_attacked_by_ddos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icjg39/some_evidence_of_deepseek_being_attacked_by_ddos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icjg39/some_evidence_of_deepseek_being_attacked_by_ddos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T02:27:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ichk40</id>
    <title>So much DeepSeek fear mongering</title>
    <updated>2025-01-29T00:54:17+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ichk40/so_much_deepseek_fear_mongering/"&gt; &lt;img alt="So much DeepSeek fear mongering" src="https://preview.redd.it/zfykihriztfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9e955a87992919d24e54df67893449f4eab310d" title="So much DeepSeek fear mongering" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How are so many people, who have no idea what they're talking about dominating the stage about deep seek?&lt;/p&gt; &lt;p&gt;Stuff like this. WTF &lt;a href="https://www.linkedin.com/posts/roch-mamenas-4714a979_deepseek-as-a-trojan-horse-threat-deepseek-activity-7288965743507894272-xvNq"&gt;https://www.linkedin.com/posts/roch-mamenas-4714a979_deepseek-as-a-trojan-horse-threat-deepseek-activity-7288965743507894272-xvNq&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zfykihriztfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ichk40/so_much_deepseek_fear_mongering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ichk40/so_much_deepseek_fear_mongering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T00:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1icaq2z</id>
    <title>DeepSeek's AI breakthrough bypasses Nvidia's industry-standard CUDA, uses assembly-like PTX programming instead</title>
    <updated>2025-01-28T20:00:18+00:00</updated>
    <author>
      <name>/u/Slasher1738</name>
      <uri>https://old.reddit.com/user/Slasher1738</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This level of optimization is nuts but would definitely allow them to eek out more performance at a lower cost. &lt;a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead"&gt;https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;DeepSeek made quite a splash in the AI industry by training its Mixture-of-Experts (MoE) language model with 671 billion parameters &lt;a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/chinese-ai-company-says-breakthroughs-enabled-creating-a-leading-edge-ai-model-with-11x-less-compute-deepseeks-optimizations-highlight-limits-of-us-sanctions"&gt;using a cluster featuring 2,048 Nvidia H800 GPUs in about two months&lt;/a&gt;, showing 10X higher efficiency than AI industry leaders like Meta. The breakthrough was achieved by implementing tons of fine-grained optimizations and usage of assembly-like PTX (Parallel Thread Execution) programming instead of Nvidia's CUDA, according to an analysis from Mirae Asset Securities Korea cited by &lt;a href="https://x.com/Jukanlosreve/status/1883304958432624881"&gt;u/Jukanlosreve&lt;/a&gt;. &lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slasher1738"&gt; /u/Slasher1738 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icaq2z/deepseeks_ai_breakthrough_bypasses_nvidias/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icaq2z/deepseeks_ai_breakthrough_bypasses_nvidias/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icaq2z/deepseeks_ai_breakthrough_bypasses_nvidias/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T20:00:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1icmxb5</id>
    <title>4D Chess by the DeepSeek CEO</title>
    <updated>2025-01-29T05:40:12+00:00</updated>
    <author>
      <name>/u/HippoNut</name>
      <uri>https://old.reddit.com/user/HippoNut</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Liang Wenfeng: &amp;quot;I&lt;strong&gt;n the face of disruptive technologies, moats created by closed source are temporary. Even OpenAI‚Äôs closed source approach can‚Äôt prevent others from catching up&lt;/strong&gt;. S&lt;strong&gt;o we anchor our value in our team ‚Äî our colleagues grow through this process, accumulate know-how, and form an organization and culture capable of innovation. That‚Äôs our moat.&lt;/strong&gt;&amp;quot;&lt;br /&gt; Source: &lt;a href="https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas"&gt;https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HippoNut"&gt; /u/HippoNut &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icmxb5/4d_chess_by_the_deepseek_ceo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icmxb5/4d_chess_by_the_deepseek_ceo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icmxb5/4d_chess_by_the_deepseek_ceo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T05:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1icsa5o</id>
    <title>PSA: your 7B/14B/32B/70B "R1" is NOT DeepSeek.</title>
    <updated>2025-01-29T12:06:25+00:00</updated>
    <author>
      <name>/u/Zalathustra</name>
      <uri>https://old.reddit.com/user/Zalathustra</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's not even an MoE, for that matter. It's a finetune of an existing dense model (Qwen 2.5 for most, Llama 3.3 for 70B). &lt;em&gt;ONLY&lt;/em&gt; the full, 671B model is the real stuff. &lt;/p&gt; &lt;p&gt;(Making a post about this because I'm getting really tired of having to explain this under every &amp;quot;R1 on a potato&amp;quot; and &amp;quot;why is my R1 not as smart as people say&amp;quot; post separately.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zalathustra"&gt; /u/Zalathustra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icsa5o/psa_your_7b14b32b70b_r1_is_not_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icsa5o/psa_your_7b14b32b70b_r1_is_not_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icsa5o/psa_your_7b14b32b70b_r1_is_not_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T12:06:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1icer8t</id>
    <title>Will Deepseek soon be banned in the US?</title>
    <updated>2025-01-28T22:48:18+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icer8t/will_deepseek_soon_be_banned_in_the_us/"&gt; &lt;img alt="Will Deepseek soon be banned in the US?" src="https://preview.redd.it/5gpitg40dtfe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=785ab6a8af1daeae906fcf4071ac93f79583ffb0" title="Will Deepseek soon be banned in the US?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5gpitg40dtfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icer8t/will_deepseek_soon_be_banned_in_the_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icer8t/will_deepseek_soon_be_banned_in_the_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T22:48:18+00:00</published>
  </entry>
</feed>
