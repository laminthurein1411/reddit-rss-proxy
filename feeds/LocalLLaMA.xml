<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-24T20:48:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ixb1u2</id>
    <title>Hardware recommendation - AMD FX and mi50</title>
    <updated>2025-02-24T19:47:52+00:00</updated>
    <author>
      <name>/u/SirTwitchALot</name>
      <uri>https://old.reddit.com/user/SirTwitchALot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been trying to come up to speed on LLMs, just playing around to develop my skills. I've done some experimentation writing some simple assistants in python. I have an old PC collecting dust on the shelf that I'm thinking of repurposing to rum llama instead of my laptop. It has&lt;/p&gt; &lt;p&gt;AMD fx-8350&lt;/p&gt; &lt;p&gt;32GB ddr3&lt;/p&gt; &lt;p&gt;RTX 960 (only 2GB)&lt;/p&gt; &lt;p&gt;I was thinking about throwing an ebay mi50 into this system. I can get a 16gb card used for $125 right now. I'm thinking that's a good way to get my feet wet without a big investment. I read something about the mi cards not working with CPUs prior to Zen though? &lt;/p&gt; &lt;p&gt;Are there any caveats to what I'm considering that I'm missing? &lt;/p&gt; &lt;p&gt;I know I'm not going to get amazing performance out of this setup, but will it be usable for experimentation (maybe in the tens of tokens a second on say an 8b model?)&lt;/p&gt; &lt;p&gt;Are there better low cost options I might want to look at instead? I know Jetson starts at $250, but with only 8gb of memory it seems like it might be worse than this setup since I would have 32gb of system ram and 16gb GPU&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SirTwitchALot"&gt; /u/SirTwitchALot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixb1u2/hardware_recommendation_amd_fx_and_mi50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixb1u2/hardware_recommendation_amd_fx_and_mi50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixb1u2/hardware_recommendation_amd_fx_and_mi50/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T19:47:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix5ofd</id>
    <title>Has anyone ran the 1.58 and 2.51bit quants of DeepSeek R1 using KTransformers?</title>
    <updated>2025-02-24T16:11:14+00:00</updated>
    <author>
      <name>/u/tim1234525</name>
      <uri>https://old.reddit.com/user/tim1234525</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Also is there any data of comparisons of the pp and tg using different CPUs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tim1234525"&gt; /u/tim1234525 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5ofd/has_anyone_ran_the_158_and_251bit_quants_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5ofd/has_anyone_ran_the_158_and_251bit_quants_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5ofd/has_anyone_ran_the_158_and_251bit_quants_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T16:11:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwn617</id>
    <title>Benchmarks are a lie, and I have some examples</title>
    <updated>2025-02-23T23:00:15+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This was talked about a lot, but the recent HuggingFace eval results still took me by surprise.&lt;/p&gt; &lt;p&gt;My favorite RP model- &lt;strong&gt;Midnight Miqu 1.5&lt;/strong&gt; got &lt;strong&gt;LOWER&lt;/strong&gt; benchmarks all across the board than my own &lt;strong&gt;Wingless_Imp_8B&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;As much as I'd like to say &amp;quot;Yeah guys, my 8B model &lt;strong&gt;outperforms the legendary Miqu&lt;/strong&gt;&amp;quot;, &lt;strong&gt;no&lt;/strong&gt;, it &lt;strong&gt;does not&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It's not even close. &lt;strong&gt;Midnight Miqu&lt;/strong&gt; (1.5) is orders of magnitude &lt;strong&gt;better than ANY 8B&lt;/strong&gt; model, it's not even remotely close.&lt;/p&gt; &lt;p&gt;Now, I know exactly what went into &lt;strong&gt;Wingless_Imp_8B&lt;/strong&gt;, and &lt;strong&gt;I did NOT benchmaxxed&lt;/strong&gt;, as I simply do not care for these things, I started doing the evals only recently, and solely because people asked for it. What I am saying is:&lt;/p&gt; &lt;p&gt;1) Wingless_Imp_8B high benchmarks results were &lt;strong&gt;NOT cooked&lt;/strong&gt; (not on purpose anyway)&lt;br /&gt; 2) Even despite it was not benchmaxxed, and the results are &amp;quot;&lt;strong&gt;organic&lt;/strong&gt;&amp;quot;, they &lt;strong&gt;still do not reflect actual smarts&lt;/strong&gt;&lt;br /&gt; 2) The high benchmarks are &lt;strong&gt;randomly high&lt;/strong&gt;, while in practice have &lt;strong&gt;ALMOST no correlation to actual &amp;quot;organic&amp;quot; smarts&lt;/strong&gt; vs ANY 70B model, especially midnight miqu&lt;/p&gt; &lt;p&gt;Now, this case above is sus in itself, but the following case should settle it once and for all, the case of &lt;strong&gt;Phi-Lthy&lt;/strong&gt; and &lt;strong&gt;Phi-Line_14B&lt;/strong&gt; (TL;DR 1 is lobotomized, the other is not, the lobotmized is better at following instructions):&lt;/p&gt; &lt;p&gt;I used the &lt;strong&gt;exact same dataset for both&lt;/strong&gt;, but for Phi-Lthy, &lt;strong&gt;I literally lobotomized it by yeeting 8 layers&lt;/strong&gt; out of its brain, &lt;strong&gt;yet&lt;/strong&gt; its IFeval is &lt;strong&gt;significantly higher than the unlobotomized model&lt;/strong&gt;. How does &lt;strong&gt;removing 8 layers&lt;/strong&gt; out of 40 make it follow instructions &lt;strong&gt;better&lt;/strong&gt;?&lt;/p&gt; &lt;p&gt;I believe we should have a serious discussion about whether benchmarks for LLMs even hold any weight anymore, because I am straight up doubting their accuracy to reflect model capabilities alltogether at this point. A model can be in practice almost orders of magnitude smarter than the rest, yet people will ignore it because of low benchmarks. There might be somewhere in hugging face a real SOTA model, yet we might just dismiss it due to mediocre benchmarks.&lt;/p&gt; &lt;p&gt;What if I told you last year that I have &lt;strong&gt;the best roleplay model in the world&lt;/strong&gt;, but when you'd look at its benchmarks, you would see that the &amp;quot;best roleplay model in the world, of 70B size, &lt;strong&gt;has worst benchmarks than a shitty 8B model&lt;/strong&gt;&amp;quot;, most would have called BS. &lt;/p&gt; &lt;p&gt;That model was &lt;strong&gt;Midnight Miqu&lt;/strong&gt; (1.5) 70B, and I still think it blows away many 'modern' models even today.&lt;/p&gt; &lt;p&gt;The unlobtomized Phi-4:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Phi-Line_14B"&gt;https://huggingface.co/SicariusSicariiStuff/Phi-Line_14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The lobtomized Phi-4:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Phi-lthy4"&gt;https://huggingface.co/SicariusSicariiStuff/Phi-lthy4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwn617/benchmarks_are_a_lie_and_i_have_some_examples/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwn617/benchmarks_are_a_lie_and_i_have_some_examples/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwn617/benchmarks_are_a_lie_and_i_have_some_examples/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T23:00:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixcaur</id>
    <title>New Deepseek integation repo</title>
    <updated>2025-02-24T20:37:59+00:00</updated>
    <author>
      <name>/u/lucitatecapacita</name>
      <uri>https://old.reddit.com/user/lucitatecapacita</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like DeepSeek has released a repo with new integrations with several frameworks:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/deepseek-ai/awesome-deepseek-integration"&gt;https://github.com/deepseek-ai/awesome-deepseek-integration&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lucitatecapacita"&gt; /u/lucitatecapacita &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixcaur/new_deepseek_integation_repo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixcaur/new_deepseek_integation_repo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixcaur/new_deepseek_integation_repo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T20:37:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixavpv</id>
    <title>Anyone using RAG with Query-Aware Chunking?</title>
    <updated>2025-02-24T19:41:01+00:00</updated>
    <author>
      <name>/u/Timely-Jackfruit8885</name>
      <uri>https://old.reddit.com/user/Timely-Jackfruit8885</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m the developer of d.ai, a mobile app that lets you chat offline with LLMs while keeping everything private and free. Iâ€™m currently working on adding long-term memory using Retrieval-Augmented Generation (RAG), and Iâ€™m exploring query-aware chunking to improve the relevance of the results.&lt;/p&gt; &lt;p&gt;For those unfamiliar, query-aware chunking is a technique where the text is split into chunks dynamically based on the context of the userâ€™s query, instead of fixed-size chunks. The idea is to retrieve information thatâ€™s more relevant to the actual question being asked.&lt;/p&gt; &lt;p&gt;Has anyone here implemented something similar or worked with this approach?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Timely-Jackfruit8885"&gt; /u/Timely-Jackfruit8885 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixavpv/anyone_using_rag_with_queryaware_chunking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixavpv/anyone_using_rag_with_queryaware_chunking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixavpv/anyone_using_rag_with_queryaware_chunking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T19:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix3lsw</id>
    <title>Tutorial: 100 Lines to Let Cursor AI Build Agents for You</title>
    <updated>2025-02-24T14:42:36+00:00</updated>
    <author>
      <name>/u/Willing-Site-8137</name>
      <uri>https://old.reddit.com/user/Willing-Site-8137</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3lsw/tutorial_100_lines_to_let_cursor_ai_build_agents/"&gt; &lt;img alt="Tutorial: 100 Lines to Let Cursor AI Build Agents for You" src="https://external-preview.redd.it/owYpiYxzMbFoIy2mEgwHQu4HBDMye8iivJkM1wZ1bsE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fff8bc2c79be0acb98d587fba0d423ac51045588" title="Tutorial: 100 Lines to Let Cursor AI Build Agents for You" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Willing-Site-8137"&gt; /u/Willing-Site-8137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=0Pv5HVoVBYE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3lsw/tutorial_100_lines_to_let_cursor_ai_build_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3lsw/tutorial_100_lines_to_let_cursor_ai_build_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T14:42:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix3cxy</id>
    <title>I updated my personal open source Chat UI to support reasoning models.</title>
    <updated>2025-02-24T14:31:27+00:00</updated>
    <author>
      <name>/u/DivineAscension</name>
      <uri>https://old.reddit.com/user/DivineAscension</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3cxy/i_updated_my_personal_open_source_chat_ui_to/"&gt; &lt;img alt="I updated my personal open source Chat UI to support reasoning models." src="https://external-preview.redd.it/hrIhTomEzBpvg6FqJVsML_mUgvpVbATgJKk-LAnZKxE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97f0ff1022e7b1bac913b25515c4392fafa4d943" title="I updated my personal open source Chat UI to support reasoning models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Toy-97/Chat-WebUI"&gt;Here is the link to the open source repos&lt;/a&gt;. I've posted about my personal Chat UI before, and now I've updated it to support reasoning models. I use this personally because this has built-in tools to summarize YouTube videos and perform online web searches. There have been tons of improvements made too, so this version should be extremely stable. I hope you guys find it useful!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DivineAscension"&gt; /u/DivineAscension &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3cxy/i_updated_my_personal_open_source_chat_ui_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3cxy/i_updated_my_personal_open_source_chat_ui_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3cxy/i_updated_my_personal_open_source_chat_ui_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T14:31:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix3xn4</id>
    <title>200 Combinatorial Identities and Theorems Dataset for LLM finetuning [Dataset]</title>
    <updated>2025-02-24T14:57:24+00:00</updated>
    <author>
      <name>/u/DataBaeBee</name>
      <uri>https://old.reddit.com/user/DataBaeBee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3xn4/200_combinatorial_identities_and_theorems_dataset/"&gt; &lt;img alt="200 Combinatorial Identities and Theorems Dataset for LLM finetuning [Dataset]" src="https://external-preview.redd.it/hWfudl7kX8SLOLHp3nzBGVBq-MkP2SBqtM5dmm1DsOk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a2aa6e2090df556a49e222ec8d0b21997e38277" title="200 Combinatorial Identities and Theorems Dataset for LLM finetuning [Dataset]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataBaeBee"&gt; /u/DataBaeBee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://leetarxiv.substack.com/p/oeic"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3xn4/200_combinatorial_identities_and_theorems_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3xn4/200_combinatorial_identities_and_theorems_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T14:57:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix11go</id>
    <title>aspen - Open-source voice assistant you can call, at only $0.01025/min!</title>
    <updated>2025-02-24T12:37:28+00:00</updated>
    <author>
      <name>/u/thooton</name>
      <uri>https://old.reddit.com/user/thooton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix11go/aspen_opensource_voice_assistant_you_can_call_at/"&gt; &lt;img alt="aspen - Open-source voice assistant you can call, at only $0.01025/min!" src="https://external-preview.redd.it/Y8cU497M8VmMsSvykiiACmZpJ9cu5NkYzryYit_2lHY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b00ce71267ae00f4c36a6263a4dd0cc5d7b9aee" title="aspen - Open-source voice assistant you can call, at only $0.01025/min!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1ix11go/video/ohkvv8g9z2le1/player"&gt;https://reddit.com/link/1ix11go/video/ohkvv8g9z2le1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;hi everyone, hope you're all doing great :) I thought I'd share a little project that I've been working on for the past few days. It's a voice assistant that uses Twilio's API to be accessible through a real phone number, so you can call it just like a person!&lt;/p&gt; &lt;p&gt;Using Groq's STT free tier and Google's TTS free tier, the only costs come from Twilio and Anthropic and add up to about $0.01025/min, which is a lot cheaper than the conversational agents from ElevenLabs or PlayAI which approach $0.10/min or $0.18/min respectively.&lt;/p&gt; &lt;p&gt;I wrote the code to be as modular as possible so it should be easy to modify it to use your own local LLM or whatever you like! all PRs are welcome :)&lt;/p&gt; &lt;p&gt;have an awesome day!!!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/thooton/aspen"&gt;https://github.com/thooton/aspen&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thooton"&gt; /u/thooton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix11go/aspen_opensource_voice_assistant_you_can_call_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix11go/aspen_opensource_voice_assistant_you_can_call_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix11go/aspen_opensource_voice_assistant_you_can_call_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T12:37:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix46wr</id>
    <title>R1 for Spatial Reasoning</title>
    <updated>2025-02-24T15:08:13+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix46wr/r1_for_spatial_reasoning/"&gt; &lt;img alt="R1 for Spatial Reasoning" src="https://b.thumbs.redditmedia.com/jyqIGUOvKtGUJMPde7zV_1b5qWCF6lHqv8cfpN-Wi_Q.jpg" title="R1 for Spatial Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing an experiment in data synthesis for R1-style reasoning in my VLM, fine-tuned for enhanced spatial reasoning, more in &lt;a href="https://huggingface.co/spaces/open-r1/README/discussions/10"&gt;this discussion&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;After finding &lt;a href="https://spatial-vlm.github.io/"&gt;SpatialVLM&lt;/a&gt; last year, we open-sourced a similar 3D scene reconstruction pipeline: &lt;a href="https://github.com/remyxai/VQASynth"&gt;VQASynth&lt;/a&gt; to generate instruction following data for spatial reasoning.&lt;/p&gt; &lt;p&gt;Inspired by &lt;a href="https://typefly.github.io/"&gt;TypeFly&lt;/a&gt;, we tried applying &lt;a href="https://x.com/smellslikeml/status/1790069289413914956"&gt;this idea to VLMs&lt;/a&gt;, but it wasn't robust enough to fly our drone.&lt;/p&gt; &lt;p&gt;With R1-style reasoning, can't we ground our response on a set of observations from the VQASynth pipeline to train a VLM for better scene understanding and planning?&lt;/p&gt; &lt;p&gt;That's the goal for an upcoming VLM release &lt;a href="https://colab.research.google.com/drive/1R64daHgR50GnxH3yn7mcs8rnldWL1ZxF"&gt;based on this&lt;/a&gt; colab.&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts on making a dataset and VLM which could power the next generation of more reliable embodied AI applications, join us on &lt;a href="https://github.com/remyxai/VQASynth/issues/36"&gt;github&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rwcajdccv3le1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d924f00d5dd56b8bb5c6900799071783993fe1a4"&gt;https://preview.redd.it/rwcajdccv3le1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d924f00d5dd56b8bb5c6900799071783993fe1a4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gzs74o4dv3le1.png?width=754&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d20c61ca9f0cb8e8f3d97b21be473c6002abdc3a"&gt;https://preview.redd.it/gzs74o4dv3le1.png?width=754&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d20c61ca9f0cb8e8f3d97b21be473c6002abdc3a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix46wr/r1_for_spatial_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix46wr/r1_for_spatial_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix46wr/r1_for_spatial_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T15:08:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwhfl5</id>
    <title>96GB modded RTX 4090 for $4.5k</title>
    <updated>2025-02-23T18:55:09+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwhfl5/96gb_modded_rtx_4090_for_45k/"&gt; &lt;img alt="96GB modded RTX 4090 for $4.5k" src="https://preview.redd.it/5rf8m3k1rxke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d35cdb0e62ea887c4da38324fff2ccbbf226f9f" title="96GB modded RTX 4090 for $4.5k" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5rf8m3k1rxke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwhfl5/96gb_modded_rtx_4090_for_45k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwhfl5/96gb_modded_rtx_4090_for_45k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T18:55:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix5vgm</id>
    <title>TIP: Open WebUI "Overview" mode</title>
    <updated>2025-02-24T16:19:18+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5vgm/tip_open_webui_overview_mode/"&gt; &lt;img alt="TIP: Open WebUI &amp;quot;Overview&amp;quot; mode" src="https://a.thumbs.redditmedia.com/xhTnipJlQp9kwvDNpjx8F2VXJDkn8RgN7RNScLB8_00.jpg" title="TIP: Open WebUI &amp;quot;Overview&amp;quot; mode" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As Google added &lt;a href="https://nitter.net/OfficialLoganK/status/1894049802557456669"&gt;branching support&lt;/a&gt; for its AI Studio product, I think the crown in terms of implementation is still held by the Open WebUI.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nfdla1lq34le1.png?width=2492&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70419e34b5c6474913c5d005bf6a5125561d8302"&gt;Overview mode&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;To activate: click &amp;quot;...&amp;quot; at the top right and select &amp;quot;Overview&amp;quot; in the menu&lt;/li&gt; &lt;li&gt;Clicking any leaf node in the graph will update the chat state accordingly&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5vgm/tip_open_webui_overview_mode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5vgm/tip_open_webui_overview_mode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5vgm/tip_open_webui_overview_mode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T16:19:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix6bjw</id>
    <title>Is there any image models coming out?</title>
    <updated>2025-02-24T16:37:38+00:00</updated>
    <author>
      <name>/u/hoja_nasredin</name>
      <uri>https://old.reddit.com/user/hoja_nasredin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We were extremely spoiled this summer with Flux and SD3.1 coming out. But was anything else have been released since? Flux cannot be trained in a serious way apparently since it is distilled, and SD3 is hated by the community (or it might have some other issues I'm not aware). &lt;/p&gt; &lt;p&gt;What is happening with the image models right now? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hoja_nasredin"&gt; /u/hoja_nasredin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix6bjw/is_there_any_image_models_coming_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix6bjw/is_there_any_image_models_coming_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix6bjw/is_there_any_image_models_coming_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T16:37:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwvo4b</id>
    <title>An Open-Source Implementation of Deep Research using Gemini Flash 2.0</title>
    <updated>2025-02-24T06:32:56+00:00</updated>
    <author>
      <name>/u/CarpetNo5579</name>
      <uri>https://old.reddit.com/user/CarpetNo5579</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an open source version of deep research using Gemini Flash 2.0!&lt;/p&gt; &lt;p&gt;Feed it any topic and it'll explore it thoroughly, building and displaying a research tree in real-time as it works. &lt;/p&gt; &lt;p&gt;This implementation has three research modes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fast (1-3min): Quick surface research, perfect for initial exploration&lt;/li&gt; &lt;li&gt;Balanced (3-6min): Moderate depth, explores main concepts and relationships&lt;/li&gt; &lt;li&gt;Comprehensive (5-12min): Deep recursive research, builds query trees, explores counter-arguments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The coolest part is watching it think - it prints out the research tree as it explores, so you can see exactly how it's approaching your topic.&lt;/p&gt; &lt;p&gt;I built this because I haven't seen any implementation that uses Gemini and its built in search tool and thought others might find it useful too.&lt;/p&gt; &lt;p&gt;Here's the github link: &lt;a href="https://github.com/eRuaro/open-gemini-deep-research"&gt;https://github.com/eRuaro/open-gemini-deep-research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CarpetNo5579"&gt; /u/CarpetNo5579 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwvo4b/an_opensource_implementation_of_deep_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwvo4b/an_opensource_implementation_of_deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwvo4b/an_opensource_implementation_of_deep_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T06:32:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix3gao</id>
    <title>nvidia / Evo 2 Protein Design</title>
    <updated>2025-02-24T14:35:47+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3gao/nvidia_evo_2_protein_design/"&gt; &lt;img alt="nvidia / Evo 2 Protein Design" src="https://preview.redd.it/fp2o6r9ql3le1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=174a8acebfd2e90b81df658a8e8c6f3c7d031293" title="nvidia / Evo 2 Protein Design" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://build.nvidia.com/nvidia/evo2-protein-design/blueprintcard"&gt;https://build.nvidia.com/nvidia/evo2-protein-design/blueprintcard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fp2o6r9ql3le1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3gao/nvidia_evo_2_protein_design/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3gao/nvidia_evo_2_protein_design/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T14:35:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix1ddj</id>
    <title>ragit 0.3.0 released</title>
    <updated>2025-02-24T12:55:49+00:00</updated>
    <author>
      <name>/u/baehyunsol</name>
      <uri>https://old.reddit.com/user/baehyunsol</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix1ddj/ragit_030_released/"&gt; &lt;img alt="ragit 0.3.0 released" src="https://external-preview.redd.it/TipJWadkvg51FCh2k5yn7L-J4VOuk7fkeumbFTDL_OM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91ff6bfc615eb15181fb1437f1d20a7a4e5656c6" title="ragit 0.3.0 released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on this open source RAG solution for a while.&lt;/p&gt; &lt;p&gt;It gives you a simple CLI for local rag, without any need for writing code!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/baehyunsol"&gt; /u/baehyunsol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/baehyunsol/ragit"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix1ddj/ragit_030_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix1ddj/ragit_030_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T12:55:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwtl7f</id>
    <title>Most people are worried about LLM's executing code. Then theres me...... ðŸ˜‚</title>
    <updated>2025-02-24T04:24:24+00:00</updated>
    <author>
      <name>/u/DataScientist305</name>
      <uri>https://old.reddit.com/user/DataScientist305</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwtl7f/most_people_are_worried_about_llms_executing_code/"&gt; &lt;img alt="Most people are worried about LLM's executing code. Then theres me...... ðŸ˜‚" src="https://preview.redd.it/92abn3ekk0le1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09547c1d04e4bb052014aac1d2d58fba8d76d0ee" title="Most people are worried about LLM's executing code. Then theres me...... ðŸ˜‚" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataScientist305"&gt; /u/DataScientist305 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/92abn3ekk0le1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwtl7f/most_people_are_worried_about_llms_executing_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwtl7f/most_people_are_worried_about_llms_executing_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T04:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix5np0</id>
    <title>I built OLLAMA GUI in next.js how do you like it?</title>
    <updated>2025-02-24T16:10:21+00:00</updated>
    <author>
      <name>/u/Itsaliensbro453</name>
      <uri>https://old.reddit.com/user/Itsaliensbro453</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5np0/i_built_ollama_gui_in_nextjs_how_do_you_like_it/"&gt; &lt;img alt="I built OLLAMA GUI in next.js how do you like it?" src="https://preview.redd.it/f0j99mmn24le1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=494ec50651dc68222e262f195d12282a270ea7e0" title="I built OLLAMA GUI in next.js how do you like it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hellou guys im a developer trying to land my first job so im creating projects for my portfolio!&lt;/p&gt; &lt;p&gt;I have built this OLLAMA GUI with Next.js and Typescrypt!ðŸ˜€&lt;/p&gt; &lt;p&gt;How do you like it? Feel free to use the app and contribute its 100% free and open source! &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Ablasko32/Project-Shard---GUI-for-local-LLM-s"&gt;https://github.com/Ablasko32/Project-Shard---GUI-for-local-LLM-s&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Itsaliensbro453"&gt; /u/Itsaliensbro453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f0j99mmn24le1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5np0/i_built_ollama_gui_in_nextjs_how_do_you_like_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5np0/i_built_ollama_gui_in_nextjs_how_do_you_like_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T16:10:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix0d4z</id>
    <title>Polish Ministry of Digital Affairs shared PLLuM model family on HF</title>
    <updated>2025-02-24T11:57:56+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/CYFRAGOVPL"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix0d4z/polish_ministry_of_digital_affairs_shared_pllum/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix0d4z/polish_ministry_of_digital_affairs_shared_pllum/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T11:57:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwvvmy</id>
    <title>Qwen is releasing something tonight!</title>
    <updated>2025-02-24T06:46:53+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwvvmy/qwen_is_releasing_something_tonight/"&gt; &lt;img alt="Qwen is releasing something tonight!" src="https://external-preview.redd.it/vArUV2h82u8EtPauQRu5bQrqvRa1QZ1C_bg0wPIoH5o.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=263b5e9873bf302385907f40a338f7412dc9b280" title="Qwen is releasing something tonight!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://twitter.com/Alibaba_Qwen/status/1893907569724281088"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwvvmy/qwen_is_releasing_something_tonight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwvvmy/qwen_is_releasing_something_tonight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T06:46:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwqf3z</id>
    <title>FlashMLA - Day 1 of OpenSourceWeek</title>
    <updated>2025-02-24T01:37:17+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwqf3z/flashmla_day_1_of_opensourceweek/"&gt; &lt;img alt="FlashMLA - Day 1 of OpenSourceWeek" src="https://preview.redd.it/to631nzvqzke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3551b21c98cfb01ba242529b337443a5c85b4481" title="FlashMLA - Day 1 of OpenSourceWeek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/deepseek-ai/FlashMLA"&gt;https://github.com/deepseek-ai/FlashMLA&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/to631nzvqzke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwqf3z/flashmla_day_1_of_opensourceweek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwqf3z/flashmla_day_1_of_opensourceweek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T01:37:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixamd9</id>
    <title>QwQ-Max-Preview soon</title>
    <updated>2025-02-24T19:30:38+00:00</updated>
    <author>
      <name>/u/pkmxtw</name>
      <uri>https://old.reddit.com/user/pkmxtw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found that they have been updating their website on another branch:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/qwenlm.github.io/commit/5d009b319931d473211cb4225d726b322afbb734"&gt;https://github.com/QwenLM/qwenlm.github.io/commit/5d009b319931d473211cb4225d726b322afbb734&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;: Apache 2.0 licensed QwQ-Max, Qwen2.5-Max, QwQ-32B and probably other smaller QwQ variants, and an app for qwen chat.&lt;/p&gt; &lt;hr /&gt; &lt;blockquote&gt; &lt;p&gt;Weâ€™re happy to unveil QwQ-Max-Preview , the latest advancement in the Qwen series, designed to push the boundaries of deep reasoning and versatile problem-solving. Built on the robust foundation of Qwen2.5-Max , this preview model excels in mathematics, coding, and general-domain tasks, while delivering outstanding performance in Agent-related workflows. As a sneak peek into our upcoming QwQ-Max release, this version offers a glimpse of its enhanced capabilities, with ongoing refinements and an official Apache 2.0-licensed open-source launch of QwQ-Max and Qwen2.5-Max planned soon. Stay tuned for a new era of intelligent reasoning.&lt;/p&gt; &lt;p&gt;As we prepare for the official open-source release of QwQ-Max under the Apache 2.0 License, our roadmap extends beyond sharing cutting-edge research. We are committed to democratizing access to advanced reasoning capabilities and fostering innovation across diverse applications. Hereâ€™s whatâ€™s next:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;APP Release&lt;/strong&gt; To bridge the gap between powerful AI and everyday users, we will launch a dedicated APP for Qwen Chat. This intuitive interface will enable seamless interaction with the model for tasks like problem-solving, code generation, and logical reasoningâ€”no technical expertise required. The app will prioritize real-time responsiveness and integration with popular productivity tools, making advanced AI accessible to a global audience.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Open-Sourcing Smaller Reasoning Models&lt;/strong&gt; Recognizing the need for lightweight, resource-efficient solutions, we will release a series of smaller QwQ variants , such as QwQ-32B, for local device deployment. These models will retain robust reasoning capabilities while minimizing computational demands, allowing developers to integrate them into devices. Perfect for privacy-sensitive applications or low-latency workflows, they will empower creators to build custom AI solutions.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Community-Driven Innovation&lt;/strong&gt; By open-sourcing QwQ-Max, Qwen2.5-Max, and its smaller counterparts, we aim to spark collaboration among developers, researchers, and hobbyists. We invite the community to experiment, fine-tune, and extend these models for specialized use casesâ€”from education tools to autonomous agents. Our goal is to cultivate an ecosystem where innovation thrives through shared knowledge and collective problem-solving.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Stay tuned as we roll out these initiatives, designed to empower users at every level and redefine the boundaries of what AI can achieve. Together, weâ€™re building a future where intelligence is not just powerful, but universally accessible.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pkmxtw"&gt; /u/pkmxtw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixamd9/qwqmaxpreview_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixamd9/qwqmaxpreview_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixamd9/qwqmaxpreview_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T19:30:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix98kq</id>
    <title>Claude 3.7 Sonnet and Claude Code</title>
    <updated>2025-02-24T18:34:27+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix98kq/claude_37_sonnet_and_claude_code/"&gt; &lt;img alt="Claude 3.7 Sonnet and Claude Code" src="https://external-preview.redd.it/V8JG-mmrlkT02vKigktdXzK2PH-CSO-CrYueRmf_OX0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8beabe2cc993eb0a304f0f44ee00c9b8eb681095" title="Claude 3.7 Sonnet and Claude Code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.anthropic.com/news/claude-3-7-sonnet"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix98kq/claude_37_sonnet_and_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix98kq/claude_37_sonnet_and_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T18:34:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwzuqb</id>
    <title>Claude Sonnet 3.7 soon</title>
    <updated>2025-02-24T11:24:48+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwzuqb/claude_sonnet_37_soon/"&gt; &lt;img alt="Claude Sonnet 3.7 soon" src="https://preview.redd.it/agru3m1in2le1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=753ac58c540176e9b69e887c77f5a13fea6f9608" title="Claude Sonnet 3.7 soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/agru3m1in2le1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwzuqb/claude_sonnet_37_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwzuqb/claude_sonnet_37_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T11:24:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix96pq</id>
    <title>Claude 3.7 is real</title>
    <updated>2025-02-24T18:32:00+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix96pq/claude_37_is_real/"&gt; &lt;img alt="Claude 3.7 is real" src="https://preview.redd.it/2qkaymexr4le1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0bf2edcabf3c5b2063f0fb29bc1b4f7da023acfe" title="Claude 3.7 is real" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its show time folks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2qkaymexr4le1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix96pq/claude_37_is_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix96pq/claude_37_is_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T18:32:00+00:00</published>
  </entry>
</feed>
