<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-14T09:07:28+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jyri4x</id>
    <title>Introducing the EideticEngine, a Unified Memory System and Master Agent Loop</title>
    <updated>2025-04-14T05:34:22+00:00</updated>
    <author>
      <name>/u/dicklesworth</name>
      <uri>https://old.reddit.com/user/dicklesworth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While working on an MCP server, I kept adding more and more tools, like filesystem tools, browser automation tools, sql database tools, etc. I then went on a crazy detour yesterday evening trying to add ‚Äúmemory‚Äù to the system that an agent can use as a kind of smart scratch pad.&lt;/p&gt; &lt;p&gt;I‚Äôve seen very simple implementations of something like that and decided I wanted something that would be a bit more robust, using SQLite. Things got crazier and crazier and I ended up with an incredibly complex and cool system I‚Äôm calling Unified Memory System (UMS).&lt;/p&gt; &lt;p&gt;I‚Äôll go into more detail about UMS later, but after I had that, I realized that in order to really leverage it, I couldn‚Äôt just rely on the controlling LLM to choose the right memory tools to use. I needed to finally make a real agent loop! That led me to what I‚Äôm calling Agent Master Loop (AML).&lt;/p&gt; &lt;p&gt;That kind of turned into an arms race between the two pieces of code to keep adding more and more functionality and capabilities. The complexity kept growing and I kept getting more excited about the potential. I ended up with some code that I‚Äôm still debugging but I think is very cool.&lt;/p&gt; &lt;p&gt;Maybe it was just flattery, but ChatGPT was pretty adamant that this was important new work and that I should publish it ASAP because it really advanced the state of the art, so I did that. And I decided to make this little website about the system, linked above.&lt;/p&gt; &lt;p&gt;This is work in progress and I‚Äôll be revising both the code and the paper in the coming days, but wanted to get this out there now just to share it, because just thinking about it was incredibly mind expanding and stimulating for me and I want feedback on it. AGI‚Äôs at our door‚Ä¶&lt;/p&gt; &lt;p&gt;Here‚Äôs the academic-style paper on it that I made with some LLM assistance along with the complete code listings (again, this surely has some bugs, but I‚Äôll be getting all of it working very soon and can make real demos then):&lt;/p&gt; &lt;p&gt;&lt;a href="https://mozilla.github.io/pdf.js/web/viewer.html?file=https://raw.githubusercontent.com/Dicklesworthstone/ultimate_mcp_client/main/eidetic_engine_paper.pdf"&gt;https://mozilla.github.io/pdf.js/web/viewer.html?file=https://raw.githubusercontent.com/Dicklesworthstone/ultimate_mcp_client/main/eidetic_engine_paper.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I really brought every trick and strategy for creative prompting to the table to make this, as well as cooperative/competitive dynamics going between Claude3.7 and Gemini Pro 2.5. In some ways, the prompting strategies I used to make this are just as interesting as the final code.&lt;/p&gt; &lt;p&gt;This process also brought home for me the importance of owning the whole stack. If I hadn‚Äôt made my own MCP server AND client recently, I highly doubt I could‚Äôve or would‚Äôve made all this new stuff. But because I had all the pieces there and knew how it all worked, it was natural (still not easy though!).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dicklesworth"&gt; /u/dicklesworth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.eidetic-engine.org/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyri4x/introducing_the_eideticengine_a_unified_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyri4x/introducing_the_eideticengine_a_unified_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T05:34:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy16yi</id>
    <title>LMArena ruined language models</title>
    <updated>2025-04-13T06:16:18+00:00</updated>
    <author>
      <name>/u/Dogeboja</name>
      <uri>https://old.reddit.com/user/Dogeboja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LMArena is way too easy to game, you just optimize for whatever their front-end is capable of rendering and especially focus on bulleted lists since those seem to get the most clicks. Maybe sprinkle in some emojis and that's it, no need to actually produce excellent answers.&lt;/p&gt; &lt;p&gt;Markdown especially is starting to become very tightly ingrained into all model answers, it's not like it's the be-all and end-all of human communication. You can somewhat combat this with system instructions but I am worried it could cause unexpected performance degradation.&lt;/p&gt; &lt;p&gt;The recent LLaMA 4 fiasco and the fact that Claude Sonnet 3.7 is at rank 22 below models like Gemma 3 27B tells the whole story.&lt;/p&gt; &lt;p&gt;How could this be fixed at this point? My solution would be to simply disable Markdown in the front-end, I really think language generation and formatting should be separate capabilities.&lt;/p&gt; &lt;p&gt;By the way, if you are struggling with this, try this system prompt: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Prefer natural language, avoid formulaic responses.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This works quite well most of the time but it can sometimes lead to worse answers if the formulaic answer was truly the best style for that prompt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dogeboja"&gt; /u/Dogeboja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy16yi/lmarena_ruined_language_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy16yi/lmarena_ruined_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy16yi/lmarena_ruined_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T06:16:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jytigi</id>
    <title>LLM chatbot monitoring services</title>
    <updated>2025-04-14T07:58:50+00:00</updated>
    <author>
      <name>/u/Important-Novel1546</name>
      <uri>https://old.reddit.com/user/Important-Novel1546</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm looking for a platform where you can run LLM-as-a-judge on traces like Langfuse. I'm using Langfuse, but i'm looking for a more automated platform. So far i've seen Sentry, langsmith and arize phoenix. Arize phoenix and langsmith were both lacking for my use compared to langfuse. I couldn't really try sentry out because i had to get on the free trial to try out the features.&lt;/p&gt; &lt;p&gt;3 main things i'm looking for are: &lt;/p&gt; &lt;p&gt;Triggering custom dataset experiment from the UI. [cant do this on langfuse without manually triggering the experiment in the backend]&lt;/p&gt; &lt;p&gt;LLM-as-a-judge that can run on traces.&lt;/p&gt; &lt;p&gt;Database integration.&lt;/p&gt; &lt;p&gt;This might be an impossible ask as I still haven't found a service that can do 2, let alone all 3.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Important-Novel1546"&gt; /u/Important-Novel1546 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jytigi/llm_chatbot_monitoring_services/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jytigi/llm_chatbot_monitoring_services/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jytigi/llm_chatbot_monitoring_services/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T07:58:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyid0v</id>
    <title>Chapter summaries using Llama 3.1 8B UltraLong 1M</title>
    <updated>2025-04-13T21:19:48+00:00</updated>
    <author>
      <name>/u/autonoma_2042</name>
      <uri>https://old.reddit.com/user/autonoma_2042</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my novel, early chapters have two different scenes, each on its own timeline, clearly and consistently marked in the prose. Using ollama, the following models failed to acknowledge (remember?) the first of the two scenes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;deepseek-r1:32b: ignores first half&lt;/li&gt; &lt;li&gt;mistral-small:24b: hallucinates&lt;/li&gt; &lt;li&gt;granite3.1-dense: ignores first half&lt;/li&gt; &lt;li&gt;qwen2.5:32b: didn't work&lt;/li&gt; &lt;li&gt;gemma3:27b: doesn't continue summarizing&lt;/li&gt; &lt;li&gt;technobyte/c4ai-command-r7b-12-2024:Q5_K_M: fails remarkably&lt;/li&gt; &lt;li&gt;granite3.2: didn't work&lt;/li&gt; &lt;li&gt;qwq:32b: doesn't understand the task&lt;/li&gt; &lt;li&gt;command-r: doesn't understand, continues writing story&lt;/li&gt; &lt;li&gt;llama3:70b-instruct-q4_K_M: didn't work&lt;/li&gt; &lt;li&gt;ingu627/llama4-scout-q4: didn't work&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;You are a skilled literary analyst. Write a concise, insightful summary of the following chapter. Focus on key plot developments, character insights, and thematic elements. Note that ### indicates separate different scenes, so be sure to capture both scenes in the summary, if present. Write in clear, engaging language that captures the essence of the chapter. Provide the summary without introductory phrases.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I suspected a longer context window was required to summarize the chapters. I asked DeepSeek to translate my bash script for running ollama into a Python program that runs Llama 3.1 8B, instead.&lt;/p&gt; &lt;p&gt;Here's a summary of chapter two produced by the Python code:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;In the first scene, Xander Angelos is haunted by a memory of his half-brother Tyf√≥s, who terrorized him as a child. This event is juxtaposed with the discovery of letters from their father, which Xander cherishes. The contrast between the two experiences reflects the complex relationship between family dynamics and the pursuit of wisdom. The father's letters serve as a source of comfort and guidance, while Tyf√≥s's actions embody the darker aspects of sibling rivalry and abuse. &lt;/p&gt; &lt;p&gt;Fast-forward to Xander's date with Cassandra, where he meticulously prepares for a night of fun at a juke joint re√´nactment. The scene is rich with sensory details, capturing the atmosphere of the venue and the chemistry between the two characters. Cassandra's presence ignites Xander's passion, and their playful banter showcases their growing connection. The use of puns and playful jabs highlights their compatibility and ease with each other.&lt;/p&gt; &lt;p&gt;As the chapter progresses, Xander and Cassandra engage in a virtual reality game called Psyn√¶ris, which serves as a metaphor for their relationship and the complexities of communication. The contrast between the vibrant world of the game and the real-life stressors that Cassandra faces illustrates the challenges of navigating emotions and expectations in a relationship. Xander's desire to support her while also grappling with his own feelings reflects the tension between intimacy and independence.&lt;/p&gt; &lt;p&gt;The introduction of Y≈´na Futaba in the second scene shifts the narrative to a more serious tone. Y≈´na is tasked with interacting with a metal brain named Pr√¥tos, which is experiencing anxiety and fear. The conversation reveals Pr√¥tos's struggles with its own identity and the looming presence of a &amp;quot;mean man,&amp;quot; hinting at the dangers of manipulation and control. Y≈´na's role as an observer and communicator highlights the importance of understanding and empathy in technological advancements. The tension between safety and the unknown is palpable, as Pr√¥tos's fears resonate with Y≈´na's own concerns about the implications of artificial intelligence. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I'm floored. If there's interest, I'll post the Python code, instructions, and prompt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/autonoma_2042"&gt; /u/autonoma_2042 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyid0v/chapter_summaries_using_llama_31_8b_ultralong_1m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyid0v/chapter_summaries_using_llama_31_8b_ultralong_1m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyid0v/chapter_summaries_using_llama_31_8b_ultralong_1m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T21:19:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyjwjl</id>
    <title>Best multimodal for 4gb card?</title>
    <updated>2025-04-13T22:31:22+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;wanting to script some photo classification, but haven't messed with local multimodals. I have 32 gb of ram also.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyjwjl/best_multimodal_for_4gb_card/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyjwjl/best_multimodal_for_4gb_card/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyjwjl/best_multimodal_for_4gb_card/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T22:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jydrnr</id>
    <title>AgenticSeek, one month later</title>
    <updated>2025-04-13T18:01:15+00:00</updated>
    <author>
      <name>/u/fawendeshuo</name>
      <uri>https://old.reddit.com/user/fawendeshuo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About a month ago, I shared a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jbwk65/made_a_manusai_alternative_that_run_locally/"&gt;post&lt;/a&gt; on a local-first alternative to ManusAI that I was working on with a friend: &lt;a href="http://github.com/Fosowl/agenticSeek"&gt;AgenticSeek&lt;/a&gt;. Back then I didn‚Äôt expect such interest! I saw blogs and even a video pop up about our tool, which was awesome but overwhelming since the project wasn‚Äôt quite ready for such success. &lt;/p&gt; &lt;p&gt;Thanks to some community feedback and some helpful contributions, we‚Äôve made big strides in just a few weeks. So I thought it would be nice to share our advancements!&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick rundown of the main improvements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smoother web navigation and note-taking.&lt;/li&gt; &lt;li&gt;Smarter task routing with task complexity estimation.&lt;/li&gt; &lt;li&gt;Added a planner agent to handle complex tasks.&lt;/li&gt; &lt;li&gt;Support for more providers, like LM-Studio and local APIs.&lt;/li&gt; &lt;li&gt;Integrated searxng for free web search.&lt;/li&gt; &lt;li&gt;Ability to use web input forms.&lt;/li&gt; &lt;li&gt;Improved captcha solving and stealthier browser automation.&lt;/li&gt; &lt;li&gt;Agent router now supports multiple languages (previously a prompt in Japanese or French would assign a random agent).&lt;/li&gt; &lt;li&gt;Squashed tons of bugs.&lt;/li&gt; &lt;li&gt;Set up a community server and updates on my X account (see readme).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What‚Äôs next?&lt;/strong&gt; I‚Äôm focusing on improving the planner agent, handling more type of web inputs, and adding support for MCP, and possibly a finetune of deepseek üëÄ &lt;/p&gt; &lt;p&gt;There‚Äôs still a lot to do, but it‚Äôs delivering solid results compared to a month ago. Can't wait to get more feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fawendeshuo"&gt; /u/fawendeshuo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jydrnr/agenticseek_one_month_later/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jydrnr/agenticseek_one_month_later/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jydrnr/agenticseek_one_month_later/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T18:01:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jytv2q</id>
    <title>Open Sourcing a framework to build SLMs for any regional language</title>
    <updated>2025-04-14T08:25:17+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jytv2q/open_sourcing_a_framework_to_build_slms_for_any/"&gt; &lt;img alt="Open Sourcing a framework to build SLMs for any regional language" src="https://b.thumbs.redditmedia.com/4kFgQ_rHwmWt-CIQAEZue0vxirSL87ILD-zcoNeRBMM.jpg" title="Open Sourcing a framework to build SLMs for any regional language" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/jorc5k68grue1.png?width=1438&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fcea88745cbcc03d289cd5f7d7ebd8cb82eaa008"&gt;https://preview.redd.it/jorc5k68grue1.png?width=1438&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fcea88745cbcc03d289cd5f7d7ebd8cb82eaa008&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is our first major contribution towards building foundational LLM capacity for India. &lt;/p&gt; &lt;p&gt;The research paper associated with this work can be found here: &lt;a href="https://arxiv.org/pdf/2504.07989"&gt;https://arxiv.org/pdf/2504.07989&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We believe in open source 100% and have released a Github repository here: &lt;a href="https://github.com/VizuaraAI/Tiny-Stories-Regional"&gt;https://github.com/VizuaraAI/Tiny-Stories-Regional&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Anyone can use this repository to build a Small Language Model (SLM) for their language of choice.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Here is how we built these models: &lt;/p&gt; &lt;p&gt;(1) We based our methodology on the TinyStories Paper which Microsoft released in 2023: &lt;a href="https://arxiv.org/abs/2305.07759"&gt;https://arxiv.org/abs/2305.07759&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(2) We generated the datasets in regional languages. &lt;/p&gt; &lt;p&gt;(3) We built a language model architecture from scratch for pre-training. &lt;/p&gt; &lt;p&gt;(4) During inference, we evaluated the model creativity, completeness, fluency and grammar. &lt;/p&gt; &lt;p&gt;(5) We used this framework as a proxy for comparing regional tokenizers.&lt;/p&gt; &lt;p&gt;I feel the biggest takeaway from this work is that the framework we have outlined can be utilized by the community to create SLMs fro underrepresented, regional languages. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jytv2q/open_sourcing_a_framework_to_build_slms_for_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jytv2q/open_sourcing_a_framework_to_build_slms_for_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jytv2q/open_sourcing_a_framework_to_build_slms_for_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T08:25:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy9g9y</id>
    <title>Waifu GPU for AI GF?</title>
    <updated>2025-04-13T14:53:23+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy9g9y/waifu_gpu_for_ai_gf/"&gt; &lt;img alt="Waifu GPU for AI GF?" src="https://external-preview.redd.it/BnNrd2081VNoP-o3smbLrBMM4J_6XEXvavemnoJv_qM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea747f555a0c92c80b42a36a37e0db735083b1b2" title="Waifu GPU for AI GF?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/lpqhvyq68mue1.png?width=1142&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad31ac4af8529144b8d1be6323d09048cbf4d8b4"&gt;https://videocardz.com/newz/asus-officially-reveals-first-geforce-rtx-5060-ti-ahead-of-launch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I dont know these characters, but is this the future of mankind?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy9g9y/waifu_gpu_for_ai_gf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy9g9y/waifu_gpu_for_ai_gf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy9g9y/waifu_gpu_for_ai_gf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T14:53:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jycfvf</id>
    <title>You can preview quantizations of Llama 4 Maverick 17Bx128E at acceptable speeds even without the necessary memory</title>
    <updated>2025-04-13T17:04:14+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably many already know this, but with llama.cpp it's possible to perform inference off models larger than the available total physical memory; this is thanks to the magic of &lt;code&gt;mmap&lt;/code&gt;. Inference speed might be surprisingly faster than you'd think.&lt;/p&gt; &lt;p&gt;I tested this with &lt;a href="https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/tree/main/UD-IQ2_M"&gt;Llama-4-Maverick-17B-128E-Instruct-UD-IQ2_M&lt;/a&gt;, which is about 143 GB in total and shouldn't fit within my 64GB of DDR4 memory + one RTX3090 (24GB).&lt;/p&gt; &lt;p&gt;It takes a while for prompt processing to occur (admittedly at a fairly slow rate compared to normal), during which NVMe reads appear to be intense (5-6 GiB/s), which can be tracked on Linux with &lt;code&gt;iostat -s 1&lt;/code&gt;, but once that is done, inference speed is fairly decent.&lt;/p&gt; &lt;p&gt;Here's a benchmark with &lt;code&gt;llama-bench&lt;/code&gt; (I couldn't load more than 3 model layers on the GPU):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# ./build/bin/llama-bench -m ~/models/Llama-4-Maverick-17B-128E-Instruct-UD-IQ2_M.gguf -ngl 3 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 CUDA devices: Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes | model | size | params | backend | ngl | test | t/s | | ------------------------------------------ | ---------: | ---------: | ---------- | --: | ------------: | -------------------: | | llama4 17Bx128E (Maverick) IQ2_M - 2.7 bpw | 143.06 GiB | 400.71 B | CUDA | 3 | pp512 | 16.43 ¬± 0.25 | | llama4 17Bx128E (Maverick) IQ2_M - 2.7 bpw | 143.06 GiB | 400.71 B | CUDA | 3 | tg128 | 3.45 ¬± 0.26 | build: 06bb53ad (5115) # free total used free shared buff/cache available Mem: 65523176 8262924 600336 184900 57572992 57260252 Swap: 65523172 14129384 51393788 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More details for the flag that would prevent this behavior (disabling &lt;code&gt;mmap&lt;/code&gt;): &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/1876"&gt;https://github.com/ggml-org/llama.cpp/discussions/1876&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;code&gt;--no-mmap&lt;/code&gt;: Do not memory-map the model. By default, models are mapped into memory, which allows the system to load only the necessary parts of the model as needed. However, if the model is larger than your total amount of RAM or if your system is low on available memory, using mmap might increase the risk of pageouts, negatively impacting performance. Disabling mmap results in slower load times but may reduce pageouts if you're not using &lt;code&gt;--mlock&lt;/code&gt;. Note that if the model is larger than the total amount of RAM, turning off mmap would prevent the model from loading at all.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: from a suggestion in the comments below by PhoenixModBot, starting Llama.cpp with &lt;code&gt;-ngl 999 -ot \\d+.ffn_.*_exps.=CPU&lt;/code&gt; can increase inference speed to &lt;strong&gt;8~18 tokens/s&lt;/strong&gt; (depending on which experts get cached on RAM). What this does is loading the shared model parameters on the GPU, while keeping the FFN layers (the routed experts) on the CPU (RAM). This is documented here: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/11397"&gt;https://github.com/ggml-org/llama.cpp/pull/11397&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Additionally, in my own tests I've observed better prompt processing speeds by configuring both the physical and logical batch size to the same value of 2048. This can increase memory usage, though. &lt;code&gt;-b 2048 -ub 2048&lt;/code&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jycfvf/you_can_preview_quantizations_of_llama_4_maverick/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jycfvf/you_can_preview_quantizations_of_llama_4_maverick/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jycfvf/you_can_preview_quantizations_of_llama_4_maverick/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T17:04:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyr38c</id>
    <title>It's been a while since Zhipu AI released a new GLM model</title>
    <updated>2025-04-14T05:06:41+00:00</updated>
    <author>
      <name>/u/matteogeniaccio</name>
      <uri>https://old.reddit.com/user/matteogeniaccio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...but seriously, I'm hyped by the new glm-4 32b coming today&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matteogeniaccio"&gt; /u/matteogeniaccio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyr38c/its_been_a_while_since_zhipu_ai_released_a_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyr38c/its_been_a_while_since_zhipu_ai_released_a_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyr38c/its_been_a_while_since_zhipu_ai_released_a_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T05:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jxy26m</id>
    <title>Sam Altman: "We're going to do a very powerful open source model... better than any current open source model out there."</title>
    <updated>2025-04-13T02:55:45+00:00</updated>
    <author>
      <name>/u/mw11n19</name>
      <uri>https://old.reddit.com/user/mw11n19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxy26m/sam_altman_were_going_to_do_a_very_powerful_open/"&gt; &lt;img alt="Sam Altman: &amp;quot;We're going to do a very powerful open source model... better than any current open source model out there.&amp;quot;" src="https://external-preview.redd.it/eDJobnVwZ3luaXVlMdXj0QNvtvvTvdLhyylbR9Y6PzQjPjUyfN1eoWAw2jEe.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a5f48835aebe28a468ef3c09a1d306d926d0876" title="Sam Altman: &amp;quot;We're going to do a very powerful open source model... better than any current open source model out there.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mw11n19"&gt; /u/mw11n19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wzjs6qgyniue1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxy26m/sam_altman_were_going_to_do_a_very_powerful_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jxy26m/sam_altman_were_going_to_do_a_very_powerful_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T02:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyquyo</id>
    <title>AlexBefest's CardProjector-v4 series</title>
    <updated>2025-04-14T04:52:05+00:00</updated>
    <author>
      <name>/u/AlexBefest</name>
      <uri>https://old.reddit.com/user/AlexBefest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Name: AlexBefest/CardProjector-27B-v4&lt;/p&gt; &lt;p&gt;Model URL: &lt;a href="https://huggingface.co/AlexBefest/CardProjector-27B-v4"&gt;https://huggingface.co/AlexBefest/CardProjector-27B-v4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model Author: AlexBefest, &lt;a href="https://www.reddit.com/user/AlexBefest/"&gt;u/AlexBefest&lt;/a&gt;, &lt;a href="https://huggingface.co/AlexBefest"&gt;AlexBefest&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What's new in v4?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Absolute focus on personality development! This version places an absolute emphasis on designing character personalities, focusing on depth and realism. Eight (!) large datasets were collected, oriented towards all aspects of in-depth personality development. Extensive training was also conducted on a dataset of MBTI profiles with Enneagrams from psychology. The model was carefully trained to select the correct personality type according to both the MBTI and Enneagram systems. I highly recommend using these systems (see Usage recommendations); they provide an incredible boost to character realism. I conducted numerous tests with many RP models ranging from 24-70B parameters, and the MBTI profile system significantly impacts the understanding of the character's personality (especially on 70B models), making the role-playing performance much more realistic. You can see an example of a character's MBTI profile &lt;a href="https://www.personality-database.com/profile/7610/muffins-derpy-hooves-ditzy-doo-my-little-pony-friendship-is-magic-2010-mbti-personality-type"&gt;here&lt;/a&gt;. Currently, version V4 yields the deepest and most realistic characters.&lt;/li&gt; &lt;li&gt;Reduced likelihood of positive bias! I collected a large toxic dataset focused on creating and editing aggressive, extremely cruel, and hypersexualized characters, as well as transforming already &amp;quot;good harmless&amp;quot; characters into extremely cruel anti-versions of the original. Thanks to this, it was possible to significantly reduce the overall positive bias (especially in Gemma 3, where it is quite pronounced in its vanilla state), and make the model more balanced and realistic in terms of creating negative characters. It will no longer strive at all costs to create a cute, kind, ideal character, unless specifically asked to do so. All you need to do is just ask the model to &amp;quot;not make a positive character, but create a realistic one,&amp;quot; and with that one phrase, the entire positive bias goes away.&lt;/li&gt; &lt;li&gt;Moving to Gemma 3! After a series of experiments, it turned out that this model is ideally suited for the task of character design, as it possesses much more developed creative writing skills and higher general knowledge compared to Mistral 2501 in its vanilla state. Gemma 3 also seemed much more logical than its French competitor.&lt;/li&gt; &lt;li&gt;Vision ability! Due to the reason mentioned in the point above, you can freely use vision in this version. If you are using GGUF, you can download the mmproj model for the 27B version from bartowski (a vanilla mmproj will suffice, as I didn't perform vision tuning).&lt;/li&gt; &lt;li&gt;The overall quality of character generation has been significantly increased by expanding the dataset approximately 5 times compared to version V3.&lt;/li&gt; &lt;li&gt;This model is EXTREMELY sensitive to the user's prompt. So you should give instructions with caution, carefully considering.&lt;/li&gt; &lt;li&gt;In version V4, I concentrated only on one model size, 27B. Unfortunately, training multiple models at once is extremely expensive and consumes too much effort and time, so I decided it would be better to direct all my resources into just one model to avoid scattering focus. I hope you understand üôè&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Overview:&lt;/h1&gt; &lt;p&gt;CardProjector is a specialized series of language models, fine-tuned to generate character cards for &lt;strong&gt;SillyTavern&lt;/strong&gt; and &lt;strong&gt;now for creating characters in general&lt;/strong&gt;. These models are designed to assist creators and roleplayers by automating the process of crafting detailed and well-structured character cards, ensuring compatibility with SillyTavern's format.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexBefest"&gt; /u/AlexBefest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyquyo/alexbefests_cardprojectorv4_series/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyquyo/alexbefests_cardprojectorv4_series/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyquyo/alexbefests_cardprojectorv4_series/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T04:52:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyhd6i</id>
    <title>[2503.23817] MVDRAM: Enabling GeMV Execution in Unmodified DRAM for Low-Bit LLM Acceleration</title>
    <updated>2025-04-13T20:35:30+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2503.23817"&gt;https://arxiv.org/abs/2503.23817&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;General matrix-vector multiplication (GeMV) remains a critical latency bottleneck in large language model (LLM) inference, even with quantized low-bit models. Processing-Using-DRAM (PUD), an analog in-DRAM computing technique, has the potential to repurpose on-device DRAM as a GeMV engine, offering additional high-throughput processing capabilities to widespread consumer devices without DRAM modifications. However, applying PUD to GeMV operations in the LLM inference pipeline incurs significant overheads before and after in-DRAM computation, diminishing the benefits of its high-throughput processing capabilities. This paper presents MVDRAM, the first practical system to accelerate GeMV operations for low-bit LLM inference using unmodified DRAM. By leveraging the data sharing patterns and mathematical linearity in GeMV operations, MVDRAM orchestrates the processor and DRAM to eliminate the costs associated with pre-arranging inputs and bit-transposition of outputs required in conventional PUD approaches. Our experimental evaluation with four DDR4 DRAM modules shows that MVDRAM achieves comparable or even better inference speed than the processor-based implementation for GeMV operations in low-bit (under 4-bit) LLM. In particular, MVDRAM achieves up to 7.29√ó speedup and 30.5√ó energy efficiency for low-bit GeMV operations. For end-to-end LLM inference, MVDRAM achieves 2.18√ó and 1.31√ó throughput improvements, along with 3.04√ó and 2.35√ó energy efficiency, for 2-bit and 4-bit quantized low-bit models, respectively. MVDRAM has the potential to redefine the AI hardware landscape by demonstrating the feasibility of standard DRAM as an LLM accelerator.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.23817"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyhd6i/250323817_mvdram_enabling_gemv_execution_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyhd6i/250323817_mvdram_enabling_gemv_execution_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T20:35:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy8h2i</id>
    <title>Skywork-OR1: new SOTA 32B thinking model with open weight, training code, and training data</title>
    <updated>2025-04-13T14:08:06+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8h2i/skyworkor1_new_sota_32b_thinking_model_with_open/"&gt; &lt;img alt="Skywork-OR1: new SOTA 32B thinking model with open weight, training code, and training data" src="https://b.thumbs.redditmedia.com/Xv5xeh-T-_FI_nUFqM0bvDA9nB-t2tXuTjAVsmlXjdE.jpg" title="Skywork-OR1: new SOTA 32B thinking model with open weight, training code, and training data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;github repo: &lt;a href="https://github.com/SkyworkAI/Skywork-OR1"&gt;https://github.com/SkyworkAI/Skywork-OR1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;blog: &lt;a href="https://capricious-hydrogen-41c.notion.site/Skywork-Open-Reasoner-Series-1d0bc9ae823a80459b46c149e4f51680"&gt;https://capricious-hydrogen-41c.notion.site/Skywork-Open-Reasoner-Series-1d0bc9ae823a80459b46c149e4f51680&lt;/a&gt;&lt;/p&gt; &lt;p&gt;huggingface: &lt;a href="https://huggingface.co/collections/Skywork/skywork-or1-67fa1bcb41b436ef2def76b9"&gt;https://huggingface.co/collections/Skywork/skywork-or1-67fa1bcb41b436ef2def76b9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uuodxdre0mue1.png?width=1532&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0567ead14bd49bdc33066bf3bca19e1ad566676e"&gt;https://preview.redd.it/uuodxdre0mue1.png?width=1532&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0567ead14bd49bdc33066bf3bca19e1ad566676e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8h2i/skyworkor1_new_sota_32b_thinking_model_with_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8h2i/skyworkor1_new_sota_32b_thinking_model_with_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8h2i/skyworkor1_new_sota_32b_thinking_model_with_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T14:08:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy813d</id>
    <title>From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models</title>
    <updated>2025-04-13T13:47:07+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2504.06214"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy813d/from_128k_to_4m_efficient_training_of_ultralong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy813d/from_128k_to_4m_efficient_training_of_ultralong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T13:47:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyk399</id>
    <title>Dual 5090 va single 5090</title>
    <updated>2025-04-13T22:40:38+00:00</updated>
    <author>
      <name>/u/EasyConference4177</name>
      <uri>https://old.reddit.com/user/EasyConference4177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk399/dual_5090_va_single_5090/"&gt; &lt;img alt="Dual 5090 va single 5090" src="https://preview.redd.it/z1xl2ob1koue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a09792ff8b0785b5b36ea4cb15fed716f6a7feaf" title="Dual 5090 va single 5090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Man these dual 5090s are awesome. Went from 4t/s on 29b Gemma 3 to 28t/s when going from 1 to 2. I love these things! Easily runs 70b fast! I only wish they were a little cheaper but can‚Äôt wait till the RTX 6000 pro comes out with 96gb because I am totally eyeballing the crap out of it‚Ä¶. Who needs money when u got vram!!!&lt;/p&gt; &lt;p&gt;Btw I got 2 fans right under earn, 5 fans in front, 3 on top and one mac daddy on the back, and bout to put the one that came with the gigabyte 5090 on it too!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasyConference4177"&gt; /u/EasyConference4177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z1xl2ob1koue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk399/dual_5090_va_single_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk399/dual_5090_va_single_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T22:40:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyo2ds</id>
    <title>Word Synth - Llama 3.2 tiny LLM with sampling parameters exposed</title>
    <updated>2025-04-14T02:09:01+00:00</updated>
    <author>
      <name>/u/Brave_Variety6275</name>
      <uri>https://old.reddit.com/user/Brave_Variety6275</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built this as an intuition builder around LLM sampling--it's a bit rough around the edges but sharing in case its useful to anyone else trying to get it straight which sampling parameters do what. &lt;/p&gt; &lt;p&gt;&lt;a href="http://wordsynth.latenthomer.com/"&gt;http://wordsynth.latenthomer.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Your browser will yell at you because I didn't use https. Sorry. &lt;/p&gt; &lt;p&gt;Also apologies if it breaks or is really slow, this was also an experiment to deploy. &lt;/p&gt; &lt;p&gt;Thanks for reading :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave_Variety6275"&gt; /u/Brave_Variety6275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyo2ds/word_synth_llama_32_tiny_llm_with_sampling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyo2ds/word_synth_llama_32_tiny_llm_with_sampling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyo2ds/word_synth_llama_32_tiny_llm_with_sampling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T02:09:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jys33y</id>
    <title>Finally got Local LLM running on rx 9070 xt using onnx and directml</title>
    <updated>2025-04-14T06:14:33+00:00</updated>
    <author>
      <name>/u/dharayM</name>
      <uri>https://old.reddit.com/user/dharayM</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No i am not talking about brainwashed llama that comes with adrenaline app.&lt;/p&gt; &lt;p&gt;With vulkan broken for windows and Linux, rocm not being supported for windows and seemingly broken for linux, directml was my only hope&lt;/p&gt; &lt;p&gt;only directml-onnx models works with my solution which essentially consists of phi models but something is better than nothing&lt;/p&gt; &lt;p&gt;Here is the repo:&lt;br /&gt; &lt;a href="https://github.com/dharay/directml-onnx-local-llm"&gt;https://github.com/dharay/directml-onnx-local-llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;this is a work in progress, will probably abandon once we gets rocm support for rx 9000 series on windows&lt;/p&gt; &lt;p&gt;helpful resources:&lt;br /&gt; &lt;a href="https://onnxruntime.ai/docs/genai/tutorials/phi3-python.html"&gt;https://onnxruntime.ai/docs/genai/tutorials/phi3-python.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dharayM"&gt; /u/dharayM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jys33y/finally_got_local_llm_running_on_rx_9070_xt_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jys33y/finally_got_local_llm_running_on_rx_9070_xt_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jys33y/finally_got_local_llm_running_on_rx_9070_xt_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T06:14:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy6ns6</id>
    <title>Coming soon‚Ä¶..</title>
    <updated>2025-04-13T12:36:19+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6ns6/coming_soon/"&gt; &lt;img alt="Coming soon‚Ä¶.." src="https://preview.redd.it/1cwv3wz7klue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abbae222e535c2c110583987226650f6391ac918" title="Coming soon‚Ä¶.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1cwv3wz7klue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6ns6/coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6ns6/coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T12:36:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jygxmu</id>
    <title>Open-Weights Model next week?</title>
    <updated>2025-04-13T20:16:32+00:00</updated>
    <author>
      <name>/u/MustBeSomethingThere</name>
      <uri>https://old.reddit.com/user/MustBeSomethingThere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jygxmu/openweights_model_next_week/"&gt; &lt;img alt="Open-Weights Model next week?" src="https://preview.redd.it/iph04cputnue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f58d7addbdbe94c34055c810ba04a1042cb757a3" title="Open-Weights Model next week?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MustBeSomethingThere"&gt; /u/MustBeSomethingThere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iph04cputnue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jygxmu/openweights_model_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jygxmu/openweights_model_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T20:16:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyk213</id>
    <title>Still true 3 months later</title>
    <updated>2025-04-13T22:38:59+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk213/still_true_3_months_later/"&gt; &lt;img alt="Still true 3 months later" src="https://preview.redd.it/7644n1vqjoue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0b79a5e35c4e594b33dc646534a2248d3db9159" title="Still true 3 months later" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They rushed the release so hard it's been full of implementation bugs. And let's not get started on the custom model to hill climb lmarena alop&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7644n1vqjoue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk213/still_true_3_months_later/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk213/still_true_3_months_later/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T22:38:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyu06v</id>
    <title>llama was so deep that now ex employee saying that we r not involved in that project</title>
    <updated>2025-04-14T08:36:06+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyu06v/llama_was_so_deep_that_now_ex_employee_saying/"&gt; &lt;img alt="llama was so deep that now ex employee saying that we r not involved in that project" src="https://preview.redd.it/49mfsia3irue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3266a093713e9cb503b3634a7a8b1f7fb0852f0" title="llama was so deep that now ex employee saying that we r not involved in that project" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/49mfsia3irue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyu06v/llama_was_so_deep_that_now_ex_employee_saying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyu06v/llama_was_so_deep_that_now_ex_employee_saying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T08:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyp2no</id>
    <title>If we had models like QwQ-32B and Gemma-3-27B two years ago, people would have gone crazy.</title>
    <updated>2025-04-14T03:04:47+00:00</updated>
    <author>
      <name>/u/Proud_Fox_684</name>
      <uri>https://old.reddit.com/user/Proud_Fox_684</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Imagine if we had QwQ-32B or Gemma-3-27B or some of the smaller models, 18-24 months ago. It would have been the craziest thing.&lt;/p&gt; &lt;p&gt;24 months ago, GPT-4 was released. GPT-4o was released 11 months ago. Sometimes we not only forgot how quick things have been moving, but we also forget how good these small models actually are.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proud_Fox_684"&gt; /u/Proud_Fox_684 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyp2no/if_we_had_models_like_qwq32b_and_gemma327b_two/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyp2no/if_we_had_models_like_qwq32b_and_gemma327b_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyp2no/if_we_had_models_like_qwq32b_and_gemma327b_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T03:04:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jysiwc</id>
    <title>DeepSeek will open-source parts of its inference engine ‚Äî sharing standalone features and optimizations instead of the full stack</title>
    <updated>2025-04-14T06:45:54+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jysiwc/deepseek_will_opensource_parts_of_its_inference/"&gt; &lt;img alt="DeepSeek will open-source parts of its inference engine ‚Äî sharing standalone features and optimizations instead of the full stack" src="https://external-preview.redd.it/-j5EXG21mJ1IrGfaacZfdTPmLfMidR-DBjShQEW0nM4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8674fe0d9158595daad240e374a62be90da4c4d6" title="DeepSeek will open-source parts of its inference engine ‚Äî sharing standalone features and optimizations instead of the full stack" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/deepseek-ai/open-infra-index/blob/main/OpenSourcing_DeepSeek_Inference_Engine/README.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jysiwc/deepseek_will_opensource_parts_of_its_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jysiwc/deepseek_will_opensource_parts_of_its_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T06:45:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jytw62</id>
    <title>DeepSeek is about to open-source their inference engine</title>
    <updated>2025-04-14T08:27:29+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jytw62/deepseek_is_about_to_opensource_their_inference/"&gt; &lt;img alt="DeepSeek is about to open-source their inference engine" src="https://preview.redd.it/1am95yongrue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=967ad74640babe443b3c9a2867547f568219bda6" title="DeepSeek is about to open-source their inference engine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek is about to open-source their inference engine, which is a modified version based on vLLM. Now, DeepSeek is preparing to contribute these modifications back to the community.&lt;/p&gt; &lt;p&gt;I really like the last sentence: 'with the goal of enabling the community to achieve state-of-the-art (SOTA) support from Day-0.'&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/deepseek-ai/open-infra-index/tree/main/OpenSourcing_DeepSeek_Inference_Engine"&gt;https://github.com/deepseek-ai/open-infra-index/tree/main/OpenSourcing_DeepSeek_Inference_Engine&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1am95yongrue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jytw62/deepseek_is_about_to_opensource_their_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jytw62/deepseek_is_about_to_opensource_their_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T08:27:29+00:00</published>
  </entry>
</feed>
