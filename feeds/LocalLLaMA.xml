<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-17T00:49:32+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i2kca9</id>
    <title>Zhipu AI added to US sanctions blacklist</title>
    <updated>2025-01-16T08:20:05+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is this the first time that a LLM producer has been sanctioned?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reuters.com/world/us/us-adds-16-entities-its-trade-blacklist-14-china-2025-01-15/"&gt;https://www.reuters.com/world/us/us-adds-16-entities-its-trade-blacklist-14-china-2025-01-15/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2kca9/zhipu_ai_added_to_us_sanctions_blacklist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2kca9/zhipu_ai_added_to_us_sanctions_blacklist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2kca9/zhipu_ai_added_to_us_sanctions_blacklist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T08:20:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1i26nk4</id>
    <title>ATTENTION IS ALL YOU NEED PT. 2 - TITANS: Learning to Memorize at Test Time</title>
    <updated>2025-01-15T20:16:09+00:00</updated>
    <author>
      <name>/u/AIGuy3000</name>
      <uri>https://old.reddit.com/user/AIGuy3000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2501.00663v1"&gt;https://arxiv.org/pdf/2501.00663v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The innovation in this field has been iterating at light speed, and I think we have something special here. I tried something similar but I‚Äôm no PhD student and the Math is beyond me. &lt;/p&gt; &lt;p&gt;TLDR; Google Research introduces Titans, a new Al model that learns to store information in a dedicated &amp;quot;long-term memory&amp;quot; at test time. This means it can adapt whenever it sees something surprising, updating its memory on-the-fly. Unlike standard Transformers that handle only the current text window, Titans keep a deeper, more permanent record-similar to short-term vs. long-term memory in humans. The method scales more efficiently (linear time) than traditional Transformers(qudratic time) for very long input sequences. i.e theoretically infinite context windows.&lt;/p&gt; &lt;p&gt;Don‚Äôt be mistaken, this isn‚Äôt just a next-gen ‚Äúartificial intelligence‚Äù, but a step towards to ‚Äúartificial consciousness‚Äù with persistent memory - IF we define consciousness as the ability to model internally(self-modeling), organize, integrate, and recollect of data (with respect to a real-time input)as posited by IIT‚Ä¶ would love to hear y‚Äôall‚Äôs thoughts üß†üëÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIGuy3000"&gt; /u/AIGuy3000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i26nk4/attention_is_all_you_need_pt_2_titans_learning_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i26nk4/attention_is_all_you_need_pt_2_titans_learning_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i26nk4/attention_is_all_you_need_pt_2_titans_learning_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T20:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2mnmp</id>
    <title>Releasing the paper "Enhancing Human-Like Responses in Large Language Models", along with the Human-Like DPO Dataset and Human-Like LLMs</title>
    <updated>2025-01-16T11:14:47+00:00</updated>
    <author>
      <name>/u/Weyaxi</name>
      <uri>https://old.reddit.com/user/Weyaxi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ Introducing our paper: &lt;strong&gt;Enhancing Human-Like Responses in Large Language Models.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We've been working on improving conversational AI with &lt;strong&gt;more natural, human-like responses&lt;/strong&gt;‚Äîwhile keeping performance strong on standard benchmarks!&lt;/p&gt; &lt;p&gt;üìÑ &lt;strong&gt;Paper:&lt;/strong&gt; &lt;a href="https://huggingface.co/papers/2501.05032"&gt;Enhancing Human-Like Responses in Large Language Models&lt;/a&gt;&lt;br /&gt; üìä &lt;strong&gt;Dataset:&lt;/strong&gt; &lt;a href="https://huggingface.co/datasets/HumanLLMs/Human-Like-DPO-Dataset"&gt;Human-Like DPO Dataset&lt;/a&gt;&lt;br /&gt; ü§ñ &lt;strong&gt;Models:&lt;/strong&gt; &lt;a href="https://huggingface.co/collections/HumanLLMs/human-like-llms-6759fa68f22e11eb1a10967e"&gt;Human-Like LLMs Collection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Related Tweet: &lt;a href="https://x.com/Weyaxi/status/1877763008257986846"&gt;https://x.com/Weyaxi/status/1877763008257986846&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What We Did:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Used &lt;strong&gt;synthetic datasets&lt;/strong&gt; generated from Llama3 family to fine-tune models with &lt;strong&gt;DPO&lt;/strong&gt; and &lt;strong&gt;LoRA&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Achieved &lt;strong&gt;90% selection rate&lt;/strong&gt; in human-likeness when compared with the offical instruct models we fine-tuned.&lt;/li&gt; &lt;li&gt;Maintained strong performance (nearly no loss) on benchmarks like Open LLM Leaderboard.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These models and our dataset are &lt;strong&gt;open-source&lt;/strong&gt; on Hugging Face‚Äîfeel free to test them out, fine-tune them further, or contribute! üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weyaxi"&gt; /u/Weyaxi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2mnmp/releasing_the_paper_enhancing_humanlike_responses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2mnmp/releasing_the_paper_enhancing_humanlike_responses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2mnmp/releasing_the_paper_enhancing_humanlike_responses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T11:14:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i34642</id>
    <title>Why do tools like Perplexity struggle to calculate accurate stats from different sources when the exact number is not posted online?</title>
    <updated>2025-01-17T00:46:39+00:00</updated>
    <author>
      <name>/u/vamos-viendo</name>
      <uri>https://old.reddit.com/user/vamos-viendo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been wondering why tools like Perplexity seem to fall short on calculating stats that don‚Äôt already exist online. Perplexity tries‚Äîwith its reasoning steps‚Äîbut the results often fail in accuracy or iterative depth. &lt;/p&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;‚ÄúWhat percentage of countries with universal healthcare also have female leaders?‚Äù&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If this functionality exists, I haven‚Äôt seen it work well. Curious‚Äîwhat do you think is the blocker here?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is it a complexity or cost issue (the multi-step iterative reasoning)?&lt;/li&gt; &lt;li&gt;Is the demand just not there?&lt;/li&gt; &lt;li&gt;Are these tools just focusing elsewhere?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vamos-viendo"&gt; /u/vamos-viendo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i34642/why_do_tools_like_perplexity_struggle_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i34642/why_do_tools_like_perplexity_struggle_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i34642/why_do_tools_like_perplexity_struggle_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T00:46:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i31sxj</id>
    <title>How to use chat templates for multicharacter roleplays?</title>
    <updated>2025-01-16T22:54:17+00:00</updated>
    <author>
      <name>/u/martinerous</name>
      <uri>https://old.reddit.com/user/martinerous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have implemented my own roleplay front-end for KoboldCpp. In contrast to SillyTavern and BackyardAI, my approach is not character-centric but rather scenario-centric. Both AI and the user can control multiple characters, and AI makes its own choice of who should speak next.&lt;/p&gt; &lt;p&gt;At first, I did not even bother to figure out how to use chat templates. I just send a simple example dialogue to the LLM together with my scenario:&lt;/p&gt; &lt;p&gt;Bob: Hi!&lt;/p&gt; &lt;p&gt;Anna: Hello!&lt;/p&gt; &lt;p&gt;Then I launch the generation and poll the API to check for the result. I look for a valid `Character Name:` marker in the response and allow only the characters that are setup for AI control. If I receive one more character marker, I stop the generation to avoid the infamous &amp;quot;speaking for others&amp;quot; issue, and clean up the response to remove the unnecessary text.&lt;/p&gt; &lt;p&gt;I'm testing it now and even Llama 3.2 3B seems to work quite OK with this setup.&lt;/p&gt; &lt;p&gt;However, I've heard that some models benefit from system prompts, and, as I understand, to pass the system prompt to the model, I need to use a proper chat template for the specific model. &lt;/p&gt; &lt;p&gt;And now we come to the root of the problem. &lt;strong&gt;Chat templates seem to be centered on the idea of only two parties - the user and the assistant. I have more parties. How would I encode their messages in a chat template?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A naive approach would be to send the system prompt with the proper formatting for the template, and then just dump the entire accumulated context with the scenario, character descriptions and all the chat messages into a single &amp;quot;assistant&amp;quot; message and ignore the user part of the template completely. &lt;/p&gt; &lt;p&gt;But wouldn't this somehow make the model less smart and not obey the scenario as well as it would if I separate the chat messages and create a single assistant (or user) message for every character's reply? &lt;/p&gt; &lt;p&gt;What are the practical effects of the chat template on the inference quality? Is the chat template just a convenient wrapper to properly separate messages in more complex situations or does it actually improve the model's behavior?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martinerous"&gt; /u/martinerous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i31sxj/how_to_use_chat_templates_for_multicharacter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i31sxj/how_to_use_chat_templates_for_multicharacter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i31sxj/how_to_use_chat_templates_for_multicharacter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T22:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2ww9q</id>
    <title>benchmarks and real world comparisons QwQ 72B vs. DeepSeek V3 vs. Claude 3.5 Sonnet vs. Llama405B</title>
    <updated>2025-01-16T19:20:51+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking specifically at these models and want to understand how they compare in real world situations. Hoping someone has a good table and details on what model did best for a particular task.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ww9q/benchmarks_and_real_world_comparisons_qwq_72b_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ww9q/benchmarks_and_real_world_comparisons_qwq_72b_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ww9q/benchmarks_and_real_world_comparisons_qwq_72b_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T19:20:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2ycgi</id>
    <title>Thoughts on Langfuse?</title>
    <updated>2025-01-16T20:22:15+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And the other observatbility frameworks. &lt;/p&gt; &lt;p&gt;Did you find them useful? Which one do you recommend? What about their eval features (eg llm as a judge) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ycgi/thoughts_on_langfuse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ycgi/thoughts_on_langfuse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ycgi/thoughts_on_langfuse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T20:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2xf52</id>
    <title>Thoughts on an open source AI Agent Marketplace?</title>
    <updated>2025-01-16T19:43:06+00:00</updated>
    <author>
      <name>/u/StatisticianSome5986</name>
      <uri>https://old.reddit.com/user/StatisticianSome5986</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been thinking about how scattered AI agent projects are and how expensive LLMs will be in terms of GPU costs, especially for larger projects in the future. &lt;/p&gt; &lt;p&gt;There are two main problems I've identified. First, we have cool stuff on GitHub, but it‚Äôs tough to figure out which ones are reliable or to run them if you‚Äôre not super technical. There are emerging AI agent marketplaces for non-technical people, but it is difficult to trust an AI agent without seeing them as they still require customization. &lt;/p&gt; &lt;p&gt;The second problem is that as LLMs become more advanced, creating AI agents that require more GPU power will be difficult. So, in the next few years, I think larger companies will completely monopolize AI agents of scale because they will be the only ones able to afford the GPU power for advanced models. In fact, if there was a way to do this, the general public could benefit more. &lt;/p&gt; &lt;p&gt;So my idea is a website that ranks these open-source AI agents by performance (e.g., the top 5 for coding tasks, the top five for data analysis, etc.) and then provides a simple ‚ÄòLaunch‚Äô button to run them on a cloud GPU for non-technical users (with the GPU cost paid by users in a pay as you go model). Users could upload a dataset or input a prompt, and boom‚Äîthe agent does the work. Meanwhile, the community can upvote or provide feedback on which agents actually work best because they are open-source. I think that for the top 5-10 agents, the website can provide efficiency ratings on different LLMs with no cost to the developers as an incentive to code open source (in the future).&lt;/p&gt; &lt;p&gt;In line with this, for larger AI agent models that require more GPU power, the website can integrate a crowd-funding model where a certain benchmark is reached, and the agent will run. Everyone who contributes to the GPU cost can benefit from the agent once the benchmark is reached, and people can see the work of the coder/s each day. I see this option as more catered for passion projects/independent research where, otherwise, the developers or researchers will not have enough funds to test their agents. This could be a continuous funding effort for people really needing/believing in the potential of that agent, causing big models to need updating, retraining, or fine-tuning.&lt;/p&gt; &lt;p&gt;The website can also offer closed repositories, and developers can choose the repo type they want to use. However, I think community feedback and the potential to run the agents on different LLMs for no cost to test their efficiencies is a good incentive for developers to choose open-source development. I see the open-source models as being perceived as more reliable by the community and having continuous feedback. &lt;/p&gt; &lt;p&gt;If done well, this platform could democratize access to advanced AI agents, bridging the gap between complex open-source code and real-world users who want to leverage it without huge setup costs. It can also create an incentive to prevent larger corporations from monopolizing AI research and advanced agents due to GPU costs. &lt;/p&gt; &lt;p&gt;Any thoughts on this? I would appreciate any comments/dms.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StatisticianSome5986"&gt; /u/StatisticianSome5986 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2xf52/thoughts_on_an_open_source_ai_agent_marketplace/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2xf52/thoughts_on_an_open_source_ai_agent_marketplace/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2xf52/thoughts_on_an_open_source_ai_agent_marketplace/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T19:43:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1i30bjx</id>
    <title>What LLMs Do You Recommend For a RTX 2060 (6 GB) For Roleplay?</title>
    <updated>2025-01-16T21:48:24+00:00</updated>
    <author>
      <name>/u/AdvertisingOk6742</name>
      <uri>https://old.reddit.com/user/AdvertisingOk6742</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i‚Äôm interested in both sfw, nsfw and even nsfl roleplay&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdvertisingOk6742"&gt; /u/AdvertisingOk6742 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i30bjx/what_llms_do_you_recommend_for_a_rtx_2060_6_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i30bjx/what_llms_do_you_recommend_for_a_rtx_2060_6_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i30bjx/what_llms_do_you_recommend_for_a_rtx_2060_6_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T21:48:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2lh3b</id>
    <title>All new SOTA MOE open source model, up to 4M context. - MiniMax-AI/MiniMax-01</title>
    <updated>2025-01-16T09:48:38+00:00</updated>
    <author>
      <name>/u/bidet_enthusiast</name>
      <uri>https://old.reddit.com/user/bidet_enthusiast</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2lh3b/all_new_sota_moe_open_source_model_up_to_4m/"&gt; &lt;img alt="All new SOTA MOE open source model, up to 4M context. - MiniMax-AI/MiniMax-01" src="https://external-preview.redd.it/CaDa8UUx90v9PcEOeKGi-HSkE2urc6XyHG74Upv4XCw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=951db4cb8b0737b142d2311a74092a236c8f4e90" title="All new SOTA MOE open source model, up to 4M context. - MiniMax-AI/MiniMax-01" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bidet_enthusiast"&gt; /u/bidet_enthusiast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MiniMax-AI/MiniMax-01"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2lh3b/all_new_sota_moe_open_source_model_up_to_4m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2lh3b/all_new_sota_moe_open_source_model_up_to_4m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T09:48:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2pvpc</id>
    <title>Now you can running InternLM3 8B using Qualcomm NPU with PowerServe!</title>
    <updated>2025-01-16T14:17:23+00:00</updated>
    <author>
      <name>/u/Zealousideal_Bad_52</name>
      <uri>https://old.reddit.com/user/Zealousideal_Bad_52</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2pvpc/now_you_can_running_internlm3_8b_using_qualcomm/"&gt; &lt;img alt="Now you can running InternLM3 8B using Qualcomm NPU with PowerServe!" src="https://external-preview.redd.it/vqjX5SwLNlGMS1SSiHAT804_sRvPBHuMxyMU2GJpb1U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fe0aaff34a61b4f8f3fa0a386d0f9d14fa68f78" title="Now you can running InternLM3 8B using Qualcomm NPU with PowerServe!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We introduced &lt;strong&gt;PowerServe&lt;/strong&gt;, a serving framework designed specifically for Qualcomm NPU. Now we have already support &lt;strong&gt;Qwen, Llama&lt;/strong&gt; and &lt;strong&gt;InternLM3 8B&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/powerserve-project/PowerServe"&gt;powerserve-project/PowerServe: High-speed and easy-use LLM serving framework for local deployment (github.com)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Current open-source serving frameworks perform poorly in prefill speed on mobile devices, mainly due to limited CPU computing power. So we design PowerServe, a serving framework designed specifically for Qualcomm NPU, which achieves a prefill speed of &lt;strong&gt;1000&lt;/strong&gt; tokens/s of tokens per second for &lt;strong&gt;3B&lt;/strong&gt; models. This represents a &lt;strong&gt;100x&lt;/strong&gt; speedup compared to llama.cpp's &lt;strong&gt;15&lt;/strong&gt; tokens per second. For InternLM 8B, you can run it with &lt;strong&gt;250&lt;/strong&gt; tokens/s, significantly accelerating the prefill speed.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1i2pvpc/video/y93hx9ss6dde1/player"&gt;Running InternLM3 8B with Qualcomm 8Gen3 NPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pzxssjtw6dde1.png?width=2056&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2918387653f5940983a6718fd10bf9659d458e52"&gt;Performance comparison between Llama.cpp and PowerServe.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal_Bad_52"&gt; /u/Zealousideal_Bad_52 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2pvpc/now_you_can_running_internlm3_8b_using_qualcomm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2pvpc/now_you_can_running_internlm3_8b_using_qualcomm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2pvpc/now_you_can_running_internlm3_8b_using_qualcomm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T14:17:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2nkui</id>
    <title>Do you think that LLMs can do better natural language translation than services like DeepL, GoogleTranslate, Microsoft Translate etc.?</title>
    <updated>2025-01-16T12:14:11+00:00</updated>
    <author>
      <name>/u/sassyhusky</name>
      <uri>https://old.reddit.com/user/sassyhusky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My personal experience (which could be very subjective) with these translators is that even regular old chat bots with not much prompt engineering already produce better results with translations. Is this really just an unpopular opinion?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sassyhusky"&gt; /u/sassyhusky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2nkui/do_you_think_that_llms_can_do_better_natural/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2nkui/do_you_think_that_llms_can_do_better_natural/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2nkui/do_you_think_that_llms_can_do_better_natural/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T12:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i27l37</id>
    <title>Deepseek is overthinking</title>
    <updated>2025-01-15T20:57:13+00:00</updated>
    <author>
      <name>/u/Mr_Jericho</name>
      <uri>https://old.reddit.com/user/Mr_Jericho</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i27l37/deepseek_is_overthinking/"&gt; &lt;img alt="Deepseek is overthinking" src="https://preview.redd.it/rz378lgd18de1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=deff4f920457d1affd3bc98d78e4fc3601dda4b9" title="Deepseek is overthinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Jericho"&gt; /u/Mr_Jericho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rz378lgd18de1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i27l37/deepseek_is_overthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i27l37/deepseek_is_overthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T20:57:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1i30yy4</id>
    <title>Where do people get news about upcoming LLM releases?</title>
    <updated>2025-01-16T22:17:01+00:00</updated>
    <author>
      <name>/u/gamblingapocalypse</name>
      <uri>https://old.reddit.com/user/gamblingapocalypse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm curious about how people stay up-to-date on news about upcoming LLM releases, especially ones that haven‚Äôt been released yet. Are there specific websites, forums, newsletters, or communities you follow to learn about this kind of stuff?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gamblingapocalypse"&gt; /u/gamblingapocalypse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i30yy4/where_do_people_get_news_about_upcoming_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i30yy4/where_do_people_get_news_about_upcoming_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i30yy4/where_do_people_get_news_about_upcoming_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T22:17:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2ucxu</id>
    <title>I created a vscode extension that does inline edits using deepseek</title>
    <updated>2025-01-16T17:34:09+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ucxu/i_created_a_vscode_extension_that_does_inline/"&gt; &lt;img alt="I created a vscode extension that does inline edits using deepseek" src="https://external-preview.redd.it/c2Y1NHdiamk1ZWRlMSXLRLoBTWH7BkELeo8cMATHejXfU-O8HPWWGk2XwKZI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=822e126be45f676ab516da632460316140e9e985" title="I created a vscode extension that does inline edits using deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wo2fucji5ede1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ucxu/i_created_a_vscode_extension_that_does_inline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ucxu/i_created_a_vscode_extension_that_does_inline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T17:34:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2e23v</id>
    <title>I used Kokoro-82M, Llama 3.2, and Whisper Small to build a real-time speech-to-speech chatbot that runs locally on my MacBook!</title>
    <updated>2025-01-16T01:57:31+00:00</updated>
    <author>
      <name>/u/tycho_brahes_nose_</name>
      <uri>https://old.reddit.com/user/tycho_brahes_nose_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2e23v/i_used_kokoro82m_llama_32_and_whisper_small_to/"&gt; &lt;img alt="I used Kokoro-82M, Llama 3.2, and Whisper Small to build a real-time speech-to-speech chatbot that runs locally on my MacBook!" src="https://external-preview.redd.it/ajBjajZ2YTFpOWRlMdVERFdEQKrY8cptLv00gyZBVqtju60x3iy8w-FpWSZ2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7aa615b1ccbb81cee65b5735b41605e27fcb9ed" title="I used Kokoro-82M, Llama 3.2, and Whisper Small to build a real-time speech-to-speech chatbot that runs locally on my MacBook!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tycho_brahes_nose_"&gt; /u/tycho_brahes_nose_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yw01bva1i9de1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2e23v/i_used_kokoro82m_llama_32_and_whisper_small_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2e23v/i_used_kokoro82m_llama_32_and_whisper_small_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T01:57:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2pbyp</id>
    <title>Seems like used 3090 price is up near $850/$900?</title>
    <updated>2025-01-16T13:50:36+00:00</updated>
    <author>
      <name>/u/Synaps3</name>
      <uri>https://old.reddit.com/user/Synaps3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a bit of a sanity check here; it seems like used 3090's on eBay are up from around $650-$700 two weeks ago to $850-$1000 depending on the model after the disappointing 5090 announcement. Is this still a decent value proposition for an inference box? I'm about to pull the trigger on an H12SSL-i, but am on the fence about whether to wait for a potentially non-existent price drop on 3090 after 5090's are actually available and people try to flip their current cards. Short term goal is 70b Q4 inference server and NVLink for training non-language models. Any thoughts from secondhand GPU purchasing veterans?&lt;/p&gt; &lt;p&gt;Edit: also, does anyone know how long NVIDIA tends to provide driver support for their cards? I read somehow that 3090s inherit A100 driver support but I haven't been able to find any verification of this. It'd be a shame to buy two and have them be end-of-life in a year or two.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Synaps3"&gt; /u/Synaps3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2pbyp/seems_like_used_3090_price_is_up_near_850900/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2pbyp/seems_like_used_3090_price_is_up_near_850900/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2pbyp/seems_like_used_3090_price_is_up_near_850900/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T13:50:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i29wz5</id>
    <title>Google just released a new architecture</title>
    <updated>2025-01-15T22:38:26+00:00</updated>
    <author>
      <name>/u/FeathersOfTheArrow</name>
      <uri>https://old.reddit.com/user/FeathersOfTheArrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like a big deal? &lt;a href="https://x.com/behrouz_ali/status/1878859086227255347"&gt;Thread by lead author&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeathersOfTheArrow"&gt; /u/FeathersOfTheArrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2501.00663"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i29wz5/google_just_released_a_new_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i29wz5/google_just_released_a_new_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T22:38:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2xk8h</id>
    <title>Context &gt;</title>
    <updated>2025-01-16T19:49:05+00:00</updated>
    <author>
      <name>/u/MrCyclopede</name>
      <uri>https://old.reddit.com/user/MrCyclopede</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2xk8h/context/"&gt; &lt;img alt="Context &amp;gt;" src="https://preview.redd.it/281mgak4uede1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0cb3b1f73fa5516de74f745937554917437aeb73" title="Context &amp;gt;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrCyclopede"&gt; /u/MrCyclopede &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/281mgak4uede1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2xk8h/context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2xk8h/context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T19:49:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2y810</id>
    <title>Is DeepSeek V3 overhyped?</title>
    <updated>2025-01-16T20:17:06+00:00</updated>
    <author>
      <name>/u/YourAverageDev0</name>
      <uri>https://old.reddit.com/user/YourAverageDev0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have been using DeepSeek V3 for some time after all the time it came out. Coding wise (I work on web frontend, mostly react/svelte), I do not find it nearly as impressive as 3.5 Sonnet. The benchmarks seems to be matching, but the feel is just different, sometimes DeepSeek does give interesting stuff when asked. For me personally, it feels like a base 405B that has even been further scaled, it has little scars of brutal human RLHF (unlike OAI, LLaMa and etc Models). It just doesn't have that taste of Claude 3.5 Sonnet.&lt;/p&gt; &lt;p&gt;Curious what you guys think&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YourAverageDev0"&gt; /u/YourAverageDev0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2y810/is_deepseek_v3_overhyped/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2y810/is_deepseek_v3_overhyped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2y810/is_deepseek_v3_overhyped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T20:17:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1i31ji5</id>
    <title>What is ElevenLabs doing? How is it so good?</title>
    <updated>2025-01-16T22:42:26+00:00</updated>
    <author>
      <name>/u/Independent_Aside225</name>
      <uri>https://old.reddit.com/user/Independent_Aside225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically the title. What's their trick? On everything but voice, local models are pretty good for what they are, but ElevenLabs just blows everyone out of the water. &lt;/p&gt; &lt;p&gt;Is it full Transformer? Some sort of Diffuser? Do they model the human anatomy to add accuracy to the model? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent_Aside225"&gt; /u/Independent_Aside225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i31ji5/what_is_elevenlabs_doing_how_is_it_so_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i31ji5/what_is_elevenlabs_doing_how_is_it_so_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i31ji5/what_is_elevenlabs_doing_how_is_it_so_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T22:42:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2p6n3</id>
    <title>Why can't GPUs have removable memory like PC ram?</title>
    <updated>2025-01-16T13:43:00+00:00</updated>
    <author>
      <name>/u/Delicious-Farmer-234</name>
      <uri>https://old.reddit.com/user/Delicious-Farmer-234</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was thinking, why doesn't Intel, Nvidia, or AMD come up with the idea of being able to expand the memory? I get it that DDR6 is pricey but if one of them were to create modules and sell them wouldn't they be able to profit? Image if Intel came out with this first, I bet most of us will max out the vram and the whole community will push away from Nvidia and create better or comparable frameworks other cuda. Thoughts ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious-Farmer-234"&gt; /u/Delicious-Farmer-234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2p6n3/why_cant_gpus_have_removable_memory_like_pc_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2p6n3/why_cant_gpus_have_removable_memory_like_pc_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2p6n3/why_cant_gpus_have_removable_memory_like_pc_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T13:43:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2qokt</id>
    <title>Introducing Kokoro.js: a new JavaScript library for running Kokoro TTS (82M) locally in the browser w/ WASM.</title>
    <updated>2025-01-16T14:55:36+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2qokt/introducing_kokorojs_a_new_javascript_library_for/"&gt; &lt;img alt="Introducing Kokoro.js: a new JavaScript library for running Kokoro TTS (82M) locally in the browser w/ WASM." src="https://external-preview.redd.it/c2Y2dHB4cGdkZGRlMblxftDnj1ubBLQxBS031TPNonm7GOuytqVIBIDUD3XU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b475390d835ebe3032e27539598bb0f968273c4" title="Introducing Kokoro.js: a new JavaScript library for running Kokoro TTS (82M) locally in the browser w/ WASM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uv6trvpgddde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2qokt/introducing_kokorojs_a_new_javascript_library_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2qokt/introducing_kokorojs_a_new_javascript_library_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T14:55:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2n0il</id>
    <title>How would you build an LLM agent application without using LangChain?</title>
    <updated>2025-01-16T11:37:48+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2n0il/how_would_you_build_an_llm_agent_application/"&gt; &lt;img alt="How would you build an LLM agent application without using LangChain?" src="https://preview.redd.it/q1d445cdecde1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d53957b0b5cd83b245d383aec699f6fb075f1d50" title="How would you build an LLM agent application without using LangChain?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q1d445cdecde1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2n0il/how_would_you_build_an_llm_agent_application/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2n0il/how_would_you_build_an_llm_agent_application/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T11:37:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2t82i</id>
    <title>Introducing Wayfarer: a brutally challenging roleplay model trained to let you fail and die.</title>
    <updated>2025-01-16T16:46:20+00:00</updated>
    <author>
      <name>/u/Nick_AIDungeon</name>
      <uri>https://old.reddit.com/user/Nick_AIDungeon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One frustration we‚Äôve heard from many AI Dungeon players is that AI models are too nice, never letting them fail or die. So we decided to fix that. We trained a model we call Wayfarer where adventures are much more challenging with failure and death happening frequently.&lt;/p&gt; &lt;p&gt;We released it on AI Dungeon several weeks ago and players loved it, so we‚Äôve decided to open source the model for anyone to experience unforgivingly brutal AI adventures!&lt;/p&gt; &lt;p&gt;Would love to hear your feedback as we plan to continue to improve and open source similar models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LatitudeGames/Wayfarer-12B"&gt;https://huggingface.co/LatitudeGames/Wayfarer-12B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nick_AIDungeon"&gt; /u/Nick_AIDungeon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2t82i/introducing_wayfarer_a_brutally_challenging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2t82i/introducing_wayfarer_a_brutally_challenging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2t82i/introducing_wayfarer_a_brutally_challenging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T16:46:20+00:00</published>
  </entry>
</feed>
