<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-11T07:23:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j7n2s5</id>
    <title>Manus turns out to be just Claude Sonnet + 29 other tools, Reflection 70B vibes ngl</title>
    <updated>2025-03-10T01:18:27+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"&gt; &lt;img alt="Manus turns out to be just Claude Sonnet + 29 other tools, Reflection 70B vibes ngl" src="https://b.thumbs.redditmedia.com/aZROr3LtwGC89EWOHjMHoIbCvPQTB8Fs1jwIfYjEb8U.jpg" title="Manus turns out to be just Claude Sonnet + 29 other tools, Reflection 70B vibes ngl" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yi72dtz4krne1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f8e56eaed3a6e5eee634567caa3f64f1fbcc2f1"&gt;https://preview.redd.it/yi72dtz4krne1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f8e56eaed3a6e5eee634567caa3f64f1fbcc2f1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/Dorialexander/status/1898719861284454718"&gt;https://x.com/Dorialexander/status/1898719861284454718&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6b84agl7krne1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ce944225e1c92f7d0d466c896fdf2a80c667837"&gt;https://preview.redd.it/6b84agl7krne1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ce944225e1c92f7d0d466c896fdf2a80c667837&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/jianxliao/status/1898861051183349870"&gt;https://x.com/jianxliao/status/1898861051183349870&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T01:18:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8eyxq</id>
    <title>How to train Deepseek R1 with cases that build on top of each other?</title>
    <updated>2025-03-11T01:11:37+00:00</updated>
    <author>
      <name>/u/Fickle-Conversation1</name>
      <uri>https://old.reddit.com/user/Fickle-Conversation1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TITLE EDIT: How to fine tune Deepseek R1 with cases that build on the previous one?&lt;/p&gt; &lt;p&gt;I am very new to this, just thinking out loud...&lt;br /&gt; Say I want to make DeepSeek understands that I have a list of options [1, 2, 3, 4] for problem A&lt;/p&gt; &lt;p&gt;&lt;code&gt;{'question': 'what are the options available for problem A', 'cot': 'okay..........I have these options 1, 2, 3 4', 'respsonse': '...'}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;then can I have my second case based on the previous one say&lt;/p&gt; &lt;p&gt;&lt;code&gt;{'question': 'Is 6 a valid option for problem A?': 'cot': okay, 6 is not in the list of options in problem A, then it's not a valid option', 'response': '...'}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Is this even possible? Also is SFTTrainer a correct trainer to use? Thanks alot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fickle-Conversation1"&gt; /u/Fickle-Conversation1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8eyxq/how_to_train_deepseek_r1_with_cases_that_build_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8eyxq/how_to_train_deepseek_r1_with_cases_that_build_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8eyxq/how_to_train_deepseek_r1_with_cases_that_build_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T01:11:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j87q8g</id>
    <title>Insights about the frontier math benchmark.</title>
    <updated>2025-03-10T19:55:45+00:00</updated>
    <author>
      <name>/u/pier4r</name>
      <uri>https://old.reddit.com/user/pier4r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j87q8g/insights_about_the_frontier_math_benchmark/"&gt; &lt;img alt="Insights about the frontier math benchmark." src="https://preview.redd.it/9i47enyl3xne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9154ad9f0a41f80760fd091fd8b569a65f600129" title="Insights about the frontier math benchmark." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pier4r"&gt; /u/pier4r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9i47enyl3xne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j87q8g/insights_about_the_frontier_math_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j87q8g/insights_about_the_frontier_math_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T19:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7zzdt</id>
    <title>All about LLMs</title>
    <updated>2025-03-10T14:32:28+00:00</updated>
    <author>
      <name>/u/meme_watcher69420</name>
      <uri>https://old.reddit.com/user/meme_watcher69420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was given an offer to join this startup. They were impressed with my &amp;quot;knowledge&amp;quot; about AI and LLMs. But in reality, all my projects are made by pasting stuff from Claude, stackoverflow and improved with reading a few documents.&lt;/p&gt; &lt;p&gt;How do I get to know everything about setting up LLMs, integrating them into an application and deploying them? Is there a guide or a roadmap to it? I'll join this startup in a month so I got a bit of time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/meme_watcher69420"&gt; /u/meme_watcher69420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7zzdt/all_about_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7zzdt/all_about_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7zzdt/all_about_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T14:32:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j83md3</id>
    <title>Could GEMMA-3 Be Unveiled at GDC 2025 (March 18)?</title>
    <updated>2025-03-10T17:05:48+00:00</updated>
    <author>
      <name>/u/hCKstp4BtL</name>
      <uri>https://old.reddit.com/user/hCKstp4BtL</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://schedule.gdconf.com/session/beyond-the-hype-real-world-applications-of-google-ai-in-gaming-presented-by-google-play/911129"&gt;https://schedule.gdconf.com/session/beyond-the-hype-real-world-applications-of-google-ai-in-gaming-presented-by-google-play/911129&lt;/a&gt;&lt;/p&gt; &lt;p&gt;in this session description, we can read that they will talk about &amp;quot;Gemma models&amp;quot; (among other things). I think everyone knows about &amp;quot;Gemma 2&amp;quot; and there is no need to mention it because everyone knows how it works, right? Bigger chance is that they will show &amp;quot;Gemma 3&amp;quot; and they will release it shorly? because it seems to me that the deadline of May 20-21 (Google I/O) is a bit too late.&lt;/p&gt; &lt;p&gt;It looks like Google wants to focus the eyes of game developers on Gemma, so that they can combine the models with their games to create: ‚Äúnew AI-based game features and mechanics.‚Äù&lt;/p&gt; &lt;p&gt;... and to make it work, I think such a &amp;quot;Gemma 3&amp;quot; model should be prioritize with &amp;quot;perfect JSON generation&amp;quot; for the interface model&amp;lt;-&amp;gt;game and also improved instruction following.&lt;/p&gt; &lt;p&gt;I waiting for a small model (7b-9b) to be good enough to make a game with llm controlling npc (not only talk).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hCKstp4BtL"&gt; /u/hCKstp4BtL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83md3/could_gemma3_be_unveiled_at_gdc_2025_march_18/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83md3/could_gemma3_be_unveiled_at_gdc_2025_march_18/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j83md3/could_gemma3_be_unveiled_at_gdc_2025_march_18/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T17:05:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7usrm</id>
    <title>EuroBERT: A High-Performance Multilingual Encoder Model</title>
    <updated>2025-03-10T09:33:09+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7usrm/eurobert_a_highperformance_multilingual_encoder/"&gt; &lt;img alt="EuroBERT: A High-Performance Multilingual Encoder Model" src="https://external-preview.redd.it/CnsI7xgYaguHri1_uGabuT9boB9PVspWCoeHvZsE1IM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f747438c0575ca5fca91283bb815527cfb8627a" title="EuroBERT: A High-Performance Multilingual Encoder Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/EuroBERT/release"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7usrm/eurobert_a_highperformance_multilingual_encoder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7usrm/eurobert_a_highperformance_multilingual_encoder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T09:33:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8k03m</id>
    <title>gpu offload and recommendations</title>
    <updated>2025-03-11T05:53:10+00:00</updated>
    <author>
      <name>/u/im_not_here_</name>
      <uri>https://old.reddit.com/user/im_not_here_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using some tiny models, just because my laptop is old and relatively speaking very basic. But I know it can do slightly better, is there a recognised good base model for; checking sections of writing and making suggestions (academic mainly), basic coding, and maths help/explanations that will work well with 32GB RAM, and 1660ti 6GB card. I can always find more specific fine tunes later and experiment.&lt;/p&gt; &lt;p&gt;I have also noticed after trying some larger models than the tiny 1-3b ones, that either the gpu offload in lm studio doesn't work automatically, or I misunderstood what it was about. With some models that a vram calculator says should fit with context, by default it will not select all gpu offload and be a lot slower than if I manually put it to all (~35 compared to nearly ~60). Should I not do that manually? And if it is ok, is there a quick and easy way to know what to use? For some larger models than the vram I have tested, I had to test one by one until I found the gpu offload that worked the best. Surely that's not what everyone does?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/im_not_here_"&gt; /u/im_not_here_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8k03m/gpu_offload_and_recommendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8k03m/gpu_offload_and_recommendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8k03m/gpu_offload_and_recommendations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T05:53:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j892ii</id>
    <title>RTX 3090 supply drying up on marketplaces in Europe</title>
    <updated>2025-03-10T20:51:17+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems the flopped launches are leaving their traces in the GPU second hand markets. Even more so since the 4090 stopped production last fall already.&lt;/p&gt; &lt;p&gt;As popularity to self host models is on the rise and supply of new 24Gb+ cards stays dry, the all star for local AI models, the RTX 3090 is getting rare on marketplaces. In Switzerland they used to go for around CHF 650 - CHF 750. The lowest you find them now is 800.- if you're lucky, more likely CHF 900.-&lt;/p&gt; &lt;p&gt;Germany looks a little better at ‚Ç¨650 the lowest but these are usually gone within three days and most supply is around ‚Ç¨750 upwards. It's only a matter of time when sellers at the ‚Ç¨650 mark will dry up.&lt;/p&gt; &lt;p&gt;On international Ebay the cards go for $800 upwards, used to be lower if I remember correctly.&lt;/p&gt; &lt;p&gt;What is your experience, are you looking for 3090s? What's your choice for your home servers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j892ii/rtx_3090_supply_drying_up_on_marketplaces_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j892ii/rtx_3090_supply_drying_up_on_marketplaces_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j892ii/rtx_3090_supply_drying_up_on_marketplaces_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T20:51:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8780s</id>
    <title>Kokoro: Improving LLM's Emotional Intelligence [Research]</title>
    <updated>2025-03-10T19:33:42+00:00</updated>
    <author>
      <name>/u/yukiarimo</name>
      <uri>https://old.reddit.com/user/yukiarimo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yo community! Kokoro Research just dropped! It‚Äôs a prequel paper to upcoming research called, LOLI Trigger: Ludic Operant Learning Integration in Transcendent Emergence Triggering of LLMs‚Äô about making AI more humane! Coming this week!&lt;/p&gt; &lt;p&gt;This one talks more about new classification approach which later can be directly merge into an LLM model!&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://www.academia.edu/128122586/Kokoro_Improving_LLMs_Emotional_Intelligence"&gt;https://www.academia.edu/128122586/Kokoro_Improving_LLMs_Emotional_Intelligence&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can check other researches, especially TaMeR (novel training approach), and ELiTA (better datasets). Hope you like them! Note: this is mostly theoretical paper, do not expect too much math!&lt;/p&gt; &lt;p&gt;[THIS IS NOT AN AD, JUST SHARING STUFF WITH THE COMMUNITY]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yukiarimo"&gt; /u/yukiarimo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8780s/kokoro_improving_llms_emotional_intelligence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8780s/kokoro_improving_llms_emotional_intelligence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8780s/kokoro_improving_llms_emotional_intelligence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T19:33:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7t18m</id>
    <title>Framework and DIGITS suddenly seem underwhelming compared to the 512GB Unified Memory on the new Mac.</title>
    <updated>2025-03-10T07:15:44+00:00</updated>
    <author>
      <name>/u/Common_Ad6166</name>
      <uri>https://old.reddit.com/user/Common_Ad6166</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was holding out on purchasing a FrameWork desktop until we could see what kind of performance the DIGITS would get when it comes out in May. But now that Apple has announced the new M4 Max/ M3 Ultra Mac's with 512 GB Unified memory, the 128 GB options on the other two seem paltry in comparison. &lt;/p&gt; &lt;p&gt;Are we actually going to be locked into the Apple ecosystem for another decade? This can't be true!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Common_Ad6166"&gt; /u/Common_Ad6166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7t18m/framework_and_digits_suddenly_seem_underwhelming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7t18m/framework_and_digits_suddenly_seem_underwhelming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7t18m/framework_and_digits_suddenly_seem_underwhelming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T07:15:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j80hbo</id>
    <title>Hunyuan-TurboS.</title>
    <updated>2025-03-10T14:54:37+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://twitter.com/TXhunyuan/status/1899105803073958010"&gt;https://twitter.com/TXhunyuan/status/1899105803073958010&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j80hbo/hunyuanturbos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j80hbo/hunyuanturbos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j80hbo/hunyuanturbos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T14:54:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j85snw</id>
    <title>[Experimental] Control the 'Thinking Effort' of QwQ &amp; R1 Models with a Custom Logits Processor</title>
    <updated>2025-03-10T18:34:42+00:00</updated>
    <author>
      <name>/u/ASL_Dev</name>
      <uri>https://old.reddit.com/user/ASL_Dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed several posts lately discussing how the QwQ model tends to produce an excessive amount of tokens, often leading it to &amp;quot;overthink&amp;quot; unnecessarily. I've also seen some creative attempts to control this behavior using carefully crafted system prompts.&lt;/p&gt; &lt;p&gt;To help address this issue more systematically, I've put together a small and simple solution using a custom &lt;strong&gt;logits processor&lt;/strong&gt;. This approach dynamically adjusts the likelihood of the end-of-thinking token (&lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt;) appearing during generation.&lt;/p&gt; &lt;p&gt;The basic idea:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can set a &amp;quot;thinking effort&amp;quot; parameter (&lt;code&gt;0.0&lt;/code&gt; = minimal thinking, token &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; quickly appears; &lt;code&gt;1.0&lt;/code&gt; = normal behavior, &amp;gt;&lt;code&gt;1.0&lt;/code&gt; = it takes longer to output the &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; token).&lt;/li&gt; &lt;li&gt;The logic is straightforward: once the &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; token has been generated, the processor stops adjusting logits for that sequence.&lt;/li&gt; &lt;li&gt;This allows controlling how much the model thinks (or ‚Äúoverthinks‚Äù) without complicated prompt engineering.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've seen good results in reducing unnecessary thinking tokens in simple tasks, but I haven't yet extensively tested how this might influence longer chain-of-thought (CoT) reasoning.&lt;/p&gt; &lt;p&gt;I'd love for others to try it out and share your experiences or thoughts!&lt;/p&gt; &lt;p&gt;Here‚Äôs the repo with code and examples for both llama-cpp-python (gguf models) and Hugging Face Transformers (&lt;strong&gt;Note:&lt;/strong&gt; The code is still very raw, not optimized, and not organized lol... ‚Äîthis is just to share the basic idea quickly with the community!):&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/and270/thinking_effort_processor"&gt;https://github.com/and270/thinking_effort_processor&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Quick Example (Qwen-1.5B R1-Distill)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; What is the capital of France?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Regular Inference:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Okay, so I need to figure out what the capital of France is. I've heard a few things before, but I'm not entirely sure. Let me start by recalling what I know about France. France is a country in Europe, known for its diverse landscapes and vibrant culture. The name &amp;quot;France&amp;quot; itself comes from the French word &amp;quot;fran√ßais,&amp;quot; which means &amp;quot;french&amp;quot; or &amp;quot;colorful.&amp;quot; I think the capital is a significant city, maybe something like Paris or maybe another city...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;(The model generates a lengthy reasoning sequence before concluding)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;...To summarize, I believe the capital of France is Paris.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Thinking Effort Inference (0.1):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;The capital of France is Paris.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Any feedback or tests are very welcome!&lt;/p&gt; &lt;p&gt;Let me know your thoughts or experiences‚ÄîI'm especially curious how this affects your use-cases with the QwQ or similar models. üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ASL_Dev"&gt; /u/ASL_Dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j85snw/experimental_control_the_thinking_effort_of_qwq/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j85snw/experimental_control_the_thinking_effort_of_qwq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j85snw/experimental_control_the_thinking_effort_of_qwq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T18:34:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j83zkt</id>
    <title>Don't underestimate the power of RAG</title>
    <updated>2025-03-10T17:21:09+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83zkt/dont_underestimate_the_power_of_rag/"&gt; &lt;img alt="Don't underestimate the power of RAG" src="https://preview.redd.it/moz1h1pzbwne1.gif?width=640&amp;amp;crop=smart&amp;amp;s=adc13823a2909ba4af349f77c5405bf1ce990c2e" title="Don't underestimate the power of RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/moz1h1pzbwne1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83zkt/dont_underestimate_the_power_of_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j83zkt/dont_underestimate_the_power_of_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T17:21:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8f6nf</id>
    <title>Running QwQ-32B LLM locally: Model sharding between M1 MacBook Pro + RTX 4060 Ti</title>
    <updated>2025-03-11T01:22:20+00:00</updated>
    <author>
      <name>/u/Status-Hearing-4084</name>
      <uri>https://old.reddit.com/user/Status-Hearing-4084</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8f6nf/running_qwq32b_llm_locally_model_sharding_between/"&gt; &lt;img alt="Running QwQ-32B LLM locally: Model sharding between M1 MacBook Pro + RTX 4060 Ti" src="https://b.thumbs.redditmedia.com/7cMOZxgSRdWpuSo77V1xrJreEKqUtJqAKTR_RaISKXw.jpg" title="Running QwQ-32B LLM locally: Model sharding between M1 MacBook Pro + RTX 4060 Ti" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Successfully running QwQ-32B (@Alibaba_Qwen) across M1 MacBook Pro and RTX 4060 Ti through model sharding.&lt;/p&gt; &lt;p&gt;Demo video exceeds Reddit's size limit. You can view it here: [ &lt;a href="https://x.com/tensorblock_aoi/status/1899266661888512004"&gt;https://x.com/tensorblock_aoi/status/1899266661888512004&lt;/a&gt; ]&lt;/p&gt; &lt;p&gt;Hardware:&lt;/p&gt; &lt;p&gt;- MacBook Pro 2021 (M1 Pro, 16GB RAM)&lt;/p&gt; &lt;p&gt;- RTX 4060 Ti (16GB VRAM)&lt;/p&gt; &lt;p&gt;Model:&lt;/p&gt; &lt;p&gt;- QwQ-32B (Q4_K_M quantization)&lt;/p&gt; &lt;p&gt;- Original size: 20GB&lt;/p&gt; &lt;p&gt;- Distributed across devices with 16GB limitation&lt;/p&gt; &lt;p&gt;Implementation:&lt;/p&gt; &lt;p&gt;- Cross-architecture model sharding&lt;/p&gt; &lt;p&gt;- Custom memory management&lt;/p&gt; &lt;p&gt;- Parallel inference pipeline&lt;/p&gt; &lt;p&gt;- TensorBlock orchestration&lt;/p&gt; &lt;p&gt;Current Progress:&lt;/p&gt; &lt;p&gt;- Model successfully loaded and running&lt;/p&gt; &lt;p&gt;- Stable inference achieved&lt;/p&gt; &lt;p&gt;- Optimization in progress&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ij3j83poryne1.jpg?width=3176&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4dfa6929805da97868dda9ee90bfcee19d08a011"&gt;https://preview.redd.it/ij3j83poryne1.jpg?width=3176&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4dfa6929805da97868dda9ee90bfcee19d08a011&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're excited to announce TensorBlock, our upcoming local inference solution. The software enables efficient cross-device LLM deployment, featuring:&lt;/p&gt; &lt;p&gt;- Distributed inference across multiple hardware platforms&lt;/p&gt; &lt;p&gt;- Comprehensive support for Intel, AMD, NVIDIA, and Apple Silicon&lt;/p&gt; &lt;p&gt;- Smart memory management for resource-constrained devices&lt;/p&gt; &lt;p&gt;- Real-time performance monitoring and optimization&lt;/p&gt; &lt;p&gt;- User-friendly interface for model deployment and management&lt;/p&gt; &lt;p&gt;- Advanced parallel computing capabilities&lt;/p&gt; &lt;p&gt;We'll be releasing detailed benchmarks, comprehensive documentation, and deployment guides along with the software launch. Stay tuned for more updates on performance metrics and cross-platform compatibility testing.&lt;/p&gt; &lt;p&gt;Technical questions and feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Status-Hearing-4084"&gt; /u/Status-Hearing-4084 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8f6nf/running_qwq32b_llm_locally_model_sharding_between/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8f6nf/running_qwq32b_llm_locally_model_sharding_between/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8f6nf/running_qwq32b_llm_locally_model_sharding_between/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T01:22:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1j85q5m</id>
    <title>every LLM metric you need to know</title>
    <updated>2025-03-10T18:31:58+00:00</updated>
    <author>
      <name>/u/FlimsyProperty8544</name>
      <uri>https://old.reddit.com/user/FlimsyProperty8544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The best way to improve LLM performance is to consistently benchmark your model using a well-defined set of metrics throughout development, rather than relying on ‚Äúvibe check‚Äù coding‚Äîthis approach helps ensure that any modifications don‚Äôt inadvertently cause regressions.&lt;/p&gt; &lt;p&gt;I‚Äôve listed below some essential LLM metrics to know before you begin benchmarking your LLM. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;A Note about Statistical Metrics:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Traditional NLP evaluation methods like BERT and ROUGE are fast, affordable, and reliable. However, their reliance on reference texts and inability to capture the nuanced semantics of open-ended, often complexly formatted LLM outputs make them less suitable for production-level evaluations. &lt;/p&gt; &lt;p&gt;LLM judges are much more effective if you care about evaluation accuracy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RAG metrics&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-answer-relevancy"&gt;Answer Relevancy:&lt;/a&gt; measures the quality of your RAG pipeline's generator by evaluating how relevant the actual output of your LLM application is compared to the provided input&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-faithfulness"&gt;Faithfulness:&lt;/a&gt; measures the quality of your RAG pipeline's generator by evaluating whether the actual output factually aligns with the contents of your retrieval context&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-contextual-precision"&gt;Contextual Precision:&lt;/a&gt; measures your RAG pipeline's retriever by evaluating whether nodes in your retrieval context that are relevant to the given input are ranked higher than irrelevant ones.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-contextual-recall"&gt;Contextual Recall:&lt;/a&gt; measures the quality of your RAG pipeline's retriever by evaluating the extent of which the retrieval context aligns with the expected output&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-contextual-relevancy"&gt;Contextual Relevancy:&lt;/a&gt; measures the quality of your RAG pipeline's retriever by evaluating the overall relevance of the information presented in your retrieval context for a given input&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Agentic metrics&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-tool-correctness"&gt;Tool Correctness:&lt;/a&gt; assesses your LLM agent's function/tool calling ability. It is calculated by comparing whether every tool that is expected to be used was indeed called.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-task-completion"&gt;Task Completion:&lt;/a&gt; evaluates how effectively an LLM agent accomplishes a task as outlined in the input, based on tools called and the actual output of the agent.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Conversational metrics&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-role-adherence"&gt;Role Adherence:&lt;/a&gt; determines whether your LLM chatbot is able to adhere to its given role throughout a conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-knowledge-retention"&gt;Knowledge Retention:&lt;/a&gt; determines whether your LLM chatbot is able to retain factual information presented throughout a conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-conversation-completeness"&gt;Conversational Completeness:&lt;/a&gt; determines whether your LLM chatbot is able to complete an end-to-end conversation by satisfying user needs throughout a conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-conversation-relevancy"&gt;Conversational Relevancy:&lt;/a&gt; determines whether your LLM chatbot is able to consistently generate relevant responses throughout a conversation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Robustness&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-prompt-alignment"&gt;Prompt Alignment:&lt;/a&gt; measures whether your LLM application is able to generate outputs that aligns with any instructions specified in your prompt template.&lt;/li&gt; &lt;li&gt;Output Consistency: measures the consistency of your LLM output given the same input.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Custom metrics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Custom metrics are particularly effective when you have a specialized use case, such as in medicine or healthcare, where it is necessary to define your own criteria.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-llm-evals"&gt;GEval:&lt;/a&gt; a framework that uses LLMs with chain-of-thoughts (CoT) to evaluate LLM outputs based on ANY custom criteria.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-dag"&gt;DAG (Directed Acyclic Graphs):&lt;/a&gt; the most versatile custom metric for you to easily build deterministic decision trees for evaluation with the help of using LLM-as-a-judge&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Red-teaming metrics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There are hundreds of red-teaming metrics available, but bias, toxicity, and hallucination are among the most common. These metrics are particularly valuable for detecting harmful outputs and ensuring that the model maintains high standards of safety and reliability.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-bias"&gt;Bias&lt;/a&gt;: determines whether your LLM output contains gender, racial, or political bias.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-toxicity"&gt;Toxicity&lt;/a&gt;: evaluates toxicity in your LLM outputs.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-hallucination"&gt;Hallucination&lt;/a&gt;: determines whether your LLM generates factually correct information by comparing the output to the provided context&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Although this is quite lengthy, and a good starting place, it is by no means comprehensive. Besides this there are other categories of metrics like multimodal metrics, which can range from image quality metrics like image coherence to multimodal RAG metrics like multimodal contextual precision or recall. &lt;/p&gt; &lt;p&gt;For a more comprehensive list + calculations, you might want to visit &lt;a href="https://docs.confident-ai.com/"&gt;deepeval docs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/confident-ai/deepeval"&gt;Github Repo&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FlimsyProperty8544"&gt; /u/FlimsyProperty8544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j85q5m/every_llm_metric_you_need_to_know/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j85q5m/every_llm_metric_you_need_to_know/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j85q5m/every_llm_metric_you_need_to_know/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T18:31:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j83imv</id>
    <title>We tested open and closed models for embodied decision alignment, and we found Qwen 2.5 VL is surprisingly stronger than most closed frontier models.</title>
    <updated>2025-03-10T17:01:38+00:00</updated>
    <author>
      <name>/u/ProKil_Chu</name>
      <uri>https://old.reddit.com/user/ProKil_Chu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"&gt; &lt;img alt="We tested open and closed models for embodied decision alignment, and we found Qwen 2.5 VL is surprisingly stronger than most closed frontier models." src="https://external-preview.redd.it/Y3fzRflurvaH7gQ6GXbY5iJSr6_6d8TJO5p2Fiagr1c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e64d3affd381babd5da7c46900f14ec9e95d3813" title="We tested open and closed models for embodied decision alignment, and we found Qwen 2.5 VL is surprisingly stronger than most closed frontier models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1j83imv/video/t190t6fsewne1/player"&gt;https://reddit.com/link/1j83imv/video/t190t6fsewne1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One thing that surprised us during benchmarking with EgoNormia is that Qwen 2.5 VL is indeed a very strong model for vision which rivals Gemini 1.5/2.0, better than GPT-4o and Claude 3.5 Sonnet.&lt;/p&gt; &lt;p&gt;Please read the blog: &lt;a href="https://opensocial.world/articles/egonormia"&gt;https://opensocial.world/articles/egonormia&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Leaderboard: &lt;a href="https://egonormia.org"&gt;https://egonormia.org&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Eval code: &lt;a href="https://github.com/Open-Social-World/EgoNormia"&gt;https://github.com/Open-Social-World/EgoNormia&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ProKil_Chu"&gt; /u/ProKil_Chu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T17:01:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7r47l</id>
    <title>I just made an animation of a ball bouncing inside a spinning hexagon</title>
    <updated>2025-03-10T05:01:09+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/"&gt; &lt;img alt="I just made an animation of a ball bouncing inside a spinning hexagon" src="https://external-preview.redd.it/aHcybDc4eW5tc25lMWpXkBeJA0bkbXxKyNPWYhDqX6Z4Wwq4cQiczMXRiEBU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1910662e66472f313e9a9c19401be8a1be2f181a" title="I just made an animation of a ball bouncing inside a spinning hexagon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cy79860omsne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T05:01:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8i5s2</id>
    <title>Why doesn't Groq Sell its LPUs? By Extension, Why doesn't Google do that?</title>
    <updated>2025-03-11T03:57:03+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When Groq first announced and demoed its LPUs cluster, I was so excited. I believed that finally we get HW that's cost effective. But, it seems the company is not interested in selling its HW at all. &lt;/p&gt; &lt;p&gt;And I DON'T UNDERSTAND THE LOGIC BEHIND such a decision. Does is have something to do with Google since the founder of Groq are ex-Google engineers who worked and developed Googles TPUs?&lt;/p&gt; &lt;p&gt;Why doesn't Google sell its own TPUs? I think now is the right time to enter the HW market.&lt;/p&gt; &lt;p&gt;Can someone shed some light on this topic, please?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8i5s2/why_doesnt_groq_sell_its_lpus_by_extension_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8i5s2/why_doesnt_groq_sell_its_lpus_by_extension_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8i5s2/why_doesnt_groq_sell_its_lpus_by_extension_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T03:57:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8554a</id>
    <title>Qwen QwQ-32B joins DeepSeek R1 and Claude Sonnets at the top of the Creative Story-Writing Benchmark</title>
    <updated>2025-03-10T18:08:27+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8554a/qwen_qwq32b_joins_deepseek_r1_and_claude_sonnets/"&gt; &lt;img alt="Qwen QwQ-32B joins DeepSeek R1 and Claude Sonnets at the top of the Creative Story-Writing Benchmark" src="https://b.thumbs.redditmedia.com/Yaa0ATPdfMfRcdIPRl3zAR-18YhojdxTeqLYJXaDdUk.jpg" title="Qwen QwQ-32B joins DeepSeek R1 and Claude Sonnets at the top of the Creative Story-Writing Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j8554a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8554a/qwen_qwq32b_joins_deepseek_r1_and_claude_sonnets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8554a/qwen_qwq32b_joins_deepseek_r1_and_claude_sonnets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T18:08:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1j84c79</id>
    <title>Qwen QwQ-32B is the LLM most frequently voted out first by its peers in the Elimination Game Benchmark, resulting in poor overall performance</title>
    <updated>2025-03-10T17:35:19+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j84c79/qwen_qwq32b_is_the_llm_most_frequently_voted_out/"&gt; &lt;img alt="Qwen QwQ-32B is the LLM most frequently voted out first by its peers in the Elimination Game Benchmark, resulting in poor overall performance" src="https://b.thumbs.redditmedia.com/GITV-BcVRUV84azdQvU9AjF2LmByzEd0hc-J34tPTRc.jpg" title="Qwen QwQ-32B is the LLM most frequently voted out first by its peers in the Elimination Game Benchmark, resulting in poor overall performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j84c79"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j84c79/qwen_qwq32b_is_the_llm_most_frequently_voted_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j84c79/qwen_qwq32b_is_the_llm_most_frequently_voted_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T17:35:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8iqns</id>
    <title>Hello world :)</title>
    <updated>2025-03-11T04:30:38+00:00</updated>
    <author>
      <name>/u/No-Abalone1029</name>
      <uri>https://old.reddit.com/user/No-Abalone1029</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8iqns/hello_world/"&gt; &lt;img alt="Hello world :)" src="https://preview.redd.it/hy3131ghnzne1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57704da9936e283da78771a5685c4f870a144bfe" title="Hello world :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA rtx 3060 12gb vram, hyte revolt 3 asrock b760 w wifi intel i5 16gb t-force vulcan ram&lt;/p&gt; &lt;p&gt;$1k. what do we think, and what should I do for my first project?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Abalone1029"&gt; /u/No-Abalone1029 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hy3131ghnzne1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8iqns/hello_world/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8iqns/hello_world/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T04:30:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j87eum</id>
    <title>QwQ 32B can do it if you coach it 2 times</title>
    <updated>2025-03-10T19:41:49+00:00</updated>
    <author>
      <name>/u/DrVonSinistro</name>
      <uri>https://old.reddit.com/user/DrVonSinistro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j87eum/qwq_32b_can_do_it_if_you_coach_it_2_times/"&gt; &lt;img alt="QwQ 32B can do it if you coach it 2 times" src="https://external-preview.redd.it/bGFmOXk2NDIxeG5lMalrzKbbY1wxsyua5vTpp1g3RTatq_ecPpvEXRJ-_J8E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3bc2c544369e356552a9d78fa1f23bdc00fdf6c3" title="QwQ 32B can do it if you coach it 2 times" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrVonSinistro"&gt; /u/DrVonSinistro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6wn0l7421xne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j87eum/qwq_32b_can_do_it_if_you_coach_it_2_times/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j87eum/qwq_32b_can_do_it_if_you_coach_it_2_times/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T19:41:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8i9rc</id>
    <title>NVLINK improves dual RTX 3090 inference performance by nearly 50%</title>
    <updated>2025-03-11T04:02:55+00:00</updated>
    <author>
      <name>/u/hp1337</name>
      <uri>https://old.reddit.com/user/hp1337</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8i9rc/nvlink_improves_dual_rtx_3090_inference/"&gt; &lt;img alt="NVLINK improves dual RTX 3090 inference performance by nearly 50%" src="https://external-preview.redd.it/vlUgVTNeRZS9-bbTFaZ7ayYcVwvIPXEw74izW1rJLuI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ba6a9bb89057dbbff64dd02958275d4ac3df306" title="NVLINK improves dual RTX 3090 inference performance by nearly 50%" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hp1337"&gt; /u/hp1337 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://himeshp.blogspot.com/2025/03/vllm-performance-benchmarks-4x-rtx-3090.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8i9rc/nvlink_improves_dual_rtx_3090_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8i9rc/nvlink_improves_dual_rtx_3090_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T04:02:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8766b</id>
    <title>New rig who dis</title>
    <updated>2025-03-10T19:31:29+00:00</updated>
    <author>
      <name>/u/MotorcyclesAndBizniz</name>
      <uri>https://old.reddit.com/user/MotorcyclesAndBizniz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8766b/new_rig_who_dis/"&gt; &lt;img alt="New rig who dis" src="https://b.thumbs.redditmedia.com/0XSP2n-GAI5n3Op8qnPsulZZgY7u_Dk_E6IZd3L-Ixg.jpg" title="New rig who dis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPU: 6x 3090 FE via 6x PCIe 4.0 x4 Oculink&lt;br /&gt; CPU: AMD 7950x3D&lt;br /&gt; MoBo: B650M WiFi&lt;br /&gt; RAM: 192GB DDR5 @ 4800MHz&lt;br /&gt; NIC: 10Gbe&lt;br /&gt; NVMe: Samsung 980 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MotorcyclesAndBizniz"&gt; /u/MotorcyclesAndBizniz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j8766b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8766b/new_rig_who_dis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8766b/new_rig_who_dis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T19:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8ibs2</id>
    <title>Don't underestimate the power of local models executing recursive agent workflows. (mistral-small)</title>
    <updated>2025-03-11T04:06:03+00:00</updated>
    <author>
      <name>/u/LocoMod</name>
      <uri>https://old.reddit.com/user/LocoMod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8ibs2/dont_underestimate_the_power_of_local_models/"&gt; &lt;img alt="Don't underestimate the power of local models executing recursive agent workflows. (mistral-small)" src="https://external-preview.redd.it/aHB6YWN6MG1pem5lMRehscSTBN6MsWNS82nQXiny-IBLyecHf_sStrTrfL-k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe0eebd0390412c4dbf32e51fec56621c4f2ca18" title="Don't underestimate the power of local models executing recursive agent workflows. (mistral-small)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LocoMod"&gt; /u/LocoMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/51m4yx0mizne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8ibs2/dont_underestimate_the_power_of_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8ibs2/dont_underestimate_the_power_of_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T04:06:03+00:00</published>
  </entry>
</feed>
