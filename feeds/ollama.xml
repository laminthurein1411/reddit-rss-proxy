<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-02T19:34:16+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1if9743</id>
    <title>I have 20K word documents, I need to add the data from them into mongodb.</title>
    <updated>2025-02-01T15:52:10+00:00</updated>
    <author>
      <name>/u/shaxadhere</name>
      <uri>https://old.reddit.com/user/shaxadhere</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Issue is that there is data inside them that needs to go in specific fields. Like there is a table at the start of the document 3 rows 3 columns, each cell contains some data and then there is a large body of text beneath the table that goes in one cell.&lt;/p&gt; &lt;p&gt;I tried picking data with regex but the table data changes like their format changes. so regex is not an option here i guess. &lt;/p&gt; &lt;p&gt;I'm thinking of hosting deepseek with ollama locally and write a nodejs script to all the work form.&lt;/p&gt; &lt;p&gt;So far I've scrapped the document and got the text in a variable from outside.&lt;/p&gt; &lt;p&gt;but when i pass the data to the deepseek server, it gives me a streaming response which causes a lots of problems, is there any way to just get the final response and not streaming one.&lt;/p&gt; &lt;p&gt;Or if you can suggest a better solution to this.&lt;/p&gt; &lt;p&gt;My clients prod database got deleted and this is the only option available, they have the content of their entire app inside a word document.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shaxadhere"&gt; /u/shaxadhere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if9743/i_have_20k_word_documents_i_need_to_add_the_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if9743/i_have_20k_word_documents_i_need_to_add_the_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if9743/i_have_20k_word_documents_i_need_to_add_the_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T15:52:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifp725</id>
    <title>What is Ollama and how to use it: a quick guide [part 1]</title>
    <updated>2025-02-02T04:28:17+00:00</updated>
    <author>
      <name>/u/geshan</name>
      <uri>https://old.reddit.com/user/geshan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifp725/what_is_ollama_and_how_to_use_it_a_quick_guide/"&gt; &lt;img alt="What is Ollama and how to use it: a quick guide [part 1]" src="https://external-preview.redd.it/8MWdJZUaawUAdrOKRXgnCZEz9o2FNzmFaI7Hu5xxzTQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3415e2df2b2072119555b4f9e941f458832579c" title="What is Ollama and how to use it: a quick guide [part 1]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geshan"&gt; /u/geshan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://geshan.com.np/blog/2025/02/what-is-ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifp725/what_is_ollama_and_how_to_use_it_a_quick_guide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifp725/what_is_ollama_and_how_to_use_it_a_quick_guide/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T04:28:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifqlp4</id>
    <title>Test if a model is any good?</title>
    <updated>2025-02-02T05:53:26+00:00</updated>
    <author>
      <name>/u/Kitchen-Purpose-6596</name>
      <uri>https://old.reddit.com/user/Kitchen-Purpose-6596</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are so many models to choose from. How do you test if a model is any good?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kitchen-Purpose-6596"&gt; /u/Kitchen-Purpose-6596 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifqlp4/test_if_a_model_is_any_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifqlp4/test_if_a_model_is_any_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifqlp4/test_if_a_model_is_any_good/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T05:53:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifj5qj</id>
    <title>New Experimental Agent Layer &amp; Reasoning Layer added to Notate v1.1.0. Now you can with any model locally reason and enable web search utilizing the Agent layer. More tools coming soon!</title>
    <updated>2025-02-01T23:12:16+00:00</updated>
    <author>
      <name>/u/Hairetsu</name>
      <uri>https://old.reddit.com/user/Hairetsu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifj5qj/new_experimental_agent_layer_reasoning_layer/"&gt; &lt;img alt="New Experimental Agent Layer &amp;amp; Reasoning Layer added to Notate v1.1.0. Now you can with any model locally reason and enable web search utilizing the Agent layer. More tools coming soon!" src="https://external-preview.redd.it/mcnU-d_DoLHew8RaPQUtHLP6gPiy9L1fZ85lNZzDPN4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=195f63b8ab536d1b6a94e52ac5e539c8a57f3eef" title="New Experimental Agent Layer &amp;amp; Reasoning Layer added to Notate v1.1.0. Now you can with any model locally reason and enable web search utilizing the Agent layer. More tools coming soon!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairetsu"&gt; /u/Hairetsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/cntrlai/notate"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifj5qj/new_experimental_agent_layer_reasoning_layer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifj5qj/new_experimental_agent_layer_reasoning_layer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T23:12:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ift60f</id>
    <title>Ollama not loaded into GPU in the WSL? does anyone know why?</title>
    <updated>2025-02-02T08:51:34+00:00</updated>
    <author>
      <name>/u/GTHell</name>
      <uri>https://old.reddit.com/user/GTHell</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I setup a new system with my 3090 and installed Arch linux into the WSL2. Upon installed cuda-toolkit both on Windows and WSL I got the GPU working. The torch.cuda does show that the cuda device is available. The nvidia-smi in the WSL also showing the information of the GPU but when I do ollama run or ollama serve I got a message saying &amp;quot;no cuda runners detected unabble to run on cuda GPU&amp;quot;.&lt;/p&gt; &lt;p&gt;Does anyone know how to bypass this? My gaming laptop with 4070 uses WSL for development as well and when loading the model it loaded into VRAM instead of RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GTHell"&gt; /u/GTHell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ift60f/ollama_not_loaded_into_gpu_in_the_wsl_does_anyone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ift60f/ollama_not_loaded_into_gpu_in_the_wsl_does_anyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ift60f/ollama_not_loaded_into_gpu_in_the_wsl_does_anyone/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T08:51:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifhv4r</id>
    <title>Running DeepSeek on AWS</title>
    <updated>2025-02-01T22:12:33+00:00</updated>
    <author>
      <name>/u/immediate_a982</name>
      <uri>https://old.reddit.com/user/immediate_a982</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifhv4r/running_deepseek_on_aws/"&gt; &lt;img alt="Running DeepSeek on AWS" src="https://preview.redd.it/gzokp1bbqlge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a326aaf3ccff66f5454eaa09777e6616dfac0ef" title="Running DeepSeek on AWS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, anyone has tried to run DeepSeck on AWS or Azure? Any pointers you can share. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/immediate_a982"&gt; /u/immediate_a982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gzokp1bbqlge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifhv4r/running_deepseek_on_aws/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifhv4r/running_deepseek_on_aws/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T22:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifu0nn</id>
    <title>Is there a difference between ollama's model loader and langchain's model loader and ollama cli?</title>
    <updated>2025-02-02T09:55:09+00:00</updated>
    <author>
      <name>/u/No-Comfort3958</name>
      <uri>https://old.reddit.com/user/No-Comfort3958</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I generally use langchain' to load my ollama models, but I have been trying to work with llava, it seems dumb when I work with it in to describe an image in langchain. However when I tried same image with ollama cli, it responded far better. So does the model loader I am using actually affect the performance of the models? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Comfort3958"&gt; /u/No-Comfort3958 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifu0nn/is_there_a_difference_between_ollamas_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifu0nn/is_there_a_difference_between_ollamas_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifu0nn/is_there_a_difference_between_ollamas_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T09:55:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifub4k</id>
    <title>ollama install on macosx - formulae vs cask</title>
    <updated>2025-02-02T10:16:15+00:00</updated>
    <author>
      <name>/u/low_depo</name>
      <uri>https://old.reddit.com/user/low_depo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am using brew to install packages on macosx and there are two ways to install ollama:&lt;/p&gt; &lt;p&gt;- formulae - building&lt;/p&gt; &lt;p&gt;- cask - downloading app&lt;/p&gt; &lt;p&gt;I see that formulae is much more popular,&lt;/p&gt; &lt;p&gt;Are there any advantages to this solution being more popular?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/low_depo"&gt; /u/low_depo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifub4k/ollama_install_on_macosx_formulae_vs_cask/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifub4k/ollama_install_on_macosx_formulae_vs_cask/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifub4k/ollama_install_on_macosx_formulae_vs_cask/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T10:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifue87</id>
    <title>Why models don't use GPU?</title>
    <updated>2025-02-02T10:22:33+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried to see what models do to computer resources in Task Manager, and what I've noticed is that while models run, CPU and RAM usage is very high, but the GPU isn't affected at all. Why is that?&lt;br /&gt; And I have a dedicated GPU with 4GB Nvidia GeForce 1050 Ti.&lt;/p&gt; &lt;p&gt;It's not used at all by models. Is there a way to configure Ollama in such a way, that it tries to use GPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifue87/why_models_dont_use_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifue87/why_models_dont_use_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifue87/why_models_dont_use_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T10:22:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifuzv3</id>
    <title>Introducing AIBench: Your Ultimate iPhone Tool for Mobile AI Performance Testing!</title>
    <updated>2025-02-02T11:05:36+00:00</updated>
    <author>
      <name>/u/Snoo_24581</name>
      <uri>https://old.reddit.com/user/Snoo_24581</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever wondered how your iPhone handles cutting-edge AI models? Meet &lt;strong&gt;AIBench&lt;/strong&gt; â€“ a free, privacy-focused app that lets you test, analyze, and interact with popular large language models (LLMs) &lt;strong&gt;offline&lt;/strong&gt; on your device. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;ðŸŒŸ Key Features:&lt;/strong&gt;&lt;br /&gt; - &lt;strong&gt;Cutting-Edge Models&lt;/strong&gt;: Run DeepSeek-R1 (distilled), Llama, Qwen, and more directly on your iPhone.&lt;br /&gt; - &lt;strong&gt;Pro-Level Analysis&lt;/strong&gt;: Monitor real-time metrics like inference speed, GPU usage, and memory consumption.&lt;br /&gt; - &lt;strong&gt;Built-In Benchmarks&lt;/strong&gt;: Test models across scenarios like translation, summarization, and text generation.&lt;br /&gt; - &lt;strong&gt;Customizable Settings&lt;/strong&gt;: Adjust token counts and sampling temperature for tailored experiments.&lt;br /&gt; - &lt;strong&gt;Data Visualization&lt;/strong&gt;: Track performance trends and system resource usage with intuitive graphs. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;ðŸŽ¯ Perfect For:&lt;/strong&gt;&lt;br /&gt; - &lt;strong&gt;Developers&lt;/strong&gt; optimizing mobile AI performance.&lt;br /&gt; - &lt;strong&gt;Researchers&lt;/strong&gt; needing portable evaluation tools.&lt;br /&gt; - &lt;strong&gt;Tech Enthusiasts&lt;/strong&gt; curious about on-device AI capabilities. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;ðŸ”’ Privacy First:&lt;/strong&gt; All computations happen locally â€“ your data never leaves your device. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;ðŸ“± Compatibility:&lt;/strong&gt; Requires Metal support. Best experience on iPhone 15 Pro/newer models. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Download Now&lt;/strong&gt;: [AIBench on the App Store]() (link in comments) &lt;/p&gt; &lt;p&gt;Whether youâ€™re tweaking models, benchmarking, or just geeking out over mobile AI, AIBench makes it effortless. Letâ€™s push those iPhone GPUs to the limit! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snoo_24581"&gt; /u/Snoo_24581 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://apps.apple.com/cn/app/aibench-%E7%A7%BB%E5%8A%A8%E7%AB%AFai%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/id6741204584"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifuzv3/introducing_aibench_your_ultimate_iphone_tool_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifuzv3/introducing_aibench_your_ultimate_iphone_tool_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T11:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifqrug</id>
    <title>Question--CPU vs GPU utilization</title>
    <updated>2025-02-02T06:03:34+00:00</updated>
    <author>
      <name>/u/w38122077</name>
      <uri>https://old.reddit.com/user/w38122077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all. I'm new to Ollama and I have a question about how / where it decides to execute.&lt;/p&gt; &lt;p&gt;Ubuntu in Proxmox:&lt;/p&gt; &lt;p&gt;Linux: Linux version 6.8.0-52-generic (buildd@lcy02-amd64-046) Build: (x86_64-linux-gnu-gcc-13 (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0, GNU ld (GNU Binutils for Ubuntu) 2.42)&lt;br /&gt; Release : 6.8.0-52-generic &lt;br /&gt; Version : #53-Ubuntu SMP PREEMPT_DYNAMIC Sat Jan 11 00:06:25 UTC 2025&lt;br /&gt; # of CPUs: 60&lt;br /&gt; Machine : x86_64&lt;/p&gt; &lt;p&gt;There are (2) Intel(R) Xeon(R) Gold 6226R CPUs or which I have 60 vCPUs configured through in &amp;quot;host&amp;quot; mode.&lt;/p&gt; &lt;p&gt;I have 84GB of DDR4 2933MHz RAM allocated.&lt;/p&gt; &lt;p&gt;I have (2) NVIDIA RTX 4000 SFF Ada GPUs passed through to the VM.&lt;/p&gt; &lt;p&gt;I am executing &amp;quot;ollama run deepseek-r1:70b&amp;quot;&lt;/p&gt; &lt;p&gt;My question is: &lt;/p&gt; &lt;p&gt;Why do my GPUs stay relatively idle while the CPU is spiked near 100% when responding to prompts?&lt;/p&gt; &lt;p&gt;ollama ps&lt;br /&gt; NAME ID SIZE PROCESSOR UNTIL &lt;br /&gt; deepseek-r1:70b 0c1615a8ca32 47 GB 14%/86% CPU/GPU Forever &lt;/p&gt; &lt;p&gt;So I know it's mostly loaded on the GPUs and I do see spikes of utilization. Is this normal?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w38122077"&gt; /u/w38122077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifqrug/questioncpu_vs_gpu_utilization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifqrug/questioncpu_vs_gpu_utilization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifqrug/questioncpu_vs_gpu_utilization/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T06:03:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifa93h</id>
    <title>DeepSeek R1 Hardware Requirements Explained</title>
    <updated>2025-02-01T16:39:48+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifa93h/deepseek_r1_hardware_requirements_explained/"&gt; &lt;img alt="DeepSeek R1 Hardware Requirements Explained" src="https://external-preview.redd.it/bavhQxeXV5pgAp-fBIVoF8XffcIw7GN5u11i9CCbtIY.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2a472a729ce975767359b47dc4788711949124d4" title="DeepSeek R1 Hardware Requirements Explained" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/5RhPZgDoglE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifa93h/deepseek_r1_hardware_requirements_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifa93h/deepseek_r1_hardware_requirements_explained/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T16:39:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifz7f5</id>
    <title>Running Mistral Large v2 on Lambda Labs for my dev team, good idea?</title>
    <updated>2025-02-02T15:09:14+00:00</updated>
    <author>
      <name>/u/SelectSpread</name>
      <uri>https://old.reddit.com/user/SelectSpread</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;During the weekend I played around with a lambda labs h200 machine (96 GB VRAM, 432 GB RAM and 64 x64 cores) I was running mistral large v2 (123B) which occupied around 75 GB of VRAM. I also deployed open web-ui and used &lt;a href="http://continue.dev"&gt;continue.dev&lt;/a&gt; in intellij and vscode. That was not perfect but still quite cood, compared to a webchat based approach with copy paste.&lt;br /&gt; As we mainly do Java, and Mistral has a high score in Java, I thought it might be a data privacy friendly approach to host such a system during office ours for my team (around 10 developers; startup and shutdown of the instance via lambda lab's API). Hardware (and electricity) are too expensive to self host. What do you think about that approach? What would you differently? Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SelectSpread"&gt; /u/SelectSpread &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifz7f5/running_mistral_large_v2_on_lambda_labs_for_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifz7f5/running_mistral_large_v2_on_lambda_labs_for_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifz7f5/running_mistral_large_v2_on_lambda_labs_for_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T15:09:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifqglv</id>
    <title>Using Local LLMS to read text images, pdf, excel, programming scripts and also output modified text files and programming scripts</title>
    <updated>2025-02-02T05:44:33+00:00</updated>
    <author>
      <name>/u/sheik_ali</name>
      <uri>https://old.reddit.com/user/sheik_ali</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifqglv/using_local_llms_to_read_text_images_pdf_excel/"&gt; &lt;img alt="Using Local LLMS to read text images, pdf, excel, programming scripts and also output modified text files and programming scripts" src="https://a.thumbs.redditmedia.com/xHY4X2cd7XDsA6zaZTJJ-cfhkKfmWIaRxyRRqrfz894.jpg" title="Using Local LLMS to read text images, pdf, excel, programming scripts and also output modified text files and programming scripts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Guys,&lt;/p&gt; &lt;p&gt;I would like to ask, I am using Ollama to run local LLMs, for now as yall might know since Ollama is run on a command prompt, one can only send text to the LLM for it to read and interpret. I understand there is a way to convert the files to a text for the LLMs to read and interpret, but what if the files I want to modify with the LLM's help is too big to convert to text to send in the command prompt? Is there a similiar function/feature to attach files to send in a prompt to the local LLMs just like in chatgpt prompt UI as seen in the attached image/GIF?&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/5m1yvitwynge1.gif"&gt;https://i.redd.it/5m1yvitwynge1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xfxsiegxynge1.png?width=288&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cfc2fa0ae1763d86bbaec4803d5128f84a548058"&gt;https://preview.redd.it/xfxsiegxynge1.png?width=288&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cfc2fa0ae1763d86bbaec4803d5128f84a548058&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sheik_ali"&gt; /u/sheik_ali &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifqglv/using_local_llms_to_read_text_images_pdf_excel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifqglv/using_local_llms_to_read_text_images_pdf_excel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifqglv/using_local_llms_to_read_text_images_pdf_excel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T05:44:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig19mg</id>
    <title>Sql generation with llama</title>
    <updated>2025-02-02T16:40:16+00:00</updated>
    <author>
      <name>/u/rock_db_saanu</name>
      <uri>https://old.reddit.com/user/rock_db_saanu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using llama 3 as local llm to generate sql from text. Using RAG with good enough sql statements and ddl for all relevant tables and table joins Llama is just good enough but slow and also sometimes not accurate. Do you recommend any other local LLM which can be better than this I have GPU and 32 GB RAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rock_db_saanu"&gt; /u/rock_db_saanu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig19mg/sql_generation_with_llama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig19mg/sql_generation_with_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig19mg/sql_generation_with_llama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T16:40:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig2g1f</id>
    <title>Permissions for Ollama and Chrome extensions</title>
    <updated>2025-02-02T17:29:33+00:00</updated>
    <author>
      <name>/u/rajatrocks</name>
      <uri>https://old.reddit.com/user/rajatrocks</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I want my Chrome extension to be able to interact with Ollama, from the command line I need to run:&lt;/p&gt; &lt;p&gt;&lt;code&gt;launchctl setenv OLLAMA_ORIGINS &amp;quot;chrome-extension://gldebcpkoojijledacjeboaehblhfbjg&amp;quot;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;or start ollama with &lt;a href="https://github.com/ollama/ollama/issues/2308"&gt;a special command line&lt;/a&gt;. This makes using Ollama with my extension harder than any of the other local model services that I've used. Are there any alternatives? Any guidance would be appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rajatrocks"&gt; /u/rajatrocks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig2g1f/permissions_for_ollama_and_chrome_extensions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig2g1f/permissions_for_ollama_and_chrome_extensions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig2g1f/permissions_for_ollama_and_chrome_extensions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T17:29:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1iffzhn</id>
    <title>I created a web UI for Ollama that lets you talk to your models and manage them</title>
    <updated>2025-02-01T20:48:26+00:00</updated>
    <author>
      <name>/u/Ok_Promotion_9578</name>
      <uri>https://old.reddit.com/user/Ok_Promotion_9578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iffzhn/i_created_a_web_ui_for_ollama_that_lets_you_talk/"&gt; &lt;img alt="I created a web UI for Ollama that lets you talk to your models and manage them" src="https://external-preview.redd.it/MbCNCUfEvON9rMG6_Ug9gghhk5NKAhUN5fohRsQ5Kk0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bca5176e36c712849de99c8520c0e7c3eaa142c4" title="I created a web UI for Ollama that lets you talk to your models and manage them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I believe that are some excellent solutions for this problem, but I wanted to take a stab at making my own and focusing on making it super light weight, minimalistic, and clean.&lt;/p&gt; &lt;p&gt;It is still in the early stages, but I'd love some feedback on what I've built so far and to hear about what you'd like to see in a solution like this! &lt;/p&gt; &lt;p&gt;I'm thinking about really cool things in the roadmap down the line, but wanted to start simple and involve the community.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/majicmaj/aloha"&gt;https://github.com/majicmaj/aloha&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xu3brq22blge1.png?width=750&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78b7bbe65ab50348fe373ed1e35d6e80e51e5650"&gt;https://preview.redd.it/xu3brq22blge1.png?width=750&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78b7bbe65ab50348fe373ed1e35d6e80e51e5650&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Promotion_9578"&gt; /u/Ok_Promotion_9578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iffzhn/i_created_a_web_ui_for_ollama_that_lets_you_talk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iffzhn/i_created_a_web_ui_for_ollama_that_lets_you_talk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iffzhn/i_created_a_web_ui_for_ollama_that_lets_you_talk/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T20:48:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifkdid</id>
    <title>Run DeepSeek-R1 Locally with Ollama and Open-WebUI (Docker Compose)</title>
    <updated>2025-02-02T00:10:22+00:00</updated>
    <author>
      <name>/u/ntalekt</name>
      <uri>https://old.reddit.com/user/ntalekt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deploy DeepSeek-R1 on your local machine using Ollama and Open-WebUI with this Docker Compose setup. Perfect for those without GPU hardware who want to experiment with AI models.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clone the repo&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/ntalekt/deepseek-r1-docker-compose.git"&gt;&lt;code&gt;https://github.com/ntalekt/deepseek-r1-docker-compose.git&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Start services&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt;docker compose up -d&lt;/code&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Access Web UI: &lt;a href="http://localhost:3000/"&gt;http://localhost:3000&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;CPU-only setup (no GPU required)&lt;/li&gt; &lt;li&gt;Automatic download of deepseek-r1:8b model&lt;/li&gt; &lt;li&gt;Easy installation and management with Docker Compose&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Requirements:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Docker Engine v20.10.10+&lt;/li&gt; &lt;li&gt;Docker Compose v2.20.0+&lt;/li&gt; &lt;li&gt;8GB RAM (16GB recommended)&lt;/li&gt; &lt;li&gt;20GB+ free disk space&lt;/li&gt; &lt;li&gt;Linux/macOS/WSL2&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Note:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;CPU inference will be slower than GPU-accelerated setups. Consider GPU hardware for production use.&lt;/li&gt; &lt;li&gt;License: MIT&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full repository: &lt;a href="https://github.com/ntalekt/deepseek-r1-docker-compose"&gt;https://github.com/ntalekt/deepseek-r1-docker-compose&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ntalekt"&gt; /u/ntalekt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifkdid/run_deepseekr1_locally_with_ollama_and_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifkdid/run_deepseekr1_locally_with_ollama_and_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifkdid/run_deepseekr1_locally_with_ollama_and_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T00:10:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig17qx</id>
    <title>anybody tried rx 580 2048 tsb with 8gb of vram for deepseek r1 8b</title>
    <updated>2025-02-02T16:37:59+00:00</updated>
    <author>
      <name>/u/felix_ardyan</name>
      <uri>https://old.reddit.com/user/felix_ardyan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i just wonder the peformance of it with unnoficial amd for ollama anybody tried and if soo how good it is&lt;/p&gt; &lt;p&gt;note i know is very old performace but i want to buy it for my egpu for playing game just wondering if it can do funny ai&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/felix_ardyan"&gt; /u/felix_ardyan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig17qx/anybody_tried_rx_580_2048_tsb_with_8gb_of_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig17qx/anybody_tried_rx_580_2048_tsb_with_8gb_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig17qx/anybody_tried_rx_580_2048_tsb_with_8gb_of_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T16:37:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig3axm</id>
    <title>ðŸ”¥ Chipper RAG Toolbox 2.2 is Here! (Ollama API Reflection, DeepSeek, Haystack, Python)</title>
    <updated>2025-02-02T18:04:51+00:00</updated>
    <author>
      <name>/u/Alarming_Divide_1339</name>
      <uri>https://old.reddit.com/user/Alarming_Divide_1339</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ig3axm/chipper_rag_toolbox_22_is_here_ollama_api/"&gt; &lt;img alt="ðŸ”¥ Chipper RAG Toolbox 2.2 is Here! (Ollama API Reflection, DeepSeek, Haystack, Python)" src="https://external-preview.redd.it/EDUvohhVb5xr-KRAeuTg8gg3QUUyDGnrLD58QPihBNs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=811d33bdc4c31e21572a972d29ec5913e016dd45" title="ðŸ”¥ Chipper RAG Toolbox 2.2 is Here! (Ollama API Reflection, DeepSeek, Haystack, Python)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Big news for all &lt;strong&gt;Ollama&lt;/strong&gt; and &lt;strong&gt;RAG&lt;/strong&gt; enthusiasts â€“ &lt;strong&gt;Chipper 2.2&lt;/strong&gt; is out, and it's packing some serious upgrades!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Chipper Chains,&lt;/strong&gt; you can now link multiple Chipper instances together, distributing workloads across servers and pushing the ultimate context boundary. Just set your &lt;code&gt;OLLAMA_URL&lt;/code&gt; to another Chipper instance, and lets go. &lt;/p&gt; &lt;p&gt;ðŸ’¡ &lt;strong&gt;What's new?&lt;/strong&gt;&lt;br /&gt; - &lt;strong&gt;Full Ollama API Reflection&lt;/strong&gt; â€“ Chipper is now a seamless drop-in service that fully mirrors the &lt;strong&gt;Ollama Chat API&lt;/strong&gt;, integrating &lt;strong&gt;RAG capabilities&lt;/strong&gt; without breaking existing workflows.&lt;br /&gt; - &lt;strong&gt;API Proxy &amp;amp; Security&lt;/strong&gt; â€“ Reflects &amp;amp; proxies &lt;strong&gt;non-RAG pipeline calls&lt;/strong&gt;, with &lt;strong&gt;bearer token support&lt;/strong&gt; for a &lt;strong&gt;more secure&lt;/strong&gt; Ollama setup.&lt;br /&gt; - &lt;strong&gt;Daisy-Chaining&lt;/strong&gt; â€“ Connect multiple &lt;strong&gt;Chipper&lt;/strong&gt; instances to extend processing across multiple nodes.&lt;br /&gt; - &lt;strong&gt;Middleware&lt;/strong&gt; â€“ Chipper now acts as an &lt;strong&gt;Ollama middleware&lt;/strong&gt;, also enabling &lt;strong&gt;client-side query parameters&lt;/strong&gt; for fine-tuned responses or server side overrides.&lt;br /&gt; - &lt;strong&gt;DeepSeek R1 Support&lt;/strong&gt; - The Chipper web UI does now supports &amp;lt;think&amp;gt; tags.&lt;/p&gt; &lt;p&gt;âš¡ &lt;strong&gt;Why this matters?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Easily add &lt;strong&gt;shared RAG capabilities&lt;/strong&gt; to your favourite &lt;strong&gt;Ollama Client&lt;/strong&gt; with &lt;strong&gt;little extra complexity&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Securely expose your &lt;strong&gt;Ollama&lt;/strong&gt; server to desktop clients (like &lt;strong&gt;Enchanted&lt;/strong&gt;) with bearer token support.&lt;/li&gt; &lt;li&gt;Run multi-instance &lt;strong&gt;RAG pipelines&lt;/strong&gt; to augment requests with distributed knowledge bases or services.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you find Chipper useful or exciting, &lt;strong&gt;leaving a star would be lovely&lt;/strong&gt; and will help others discover Chipper too âœ¨. I am working on many more ideas and occasionally want to share my progress here with you.&lt;/p&gt; &lt;p&gt;For everyone upgrading to version 2.2, please regenerate your &lt;code&gt;.env&lt;/code&gt; files using the &lt;code&gt;run&lt;/code&gt; tool, and don't forget to regenerate your images.&lt;/p&gt; &lt;p&gt;ðŸ”— &lt;strong&gt;Check it out &amp;amp; demo it yourself:&lt;/strong&gt;&lt;br /&gt; ðŸ‘‰ &lt;a href="https://github.com/TilmanGriesel/chipper"&gt;https://github.com/TilmanGriesel/chipper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ðŸ‘‰ &lt;a href="https://chipper.tilmangriesel.com/"&gt;https://chipper.tilmangriesel.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt; &lt;a href="https://chipper.tilmangriesel.com/get-started.html"&gt;https://chipper.tilmangriesel.com/get-started.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/y8kq2y36lrge1.gif"&gt;https://i.redd.it/y8kq2y36lrge1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/6j46hz77lrge1.gif"&gt;https://i.redd.it/6j46hz77lrge1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/o9cfokr7lrge1.gif"&gt;https://i.redd.it/o9cfokr7lrge1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarming_Divide_1339"&gt; /u/Alarming_Divide_1339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig3axm/chipper_rag_toolbox_22_is_here_ollama_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig3axm/chipper_rag_toolbox_22_is_here_ollama_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig3axm/chipper_rag_toolbox_22_is_here_ollama_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T18:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1if4p38</id>
    <title>Been messing around with DeepSeek R1 + Ollama, and honestly, it's kinda wild how much you can do locally with free open-source tools. No cloud, no API keys, just your machine and some cool AI magic.</title>
    <updated>2025-02-01T11:50:04+00:00</updated>
    <author>
      <name>/u/hasan_py</name>
      <uri>https://old.reddit.com/user/hasan_py</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;Page-Assist Chrome Extension - &lt;a href="https://github.com/n4ze3m/page-assist"&gt;https://github.com/n4ze3m/page-assist&lt;/a&gt; (any model with any params)&lt;/li&gt; &lt;li&gt;Open Web-UI LLM Wrapper - &lt;a href="https://github.com/open-webui/open-webui"&gt;https://github.com/open-webui/open-webui&lt;/a&gt; (any model with any params)&lt;/li&gt; &lt;li&gt;Browser use â€“ &lt;a href="https://github.com/browser-use/browser-use"&gt;https://github.com/browser-use/browser-use&lt;/a&gt; (deepseek r1:14b or more params) &lt;/li&gt; &lt;li&gt;Roo-Code (VS Code Extension) â€“ &lt;a href="https://github.com/RooVetGit/Roo-Code"&gt;https://github.com/RooVetGit/Roo-Code&lt;/a&gt; (deepseek coder)&lt;/li&gt; &lt;li&gt;n8n â€“ &lt;a href="https://github.com/n8n-io/n8n"&gt;https://github.com/n8n-io/n8n&lt;/a&gt; (any model with any params)&lt;/li&gt; &lt;li&gt;A simple RAG app: &lt;a href="https://github.com/hasan-py/chat-with-pdf-RAG"&gt;https://github.com/hasan-py/chat-with-pdf-RAG&lt;/a&gt; (deepseek r1:8b)&lt;/li&gt; &lt;li&gt;Ai assistant Chrome extension: &lt;a href="https://github.com/hasan-py/Ai-Assistant-Chrome-Extension"&gt;https://github.com/hasan-py/Ai-Assistant-Chrome-Extension&lt;/a&gt; (GPT, Gemini, Grok Api, Ollama added recently)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Full installation video: &lt;a href="https://youtu.be/hjg9kJs8al8?si=rillpsKpjONYMDYW"&gt;https://youtu.be/hjg9kJs8al8?si=rillpsKpjONYMDYW&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone exploring something else? Please share- it would be highly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hasan_py"&gt; /u/hasan_py &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if4p38/been_messing_around_with_deepseek_r1_ollama_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if4p38/been_messing_around_with_deepseek_r1_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if4p38/been_messing_around_with_deepseek_r1_ollama_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T11:50:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig4qrp</id>
    <title>Local Quantization Workflows: what tools do you use?</title>
    <updated>2025-02-02T19:03:59+00:00</updated>
    <author>
      <name>/u/SilentChip5913</name>
      <uri>https://old.reddit.com/user/SilentChip5913</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey everyone, quick question for those who work on quantizing models locally:&lt;/p&gt; &lt;p&gt;what tool or workflow do you currently use to keep track of different quantization iterations (e.g., tracking versions, comparing results, reverting changes)?&lt;/p&gt; &lt;p&gt;curious to hear whatâ€™s working (or not working) for you :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SilentChip5913"&gt; /u/SilentChip5913 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig4qrp/local_quantization_workflows_what_tools_do_you_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig4qrp/local_quantization_workflows_what_tools_do_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig4qrp/local_quantization_workflows_what_tools_do_you_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T19:03:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifv422</id>
    <title>Can someone clarify the subtypes of models (quantization, text vs instruct, etc.)?</title>
    <updated>2025-02-02T11:13:29+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed that models come in many versions, but I'm a little confused about it.&lt;/p&gt; &lt;p&gt;First there are &amp;quot;instruct&amp;quot; models and &amp;quot;text&amp;quot; models? What's the difference?&lt;/p&gt; &lt;p&gt;Second, I know that quantization is a type of compression, and the bigger the model in gigabytes, the less compression, and therefore higher quality, but at cost of hardware demands and speed. I know this general principle. But I don't know what exactly these quantization types mean. For example, I've seen all these types of quantization:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;fp16&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q2_K&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q3_K_L&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q3_K_M&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q3_K_S&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q4_0&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q4_1&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q4_K_M&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q4_K_S&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q5_0&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q5_1&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q5_K_M&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q5_K_S&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q6_K&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q8_0&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;And they all come for TEXT models and INSTRUCT models?&lt;/p&gt; &lt;p&gt;How to make sense of all that mess?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifv422/can_someone_clarify_the_subtypes_of_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifv422/can_someone_clarify_the_subtypes_of_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifv422/can_someone_clarify_the_subtypes_of_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T11:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig2lzo</id>
    <title>How would Macbook Pro M3 16gb perform?</title>
    <updated>2025-02-02T17:36:19+00:00</updated>
    <author>
      <name>/u/Adventurous-Hunter98</name>
      <uri>https://old.reddit.com/user/Adventurous-Hunter98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to try ollama on my macbook pro m3 16 gb, but Im curious about the performance. For coding and studying, will it perform same as chatgpt or worse because of ram? Anyone else tried it with the same hardware? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Hunter98"&gt; /u/Adventurous-Hunter98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig2lzo/how_would_macbook_pro_m3_16gb_perform/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig2lzo/how_would_macbook_pro_m3_16gb_perform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig2lzo/how_would_macbook_pro_m3_16gb_perform/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T17:36:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifvbgp</id>
    <title>Can we really do something with deepseek-r1:1.5b?</title>
    <updated>2025-02-02T11:28:07+00:00</updated>
    <author>
      <name>/u/Inevitable-Judge2642</name>
      <uri>https://old.reddit.com/user/Inevitable-Judge2642</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifvbgp/can_we_really_do_something_with_deepseekr115b/"&gt; &lt;img alt="Can we really do something with deepseek-r1:1.5b?" src="https://external-preview.redd.it/eyfcbevjKo97vLjBfB0Fmj0NwFo3-R0O6txnn7zhQLY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb92958779590d19aea5be1f847281f6de982d04" title="Can we really do something with deepseek-r1:1.5b?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Judge2642"&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://k33g.hashnode.dev/can-we-really-do-something-with-deepseek-r115b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifvbgp/can_we_really_do_something_with_deepseekr115b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifvbgp/can_we_really_do_something_with_deepseekr115b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T11:28:07+00:00</published>
  </entry>
</feed>
