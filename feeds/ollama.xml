<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-11T02:39:02+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ilg690</id>
    <title>Start chat with message from model.</title>
    <updated>2025-02-09T14:43:23+00:00</updated>
    <author>
      <name>/u/Velskadi</name>
      <uri>https://old.reddit.com/user/Velskadi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm having a hard time finding any info on this, so I am hoping someone here might have some guidance. I would like to start a chat with a model using ollama start &amp;lt;MODEL NAME&amp;gt;, and have the model start the conversation with a response before I give it a prompt.&lt;/p&gt; &lt;p&gt;Preferably I'd like this message to be static, something like &amp;quot;I am your workshop assistant. Please give me these pieces of information so I can assist. etc. etc&amp;quot;&lt;/p&gt; &lt;p&gt;Is this possible using Ollama? If so, would it be possible to do this in Openwebui as well? Any advice would be appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Velskadi"&gt; /u/Velskadi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilg690/start_chat_with_message_from_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilg690/start_chat_with_message_from_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilg690/start_chat_with_message_from_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T14:43:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilfj2d</id>
    <title>hardware question</title>
    <updated>2025-02-09T14:11:48+00:00</updated>
    <author>
      <name>/u/quantrpeter</name>
      <uri>https://old.reddit.com/user/quantrpeter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Jetson Orin Nano Super = 1024 CUDA&lt;br /&gt;&lt;/li&gt; &lt;li&gt;2070 = 2560 CUDA&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Telsa K80 24GB = 4992 CUDA&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For second hand price, K80 &amp;lt; 2070 &amp;lt; Jetson. For real ollama performance, isn't it more cuda core must win? If so, Jetson is not valuable.&lt;/p&gt; &lt;p&gt;thanks&lt;br /&gt; Peter&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantrpeter"&gt; /u/quantrpeter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilfj2d/hardware_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilfj2d/hardware_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilfj2d/hardware_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T14:11:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1iljej9</id>
    <title>Model occasionally continues to use CPU despite having finished responding.</title>
    <updated>2025-02-09T17:04:21+00:00</updated>
    <author>
      <name>/u/Velskadi</name>
      <uri>https://old.reddit.com/user/Velskadi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty much the title. I am running the magnum-v4-9b model through Open-webui, using my CPU (Ryzen 9 5900X). The model runs well, but brings my CPU usage to about 80-90% while it is generating a response. After it finishes it will sometimes keep my CPU usage pegged at these levels.&lt;/p&gt; &lt;p&gt;The last time this happened I tried stopping it with &lt;code&gt;ollama stop &amp;lt;model name&amp;gt;&lt;/code&gt; but it was then stuck in the &amp;quot;Stopping&amp;quot; state, and my CPU useage stayed high. I had to restart the Ollama service to fix this issue.&lt;/p&gt; &lt;p&gt;I may have seen this issue with other models as well but not realized it, as it was only today that I started monitoring the CPU usage. Any advice is appreciated!&lt;/p&gt; &lt;p&gt;-SPECS-&lt;br /&gt; CPU: Ryzen 9 5900X&lt;br /&gt; GPU (Unused): AMD Radeon 6700 XT&lt;br /&gt; RAM: 33GB DDR4&lt;br /&gt; OS: Arch Linux&lt;/p&gt; &lt;p&gt;EDIT: I'd like to note that all I had prompted when this happened was &amp;quot;This is a test. Please respond with Hello&amp;quot;, which it did.&lt;/p&gt; &lt;p&gt;While it is stuck like this the model takes a long time to start responding to any new prompts, and it generates it much slower. The CPU stays almost maxed out even after these subsequent prompts as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Velskadi"&gt; /u/Velskadi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iljej9/model_occasionally_continues_to_use_cpu_despite/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iljej9/model_occasionally_continues_to_use_cpu_despite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iljej9/model_occasionally_continues_to_use_cpu_despite/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T17:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1il0zea</id>
    <title>Just released an open-source Mac client for Ollama built with Swift/SwiftUI</title>
    <updated>2025-02-08T23:34:21+00:00</updated>
    <author>
      <name>/u/billythepark</name>
      <uri>https://old.reddit.com/user/billythepark</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently created a new Mac app using Swift. Last year, I released an open-source iPhone client for Ollama (a program for running LLMs locally) called MyOllama using Flutter. I planned to make a Mac version too, but when I tried with Flutter, the design didn't feel very Mac-native, so I put it aside.&lt;/p&gt; &lt;p&gt;Early this year, I decided to rebuild it from scratch using Swift/SwiftUI. This app lets you install and chat with LLMs like Deepseek on your Mac using Ollama. Features include:&lt;/p&gt; &lt;p&gt;- Contextual conversations&lt;/p&gt; &lt;p&gt;- Save and search chat history&lt;/p&gt; &lt;p&gt;- Customize system prompts&lt;/p&gt; &lt;p&gt;- And more...&lt;/p&gt; &lt;p&gt;It's completely open-source! Check out the code here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bipark/mac_ollama_client"&gt;https://github.com/bipark/mac_ollama_client&lt;/a&gt;&lt;/p&gt; &lt;p&gt;#Ollama #LLMHippo&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/billythepark"&gt; /u/billythepark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il0zea/just_released_an_opensource_mac_client_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il0zea/just_released_an_opensource_mac_client_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1il0zea/just_released_an_opensource_mac_client_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T23:34:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilnx95</id>
    <title>Training a local model w/ Confluence?</title>
    <updated>2025-02-09T20:11:50+00:00</updated>
    <author>
      <name>/u/thenyx</name>
      <uri>https://old.reddit.com/user/thenyx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to train llama3.2:8b with content from Confluence - what would be the best way to go about this?&lt;/p&gt; &lt;p&gt;I've seen mention of RAG, but how would this apply? Fairly new to this part of LLMs. Running MacOS if this matters.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thenyx"&gt; /u/thenyx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilnx95/training_a_local_model_w_confluence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilnx95/training_a_local_model_w_confluence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilnx95/training_a_local_model_w_confluence/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T20:11:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1im52z2</id>
    <title>Has anyone had ollama download deepseek-r1:70b by itsself and then all other models get deleted.</title>
    <updated>2025-02-10T12:33:57+00:00</updated>
    <author>
      <name>/u/StressOwn</name>
      <uri>https://old.reddit.com/user/StressOwn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1im52z2/has_anyone_had_ollama_download_deepseekr170b_by/"&gt; &lt;img alt="Has anyone had ollama download deepseek-r1:70b by itsself and then all other models get deleted." src="https://b.thumbs.redditmedia.com/r85fwXLhtXLKmsG8O-TW-TJityUs7VIQRVRocubhyPw.jpg" title="Has anyone had ollama download deepseek-r1:70b by itsself and then all other models get deleted." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/b3ypue203bie1.png?width=699&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4eedc738adedd0148e2040f7247fa2f6ac196345"&gt;https://preview.redd.it/b3ypue203bie1.png?width=699&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4eedc738adedd0148e2040f7247fa2f6ac196345&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ollama somehow downloaded this by itself, then deleted all other models, llmam3.2 deepseekr1-14b, bllava,qwencoder etc.was running openui but not open to outside traffic, but was using port forwarding to expose the api&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StressOwn"&gt; /u/StressOwn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1im52z2/has_anyone_had_ollama_download_deepseekr170b_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1im52z2/has_anyone_had_ollama_download_deepseekr170b_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1im52z2/has_anyone_had_ollama_download_deepseekr170b_by/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T12:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ildvgc</id>
    <title>Local TTS (text-to-speech) AI model with a human voice and file output?</title>
    <updated>2025-02-09T12:41:08+00:00</updated>
    <author>
      <name>/u/simo41993</name>
      <uri>https://old.reddit.com/user/simo41993</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't know if this is the right place to ask, but... i was looking for a text to speech alternative to the quite expensive online ones i was looking for recently.&lt;/p&gt; &lt;p&gt;I'm partially blind and it would be of great help to have a recorded and narrated version of some technical e-books i own.&lt;/p&gt; &lt;p&gt;As i was saying, models like Elevenlabs and similar are really quite good but absolutely too expensive in terms of ‚Ç¨/time for what i need to do (and the books are quite long too).&lt;/p&gt; &lt;p&gt;I was wondering, because of that, &lt;strong&gt;if there was a good (the normal TTS is quite abismal and distracting) alternative to run locally that can transpose the book in audio&lt;/strong&gt; and let me save a mp3 or similar file for later use.&lt;/p&gt; &lt;p&gt;I have to say, also, that i'm not a programmer whatsoever, so i should be able to follow simple instructions but, sadly, nothing more. so... a ready to use solution would be quite nice (or a detailed, like i'm a 3yo, set of instructions).&lt;/p&gt; &lt;p&gt;i'm using ollama + docker and free open web-ui for playing (literally) with some offline models and also thinking about using something compatible with this already running system... hopefully, possibly?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Another complication it's that i'm italian, so... the probably unexisting model should be capable to use italian language too...&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The following are my PC specs, if needed:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Processor: intel i7 13700k&lt;/li&gt; &lt;li&gt;MB: Asus ROG Z790-H&lt;/li&gt; &lt;li&gt;Ram: 64gb Corsair 5600 MT/S&lt;/li&gt; &lt;li&gt;Gpu: RTX 4070TI 12gb - MSI Ventus 3X&lt;/li&gt; &lt;li&gt;Storage: Samsung 970EVO NVME SSD + others&lt;/li&gt; &lt;li&gt;Windows 11 PRO 64bit&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sorry for the long post and thank you for any help :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simo41993"&gt; /u/simo41993 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ildvgc/local_tts_texttospeech_ai_model_with_a_human/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ildvgc/local_tts_texttospeech_ai_model_with_a_human/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ildvgc/local_tts_texttospeech_ai_model_with_a_human/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T12:41:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilp9bc</id>
    <title>script to import / export models between devices locally</title>
    <updated>2025-02-09T21:08:35+00:00</updated>
    <author>
      <name>/u/nahushrk</name>
      <uri>https://old.reddit.com/user/nahushrk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;wanted to share this simple scrip that lets you export the models downloaded to a machine to another machine without re-downloading it again&lt;/p&gt; &lt;p&gt;particularly useful when models are large and/or you want to share the models locally, saves time and bandwidth&lt;/p&gt; &lt;p&gt;just make sure the ollama version is same on both machines in case the storage mechanism changes&lt;/p&gt; &lt;p&gt;&lt;a href="https://gist.github.com/nahushrk/5d980e676c4f2762ca385bd6fb9498a9"&gt;https://gist.github.com/nahushrk/5d980e676c4f2762ca385bd6fb9498a9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the way this works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;export a model by name and size&lt;/li&gt; &lt;li&gt;a .tar file is created in dir where you ran this script&lt;/li&gt; &lt;li&gt;copy .tar file and this script to another machine&lt;/li&gt; &lt;li&gt;run import subcommand pointing to .tar file&lt;/li&gt; &lt;li&gt;run ollama list to see new model being added&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nahushrk"&gt; /u/nahushrk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilp9bc/script_to_import_export_models_between_devices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilp9bc/script_to_import_export_models_between_devices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilp9bc/script_to_import_export_models_between_devices/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T21:08:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilzgwn</id>
    <title>Controlling model swapping</title>
    <updated>2025-02-10T06:05:16+00:00</updated>
    <author>
      <name>/u/Bukt</name>
      <uri>https://old.reddit.com/user/Bukt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the best way to control model swapping? For example, if a user sends a request with the context size, I don't want the model to unload and reload. Can Ollama ignore certain parameters to prevent this? Or would I need to have a sanatizing proxy?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bukt"&gt; /u/Bukt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilzgwn/controlling_model_swapping/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilzgwn/controlling_model_swapping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilzgwn/controlling_model_swapping/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T06:05:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilwrme</id>
    <title>Ollama Research Agent | Agentic AI | DeepSeek and Llama 3.2 based Local ...</title>
    <updated>2025-02-10T03:27:43+00:00</updated>
    <author>
      <name>/u/Sangwan70</name>
      <uri>https://old.reddit.com/user/Sangwan70</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ilwrme/ollama_research_agent_agentic_ai_deepseek_and/"&gt; &lt;img alt="Ollama Research Agent | Agentic AI | DeepSeek and Llama 3.2 based Local ..." src="https://external-preview.redd.it/oodCpe-QxVzXvuRh_YIKkrAcNgKdJZhqS2aP14QHVOE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1bd07b4f716cef864694e37af43772f2ac4bdc74" title="Ollama Research Agent | Agentic AI | DeepSeek and Llama 3.2 based Local ..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sangwan70"&gt; /u/Sangwan70 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=lXjSEi6cmDE&amp;amp;si=gwQsckmdXyXL4qTO"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilwrme/ollama_research_agent_agentic_ai_deepseek_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilwrme/ollama_research_agent_agentic_ai_deepseek_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T03:27:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1im1njg</id>
    <title>second hand 2080 vs brand new jetson</title>
    <updated>2025-02-10T08:42:24+00:00</updated>
    <author>
      <name>/u/quantrpeter</name>
      <uri>https://old.reddit.com/user/quantrpeter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi&lt;/p&gt; &lt;p&gt;second hand 2080 vs brand new jetson. which on can run ollama faster?&lt;/p&gt; &lt;p&gt;thanks&lt;br /&gt; Peter&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantrpeter"&gt; /u/quantrpeter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1im1njg/second_hand_2080_vs_brand_new_jetson/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1im1njg/second_hand_2080_vs_brand_new_jetson/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1im1njg/second_hand_2080_vs_brand_new_jetson/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T08:42:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilo9cz</id>
    <title>I built an agentic Spotify app with 50 lines of YAML and ollama-supported LLMs</title>
    <updated>2025-02-09T20:26:11+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ilo9cz/i_built_an_agentic_spotify_app_with_50_lines_of/"&gt; &lt;img alt="I built an agentic Spotify app with 50 lines of YAML and ollama-supported LLMs" src="https://external-preview.redd.it/NHM0bW05dGphNmllMceFbxA9VETScbYkQkd_Y6Vr5Y0XlSdRTwjLPPGK61FC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e806715355fc40a96e65c9307c08a71adecc04be" title="I built an agentic Spotify app with 50 lines of YAML and ollama-supported LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a Spotify agent with 50 lines of YAML and an open source model.&lt;/p&gt; &lt;p&gt;The second most requested feature for Arch Gateway was bearer authorization for function calling scenarios to secure business APIs.&lt;/p&gt; &lt;p&gt;So when we added support for bearer authorization it opened up new possibilities- including connecting to third-party APIs so that user queries can be fulfilled via existing SaaS tools. Or consumer apps like Spotify. &lt;/p&gt; &lt;p&gt;For those not familiar with the project - Arch is an intelligent (edge and LLM) proxy designed for agentic apps and prompts - it handles the pesky stuff in handling, processing and routing prompts so that you can focus on the core business objectives is your AI app. You can read more here: &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;here is the 20+ lines of yaml that can help you achieve the above experience. Of course, you need the Gradio app too.&lt;/p&gt; &lt;p&gt;prompt_targets: - name: get_new_releases description: Get a list of new album releases featured in Spotify (shown, for example, on a Spotify player‚Äôs ‚ÄúBrowse‚Äù tab). parameters: - name: country description: the country where the album is released required: true type: str in_path: true - name: limit type: integer description: The maximum number of results to return default: &amp;quot;5&amp;quot; endpoint: name: spotify path: /v1/browse/new-releases http_headers: Authorization: &amp;quot;Bearer $SPOTIFY_CLIENT_KEY&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rqoxdb4ka6ie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilo9cz/i_built_an_agentic_spotify_app_with_50_lines_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilo9cz/i_built_an_agentic_spotify_app_with_50_lines_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T20:26:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilq0gd</id>
    <title>Goose + Ollama best model for agent coding</title>
    <updated>2025-02-09T21:40:17+00:00</updated>
    <author>
      <name>/u/einthecorgi2</name>
      <uri>https://old.reddit.com/user/einthecorgi2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just starting to mess around with goose, would love to start using it more. Current daily driver is cursor. Just wondering if anyone has any feedback on which model would work the best for code generation. I have been experimenting with a couple but I do not have a machine setup to run anything larger yet. So far my experience has been (all these are through Groq)&lt;br /&gt; - llama 3: would not maintain the main purpose of the app as the prompting lengthened and eventually just do whatever to make the code run.&lt;br /&gt; - Deepseek R1: would not actually edit or change any code (i think there is a specific &amp;quot;action&amp;quot; version of the model that is needed). But would run CLI commands, and if I kept asking would eventually put some code in a file. &lt;/p&gt; &lt;p&gt;Will update my progress as Goose gets better and I test more models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/einthecorgi2"&gt; /u/einthecorgi2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilq0gd/goose_ollama_best_model_for_agent_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilq0gd/goose_ollama_best_model_for_agent_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilq0gd/goose_ollama_best_model_for_agent_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T21:40:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilzwvr</id>
    <title>3B model with a N100 and 32GB DDR4 RAM</title>
    <updated>2025-02-10T06:34:05+00:00</updated>
    <author>
      <name>/u/Tuxedotux83</name>
      <uri>https://old.reddit.com/user/Tuxedotux83</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone here tried a 3B model (e.g. as Q8) with Intel N100, 32GB of DDR4 RAM and NVMe storage? CPU inference. What kind of t/s were you able to get?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tuxedotux83"&gt; /u/Tuxedotux83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilzwvr/3b_model_with_a_n100_and_32gb_ddr4_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilzwvr/3b_model_with_a_n100_and_32gb_ddr4_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilzwvr/3b_model_with_a_n100_and_32gb_ddr4_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T06:34:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1im3m1p</id>
    <title>Ollama windows run uses gpu however through running on api does not</title>
    <updated>2025-02-10T11:03:41+00:00</updated>
    <author>
      <name>/u/i-have-the-stash</name>
      <uri>https://old.reddit.com/user/i-have-the-stash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am using ollama mistral model as a classifier on my project which is around 4.1 gb. I have around 16gb vram&lt;/p&gt; &lt;p&gt;When i use it with ollama run and inference via cli, it is properly set up and utilizes gpu on inference run time but keeps it on system memory when not used.&lt;/p&gt; &lt;p&gt;When i api call its just uses it on memory, does not utilize gpu at all.&lt;/p&gt; &lt;p&gt;I would like to keep the model always on vram, and never utilize system memory if possible. Are there settings for that ?&lt;/p&gt; &lt;p&gt;Edit: Is there also an option to make it greedy ? I would like to fetch into vram when its inference time and dump it the moment it completely executes. I dont want gradual decrease at all since i will always call it via api and it stacks up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i-have-the-stash"&gt; /u/i-have-the-stash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1im3m1p/ollama_windows_run_uses_gpu_however_through/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1im3m1p/ollama_windows_run_uses_gpu_however_through/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1im3m1p/ollama_windows_run_uses_gpu_however_through/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T11:03:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1im77ts</id>
    <title>understanding the ollama chat template for Granite Guardian</title>
    <updated>2025-02-10T14:22:44+00:00</updated>
    <author>
      <name>/u/burrzinga</name>
      <uri>https://old.reddit.com/user/burrzinga</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Normally my interactions with Ollama-based models is relatively simple, system prompt + user prompt or Open Web UI. There a model called Granite Guardian &lt;a href="https://ollama.com/library/granite3-guardian:8b"&gt;https://ollama.com/library/granite3-guardian:8b&lt;/a&gt; that I would like to use. By default, it seems you just give it a user prompt and it responds with &amp;quot;Yes&amp;quot; or &amp;quot;No. I would like to try to get more out of the model, ideally with a Pydantic-based structured output. Where I can get the risky flag (yes or no as a bool) but also the probability and ideally which &amp;quot;rule&amp;quot; the user prompt failed (e.g. jailbreak). I am simply trying to use the OpenAI pip and either client.chat.completions.create or client.beta.chat.completions.parse for interactions. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/burrzinga"&gt; /u/burrzinga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1im77ts/understanding_the_ollama_chat_template_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1im77ts/understanding_the_ollama_chat_template_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1im77ts/understanding_the_ollama_chat_template_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T14:22:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1imd5up</id>
    <title>Any hope for QWEN-2.5-VL on Ollama?</title>
    <updated>2025-02-10T18:29:04+00:00</updated>
    <author>
      <name>/u/Background-Clerk-357</name>
      <uri>https://old.reddit.com/user/Background-Clerk-357</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried to convert this model myself by downloading the git repo containing the qwen-2.5-vl-7b-instruct safetensors data, creating a &lt;em&gt;Modelfile&lt;/em&gt; with &lt;code&gt;FROM .&lt;/code&gt;, and then executing &lt;code&gt;ollama create qwen-2.5-vl-7b-instruct&lt;/code&gt;&lt;/p&gt; &lt;p&gt;However it gave an error:&lt;br /&gt; &lt;code&gt;Error: path or modelfile are required&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I'm not sure what is wrong because I am following the recipe in the docs for converting a Safetensors model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Background-Clerk-357"&gt; /u/Background-Clerk-357 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imd5up/any_hope_for_qwen25vl_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imd5up/any_hope_for_qwen25vl_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1imd5up/any_hope_for_qwen25vl_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T18:29:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1img1gn</id>
    <title>Forgetting /bye and Deepseek reasoning that I might be upset.</title>
    <updated>2025-02-10T20:23:10+00:00</updated>
    <author>
      <name>/u/hawkedmd</name>
      <uri>https://old.reddit.com/user/hawkedmd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; .exit&lt;/p&gt; &lt;p&gt;&amp;lt;think&amp;gt;&lt;/p&gt; &lt;p&gt;Alright, the user typed &amp;quot;exit&amp;quot;. They might be trying to leave the conversation. Maybe they're upset or just finished what they needed help with.&lt;/p&gt; &lt;p&gt;I should acknowledge their choice and let them know I'm here if they change their mind. It's important not to push too hard but keep the door open for further assistance.&lt;/p&gt; &lt;p&gt;Expressing understanding and offering continued support in a friendly manner is probably the best approach.&lt;/p&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;Understood! If you need anything else or just want to chat, feel free to come back anytime‚ÄîI'm here for you! üòä&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/p&gt; &lt;p&gt;Use Ctrl + d or /bye to exit.&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hawkedmd"&gt; /u/hawkedmd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1img1gn/forgetting_bye_and_deepseek_reasoning_that_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1img1gn/forgetting_bye_and_deepseek_reasoning_that_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1img1gn/forgetting_bye_and_deepseek_reasoning_that_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T20:23:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1imh953</id>
    <title>CPU only with unraid in docker or vm?</title>
    <updated>2025-02-10T21:12:49+00:00</updated>
    <author>
      <name>/u/nicesliceoice</name>
      <uri>https://old.reddit.com/user/nicesliceoice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an unraid server with an i5 and igpu, I know it wont be the fastest or best but wanted to spin up some local llms to play around. (Thinking 1.5b deepseek r1 to see what the fuss is about)&lt;/p&gt; &lt;p&gt;Trying to install the official ollama docker through CA and it keeps giving me an error cause there is no gpu. Is it possible to install through CA or do I need to use a docker compose file? Or alternatively, is it better to spin up a vm and run ollama and open webui through that? Any advice, and models to try would be great. &lt;/p&gt; &lt;p&gt;I would love to use for my paperless-ngx instance as well if I could. But any sorting for images, pdfs, or simply summarising and organising my text inputs (formatting emails from dot points etc.) Would be my dream uses.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nicesliceoice"&gt; /u/nicesliceoice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imh953/cpu_only_with_unraid_in_docker_or_vm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imh953/cpu_only_with_unraid_in_docker_or_vm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1imh953/cpu_only_with_unraid_in_docker_or_vm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T21:12:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilq95h</id>
    <title>Why does ollama use the port 11434 as the default port?</title>
    <updated>2025-02-09T21:50:33+00:00</updated>
    <author>
      <name>/u/mozophe</name>
      <uri>https://old.reddit.com/user/mozophe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Because, (0)11434 = (o)llama in leetspeak.&lt;/p&gt; &lt;p&gt;For info, the max port number is 65535, so having the first zero as well is not possible as it is a 6 digit number and greater than 65535.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mozophe"&gt; /u/mozophe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilq95h/why_does_ollama_use_the_port_11434_as_the_default/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilq95h/why_does_ollama_use_the_port_11434_as_the_default/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilq95h/why_does_ollama_use_the_port_11434_as_the_default/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T21:50:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilwupn</id>
    <title>sqluniversal</title>
    <updated>2025-02-10T03:32:18+00:00</updated>
    <author>
      <name>/u/tech215</name>
      <uri>https://old.reddit.com/user/tech215</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ilwupn/sqluniversal/"&gt; &lt;img alt="sqluniversal" src="https://b.thumbs.redditmedia.com/dFkexJfNAWq3YjMNTqLgonhLsU2UtWzEpih2qdx8ooI.jpg" title="sqluniversal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Great news! Here is the corrected and improved text:&lt;/p&gt; &lt;p&gt;&amp;quot;Important announcement! I am excited to share with you that SQLUniversal is compatible with all databases. This means that you will be able to use our tool to manage and analyze data efficiently, no matter what type of database you use.&lt;/p&gt; &lt;p&gt;I am currently working on developing the front-end, but I wanted to share this news with you so that you know that we are making progress on the project.&lt;/p&gt; &lt;p&gt;Also, I want to highlight that we have tested SQLUniversal on Granite3.1-moe:1b-instruct-fp16 and the results have been excellent. This gives us confidence that our tool will be able to handle large amounts of data efficiently.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Python and the library are used: pip install flask ---{ &amp;quot;prompt&amp;quot;: &amp;quot;Get all users whose name is 'John'&amp;quot;, &amp;quot;database&amp;quot;: &amp;quot;postgresql&amp;quot; } { &amp;quot;output&amp;quot;: &amp;quot;SELECT * FROM users WHERE name = 'John';&amp;quot; }&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;/h2&gt; &lt;p&gt;&lt;a href="https://github.com/techindev/sqluniversal/tree/main"&gt;https://github.com/techindev/sqluniversal/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here you choose MySQL sqlite etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tech215"&gt; /u/tech215 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ilwupn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilwupn/sqluniversal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilwupn/sqluniversal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T03:32:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1imnu7l</id>
    <title>CPU only crashing</title>
    <updated>2025-02-11T02:18:49+00:00</updated>
    <author>
      <name>/u/justusiv</name>
      <uri>https://old.reddit.com/user/justusiv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From a pure curiosity aspect i was wondering how differently a Q4 vs a FP16 model would perform in a cpu only mode(Follow up question is how would this scale to models that is lets say 671B parameters). i started my testing with 7B and 14B models. I added CUDA_VISIBLE_DEVICES=-1 to my system variable and noticed ollama did not take any vram. However at this point ollama would continually crash and i could never actually get the verbose data as it would never complete a single prompt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/justusiv"&gt; /u/justusiv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imnu7l/cpu_only_crashing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imnu7l/cpu_only_crashing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1imnu7l/cpu_only_crashing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T02:18:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1im8t47</id>
    <title>I need your help with training an LMM</title>
    <updated>2025-02-10T15:33:20+00:00</updated>
    <author>
      <name>/u/karl27_</name>
      <uri>https://old.reddit.com/user/karl27_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Reddit,&lt;/p&gt; &lt;p&gt;(please let me know if this is the wrong sub for this kind of question, and I'll remove it.)&lt;br /&gt; (Also sorry for bad English)&lt;/p&gt; &lt;p&gt;So, my Boss just gave a New project to me: He wants me to Train an AI for my Institute, where we can Analyze Reaction Protocols (from Students) to find any Errors. Also he want it to be able to help us with Retrosynthesis (not as important as the first one).&lt;/p&gt; &lt;p&gt;The problem is: I'm a Lab assistant, i do know a Thing or two about PC's, but I'm not an expert in training an LLM. I'm struggling with quiet a lot of things. First i need to build a Machine to run the LLM, than i need to train it, to consistently analyze protocols. I've Googled quiet a lot the last Days, and learned some things, but I'm totally not comfortable in my knowledge. &lt;/p&gt; &lt;p&gt;I have a Budged of up to 4000‚Ç¨ for the Machine, which should be enough.&lt;br /&gt; I was thinking about 2 4060Ti with 16gb each, for the rest i would go with some decent but not over the top components, sins as far as i understood, only the VRAM and RAM is really important for this.&lt;/p&gt; &lt;p&gt;my Boss wants me to use DeepSeek, which I'm not sure if it would be the best choice for this kind of task, but i haven't really found something for what i need really. also I'm not really sure if i can even really train this kind of LLM or if i have to go from 0.&lt;/p&gt; &lt;p&gt;I would really appreciate any input. Thanks for your help.&lt;/p&gt; &lt;p&gt;TL;DR&lt;/p&gt; &lt;p&gt;I need help with building A Machine to train an LMM for some specific tasks and also with the Training itself&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/karl27_"&gt; /u/karl27_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1im8t47/i_need_your_help_with_training_an_lmm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1im8t47/i_need_your_help_with_training_an_lmm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1im8t47/i_need_your_help_with_training_an_lmm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T15:33:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1imi0fi</id>
    <title>How to get started with AMD Zen 4 (Ryzen 7 8845HS)</title>
    <updated>2025-02-10T21:43:53+00:00</updated>
    <author>
      <name>/u/Morpheus90x</name>
      <uri>https://old.reddit.com/user/Morpheus90x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry folks, I am lost and do not know how to start best.&lt;/p&gt; &lt;p&gt;I searched the subreddit and web but I can't find a definitive answer.&lt;/p&gt; &lt;p&gt;I have a homeserver with an AMD Ryzen 7 8845HS (Zen 4).&lt;/p&gt; &lt;p&gt;I have 96 GB DDR5 RAM.&lt;/p&gt; &lt;p&gt;The CPU has a AMD 780M GPU built in - but no dedicated VRAM (?)&lt;/p&gt; &lt;p&gt;Also it has a dedicated AI NPU.&lt;/p&gt; &lt;p&gt;I was planning to also host a few smaller services on the machine and throw the ollama stack on top on proxmox, either as an LXC or via docker&lt;/p&gt; &lt;p&gt;Let's assume I have 64+ GB free only for ollama.&lt;/p&gt; &lt;p&gt;Questions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;lxc, docker or baremetal? Any significant differences?&lt;/li&gt; &lt;li&gt;Run ollama/models default on cpu or should I utilize gpu/npu capabilties? If so how?&lt;/li&gt; &lt;li&gt;Which model should I start with? I basically have two main use cases: &amp;quot;default chatbot&amp;quot; with file upload/analysis and text OCR. Maybe I should use two dedicated models? I was thinking of deepseek-r1 and minicpm-v.&lt;/li&gt; &lt;li&gt;Can I expect acceptable performance at all for my use cases? Or am I completely lost and should rather user chatgpt cloud?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks y'all - Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Morpheus90x"&gt; /u/Morpheus90x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imi0fi/how_to_get_started_with_amd_zen_4_ryzen_7_8845hs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imi0fi/how_to_get_started_with_amd_zen_4_ryzen_7_8845hs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1imi0fi/how_to_get_started_with_amd_zen_4_ryzen_7_8845hs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T21:43:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1imdf1h</id>
    <title>My experience with Mac Mini M4 and ollama models</title>
    <updated>2025-02-10T18:39:24+00:00</updated>
    <author>
      <name>/u/Fabulous_Can_2215</name>
      <uri>https://old.reddit.com/user/Fabulous_Can_2215</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1imdf1h/my_experience_with_mac_mini_m4_and_ollama_models/"&gt; &lt;img alt="My experience with Mac Mini M4 and ollama models" src="https://b.thumbs.redditmedia.com/3PpESgQA7u3Ba8-wSSN0S42xYYghfzrLBKOgNzih0OU.jpg" title="My experience with Mac Mini M4 and ollama models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;Yesterday I bought a MacMini M4 with 24 GB RAM.&lt;/p&gt; &lt;p&gt;I was worried that it wouldn't be enough to run 7b and 8b models but it even works fine with 14b models!&lt;/p&gt; &lt;p&gt;Memory goes yellow but nothing freezes in my system!&lt;/p&gt; &lt;p&gt;So, I'm more than impressed and absolutely in love with my new computer!&lt;/p&gt; &lt;p&gt;I didn't quite understand how to train models. So if you know any tutorials, I'd be happy!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wh5lz3mewcie1.jpg?width=3072&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d432f5eed816fe16a1bdf9f402597ab413857528"&gt;https://preview.redd.it/wh5lz3mewcie1.jpg?width=3072&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d432f5eed816fe16a1bdf9f402597ab413857528&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you all for your help and suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Can_2215"&gt; /u/Fabulous_Can_2215 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imdf1h/my_experience_with_mac_mini_m4_and_ollama_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imdf1h/my_experience_with_mac_mini_m4_and_ollama_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1imdf1h/my_experience_with_mac_mini_m4_and_ollama_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T18:39:24+00:00</published>
  </entry>
</feed>
