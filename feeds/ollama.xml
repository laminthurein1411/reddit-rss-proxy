<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-30T16:49:39+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1id5z7c</id>
    <title>Multimodal ollama</title>
    <updated>2025-01-29T22:02:35+00:00</updated>
    <author>
      <name>/u/DifficultTomatillo29</name>
      <uri>https://old.reddit.com/user/DifficultTomatillo29</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using ollama constantly on my mac - and love it - have python programs that talk to ollama via rest api using various models - but now I have some tasks I want to move into multimodal - particularly in terms of getting an image back - is that possible using ollama? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DifficultTomatillo29"&gt; /u/DifficultTomatillo29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id5z7c/multimodal_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id5z7c/multimodal_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id5z7c/multimodal_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T22:02:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1idengd</id>
    <title>Production-Ready Ollama Alternative for Handling Next.js Requests?</title>
    <updated>2025-01-30T04:40:20+00:00</updated>
    <author>
      <name>/u/Apprehensive-Sale-52</name>
      <uri>https://old.reddit.com/user/Apprehensive-Sale-52</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, a few weeks ago I saw a post saying Ollama isn't for production, but now I can't find that post and I don't remember the answer, so could you say me what is the best Ollama alternative for production? I want to expose a port to recive requests from a nextjs website, or just link me to that post, thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apprehensive-Sale-52"&gt; /u/Apprehensive-Sale-52 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idengd/productionready_ollama_alternative_for_handling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idengd/productionready_ollama_alternative_for_handling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idengd/productionready_ollama_alternative_for_handling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T04:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1idgkww</id>
    <title>Download bandwidth restrictions</title>
    <updated>2025-01-30T06:24:39+00:00</updated>
    <author>
      <name>/u/Mrfudog</name>
      <uri>https://old.reddit.com/user/Mrfudog</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have set up ollama on a ubuntu VM on my proxmox host. The GPU is mounted directly on the VM. When pulling a model I get no more than 10MBps bandwidth even though I can get more on the VM (tested through ookla). Anybody got an idea if this is restricted on ubuntu or ollama itself and if I can increase this or is it limited by the server providing the model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mrfudog"&gt; /u/Mrfudog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idgkww/download_bandwidth_restrictions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idgkww/download_bandwidth_restrictions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idgkww/download_bandwidth_restrictions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T06:24:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1idcbpw</id>
    <title>How to run Ollama remotely.</title>
    <updated>2025-01-30T02:48:56+00:00</updated>
    <author>
      <name>/u/Lelentos</name>
      <uri>https://old.reddit.com/user/Lelentos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not as tech savvy as most of you, so sorry if this is a dumb question.&lt;/p&gt; &lt;p&gt;I've managed to get Ollama working on my local network, so I have it running on my desktop but can use chatbox.app on my laptop or phone. Simple enough.&lt;/p&gt; &lt;p&gt;Now I want to find out how to use it while out of the house. How do I route back to it? Would I be better off with remote desktop?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lelentos"&gt; /u/Lelentos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idcbpw/how_to_run_ollama_remotely/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idcbpw/how_to_run_ollama_remotely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idcbpw/how_to_run_ollama_remotely/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T02:48:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1idiu7q</id>
    <title>Any advice on how to get rocm working on Ubuntu?</title>
    <updated>2025-01-30T08:52:16+00:00</updated>
    <author>
      <name>/u/AxlIsAShoto</name>
      <uri>https://old.reddit.com/user/AxlIsAShoto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rocm is installed, I added my user to some groups, tried installing ollama directly in ubuntu, and also using docker.&lt;/p&gt; &lt;p&gt;When I install ollama directly it says my amd gpu is ready for use or something along those lines.&lt;/p&gt; &lt;p&gt;No error messages at all just the gpu is mostly idle and my cpu goes crazy when any model I've tried is replying to my questions.&lt;/p&gt; &lt;p&gt;I'm using an RX 6700 XT on Ubuntu 24.04. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AxlIsAShoto"&gt; /u/AxlIsAShoto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idiu7q/any_advice_on_how_to_get_rocm_working_on_ubuntu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idiu7q/any_advice_on_how_to_get_rocm_working_on_ubuntu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idiu7q/any_advice_on_how_to_get_rocm_working_on_ubuntu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T08:52:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1icv7wv</id>
    <title>Hardware requirements for running the full size deepseek R1 with ollama?</title>
    <updated>2025-01-29T14:39:12+00:00</updated>
    <author>
      <name>/u/BC547</name>
      <uri>https://old.reddit.com/user/BC547</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My machine runs the Deepseek R1-14B model fine, but the 34B and 70B are too slow for practical use. I am looking at building a machine capable of running the full 671B model fast enough that it's not too annoying as a coding assistant. What kind of hardware do i need?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BC547"&gt; /u/BC547 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icv7wv/hardware_requirements_for_running_the_full_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icv7wv/hardware_requirements_for_running_the_full_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icv7wv/hardware_requirements_for_running_the_full_size/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T14:39:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1idjux3</id>
    <title>How can I use Ollama models in Linux as an alternative to Mac Siri?</title>
    <updated>2025-01-30T10:12:16+00:00</updated>
    <author>
      <name>/u/ReceptionLow2817</name>
      <uri>https://old.reddit.com/user/ReceptionLow2817</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently I've installed the Ollama and available models from library but wanna use it like Siri in Mac or iPad or iPhone to automate my tasks. Anyone have thought how can I accomplish this using these open-source models??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ReceptionLow2817"&gt; /u/ReceptionLow2817 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idjux3/how_can_i_use_ollama_models_in_linux_as_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idjux3/how_can_i_use_ollama_models_in_linux_as_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idjux3/how_can_i_use_ollama_models_in_linux_as_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T10:12:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1idjvga</id>
    <title>Troubleshooting Ollama docker/Hardware</title>
    <updated>2025-01-30T10:13:26+00:00</updated>
    <author>
      <name>/u/notdademurphy</name>
      <uri>https://old.reddit.com/user/notdademurphy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Need suggestions on where to start trouble shooting slow OpenWebUI in docker. I'm new to running LLM's and i'm unsure of where to start. &lt;/p&gt; &lt;p&gt;Hardware CPU: AMD Ryzen 7950x&lt;br /&gt; RAM: 64GB&lt;br /&gt; GPU: Nvidia RTX 4090 24GB&lt;br /&gt; Storage: Samsung m2 SSD 980 PRO 2TB &lt;/p&gt; &lt;p&gt;When trying to use OpenWebUI inside docker connected to my Ollama container the performance is really slow. Before getting a response from a prompt the UI waits two minutes but then render the response quickly. &lt;/p&gt; &lt;p&gt;When i earlier used OpenwebUI in docker connected to a Ollama instance on the windows host there was no waiting time. &lt;/p&gt; &lt;p&gt;Running Ollama in docker with nvidia support.&lt;br /&gt; Did a basic test running 'ollama run llama3.1 --verbose' inside the docker container. &lt;/p&gt; &lt;p&gt;The result is 114 tokens/s.&lt;br /&gt; total duration: 6.734057125s&lt;br /&gt; load duration: 69.839869ms&lt;br /&gt; prompt eval count: 14 token(s)&lt;br /&gt; prompt eval duration: 3ms&lt;br /&gt; prompt eval rate: 4666.67 tokens/s&lt;br /&gt; eval count: 762 token(s)&lt;br /&gt; eval duration: 6.659s&lt;br /&gt; eval rate: 114.43 tokens/s &lt;/p&gt; &lt;p&gt;Separate question, what type/size of models are reasonable to run on my hardware?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notdademurphy"&gt; /u/notdademurphy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idjvga/troubleshooting_ollama_dockerhardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idjvga/troubleshooting_ollama_dockerhardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idjvga/troubleshooting_ollama_dockerhardware/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T10:13:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1idk3gm</id>
    <title>Running a single LLM across multiple GPUs</title>
    <updated>2025-01-30T10:30:13+00:00</updated>
    <author>
      <name>/u/Agreeable-Worker7659</name>
      <uri>https://old.reddit.com/user/Agreeable-Worker7659</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was recently thinking of running a LLM like Deepseek r1 32b on a GPU, but the problem is that it won't fit into the memory of any single GPU I could afford. Funnily enough, it runs at around human speech speed on my Ryzen 9 9950x and 64GB DDR5, but being able to run it a bit faster on GPUs would be really good.&lt;/p&gt; &lt;p&gt;Therefore the idea was to see if it could be somehow distributed across several GPUs, but if I understand correctly, that's only possible with nVlink that's available only since Volta architecture pro-grade GPUs alike Quadro or Tesla? Would it be correct to assume that with something like 2x Tesla P40 it just won't work, since they can't appear as a single unit with shared memory? Are there any AMD alternatives capable of running such setup at a budget?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agreeable-Worker7659"&gt; /u/Agreeable-Worker7659 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idk3gm/running_a_single_llm_across_multiple_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idk3gm/running_a_single_llm_across_multiple_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idk3gm/running_a_single_llm_across_multiple_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T10:30:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1id5kkt</id>
    <title>Multi GPU</title>
    <updated>2025-01-29T21:45:21+00:00</updated>
    <author>
      <name>/u/666devi</name>
      <uri>https://old.reddit.com/user/666devi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Can I mix a 4080super and a 1080ti to handle bigger models? Is it worth it or should i just sell the 1080?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/666devi"&gt; /u/666devi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id5kkt/multi_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id5kkt/multi_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id5kkt/multi_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T21:45:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1icyl2l</id>
    <title>Will there ever be uncensored self hosted AI?</title>
    <updated>2025-01-29T17:00:55+00:00</updated>
    <author>
      <name>/u/mshriver2</name>
      <uri>https://old.reddit.com/user/mshriver2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tried out Ollama today for the first time as I was excited at the possibility of having a fully uncensored and unrestricted AI that could answer any question. Unfortunately even self hosted it is just as censored as Chat-GPT or any other large AI model. Do you think we will ever have a fully open source completely unrestricted AI? I don't understand how a company gets to decide what code runs or doesn't run on my own hardware.&lt;/p&gt; &lt;p&gt;Apologies for the rant in advance.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I should be the one deciding what is &amp;quot;legal&amp;quot; or &amp;quot;ethical&amp;quot; not my computer.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model used for testing: DeepSeek-R1-Distill-Qwen-32B&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mshriver2"&gt; /u/mshriver2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icyl2l/will_there_ever_be_uncensored_self_hosted_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icyl2l/will_there_ever_be_uncensored_self_hosted_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icyl2l/will_there_ever_be_uncensored_self_hosted_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T17:00:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1idlh6k</id>
    <title>local model for animated image</title>
    <updated>2025-01-30T12:05:16+00:00</updated>
    <author>
      <name>/u/24Gameplay_</name>
      <uri>https://old.reddit.com/user/24Gameplay_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm looking for a local LLM that can generate animated pictures similar to the style shown in the video below. If you have any recommendations, please let me know!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=wn0IyvGBeUI"&gt;https://www.youtube.com/watch?v=wn0IyvGBeUI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/24Gameplay_"&gt; /u/24Gameplay_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idlh6k/local_model_for_animated_image/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idlh6k/local_model_for_animated_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idlh6k/local_model_for_animated_image/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T12:05:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1idlr85</id>
    <title>R1 Reasoning Effort for the Open-Webui</title>
    <updated>2025-01-30T12:22:15+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1idflkk/r1_reasoning_effort_for_the_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idlr85/r1_reasoning_effort_for_the_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idlr85/r1_reasoning_effort_for_the_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T12:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1idm9ou</id>
    <title>Help needed - Creating Ollama mode from GGUF</title>
    <updated>2025-01-30T12:52:12+00:00</updated>
    <author>
      <name>/u/fall2</name>
      <uri>https://old.reddit.com/user/fall2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Im freshly new to Ollama. I wold like to be able to load the following model: &lt;a href="https://huggingface.co/TheBloke/firefly-mixtral-8x7b-GGUF"&gt;https://huggingface.co/TheBloke/firefly-mixtral-8x7b-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If I understand completely;&lt;/p&gt; &lt;p&gt;Step 1: I need a file named Modelfile(without extension. With the following content:&lt;/p&gt; &lt;p&gt;FROM &amp;quot;./firefly-mixtral-8x7b.Q2_K.gguf&amp;quot;&lt;/p&gt; &lt;p&gt;Step 2: After I need to run the following command prompt: lama create Mixtral -f Modelfile&lt;/p&gt; &lt;p&gt;Am I missing something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fall2"&gt; /u/fall2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idm9ou/help_needed_creating_ollama_mode_from_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idm9ou/help_needed_creating_ollama_mode_from_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idm9ou/help_needed_creating_ollama_mode_from_gguf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T12:52:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1id92b2</id>
    <title>Cursor + Ollama -- Help a Blind Guy?</title>
    <updated>2025-01-30T00:18:10+00:00</updated>
    <author>
      <name>/u/mdizak</name>
      <uri>https://old.reddit.com/user/mdizak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Life decided to play a funny prank on me years ago and make me suddenly and totally blind. Can anyone quickly help here?&lt;/p&gt; &lt;p&gt;Looking to get Cursor IDE working with local install of Ollama. Within Cursor, I can go into Preferences -&amp;gt; Models screen and see the various textboxes for API keys, and can also see just under the OpenAI API Key textbox there's a link to override the API URL. I'm assuming this is what I want to flip the API endpoint to my local Ollama install.&lt;/p&gt; &lt;p&gt;For the life of me though, I'm unable to click on that link via screen reader, and posting on &lt;a href="/r/blind"&gt;/r/blind&lt;/a&gt; was of no help. So let's go with manual settings change route. &lt;/p&gt; &lt;p&gt;Found the settings file located at ~APP_CONFIG/Cursor/User/settings.json. On Linux Mint at least, this means at: ~/.config/Cursor/User/settings.json&lt;/p&gt; &lt;p&gt;Unfortunately, this file only lists modified settings instead of all settings. This is where I need help. &lt;/p&gt; &lt;p&gt;Could one of you kind souls that has Cursor running with local Ollama look in your settings.json file, and look for the settings that seem appropriate? Just need to send the API endpoint to Ollama's of &lt;a href="http://127.0.0.1:11434/api/generate"&gt;http://127.0.0.1:11434/api/generate&lt;/a&gt;, and put &amp;quot;deepseek-r1&amp;quot; as the model name. Looking through the setting names, it should be pretty obvious, plus any settings that looks like it's needed to enable / activate it.&lt;/p&gt; &lt;p&gt;If anyone is kind enough to reply with those setting names, would be greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mdizak"&gt; /u/mdizak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id92b2/cursor_ollama_help_a_blind_guy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id92b2/cursor_ollama_help_a_blind_guy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id92b2/cursor_ollama_help_a_blind_guy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T00:18:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1idjwkh</id>
    <title>how to use Ollama with C++</title>
    <updated>2025-01-30T10:15:50+00:00</updated>
    <author>
      <name>/u/Reasonable-Falcon470</name>
      <uri>https://old.reddit.com/user/Reasonable-Falcon470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi i am bad with python i just cant i tried and i cant find any C++ instructions for Ollama&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable-Falcon470"&gt; /u/Reasonable-Falcon470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idjwkh/how_to_use_ollama_with_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idjwkh/how_to_use_ollama_with_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idjwkh/how_to_use_ollama_with_c/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T10:15:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ido2en</id>
    <title>Where to find Model system requirements ?</title>
    <updated>2025-01-30T14:23:41+00:00</updated>
    <author>
      <name>/u/RedditNoobie777</name>
      <uri>https://old.reddit.com/user/RedditNoobie777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I upgrade I GPU from 4GB VRAM to 16GB what models will I be able to run. what token and parameter number ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedditNoobie777"&gt; /u/RedditNoobie777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ido2en/where_to_find_model_system_requirements/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ido2en/where_to_find_model_system_requirements/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ido2en/where_to_find_model_system_requirements/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T14:23:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1idq0ou</id>
    <title>How do I set context size when using openai compatible API?</title>
    <updated>2025-01-30T15:52:48+00:00</updated>
    <author>
      <name>/u/sobolanul11</name>
      <uri>https://old.reddit.com/user/sobolanul11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using openai library (I already had it setup in the project, I just added another url). I added &lt;/p&gt; &lt;pre&gt;&lt;code&gt;options: { num_ctx: 16384 } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;to the request with no result &lt;/p&gt; &lt;p&gt;I also run the ollama with /set parameter num_ctx 8192, but with no luck&lt;/p&gt; &lt;p&gt;Every request is truncated at 2048, I see this in logs:&lt;br /&gt; &lt;code&gt;level=WARN source=runner.go:129 msg=&amp;quot;truncating input prompt&amp;quot; limit=2048 prompt=3246 keep=5 new=2048&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sobolanul11"&gt; /u/sobolanul11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idq0ou/how_do_i_set_context_size_when_using_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idq0ou/how_do_i_set_context_size_when_using_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idq0ou/how_do_i_set_context_size_when_using_openai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T15:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1idqxto</id>
    <title>Why Are All Local AI Models So Bad? No One Talks About This!</title>
    <updated>2025-01-30T16:31:59+00:00</updated>
    <author>
      <name>/u/NikkEvan</name>
      <uri>https://old.reddit.com/user/NikkEvan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with local AI models, even &amp;quot;high-end ones&amp;quot; like the recent DeepSeek-R1 32B, using Open WebUI.&lt;br /&gt; I expected them to be weaker than online models, but the gap is just ridiculous.&lt;br /&gt; Even for the simplest questions, they either fail, give nonsense answers, or completely misunderstand the input.&lt;/p&gt; &lt;p&gt;Iâ€™ve set the parameters and all the settings at the best i could, tried different setups, system prompts, and still , even after parsing a basic document just a few pages long, is a struggle.&lt;br /&gt; If it already fails here, how am I supposed to use it for hundreds of internal company documents?&lt;/p&gt; &lt;p&gt;The crazy part? No one talks about this!&lt;br /&gt; Instead, i see every video in youtube saying :&lt;br /&gt; &amp;quot;How to run locally (modelname) much better than chat-gpt&amp;quot;&lt;br /&gt; &amp;quot;Local Deepseek beats Chat-gpt&amp;quot;&lt;br /&gt; Than the question they ask to those local models are : How many 'R' are in the word Strawberry and the model answer: 2 ... lol &lt;/p&gt; &lt;p&gt;Why is the performance so bad, even on 32B models?&lt;/p&gt; &lt;p&gt;Why are there no proper guides to get the best out of local AI?&lt;br /&gt; Having a big hardware such as the Nvidia project DIGITS will make a big model work close to the online Chat-gpt 3 or 4 ? I see those has 175b parameters. &lt;/p&gt; &lt;p&gt;What are we missing?&lt;/p&gt; &lt;p&gt;I really want to make local AI work as close as the online models, even buying bigger and stronger hardware, but, right now, it just feels like a waste of time.&lt;br /&gt; Has anyone actually succeeded in making these models work well? If so, how? And , what do you intend for Working Well for a local Model ? &lt;/p&gt; &lt;p&gt;Letâ€™s discuss this! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NikkEvan"&gt; /u/NikkEvan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T16:31:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1idmtrv</id>
    <title>Deepseek on an RTX 4060 ti 8gb</title>
    <updated>2025-01-30T13:22:25+00:00</updated>
    <author>
      <name>/u/whymeimbusysleeping</name>
      <uri>https://old.reddit.com/user/whymeimbusysleeping</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi gang. Can I get a recommendation on which model to use that would suit my GPU? And how to install it?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/whymeimbusysleeping"&gt; /u/whymeimbusysleeping &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idmtrv/deepseek_on_an_rtx_4060_ti_8gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idmtrv/deepseek_on_an_rtx_4060_ti_8gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idmtrv/deepseek_on_an_rtx_4060_ti_8gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T13:22:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1idf5ez</id>
    <title>How can I force Ollama to use the cpu instead of gpu?</title>
    <updated>2025-01-30T05:06:19+00:00</updated>
    <author>
      <name>/u/ResponsibleTruck4717</name>
      <uri>https://old.reddit.com/user/ResponsibleTruck4717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know I can force it use cpu instead of gpu when I'm using docker but I'm looking for solution without docker.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleTruck4717"&gt; /u/ResponsibleTruck4717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idf5ez/how_can_i_force_ollama_to_use_the_cpu_instead_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idf5ez/how_can_i_force_ollama_to_use_the_cpu_instead_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idf5ez/how_can_i_force_ollama_to_use_the_cpu_instead_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T05:06:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1idpjtj</id>
    <title>What would be the simplest way to train deepseek model on self hosted server.</title>
    <updated>2025-01-30T15:31:52+00:00</updated>
    <author>
      <name>/u/shaxadhere</name>
      <uri>https://old.reddit.com/user/shaxadhere</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to train model on specific set of data, I have the data in raw text and I have questions and answers from that raw text.&lt;/p&gt; &lt;p&gt;Is there any way I can train my model on all of that data.&lt;/p&gt; &lt;p&gt;Documents in total are 400,000 pages, and questions and answers are around 1 million+&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shaxadhere"&gt; /u/shaxadhere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idpjtj/what_would_be_the_simplest_way_to_train_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idpjtj/what_would_be_the_simplest_way_to_train_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idpjtj/what_would_be_the_simplest_way_to_train_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T15:31:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1idlbhw</id>
    <title>What's the best model that can be comfortably run on a normal PC?</title>
    <updated>2025-01-30T11:55:30+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've installed llama3.2 as this is what Ollama itself recommend at the start, so I just installed the default option, but it seems kind of bad, especially in comparison to current popular models that we access online. Not only that, its skills in non-English languages are abysmal. In English, it's sort of decent, not that bad, but I tried it in Serbian, it doesn't even properly speak the language.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idlbhw/whats_the_best_model_that_can_be_comfortably_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idlbhw/whats_the_best_model_that_can_be_comfortably_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idlbhw/whats_the_best_model_that_can_be_comfortably_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T11:55:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1id6y97</id>
    <title>I feel bad for the AI lol after seeing its chain of thought. ðŸ˜­</title>
    <updated>2025-01-29T22:44:12+00:00</updated>
    <author>
      <name>/u/Tricky_Reflection_75</name>
      <uri>https://old.reddit.com/user/Tricky_Reflection_75</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1id6y97/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt; &lt;img alt="I feel bad for the AI lol after seeing its chain of thought. ðŸ˜­" src="https://b.thumbs.redditmedia.com/enUEH3eKOb0e3umQhJmhNKG1mhLeM6dFReIv_8oKkzc.jpg" title="I feel bad for the AI lol after seeing its chain of thought. ðŸ˜­" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/910r117yg0ge1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c699f0b744adc486372072f5a73072f6d893f97e"&gt;https://preview.redd.it/910r117yg0ge1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c699f0b744adc486372072f5a73072f6d893f97e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tricky_Reflection_75"&gt; /u/Tricky_Reflection_75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id6y97/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id6y97/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id6y97/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T22:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1idh8ft</id>
    <title>Deepseek r1 671b on my local PC</title>
    <updated>2025-01-30T07:03:27+00:00</updated>
    <author>
      <name>/u/Geschirrtuch</name>
      <uri>https://old.reddit.com/user/Geschirrtuch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;Two days ago, I turned night into day, and in the end, I managed to get R1 running on my local PC. Yesterday, I uploaded a video on YouTube showing how I did it: &lt;a href="https://www.youtube.com/watch?v=O3Lk3xSkAdk"&gt;https://www.youtube.com/watch?v=O3Lk3xSkAdk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I don't post here often, so I'm not sure if sharing the link is okayâ€”I hope it is.&lt;/p&gt; &lt;p&gt;The video is in German, but with subtitles, everyone should be able to understand it.&lt;br /&gt; Be careful if you want to try this yourself! ;)&lt;/p&gt; &lt;p&gt;Update:&lt;/p&gt; &lt;p&gt;For those who don't feel like watching the video: The &amp;quot;trick&amp;quot; was using Windows' pagefile. I set up three of them on three different SSDs, which gave me around 750GB of virtual memory in total.&lt;/p&gt; &lt;p&gt;Loading the model and answering a question took my PC about 90 minutes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Geschirrtuch"&gt; /u/Geschirrtuch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T07:03:27+00:00</published>
  </entry>
</feed>
