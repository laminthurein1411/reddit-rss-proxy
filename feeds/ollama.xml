<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-05T12:26:30+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ignq9z</id>
    <title>Is there a way to "train" an open-source LLM to do one type of task really well?</title>
    <updated>2025-02-03T12:15:45+00:00</updated>
    <author>
      <name>/u/ArtPerToken</name>
      <uri>https://old.reddit.com/user/ArtPerToken</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, forgive me if its a silly question, but is there a way to train or modify an existing LLM (i guess an open source one) to do one type of tasks really well?&lt;/p&gt; &lt;p&gt;For example if I have 50 poems I wrote in my own unique style, how can I &amp;quot;feed&amp;quot; it to the LLM and then ask it to generate a new poem about a new subject in the same style?&lt;/p&gt; &lt;p&gt;Would appreciate any thoughts on the best way to go about this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArtPerToken"&gt; /u/ArtPerToken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ignq9z/is_there_a_way_to_train_an_opensource_llm_to_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ignq9z/is_there_a_way_to_train_an_opensource_llm_to_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ignq9z/is_there_a_way_to_train_an_opensource_llm_to_do/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T12:15:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihll6s</id>
    <title>Enhanced Privacy with Ollama and others</title>
    <updated>2025-02-04T16:26:08+00:00</updated>
    <author>
      <name>/u/Key_Opening_3243</name>
      <uri>https://old.reddit.com/user/Key_Opening_3243</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Iâ€™m excited to announce my Open Source tool focused on privacy during inference with AI models locally via Ollama or generic obfuscation for any case.&lt;/p&gt; &lt;p&gt;&lt;a href="https://maltese.johan.chat/"&gt;https://maltese.johan.chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I invite you all to contribute to this idea, which, although quite simple, can be highly effective in certain cases.&lt;br /&gt; Feel free to reach out to discuss the idea and how to evolve it.&lt;/p&gt; &lt;p&gt;Best regards, Johan.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Key_Opening_3243"&gt; /u/Key_Opening_3243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihll6s/enhanced_privacy_with_ollama_and_others/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihll6s/enhanced_privacy_with_ollama_and_others/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihll6s/enhanced_privacy_with_ollama_and_others/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T16:26:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihf1ou</id>
    <title>Ollama Flashcard creation</title>
    <updated>2025-02-04T10:52:15+00:00</updated>
    <author>
      <name>/u/Key_King_1216</name>
      <uri>https://old.reddit.com/user/Key_King_1216</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not a programmer. I need help creating flashcards in a csv file format and exporting it to anki using a reliable language model. Can anyone explain to me how I would do this. After downloading ollama and downloading the model whether it be deepseek-r1, or llama3, what do I do after?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Key_King_1216"&gt; /u/Key_King_1216 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihf1ou/ollama_flashcard_creation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihf1ou/ollama_flashcard_creation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihf1ou/ollama_flashcard_creation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T10:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihu2by</id>
    <title>Titan XP vs 2 Tesla M40 12GB Cards</title>
    <updated>2025-02-04T22:10:32+00:00</updated>
    <author>
      <name>/u/MajorJakePennington</name>
      <uri>https://old.reddit.com/user/MajorJakePennington</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking at building a box for running local DeepSeek models, but am having difficulty finding performance metrics for the Titan XP and the Tesla M40. For 1/2 the price of a Titan XP I can buy 2 Tesla M40 cards and have 24GB of VRAM, but is the performance there for 8b+ models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MajorJakePennington"&gt; /u/MajorJakePennington &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihu2by/titan_xp_vs_2_tesla_m40_12gb_cards/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihu2by/titan_xp_vs_2_tesla_m40_12gb_cards/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihu2by/titan_xp_vs_2_tesla_m40_12gb_cards/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T22:10:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihjfy9</id>
    <title>Slow performance on K8S</title>
    <updated>2025-02-04T14:54:29+00:00</updated>
    <author>
      <name>/u/geeky217</name>
      <uri>https://old.reddit.com/user/geeky217</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m trying to run ollama with deepseek-r1 7b on CPU only inside K8S. Iâ€™m using the official helm chart inside RedHat Openshift 4.12.2 with cephRBD storage. This is all on a vm (itâ€™s a single node openshift dev box) with 24 cores of 6148 gold Xeon cpu and 96GB. The ollama deployment has 16 cores and 16GB set as reserved. Now the issue is that it runs like a dog compared to ollama on a basic Ubuntu vm on the same esx host which has half the resources (8&amp;amp;8). The only difference is one is containerised the other just a vm. Iâ€™m at a loss why there is such a performance difference. Both ollama instances run off nvme local storage so have plenty of bandwidth and low latency. Anyone got any insights here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geeky217"&gt; /u/geeky217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihjfy9/slow_performance_on_k8s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihjfy9/slow_performance_on_k8s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihjfy9/slow_performance_on_k8s/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T14:54:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihzvrd</id>
    <title>Deepseek benchmarks</title>
    <updated>2025-02-05T02:39:54+00:00</updated>
    <author>
      <name>/u/darkgamer_nw</name>
      <uri>https://old.reddit.com/user/darkgamer_nw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where can I find benchmarks comparing the performance of deepseek-r1 8b with llama with a similar number of parameters?&lt;/p&gt; &lt;p&gt;I cannot find a page with benchmarks on recent models&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkgamer_nw"&gt; /u/darkgamer_nw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihzvrd/deepseek_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihzvrd/deepseek_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihzvrd/deepseek_benchmarks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T02:39:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihu6s2</id>
    <title>Need help with training</title>
    <updated>2025-02-04T22:15:47+00:00</updated>
    <author>
      <name>/u/Kind_Ad_2866</name>
      <uri>https://old.reddit.com/user/Kind_Ad_2866</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just entered the AI race not too long ago and I have some concepts to wrap my head around &lt;/p&gt; &lt;p&gt;1- whatâ€™s the difference between training the model via chat and training the model via unsloth&lt;/p&gt; &lt;p&gt;2- whatâ€™s the difference between datasets fed to unsloth and standard RAG where you upload bunch of files then you can ask the model about them? Iâ€™m asking because I have pdf text files (books, novels, etc) and I want to chat with the models about it or ask the AI to give me a decision based on data in these files. &lt;/p&gt; &lt;p&gt;3- if unsloth is the way to go, how would I go about creating a dataset for a novel? I have seen datasets where they mention the characters, but I donâ€™t understand how the model would piece the story just buy using the character description!&lt;/p&gt; &lt;p&gt;here is an example on the dataset I mean: &lt;a href="https://huggingface.co/datasets/xywang1/OpenCharacter?row=1"&gt;https://huggingface.co/datasets/xywang1/OpenCharacter?row=1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind_Ad_2866"&gt; /u/Kind_Ad_2866 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihu6s2/need_help_with_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihu6s2/need_help_with_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihu6s2/need_help_with_training/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T22:15:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii3nvf</id>
    <title>Deepseek Ollama download issue</title>
    <updated>2025-02-05T06:13:17+00:00</updated>
    <author>
      <name>/u/RecordSimilar2356</name>
      <uri>https://old.reddit.com/user/RecordSimilar2356</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am downloading deepseek-r1:7b using Ollama in my windows 11, but the downloading keep rolling back even if the internet speed is fine. The downloading reaches 45% let's say, then after sometime it back to 30% or even less. The downloading never completes! Any help or guide?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RecordSimilar2356"&gt; /u/RecordSimilar2356 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii3nvf/deepseek_ollama_download_issue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii3nvf/deepseek_ollama_download_issue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii3nvf/deepseek_ollama_download_issue/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T06:13:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihqub9</id>
    <title>I want to get into Local LLMs for coding, home assistant, and maybe a little conversation. Low token/s is fine to see if I even like it. Which hardware that I have listed could do it? Or do I need a GPU solely for this, even to start off?</title>
    <updated>2025-02-04T19:59:03+00:00</updated>
    <author>
      <name>/u/bigrjsuto</name>
      <uri>https://old.reddit.com/user/bigrjsuto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have the following hardware:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Personal PC (LInux Mint 21) &lt;ul&gt; &lt;li&gt;Ryzen 5800X, 64GB DDR4, 3060 12GB, 1TB NVMe + 2TB NVMe&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Server PC (Proxmox) &lt;ul&gt; &lt;li&gt;Intel 12500T, 128GB DDR4, A4000 16GB (passthrough to Windows 11 VM for Solidworks), 128GB NVMe (boot) + 1TB NVMe (VMs/LXCs), 2x 18TB HDDs&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Gaming/HTPC (Windows) &lt;ul&gt; &lt;li&gt;Intel 10600K, 32GB DDR4, RX 590 8GB, 128GB + 1TB SSD&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;MiniPCs &lt;ul&gt; &lt;li&gt;20x Datto ALTO 3 V2 &lt;ul&gt; &lt;li&gt;Celeron 3865U, 2-16GB DDR4 (Can configure as needed)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;2x Datto S3X2 Dual-NIC &lt;ul&gt; &lt;li&gt;Intel i3-7100U, 2-16GB DDR4&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;4x Optiplex 3040 &lt;ul&gt; &lt;li&gt;Intel i3-6100T, 2-8GB DDR3&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Intel NUC &lt;ul&gt; &lt;li&gt;Intel i7-7567U, 2-16GB DDR4&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Other Hardware &lt;ul&gt; &lt;li&gt;RX 560 4GB LP&lt;/li&gt; &lt;li&gt;Various extra HDDs 160GB - 8TB&lt;/li&gt; &lt;li&gt;Various extra NVMe 32GB - 500GB&lt;/li&gt; &lt;li&gt;A few extra network switches (if needed for clustering)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wanted to add the RX 560 to the server, but after some research, everything I've seen says that 4GB is too little VRAM for even slow output. That's the case, right?&lt;/p&gt; &lt;p&gt;How about a Coral TPU? Or multiple? Each of those Datto ALTO MiniPCs have a A+E keyed m.2 slot, where I could place them and cluster them together.&lt;/p&gt; &lt;p&gt;Could I just run it on my PC? Would the 3060 be good enough to get some output?&lt;/p&gt; &lt;p&gt;I know there's the A4000, but I need it for CAD work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigrjsuto"&gt; /u/bigrjsuto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihqub9/i_want_to_get_into_local_llms_for_coding_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihqub9/i_want_to_get_into_local_llms_for_coding_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihqub9/i_want_to_get_into_local_llms_for_coding_home/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T19:59:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii4wb3</id>
    <title>As a noob, which language model could you suggest for me?</title>
    <updated>2025-02-05T07:39:53+00:00</updated>
    <author>
      <name>/u/Live-Pause-6543</name>
      <uri>https://old.reddit.com/user/Live-Pause-6543</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a macbook pro with m1 pro chip. And every model writes the requirements based on vram. Therefore, I have no Idea which model I can run without any issues. &lt;/p&gt; &lt;p&gt;For now, Iâ€™ve only tried the distilled version of deepseek r1 with 7b. It worked really fast but the results were awful, although my expectations were low bc I know that these models performs worse than the website version of deepseek. &lt;/p&gt; &lt;p&gt;I will mostly use this for coding, but the languages I use changes often. (For school, I use Java, at a student internship Tailwind and vue and for personal coding, I try to explore game development, so c# c++ either. &lt;/p&gt; &lt;p&gt;Basically for general coding and sometimes for writing essays for school and so on. &lt;/p&gt; &lt;p&gt;For writing essays etc. I did not use any local model, I know the distilled version of r1 performs well just in coding and math. So, for other models, I am open to use. &lt;/p&gt; &lt;p&gt;With that being said, can you suggest me any model to discover the local llms? &lt;/p&gt; &lt;p&gt;Thx for your answers :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Live-Pause-6543"&gt; /u/Live-Pause-6543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii4wb3/as_a_noob_which_language_model_could_you_suggest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii4wb3/as_a_noob_which_language_model_could_you_suggest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii4wb3/as_a_noob_which_language_model_could_you_suggest/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T07:39:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii4zg6</id>
    <title>500: Ollama: 500, message='Internal Server Error', url='http://host.docker.internal:11434/api/chat '</title>
    <updated>2025-02-05T07:46:19+00:00</updated>
    <author>
      <name>/u/PCOwner12</name>
      <uri>https://old.reddit.com/user/PCOwner12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm relatively new to the local models and AI. I just ran my 1st docker with Ollama Win 11 and it was working perfectly, but now I consitently get this error. Does anyone know how to resolve it?&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PCOwner12"&gt; /u/PCOwner12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii4zg6/500_ollama_500_messageinternal_server_error/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii4zg6/500_ollama_500_messageinternal_server_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii4zg6/500_ollama_500_messageinternal_server_error/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T07:46:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii5ky5</id>
    <title>Facing error while installing deepseek 1.5b</title>
    <updated>2025-02-05T08:31:00+00:00</updated>
    <author>
      <name>/u/The_Arcane19</name>
      <uri>https://old.reddit.com/user/The_Arcane19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ii5ky5/facing_error_while_installing_deepseek_15b/"&gt; &lt;img alt="Facing error while installing deepseek 1.5b" src="https://preview.redd.it/l5hpg8997ahe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=301ecaa1fa019c3b33920f10364ee9848f6749b7" title="Facing error while installing deepseek 1.5b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_Arcane19"&gt; /u/The_Arcane19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l5hpg8997ahe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii5ky5/facing_error_while_installing_deepseek_15b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii5ky5/facing_error_while_installing_deepseek_15b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T08:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1iht2tf</id>
    <title>How To Setup DeepSeek-R1 LLM AI ChatBot Using Ollama On An Ubuntu Linux GPU Cloud Server (VPS)</title>
    <updated>2025-02-04T21:29:42+00:00</updated>
    <author>
      <name>/u/websplaining</name>
      <uri>https://old.reddit.com/user/websplaining</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iht2tf/how_to_setup_deepseekr1_llm_ai_chatbot_using/"&gt; &lt;img alt="How To Setup DeepSeek-R1 LLM AI ChatBot Using Ollama On An Ubuntu Linux GPU Cloud Server (VPS)" src="https://external-preview.redd.it/uNOHV2Maw2LzcoSfFMG6sju1JNogiBy71eR2QYEqjOQ.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbe5a0f4cb1ca54f001e5f84b38b48e405eb752e" title="How To Setup DeepSeek-R1 LLM AI ChatBot Using Ollama On An Ubuntu Linux GPU Cloud Server (VPS)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/websplaining"&gt; /u/websplaining &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/S_JEkuE9EyU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iht2tf/how_to_setup_deepseekr1_llm_ai_chatbot_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iht2tf/how_to_setup_deepseekr1_llm_ai_chatbot_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T21:29:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihnhpl</id>
    <title>Whats the best open source model for video generation?</title>
    <updated>2025-02-04T17:43:10+00:00</updated>
    <author>
      <name>/u/gl2101</name>
      <uri>https://old.reddit.com/user/gl2101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im currently running a 3060 setup but planning to upgrade to a more powerful GPU. &lt;/p&gt; &lt;p&gt;My main goal is to build ai videos but I donâ€™t know where to start. &lt;/p&gt; &lt;p&gt;Any recommendations are greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gl2101"&gt; /u/gl2101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihnhpl/whats_the_best_open_source_model_for_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihnhpl/whats_the_best_open_source_model_for_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihnhpl/whats_the_best_open_source_model_for_video/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T17:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii6duy</id>
    <title>How do I connect vs code on a client machine to my LLM running on my local server (in Docker)?</title>
    <updated>2025-02-05T09:33:49+00:00</updated>
    <author>
      <name>/u/Serious_Gap_3403</name>
      <uri>https://old.reddit.com/user/Serious_Gap_3403</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Goal: A free, locally hosted and customizable 'co-pilot'.&lt;/p&gt; &lt;p&gt;I have Ollama running in a container in docker on my ubuntu server and want to utilize a code-generating LLM when I use vs code on my windows machine, I am guessing I need to configure a vs code extension to connect to the port of this container somehow? If anyone has tried anything similar any recommendations for such an extension or any guides/resources that were helpful It would be very appreciated, Cheers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Serious_Gap_3403"&gt; /u/Serious_Gap_3403 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6duy/how_do_i_connect_vs_code_on_a_client_machine_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6duy/how_do_i_connect_vs_code_on_a_client_machine_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii6duy/how_do_i_connect_vs_code_on_a_client_machine_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T09:33:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii6jgj</id>
    <title>why Im not able to install</title>
    <updated>2025-02-05T09:46:02+00:00</updated>
    <author>
      <name>/u/Delicious-Ad4105</name>
      <uri>https://old.reddit.com/user/Delicious-Ad4105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ii6jgj/why_im_not_able_to_install/"&gt; &lt;img alt="why Im not able to install" src="https://b.thumbs.redditmedia.com/qdsRLqINGpalRJE3F5p1mZDWIIjWGTMvJ9CLGTer7Gw.jpg" title="why Im not able to install" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/dkmgee4lkahe1.png?width=1453&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02b4979f39589368298d4812e4b4f6234473d89e"&gt;https://preview.redd.it/dkmgee4lkahe1.png?width=1453&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02b4979f39589368298d4812e4b4f6234473d89e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h7shcwtpkahe1.png?width=344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1095f1c38997641dd03e46a1e09933692503a70"&gt;https://preview.redd.it/h7shcwtpkahe1.png?width=344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1095f1c38997641dd03e46a1e09933692503a70&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious-Ad4105"&gt; /u/Delicious-Ad4105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6jgj/why_im_not_able_to_install/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6jgj/why_im_not_able_to_install/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii6jgj/why_im_not_able_to_install/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T09:46:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihibp9</id>
    <title>Is Wikipedia RAG possible entirely locally with a gaming machine?</title>
    <updated>2025-02-04T14:02:46+00:00</updated>
    <author>
      <name>/u/trichofobia</name>
      <uri>https://old.reddit.com/user/trichofobia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all, I'm super super new, so I'm sorry if this is a stupid question, but I just heard what RAG is, I'd like to improve a local model (I'm only really familiar with deepseek, but I understand that ollama is great with RAG) with RAG.&lt;/p&gt; &lt;p&gt;I'd like to download Wikipedia locally, and use that for RAG. I've got a passable gaming laptop I don't use which has 32gb RAM, an RTX 3070 and an i7, along with an SSD.&lt;/p&gt; &lt;p&gt;I know I can download Wikipedia without images and it's something like 12-17gb. Would a local LLM be capable of searching through it automatically and choosing the best 2-3 articles based on my question? Or am I opening a can of worms?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/trichofobia"&gt; /u/trichofobia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihibp9/is_wikipedia_rag_possible_entirely_locally_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihibp9/is_wikipedia_rag_possible_entirely_locally_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihibp9/is_wikipedia_rag_possible_entirely_locally_with_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T14:02:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii7zdv</id>
    <title>Does anyone know a good AI voice-changing model for ollama or any other model that has a similar function?</title>
    <updated>2025-02-05T11:33:18+00:00</updated>
    <author>
      <name>/u/Mallowfanthe4th</name>
      <uri>https://old.reddit.com/user/Mallowfanthe4th</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean as in the replication other people's voices (just for fun).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mallowfanthe4th"&gt; /u/Mallowfanthe4th &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii7zdv/does_anyone_know_a_good_ai_voicechanging_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii7zdv/does_anyone_know_a_good_ai_voicechanging_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii7zdv/does_anyone_know_a_good_ai_voicechanging_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T11:33:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii8af6</id>
    <title>Any help for deepseek r1:70b</title>
    <updated>2025-02-05T11:54:12+00:00</updated>
    <author>
      <name>/u/isikkusgoz</name>
      <uri>https://old.reddit.com/user/isikkusgoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi i have a setup which is like Nvdia A4000 16gb, 256 gb ram Do you guys think i can run 70b model with reasonable token speeds?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/isikkusgoz"&gt; /u/isikkusgoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii8af6/any_help_for_deepseek_r170b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii8af6/any_help_for_deepseek_r170b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii8af6/any_help_for_deepseek_r170b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T11:54:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii6zut</id>
    <title>Optimizing Local LLMs on Mac Mini M4: Seeking Advice for Better Performance</title>
    <updated>2025-02-05T10:20:42+00:00</updated>
    <author>
      <name>/u/Killtec_Gaming</name>
      <uri>https://old.reddit.com/user/Killtec_Gaming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt; community! &lt;/p&gt; &lt;p&gt;We recently purchased a Mac Mini M4 (base model) for our office to run local AI operations. Our primary setup involves n8n for automation workflows integrated with Ollama, using mainly 7B and 14B models. &lt;/p&gt; &lt;p&gt;However, we've noticed that the results from these quantized models are significantly less impressive compared to cloud-based solutions. &lt;/p&gt; &lt;p&gt;We're looking for guidance on: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Are there specific optimization techniques or fine-tuning approaches we should consider? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What settings have you found most effective for 7B/14B models on Apple Silicon? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Would investing in more powerful hardware for running larger models be the only way to achieve cloud-like quality? &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any insights from those running similar setups would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Killtec_Gaming"&gt; /u/Killtec_Gaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6zut/optimizing_local_llms_on_mac_mini_m4_seeking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6zut/optimizing_local_llms_on_mac_mini_m4_seeking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii6zut/optimizing_local_llms_on_mac_mini_m4_seeking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T10:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii88up</id>
    <title>Deepseek r1 1.5B model freezes PC when prompting to code</title>
    <updated>2025-02-05T11:51:19+00:00</updated>
    <author>
      <name>/u/Daedalusayomi</name>
      <uri>https://old.reddit.com/user/Daedalusayomi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Newbie here.&lt;/p&gt; &lt;p&gt;My specs: GTX 1060 6GB VRAM, 24GB Ram, i7-8750H, nvme SSD.&lt;/p&gt; &lt;p&gt;I understand specs are not very optimal, but the 1.5B model is distilled. From what I've read, should run well. I mean it runs on smartphones.&lt;/p&gt; &lt;p&gt;When prompting some simple stuff, like &amp;quot;what is a flower&amp;quot;, r1 answers fast, but if I prompt something more complicated, example: &amp;quot;build me a next.js front-end app&amp;quot;, r1 freezes my PC.&lt;/p&gt; &lt;p&gt;Using Windows 10. No BSOD, nothing, just cursor stops moving and I have to manually turn off PC. PC fans doesn't go full, PC just freezes. Any help would be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daedalusayomi"&gt; /u/Daedalusayomi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii88up/deepseek_r1_15b_model_freezes_pc_when_prompting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii88up/deepseek_r1_15b_model_freezes_pc_when_prompting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii88up/deepseek_r1_15b_model_freezes_pc_when_prompting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T11:51:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii5o2q</id>
    <title>How do you know which LLM to use and for what use case?</title>
    <updated>2025-02-05T08:37:46+00:00</updated>
    <author>
      <name>/u/hexarthrius</name>
      <uri>https://old.reddit.com/user/hexarthrius</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there, I'm a newbie in local LLMs and maybe AI in general. Is there a guide out there that allows me to assess quickly which LLMs from Ollama are capable of doing which task?&lt;/p&gt; &lt;p&gt;I'd like to leverage AI in my local computer and later maybe branch it out to hosting my own personal service to do most stuff and maybe make an agent of myself to help me with my work (Corporate IT stuff).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hexarthrius"&gt; /u/hexarthrius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii5o2q/how_do_you_know_which_llm_to_use_and_for_what_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii5o2q/how_do_you_know_which_llm_to_use_and_for_what_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii5o2q/how_do_you_know_which_llm_to_use_and_for_what_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T08:37:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihfyi2</id>
    <title>Ollama + DeepSeek + Obsidian = The Ultimate Offline AI Assistant!</title>
    <updated>2025-02-04T11:54:17+00:00</updated>
    <author>
      <name>/u/Kind-Industry-609</name>
      <uri>https://old.reddit.com/user/Kind-Industry-609</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ihfyi2/ollama_deepseek_obsidian_the_ultimate_offline_ai/"&gt; &lt;img alt="Ollama + DeepSeek + Obsidian = The Ultimate Offline AI Assistant!" src="https://external-preview.redd.it/32JgoJVP2Vxa0PebR1pmCtaV_33XwoDfHhsNkStqIjE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49a27f19a9e63b070eb41d43f95929897b33eb6f" title="Ollama + DeepSeek + Obsidian = The Ultimate Offline AI Assistant!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind-Industry-609"&gt; /u/Kind-Industry-609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/qAsGO5N7OCk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihfyi2/ollama_deepseek_obsidian_the_ultimate_offline_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihfyi2/ollama_deepseek_obsidian_the_ultimate_offline_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T11:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii6y3n</id>
    <title>Which model is best for RAG or chatting document?</title>
    <updated>2025-02-05T10:16:59+00:00</updated>
    <author>
      <name>/u/Interesting_Music464</name>
      <uri>https://old.reddit.com/user/Interesting_Music464</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to train a model locally on my Macbook Pro M1 32GB based on a technical standard/specifications that is written in a document format like PDF. Which model would you recommend for this case? I saw that MLX is best for Apple Silicon so that is my only lead on how to properly choose a model aside from choosing the number of parameters and available unified memory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting_Music464"&gt; /u/Interesting_Music464 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6y3n/which_model_is_best_for_rag_or_chatting_document/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6y3n/which_model_is_best_for_rag_or_chatting_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii6y3n/which_model_is_best_for_rag_or_chatting_document/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T10:16:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii11dz</id>
    <title>Deepseek r1 1.5b thinking about rose flower ðŸŒ¹</title>
    <updated>2025-02-05T03:39:16+00:00</updated>
    <author>
      <name>/u/False-Woodpecker5604</name>
      <uri>https://old.reddit.com/user/False-Woodpecker5604</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ii11dz/deepseek_r1_15b_thinking_about_rose_flower/"&gt; &lt;img alt="Deepseek r1 1.5b thinking about rose flower ðŸŒ¹" src="https://preview.redd.it/7axbiaybr8he1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=719cdceebe4b50f16a0fc624b6247f706549c65f" title="Deepseek r1 1.5b thinking about rose flower ðŸŒ¹" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/False-Woodpecker5604"&gt; /u/False-Woodpecker5604 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7axbiaybr8he1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii11dz/deepseek_r1_15b_thinking_about_rose_flower/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii11dz/deepseek_r1_15b_thinking_about_rose_flower/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T03:39:16+00:00</published>
  </entry>
</feed>
