<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-06T08:25:00+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1j3riix</id>
    <title>OpenArc v1.0.1: openai endpoints, gradio dashboard with chat- get faster inference on intel CPUs, GPUs and NPUs</title>
    <updated>2025-03-05T01:31:25+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;My project, &lt;a href="https://github.com/SearchSavior/OpenArc"&gt;OpenArc&lt;/a&gt;, is an inference engine built with OpenVINO for leveraging hardware acceleration on Intel CPUs, GPUs and NPUs. Users can expect similar workflows to what's possible with Ollama, LM-Studio, Jan, OpenRouter, including a built in gradio chat, management dashboard and tools for working with Intel devices.&lt;/p&gt; &lt;p&gt;OpenArc is one of the first FOSS projects to offer a model agnostic serving engine taking full advantage of the OpenVINO runtime available from Transformers. Many other projects have support for OpenVINO as an extension but OpenArc features detailed documentation, GUI tools and discussion. Infer at the edge with text-based large language models with openai compatible endpoints tested with Gradio, OpenWebUI and SillyTavern. &lt;/p&gt; &lt;p&gt;Vision support is coming soon.&lt;/p&gt; &lt;p&gt;Since launch community support has been overwhelming; I even have a funding opportunity for OpenArc! For my first project that's pretty cool.&lt;/p&gt; &lt;p&gt;One thing we talked about was that OpenArc needs contributors who are excited about inference and getting good performance from their Intel devices.&lt;/p&gt; &lt;p&gt;Here's the ripcord:&lt;/p&gt; &lt;p&gt;An official &lt;a href="https://discord.gg/PnuTBVcr"&gt;Discord!&lt;/a&gt; - Best way to reach me. - If you are interested in contributing join the Discord!&lt;/p&gt; &lt;p&gt;Discussions on GitHub for:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SearchSavior/OpenArc/discussions/11"&gt;Linux Drivers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SearchSavior/OpenArc/discussions/12"&gt;Windows Drivers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SearchSavior/OpenArc/discussions/13"&gt;Environment Setup&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instructions and models for testing out text generation for &lt;a href="https://github.com/SearchSavior/OpenArc/issues/14"&gt;NPU devices&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;A sister repo, &lt;a href="https://github.com/SearchSavior/OpenArcProjects"&gt;OpenArcProjects&lt;/a&gt;! - Share the things you build with OpenArc, OpenVINO, oneapi toolkit, IPEX-LLM and future tooling from Intel&lt;/p&gt; &lt;p&gt;Thanks for checking out OpenArc. I hope it ends up being a useful tool.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3riix/openarc_v101_openai_endpoints_gradio_dashboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3riix/openarc_v101_openai_endpoints_gradio_dashboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3riix/openarc_v101_openai_endpoints_gradio_dashboard/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T01:31:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3y8rg</id>
    <title>What models could I reasonably use on a system with 32G RAM and 8G VRAM?</title>
    <updated>2025-03-05T08:06:49+00:00</updated>
    <author>
      <name>/u/prodego</name>
      <uri>https://old.reddit.com/user/prodego</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Arch&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prodego"&gt; /u/prodego &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3y8rg/what_models_could_i_reasonably_use_on_a_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3y8rg/what_models_could_i_reasonably_use_on_a_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3y8rg/what_models_could_i_reasonably_use_on_a_system/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T08:06:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3smbl</id>
    <title>Recommendations for a 24GB VRAM card for running Ollama and mistral-small?</title>
    <updated>2025-03-05T02:26:32+00:00</updated>
    <author>
      <name>/u/THenrich</name>
      <uri>https://old.reddit.com/user/THenrich</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recommendations for a 24GB VRAM card for running Ollama and mistral-small?&lt;br /&gt; Preferably under $1000.&lt;/p&gt; &lt;p&gt;I might run larger models in the future. I am going to send thousands of prompts to Ollama so I need something performant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/THenrich"&gt; /u/THenrich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3smbl/recommendations_for_a_24gb_vram_card_for_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3smbl/recommendations_for_a_24gb_vram_card_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3smbl/recommendations_for_a_24gb_vram_card_for_running/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T02:26:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3xirv</id>
    <title>Ollama autocomplete plugin for vim</title>
    <updated>2025-03-05T07:12:28+00:00</updated>
    <author>
      <name>/u/EMurph55</name>
      <uri>https://old.reddit.com/user/EMurph55</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j3xirv/ollama_autocomplete_plugin_for_vim/"&gt; &lt;img alt="Ollama autocomplete plugin for vim" src="https://external-preview.redd.it/0acOiSmWxXJhzHA-0AmG5I6VnFmecE3jldQQztOfa0E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be3b91aa1b587ec673db53bdb1eb9939b81aaaa4" title="Ollama autocomplete plugin for vim" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EMurph55"&gt; /u/EMurph55 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/whatever555/free-pilot-vim"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3xirv/ollama_autocomplete_plugin_for_vim/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3xirv/ollama_autocomplete_plugin_for_vim/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T07:12:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4fxfj</id>
    <title>Browser-Use + vLLM + 8x AMD Instinct Mi60 Server</title>
    <updated>2025-03-05T22:24:34+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kdwoez165yme1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4fxfj/browseruse_vllm_8x_amd_instinct_mi60_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4fxfj/browseruse_vllm_8x_amd_instinct_mi60_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T22:24:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4g5t2</id>
    <title>puterjs and ollama or any other system</title>
    <updated>2025-03-05T22:34:04+00:00</updated>
    <author>
      <name>/u/Creepy-Being-6900</name>
      <uri>https://old.reddit.com/user/Creepy-Being-6900</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey guys please google puter.js and tell me, i wonder can we set cline or roo code or any other autonom coding platform using with puter.js. they gut free sonnet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creepy-Being-6900"&gt; /u/Creepy-Being-6900 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4g5t2/puterjs_and_ollama_or_any_other_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4g5t2/puterjs_and_ollama_or_any_other_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4g5t2/puterjs_and_ollama_or_any_other_system/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T22:34:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4anpe</id>
    <title>Anyone else having slow LLM response issues with 0.5.13?</title>
    <updated>2025-03-05T18:53:02+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;0.5.13 (accessed via Open WebUI 0.5.18) seems to make everything very slow, non-responsive and generally unusable for me. I‚Äôve tried it on 3 different computers and same issues on all of them. I downgraded to 0.5.12 and things are running perfectly on that release but 0.5.13 runs like hot garbage. Anyone experiencing similar issues? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4anpe/anyone_else_having_slow_llm_response_issues_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4anpe/anyone_else_having_slow_llm_response_issues_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4anpe/anyone_else_having_slow_llm_response_issues_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T18:53:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3nwiw</id>
    <title>Generate an Entire Project from ONE Prompt</title>
    <updated>2025-03-04T22:44:22+00:00</updated>
    <author>
      <name>/u/No-Mulberry6961</name>
      <uri>https://old.reddit.com/user/No-Mulberry6961</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created an AI platform that allows a user to enter a single prompt with technical requirements and the LLM of choice thoroughly plans out and builds the entire thing nonstop until it is completely finished.&lt;/p&gt; &lt;p&gt;Here is a project it built last night using Claude 3.7, which took about 3 hours and has 214 files (can use any LLM, local, API, ollama etc‚Ä¶)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/justinlietz93/neuroca"&gt;https://github.com/justinlietz93/neuroca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm improving it every day as well and building an extension that locks into existing projects to finish them or add functionality&lt;/p&gt; &lt;p&gt;I have been asked to use my system to finish this project:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Recruitler/SortableTS"&gt;https://github.com/Recruitler/SortableTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If it is capable of doing that with a single prompt, then I can prove this legitimately is a novel and potentially breakthrough strategy for software development using AI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Mulberry6961"&gt; /u/No-Mulberry6961 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3nwiw/generate_an_entire_project_from_one_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3nwiw/generate_an_entire_project_from_one_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3nwiw/generate_an_entire_project_from_one_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T22:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1j484ba</id>
    <title>Is it possible to run models on a pc with 2 gpu-s, but one is amd and one is nvidia? Has anyone tried that</title>
    <updated>2025-03-05T17:12:32+00:00</updated>
    <author>
      <name>/u/fracturedbudhole</name>
      <uri>https://old.reddit.com/user/fracturedbudhole</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello llama friends. I am wondering if it is possible to run local models on a pc with 2 gpu-s, but one is amd amd one ia nvidia. I am currently running on amd 6800xt 16gb and i would like to get more video memory to run bigger models, recently i have seen a GTX 1080ti offering used for a good deal - 200e i think, so i am wondering if it is possible to use both cards to run moddels with 26gb combined vram. Or is it better to get another amd gpu (does it need to be the same or could it be newer model)? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fracturedbudhole"&gt; /u/fracturedbudhole &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j484ba/is_it_possible_to_run_models_on_a_pc_with_2_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j484ba/is_it_possible_to_run_models_on_a_pc_with_2_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j484ba/is_it_possible_to_run_models_on_a_pc_with_2_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T17:12:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3fh7d</id>
    <title>Ollama-OCR</title>
    <updated>2025-03-04T16:58:47+00:00</updated>
    <author>
      <name>/u/imanoop7</name>
      <uri>https://old.reddit.com/user/imanoop7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I open-sourced &lt;strong&gt;Ollama-OCR&lt;/strong&gt; ‚Äì an advanced &lt;strong&gt;OCR tool&lt;/strong&gt; powered by &lt;strong&gt;LLaVA 7B&lt;/strong&gt; and &lt;strong&gt;Llama 3.2 Vision&lt;/strong&gt; to extract text from images with high accuracy! üöÄ&lt;/p&gt; &lt;p&gt;üîπ &lt;strong&gt;Features:&lt;/strong&gt;&lt;br /&gt; ‚úÖ Supports &lt;strong&gt;Markdown, Plain Text, JSON, Structured, Key-Value Pairs&lt;/strong&gt;&lt;br /&gt; ‚úÖ &lt;strong&gt;Batch processing&lt;/strong&gt; for handling multiple images efficiently&lt;br /&gt; ‚úÖ Uses &lt;strong&gt;state-of-the-art vision-language models&lt;/strong&gt; for better OCR&lt;br /&gt; ‚úÖ Ideal for &lt;strong&gt;document digitization, data extraction, and automation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Check it out &amp;amp; contribute! üîó &lt;a href="https://github.com/imanoop7/Ollama-OCR"&gt;GitHub: Ollama-OCR&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Details about Python Package -&lt;a href="https://medium.com/@mauryaanoop3/ollama-ocr-now-available-as-a-python-package-ff5e4240eb26"&gt; &lt;strong&gt;Guide&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thoughts? Feedback? Let‚Äôs discuss! üî•&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/imanoop7"&gt; /u/imanoop7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3fh7d/ollamaocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3fh7d/ollamaocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3fh7d/ollamaocr/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T16:58:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4hojl</id>
    <title>Embeddings - example needed</title>
    <updated>2025-03-05T23:38:10+00:00</updated>
    <author>
      <name>/u/No-Respond-9340</name>
      <uri>https://old.reddit.com/user/No-Respond-9340</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a bit confused. I am trying to understand embedding and vectors and wrote a little bare bones node program to be able to compare the cosine similarity between different words and sentences. As suspected, using the same sentences gives me a score of 1.0 . Closely related sentences score between 0.75 and 0.91. But here's the kicker: comparing &amp;quot;alive&amp;quot; and &amp;quot;dead&amp;quot; also gives me a score of 0.74 (using both, the mxbai-embed-large and nomic-embed-text models). That doesn't make sense to me as both words (or related sentences) have a completely different meaning. I already looked at my cosineSimilarity function and replaced it with another approach, but the result stays the same. &lt;/p&gt; &lt;p&gt;So - my question: Is my little demo software screwing up or is that expected behavior?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Respond-9340"&gt; /u/No-Respond-9340 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4hojl/embeddings_example_needed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4hojl/embeddings_example_needed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4hojl/embeddings_example_needed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T23:38:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j44psq</id>
    <title>Server Room / Storage</title>
    <updated>2025-03-05T14:47:54+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j44psq/server_room_storage/"&gt; &lt;img alt="Server Room / Storage" src="https://preview.redd.it/u0r5ec7xuvme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7656952a35b5e1c04c19106e61522e536e80e6ae" title="Server Room / Storage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u0r5ec7xuvme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j44psq/server_room_storage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j44psq/server_room_storage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T14:47:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4eesh</id>
    <title>Models for coding</title>
    <updated>2025-03-05T21:23:45+00:00</updated>
    <author>
      <name>/u/waeljlassii</name>
      <uri>https://old.reddit.com/user/waeljlassii</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 32gb ram / 8 vram Which model should be suitable/ best for full coding tasks Anyone tried something or cam advise?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waeljlassii"&gt; /u/waeljlassii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4eesh/models_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4eesh/models_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4eesh/models_for_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T21:23:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4mf0d</id>
    <title>Build a RAG with a Validation Refine Prompt to Talk to Your PDF Using Ollama &amp; LangChain</title>
    <updated>2025-03-06T03:35:36+00:00</updated>
    <author>
      <name>/u/Spirited-Wind6803</name>
      <uri>https://old.reddit.com/user/Spirited-Wind6803</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Hi everyone. I just want to share with you this tutorial that I am:&lt;/h1&gt; &lt;p&gt;explore how the &lt;strong&gt;Refine Prompt&lt;/strong&gt; apply to RAG PDF iteratively improves the chatbot‚Äôs responses by revisiting each chunk of text‚Äîensuring higher accuracy and less ‚Äúhallucination.‚Äù &lt;/p&gt; &lt;p&gt;The link is here: &lt;a href="https://www.youtube.com/watch?v=E-6L5an388E"&gt;https://www.youtube.com/watch?v=E-6L5an388E&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would love to get your feedback, does refine prompt help your RAG application?&lt;/p&gt; &lt;p&gt;Basically, let's say you have 3 relevant chunks, for the first chunk, the bot will generate an initial response. Then the refine prompt will be:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;refine_prompt_template = &amp;quot;&amp;quot;&amp;quot;You are a teaching chatbot. We have an existing answer: {existing_answer} We have the following new context to consider: {context} Please refine the original answer if there's new or better information. If the new context does not change or add anything to the original answer, keep it the same. If the answer is not in the source data or is incomplete, say: &amp;quot;I‚Äôm sorry, but I couldn‚Äôt find the information in the provided data.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spirited-Wind6803"&gt; /u/Spirited-Wind6803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4mf0d/build_a_rag_with_a_validation_refine_prompt_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4mf0d/build_a_rag_with_a_validation_refine_prompt_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4mf0d/build_a_rag_with_a_validation_refine_prompt_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T03:35:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4mw58</id>
    <title>Total noob, GPU offloading in docker on ubuntu</title>
    <updated>2025-03-06T04:02:16+00:00</updated>
    <author>
      <name>/u/Inner-End7733</name>
      <uri>https://old.reddit.com/user/Inner-End7733</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After a quick search of the sub, I can tell most people doing this stuff know more than me, but here goes: I've been running mistral 7b and deepseek-r1 7b on docker on Ubuntu, I installed an app for monitoring my gpu usage since system monitor doesn't display GPUs, and I noticed pretty steady 30% usage on my rtx 3060, and 60% usage on my CPU when running inference. &lt;/p&gt; &lt;p&gt;I followed the instructions here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image"&gt;https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image&lt;/a&gt; under the linux section, including installing the nvidia toolkit and running the container with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm new to all the things so I'm hoping someone will be generous with me here haha. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inner-End7733"&gt; /u/Inner-End7733 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4mw58/total_noob_gpu_offloading_in_docker_on_ubuntu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4mw58/total_noob_gpu_offloading_in_docker_on_ubuntu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4mw58/total_noob_gpu_offloading_in_docker_on_ubuntu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T04:02:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j453s7</id>
    <title>Apple released Mac Studio with M4 Max and M3 Ultra</title>
    <updated>2025-03-05T15:05:06+00:00</updated>
    <author>
      <name>/u/vsurresh</name>
      <uri>https://old.reddit.com/user/vsurresh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;M3 Ultra supports up to 512 GB of RAM for almost ¬£10k&lt;/p&gt; &lt;p&gt;M4 Max with 128 GB of RAM is around ¬£3600&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.apple.com/uk/shop/buy-mac/mac-studio"&gt;https://www.apple.com/uk/shop/buy-mac/mac-studio&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vsurresh"&gt; /u/vsurresh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j453s7/apple_released_mac_studio_with_m4_max_and_m3_ultra/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j453s7/apple_released_mac_studio_with_m4_max_and_m3_ultra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j453s7/apple_released_mac_studio_with_m4_max_and_m3_ultra/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T15:05:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4gmqr</id>
    <title>Run DeepSeek R1 671B Q4_K_M with 1~2 Arc A770 on Xeon</title>
    <updated>2025-03-05T22:53:19+00:00</updated>
    <author>
      <name>/u/bigbigmind</name>
      <uri>https://old.reddit.com/user/bigbigmind</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;gt;8 token/s using the latest llama.cpp Portable Zip from IPEX-LLM: &lt;a href="https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llamacpp_portable_zip_gpu_quickstart.md#flashmoe-for-deepseek-v3r1"&gt;https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llamacpp_portable_zip_gpu_quickstart.md#flashmoe-for-deepseek-v3r1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigbigmind"&gt; /u/bigbigmind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4gmqr/run_deepseek_r1_671b_q4_k_m_with_12_arc_a770_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4gmqr/run_deepseek_r1_671b_q4_k_m_with_12_arc_a770_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4gmqr/run_deepseek_r1_671b_q4_k_m_with_12_arc_a770_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T22:53:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4egbh</id>
    <title>How does num_ctx and model's context length work (together)?</title>
    <updated>2025-03-05T21:25:24+00:00</updated>
    <author>
      <name>/u/Kubas_inko</name>
      <uri>https://old.reddit.com/user/Kubas_inko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone. I searched for this but didn't find any useful answers. In ollama, you can set the context length of a model by setting its &lt;code&gt;num_ctx&lt;/code&gt; parameter. But, the model also has its own &lt;code&gt;context length&lt;/code&gt; information when you do &lt;code&gt;ollama show __model__&lt;/code&gt;. How are these 2 related? What happens when &lt;code&gt;num_ctx&lt;/code&gt; is lower than the &lt;code&gt;context length&lt;/code&gt; (or the other way around)? If a model does not have the &lt;code&gt;num_ctx&lt;/code&gt; parameter, what is its context length?&lt;/p&gt; &lt;p&gt;For example, if a model has &lt;code&gt;context length&lt;/code&gt; = &lt;code&gt;102400&lt;/code&gt; and &lt;code&gt;num_ctx&lt;/code&gt; is set to &lt;code&gt;32764&lt;/code&gt;, what is the context length? Or if the values were flipped?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kubas_inko"&gt; /u/Kubas_inko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4egbh/how_does_num_ctx_and_models_context_length_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4egbh/how_does_num_ctx_and_models_context_length_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4egbh/how_does_num_ctx_and_models_context_length_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T21:25:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4p2eh</id>
    <title>Recommended settings for QwQ 32B</title>
    <updated>2025-03-06T06:12:24+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j4p2eh/recommended_settings_for_qwq_32b/"&gt; &lt;img alt="Recommended settings for QwQ 32B" src="https://external-preview.redd.it/lBZs0Q7c65_lYRXO4ivgUAuYiqvkD7hvWkRogLEWXvw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85d8af51e17945c43660cee6e1f3aae2b98b5f55" title="Recommended settings for QwQ 32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1j4p1fb/recommended_settings_for_qwq_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4p2eh/recommended_settings_for_qwq_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4p2eh/recommended_settings_for_qwq_32b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T06:12:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4cph2</id>
    <title>Ollama 32B on Nvidia Jetson AGX</title>
    <updated>2025-03-05T20:15:15+00:00</updated>
    <author>
      <name>/u/einthecorgi2</name>
      <uri>https://old.reddit.com/user/einthecorgi2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;code&gt;ollama run deepseek-r1:32b --verbose [14:32:21]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; hellow, how are you?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Hello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with whatever you need. How are *you* doing? üòä&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;total duration: 21.143970238s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;load duration: 52.6187ms&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval count: 10 token(s)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval duration: 1.126s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval rate: 8.88 tokens/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval count: 44 token(s)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval duration: 19.963s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval rate: 2.20 tokens/s&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/einthecorgi2"&gt; /u/einthecorgi2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4cph2/ollama_32b_on_nvidia_jetson_agx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4cph2/ollama_32b_on_nvidia_jetson_agx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4cph2/ollama_32b_on_nvidia_jetson_agx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T20:15:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4pecf</id>
    <title>Using "tools" support (or function calling) with LangchainJS and Ollama</title>
    <updated>2025-03-06T06:35:07+00:00</updated>
    <author>
      <name>/u/Inevitable-Judge2642</name>
      <uri>https://old.reddit.com/user/Inevitable-Judge2642</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j4pecf/using_tools_support_or_function_calling_with/"&gt; &lt;img alt="Using &amp;quot;tools&amp;quot; support (or function calling) with LangchainJS and Ollama" src="https://external-preview.redd.it/uOAwdw5hhwqw_jo_ftA1OGWP6olZQCGU2qrjZd0cVGc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=414eaea7fd8aeb7667d7a1b1288a4833cc3917ed" title="Using &amp;quot;tools&amp;quot; support (or function calling) with LangchainJS and Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Judge2642"&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://k33g.hashnode.dev/using-tools-support-or-function-calling-with-langchainjs-and-ollama"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4pecf/using_tools_support_or_function_calling_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4pecf/using_tools_support_or_function_calling_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T06:35:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4nl15</id>
    <title>LLM Inference Hardware Calculator</title>
    <updated>2025-03-06T04:41:10+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just wanted to share Youtuber Alex Ziskind's cool LLM Inference Hardware Calculator tool. You can gauge what model sizes, quant levels, and context sizes certain hardware can handle before you buy.&lt;/p&gt; &lt;p&gt;I find it very useful in aiding in the decision of buying the newly released Mac Studio M3 Ultra or NVIDIA digits that is coming out soon.&lt;/p&gt; &lt;p&gt;Here it is:&lt;br /&gt; &lt;a href="https://llm-inference-calculator-rki02.kinsta.page/"&gt;https://llm-inference-calculator-rki02.kinsta.page/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4nl15/llm_inference_hardware_calculator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4nl15/llm_inference_hardware_calculator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4nl15/llm_inference_hardware_calculator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T04:41:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4pyzt</id>
    <title>Mac Studio M3 Ultra: Is it worth the hype?</title>
    <updated>2025-03-06T07:15:15+00:00</updated>
    <author>
      <name>/u/_ggsa</name>
      <uri>https://old.reddit.com/user/_ggsa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see many people excited about the new Mac Studio with 512GB RAM (and M3 Ultra), but not everyone understands that LLM inference speed is directly tied to bandwidth, which has remained roughly the same. Also, there's a direct correlation between token/s and model size - so even if a 671B model fits in your VRAM, the benefits of 1-2 token/s (even with less than q4 quantization) are negligible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_ggsa"&gt; /u/_ggsa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4pyzt/mac_studio_m3_ultra_is_it_worth_the_hype/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4pyzt/mac_studio_m3_ultra_is_it_worth_the_hype/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4pyzt/mac_studio_m3_ultra_is_it_worth_the_hype/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T07:15:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4l7of</id>
    <title>Made a simple playground for easy experiment with 8+ open-source PDF-to-markdown for document ingestion (+ visualization)</title>
    <updated>2025-03-06T02:33:06+00:00</updated>
    <author>
      <name>/u/taprosoft</name>
      <uri>https://old.reddit.com/user/taprosoft</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j4l7of/made_a_simple_playground_for_easy_experiment_with/"&gt; &lt;img alt="Made a simple playground for easy experiment with 8+ open-source PDF-to-markdown for document ingestion (+ visualization)" src="https://external-preview.redd.it/Ik_UOXUVGTKj5B9oOW8_FISqZe0LfJ9NkHqhzs4tgyU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c48cee8cb3ea64e35dbb0891bce79f17bc38eb0" title="Made a simple playground for easy experiment with 8+ open-source PDF-to-markdown for document ingestion (+ visualization)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taprosoft"&gt; /u/taprosoft &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/chunking-ai/pdf-playground"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4l7of/made_a_simple_playground_for_easy_experiment_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4l7of/made_a_simple_playground_for_easy_experiment_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T02:33:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4fj53</id>
    <title>Tool for finding max context for your GPU</title>
    <updated>2025-03-05T22:08:43+00:00</updated>
    <author>
      <name>/u/Daemonero</name>
      <uri>https://old.reddit.com/user/Daemonero</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I put this together over the past few days and thought it might be useful for others. I am still working on adding features and fixing some stalling issues, but it works well as is.&lt;/p&gt; &lt;p&gt;The MaxContextFinder is a tool that tests and determines the maximum usable context size for Ollama models by incrementally testing larger context windows while monitoring key performance metrics like token processing speed, VRAM usage, and response times. It helps users find the optimal balance between context size and performance for their specific hardware setup, stopping tests when it detects performance degradation or resource limits being reached, and provides recommendations for the largest reliable context window size.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Scionero/MaxContextFinder"&gt;Github Repo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daemonero"&gt; /u/Daemonero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4fj53/tool_for_finding_max_context_for_your_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4fj53/tool_for_finding_max_context_for_your_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4fj53/tool_for_finding_max_context_for_your_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T22:08:43+00:00</published>
  </entry>
</feed>
