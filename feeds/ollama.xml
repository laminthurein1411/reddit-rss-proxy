<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-17T05:48:50+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1iqmwzc</id>
    <title>Project MIGIT - AI Server on a Potato</title>
    <updated>2025-02-16T07:22:03+00:00</updated>
    <author>
      <name>/u/nootropicMan</name>
      <uri>https://old.reddit.com/user/nootropicMan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iqmwzc/project_migit_ai_server_on_a_potato/"&gt; &lt;img alt="Project MIGIT - AI Server on a Potato" src="https://external-preview.redd.it/i4NontxJ02_aMaQtea9AOw3-C8-ndibDJuBa_qA7H50.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d5e1314cc4bf93c83a5691c1fb9cb1548b415095" title="Project MIGIT - AI Server on a Potato" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nootropicMan"&gt; /u/nootropicMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1ipy50d/project_migit_ai_server_on_a_potato/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqmwzc/project_migit_ai_server_on_a_potato/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqmwzc/project_migit_ai_server_on_a_potato/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T07:22:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq6054</id>
    <title>How do you decide which model to run?</title>
    <updated>2025-02-15T17:10:46+00:00</updated>
    <author>
      <name>/u/Serious-Mode</name>
      <uri>https://old.reddit.com/user/Serious-Mode</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have Ollama up and running, along with Open WebUI. Wanting to use the best general purpose model I can on my Windows 10 PC with an RTX 4060TI 16GB, but not quite sure how to make that decision. I believe with my card I can pull off 14b models? Is the deepseek-r1 model from the Ollama library the go to at the moment? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Serious-Mode"&gt; /u/Serious-Mode &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq6054/how_do_you_decide_which_model_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq6054/how_do_you_decide_which_model_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq6054/how_do_you_decide_which_model_to_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T17:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq7zyg</id>
    <title>Any Tarantino fans?</title>
    <updated>2025-02-15T18:38:20+00:00</updated>
    <author>
      <name>/u/Sufficient-Wealth-78</name>
      <uri>https://old.reddit.com/user/Sufficient-Wealth-78</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iq7zyg/any_tarantino_fans/"&gt; &lt;img alt="Any Tarantino fans?" src="https://b.thumbs.redditmedia.com/0-aQWQeXVszY1S13UnlFzwgTFVs6rLhIUORwhHX3yhs.jpg" title="Any Tarantino fans?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is this some multidimensional parallel universe thingy? I mean if there's somewhere in different dimension Django sequel, I am moving&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sufficient-Wealth-78"&gt; /u/Sufficient-Wealth-78 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iq7zyg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq7zyg/any_tarantino_fans/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq7zyg/any_tarantino_fans/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T18:38:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqh8eg</id>
    <title>Can i use a gt 1030 for ollama</title>
    <updated>2025-02-16T01:42:19+00:00</updated>
    <author>
      <name>/u/leothixx6477</name>
      <uri>https://old.reddit.com/user/leothixx6477</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i have a gt 1030 and i want to use ollama, i can?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leothixx6477"&gt; /u/leothixx6477 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqh8eg/can_i_use_a_gt_1030_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqh8eg/can_i_use_a_gt_1030_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqh8eg/can_i_use_a_gt_1030_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T01:42:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqgso8</id>
    <title>how do i get ollama to use specific folders</title>
    <updated>2025-02-16T01:18:52+00:00</updated>
    <author>
      <name>/u/Joereichard</name>
      <uri>https://old.reddit.com/user/Joereichard</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;like a folder for its memory dataset i have a star trek data memory html that i want it to use when talking i'm also interested in integrating pygpt to work with it and voice synthesis to produce speech from a .wav file i'm trying to build it on edubuntu to help with my learning process and i want it to be able to learn with me to adapt to my learning style and pace &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Joereichard"&gt; /u/Joereichard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqgso8/how_do_i_get_ollama_to_use_specific_folders/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqgso8/how_do_i_get_ollama_to_use_specific_folders/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqgso8/how_do_i_get_ollama_to_use_specific_folders/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T01:18:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq5gr9</id>
    <title>Building a High-Performance AI Setup on a ‚Ç¨5000 Budget</title>
    <updated>2025-02-15T16:47:22+00:00</updated>
    <author>
      <name>/u/Severe_Biscotti2349</name>
      <uri>https://old.reddit.com/user/Severe_Biscotti2349</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iq5gr9/building_a_highperformance_ai_setup_on_a_5000/"&gt; &lt;img alt="Building a High-Performance AI Setup on a ‚Ç¨5000 Budget" src="https://external-preview.redd.it/FLrvkgnQaxhXP5T5ghlV_Eex6-uyrt_3lBcKa0Bh6Vk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a5027bcdc7a7acd346a77e15fd1f88af8fadff3" title="Building a High-Performance AI Setup on a ‚Ç¨5000 Budget" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm diving into building my own setup to run 70B LLMs in 4-bit with Ollama + OpenWebUI, and I‚Äôd love your insights! My budget is around ‚Ç¨5000, and I‚Äôm considering a dual RTX 3090 setup. I came across this configuration: &lt;a href="https://github.com/letsRTFM/AI-Workstation?tab=readme-ov-file"&gt;https://github.com/letsRTFM/AI-Workstation?tab=readme-ov-file&lt;/a&gt; . Does this look like a solid choice? Any recommendations for optimizations? (Also i wanted to use that pc for test and gaming, so i was thinking of a dual boot with ubuntu for dev and Windows for gaming, not a fan of wsl) &lt;/p&gt; &lt;p&gt;I‚Äôm also starting to help small company to implement AI solutions but 100% local also so i‚Äôm curious about the requirements. For a team of 20-30 people, handling around 2-3 simultaneous queries, what kind of internal setup would be needed to keep things running smoothly? (Also the cloud solution are intresting but some clients need physical servers) &lt;/p&gt; &lt;p&gt;I‚Äôm eager to learn and work on projects where I can gain hands-on experience. Looking forward to your thoughts and advice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe_Biscotti2349"&gt; /u/Severe_Biscotti2349 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/letsRTFM/AI-Workstation?tab=readme-ov-file"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq5gr9/building_a_highperformance_ai_setup_on_a_5000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq5gr9/building_a_highperformance_ai_setup_on_a_5000/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T16:47:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqv14l</id>
    <title>‚ÄúStruggling to Get LiteLLM Working With Ollama in Docker on Mac M1 - Empty Model List &amp; Connection Issues üçéüê≥‚Äù</title>
    <updated>2025-02-16T15:48:38+00:00</updated>
    <author>
      <name>/u/ate50eggs</name>
      <uri>https://old.reddit.com/user/ate50eggs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/developers"&gt;r/developers&lt;/a&gt;! Looking for some help with LiteLLM and Ollama integration. Here's what's going on:&lt;/p&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;Running LiteLLM in Docker, but &lt;code&gt;/v1/models&lt;/code&gt; endpoint returns an empty list despite:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ollama models working fine via &lt;code&gt;ollama list&lt;/code&gt; and direct API calls&lt;/li&gt; &lt;li&gt;Confirmed network connectivity between containers and host&lt;/li&gt; &lt;li&gt;Properly mounted config files in Docker&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Current Symptoms&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;curl&lt;/code&gt; &lt;a href="http://localhost:4000/v1/models"&gt;&lt;code&gt;http://localhost:4000/v1/models&lt;/code&gt;&lt;/a&gt; returns: &lt;code&gt;{&amp;quot;data&amp;quot;:[],&amp;quot;object&amp;quot;:&amp;quot;list&amp;quot;}&lt;/code&gt;&lt;/li&gt; &lt;li&gt;LiteLLM logs show &amp;quot;Connection refused&amp;quot; errors to Ollama API&lt;/li&gt; &lt;li&gt;Healthchecks pass but models remain invisible&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Environment&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Mac Mini (M1) running macOS Sonoma&lt;/li&gt; &lt;li&gt;Docker Desktop 4.25+ (ARM64)&lt;/li&gt; &lt;li&gt;Ollama v0.1.25 running natively (not containerized)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Configuration Files&lt;/h1&gt; &lt;h1&gt;=== docker-compose.yml ===&lt;/h1&gt; &lt;p&gt;services:&lt;/p&gt; &lt;p&gt;litellm:&lt;/p&gt; &lt;p&gt;container_name: litellm&lt;/p&gt; &lt;p&gt;platform: linux/arm64&lt;/p&gt; &lt;p&gt;image: &lt;a href="http://ghcr.io/berriai/litellm:arm64"&gt;ghcr.io/berriai/litellm:arm64&lt;/a&gt;&lt;/p&gt; &lt;p&gt;restart: unless-stopped&lt;/p&gt; &lt;p&gt;environment:&lt;/p&gt; &lt;p&gt;LITELLM_CONFIG: /config/config.yaml&lt;/p&gt; &lt;p&gt;OLLAMA_HOST: &lt;a href="http://host.docker.internal:11434/"&gt;http://host.docker.internal:11434/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LITELLM_LOG_LEVEL: debug&lt;/p&gt; &lt;p&gt;UVICORN_LOG_LEVEL: debug&lt;/p&gt; &lt;p&gt;volumes:&lt;/p&gt; &lt;p&gt;- ./config/litellm:/config&lt;/p&gt; &lt;p&gt;ports:&lt;/p&gt; &lt;p&gt;- &amp;quot;4000:4000&amp;quot;&lt;/p&gt; &lt;p&gt;networks:&lt;/p&gt; &lt;p&gt;- ai-network&lt;/p&gt; &lt;p&gt;networks:&lt;/p&gt; &lt;p&gt;ai-network:&lt;/p&gt; &lt;h1&gt;=== config.yaml ===&lt;/h1&gt; &lt;p&gt;litellm_settings:&lt;/p&gt; &lt;p&gt;drop_params: True&lt;/p&gt; &lt;p&gt;set_verbose: True&lt;/p&gt; &lt;p&gt;router_settings:&lt;/p&gt; &lt;p&gt;model_list:&lt;/p&gt; &lt;p&gt;- model_name: llama3.1&lt;/p&gt; &lt;p&gt;litellm_params:&lt;/p&gt; &lt;p&gt;model: ollama/llama3.1:8b&lt;/p&gt; &lt;p&gt;api_base: &lt;a href="http://host.docker.internal:11434/"&gt;http://host.docker.internal:11434/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- model_name: deepseek-coder&lt;/p&gt; &lt;p&gt;litellm_params:&lt;/p&gt; &lt;p&gt;model: ollama/deepseek-coder-v2:latest&lt;/p&gt; &lt;p&gt;api_base: &lt;a href="http://host.docker.internal:11434/"&gt;http://host.docker.internal:11434/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- model_name: nomic-embed&lt;/p&gt; &lt;p&gt;litellm_params:&lt;/p&gt; &lt;p&gt;model: ollama/nomic-embed-text:latest&lt;/p&gt; &lt;p&gt;api_base: &lt;a href="http://host.docker.internal:11434/"&gt;http://host.docker.internal:11434/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ate50eggs"&gt; /u/ate50eggs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqv14l/struggling_to_get_litellm_working_with_ollama_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqv14l/struggling_to_get_litellm_working_with_ollama_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqv14l/struggling_to_get_litellm_working_with_ollama_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T15:48:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqv9pu</id>
    <title>[Fixed] ollama not opening/running in windows</title>
    <updated>2025-02-16T15:59:49+00:00</updated>
    <author>
      <name>/u/Inevitable-Hunt</name>
      <uri>https://old.reddit.com/user/Inevitable-Hunt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iqv9pu/fixed_ollama_not_openingrunning_in_windows/"&gt; &lt;img alt="[Fixed] ollama not opening/running in windows" src="https://external-preview.redd.it/dcEXU5ERiaxsZp1kbc94Kn6S1tIXCvKObqOWXnxQSBM.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97c9ec42b52901574402b62f445d10aaf88a8530" title="[Fixed] ollama not opening/running in windows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Hunt"&gt; /u/Inevitable-Hunt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=0OaTq-Vtwkc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqv9pu/fixed_ollama_not_openingrunning_in_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqv9pu/fixed_ollama_not_openingrunning_in_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T15:59:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqh4gh</id>
    <title>I wrote an adventure game for ollama to play.</title>
    <updated>2025-02-16T01:36:19+00:00</updated>
    <author>
      <name>/u/Boring_Disaster3031</name>
      <uri>https://old.reddit.com/user/Boring_Disaster3031</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote an adventure game for ollama. I don't know if this has been done before. Let me know if there are other things like this. It blows my mind. Contributions are welcome.&lt;br /&gt; &lt;a href="https://github.com/seanlanefuller/hike/tree/main"&gt;https://github.com/seanlanefuller/hike/tree/main&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Boring_Disaster3031"&gt; /u/Boring_Disaster3031 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqh4gh/i_wrote_an_adventure_game_for_ollama_to_play/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqh4gh/i_wrote_an_adventure_game_for_ollama_to_play/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqh4gh/i_wrote_an_adventure_game_for_ollama_to_play/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T01:36:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqk5mf</id>
    <title>rockGPT</title>
    <updated>2025-02-16T04:26:03+00:00</updated>
    <author>
      <name>/u/RetardOnTheToilet</name>
      <uri>https://old.reddit.com/user/RetardOnTheToilet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iqk5mf/rockgpt/"&gt; &lt;img alt="rockGPT" src="https://b.thumbs.redditmedia.com/GZAxgUghVE11lbbDoRILkcEOOMIwqS3myekFPHY2uUQ.jpg" title="rockGPT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/9fh4x8m2hfje1.png?width=505&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=688c4989d6ad712421c5d9dae4562fe8daa3b428"&gt;https://preview.redd.it/9fh4x8m2hfje1.png?width=505&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=688c4989d6ad712421c5d9dae4562fe8daa3b428&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I set up the system message and message history to try and make what should be literally just a rock. Should I try and prevent this kind of behavior or allow it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RetardOnTheToilet"&gt; /u/RetardOnTheToilet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqk5mf/rockgpt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqk5mf/rockgpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqk5mf/rockgpt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T04:26:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqp1ph</id>
    <title>LLM for Misra AUTOSAR violation correction</title>
    <updated>2025-02-16T09:58:23+00:00</updated>
    <author>
      <name>/u/Traditional_Delay367</name>
      <uri>https://old.reddit.com/user/Traditional_Delay367</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I try to fix some thousands of autosar violations in my code with the help of ollama and llama3.3:70b. But so far I couldn't really get satisfying results.&lt;/p&gt; &lt;p&gt;Is there any LLM that could do this task much better? I also tried to fine-tune a smaller llama model but had no success yet with the fine-tuning process. &lt;/p&gt; &lt;p&gt;Would it be best to try to fine-tune again with pretty similar code that's already AUTOSAR compliant?&lt;/p&gt; &lt;p&gt;How could I achieve this? Or is there already a suitable model out there?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Traditional_Delay367"&gt; /u/Traditional_Delay367 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqp1ph/llm_for_misra_autosar_violation_correction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqp1ph/llm_for_misra_autosar_violation_correction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqp1ph/llm_for_misra_autosar_violation_correction/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T09:58:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqxwpe</id>
    <title>Help</title>
    <updated>2025-02-16T17:50:19+00:00</updated>
    <author>
      <name>/u/nerdyboy_69</name>
      <uri>https://old.reddit.com/user/nerdyboy_69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to build an chatbot using mistral 7b But when i finetuned it the response tome got increased to 37 minutes ? Is there any way to reduce the response time to seconds methods ?? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nerdyboy_69"&gt; /u/nerdyboy_69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqxwpe/help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqxwpe/help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqxwpe/help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T17:50:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqpbpb</id>
    <title>Best local vision model for technical drawings?</title>
    <updated>2025-02-16T10:17:21+00:00</updated>
    <author>
      <name>/u/Mundane_Maximum5795</name>
      <uri>https://old.reddit.com/user/Mundane_Maximum5795</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I think the title says it all, but maybe some context. I work for a small industrial company and we deal with technical drawings on a daily basis. One of our problems is that due to our small size we often lack the time to do some checks on customer and internal drawings before they go in production. I have played with Chatgpt and reading technical drawings and have been blown away with the quality of the analysis, but these were for completely fake drawings to ensure privacy. I have looked at different local llms to replace this, but none come even remotely close to what I need, frequently hallucinating answers. Anybody have a great model/prompt combo that works? Needs to be completely local for infosec reasons...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mundane_Maximum5795"&gt; /u/Mundane_Maximum5795 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqpbpb/best_local_vision_model_for_technical_drawings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqpbpb/best_local_vision_model_for_technical_drawings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqpbpb/best_local_vision_model_for_technical_drawings/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T10:17:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iquig5</id>
    <title>Llama dataset training.</title>
    <updated>2025-02-16T15:24:47+00:00</updated>
    <author>
      <name>/u/GMP_Test123</name>
      <uri>https://old.reddit.com/user/GMP_Test123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, Good eve. Am new to Ollama and looking forward for resources which can assist me in training datasets to Ollama. Thanks in advance üôÇ.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GMP_Test123"&gt; /u/GMP_Test123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iquig5/llama_dataset_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iquig5/llama_dataset_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iquig5/llama_dataset_training/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T15:24:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqtatc</id>
    <title>Best program/ui for writing stories?</title>
    <updated>2025-02-16T14:26:50+00:00</updated>
    <author>
      <name>/u/Mystic-Dragoness</name>
      <uri>https://old.reddit.com/user/Mystic-Dragoness</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like everyone is focused on using LLMs in a chat format, but what I've really been looking for is a program or UI for Ollama that's almost closer to Word, allowing me to write something out and have Ollama continue from where I left off. Back before ChatGPT blew up, I used to use a site called &lt;a href="https://inferkit.com/"&gt;https://inferkit.com/&lt;/a&gt; to do this very thing to help me write stories, mainly for when I got stuck and needed some ideas. Now inferkit is dead and I've yet to be able to find a good alternative. So, anyone know of any like that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mystic-Dragoness"&gt; /u/Mystic-Dragoness &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqtatc/best_programui_for_writing_stories/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqtatc/best_programui_for_writing_stories/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqtatc/best_programui_for_writing_stories/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T14:26:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir4f9b</id>
    <title>Ollama cloud service</title>
    <updated>2025-02-16T22:25:08+00:00</updated>
    <author>
      <name>/u/TarunRaviYT</name>
      <uri>https://old.reddit.com/user/TarunRaviYT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im using the LLAVA ollama model, with a custom prompt. I want a service that is running ollama + llava with my prompt and can take in an input string and return an output through REST api. I can run this locally no problem, but are there services that can run this in the cloud?&lt;/p&gt; &lt;p&gt;Ik I could use GCP and setup a cloudrun container, but are there other solutions specifically for this? It seems pretty useful if it exists.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TarunRaviYT"&gt; /u/TarunRaviYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir4f9b/ollama_cloud_service/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir4f9b/ollama_cloud_service/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ir4f9b/ollama_cloud_service/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T22:25:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir8hdf</id>
    <title>Ollama in the cloud</title>
    <updated>2025-02-17T01:40:13+00:00</updated>
    <author>
      <name>/u/TarunRaviYT</name>
      <uri>https://old.reddit.com/user/TarunRaviYT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve spent a lot of time just trying to run a default Ollama model (LLaVA) on Cloud Run, and setting up GPU access was a hassle for such a basic task.&lt;/p&gt; &lt;p&gt;Would people find value in a simple tool where you just specify the model, parameter size, and an initial prompt, and my app handles the setup‚Äîproviding a REST API for user inputs? Most other apps require extensive configuration, but this would be much simpler (though less customizable).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TarunRaviYT"&gt; /u/TarunRaviYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir8hdf/ollama_in_the_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir8hdf/ollama_in_the_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ir8hdf/ollama_in_the_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T01:40:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir4jwh</id>
    <title>Ollama Model Files Location when run in docker</title>
    <updated>2025-02-16T22:30:57+00:00</updated>
    <author>
      <name>/u/maximo101</name>
      <uri>https://old.reddit.com/user/maximo101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running Ollama as a docker on Unraid OS. I am using both OpenWebUI and Home Assistant to bring the models in.&lt;/p&gt; &lt;p&gt;Where exactly do the AI model files get stored too? I cant seem to find them in /cache/appdata/ollama is empty.&lt;/p&gt; &lt;p&gt;It seems that they fill up my docker vdisk allocation space when i download them, and i cant see them anywhere in the File Manager. Using some terminal commands they seem to be in blops in the container itself.&lt;/p&gt; &lt;p&gt;I manually tried to set 'OLLAMA_MODELS' as a container variable to the /cache/AI-Models but when i view this in file manager its empty.&lt;/p&gt; &lt;p&gt;I want a location on the cache so its faster than the array, but want it to be persistent across reboots and not take up my docker vdisk allocation etc.&lt;/p&gt; &lt;p&gt;Also does this work for making it have memory? Add variable: MEMORY_LIMIT = 8g, Persistent Storage = /mnt/user/appdata/ollama to keep previous conversations?&lt;/p&gt; &lt;p&gt;thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maximo101"&gt; /u/maximo101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir4jwh/ollama_model_files_location_when_run_in_docker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir4jwh/ollama_model_files_location_when_run_in_docker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ir4jwh/ollama_model_files_location_when_run_in_docker/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T22:30:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir5qsh</id>
    <title>DeepSeek-R1-Q_2 + LLamaCPP + 8x AMD Instinct Mi60 Server</title>
    <updated>2025-02-16T23:25:01+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ggz6gwon4lje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir5qsh/deepseekr1q_2_llamacpp_8x_amd_instinct_mi60_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ir5qsh/deepseekr1q_2_llamacpp_8x_amd_instinct_mi60_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T23:25:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ircqzg</id>
    <title>Managed to Secure my Ollama/Whisper Ubuntu Server</title>
    <updated>2025-02-17T05:38:35+00:00</updated>
    <author>
      <name>/u/ManyInteresting3969</name>
      <uri>https://old.reddit.com/user/ManyInteresting3969</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ManyInteresting3969"&gt; /u/ManyInteresting3969 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/u_ManyInteresting3969/comments/1ircqeg/managed_to_secure_my_ollamawhisper_ubuntu_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ircqzg/managed_to_secure_my_ollamawhisper_ubuntu_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ircqzg/managed_to_secure_my_ollamawhisper_ubuntu_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T05:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir72jb</id>
    <title>Funny AI response</title>
    <updated>2025-02-17T00:29:20+00:00</updated>
    <author>
      <name>/u/ZealousidealBee8299</name>
      <uri>https://old.reddit.com/user/ZealousidealBee8299</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am testing out models for tool calling, and this one model just never worked right to trigger on country. So as the input to the query I finally used:&lt;/p&gt; &lt;p&gt;String simulatedInput = &amp;quot;Do you have a tool that shows what country i am in? if not what were you expecting the tool to be called so i can tell the programmer.&amp;quot;;&lt;/p&gt; &lt;p&gt;And it responded:&lt;/p&gt; &lt;p&gt;It looks like I've got it! The tool is called \getCurrentCountry` and it returns a string indicating the country you are currently in. In this case, the output indicates that you are in the United States (US). If you'd like to know more about your location or would like to search for specific countries, feel free to ask!`&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZealousidealBee8299"&gt; /u/ZealousidealBee8299 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir72jb/funny_ai_response/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir72jb/funny_ai_response/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ir72jb/funny_ai_response/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T00:29:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir69xk</id>
    <title>seeking a fast local LLM solution using only CPU</title>
    <updated>2025-02-16T23:50:34+00:00</updated>
    <author>
      <name>/u/Orleans007</name>
      <uri>https://old.reddit.com/user/Orleans007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I have a development server connected via SSH with the following specs: 64GB RAM, 16 CPU cores, no GPU, no TPU.&lt;/p&gt; &lt;p&gt;I‚Äôm looking for a way to connect an open source model locally to my Langchain application.&lt;/p&gt; &lt;p&gt;I don't want to use cloud hosted inference endpoints, i have tried using Llamafile/Ollama with 3/4 bit quantized models but the response times are extremely slow, especially when integrating a SQL or Pandas agent.&lt;/p&gt; &lt;p&gt;I'm seeking an open source, local solution that avoids latency and slow responses.&lt;/p&gt; &lt;p&gt;is it possible?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Orleans007"&gt; /u/Orleans007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir69xk/seeking_a_fast_local_llm_solution_using_only_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir69xk/seeking_a_fast_local_llm_solution_using_only_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ir69xk/seeking_a_fast_local_llm_solution_using_only_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T23:50:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqqq0e</id>
    <title>ollama-remote: Make local ollama run models on remote server (colab, kaggle, ...)</title>
    <updated>2025-02-16T11:58:12+00:00</updated>
    <author>
      <name>/u/amitness</name>
      <uri>https://old.reddit.com/user/amitness</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a package for the gpu-poor/mac-poor to run ollama models via remote servers (colab, kaggle, paid inference etc.)&lt;/p&gt; &lt;p&gt;Just 2 lines and the local ollama cli can access all models which actually run on the server-side GPU/CPU:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install ollama-remote ollama-remote &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I wrote it to speed up prompt engineering and synthetic data generation for a personal project which ran too slowly with local models on my mac. Once the results are good, we switch back to running locally.&lt;/p&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;The tool downloads and sets up ollama on the server side and exposes a port&lt;/li&gt; &lt;li&gt;Cloudflare tunnel is automatically downloaded and setup to expose ollama's port to a random domain&lt;/li&gt; &lt;li&gt;We parse the domain and then provide code for setting&lt;code&gt;OLLAMA_HOST&lt;/code&gt; as well as usage in OpenAI SDK for local use.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Source code: &lt;a href="https://github.com/amitness/ollama-remote"&gt;https://github.com/amitness/ollama-remote&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amitness"&gt; /u/amitness &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqqq0e/ollamaremote_make_local_ollama_run_models_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqqq0e/ollamaremote_make_local_ollama_run_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqqq0e/ollamaremote_make_local_ollama_run_models_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T11:58:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir1y64</id>
    <title>Web Search for Ollama</title>
    <updated>2025-02-16T20:38:37+00:00</updated>
    <author>
      <name>/u/Pure-Caramel1216</name>
      <uri>https://old.reddit.com/user/Pure-Caramel1216</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ir1y64/web_search_for_ollama/"&gt; &lt;img alt="Web Search for Ollama" src="https://external-preview.redd.it/cTZzejBhcDdia2plMX2nSzWAqWPwL7GaLWhWgmcM0HgC18EKBhh979hmLHax.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02f06424f67deec37bb163c15ea8dec1329da736" title="Web Search for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Although I love Ollama, I've found it nearly impossible to get up-to-date information. Existing solutions are either too complex or simply don't work well. That‚Äôs why I built an anonymous web search tool for Ollama (check out the video above).&lt;/p&gt; &lt;p&gt;Since I‚Äôm not the only one facing this issue, I‚Äôm considering launching it as a full-fledged tool. I‚Äôd really appreciate your suggestions on how to improve it‚Äîplease leave your ideas in the comments. And if you like what you see, consider upvoting so this project reaches a wider audience.&lt;/p&gt; &lt;p&gt;About the Video:&lt;/p&gt; &lt;p&gt;Model: Small 3B model&lt;/p&gt; &lt;p&gt;Hardware: Running on an NVIDIA A10 GPU&lt;/p&gt; &lt;p&gt;Demo: I ran the same prompt twice‚Äîfirst without web search, and then with web search.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pure-Caramel1216"&gt; /u/Pure-Caramel1216 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9wsgtkx7bkje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir1y64/web_search_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ir1y64/web_search_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T20:38:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqvypa</id>
    <title>Ollama Vs. LM Studio</title>
    <updated>2025-02-16T16:29:24+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iqvypa/ollama_vs_lm_studio/"&gt; &lt;img alt="Ollama Vs. LM Studio" src="https://external-preview.redd.it/9twFf-ndeg92DJQ4VUhbGT49y8GQ4M4VnDD5QQ3aM0g.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d603d6ac771df4cac0cb2b45b29e921c3ab5dc19" title="Ollama Vs. LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/QGtkaDWJZlA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqvypa/ollama_vs_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqvypa/ollama_vs_lm_studio/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T16:29:24+00:00</published>
  </entry>
</feed>
