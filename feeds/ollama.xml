<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-04T21:34:18+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1j2hw55</id>
    <title>I just downloaded cuda. How should I now be able to give ollama access to the power of my gpu?</title>
    <updated>2025-03-03T12:47:47+00:00</updated>
    <author>
      <name>/u/Dalar42</name>
      <uri>https://old.reddit.com/user/Dalar42</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dalar42"&gt; /u/Dalar42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2hw55/i_just_downloaded_cuda_how_should_i_now_be_able/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2hw55/i_just_downloaded_cuda_how_should_i_now_be_able/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2hw55/i_just_downloaded_cuda_how_should_i_now_be_able/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T12:47:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2i2bv</id>
    <title>Best open source models to try out different RAG techniques.</title>
    <updated>2025-03-03T12:57:14+00:00</updated>
    <author>
      <name>/u/Superb_Practice_4544</name>
      <uri>https://old.reddit.com/user/Superb_Practice_4544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hii all, hope you guys are doing great. Recently I am learning about RAGs and want to try out different RAG techniques and their differences.&lt;/p&gt; &lt;p&gt;Which 7b parameter model works best for RAG use case ? Also need suggestion on best Open source embedding models. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Superb_Practice_4544"&gt; /u/Superb_Practice_4544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2i2bv/best_open_source_models_to_try_out_different_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2i2bv/best_open_source_models_to_try_out_different_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2i2bv/best_open_source_models_to_try_out_different_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T12:57:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2rfky</id>
    <title>Newbie Question - Ollama with Open-Webui and deepscaler / deepseek r1</title>
    <updated>2025-03-03T19:42:44+00:00</updated>
    <author>
      <name>/u/Lipora</name>
      <uri>https://old.reddit.com/user/Lipora</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running the above and running into some interesting issues.&lt;/p&gt; &lt;p&gt;I need some help understanding where my problem actually exists. Uploading some Word Documents as part of my query to the LLM and wanting it to combine and use the best information from all the documents to create essentially a distilled version of the information that aligns with the question being asked. Think of the example: Here are a bunch of my old resumes. Help me take the information from these resumes and compile them into a resume I can use to apply on the following position... And then listing all the details of the job posting. Deekseek R1 seems to be able to read &amp;quot;parts of the documents&amp;quot; and provide a reasonable response, but other models don't even seem to be able to open the documents or understand what is in them. Is this a tool that's needed to be added to Open-WebUI to assist with taking the uploaded content and getting it into a format that the LLM can understand? or the LLM itself? or some addition to Ollama that is needed? I guess I'm just trying to truly understand how the three tools, ollama, the LLM models themselves and Open-WebUI work together.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lipora"&gt; /u/Lipora &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2rfky/newbie_question_ollama_with_openwebui_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2rfky/newbie_question_ollama_with_openwebui_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2rfky/newbie_question_ollama_with_openwebui_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T19:42:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j26bbr</id>
    <title>Impressed with how well Ollama runs on the RasPi, this is Granite3.1 MoE</title>
    <updated>2025-03-03T00:45:13+00:00</updated>
    <author>
      <name>/u/RasPiBuilder</name>
      <uri>https://old.reddit.com/user/RasPiBuilder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j26bbr/impressed_with_how_well_ollama_runs_on_the_raspi/"&gt; &lt;img alt="Impressed with how well Ollama runs on the RasPi, this is Granite3.1 MoE" src="https://preview.redd.it/u1578jgzfdme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70291d2ca9084c471397d78898f453ae38d57293" title="Impressed with how well Ollama runs on the RasPi, this is Granite3.1 MoE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RasPiBuilder"&gt; /u/RasPiBuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u1578jgzfdme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j26bbr/impressed_with_how_well_ollama_runs_on_the_raspi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j26bbr/impressed_with_how_well_ollama_runs_on_the_raspi/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T00:45:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2nyut</id>
    <title>Toolmaker ‚Äì A Tool SDK to Standardize AI Agent Capabilities</title>
    <updated>2025-03-03T17:23:13+00:00</updated>
    <author>
      <name>/u/suvsuvsuv</name>
      <uri>https://old.reddit.com/user/suvsuvsuv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI agents are powerful, but building tools for them is still chaotic.&lt;/p&gt; &lt;p&gt;We built Toolmaker, an SDK that provides a structured, scalable way to create and manage AI agent tools.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/suvsuvsuv"&gt; /u/suvsuvsuv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://docs.try-synaptic.ai/atm/toolmaker"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2nyut/toolmaker_a_tool_sdk_to_standardize_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2nyut/toolmaker_a_tool_sdk_to_standardize_ai_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T17:23:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2udva</id>
    <title>Get Started Easily with LangchainJS and Ollama</title>
    <updated>2025-03-03T21:45:22+00:00</updated>
    <author>
      <name>/u/Inevitable-Judge2642</name>
      <uri>https://old.reddit.com/user/Inevitable-Judge2642</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j2udva/get_started_easily_with_langchainjs_and_ollama/"&gt; &lt;img alt="Get Started Easily with LangchainJS and Ollama" src="https://external-preview.redd.it/dSL_MDQNqcxsEs3DXiQe3svJ7ZmGtARuqD1bteUJo8c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7046a30c4ff8975305969bdd0d4420225b41c3c" title="Get Started Easily with LangchainJS and Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Judge2642"&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://k33g.hashnode.dev/get-started-easily-with-langchainjs-and-ollama"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2udva/get_started_easily_with_langchainjs_and_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2udva/get_started_easily_with_langchainjs_and_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T21:45:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2v629</id>
    <title>Python libs for local LLM + use cases</title>
    <updated>2025-03-03T22:18:47+00:00</updated>
    <author>
      <name>/u/jshre</name>
      <uri>https://old.reddit.com/user/jshre</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New to local LLM but with some background in machine learning (good old Scikit Learn). Does anyone have pointers for powerful python libraries / tools to use with Ollama server? What could be practical use cases?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jshre"&gt; /u/jshre &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2v629/python_libs_for_local_llm_use_cases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2v629/python_libs_for_local_llm_use_cases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2v629/python_libs_for_local_llm_use_cases/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T22:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j324wb</id>
    <title>Proper local llm</title>
    <updated>2025-03-04T04:00:47+00:00</updated>
    <author>
      <name>/u/Daedric800</name>
      <uri>https://old.reddit.com/user/Daedric800</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys i need help to find local light weight Ilm that is specified and fine-tuned just for the task of coding, which means a model that is trained only for coding and nothing else which make it very light weight and small in size since it does not do chat, math, etc.. which makes it small in size yet powerful in coding like claude or deepseek models, i cant see why i havent came across a model like that yet, why are not people making a specific coding models, we are at 2025, so please if you have a model with these specs please do tell me, so i could use it for a proper coding tasks on my low end gpu locally or maybe someonw of you guys could train a simple unsloth model for just coding and upload it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daedric800"&gt; /u/Daedric800 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j324wb/proper_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j324wb/proper_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j324wb/proper_local_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T04:00:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j32eir</id>
    <title>Need help with an error message</title>
    <updated>2025-03-04T04:13:55+00:00</updated>
    <author>
      <name>/u/First_Handle_7722</name>
      <uri>https://old.reddit.com/user/First_Handle_7722</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm getting an error ‚Äúmodel requires more system memory (747.4 MiB) than is available (694.8 MiB)‚Äù how can I fix it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/First_Handle_7722"&gt; /u/First_Handle_7722 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j32eir/need_help_with_an_error_message/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j32eir/need_help_with_an_error_message/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j32eir/need_help_with_an_error_message/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T04:13:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3550x</id>
    <title>Exam Questions Ai Saas</title>
    <updated>2025-03-04T07:04:55+00:00</updated>
    <author>
      <name>/u/Dry_Ingenuity_8009</name>
      <uri>https://old.reddit.com/user/Dry_Ingenuity_8009</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I working on make Saas that help teachers to put exam questions for high school In Egypt I have noticed that this process take from teachers a lot of time and money so the system is like this I ask the ai for like 100 low-level,mid-level or high-level questions for lets say physics subject so it has to give me exactly the 100 question with the chosen level or a group of levels so I want also this to be generated question not just retrieved from the knowledge base to prevent any copyright issues so what is the best technique to achieve this using fine tune only or rag or both of them and if there any one have the right way to do it please tell me &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Ingenuity_8009"&gt; /u/Dry_Ingenuity_8009 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3550x/exam_questions_ai_saas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3550x/exam_questions_ai_saas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3550x/exam_questions_ai_saas/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T07:04:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2ibxv</id>
    <title>Chat with my own PDF documents</title>
    <updated>2025-03-03T13:11:14+00:00</updated>
    <author>
      <name>/u/9elpi8</name>
      <uri>https://old.reddit.com/user/9elpi8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, as title says I would like to chat with my PDF documents. Which model would you recommend me to use? Best would be with multilanguage support. I have Nvidia 4060Ti 16GB.&lt;/p&gt; &lt;p&gt;My idea is make several threads inside AnythingLLM where I would have my receipts in other thread books related to engineering or some other learning stuff.&lt;/p&gt; &lt;p&gt;Thank you for your recommendation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/9elpi8"&gt; /u/9elpi8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2ibxv/chat_with_my_own_pdf_documents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2ibxv/chat_with_my_own_pdf_documents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2ibxv/chat_with_my_own_pdf_documents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T13:11:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j36zmn</id>
    <title>tool calls, how?</title>
    <updated>2025-03-04T09:26:22+00:00</updated>
    <author>
      <name>/u/Expensive-Award1965</name>
      <uri>https://old.reddit.com/user/Expensive-Award1965</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i apologize for this not well thought out post. i'm just frustrated, perhaps because i don't understand python, but i think mostly i have no idea how the thing actually calls the tools? i don't understand how it knows to call the functions, does it just come across a bit of the prompt and think oh i need to call this function so i can get that information? &lt;/p&gt; &lt;p&gt;is there like a way to call php lol? does anyone have a tool call that will then call php that i can use? &lt;/p&gt; &lt;p&gt;like the example has an array of tools right, but where does it call those tools from? where's the `get_current_weather` functions at? how do i define it?&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import ollama response = ollama.chat( model='llama3.1', messages=[{'role': 'user', 'content': 'What is the weather in Toronto?'}], # provide a weather checking tool to the model tools=[{ 'type': 'function', 'function': { 'name': 'get_current_weather', 'description': 'Get the current weather for a city', 'parameters': { 'type': 'object', 'properties': { 'city': { 'type': 'string', 'description': 'The name of the city', }, }, 'required': ['city'], }, }, }, ], ) print(response['message']['tool_calls'])import ollama &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expensive-Award1965"&gt; /u/Expensive-Award1965 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j36zmn/tool_calls_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j36zmn/tool_calls_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j36zmn/tool_calls_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T09:26:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1j383tj</id>
    <title>Looking for a Local AI Model Manager with API Proxy &amp; Web Interface</title>
    <updated>2025-03-04T10:48:41+00:00</updated>
    <author>
      <name>/u/Niutaokkul</name>
      <uri>https://old.reddit.com/user/Niutaokkul</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm looking for a self-hosted solution to manage AI models and monitor API usage, ideally with a web interface for easy administration.&lt;/p&gt; &lt;h1&gt;My needs:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I have an &lt;strong&gt;OpenAI API key&lt;/strong&gt; provided by my company, but I &lt;strong&gt;don't have access to usage stats&lt;/strong&gt; (requests made, tokens consumed).&lt;/li&gt; &lt;li&gt;I also want to &lt;strong&gt;run smaller local models&lt;/strong&gt; (like Ollama) for certain tasks without always relying on OpenAI.&lt;/li&gt; &lt;li&gt;Ideally, the platform should: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Host and serve local models&lt;/strong&gt; (e.g., Ollama)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Act as a proxy/API gateway&lt;/strong&gt; for OpenAI keys&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Log and track API usage&lt;/strong&gt; (requests, token counts, etc.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Provide a web interface&lt;/strong&gt; to monitor activity and manage models easily&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I came across &lt;strong&gt;AI-Server by ServiceStack&lt;/strong&gt;, but it seems more like a client for interacting with models rather than a full-fledged management solution.&lt;/p&gt; &lt;p&gt;Is there any &lt;strong&gt;open-source or self-hosted&lt;/strong&gt; tool that fits these needs?&lt;/p&gt; &lt;p&gt;Thanks in advance for any recommendations!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Niutaokkul"&gt; /u/Niutaokkul &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j383tj/looking_for_a_local_ai_model_manager_with_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j383tj/looking_for_a_local_ai_model_manager_with_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j383tj/looking_for_a_local_ai_model_manager_with_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T10:48:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3d5uj</id>
    <title>How to setup local Hosted AI API for coded project?</title>
    <updated>2025-03-04T15:22:04+00:00</updated>
    <author>
      <name>/u/Shot-Negotiation5968</name>
      <uri>https://old.reddit.com/user/Shot-Negotiation5968</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have coded a project (AI Chat) in html and I installed Ollama llama2 locally. I want to request the AI with API on my coded project, Could you please help me how to do that? I found nothing on Youtube for this certain case Thank you &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shot-Negotiation5968"&gt; /u/Shot-Negotiation5968 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3d5uj/how_to_setup_local_hosted_ai_api_for_coded_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3d5uj/how_to_setup_local_hosted_ai_api_for_coded_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3d5uj/how_to_setup_local_hosted_ai_api_for_coded_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T15:22:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1j30893</id>
    <title>Qwen2.5 32b will start to put the tool calls in the content instead of the tool_calls</title>
    <updated>2025-03-04T02:20:23+00:00</updated>
    <author>
      <name>/u/nstevnc77</name>
      <uri>https://old.reddit.com/user/nstevnc77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I've been building a small application with Ollama for personal use that involves tool calling. I've been really impressed with Qwen2.5's ability to figure out when to do tool calls, which tools to use, and its overall reliability.&lt;/p&gt; &lt;p&gt;The only problem I've been running into is that Qwen2.5 will start putting its tool calls (JSON) in the content instead of the proper tool_calls part of the JSON. This is frustrating because it works so well otherwise.&lt;/p&gt; &lt;p&gt;It always seems to get the tool calls correct in the beginning, but about 20-40 messages in, it just starts putting the JSON in the content. Has anyone found a solution to this issue? I'm thinking that maybe because I'm saving those tool call messages in its list of messages or I'm adding &amp;quot;toolresult&amp;quot; responses that maybe it's getting confused?&lt;/p&gt; &lt;p&gt;Just wanted to see if anybody has had a similar experience!&lt;/p&gt; &lt;p&gt;Edit: I've tried llama models but they will ALWAYS call tools given the chance. Not very useful for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nstevnc77"&gt; /u/nstevnc77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j30893/qwen25_32b_will_start_to_put_the_tool_calls_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j30893/qwen25_32b_will_start_to_put_the_tool_calls_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j30893/qwen25_32b_will_start_to_put_the_tool_calls_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T02:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j39jx2</id>
    <title>Nvidia and AMD graphics card at the same time in ollama?</title>
    <updated>2025-03-04T12:22:32+00:00</updated>
    <author>
      <name>/u/Other_Button_3775</name>
      <uri>https://old.reddit.com/user/Other_Button_3775</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently running Ollama on an Ubuntu system with a Nvidia 3060ti and an AMD ROG RX580. I'm trying to set it up so that Ollama uses the 3060ti primarily and falls back to the RX580 if needed.&lt;/p&gt; &lt;p&gt;Has anyone had experience with this kind of setup? Is it even possible? Are there any specific configurations or settings I should be aware of to make sure both GPUs are utilized effectively?&lt;/p&gt; &lt;p&gt;Any help or insights would be greatly appreciated! Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other_Button_3775"&gt; /u/Other_Button_3775 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j39jx2/nvidia_and_amd_graphics_card_at_the_same_time_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j39jx2/nvidia_and_amd_graphics_card_at_the_same_time_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j39jx2/nvidia_and_amd_graphics_card_at_the_same_time_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T12:22:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3hyxo</id>
    <title>Why we can't run vlm in ollama ? I want to run qwen2.5 VL 3b locally with ollama</title>
    <updated>2025-03-04T18:38:22+00:00</updated>
    <author>
      <name>/u/Kuggy1105</name>
      <uri>https://old.reddit.com/user/Kuggy1105</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kuggy1105"&gt; /u/Kuggy1105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3hyxo/why_we_cant_run_vlm_in_ollama_i_want_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3hyxo/why_we_cant_run_vlm_in_ollama_i_want_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3hyxo/why_we_cant_run_vlm_in_ollama_i_want_to_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T18:38:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1j35s3m</id>
    <title>The best small tools calls llm</title>
    <updated>2025-03-04T07:52:32+00:00</updated>
    <author>
      <name>/u/Ok-Masterpiece-0000</name>
      <uri>https://old.reddit.com/user/Ok-Masterpiece-0000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please people, I would like some help. I want to get the small open source llm like qwen2.5:3b or Mistral or some other to produce a correct tools, and even now to call the tools when they are available. HELP I tried everything but 00. Only big LLM like OpenAI one and other ‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Masterpiece-0000"&gt; /u/Ok-Masterpiece-0000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j35s3m/the_best_small_tools_calls_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j35s3m/the_best_small_tools_calls_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j35s3m/the_best_small_tools_calls_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T07:52:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3e54w</id>
    <title>Hardware recommendations</title>
    <updated>2025-03-04T16:04:17+00:00</updated>
    <author>
      <name>/u/Squirrel_daddy</name>
      <uri>https://old.reddit.com/user/Squirrel_daddy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working on a proof of concept AI rag application for a client. I have a budget of $5-$10k for hardware to use as a development and R&amp;amp;D setup. Does anyone have any recommendations as to what they would look at. I would love to run the largest mistal model i can, and I'm not concerned with hd storage in my budget number. Also I'm not opposed to used hardware nor am I really concerned with power efficiency. Just wanted peoples thoughts on best bang for buck options I may not of considered. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Squirrel_daddy"&gt; /u/Squirrel_daddy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3e54w/hardware_recommendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3e54w/hardware_recommendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3e54w/hardware_recommendations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T16:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2j8nt</id>
    <title>I open-sourced Klee today, an Ollama GUI designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities.</title>
    <updated>2025-03-03T13:58:46+00:00</updated>
    <author>
      <name>/u/w-zhong</name>
      <uri>https://old.reddit.com/user/w-zhong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j2j8nt/i_opensourced_klee_today_an_ollama_gui_designed/"&gt; &lt;img alt="I open-sourced Klee today, an Ollama GUI designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." src="https://preview.redd.it/e9n94wxjdhme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cea4c13efc5bd2a090237ef7c3fe7065ceb8f0d9" title="I open-sourced Klee today, an Ollama GUI designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w-zhong"&gt; /u/w-zhong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e9n94wxjdhme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2j8nt/i_opensourced_klee_today_an_ollama_gui_designed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2j8nt/i_opensourced_klee_today_an_ollama_gui_designed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T13:58:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3ashf</id>
    <title>Ollama on Windows isn't using my RTX 3090</title>
    <updated>2025-03-04T13:30:25+00:00</updated>
    <author>
      <name>/u/XALC1</name>
      <uri>https://old.reddit.com/user/XALC1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;As the title says my Windows 11 install isn't using my GPU but the CPU. I'm up to date on Windows and NVIDIA drivers. I'm not using docker. Could anyone help me troubleshoot?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XALC1"&gt; /u/XALC1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3ashf/ollama_on_windows_isnt_using_my_rtx_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3ashf/ollama_on_windows_isnt_using_my_rtx_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3ashf/ollama_on_windows_isnt_using_my_rtx_3090/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T13:30:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j35t4t</id>
    <title>Recommendations for small but capable LLMs?</title>
    <updated>2025-03-04T07:54:48+00:00</updated>
    <author>
      <name>/u/Apart_Cause_6382</name>
      <uri>https://old.reddit.com/user/Apart_Cause_6382</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From what i understand, the smaller the number of parameters is, the faster the model is and the smaller is it's filesize, but the smaller amount of knowledge it has&lt;/p&gt; &lt;p&gt;I am searching for a very fast yet knowledgeful LLM, any recommendations? Thank you in advance for any comments&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apart_Cause_6382"&gt; /u/Apart_Cause_6382 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j35t4t/recommendations_for_small_but_capable_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j35t4t/recommendations_for_small_but_capable_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j35t4t/recommendations_for_small_but_capable_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T07:54:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3g7i1</id>
    <title>GPU vs. CPU: Deepseek R1 Distill Qwen</title>
    <updated>2025-03-04T17:27:49+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j3g7i1/gpu_vs_cpu_deepseek_r1_distill_qwen/"&gt; &lt;img alt="GPU vs. CPU: Deepseek R1 Distill Qwen" src="https://external-preview.redd.it/Q5jlmvwaxhZCXswmxVeacfiYbEM6IsziXkzDlS6KY3g.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=464630f84bff9a55c18a79fd3c34d59aa503a988" title="GPU vs. CPU: Deepseek R1 Distill Qwen" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/teWusSZoQ-M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3g7i1/gpu_vs_cpu_deepseek_r1_distill_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3g7i1/gpu_vs_cpu_deepseek_r1_distill_qwen/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T17:27:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j38dim</id>
    <title>Tell me why NVIDIA isn't gatekeeping the future of AI for the wealthy with what ollama brings to every home of every family in the world.</title>
    <updated>2025-03-04T11:06:48+00:00</updated>
    <author>
      <name>/u/Low_Tune7301</name>
      <uri>https://old.reddit.com/user/Low_Tune7301</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR; 4090 shouldn't have been sunsetted, prices out a demographic of the entire world who can't afford the insane prices already and the new products while they're in a gold rush making $113b a year, jesus christ just make it all at this point as if you wouldn't scale everything up and sell to everyone.&lt;/p&gt; &lt;p&gt;I'm big mad lads, tell me why I'm wrong - it's incredible what tools like Ollama can bring to every single home in the world. Every human deserves to experience this revolution in their homes, ollama can support it and I think they're gate-keeping AI for the wealthy.&lt;/p&gt; &lt;p&gt;So we're in a gold rush where this company is the sole real winner of a race where every single human, store and commercial retailer is sitting on the edge of their seats for you to literally not even give new products but existing ones.&lt;/p&gt; &lt;p&gt;You can also literally have your cake and eat it too, you are the worlds leading innovator and driver for AI, you can sell cluster stacks of 50,000 H100's to big tech buddies, and you have every other human in the world playing at home crawling the web relentlessly - not even just for your new products but your old ones as well and people will love you for it forever.&lt;/p&gt; &lt;p&gt;You will be cemented in history for leading the hardware innovation that brought this experience to every home in the world, you have successfully changed the future.&lt;/p&gt; &lt;p&gt;Every product with 24GB+ of VRAM sold out everywhere, every now product you release sold at the drop of a hat - all over the entire world. You've won. You are the winner of capitalism.&lt;/p&gt; &lt;p&gt;So what does the winner receive? - A hundred and thirteen billion dollars in revenue just for last year alone.&lt;/p&gt; &lt;p&gt;So while the world is discovering fire again, the winner:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Sunsets the RTX 4090 - an incredibly performant product that could bring that revolution into every family home in the world at a reasonable price point. Ooh on that note &amp;gt;&lt;/li&gt; &lt;li&gt;NVIDIA is somehow magically undersupplied year on year yet keeps bringing out a new card and the cycle repeats. If every performant product is sold out across the entire world and you're big dog Jensen what do those meetings look like? Are people telling you they can't build enough factories? Did they get confused now after the first god knows how many you've already made? After three years those meetings are still just like nah sorry can't get it right?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It makes a lot of sense though if you've got an agenda. It doesn't make sense you wouldn't scale up and go all in we're in an insane new age you're driving and the world wants everything you could possibly offer.&lt;/p&gt; &lt;p&gt;I'm not even saying don't sell or whatever like run at capitalism big chief you've won I'm just saying sell to both sides because instead you choose to keep repeating the pattern of under-supplying and gatekeeping AI for the wealthy which is disgusting with what's possible with Ollama and tools.&lt;/p&gt; &lt;p&gt;I deliberately use the word choose because once is an accident, twice is coincidence, three times is a pattern.&lt;/p&gt; &lt;p&gt;But go on, keep promising the next big thing while under-supplying and gatekeeping this revolution, price an entire demographic of the world out and just keep making H100's for those clusters your big tech buddies want.&lt;/p&gt; &lt;p&gt;Look I'm just saying sell to both sides. The world is discovering fire again and every human in the world deserves to experience that, you can still have your cake and eat it too. Make everything.&lt;/p&gt; &lt;p&gt;What you might say to this is people can still use the products the big tech companies create and connect to commercial models which you're right but something about the fact they just commercialized hitting the enter key to the lower socio-economic demographics in the world just doesn't feel right to me but hey I never liked the pokies anyway.&lt;/p&gt; &lt;p&gt;So go, there's my ollama rant - tell me why i'm cooked for thinking a company who made $113.26 billion in revenue last year and god knows how much the years before shouldn't have figured it out at this point and isn't just gatekeeping AI which is disgusting in a world with what tools combined with ollama make possible for every family in the world.&lt;/p&gt; &lt;p&gt;God damn I hope the open source community distilling these models for cheaper hardware godspeed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Low_Tune7301"&gt; /u/Low_Tune7301 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j38dim/tell_me_why_nvidia_isnt_gatekeeping_the_future_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j38dim/tell_me_why_nvidia_isnt_gatekeeping_the_future_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j38dim/tell_me_why_nvidia_isnt_gatekeeping_the_future_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T11:06:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3fh7d</id>
    <title>Ollama-OCR</title>
    <updated>2025-03-04T16:58:47+00:00</updated>
    <author>
      <name>/u/imanoop7</name>
      <uri>https://old.reddit.com/user/imanoop7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I open-sourced &lt;strong&gt;Ollama-OCR&lt;/strong&gt; ‚Äì an advanced &lt;strong&gt;OCR tool&lt;/strong&gt; powered by &lt;strong&gt;LLaVA 7B&lt;/strong&gt; and &lt;strong&gt;Llama 3.2 Vision&lt;/strong&gt; to extract text from images with high accuracy! üöÄ&lt;/p&gt; &lt;p&gt;üîπ &lt;strong&gt;Features:&lt;/strong&gt;&lt;br /&gt; ‚úÖ Supports &lt;strong&gt;Markdown, Plain Text, JSON, Structured, Key-Value Pairs&lt;/strong&gt;&lt;br /&gt; ‚úÖ &lt;strong&gt;Batch processing&lt;/strong&gt; for handling multiple images efficiently&lt;br /&gt; ‚úÖ Uses &lt;strong&gt;state-of-the-art vision-language models&lt;/strong&gt; for better OCR&lt;br /&gt; ‚úÖ Ideal for &lt;strong&gt;document digitization, data extraction, and automation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Check it out &amp;amp; contribute! üîó &lt;a href="https://github.com/imanoop7/Ollama-OCR"&gt;GitHub: Ollama-OCR&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Details about Python Package -&lt;a href="https://medium.com/@mauryaanoop3/ollama-ocr-now-available-as-a-python-package-ff5e4240eb26"&gt; &lt;strong&gt;Guide&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thoughts? Feedback? Let‚Äôs discuss! üî•&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/imanoop7"&gt; /u/imanoop7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3fh7d/ollamaocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3fh7d/ollamaocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3fh7d/ollamaocr/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T16:58:47+00:00</published>
  </entry>
</feed>
