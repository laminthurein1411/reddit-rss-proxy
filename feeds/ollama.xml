<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-05-06T06:25:39+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1kdfc3k</id>
    <title>I was confused at first about what model types mean, but this clarified it, I found 5-bit works the best on my system without sacrificing speed or accuracy. 16 bit works, but sluggish. If you're new to this...explanations of terminology in post.</title>
    <updated>2025-05-02T23:44:04+00:00</updated>
    <author>
      <name>/u/Sandalwoodincencebur</name>
      <uri>https://old.reddit.com/user/Sandalwoodincencebur</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kdfc3k/i_was_confused_at_first_about_what_model_types/"&gt; &lt;img alt="I was confused at first about what model types mean, but this clarified it, I found 5-bit works the best on my system without sacrificing speed or accuracy. 16 bit works, but sluggish. If you're new to this...explanations of terminology in post." src="https://preview.redd.it/fbv2s5a0fgye1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2149c48778b4bb213734368aa65fe75a9c1a5747" title="I was confused at first about what model types mean, but this clarified it, I found 5-bit works the best on my system without sacrificing speed or accuracy. 16 bit works, but sluggish. If you're new to this...explanations of terminology in post." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These are different versions (tags) of the &lt;strong&gt;Llama3.2&lt;/strong&gt; model, each optimized for specific use cases, sizes, and quantization levels. Here's a breakdown of what each part of the naming convention means:&lt;/p&gt; &lt;h1&gt;1. Model Size (1b, 3b)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;1b&lt;/code&gt;: A 1-billion-parameter version of the model (smaller, faster, less resource-intensive).&lt;/li&gt; &lt;li&gt;&lt;code&gt;3b&lt;/code&gt;: A 3-billion-parameter version (larger, more capable, but requires more RAM/VRAM).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. Model Type (text, instruct)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;text&lt;/code&gt;: A base model trained for general text generation (like autocompletion or story writing).&lt;/li&gt; &lt;li&gt;&lt;code&gt;instruct&lt;/code&gt;: Fine-tuned for instruction-following (better at following prompts like chatbots or assistants).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. Precision &amp;amp; Quantization (fp16, q2_K, q4_K_M, etc.)&lt;/h1&gt; &lt;p&gt;Quantization reduces model size by lowering numerical precision, trading off some accuracy for efficiency.&lt;/p&gt; &lt;h1&gt;Full Precision (No Quantization)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;fp16&lt;/code&gt;: Full 16-bit floating-point precision (highest quality, largest file size).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What q5_K_M What q5_K_M Specifically Means&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;q5&lt;/code&gt; ‚Üí 5-bit quantization &lt;ul&gt; &lt;li&gt;Weights stored in 5 bits (vs. 32 bits in &lt;code&gt;fp32&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Balances size and accuracy (better than &lt;code&gt;q4&lt;/code&gt;, smaller than &lt;code&gt;q6&lt;/code&gt;).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;_K&lt;/code&gt; ‚Üí &amp;quot;K-means&amp;quot; clustering &lt;ul&gt; &lt;li&gt;Groups similar weights together to minimize precision loss.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;_M&lt;/code&gt; ‚Üí &amp;quot;Middle&amp;quot; precision tier &lt;ul&gt; &lt;li&gt;Optimized for &lt;strong&gt;balanced&lt;/strong&gt; performance (other options: &lt;code&gt;_S&lt;/code&gt; for small, &lt;code&gt;_L&lt;/code&gt; for large).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sandalwoodincencebur"&gt; /u/Sandalwoodincencebur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fbv2s5a0fgye1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdfc3k/i_was_confused_at_first_about_what_model_types/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdfc3k/i_was_confused_at_first_about_what_model_types/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T23:44:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kekkkc</id>
    <title>Built a LinkedIn lead gen system with automation + AI scraped 300M profiles (painful but worth it)</title>
    <updated>2025-05-04T14:04:20+00:00</updated>
    <author>
      <name>/u/Dreamer_made</name>
      <uri>https://old.reddit.com/user/Dreamer_made</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been deep in the weeds of marketing automation and AI for over a year now. Recently wrapped up building a large-scale system that scraped and enriched over &lt;strong&gt;300 million LinkedIn leads&lt;/strong&gt;. It involved:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multiple Sales Navigator accounts&lt;/li&gt; &lt;li&gt;Rotating proxies + headless browser automation&lt;/li&gt; &lt;li&gt;Queue-based architecture to avoid bans&lt;/li&gt; &lt;li&gt;ChatGPT and DeepSeek used for enrichment and parsing&lt;/li&gt; &lt;li&gt;Custom JavaScript for data cleanup + deduplication&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;LinkedIn really doesn't make it easy (lots of anti-bot mechanisms), but with enough retries and tweaks, it started flowing. The data pipelines, retry queues, and proxy rotation logic were the toughest parts.&lt;/p&gt; &lt;p&gt; If you're into large-scale scraping, lead gen, or just curious how this stuff works under the hood, happy to chat.&lt;/p&gt; &lt;p&gt;I packaged everything into a cleaned database way cheaper than ZoomInfo/Apollo if anyone ever needs it. It‚Äôs up at Leadady .com, &lt;strong&gt;one-time payment&lt;/strong&gt;, no fluff.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dreamer_made"&gt; /u/Dreamer_made &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kekkkc/built_a_linkedin_lead_gen_system_with_automation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kekkkc/built_a_linkedin_lead_gen_system_with_automation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kekkkc/built_a_linkedin_lead_gen_system_with_automation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-04T14:04:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ke0etl</id>
    <title>kb-ai-bot: probably another bot scraping sites and replies to questions (i did this)</title>
    <updated>2025-05-03T19:00:23+00:00</updated>
    <author>
      <name>/u/dowmeister_trucky</name>
      <uri>https://old.reddit.com/user/dowmeister_trucky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;during the last week i've worked on creating a small project as playground for site scraping + knowledge retrieval + vectors embedding and LLM text generation.&lt;/p&gt; &lt;p&gt;Basically I did this because i wanted to learn on my skin about LLM and KB bots but also because i have a KB site for my application with about 100 articles. After evaluated different AI bots on the market (with crazy pricing), I wanted to investigate directly what i could build.&lt;/p&gt; &lt;p&gt;Source code is available here: &lt;a href="https://github.com/dowmeister/kb-ai-bot"&gt;https://github.com/dowmeister/kb-ai-bot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Scrape recursively a site with a pluggable Site Scraper identifying the site type and applying the correct extractor for each type (currently Echo KB, Wordpress, Mediawiki and a Generic one)&lt;/p&gt; &lt;p&gt;- Create embeddings via HuggingFace MiniLM&lt;/p&gt; &lt;p&gt;- Store embeddings in QDrant&lt;/p&gt; &lt;p&gt;- Use vector search for retrieving affordable and matching content&lt;/p&gt; &lt;p&gt;- The content retrieved is used to generate a Context and a Prompt for an AI LLM and getting a natural language reply&lt;/p&gt; &lt;p&gt;- Multiple AI providers supported: Ollama, OpenAI, Claude, Cloudflare AI&lt;/p&gt; &lt;p&gt;- CLI console for asking questions&lt;/p&gt; &lt;p&gt;- Discord Bot with slash commands and automatic detection of questions\help requests&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;While the site scraping and embedding process is quite easy, having good results from LLM is another story.&lt;/p&gt; &lt;p&gt;OpenAI and Claude are good enough, Ollama has alternate replies depending on the model used, Cloudflare AI seems like Ollama but some models are really bad. Not tested on Amazon Bedrock.&lt;/p&gt; &lt;p&gt;If i would use Ollama in production, naturally the problem would be: where host Ollama at a reasonable price?&lt;/p&gt; &lt;p&gt;I'm searching for suggestions, comments, hints.&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dowmeister_trucky"&gt; /u/dowmeister_trucky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ke0etl/kbaibot_probably_another_bot_scraping_sites_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ke0etl/kbaibot_probably_another_bot_scraping_sites_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ke0etl/kbaibot_probably_another_bot_scraping_sites_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-03T19:00:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdsv32</id>
    <title>The feature I hate the bug in Ollama</title>
    <updated>2025-05-03T13:22:45+00:00</updated>
    <author>
      <name>/u/Informal-Victory8655</name>
      <uri>https://old.reddit.com/user/Informal-Victory8655</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kdsv32/the_feature_i_hate_the_bug_in_ollama/"&gt; &lt;img alt="The feature I hate the bug in Ollama" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="The feature I hate the bug in Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The default ctx is 2048 even for the embeddings model loaded using langchain. I mean, the persons who don't deep dive into the things, can't see why they are not getting any good results by using an embeddings model that supports input sequence up to 8192. :/&lt;/p&gt; &lt;p&gt;I'm using &lt;a href="https://ollama.com/library/snowflake-arctic-embed2"&gt;snowflake-arctic-embed2&lt;/a&gt;, which supports 8192 length, but default set is 2048.&lt;/p&gt; &lt;p&gt;The reason I select &lt;a href="https://ollama.com/library/snowflake-arctic-embed2"&gt;snowflake-arctic-embed2&lt;/a&gt; is longer context length, so I can avoid chunking.&lt;/p&gt; &lt;p&gt;Its crucial to monitor and see every log of the application/model you are running, don't trust anything.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dcvnzbqpikye1.png?width=1131&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=83cc4017541e7d26c45093020136dab211f8f145"&gt;https://preview.redd.it/dcvnzbqpikye1.png?width=1131&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=83cc4017541e7d26c45093020136dab211f8f145&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Informal-Victory8655"&gt; /u/Informal-Victory8655 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdsv32/the_feature_i_hate_the_bug_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdsv32/the_feature_i_hate_the_bug_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdsv32/the_feature_i_hate_the_bug_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-03T13:22:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ke9vl8</id>
    <title>ollama question : I cannot get system " If asked anything unrelated, respond with: ‚ÄòI only answer questions related." working</title>
    <updated>2025-05-04T02:52:40+00:00</updated>
    <author>
      <name>/u/jlsilicon9</name>
      <uri>https://old.reddit.com/user/jlsilicon9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have seen directions to specify :&lt;/p&gt; &lt;p&gt;SYSTEM &amp;quot;&lt;br /&gt; Only answer questions related to programming.&lt;br /&gt; If asked anything unrelated, respond with: `I only answer questions related to programming.'&lt;br /&gt; &amp;quot;&lt;/p&gt; &lt;p&gt;But, this does not seem to work.&lt;/p&gt; &lt;p&gt;If you specify the above in the Model :&lt;br /&gt; Then ask: &amp;quot;Tell me about daffy&amp;quot;&lt;br /&gt; ... it just explains about the character named daffy.&lt;/p&gt; &lt;p&gt;What am I missing ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jlsilicon9"&gt; /u/jlsilicon9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ke9vl8/ollama_question_i_cannot_get_system_if_asked/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ke9vl8/ollama_question_i_cannot_get_system_if_asked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ke9vl8/ollama_question_i_cannot_get_system_if_asked/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-04T02:52:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdyaq7</id>
    <title>How to move on from Ollama?</title>
    <updated>2025-05-03T17:28:52+00:00</updated>
    <author>
      <name>/u/jerasu_</name>
      <uri>https://old.reddit.com/user/jerasu_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been having so many problems with Ollama like Gemma3 performing worse than Gemma2 and Ollama getting stuck on some LLM calls or I have to restart ollama server once a day because it stops working. I wanna start using vLLM or llama.cpp but I couldn't make it work.vLLMt gives me &amp;quot;out of memory&amp;quot; error even though I have enough vramandt I couldn't figure out why llama.cpp won't work well. It is too slow like 5x slower than Ollama for me. I use a Linux machine with 2x 4070 Ti Super how can I stop using Ollama and make these other programs work? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jerasu_"&gt; /u/jerasu_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdyaq7/how_to_move_on_from_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdyaq7/how_to_move_on_from_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdyaq7/how_to_move_on_from_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-03T17:28:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ked8x2</id>
    <title>Feeding tool output back to LLM</title>
    <updated>2025-05-04T06:25:17+00:00</updated>
    <author>
      <name>/u/Bradymodion</name>
      <uri>https://old.reddit.com/user/Bradymodion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;I'm trying to write a program that uses the tool calling API from ollama. There is plenty of information available on the way to inform the model about the tools and the format of the tool calls (the tool_calls array). All of this works. But: what do I do then? I want to return the tool call results back to the LLM. What is the proper format? An array as well? Or several messages, one for each called tool? If a tool gets called twice (didn't happen yet, but possible), how would I handle this? Greetings!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bradymodion"&gt; /u/Bradymodion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ked8x2/feeding_tool_output_back_to_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ked8x2/feeding_tool_output_back_to_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ked8x2/feeding_tool_output_back_to_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-04T06:25:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ken0os</id>
    <title>Trouble running Ollama with Intel Arc GPU ‚Äì BSOD with VIDEO_SCHEDULER_INTERNAL_ERROR</title>
    <updated>2025-05-04T15:51:37+00:00</updated>
    <author>
      <name>/u/Mr_B1rd</name>
      <uri>https://old.reddit.com/user/Mr_B1rd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm trying to run Ollama using my Intel Arc GPU because it has more VRAM than my Nvidia card. Here's my setup:&lt;/p&gt; &lt;p&gt;Dell PC with a - Nvidia GPU with 8 GB VRAM - Intel Arc A770 GPU with 16 GB VRAM&lt;/p&gt; &lt;p&gt;I wanted to use the Intel GPU for Ollama, so I tried using the IPEX (Intel Extension for PyTorch) version of Ollama. However, every time I try to load a model, I get a bluescreen with the stopcode: VIDEO_SCHEDULER_INTERNAL_ERROR.&lt;/p&gt; &lt;p&gt;Has anyone run into this issue or know how to fix it? I'd really appreciate any help or pointers!&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_B1rd"&gt; /u/Mr_B1rd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ken0os/trouble_running_ollama_with_intel_arc_gpu_bsod/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ken0os/trouble_running_ollama_with_intel_arc_gpu_bsod/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ken0os/trouble_running_ollama_with_intel_arc_gpu_bsod/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-04T15:51:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kevih4</id>
    <title>New User: Can I add attachments for analysis to Ollama with WebUI?</title>
    <updated>2025-05-04T21:54:11+00:00</updated>
    <author>
      <name>/u/IamAlotOfMe</name>
      <uri>https://old.reddit.com/user/IamAlotOfMe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently downloaded and installed and my language model seemed to be outdated and I can't get current information from them that might be a separate problem but I'm trying to understand is there a way that I can add any attachment such as Excel sheets or Pdfs so I can analyze trading results and financial analysis?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IamAlotOfMe"&gt; /u/IamAlotOfMe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kevih4/new_user_can_i_add_attachments_for_analysis_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kevih4/new_user_can_i_add_attachments_for_analysis_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kevih4/new_user_can_i_add_attachments_for_analysis_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-04T21:54:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1keyg9k</id>
    <title>LLM not following instructions</title>
    <updated>2025-05-05T00:13:51+00:00</updated>
    <author>
      <name>/u/buttered-toasst</name>
      <uri>https://old.reddit.com/user/buttered-toasst</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am building this chatbot that uses streamlit for frontend and python with postgres for the backend, I have a vector table in my db with fragments so I can use RAG. I am trying to give memory to the bot and I found this approach that doesn't use any lanchain memory stuff and is to use the LLM to view a chat history and reformulate the user question. Like this, question -&amp;gt; first LLM -&amp;gt; reformulated question -&amp;gt; embedding and retrieval of documents in the db -&amp;gt; second LLM -&amp;gt; answer. The problem I'm facing is that the first LLM answers the question and it's not supposed to do it. I can't find a solution and If anyone wants to give me a hand, I'd really appreciate it.&lt;/p&gt; &lt;p&gt;This is the code if anybody could help:&lt;/p&gt; &lt;p&gt;from sentence_transformers import SentenceTransformer from fragmentsDAO import FragmentDAO from langchain.prompts import PromptTemplate from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain_core.messages import AIMessage, HumanMessage from langchain_community.chat_models import ChatOllama from langchain.schema.output_parser import StrOutputParser&lt;/p&gt; &lt;p&gt;class ChatOllamabot: def &lt;strong&gt;init&lt;/strong&gt;(self): self.model = SentenceTransformer(&amp;quot;all-mpnet-base-v2&amp;quot;) self.max_turns = 5&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def chat(self, question, memory): instruction_to_system = &amp;quot;&amp;quot;&amp;quot; Do NOT answer the question. Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question under ANY circumstance , just reformulate it if needed and otherwise return it as it is. Examples: 1.History: &amp;quot;Human: Wgat is a beginner friendly exercise that targets biceps? AI: A begginer friendly exercise that targets biceps is Concentration Curls?&amp;quot; Question: &amp;quot;Human: What are the steps to perform this exercise?&amp;quot; Output: &amp;quot;What are the steps to perform the Concentration Curls exercise?&amp;quot; 2.History: &amp;quot;Human: What is the category of bench press? AI: The category of bench press is strength.&amp;quot; Question: &amp;quot;Human: What are the steps to perform the child pose exercise?&amp;quot; Output: &amp;quot;What are the steps to perform the child pose exercise?&amp;quot; &amp;quot;&amp;quot;&amp;quot; llm = ChatOllama(model=&amp;quot;llama3.2&amp;quot;, temperature=0) question_maker_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, instruction_to_system), MessagesPlaceholder(variable_name=&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{question}&amp;quot;), ] ) question_chain = question_maker_prompt | llm | StrOutputParser() newQuestion = question_chain.invoke({&amp;quot;question&amp;quot;: question, &amp;quot;chat_history&amp;quot;: memory}) actual_question = self.contextualized_question(memory, newQuestion, question) emb = self.model.encode(actual_question) dao = FragmentDAO() fragments = dao.getFragments(str(emb.tolist())) context = [f[3] for f in fragments] for f in fragments: context.append(f[3]) documents = &amp;quot;\n\n---\n\n&amp;quot;.join(c for c in context) prompt = PromptTemplate( template=&amp;quot;&amp;quot;&amp;quot;You are an assistant for question answering tasks. Use the following documents to answer the question. If you dont know the answers, just say that you dont know. Use five sentences maximum and keep the answer concise: Documents: {documents} Question: {question} Answer:&amp;quot;&amp;quot;&amp;quot;, input_variables=[&amp;quot;documents&amp;quot;, &amp;quot;question&amp;quot;], ) llm = ChatOllama(model=&amp;quot;llama3.2&amp;quot;, temperature=0) rag_chain = prompt | llm | StrOutputParser() answer = rag_chain.invoke({ &amp;quot;question&amp;quot;: actual_question, &amp;quot;documents&amp;quot;: documents, }) # Keep only the last N turns (each turn = 2 messages) if len(memory) &amp;gt; 2 * self.max_turns: memory = memory[-2 * self.max_turns:] # Add new interaction as direct messages memory.append( HumanMessage(content=actual_question)) memory.append( AIMessage(content=answer)) print(newQuestion + &amp;quot; -&amp;gt; &amp;quot; + answer) for interactions in memory: print(interactions) print() return answer, memory def contextualized_question(self, chat_history, new_question, question): if chat_history: return new_question else: return question &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/buttered-toasst"&gt; /u/buttered-toasst &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1keyg9k/llm_not_following_instructions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1keyg9k/llm_not_following_instructions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1keyg9k/llm_not_following_instructions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-05T00:13:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1keoxav</id>
    <title>UI-Tars-1.5 reasoning never fails to entertain me.</title>
    <updated>2025-05-04T17:12:18+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1keoxav/uitars15_reasoning_never_fails_to_entertain_me/"&gt; &lt;img alt="UI-Tars-1.5 reasoning never fails to entertain me." src="https://preview.redd.it/ugrfilvlssye1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9b3d998fa1d7c1ac8db262966dd43f77e7e9a6f" title="UI-Tars-1.5 reasoning never fails to entertain me." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;7B parameter computer use agent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ugrfilvlssye1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1keoxav/uitars15_reasoning_never_fails_to_entertain_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1keoxav/uitars15_reasoning_never_fails_to_entertain_me/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-04T17:12:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1kemlk9</id>
    <title>Run AI Agents with Near-Native Speed on macOS‚ÄîIntroducing C/ua</title>
    <updated>2025-05-04T15:33:32+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share an exciting open-source framework called C/ua, specifically optimized for Apple Silicon Macs. C/ua allows AI agents to seamlessly control entire operating systems running inside high-performance, lightweight virtual containers.&lt;/p&gt; &lt;p&gt;Key Highlights:&lt;/p&gt; &lt;p&gt;Performance: Achieves up to 97% of native CPU speed on Apple Silicon. Compatibility: Works smoothly with any AI language model. Open Source: Fully available on GitHub for customization and community contributions.&lt;/p&gt; &lt;p&gt;Whether you're into automation, AI experimentation, or just curious about pushing your Mac's capabilities, check it out here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts and see what innovative use cases the macOS community can come up with!&lt;/p&gt; &lt;p&gt;Happy hacking!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kemlk9/run_ai_agents_with_nearnative_speed_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kemlk9/run_ai_agents_with_nearnative_speed_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kemlk9/run_ai_agents_with_nearnative_speed_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-04T15:33:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf371s</id>
    <title>LLM finetuning</title>
    <updated>2025-05-05T04:34:23+00:00</updated>
    <author>
      <name>/u/Unique_Yogurtcloset8</name>
      <uri>https://old.reddit.com/user/Unique_Yogurtcloset8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Given 22 image+JSON datasets that are mostly similar, what is the most cost-effective and time-efficient approach for LLM fine-tuning?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Train using all 22 datasets at once.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Train each dataset one by one in a sequential manner.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Start by training on the first dataset, and for subsequent training rounds, use a mixed sample: 20% from previously seen datasets and 80% from the current one.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unique_Yogurtcloset8"&gt; /u/Unique_Yogurtcloset8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kf371s/llm_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kf371s/llm_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kf371s/llm_finetuning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-05T04:34:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf52xf</id>
    <title>Local llm and framework</title>
    <updated>2025-05-05T06:41:20+00:00</updated>
    <author>
      <name>/u/lavoie005</name>
      <uri>https://old.reddit.com/user/lavoie005</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi guys it 2 days i test and search for good free framework that support mcp server, rag and so on for my coding project.&lt;br /&gt; i want it all local an compabible with all Ollama model.&lt;/p&gt; &lt;p&gt;Any idea ?&lt;br /&gt; Thx you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lavoie005"&gt; /u/lavoie005 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kf52xf/local_llm_and_framework/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kf52xf/local_llm_and_framework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kf52xf/local_llm_and_framework/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-05T06:41:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kflavu</id>
    <title>I need some help understanding how to interact a an ai with my database</title>
    <updated>2025-05-05T19:59:36+00:00</updated>
    <author>
      <name>/u/ConsequenceUnhappy33</name>
      <uri>https://old.reddit.com/user/ConsequenceUnhappy33</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm working on a project where I want an AI to suggest full meals (like lunch or dinner) by combining ingredients from a structured database. The database is divided into categories such as proteins, carbohydrates, vegetables, spices, and sauces. Under carbs you have items like rice, pasta, etc., and the same goes for the other categories. Each ingredient also has attributes, like sugar content, calories, etc. I will have database of all the ingriends, like liver of zebra etc so the database will be very large. &lt;/p&gt; &lt;p&gt;The AI should pick a meal based on the user's input. For example, if the user wants a low-carb option, it should select the best alternative that also makes sense flavor-wise‚Äîfor instance, curry and ketchup might not be a great match. And if ketchup isn‚Äôt available, the AI should reconsider. If it was going to suggest fries with the meal, but there's no ketchup, it should think again and offer a different idea.&lt;/p&gt; &lt;p&gt;What‚Äôs the best way to connect an AI to my database? I want quick responses‚Äîideally under 2‚Äì3 seconds. I've heard about the User ‚Üí RAG ‚Üí AI ‚Üí User pipeline, but I heard someone mention that RAG is not popular anymore, is that true. I also know that if i interact an AI from Ollama to my database its either hybridversion with RAG or training my data on a model which is called QnA, (im not sure). &lt;/p&gt; &lt;p&gt;Right now the data is stored in Json cause I know to little of whats best to store&lt;/p&gt; &lt;p&gt;I am really beginner in handling databases so dont judge me to hard.&lt;br /&gt; NOTE: It's not a must that the AI has to &amp;quot;think again&amp;quot; if something is out of stock.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ConsequenceUnhappy33"&gt; /u/ConsequenceUnhappy33 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kflavu/i_need_some_help_understanding_how_to_interact_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kflavu/i_need_some_help_understanding_how_to_interact_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kflavu/i_need_some_help_understanding_how_to_interact_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-05T19:59:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfkmqu</id>
    <title>Issue with OllamaSharp and Format Specifier</title>
    <updated>2025-05-05T19:32:31+00:00</updated>
    <author>
      <name>/u/TapWaterDev</name>
      <uri>https://old.reddit.com/user/TapWaterDev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm struggling to get a response to successfully generate when using Llama3.2 and a JsonFormat.&lt;/p&gt; &lt;p&gt;Here's my request:&lt;/p&gt; &lt;p&gt;&lt;code&gt; { &amp;quot;model&amp;quot; : &amp;quot;llama3.2&amp;quot;, &amp;quot;prompt&amp;quot; : &amp;quot;Fill out the details for the following Star Wars characters:\n- Darth Vader\n- Luke Skywalker\n- Padme\n- Emperor Palpatine\n\nInclude their loyalty, name, and the actor who played them.&amp;quot;, &amp;quot;options&amp;quot; : { &amp;quot;temperature&amp;quot; : 0.1, &amp;quot;num_predict&amp;quot; : 10000, &amp;quot;top_p&amp;quot; : 0.5 }, &amp;quot;system&amp;quot; : &amp;quot;You cannot prompt the user for further responses.\nDo not generate any text outside of the requested response.&amp;quot;, &amp;quot;format&amp;quot; : &amp;quot;{\n \&amp;quot;type\&amp;quot;: [\n \&amp;quot;array\&amp;quot;,\n \&amp;quot;null\&amp;quot;\n ],\n \&amp;quot;items\&amp;quot;: {\n \&amp;quot;type\&amp;quot;: [\n \&amp;quot;object\&amp;quot;,\n \&amp;quot;null\&amp;quot;\n ],\n \&amp;quot;properties\&amp;quot;: {\n \&amp;quot;CharacterName\&amp;quot;: {\n \&amp;quot;type\&amp;quot;: \&amp;quot;string\&amp;quot;\n },\n \&amp;quot;ActorName\&amp;quot;: {\n \&amp;quot;type\&amp;quot;: \&amp;quot;string\&amp;quot;\n },\n \&amp;quot;Loyalty\&amp;quot;: {\n \&amp;quot;enum\&amp;quot;: [\n \&amp;quot;Jedi\&amp;quot;,\n \&amp;quot;Rebellion\&amp;quot;,\n \&amp;quot;Empire\&amp;quot;\n ]\n }\n },\n \&amp;quot;required\&amp;quot;: [\n \&amp;quot;CharacterName\&amp;quot;,\n \&amp;quot;ActorName\&amp;quot;,\n \&amp;quot;Loyalty\&amp;quot;\n ]\n }\n}&amp;quot;, &amp;quot;stream&amp;quot; : true, &amp;quot;raw&amp;quot; : false, &amp;quot;CustomHeaders&amp;quot; : { } } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;For ease of digestion, that format is given by running these classes through the &lt;code&gt;JsonSchemaExporter&lt;/code&gt;: ``` private class StarWarsCharacter { public required string CharacterName { get; init; } public required string ActorName { get; init; } public required Loyalty Loyalty { get; init; }&lt;/p&gt; &lt;pre&gt;&lt;code&gt;} [JsonConverter(typeof(JsonStringEnumConverter&amp;lt;Loyalty&amp;gt;))] private enum Loyalty { Jedi, Rebellion, Empire } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;All chunks that come back are empty.&lt;/p&gt; &lt;p&gt;I can work around this by doing this:&lt;/p&gt; &lt;p&gt;&lt;code&gt; GenerateRequest request = new() { System = inferenceRequest.SystemPrompt + $&amp;quot;Give your response in the following schema {resultSchema}. Do not generate any text outside of that.&amp;quot;, Prompt = renderedPrompt, //Format = resultSchema, Model = mappedModel, Options = new() { Temperature = inferenceRequest.InferenceParameters.Temperature, TopP = inferenceRequest.InferenceParameters.TopP, NumPredict = inferenceRequest.InferenceParameters.MaxTokens } }; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Which nearly works (I don't actually care if the answer's right, I'm testing my implementation, not the prompt), instead returning:&lt;/p&gt; &lt;p&gt;&lt;code&gt; { &amp;quot;type&amp;quot; : [ &amp;quot;array&amp;quot;, &amp;quot;null&amp;quot; ], &amp;quot;items&amp;quot; : [ { &amp;quot;CharacterName&amp;quot; : &amp;quot;Darth Vader&amp;quot;, &amp;quot;ActorName&amp;quot; : &amp;quot;David Prowse, James Earl Jones&amp;quot;, &amp;quot;Loyalty&amp;quot; : &amp;quot;Empire&amp;quot; }, { &amp;quot;CharacterName&amp;quot; : &amp;quot;Luke Skywalker&amp;quot;, &amp;quot;ActorName&amp;quot; : &amp;quot;Mark Hamill&amp;quot;, &amp;quot;Loyalty&amp;quot; : &amp;quot;Rebellion&amp;quot; }, { &amp;quot;CharacterName&amp;quot; : &amp;quot;Padme&amp;quot;, &amp;quot;ActorName&amp;quot; : &amp;quot;Natalie Portman&amp;quot;, &amp;quot;Loyalty&amp;quot; : &amp;quot;Jedi&amp;quot; }, { &amp;quot;CharacterName&amp;quot; : &amp;quot;Emperor Palpatine&amp;quot;, &amp;quot;ActorName&amp;quot; : &amp;quot;Ian McDiarmid&amp;quot;, &amp;quot;Loyalty&amp;quot; : &amp;quot;Empire&amp;quot; } ] } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;What's going on here? Is the Schema Exporter just outputting the wrong thing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TapWaterDev"&gt; /u/TapWaterDev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kfkmqu/issue_with_ollamasharp_and_format_specifier/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kfkmqu/issue_with_ollamasharp_and_format_specifier/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kfkmqu/issue_with_ollamasharp_and_format_specifier/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-05T19:32:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfcpex</id>
    <title>üìπ Just published a new video: ‚ÄúFrom Text to Summary: LLaMA 3.1 + .NET in Action!‚Äù</title>
    <updated>2025-05-05T14:16:06+00:00</updated>
    <author>
      <name>/u/Emotional_Thought355</name>
      <uri>https://old.reddit.com/user/Emotional_Thought355</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the video, we build a Blazor WASM application that connects to LLaMA 3.1 using Microsoft.Extensions.AI.Ollama package ‚Äî showing how to summarize text interactively right in the browser.&lt;/p&gt; &lt;p&gt;üß† What‚Äôs inside:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Setting up Ollama and downloading the LLaMA 3.1 model&lt;/li&gt; &lt;li&gt;A brief look at why local LLMs matter (security, privacy, no cost etc.)&lt;/li&gt; &lt;li&gt;Creating a simple text summarization UI in Blazor&lt;/li&gt; &lt;li&gt;Calling LLaMA 3.1 from .NET and saving results as a Markdown file&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚ñ∂Ô∏è Watch it here: &lt;a href="https://www.youtube.com/watch?v=fWNj4dTXQoI"&gt;https://www.youtube.com/watch?v=fWNj4dTXQoI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emotional_Thought355"&gt; /u/Emotional_Thought355 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kfcpex/just_published_a_new_video_from_text_to_summary/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kfcpex/just_published_a_new_video_from_text_to_summary/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kfcpex/just_published_a_new_video_from_text_to_summary/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-05T14:16:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfutjm</id>
    <title>How to get an AI to check my email</title>
    <updated>2025-05-06T03:20:56+00:00</updated>
    <author>
      <name>/u/AggressiveSkirl1680</name>
      <uri>https://old.reddit.com/user/AggressiveSkirl1680</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I was wondering if I could get some general direction on how to get an AI to log in and check my email and talk to me about it, possibly respond to it, etc. I'm running Ollama and Openwebui on linux. Like, am I looking for certain tools for openwebui? and if so which ones? so far my experimentation has been pretty miserable making any progress.&lt;/p&gt; &lt;p&gt;Any input would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggressiveSkirl1680"&gt; /u/AggressiveSkirl1680 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kfutjm/how_to_get_an_ai_to_check_my_email/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kfutjm/how_to_get_an_ai_to_check_my_email/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kfutjm/how_to_get_an_ai_to_check_my_email/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-06T03:20:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfutjs</id>
    <title>Qwen3-4B works surprisingly well with MCP tools ‚Äì and it fits on a single GPU!</title>
    <updated>2025-05-06T03:20:57+00:00</updated>
    <author>
      <name>/u/AdOdd4004</name>
      <uri>https://old.reddit.com/user/AdOdd4004</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kfutjs/qwen34b_works_surprisingly_well_with_mcp_tools/"&gt; &lt;img alt="Qwen3-4B works surprisingly well with MCP tools ‚Äì and it fits on a single GPU!" src="https://external-preview.redd.it/T-fmHUiUS6d3oF7m2sdyPewJmL0qybFEJ1_FVFmWxU8.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d09e383206089d1652abe1f90dce134737a64e4" title="Qwen3-4B works surprisingly well with MCP tools ‚Äì and it fits on a single GPU!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdOdd4004"&gt; /u/AdOdd4004 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/N-B1rYJ61a8?si=ilQeL1sQmt-5ozRD"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kfutjs/qwen34b_works_surprisingly_well_with_mcp_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kfutjs/qwen34b_works_surprisingly_well_with_mcp_tools/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-06T03:20:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfkg1a</id>
    <title>Open-Source Data ETL with On-premise structured extraction with LLM using Ollama</title>
    <updated>2025-05-05T19:25:11+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Ollama community, I've been working on an ETL framework to prepare fresh data for AI &lt;a href="https://github.com/cocoindex-io/cocoindex"&gt;https://github.com/cocoindex-io/cocoindex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We've added builtin native support for running Ollama in ETL with custom logic, in this project, I did structure data extraction from PDF with ollama. &lt;/p&gt; &lt;p&gt;&lt;a href="https://cocoindex.io/blogs/cocoindex-ollama-structured-extraction-from-pdf"&gt;https://cocoindex.io/blogs/cocoindex-ollama-structured-extraction-from-pdf&lt;/a&gt; &lt;/p&gt; &lt;p&gt;source code is here: &lt;a href="https://github.com/cocoindex-io/cocoindex/blob/main/examples/manuals_llm_extraction/main.py"&gt;https://github.com/cocoindex-io/cocoindex/blob/main/examples/manuals_llm_extraction/main.py&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking forward to learn your feedback, thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kfkg1a/opensource_data_etl_with_onpremise_structured/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kfkg1a/opensource_data_etl_with_onpremise_structured/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kfkg1a/opensource_data_etl_with_onpremise_structured/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-05T19:25:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfc65l</id>
    <title>I built an open-source AI-powered library for web testing that runs on Ollama</title>
    <updated>2025-05-05T13:53:07+00:00</updated>
    <author>
      <name>/u/p0deje</name>
      <uri>https://old.reddit.com/user/p0deje</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey r/ollama,&lt;/p&gt; &lt;p&gt;My name is Alex Rodionov and I'm a tech lead and Ruby maintainer of the Selenium project. For the last few months, I‚Äôve been working on &lt;strong&gt;Alumnium&lt;/strong&gt; ‚Äî an open-source library that automates testing for web applications by leveraging Selenium or Playwright, AI, and natural language commands.&lt;/p&gt; &lt;p&gt;Just yesterday I finally shipped support for &lt;strong&gt;Ollama&lt;/strong&gt; by using &lt;strong&gt;Mistral Small 3.1 24B&lt;/strong&gt; which allows me to run the tests completely locally and not rely on cloud providers. It's super slow on my MacBook Pro, but I'm excited it's working at all.&lt;/p&gt; &lt;p&gt;Kudos to the Ollama team for creating such an easy way to use models both with vision and tool-calling support!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Website:&lt;/em&gt; &lt;a href="https://alumnium.ai/"&gt;https://alumnium.ai/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Repository:&lt;/em&gt; &lt;a href="https://github.com/alumnium-hq/alumnium"&gt;https://github.com/alumnium-hq/alumnium&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Discord:&lt;/em&gt; &lt;a href="https://discord.gg/mP29tTtKHg"&gt;https://discord.gg/mP29tTtKHg&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Docs:&lt;/em&gt; &lt;a href="https://alumnium.ai/docs/guides/self-hosting/#ollama"&gt;https://alumnium.ai/docs/guides/self-hosting/#ollama&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/p0deje"&gt; /u/p0deje &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kfc65l/i_built_an_opensource_aipowered_library_for_web/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kfc65l/i_built_an_opensource_aipowered_library_for_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kfc65l/i_built_an_opensource_aipowered_library_for_web/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-05T13:53:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfr4i4</id>
    <title>local debugging menace now supports phi4-reasoning and qwen3</title>
    <updated>2025-05-06T00:11:06+00:00</updated>
    <author>
      <name>/u/AntelopeEntire9191</name>
      <uri>https://old.reddit.com/user/AntelopeEntire9191</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kfr4i4/local_debugging_menace_now_supports_phi4reasoning/"&gt; &lt;img alt="local debugging menace now supports phi4-reasoning and qwen3" src="https://external-preview.redd.it/ZDAwMDMwaWt6MXplMWc1n15xJrjGmONCGSEDHTHNeXkVD0MktWP-lK4UFrFE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6cc28f1a9d2230b96cafce397b8ea17be0bf2e6f" title="local debugging menace now supports phi4-reasoning and qwen3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;no cap fr fr this update is straight bussin, been tweaking on building &lt;strong&gt;Cloi&lt;/strong&gt; its local debugging agent that runs in your terminal&lt;/p&gt; &lt;p&gt;Cloi deadass catches your error tracebacks, spins up a local LLM (zero api key nonsense, no cloud tax) and only with your permission drops some clean af patches directly to ur files.&lt;/p&gt; &lt;p&gt;New features dropped: run &lt;code&gt;/model&lt;/code&gt; to choose ANY models already on your mac or try the new &lt;strong&gt;phi4-reasoning&lt;/strong&gt; and &lt;strong&gt;qwen3&lt;/strong&gt; models for local usage&lt;/p&gt; &lt;p&gt;your code debugging experience about to be skibidi gyatt with these models fr&lt;/p&gt; &lt;p&gt;BTW built this bc cursor's o3 got me down astronomical ($0.30 per request??) and local models are just getting better and better (benchmarks don't lie frfr) on god!&lt;/p&gt; &lt;p&gt;If anyone's interested in the implementation or wants to issue feedback or PRs, check out da code: &lt;a href="https://github.com/cloi-ai/cloi"&gt;https://github.com/cloi-ai/cloi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AntelopeEntire9191"&gt; /u/AntelopeEntire9191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mll4i0ikz1ze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kfr4i4/local_debugging_menace_now_supports_phi4reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kfr4i4/local_debugging_menace_now_supports_phi4reasoning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-06T00:11:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfwx4r</id>
    <title>n8n AI Agent for Newsletter using Ollama tutorial</title>
    <updated>2025-05-06T05:28:10+00:00</updated>
    <author>
      <name>/u/mehul_gupta1997</name>
      <uri>https://old.reddit.com/user/mehul_gupta1997</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kfwx4r/n8n_ai_agent_for_newsletter_using_ollama_tutorial/"&gt; &lt;img alt="n8n AI Agent for Newsletter using Ollama tutorial" src="https://external-preview.redd.it/EQ51TkjXersBG7qNTnbQ81-TDwaaVyeGDrGkuwiSxfI.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e66bd017cb981486df4d9445cf5dcd299673bd9e" title="n8n AI Agent for Newsletter using Ollama tutorial" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehul_gupta1997"&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/jgXtwDGoKiU?si=35OpHENqiqspyoud"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kfwx4r/n8n_ai_agent_for_newsletter_using_ollama_tutorial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kfwx4r/n8n_ai_agent_for_newsletter_using_ollama_tutorial/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-06T05:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfknqf</id>
    <title>Local LLM with Ollama, OpenWebUI and Database with RAG</title>
    <updated>2025-05-05T19:33:37+00:00</updated>
    <author>
      <name>/u/OriginalDiddi</name>
      <uri>https://old.reddit.com/user/OriginalDiddi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I would like to set up a local LLM with Ollama in my company and it would be nice to connect a database with PDF and Docs Files to the LLM, maby with OpenWebUI if thats possible. It should be possible to ask the LLM about the documents, without refering to it directly, just as a normal prompt. &lt;/p&gt; &lt;p&gt;Maby someone can give me some tips and tools. Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OriginalDiddi"&gt; /u/OriginalDiddi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kfknqf/local_llm_with_ollama_openwebui_and_database_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kfknqf/local_llm_with_ollama_openwebui_and_database_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kfknqf/local_llm_with_ollama_openwebui_and_database_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-05T19:33:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfqbcr</id>
    <title>Ollama-based Real-time AI Voice Chat at ~500ms Latency</title>
    <updated>2025-05-05T23:32:57+00:00</updated>
    <author>
      <name>/u/Lonligrin</name>
      <uri>https://old.reddit.com/user/Lonligrin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kfqbcr/ollamabased_realtime_ai_voice_chat_at_500ms/"&gt; &lt;img alt="Ollama-based Real-time AI Voice Chat at ~500ms Latency" src="https://external-preview.redd.it/FrSeQHAfH8ucYuqX6ERN969lQMmZVCPB5a9bZoQMkEk.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52de6f797c358b77b4905f13d71e5458e1d5d320" title="Ollama-based Real-time AI Voice Chat at ~500ms Latency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built RealtimeVoiceChat because I was frustrated with the latency in most voice AI interactions. This is an open-source (MIT license) system designed for real-time, local voice conversations with LLMs.&lt;/p&gt; &lt;p&gt;I wanted to get one step closer to natural conversation speed with a system that responses back with around 500ms latency. &lt;/p&gt; &lt;p&gt;Key aspects: Designed for local LLMs (Ollama primarily, OpenAI connector included). Interruptible conversation. Turn detection to avoid cutting the user off mid-thought. Dockerized setup available.&lt;/p&gt; &lt;p&gt;It requires a decent CUDA-enabled GPU for good performance due to the STT/TTS models.&lt;/p&gt; &lt;p&gt;Would love to hear your feedback on the approach, performance, potential optimizations, or any features you think are essential for a good local voice AI experience.&lt;/p&gt; &lt;p&gt;The code is here: &lt;a href="https://github.com/KoljaB/RealtimeVoiceChat"&gt;https://github.com/KoljaB/RealtimeVoiceChat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lonligrin"&gt; /u/Lonligrin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=HM_IQuuuPX8&amp;amp;si=R5zzcLV32SOOUCq7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kfqbcr/ollamabased_realtime_ai_voice_chat_at_500ms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kfqbcr/ollamabased_realtime_ai_voice_chat_at_500ms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-05T23:32:57+00:00</published>
  </entry>
</feed>
