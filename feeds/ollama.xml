<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-25T12:56:24+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1i8yiae</id>
    <title>Lovable, Bolt‚Ä¶ self-host alternatives</title>
    <updated>2025-01-24T15:51:56+00:00</updated>
    <author>
      <name>/u/productboy</name>
      <uri>https://old.reddit.com/user/productboy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Before I build it myself &lt;em&gt;please&lt;/em&gt; tell me someone here has an open source alternative to Lovable or Bolt that‚Äôs as good, that I can self-host.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/productboy"&gt; /u/productboy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8yiae/lovable_bolt_selfhost_alternatives/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8yiae/lovable_bolt_selfhost_alternatives/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8yiae/lovable_bolt_selfhost_alternatives/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T15:51:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8m6ls</id>
    <title>Llama 3.1 405B + 8x AMD Instinct Mi60 AI Server - Shockingly Good!</title>
    <updated>2025-01-24T03:26:28+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xzhz003o1vee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8m6ls/llama_31_405b_8x_amd_instinct_mi60_ai_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8m6ls/llama_31_405b_8x_amd_instinct_mi60_ai_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T03:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8k8u7</id>
    <title>I added ollama support for an open-source operator agent</title>
    <updated>2025-01-24T01:47:28+00:00</updated>
    <author>
      <name>/u/Swimming_Driver4974</name>
      <uri>https://old.reddit.com/user/Swimming_Driver4974</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a similar Operator Agent framework like OpenAI's one, and made it open-source. Just added support for Ollama so it can be run completely locally using text and vision models. It's not perfect, but has potential: &lt;a href="https://github.com/GPT-Protocol/007-agent"&gt;https://github.com/GPT-Protocol/007-agent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Swimming_Driver4974"&gt; /u/Swimming_Driver4974 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8k8u7/i_added_ollama_support_for_an_opensource_operator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8k8u7/i_added_ollama_support_for_an_opensource_operator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8k8u7/i_added_ollama_support_for_an_opensource_operator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T01:47:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8zs2g</id>
    <title>Locally hosted deepseek-r1 and OpenWebUI</title>
    <updated>2025-01-24T16:45:35+00:00</updated>
    <author>
      <name>/u/robonova-1</name>
      <uri>https://old.reddit.com/user/robonova-1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to use deepseek-r1 with OpenWebUI. When I select that model and ask a question I can see the model loading in VRAM but the UI never loads any response. I have the latest version of OpenWebUI docker version installed. Other models work fine. Is there some config that I need to change in the settings for OpenWebUI for it to work with deepseek-r1? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robonova-1"&gt; /u/robonova-1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8zs2g/locally_hosted_deepseekr1_and_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8zs2g/locally_hosted_deepseekr1_and_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8zs2g/locally_hosted_deepseekr1_and_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T16:45:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i900ls</id>
    <title>Would 2 x 3090 double my speed?</title>
    <updated>2025-01-24T16:55:23+00:00</updated>
    <author>
      <name>/u/PositiveEnergyMatter</name>
      <uri>https://old.reddit.com/user/PositiveEnergyMatter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 3090 now, and 32b models work fine with it. My question is would two 3090s double my speed? I know it makes it so i can load large models, but i am more interested in speed boosts. IS it worth getting a 4090 or a 5090, or am i better off just getting another 3090?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PositiveEnergyMatter"&gt; /u/PositiveEnergyMatter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i900ls/would_2_x_3090_double_my_speed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i900ls/would_2_x_3090_double_my_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i900ls/would_2_x_3090_double_my_speed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T16:55:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i90dm0</id>
    <title>Import a prompt/export response with python?</title>
    <updated>2025-01-24T17:09:59+00:00</updated>
    <author>
      <name>/u/Fervolts</name>
      <uri>https://old.reddit.com/user/Fervolts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im quite new using this, I'm new in python too lmao (I mostly program with JS) but I'm trying to use Ollama to make a chatbot for my streams, I use streamer.bot and that's able to run python code so I've been working in a way to get the prompt from a twitch message and then export the response, any ideas or examples I could use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fervolts"&gt; /u/Fervolts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i90dm0/import_a_promptexport_response_with_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i90dm0/import_a_promptexport_response_with_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i90dm0/import_a_promptexport_response_with_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T17:09:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8qckb</id>
    <title>List of top Open Source Chat UI for ollama/any LLM in general. (community edition)</title>
    <updated>2025-01-24T07:47:35+00:00</updated>
    <author>
      <name>/u/VisibleLawfulness246</name>
      <uri>https://old.reddit.com/user/VisibleLawfulness246</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey community, I am trying to compile a list of all the open-source ChatGPT UI. Here is the list from my research. Let's make this thread helpful. tell me- what do you use? and what are the pros and cons along with alternatives your tool of choice.&lt;/p&gt; &lt;p&gt;personally I'm a big fan of Open WebUI but I'm looking to try out what all is new in the community,&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open WebUI&lt;/li&gt; &lt;li&gt;LibreChat&lt;/li&gt; &lt;li&gt;anythingLLM&lt;/li&gt; &lt;li&gt;GPT4all&lt;/li&gt; &lt;li&gt;oobabooga&lt;/li&gt; &lt;li&gt;verba&lt;/li&gt; &lt;li&gt;dify&lt;/li&gt; &lt;li&gt;SillyTavern&lt;/li&gt; &lt;li&gt;Danswer&lt;/li&gt; &lt;li&gt;Lobe Ui&lt;/li&gt; &lt;li&gt;hugging face chat-Ui&lt;/li&gt; &lt;li&gt;kobold Cpp/ for from llama cpp&lt;/li&gt; &lt;li&gt;private gpt&lt;/li&gt; &lt;li&gt;serge chat&lt;/li&gt; &lt;li&gt;JanHQ&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What am I missing from this list? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VisibleLawfulness246"&gt; /u/VisibleLawfulness246 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8qckb/list_of_top_open_source_chat_ui_for_ollamaany_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8qckb/list_of_top_open_source_chat_ui_for_ollamaany_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8qckb/list_of_top_open_source_chat_ui_for_ollamaany_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T07:47:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8q0gk</id>
    <title>LLM website that lets you use any model and pay as you go. I can‚Äôt remember what it‚Äôs called.</title>
    <updated>2025-01-24T07:22:08+00:00</updated>
    <author>
      <name>/u/opelly</name>
      <uri>https://old.reddit.com/user/opelly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The title says it all. I came across a website a while ago that has a ChatGPT like interface, but lets you use all kinds of different models, and pay by the token. I specifically remember a leaderboard that ranks the models by popularity and even showed the number of tokens that each model had generated among all users on the website. I can‚Äôt find it for the life of me. Please let me know what site this is if it rings a bell. Thank you!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/opelly"&gt; /u/opelly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8q0gk/llm_website_that_lets_you_use_any_model_and_pay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8q0gk/llm_website_that_lets_you_use_any_model_and_pay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8q0gk/llm_website_that_lets_you_use_any_model_and_pay/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T07:22:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8bhf3</id>
    <title>Upgraded!</title>
    <updated>2025-01-23T19:19:46+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i8bhf3/upgraded/"&gt; &lt;img alt="Upgraded!" src="https://preview.redd.it/8qdjrpbxmsee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e72aeebb6bad9946b60f373d5e38bdfa97f84a8" title="Upgraded!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qdjrpbxmsee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8bhf3/upgraded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8bhf3/upgraded/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T19:19:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8r971</id>
    <title>I want to try to replicate the RAG functionality similar to LM Studio using open source tools. Any ideas on where to start?</title>
    <updated>2025-01-24T08:57:53+00:00</updated>
    <author>
      <name>/u/ikmalsaid</name>
      <uri>https://old.reddit.com/user/ikmalsaid</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ikmalsaid"&gt; /u/ikmalsaid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8r971/i_want_to_try_to_replicate_the_rag_functionality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8r971/i_want_to_try_to_replicate_the_rag_functionality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8r971/i_want_to_try_to_replicate_the_rag_functionality/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T08:57:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i92man</id>
    <title>Llama-Vision training on CPU - I know but has things changed yet?</title>
    <updated>2025-01-24T18:41:55+00:00</updated>
    <author>
      <name>/u/groovy_mentor</name>
      <uri>https://old.reddit.com/user/groovy_mentor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using Llama3.2Vision11B LLM on Ollama to analyze my farm indoor plant growth. I run it in Debian 12 on a HP mini desktop with i5 8500 chip, 32GB RAM with 512 SSD. As my plants grow, I need to train the model to infer better. I have not seen ANY videos or posts on how to train to do this using my CPU based computer. I don't care about time it takes to train, let it be 48 hours or months. I don't have a lot of pics either to train from as I am still collecting them for each plant cycle, 500 pics currently. I can also train it one picture at a time giving context about each of the picture. Please help me! Should I look at anything other than Ollama?&lt;/p&gt; &lt;p&gt;Edit: Added more context to my question above.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/groovy_mentor"&gt; /u/groovy_mentor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i92man/llamavision_training_on_cpu_i_know_but_has_things/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i92man/llamavision_training_on_cpu_i_know_but_has_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i92man/llamavision_training_on_cpu_i_know_but_has_things/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T18:41:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1i95hjd</id>
    <title>Clarification on context length</title>
    <updated>2025-01-24T20:43:46+00:00</updated>
    <author>
      <name>/u/LjLies</name>
      <uri>https://old.reddit.com/user/LjLies</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see there is a &lt;code&gt;num_ctx&lt;/code&gt; parameter. I have various questions that the web doesn't seem to answer fully:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What is it set to? I can set it, but like other parameters, I can't seem to find a way to get the current value.&lt;/li&gt; &lt;li&gt;On the web I found claims that it's set to 2048 by default. Is that still the case? That seems like a very low number for many current models.&lt;/li&gt; &lt;li&gt;Does it currently take into account the load model's context window capabilities, or not? For instance &lt;code&gt;/show info&lt;/code&gt; on a given model will give me &lt;code&gt;context length 8192&lt;/code&gt;: does this mean num_ctx will be set to that when I load that particular model, or will it stay to whatever its default is?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The reason I'm looking into this in the first place is that on my puny CPU-based system, many models work acceptably, but only for the first few interactions, then the time to first token becomes unacceptably long. I'm thinking that for the initial interaction, prompt caching is taking place and working effectively, but at some point I go over the allowed context window, and that means the initial prompts have to be discarded, and so the cache will no longer be valid. Does this seem like a correct inference?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LjLies"&gt; /u/LjLies &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i95hjd/clarification_on_context_length/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i95hjd/clarification_on_context_length/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i95hjd/clarification_on_context_length/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T20:43:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8r79t</id>
    <title>How to run models that are not on ollama website? Especially uncensored ones</title>
    <updated>2025-01-24T08:53:40+00:00</updated>
    <author>
      <name>/u/discoveringnature12</name>
      <uri>https://old.reddit.com/user/discoveringnature12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;trying to figure out how to run models not listed &lt;a href="https://ollama.com/library"&gt;https://ollama.com/library&lt;/a&gt;. Want to run uncensored models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/discoveringnature12"&gt; /u/discoveringnature12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8r79t/how_to_run_models_that_are_not_on_ollama_website/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8r79t/how_to_run_models_that_are_not_on_ollama_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8r79t/how_to_run_models_that_are_not_on_ollama_website/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T08:53:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9ccdb</id>
    <title>Best model for following multiple rules on prompt?</title>
    <updated>2025-01-25T01:59:26+00:00</updated>
    <author>
      <name>/u/HunterHelpful9383</name>
      <uri>https://old.reddit.com/user/HunterHelpful9383</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone. I just started messing around with AI models, and I am wondering, which models are the best at following multiple rules and limitations in it's prompt?&lt;/p&gt; &lt;p&gt;for example, many times I use prompts that asks to prevents emoji usage, introdutions on responses, response length, speech-style, etc, and I see that most AIs struggle to follow all of them correctly.&lt;/p&gt; &lt;p&gt;From my testing, I noticed that deepseek-r1 is pretty good, because of it's &amp;quot;deep think&amp;quot; feature, so it re-checks the rules everytime, but since I'm a beginner, I am up to suggestions&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HunterHelpful9383"&gt; /u/HunterHelpful9383 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9ccdb/best_model_for_following_multiple_rules_on_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9ccdb/best_model_for_following_multiple_rules_on_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9ccdb/best_model_for_following_multiple_rules_on_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T01:59:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i988zt</id>
    <title>How to Pass Image-Based Math/Geometry Problems to an LLM Without a Vision Model?</title>
    <updated>2025-01-24T22:43:56+00:00</updated>
    <author>
      <name>/u/yvzyldrm</name>
      <uri>https://old.reddit.com/user/yvzyldrm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a project where I aim to build a system capable of solving math and geometry problems provided as images. The questions will be solved by an LLM (Large Language Model) named DeepSeek R1. However, DeepSeek R1 does not have a vision model, and I don‚Äôt have one either. I need to figure out how to pass these image-based questions to the LLM.&lt;/p&gt; &lt;p&gt;I‚Äôve considered using OCR (Optical Character Recognition) systems, but they don‚Äôt work well for my case because OCR struggles to convert graphical elements (like diagrams or geometric shapes) into text-based formats. On the websites of large language models like DeepSeek, there are often options to upload images. How do these systems work? If anyone can provide guidance or suggestions, I would greatly appreciate it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yvzyldrm"&gt; /u/yvzyldrm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i988zt/how_to_pass_imagebased_mathgeometry_problems_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i988zt/how_to_pass_imagebased_mathgeometry_problems_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i988zt/how_to_pass_imagebased_mathgeometry_problems_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T22:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i999sr</id>
    <title>Ollama windows pull not working</title>
    <updated>2025-01-24T23:29:54+00:00</updated>
    <author>
      <name>/u/Lporro</name>
      <uri>https://old.reddit.com/user/Lporro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm unable to complete a model pull in ollama for Windows, it always reset at some point and restart until it gives me too much retries. Is someone facing the same issue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lporro"&gt; /u/Lporro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i999sr/ollama_windows_pull_not_working/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i999sr/ollama_windows_pull_not_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i999sr/ollama_windows_pull_not_working/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T23:29:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8yp5m</id>
    <title>Use RAG to chat with PDFs using Deepseek, Langchain and Streamlit</title>
    <updated>2025-01-24T16:00:22+00:00</updated>
    <author>
      <name>/u/Special_Community179</name>
      <uri>https://old.reddit.com/user/Special_Community179</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i8yp5m/use_rag_to_chat_with_pdfs_using_deepseek/"&gt; &lt;img alt="Use RAG to chat with PDFs using Deepseek, Langchain and Streamlit" src="https://external-preview.redd.it/Kb3DxleokIDZE5pe9gpJx8YJHS033-33bKrc01NRHGA.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db0276a28b5d9b2ec24cad65f6aa21e323d49169" title="Use RAG to chat with PDFs using Deepseek, Langchain and Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special_Community179"&gt; /u/Special_Community179 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=M6vZ6b75p9k&amp;amp;list=PLp01ObP3udmq2quR-RfrX4zNut_t_kNot"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8yp5m/use_rag_to_chat_with_pdfs_using_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8yp5m/use_rag_to_chat_with_pdfs_using_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T16:00:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1i998ux</id>
    <title>Any instructions for installing ollama as a service on MacOS headless (via SSH)?</title>
    <updated>2025-01-24T23:28:44+00:00</updated>
    <author>
      <name>/u/StartupTim</name>
      <uri>https://old.reddit.com/user/StartupTim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I've been trying to get ollama to work as a service on MacOS and I just can't get it to work. I've installed it via brew, and set the brew service, but yet it just won't start on reboot. Also, I can't get it to recognize any of the environment variables. &lt;/p&gt; &lt;p&gt;Does anybody know of a guide to successfully get ollama working as a service on Mac OS (brew or not)?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StartupTim"&gt; /u/StartupTim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i998ux/any_instructions_for_installing_ollama_as_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i998ux/any_instructions_for_installing_ollama_as_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i998ux/any_instructions_for_installing_ollama_as_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T23:28:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8qav7</id>
    <title>A list of all the top Open Source Chat UI for ollama/any LLM in general. (community edition)</title>
    <updated>2025-01-24T07:43:58+00:00</updated>
    <author>
      <name>/u/VisibleLawfulness246</name>
      <uri>https://old.reddit.com/user/VisibleLawfulness246</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i8qav7/a_list_of_all_the_top_open_source_chat_ui_for/"&gt; &lt;img alt="A list of all the top Open Source Chat UI for ollama/any LLM in general. (community edition)" src="https://b.thumbs.redditmedia.com/2mlU8Ofw8DVHHHmMnu33kSfWqc1LdBv2Whegj1zwhkY.jpg" title="A list of all the top Open Source Chat UI for ollama/any LLM in general. (community edition)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's my list right now&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open Web UI&lt;/li&gt; &lt;li&gt;LibreChat&lt;/li&gt; &lt;li&gt;anythingLLM&lt;/li&gt; &lt;li&gt;GPT4all&lt;/li&gt; &lt;li&gt;oobabooga&lt;/li&gt; &lt;li&gt;verba&lt;/li&gt; &lt;li&gt;dify&lt;/li&gt; &lt;li&gt;SillyTavern&lt;/li&gt; &lt;li&gt;Danswer&lt;/li&gt; &lt;li&gt;Lobe Ui&lt;/li&gt; &lt;li&gt;hugging face chat-Ui&lt;/li&gt; &lt;li&gt;kobold Cpp/ for from llama cpp&lt;/li&gt; &lt;li&gt;private gpt&lt;/li&gt; &lt;li&gt;serge chat&lt;/li&gt; &lt;li&gt;JanHQ&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What am I missing from this list?&lt;/p&gt; &lt;p&gt;adding this image from my research&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VisibleLawfulness246"&gt; /u/VisibleLawfulness246 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8qav7/a_list_of_all_the_top_open_source_chat_ui_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8qav7/a_list_of_all_the_top_open_source_chat_ui_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8qav7/a_list_of_all_the_top_open_source_chat_ui_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T07:43:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8n38z</id>
    <title>Coming soon: 100% Local Video Understanding Engine (an open-source project that can classify, caption, transcribe, and understand any video on your local device)</title>
    <updated>2025-01-24T04:17:00+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i8n38z/coming_soon_100_local_video_understanding_engine/"&gt; &lt;img alt="Coming soon: 100% Local Video Understanding Engine (an open-source project that can classify, caption, transcribe, and understand any video on your local device)" src="https://external-preview.redd.it/MDIwNzQ0ZzNidmVlMe21Biif0sGFU8GTsH3N7D_CJugYvIxsEVZ-nvrUed0U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fa43786541e784dc36e99b0ae3b5e02c8a90ea9" title="Coming soon: 100% Local Video Understanding Engine (an open-source project that can classify, caption, transcribe, and understand any video on your local device)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4sh274g3bvee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8n38z/coming_soon_100_local_video_understanding_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8n38z/coming_soon_100_local_video_understanding_engine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T04:17:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9ewmy</id>
    <title>Help: Ollama cannot start because it try to create an existing directory</title>
    <updated>2025-01-25T04:20:05+00:00</updated>
    <author>
      <name>/u/BrianHuster</name>
      <uri>https://old.reddit.com/user/BrianHuster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h3&gt;What is the issue?&lt;/h3&gt; &lt;p&gt;Since my main partition doesn't have enough memory, I change OLLAMA_MODELS to &lt;code&gt;/media/brianhuster/E/ollama/models&lt;/code&gt;. I have created that directory, and &lt;code&gt;mv /usr/share/ollama/.ollama/models /media/brianhuster/E/ollama/models&lt;/code&gt;. However, after that I cannot restart Ollama because, according to &lt;code&gt;sudo journalctl -u ollama.service -n 50&lt;/code&gt;, the error come from &lt;code&gt; Error: mkdir /media/brianhuster/E: permission denied &lt;/code&gt; The problem is that &lt;code&gt;/media/brianhuster/E&lt;/code&gt; already exists, so why does it try to recreate it? Not to say that I have even change the owner of &lt;code&gt;/media/brianhuster/E&lt;/code&gt; to &lt;code&gt;ollama:ollama&lt;/code&gt;, and it still doesn't help &lt;code&gt; drwxrwxr-x 4 ollama ollama 4096 Thg 1 25 11:00 /media/brianhuster/E/ &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;OS&lt;/h3&gt; &lt;p&gt;Linux&lt;/p&gt; &lt;h3&gt;GPU&lt;/h3&gt; &lt;p&gt;Intel&lt;/p&gt; &lt;h3&gt;CPU&lt;/h3&gt; &lt;p&gt;Intel&lt;/p&gt; &lt;h3&gt;Ollama version&lt;/h3&gt; &lt;p&gt;Warning: could not connect to a running Ollama instance Warning: client version is 0.5.7&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BrianHuster"&gt; /u/BrianHuster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9ewmy/help_ollama_cannot_start_because_it_try_to_create/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9ewmy/help_ollama_cannot_start_because_it_try_to_create/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9ewmy/help_ollama_cannot_start_because_it_try_to_create/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T04:20:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9hs4z</id>
    <title>Book Translation using ollama</title>
    <updated>2025-01-25T07:27:10+00:00</updated>
    <author>
      <name>/u/Hefty_Cup_8160</name>
      <uri>https://old.reddit.com/user/Hefty_Cup_8160</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have ollama and openwebui installed on my PC and I'm trying to figure out how to use ollama model to translate a book of epub or pdf. I know that I can use some ocr or pandoc to convert those books into markdown first, but what should i do next? it doesn't seem like there's a feature in webui that allows me to translate a very long paragraph (the whole book). Is there a tool like Open WebUI that allows me to do that? Thanks in advance for any tips&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hefty_Cup_8160"&gt; /u/Hefty_Cup_8160 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9hs4z/book_translation_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9hs4z/book_translation_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9hs4z/book_translation_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T07:27:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9hh2l</id>
    <title>I run ollama on window cmds Is there any easy to use webui?</title>
    <updated>2025-01-25T07:04:31+00:00</updated>
    <author>
      <name>/u/labdogeth</name>
      <uri>https://old.reddit.com/user/labdogeth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hate cmd. Is there any easy to use webui?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/labdogeth"&gt; /u/labdogeth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9hh2l/i_run_ollama_on_window_cmds_is_there_any_easy_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9hh2l/i_run_ollama_on_window_cmds_is_there_any_easy_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9hh2l/i_run_ollama_on_window_cmds_is_there_any_easy_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T07:04:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i97odo</id>
    <title>DataBridge: Local, Modular, fully open source RAG System (Now easier than ever to get started!)</title>
    <updated>2025-01-24T22:18:31+00:00</updated>
    <author>
      <name>/u/yes-no-maybe_idk</name>
      <uri>https://old.reddit.com/user/yes-no-maybe_idk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I'm back with an exciting update for &lt;strong&gt;DataBridge&lt;/strong&gt;, the open-source, fully local, multimodal RAG system you've been supporting so generously. Thanks to your amazing feedback, we've made significant improvements, especially around &lt;strong&gt;Docker support&lt;/strong&gt; to make getting started easier than ever!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What‚Äôs New?&lt;/strong&gt;&lt;br /&gt; üì¶ &lt;strong&gt;Docker Support&lt;/strong&gt; ‚Äì The most requested feature is here! Now, you can spin up DataBridge effortlessly.&lt;br /&gt; ‚ö° &lt;strong&gt;CAG (Cache Augmented Generation)&lt;/strong&gt; ‚Äì Coming very, very soon to boost efficiency (you can explore the CAG branch to try it out today).&lt;br /&gt; üåê &lt;strong&gt;Graph RAG&lt;/strong&gt; ‚Äì On the way, stay tuned for exciting updates!&lt;br /&gt; üìä &lt;strong&gt;Evaluations and Comparisons&lt;/strong&gt; ‚Äì We‚Äôre working on adding benchmarking to help you compare various setups.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;New Video:&lt;/strong&gt;&lt;br /&gt; I‚Äôve put together a detailed walkthrough that covers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Installation &amp;amp; Setup&lt;/strong&gt; ‚Äì Whether you're using Docker or manual installation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Basic Ingestion &amp;amp; Querying&lt;/strong&gt; ‚Äì Learn how to quickly bring your data into DataBridge.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Shell &amp;amp; UI Demo&lt;/strong&gt; ‚Äì See DataBridge in action with both CLI and UI components.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Component Swapping&lt;/strong&gt; ‚Äì Easily switch completion models (e.g., from LLaMA to OpenAI).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üëâ &lt;a href="https://www.youtube.com/watch?v=__Kpt7tVQ6k&amp;amp;t=7s"&gt;&lt;strong&gt;Watch the video here&lt;/strong&gt;&lt;/a&gt; üëà&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Looking for:&lt;/strong&gt;&lt;br /&gt; - Your thoughts and feedback&lt;br /&gt; - Feature requests and use cases&lt;br /&gt; - Bug reports&lt;br /&gt; - Contributors to join the journey&lt;/p&gt; &lt;p&gt;A huge thanks to the community for your continued support and enthusiasm. Your feedback has been invaluable in shaping DataBridge.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;br /&gt; üîó GitHub: &lt;a href="https://github.com/databridge-org/databridge-core"&gt;https://github.com/databridge-org/databridge-core&lt;/a&gt;&lt;br /&gt; üìñ Docs: &lt;a href="https://databridge.gitbook.io/databridge-docs"&gt;https://databridge.gitbook.io/databridge-docs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PS: I used DataBridge and gpt4 to help me structure this post.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yes-no-maybe_idk"&gt; /u/yes-no-maybe_idk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i97odo/databridge_local_modular_fully_open_source_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i97odo/databridge_local_modular_fully_open_source_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i97odo/databridge_local_modular_fully_open_source_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T22:18:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i94pn4</id>
    <title>How I fixed R1 from being a whiney bitch</title>
    <updated>2025-01-24T20:10:37+00:00</updated>
    <author>
      <name>/u/redonculous</name>
      <uri>https://old.reddit.com/user/redonculous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you find R1's thoughts are whiney and lacking self confidence?&lt;br /&gt; Do you find it wasting tokens second guessing itself? &lt;/p&gt; &lt;p&gt;Simply add this to the end of your prompt for much more concise and confident output.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;You are very knowledgeable. An expert. Think and respond with confidence. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In my testing it really works! I'd be happy to hear how it responds for you guys too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redonculous"&gt; /u/redonculous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i94pn4/how_i_fixed_r1_from_being_a_whiney_bitch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i94pn4/how_i_fixed_r1_from_being_a_whiney_bitch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i94pn4/how_i_fixed_r1_from_being_a_whiney_bitch/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T20:10:37+00:00</published>
  </entry>
</feed>
