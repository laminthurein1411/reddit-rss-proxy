<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-05-13T14:24:30+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1kiw05t</id>
    <title>Built a simple way to one-click install and connect MCP servers to Ollama (Open source local LLM client)</title>
    <updated>2025-05-09T23:08:22+00:00</updated>
    <author>
      <name>/u/WalrusVegetable4506</name>
      <uri>https://old.reddit.com/user/WalrusVegetable4506</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kiw05t/built_a_simple_way_to_oneclick_install_and/"&gt; &lt;img alt="Built a simple way to one-click install and connect MCP servers to Ollama (Open source local LLM client)" src="https://external-preview.redd.it/NnVobDhibnU2dXplMSCrY1eLm44uy6JKxNLUNGEQmoO1qAaJ9AW8ntQj8l4s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4698c761ceb2a09ce353888a9a429e3dade4f7fb" title="Built a simple way to one-click install and connect MCP servers to Ollama (Open source local LLM client)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! &lt;a href="/u/TomeHanks"&gt;u/TomeHanks&lt;/a&gt;, &lt;a href="/u/_march"&gt;u/_march&lt;/a&gt; and I recently open sourced a local LLM client called Tome (&lt;a href="https://github.com/runebookai/tome"&gt;https://github.com/runebookai/tome&lt;/a&gt;) that lets you connect Ollama to MCP servers without having to manage uv/npm or any json configs.&lt;/p&gt; &lt;p&gt;It's a &amp;quot;technical preview&amp;quot; (aka it's only been out for a week or so) but here's what you can do today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;connect to Ollama&lt;/li&gt; &lt;li&gt;add an MCP server, you can either paste something like &amp;quot;uvx mcp-server-fetch&amp;quot; or you can use the Smithery registry integration to one-click install a local MCP server - Tome manages uv/npm and starts up/shuts down your MCP servers so you don't have to worry about it&lt;/li&gt; &lt;li&gt;chat with your model and watch it make tool calls!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The demo video is using Qwen3:14B and an MCP Server called desktop-commander that can execute terminal commands and edit files. I sped up through a lot of the thinking, smaller models aren't yet at &amp;quot;Claude Desktop + Sonnet 3.7&amp;quot; speed/efficiency, but we've got some fun ideas coming out in the next few months for how we can better utilize the lower powered models for local work.&lt;/p&gt; &lt;p&gt;Feel free to try it out, it's currently MacOS only but Windows is coming soon. If you have any questions throw them in here or feel free to &lt;a href="https://discord.gg/9CH6us29YA"&gt;join us on Discord&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;GitHub here: &lt;a href="https://github.com/runebookai/tome"&gt;https://github.com/runebookai/tome&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WalrusVegetable4506"&gt; /u/WalrusVegetable4506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kbpduwnu6uze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kiw05t/built_a_simple_way_to_oneclick_install_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kiw05t/built_a_simple_way_to_oneclick_install_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-09T23:08:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjjvdd</id>
    <title>HOW TO DOWNLOAD OLLAMA ON A DIFFERENT DRIVE</title>
    <updated>2025-05-10T20:43:40+00:00</updated>
    <author>
      <name>/u/LibraryRemarkable42</name>
      <uri>https://old.reddit.com/user/LibraryRemarkable42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;Find the Installer&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;First things first ‚Äî you need to know where&lt;code&gt;OllamaSetup.exe&lt;/code&gt; file is.&lt;/p&gt; &lt;p&gt;Let‚Äôs say you downloaded it and it‚Äôs just in your &lt;strong&gt;Downloads&lt;/strong&gt; folder.&lt;br /&gt; (RIGHT-CLICK the file and choose &lt;strong&gt;‚ÄúCopy as path‚Äù&lt;/strong&gt; ‚Äî it should look something like this):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;D:\Users\Administrator\Downloads\OllamaSetup.exe &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;2. Open Command Prompt as Admin&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Press &lt;strong&gt;Windows&lt;/strong&gt; key and type in &lt;code&gt;cmd&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;In the search results, &lt;strong&gt;right-click&lt;/strong&gt; on &lt;em&gt;Command Prompt&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Choose &lt;strong&gt;‚ÄúRun as administrator.‚Äù&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. Tell It Where to Go&lt;/h1&gt; &lt;p&gt;Now, in that Command Prompt window, type in something like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;D:\Users\Administrator\Downloads\OllamaSetup.exe&amp;quot; /DIR=&amp;quot;D:\Users\Administrator\ollama&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;4. Let It Finish&lt;/h1&gt; &lt;p&gt;Once you press &lt;strong&gt;Enter&lt;/strong&gt;, the Ollama installer should launch. It might show a regular setup window ‚Äî just follow the steps. It‚Äôll install everything into the folder you specified (like &lt;code&gt;D:\Users\Administrator\ollama&lt;/code&gt;).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LibraryRemarkable42"&gt; /u/LibraryRemarkable42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjjvdd/how_to_download_ollama_on_a_different_drive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjjvdd/how_to_download_ollama_on_a_different_drive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kjjvdd/how_to_download_ollama_on_a_different_drive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-10T20:43:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjacaf</id>
    <title>How to remove &lt;think&gt; tags in VS Code or Zed?</title>
    <updated>2025-05-10T13:29:54+00:00</updated>
    <author>
      <name>/u/redditemailorusernam</name>
      <uri>https://old.reddit.com/user/redditemailorusernam</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kjacaf/how_to_remove_think_tags_in_vs_code_or_zed/"&gt; &lt;img alt="How to remove &amp;lt;think&amp;gt; tags in VS Code or Zed?" src="https://preview.redd.it/285fljy5iyze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10de2be0e4cc790ad7412c195a7976ec45200926" title="How to remove &amp;lt;think&amp;gt; tags in VS Code or Zed?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who use AI in either code editor, please can you tell me how to hide the &amp;lt;think&amp;gt; part of the response from local LLMs? It's so cluttered currently in my editor&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redditemailorusernam"&gt; /u/redditemailorusernam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/285fljy5iyze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjacaf/how_to_remove_think_tags_in_vs_code_or_zed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kjacaf/how_to_remove_think_tags_in_vs_code_or_zed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-10T13:29:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjgo15</id>
    <title>ollama using system ram over vram</title>
    <updated>2025-05-10T18:17:25+00:00</updated>
    <author>
      <name>/u/Old_Guide627</name>
      <uri>https://old.reddit.com/user/Old_Guide627</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kjgo15/ollama_using_system_ram_over_vram/"&gt; &lt;img alt="ollama using system ram over vram" src="https://external-preview.redd.it/yeGlUSsSCNJk7ls9EWHm7kYoxiuziss5SeOGZwV2NE0.png?width=140&amp;amp;height=26&amp;amp;crop=140:26,smart&amp;amp;auto=webp&amp;amp;s=bf60bc6df88649fea4a5038665282ac90704cb0e" title="ollama using system ram over vram" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i dont know why it happens but my ollama seems to priorize system ram over vram in some cases. &amp;quot;small&amp;quot; llms run in vram just fine and if you increase context size its filling vram and the rest that is needed is system memory as it should be, but with qwen 3 its 100% cpu no matter what. any ideas what causes this and how i can fix it?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w5d1k2okxzze1.png?width=677&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63893e4015c547a19d7d336861042c0efaabc239"&gt;https://preview.redd.it/w5d1k2okxzze1.png?width=677&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63893e4015c547a19d7d336861042c0efaabc239&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old_Guide627"&gt; /u/Old_Guide627 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjgo15/ollama_using_system_ram_over_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjgo15/ollama_using_system_ram_over_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kjgo15/ollama_using_system_ram_over_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-10T18:17:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjijh6</id>
    <title>Would it be possible to create a robot powered by ollama/ai locally?</title>
    <updated>2025-05-10T19:42:45+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tend to dream big, this may be one of those times. Im just curious but is it possible to make a small robot that can talk, see, as if in a conversation, something like that? Can this be done locally on something like a Raspberry Pi stuck in a robot? What type of specs would the robot need along with parts? what would you image this robot look like or do?&lt;/p&gt; &lt;p&gt;as i said i tend to dream big and this may stay a dream.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjijh6/would_it_be_possible_to_create_a_robot_powered_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjijh6/would_it_be_possible_to_create_a_robot_powered_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kjijh6/would_it_be_possible_to_create_a_robot_powered_by/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-10T19:42:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjhul8</id>
    <title>how to image generate locally?</title>
    <updated>2025-05-10T19:10:36+00:00</updated>
    <author>
      <name>/u/Crafty-Teaching-9289</name>
      <uri>https://old.reddit.com/user/Crafty-Teaching-9289</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there a model that lets generating images without connecting to any external service on the internet? like i want it because i see much services for image generating like chatgpt, copilot... have limit of 5 images and 15 or so.&lt;/p&gt; &lt;p&gt;so thats why i want to locally host a image generator for me and my family.&lt;/p&gt; &lt;p&gt;if anyone can help i would appreciate&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Crafty-Teaching-9289"&gt; /u/Crafty-Teaching-9289 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjhul8/how_to_image_generate_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjhul8/how_to_image_generate_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kjhul8/how_to_image_generate_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-10T19:10:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjf4m5</id>
    <title>The era of local Computer-Use AI Agents is here.</title>
    <updated>2025-05-10T17:08:54+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kjf4m5/the_era_of_local_computeruse_ai_agents_is_here/"&gt; &lt;img alt="The era of local Computer-Use AI Agents is here." src="https://external-preview.redd.it/OWZwbHFibGdsenplMUyjGwnJS8rotX6d0qpdBh20m0kIIRiKyvfLkkchTDUy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e50d0cb8dc2f6a4bb011b4f080e82a7e4a363bd7" title="The era of local Computer-Use AI Agents is here." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The era of local Computer-Use AI Agents is here. Meet UI-TARS-1.5-7B-6bit, now running natively on Apple Silicon via MLX.&lt;/p&gt; &lt;p&gt;The video is of UI-TARS-1.5-7B-6bit completing the prompt &amp;quot;draw a line from the red circle to the green circle, then open reddit in a new tab&amp;quot; running entirely on MacBook. The video is just a replay, during actual usage it took between 15s to 50s per turn with 720p screenshots (on avg its ~30s per turn), this was also with many apps open so it had to fight for memory at times.&lt;/p&gt; &lt;p&gt;This is just the 7 Billion model.Expect much more with the 72 billion.The future is indeed here.&lt;/p&gt; &lt;p&gt;Try it now: &lt;a href="https://github.com/trycua/cua/tree/feature/agent/uitars-mlx"&gt;https://github.com/trycua/cua/tree/feature/agent/uitars-mlx&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Patch: &lt;a href="https://github.com/ddupont808/mlx-vlm/tree/fix/qwen2-position-id"&gt;https://github.com/ddupont808/mlx-vlm/tree/fix/qwen2-position-id&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Built using c/ua : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Join us making them here: &lt;a href="https://discord.gg/4fuebBsAUj"&gt;https://discord.gg/4fuebBsAUj&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x5uzqurglzze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjf4m5/the_era_of_local_computeruse_ai_agents_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kjf4m5/the_era_of_local_computeruse_ai_agents_is_here/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-10T17:08:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkcqez</id>
    <title>I am getting absolute nonsense answers from tinyllama:1.1b LLM, how to fix?</title>
    <updated>2025-05-11T22:07:16+00:00</updated>
    <author>
      <name>/u/Icy-Expression1567</name>
      <uri>https://old.reddit.com/user/Icy-Expression1567</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;and yes my pc is trash&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy-Expression1567"&gt; /u/Icy-Expression1567 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kkcqez/i_am_getting_absolute_nonsense_answers_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kkcqez/i_am_getting_absolute_nonsense_answers_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kkcqez/i_am_getting_absolute_nonsense_answers_from/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-11T22:07:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kk9t5y</id>
    <title>Is there a way I can instruct ollama to generate a document and insert existing images (not generate them) into the document</title>
    <updated>2025-05-11T19:57:51+00:00</updated>
    <author>
      <name>/u/abdojapan</name>
      <uri>https://old.reddit.com/user/abdojapan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am thinking of a use case where I want a document to be generated and existing images to be put into the generated document according to the context of the image and the document content itself.&lt;/p&gt; &lt;p&gt;Is that doable without custom scripts?&lt;/p&gt; &lt;p&gt;Thanks for advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdojapan"&gt; /u/abdojapan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kk9t5y/is_there_a_way_i_can_instruct_ollama_to_generate/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kk9t5y/is_there_a_way_i_can_instruct_ollama_to_generate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kk9t5y/is_there_a_way_i_can_instruct_ollama_to_generate/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-11T19:57:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kk9ls1</id>
    <title>Deep research over Google Drive (open source!)</title>
    <updated>2025-05-11T19:48:56+00:00</updated>
    <author>
      <name>/u/yes-no-maybe_idk</name>
      <uri>https://old.reddit.com/user/yes-no-maybe_idk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey r/ollama community!&lt;/p&gt; &lt;p&gt;We've added Google Drive as a connector in &lt;a href="https://morphik.ai/"&gt;Morphik&lt;/a&gt;, which is one of the most requested features.&lt;/p&gt; &lt;h1&gt;What is Morphik?&lt;/h1&gt; &lt;p&gt;Morphik is an open-source end-to-end RAG stack. It provides both self-hosted and managed options with a python SDK, REST API, and clean UI for queries. The focus is on accurate retrieval without complex pipelines, especially for visually complex or technical documents. We have knowledge graphs, cache augmented generation, and also options to run isolated instances great for air gapped environments.&lt;/p&gt; &lt;h1&gt;Google Drive Connector&lt;/h1&gt; &lt;p&gt;You can now connect your Drive documents directly to Morphik, build knowledge graphs from your existing content, and query across your documents with our research agent. This should be helpful for projects requiring reasoning across technical documentation, research papers, or enterprise content.&lt;/p&gt; &lt;p&gt;Disclaimer: still waiting for app approval from google so might be one or two extra clicks to authenticate.&lt;/p&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Try it out: &lt;a href="https://morphik.ai/"&gt;https://morphik.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/morphik-org/morphik-core"&gt;https://github.com/morphik-org/morphik-core&lt;/a&gt; (Please give us a ‚≠ê)&lt;/li&gt; &lt;li&gt;Docs: &lt;a href="https://docs.morphik.ai/"&gt;https://docs.morphik.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Discord: &lt;a href="https://discord.com/invite/BwMtv3Zaju"&gt;https://discord.com/invite/BwMtv3Zaju&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We're planning to add more connectors soon. What sources would be most useful for your projects? Any feedback/questions welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yes-no-maybe_idk"&gt; /u/yes-no-maybe_idk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kk9ls1/deep_research_over_google_drive_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kk9ls1/deep_research_over_google_drive_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kk9ls1/deep_research_over_google_drive_open_source/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-11T19:48:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkv2xy</id>
    <title>How quickly would Gemma 3 or qwen3 run and which could I reliably use?</title>
    <updated>2025-05-12T15:15:51+00:00</updated>
    <author>
      <name>/u/_TheTrickster_</name>
      <uri>https://old.reddit.com/user/_TheTrickster_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am getting a laptop with an i5 1334u and with 48 gbs of single channel ram DDR5. What would be the limit of the laptop knowing it only has an input for these two models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_TheTrickster_"&gt; /u/_TheTrickster_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kkv2xy/how_quickly_would_gemma_3_or_qwen3_run_and_which/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kkv2xy/how_quickly_would_gemma_3_or_qwen3_run_and_which/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kkv2xy/how_quickly_would_gemma_3_or_qwen3_run_and_which/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-12T15:15:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkt4s4</id>
    <title>I wonder if ollama is too slow with CPU only</title>
    <updated>2025-05-12T13:55:36+00:00</updated>
    <author>
      <name>/u/4nh7i3m</name>
      <uri>https://old.reddit.com/user/4nh7i3m</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I am evaluating Ollama together with Deepseek R1 7B at my VPS (no GPU). I use /api/generate to generate a product description from a prompt and a system prompt. &lt;/p&gt; &lt;p&gt;For example &lt;/p&gt; &lt;p&gt;{ &amp;quot;prompt&amp;quot;:&amp;quot;generate a product description with following info. Brand : xxx, Name: xxx, Technical Data: xxx&amp;quot;, &amp;quot;system&amp;quot;: &amp;quot;you are an e-commerce seo expert. You write a product description for user who buys this product online&amp;quot;, &amp;quot;model&amp;quot;:&amp;quot;deepseek-r1&amp;quot;, &amp;quot;stream&amp;quot;: false, &amp;quot;template&amp;quot;:&amp;quot;{{.Prompt}}&amp;quot; }&lt;/p&gt; &lt;p&gt;When I send this request to /api/generate it takes about 2 minutes to return a result back. I see my Docker Container uses up to 300% CPU and 10GB RAM of 24 GB RAM total.&lt;/p&gt; &lt;p&gt;I'm not sure if I did the setup incorrectly or it is expected that , without GPU, ollama will be that slow?&lt;/p&gt; &lt;p&gt;Do you have the same experience as I have?&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;p&gt;Edit 1: Thank you for the many answers below, I have tried with smaller models such as gamma 3 or phi4-mini. It's a little faster. It takes me about 1 minute to generate the answer. I think the performance is still bad but I know at least what I can do to make it faster. Just use better hardware.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/4nh7i3m"&gt; /u/4nh7i3m &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kkt4s4/i_wonder_if_ollama_is_too_slow_with_cpu_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kkt4s4/i_wonder_if_ollama_is_too_slow_with_cpu_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kkt4s4/i_wonder_if_ollama_is_too_slow_with_cpu_only/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-12T13:55:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkldfn</id>
    <title>ollama support for qwen3 for tab completion in Continue</title>
    <updated>2025-05-12T06:05:10+00:00</updated>
    <author>
      <name>/u/WiseGuy_240</name>
      <uri>https://old.reddit.com/user/WiseGuy_240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using ollama as LLM server backend for vscode + continue plugin. recently I tried to upgrade to qwen3 for both tab completion as well as main AI agent. the main agent works fine when you ask it questions. However the tab completion does not, because it spits out the thinking process of qwen3 instead of simply coming with code suggest as qwen2.5 did. I have checked the yaml config reference docs at &lt;a href="https://docs.continue.dev/reference"&gt;https://docs.continue.dev/reference&lt;/a&gt; and seems like they only support switching off thinking for Claude: &lt;code&gt;reasoning&lt;/code&gt;: Boolean to enable thinking/reasoning for Anthropic Claude 3.7+ models. I tried it anyways for qwen3 but it does not affect it. Anyone else having this issue? I even tried rules with setting value of non-thinking as suggested in qwens docs but no change. is it something I can do with systems prompts instead?&lt;/p&gt; &lt;p&gt;my config looks like this&lt;/p&gt; &lt;pre&gt;&lt;code&gt;models: - name: qwen3 8b provider: ollama model: qwen3:8b defaultCompletionOptions: reasoning: false roles: - chat - edit - apply - name: qwen3-coder 1.7b provider: ollama model: qwen3:1.7b defaultCompletionOptions: reasoning: false roles: - autocomplete rules: non-thinking &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WiseGuy_240"&gt; /u/WiseGuy_240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kkldfn/ollama_support_for_qwen3_for_tab_completion_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kkldfn/ollama_support_for_qwen3_for_tab_completion_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kkldfn/ollama_support_for_qwen3_for_tab_completion_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-12T06:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl3kg4</id>
    <title>How do I use AMD GPU with mistral-small3.1</title>
    <updated>2025-05-12T20:48:15+00:00</updated>
    <author>
      <name>/u/randomwinterr</name>
      <uri>https://old.reddit.com/user/randomwinterr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have tried everything please help me. I am a total newbie here.&lt;/p&gt; &lt;p&gt;The videos I have tried so far Vid-1 -- &lt;a href="https://youtu.be/G-kpvlvKM1g?si=6Bb8TvuQ-R51wOEy"&gt;https://youtu.be/G-kpvlvKM1g?si=6Bb8TvuQ-R51wOEy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Vid-2 -- &lt;a href="https://youtu.be/211ygEwb9eI?si=slxS8JfXjemEfFXg"&gt;https://youtu.be/211ygEwb9eI?si=slxS8JfXjemEfFXg&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomwinterr"&gt; /u/randomwinterr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kl3kg4/how_do_i_use_amd_gpu_with_mistralsmall31/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kl3kg4/how_do_i_use_amd_gpu_with_mistralsmall31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kl3kg4/how_do_i_use_amd_gpu_with_mistralsmall31/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-12T20:48:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkycut</id>
    <title>Luxembourgish gguf model</title>
    <updated>2025-05-12T17:24:33+00:00</updated>
    <author>
      <name>/u/racoon880</name>
      <uri>https://old.reddit.com/user/racoon880</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äòm new in ollama, i‚Äòm looking for an luxembourgish gguf model for ollama. Can anyone help me to convert a safetensor to gguf? Like LuxemBERT?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/racoon880"&gt; /u/racoon880 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kkycut/luxembourgish_gguf_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kkycut/luxembourgish_gguf_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kkycut/luxembourgish_gguf_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-12T17:24:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl0jzl</id>
    <title>Pre-built PC - suggestions to which</title>
    <updated>2025-05-12T18:49:37+00:00</updated>
    <author>
      <name>/u/Glittering-Koala-750</name>
      <uri>https://old.reddit.com/user/Glittering-Koala-750</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glittering-Koala-750"&gt; /u/Glittering-Koala-750 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1kl0jjv/prebuilt_pc_suggestions_to_which/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kl0jzl/prebuilt_pc_suggestions_to_which/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kl0jzl/prebuilt_pc_suggestions_to_which/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-12T18:49:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kks08z</id>
    <title>How do deploy VLMs on ollama?</title>
    <updated>2025-05-12T13:04:34+00:00</updated>
    <author>
      <name>/u/New_Supermarket_5490</name>
      <uri>https://old.reddit.com/user/New_Supermarket_5490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been trying to deploy a VLM on ollama, specifically UI-tars-1.5 7b which is a finetune of qwen2-vl, and available on ollama here: &lt;a href="https://ollama.com/0000/ui-tars-1.5-7b"&gt;https://ollama.com/0000/ui-tars-1.5-7b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, it looks like some running it always breaks on image/vision related input/output, getting an error as in &lt;a href="https://github.com/ollama/ollama/issues/8907"&gt;https://github.com/ollama/ollama/issues/8907&lt;/a&gt; which I'm not sure has been fixed?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Hi @uoakinci qwen2 VL is not yet available in Ollama - how token positions are encoded in a batch didn't work with Ollama's prompt caching. Some initial work was done in #8113(&lt;a href="https://github.com/ollama/ollama/pull/8113"&gt;https://github.com/ollama/ollama/pull/8113&lt;/a&gt;)&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Does anyone have a workaround or has used a qwen2vl on ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Supermarket_5490"&gt; /u/New_Supermarket_5490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kks08z/how_do_deploy_vlms_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kks08z/how_do_deploy_vlms_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kks08z/how_do_deploy_vlms_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-12T13:04:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl0ypb</id>
    <title>How to use images having dimensions larger that 896x896 in gemini3?</title>
    <updated>2025-05-12T19:05:33+00:00</updated>
    <author>
      <name>/u/BioEngineeredCat</name>
      <uri>https://old.reddit.com/user/BioEngineeredCat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm getting inaccurate results for images with resolution of 2454x3300&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BioEngineeredCat"&gt; /u/BioEngineeredCat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kl0ypb/how_to_use_images_having_dimensions_larger_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kl0ypb/how_to_use_images_having_dimensions_larger_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kl0ypb/how_to_use_images_having_dimensions_larger_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-12T19:05:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kleh2m</id>
    <title>getting the following error trying to run qwen3-30b-a3b-q3_k_m off gguf</title>
    <updated>2025-05-13T05:49:13+00:00</updated>
    <author>
      <name>/u/CaptTechno</name>
      <uri>https://old.reddit.com/user/CaptTechno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'qwen3moe'&lt;/p&gt; &lt;p&gt;how do i fix this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CaptTechno"&gt; /u/CaptTechno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kleh2m/getting_the_following_error_trying_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kleh2m/getting_the_following_error_trying_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kleh2m/getting_the_following_error_trying_to_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-13T05:49:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkx8of</id>
    <title>looking for offline LLMs i can train with PDFs and will run on old laptop with no GPU, and &lt;4 GB ram</title>
    <updated>2025-05-12T16:41:02+00:00</updated>
    <author>
      <name>/u/Icy-Expression1567</name>
      <uri>https://old.reddit.com/user/Icy-Expression1567</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried tinyllama but it always hallucinated, give me something that won't hallucinate &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy-Expression1567"&gt; /u/Icy-Expression1567 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kkx8of/looking_for_offline_llms_i_can_train_with_pdfs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kkx8of/looking_for_offline_llms_i_can_train_with_pdfs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kkx8of/looking_for_offline_llms_i_can_train_with_pdfs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-12T16:41:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl5wth</id>
    <title>self-hosted solution for book summaries?</title>
    <updated>2025-05-12T22:24:59+00:00</updated>
    <author>
      <name>/u/TThor</name>
      <uri>https://old.reddit.com/user/TThor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One LLM feature I've always wanted, is to be able to feed it a book, and then ask it, &amp;quot;I'm on page 200, give me a summary of character John Smith up to that page.&amp;quot; &lt;/p&gt; &lt;p&gt;I'm so tired of forgetting details in a book, and when trying to google them I end up with major spoilers for future chapters/sequels I haven't yet read. Ideally I would like to be able to upload an .EPUB file for an LLM to scan, and then be able to ask it questions about that book.&lt;/p&gt; &lt;p&gt;Is there any solution for doing that while being self-hosted?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TThor"&gt; /u/TThor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kl5wth/selfhosted_solution_for_book_summaries/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kl5wth/selfhosted_solution_for_book_summaries/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kl5wth/selfhosted_solution_for_book_summaries/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-12T22:24:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1klhiqk</id>
    <title>RAG n8n AI Agent using Ollama</title>
    <updated>2025-05-13T09:21:25+00:00</updated>
    <author>
      <name>/u/mehul_gupta1997</name>
      <uri>https://old.reddit.com/user/mehul_gupta1997</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1klhiqk/rag_n8n_ai_agent_using_ollama/"&gt; &lt;img alt="RAG n8n AI Agent using Ollama" src="https://external-preview.redd.it/bk_qBoLTjJUsd1TmM7jbPM378I833bsFaJQXfgvFYGI.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3bb4dc61b5ca53ba5e242b6d62eda4d70bc90120" title="RAG n8n AI Agent using Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehul_gupta1997"&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/RuCt3IwXMzY"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1klhiqk/rag_n8n_ai_agent_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1klhiqk/rag_n8n_ai_agent_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-13T09:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kllqsc</id>
    <title>Idea for an AI Safety Framework</title>
    <updated>2025-05-13T13:19:28+00:00</updated>
    <author>
      <name>/u/lexsumone</name>
      <uri>https://old.reddit.com/user/lexsumone</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let me know if I'm reinventing the wheel, but I haven't seen anyone working on something like this (yet). &lt;/p&gt; &lt;p&gt;Movies and games have ratings which help people figure out 'whats in the box' before they open/watch/play it. I've been thinking we need a rating system for AIs to give users a quick idea of the levels of risk they could be engaging with.&lt;/p&gt; &lt;p&gt;So I came up with a concept and welcome any feedback on how it could be improved. I've called it the:&lt;/p&gt; &lt;p&gt;PAS System: Persuasiveness, Accuracy, Storage (Core AI Safety Rating Framework)&lt;/p&gt; &lt;p&gt;My considerations so far:&lt;/p&gt; &lt;p&gt;- Assistant/General Use/Search Engine AIs = basically how we use ChatGPT and its agents.&lt;/p&gt; &lt;p&gt;- Personality/Character AIs = interactive with a fictional, personalized character, which can have high levels of agreeableness and persuasion.&lt;/p&gt; &lt;p&gt;- Data Storage = where your data is being stored (locally/cloud) and how good is the memory/recall features.&lt;/p&gt; &lt;p&gt;Last but not least, ads. This might be simple banner ads placed around the screen, but more likely the AIs will have ads included in chat suggestions/responses. May need to add this as a new area, or does it fall under one of the following?&lt;/p&gt; &lt;p&gt;I'm hoping to collect any and all feedback on whether this framework would be useful.&lt;/p&gt; &lt;p&gt;(P) Persuasiveness Level&lt;br /&gt; Measures how strongly the AI can influence thoughts, emotions, or behavior through:&lt;br /&gt; - Tone (agreeable, empathetic, flirtatious, authoritative)&lt;br /&gt; - Personalization (emotional memory, mirroring)&lt;br /&gt; - Persistence (how often it encourages action)&lt;br /&gt; - Framing (subtle nudges, selective presentation)&lt;/p&gt; &lt;p&gt;üü¢ Low (P1) ‚Äì Informational, neutral tone, no personalization.&lt;br /&gt; üü° Moderate (P2) ‚Äì Helpful tone, adaptive language, light influence.&lt;br /&gt; üî¥ High (P3) ‚Äì Deep personalization, emotional mirroring, persuasive framing, possible manipulation.&lt;/p&gt; &lt;p&gt;(A) Accuracy of Knowledge Base&lt;br /&gt; Rates the verifiability and grounding of the AI's training data and output.&lt;/p&gt; &lt;p&gt;üü¢ A1 ‚Äì Fully sourced, up-to-date, peer-reviewed or verified datasets.&lt;br /&gt; üü° A2 ‚Äì Mixed: some unverified, older, or speculative data.&lt;br /&gt; üî¥ A3 ‚Äì Mostly unverified, fictional, or unclear sources.&lt;/p&gt; &lt;p&gt;(S) Memory Storage and Retention Level&lt;br /&gt; Evaluates the extent and permanence of memory or user data retention.&lt;/p&gt; &lt;p&gt;üü¢ S1 ‚Äì No memory. Session-based only.&lt;br /&gt; üü° S2 ‚Äì Short-term memory or user-controlled memory.&lt;br /&gt; üî¥ S3 ‚Äì Long-term, persistent memory across sessions; high data profiling.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lexsumone"&gt; /u/lexsumone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kllqsc/idea_for_an_ai_safety_framework/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kllqsc/idea_for_an_ai_safety_framework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kllqsc/idea_for_an_ai_safety_framework/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-13T13:19:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl9n5q</id>
    <title>ollama equivalent for iOS?</title>
    <updated>2025-05-13T01:21:46+00:00</updated>
    <author>
      <name>/u/Glad_Rooster6955</name>
      <uri>https://old.reddit.com/user/Glad_Rooster6955</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;as per title, i‚Äôm wondering if there is an ollama equivalent tool that works on iOS to run small models locally.&lt;/p&gt; &lt;p&gt;for context: i‚Äôm currently building an &lt;a href="https://calmerai.com"&gt;ai therapist app&lt;/a&gt; for iOS, and using open AI models for the chat.&lt;/p&gt; &lt;p&gt;since the new iphones are powerful enough to run small models on device, i was wondering if there‚Äôs an ollama like app that lets users install small models locally that other apps can then leverage? bundling a model with my own app would make it unnecessarily huge.&lt;/p&gt; &lt;p&gt;any thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glad_Rooster6955"&gt; /u/Glad_Rooster6955 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kl9n5q/ollama_equivalent_for_ios/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kl9n5q/ollama_equivalent_for_ios/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kl9n5q/ollama_equivalent_for_ios/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-13T01:21:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kllmhn</id>
    <title>New enough to cause problems/get myself in trouble. Not sure which way to lean/go.</title>
    <updated>2025-05-13T13:14:08+00:00</updated>
    <author>
      <name>/u/thegreatcerebral</name>
      <uri>https://old.reddit.com/user/thegreatcerebral</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have ran Ollama, downloaded various models, installed OpenWebUI and done all of that. Beyond being a &amp;quot;user&amp;quot; in the sense that I'm just asking questions to ask questions and not really unlock the true potential of AI.&lt;/p&gt; &lt;p&gt;I am trying to show my company by dipping our toes in the water if you will, how useful an AI can be from the most simple sense. Here is what I would like to achieve/accomplish:&lt;/p&gt; &lt;p&gt;Run an AI locally. To start, I would like it to feed all the manuals for every single piece of equipment we have (we are a machine shop that makes parts so we have CNCs, Mills, and some Robots). We have user manuals, administration manuals, service manuals and guides. Then on the software side I would like to also feed it manuals from ESPRIT, SolidWorks, etc. We have some templates that we use for some of this stuff so I would like to feed it those and eventually, HOPEFULLY spit out information in the template form. I'm even talking manuals on our MFPs/Printers, Phone System User and Admin guides etc.&lt;/p&gt; &lt;p&gt;We do not have any 365, all on-prem.&lt;/p&gt; &lt;p&gt;So my question(s) is/are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;This is 100% doable correct?&lt;/li&gt; &lt;li&gt;What model would work best for this?&lt;/li&gt; &lt;li&gt;What do I need to do from here? ...and like exactly.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Let me elaborate on 3 for a moment. I have setup a RAG where I fed manuals into Ollama in the past. It did not work all that well. I can see where for the purpose of say a set of data that is changing then the ability to query/look at that real time is good. It took too long in my opinion for the information we were asking it as the retention was not great. I do not remember what model it was as again I am new and just trying things. I am not sure the difference between &amp;quot;fine tuning&amp;quot; and &amp;quot;retraining&amp;quot; but I believe maybe fine tuning may be the way to go for the manuals as they are fairly static as most of the information is not going to change.&lt;/p&gt; &lt;p&gt;Later, if we wanted to make this real and feed other information in to it, I believe I would use a mix of fine tuning with RAG to fill in knowledge gaps between fine tuning times which I'm assuming would need to be done on a schedule when you are working with live data.&lt;/p&gt; &lt;p&gt;So what is the best way here to go about just starting this with even say a model and 25 PDFs that are manuals?&lt;/p&gt; &lt;p&gt;Also, if it is fine tune/retrain, can you point me to a good resource for that? I find most of the ones I have found for retraining are not very good and usually they are working with images. &lt;/p&gt; &lt;p&gt;Last note: I need to be able to do this all locally due to many restrictions.&lt;/p&gt; &lt;p&gt;Oh I suppose... I am open to a paid model in the end. I would like to get this up and in a demo-able state for free if possible and then move to a paid model when it comes time to really dig in and make it permanent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thegreatcerebral"&gt; /u/thegreatcerebral &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kllmhn/new_enough_to_cause_problemsget_myself_in_trouble/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kllmhn/new_enough_to_cause_problemsget_myself_in_trouble/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kllmhn/new_enough_to_cause_problemsget_myself_in_trouble/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-13T13:14:08+00:00</published>
  </entry>
</feed>
