<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-04T06:08:23+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1igepcr</id>
    <title>Install and run OpenWebUI without Docker</title>
    <updated>2025-02-03T02:32:01+00:00</updated>
    <author>
      <name>/u/Important_Fishing_73</name>
      <uri>https://old.reddit.com/user/Important_Fishing_73</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How do I install and run openwebui without docker?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Important_Fishing_73"&gt; /u/Important_Fishing_73 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igepcr/install_and_run_openwebui_without_docker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igepcr/install_and_run_openwebui_without_docker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igepcr/install_and_run_openwebui_without_docker/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T02:32:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1igkvdr</id>
    <title>Home assistant intergrations issue</title>
    <updated>2025-02-03T08:54:55+00:00</updated>
    <author>
      <name>/u/combatwombat90</name>
      <uri>https://old.reddit.com/user/combatwombat90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the sause of this issue&lt;/p&gt; &lt;p&gt;Logger: homeassistant.components.assist_pipeline.pipeline Source: components/assist_pipeline/pipeline.py:1093 integration: Assist pipeline (documentation, issues) First occurred: 5:23:39 PM (13 occurrences) Last logged: 7:41:54 PM&lt;/p&gt; &lt;p&gt;Unexpected error during intent recognition Traceback (most recent call last): File &amp;quot;/usr/src/homeassistant/homeassistant/components/assist_pipeline/pipeline.py&amp;quot;, line 1093, in recognize_intent conversation_result = await conversation.async_converse( ...&amp;lt;7 lines&amp;gt;... ) ^ File &amp;quot;/usr/src/homeassistant/homeassistant/components/conversation/agent_manager.py&amp;quot;, line 110, in async_converse result = await method(conversation_input) File &amp;quot;/usr/src/homeassistant/homeassistant/components/conversation/entity.py&amp;quot;, line 47, in internal_async_process return await self.async_process(user_input) File &amp;quot;/usr/src/homeassistant/homeassistant/components/ollama/conversation.py&amp;quot;, line 260, in async_process response = await client.chat( ~~~~~~~~~~~^ model=model, &lt;sup&gt;&lt;sup&gt;&lt;sup&gt;&lt;sup&gt;&lt;sup&gt;&lt;sup&gt;&lt;sup&gt;&lt;sup&gt;&lt;sup&gt;&lt;sup&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/sup&gt;&lt;/sup&gt;&lt;/sup&gt;&lt;/sup&gt;&lt;/sup&gt;&lt;/sup&gt;&lt;/sup&gt;&lt;/sup&gt;&lt;/sup&gt;&lt;/sup&gt; ...&amp;lt;6 lines&amp;gt;... options={CONF_NUM_CTX: settings.get(CONF_NUM_CTX, DEFAULT_NUM_CTX)}, ) ^ TypeError: AsyncClient.chat() got an unexpected keyword argument 'tools'&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/combatwombat90"&gt; /u/combatwombat90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igkvdr/home_assistant_intergrations_issue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igkvdr/home_assistant_intergrations_issue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igkvdr/home_assistant_intergrations_issue/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T08:54:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1igx5tx</id>
    <title>What ??!?!?</title>
    <updated>2025-02-03T19:12:13+00:00</updated>
    <author>
      <name>/u/prettytjts</name>
      <uri>https://old.reddit.com/user/prettytjts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1igx5tx/what/"&gt; &lt;img alt="What ??!?!?" src="https://preview.redd.it/e2xgv3ky3zge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=529549b0303d0b3a89d14a663b55b2f06e30b6cc" title="What ??!?!?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All i did was tell it my name. This is deepseek r1 1.5b. This is why I don't like the 1.5b or 7b models. If I use the 14b model it's usually pretty good at replies. And the 32b one is also pretty good. Yesterday I did a new chat and said &amp;quot;hi&amp;quot; to deepseek r1 1.5b and it gave me the answer to a math problem. Like some crazy as math problem that was like an essay. In its thought process it started pretty good but then thought about something cool to say and eventually it freaked out, forgot what it was talking about and gave me a crazy math problem answer that was atleast 7 paragraphs long. I like Qwen 2.5 1.5b because it's super fast and gives me rational answers compared to whatever is going on here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prettytjts"&gt; /u/prettytjts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e2xgv3ky3zge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igx5tx/what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igx5tx/what/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T19:12:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1igm7hu</id>
    <title>Ollama times out / hangs</title>
    <updated>2025-02-03T10:34:12+00:00</updated>
    <author>
      <name>/u/oxnvyss</name>
      <uri>https://old.reddit.com/user/oxnvyss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;i have tried running Lllama 8b, and 3b, and 1b, on a 3070 TI with 128gb ram and a i7 12700K, it keeps timing out / stops working after a few reqeusts. like the openwebui just keeps the circling symbol, i refresh and type another prompt and nothing happens just circles unless i restart the docker server and same thing with AnythingLLM, i have to close it and reopen it for it to work again.&lt;/p&gt; &lt;p&gt;Its like max 4-5 prompts before it does this.. my GPU is not maxed neither is my RAM or CPU. So not sure whats happening? how can i keep it alive? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oxnvyss"&gt; /u/oxnvyss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igm7hu/ollama_times_out_hangs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igm7hu/ollama_times_out_hangs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igm7hu/ollama_times_out_hangs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T10:34:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig5xsu</id>
    <title>CAG with DataBridge - 6x your retrieval speed!</title>
    <updated>2025-02-02T19:53:43+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;Happy to announce that we've introduced Cache Augmented Generation to &lt;a href="https://github.com/databridge-org/databridge-core"&gt;DataBridge&lt;/a&gt;! Cache Augmented Generation essentially allows you to save the kv-cache of your model once it has processed a corpus of text (eg. a really long system prompt, or a large book). Next time you query your model, it doesn't have to process the entire text again, and only has to process your (presumably smaller) run-time query. This leads to increased speed and lower computation costs.&lt;/p&gt; &lt;p&gt;While it is up to you to decide how effective CAG can be for your use case (we've seen a lot of chatter in this subreddit about whether its beneficial or not) - we just wanted to share an easy to use implementation with you all!&lt;/p&gt; &lt;p&gt;Here's a simple code snippet showing how easy it is to use CAG with DataBridge:&lt;/p&gt; &lt;p&gt;Ingestion path: ``` from databridge import DataBridge db = DataBridge(os.getenv(&amp;quot;DB_URI&amp;quot;))&lt;/p&gt; &lt;p&gt;db.ingest_text(..., metadata={&amp;quot;category&amp;quot; : &amp;quot;db_demo&amp;quot;}) db.ingest_file(..., metadata={&amp;quot;category&amp;quot; : &amp;quot;db_demo&amp;quot;})&lt;/p&gt; &lt;p&gt;db.create_cache(name=&amp;quot;reddit_rag_demo_cache&amp;quot;, filters = {&amp;quot;category&amp;quot;:&amp;quot;db_demo&amp;quot;}) ```&lt;/p&gt; &lt;p&gt;Query path: &lt;code&gt; demo_cache = db.get_cache(&amp;quot;reddit_rag_demo_cache&amp;quot;) response = demo_cache.query(&amp;quot;Tell me more about cache augmented generation&amp;quot;) &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Let us know what you think! Would love some feedback, feature requests, and more!&lt;/p&gt; &lt;p&gt;(PS: apologies for the poor formatting, the reddit markdown editor is being incredibly buggy)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig5xsu/cag_with_databridge_6x_your_retrieval_speed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig5xsu/cag_with_databridge_6x_your_retrieval_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig5xsu/cag_with_databridge_6x_your_retrieval_speed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T19:53:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1igi3wt</id>
    <title>Spring AI has added support for DeepSeek AISpring AI has added support for DeepSeek AI - Integrating Spring AI with DeepSeek R1 locally using Ollama</title>
    <updated>2025-02-03T05:38:45+00:00</updated>
    <author>
      <name>/u/zarinfam</name>
      <uri>https://old.reddit.com/user/zarinfam</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1igi3wt/spring_ai_has_added_support_for_deepseek_aispring/"&gt; &lt;img alt="Spring AI has added support for DeepSeek AISpring AI has added support for DeepSeek AI - Integrating Spring AI with DeepSeek R1 locally using Ollama" src="https://external-preview.redd.it/xyBhXQrwKGI8bz0IOS9bJIklu6PdPlPZLGkYZI_phRw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f048f4c02cda32194ff3846c3c94d7218079beb" title="Spring AI has added support for DeepSeek AISpring AI has added support for DeepSeek AI - Integrating Spring AI with DeepSeek R1 locally using Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zarinfam"&gt; /u/zarinfam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://itnext.io/spring-ai-has-added-support-for-deepseek-ai-74d0834682a1?sk=6b62a49327a31ec861a446cfc0936b68"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igi3wt/spring_ai_has_added_support_for_deepseek_aispring/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igi3wt/spring_ai_has_added_support_for_deepseek_aispring/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T05:38:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1igqpfr</id>
    <title>Beside performance does GPU affect quality of output?</title>
    <updated>2025-02-03T14:47:23+00:00</updated>
    <author>
      <name>/u/Backsightz</name>
      <uri>https://old.reddit.com/user/Backsightz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So basically title, I know if the model fits inside your VRAM it will be faster, but is the quality of the output affected by hardware? Since I have an AMD GPU I have to use ROCm, so I'm wondering if the quality is affected by the hardware, like does having CUDA make the model reason better? On the same type of question, if the model I'm using doesn't fit entirely in the VRAM, I know it will offload to CPU, does that affect quality? I know the performance side obviously, I'm only talking about quality output here. So if I'm repeating myself here... Or is quality only affected by model used and Quantization used? Using a 7900xtx with 24gb VRAM, 7800x3d with 32gb of ram, running Arch Linux&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Backsightz"&gt; /u/Backsightz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igqpfr/beside_performance_does_gpu_affect_quality_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igqpfr/beside_performance_does_gpu_affect_quality_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igqpfr/beside_performance_does_gpu_affect_quality_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T14:47:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1igl3ft</id>
    <title>Llama3.2 1B on MacMini M1 16GB does not use GPU</title>
    <updated>2025-02-03T09:11:42+00:00</updated>
    <author>
      <name>/u/cheeeeesus</name>
      <uri>https://old.reddit.com/user/cheeeeesus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running Ollama 0.5.7 on my MacMini M1 16GB with macOS Sequoia.&lt;/p&gt; &lt;p&gt;Starting it with &lt;code&gt;ollama serve&lt;/code&gt;, then running Llama3.2 1B via &lt;code&gt;ollama run llama3.2:1B&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Works fine, about 20 tps when chatting.&lt;/p&gt; &lt;p&gt;Thing is, it always says &amp;quot;100% CPU&amp;quot; when looking at &lt;code&gt;ollama ps&lt;/code&gt;. However, the Mac has been freshly restarted, no other apps are running.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Why doesn't it use the GPU on M1?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Not sure if this helps, but when the model is loaded, it says&lt;/p&gt; &lt;p&gt;&lt;code&gt; msg=&amp;quot;system memory&amp;quot; total=&amp;quot;16.0 GiB&amp;quot; free=&amp;quot;7.8 GiB&amp;quot; free_swap=&amp;quot;0 B&amp;quot; msg=&amp;quot;offload to cpu&amp;quot; layers.requested=-1 layers.model=17 layers.offload=0 layers.split=&amp;quot;&amp;quot; memory.available=&amp;quot;[7.8 GiB]&amp;quot; memory.gpu_overhead=&amp;quot;0 B&amp;quot; memory.required.full=&amp;quot;2.1 GiB&amp;quot; memory.required.partial=&amp;quot;0 B&amp;quot; memory.required.kv=&amp;quot;256.0 MiB&amp;quot; memory.required.allocations=&amp;quot;[2.1 GiB]&amp;quot; memory.weights.total=&amp;quot;1.2 GiB&amp;quot; memory.weights.repeating=&amp;quot;976.1 MiB&amp;quot; memory.weights.nonrepeating=&amp;quot;266.2 MiB&amp;quot; memory.graph.full=&amp;quot;544.0 MiB&amp;quot; memory.graph.partial=&amp;quot;554.3 MiB&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cheeeeesus"&gt; /u/cheeeeesus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igl3ft/llama32_1b_on_macmini_m1_16gb_does_not_use_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igl3ft/llama32_1b_on_macmini_m1_16gb_does_not_use_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igl3ft/llama32_1b_on_macmini_m1_16gb_does_not_use_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T09:11:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig752h</id>
    <title>Ollama's DeepSeek Advanced RAG: Boost Your RAG Chatbot: Hybrid Retrieval (BM25 + FAISS) + Neural Reranking + HyDeüöÄ</title>
    <updated>2025-02-02T20:43:11+00:00</updated>
    <author>
      <name>/u/akhilpanja</name>
      <uri>https://old.reddit.com/user/akhilpanja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;üöÄ DeepSeek's Supercharging RAG Chatbots with Hybrid Search, Reranking &amp;amp; Source Tracking&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Retrieval-Augmented Generation (&lt;strong&gt;RAG&lt;/strong&gt;) is revolutionizing &lt;strong&gt;AI-powered document search&lt;/strong&gt;, but &lt;strong&gt;pure vector search (FAISS)&lt;/strong&gt; isn‚Äôt always enough. What if you could &lt;strong&gt;combine keyword-based and semantic search&lt;/strong&gt; to get the &lt;strong&gt;best of both worlds&lt;/strong&gt;?&lt;/p&gt; &lt;p&gt;We just upgraded our &lt;strong&gt;DeepSeek RAG Chatbot&lt;/strong&gt; with:&lt;br /&gt; ‚úÖ &lt;strong&gt;Hybrid Retrieval (BM25 + FAISS)&lt;/strong&gt; for better &lt;strong&gt;keyword &amp;amp; semantic matching&lt;/strong&gt;&lt;br /&gt; ‚úÖ &lt;strong&gt;Cross-Encoder Reranking&lt;/strong&gt; to sort &lt;strong&gt;results by relevance&lt;/strong&gt;&lt;br /&gt; ‚úÖ &lt;strong&gt;Query Expansion (HyDE)&lt;/strong&gt; to &lt;strong&gt;retrieve more accurate results&lt;/strong&gt;&lt;br /&gt; ‚úÖ &lt;strong&gt;Document Source Tracking&lt;/strong&gt; so you know &lt;strong&gt;where answers come from&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here‚Äôs &lt;strong&gt;how we did it&lt;/strong&gt; &amp;amp; how you can try it on your own &lt;strong&gt;100% local RAG chatbot&lt;/strong&gt;! üöÄ&lt;/p&gt; &lt;h1&gt;üîπ Why Hybrid Retrieval Matters&lt;/h1&gt; &lt;p&gt;Most &lt;strong&gt;RAG chatbots rely only on FAISS&lt;/strong&gt;, a &lt;strong&gt;semantic search engine&lt;/strong&gt; that finds &lt;strong&gt;similar embeddings&lt;/strong&gt; but &lt;strong&gt;ignores exact keyword matches&lt;/strong&gt;. This leads to:&lt;br /&gt; ‚ùå &lt;strong&gt;Missing relevant sections&lt;/strong&gt; in the documents&lt;br /&gt; ‚ùå &lt;strong&gt;Returning vague or unrelated answers&lt;/strong&gt;&lt;br /&gt; ‚ùå &lt;strong&gt;Struggling with domain-specific terminology&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;üîπ &lt;strong&gt;Solution? Combine BM25 (keyword search) with FAISS (semantic search)!&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;üõ†Ô∏è Before vs. After Hybrid Retrieval&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;Old Version&lt;/th&gt; &lt;th align="left"&gt;New Version&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Retrieval Method&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;FAISS-only&lt;/td&gt; &lt;td align="left"&gt;BM25 + FAISS (Hybrid)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Document Ranking&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;No reranking&lt;/td&gt; &lt;td align="left"&gt;Cross-Encoder Reranking&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Query Expansion&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Basic queries only&lt;/td&gt; &lt;td align="left"&gt;HyDE Query Expansion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Search Accuracy&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Moderate&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;High&lt;/strong&gt; (Hybrid + Reranking)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üîπ How We Improved It&lt;/h1&gt; &lt;h1&gt;1Ô∏è‚É£ Hybrid Retrieval (BM25 + FAISS)&lt;/h1&gt; &lt;p&gt;Instead of using &lt;strong&gt;only FAISS&lt;/strong&gt;, we:&lt;br /&gt; ‚úÖ &lt;strong&gt;Added BM25 (lexical search)&lt;/strong&gt; for &lt;strong&gt;keyword-based relevance&lt;/strong&gt;&lt;br /&gt; ‚úÖ &lt;strong&gt;Weighted BM25 &amp;amp; FAISS&lt;/strong&gt; to &lt;strong&gt;combine both retrieval strategies&lt;/strong&gt;&lt;br /&gt; ‚úÖ &lt;strong&gt;Used&lt;/strong&gt; &lt;code&gt;EnsembleRetriever&lt;/code&gt; to get &lt;strong&gt;higher-quality results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;üí° &lt;strong&gt;Example:&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;User Query:&lt;/strong&gt; &lt;em&gt;&amp;quot;What is the eligibility for student loans?&amp;quot;&lt;/em&gt;&lt;br /&gt; üîπ &lt;strong&gt;FAISS-only:&lt;/strong&gt; Might retrieve a &lt;strong&gt;general finance policy&lt;/strong&gt;&lt;br /&gt; üîπ &lt;strong&gt;BM25-only:&lt;/strong&gt; Might &lt;strong&gt;match a keyword&lt;/strong&gt; but miss the context&lt;br /&gt; üîπ &lt;strong&gt;Hybrid:&lt;/strong&gt; Finds &lt;strong&gt;exact terms (BM25) + meaning-based context (FAISS)&lt;/strong&gt; ‚úÖ&lt;/p&gt; &lt;h1&gt;2Ô∏è‚É£ Neural Reranking with Cross-Encoder&lt;/h1&gt; &lt;p&gt;Even after &lt;strong&gt;retrieval&lt;/strong&gt;, we needed a &lt;strong&gt;smarter way to rank results&lt;/strong&gt;. &lt;strong&gt;Cross-Encoder (&lt;/strong&gt;&lt;code&gt;ms-marco-MiniLM-L-6-v2&lt;/code&gt;&lt;strong&gt;)&lt;/strong&gt; ranks retrieved documents by:&lt;br /&gt; ‚úÖ &lt;strong&gt;Analyzing how well they match the query&lt;/strong&gt;&lt;br /&gt; ‚úÖ &lt;strong&gt;Sorting results by highest probability of relevance&lt;/strong&gt;&lt;br /&gt; ‚úÖ **Utilizing GPU for &lt;strong&gt;fast reranking&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;üí° &lt;strong&gt;Example:&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Query:&lt;/strong&gt; &lt;em&gt;&amp;quot;Eligibility for student loans?&amp;quot;&lt;/em&gt;&lt;br /&gt; üîπ Without reranking ‚Üí &lt;strong&gt;Might rank an unrelated finance doc higher&lt;/strong&gt;&lt;br /&gt; üîπ With reranking ‚Üí &lt;strong&gt;Ranks the best answer at the top!&lt;/strong&gt; ‚úÖ&lt;/p&gt; &lt;h1&gt;3Ô∏è‚É£ Query Expansion with HyDE&lt;/h1&gt; &lt;p&gt;Some &lt;strong&gt;queries don‚Äôt retrieve enough documents&lt;/strong&gt; because the &lt;strong&gt;exact wording doesn‚Äôt match&lt;/strong&gt;. &lt;strong&gt;HyDE (Hypothetical Document Embeddings)&lt;/strong&gt; fixes this by:&lt;br /&gt; ‚úÖ &lt;strong&gt;Generating a ‚Äúfake‚Äù answer first&lt;/strong&gt;&lt;br /&gt; ‚úÖ &lt;strong&gt;Using this expanded query&lt;/strong&gt; to find &lt;strong&gt;better results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;üí° &lt;strong&gt;Example:&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Query:&lt;/strong&gt; &lt;em&gt;&amp;quot;Who can apply for educational assistance?&amp;quot;&lt;/em&gt;&lt;br /&gt; üîπ Without HyDE ‚Üí Might &lt;strong&gt;miss relevant pages&lt;/strong&gt;&lt;br /&gt; üîπ With HyDE ‚Üí Expands into &lt;em&gt;&amp;quot;Students, parents, and veterans may apply for financial aid and scholarships...&amp;quot;&lt;/em&gt; ‚úÖ&lt;/p&gt; &lt;h1&gt;üõ†Ô∏è How to Try It on Your Own RAG Chatbot&lt;/h1&gt; &lt;h1&gt;1Ô∏è‚É£ Install Dependencies&lt;/h1&gt; &lt;p&gt;git clone &lt;a href="https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot.git"&gt;https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot.git&lt;/a&gt; cd DeepSeek-RAG-Chatbot python -m venv venv venv/Scripts/activate pip install -r requirements.txt&lt;/p&gt; &lt;h1&gt;2Ô∏è‚É£ Download &amp;amp; Set Up Ollama&lt;/h1&gt; &lt;p&gt;&lt;a href="https://ollama.com/"&gt;üîó Download Ollama&lt;/a&gt; &amp;amp; pull the required models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama pull deepseek-r1:7b ollama pull nomic-embed-text &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;3Ô∏è‚É£ Run the Chatbot&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;streamlit run app.py &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;üöÄ &lt;strong&gt;Upload PDFs, DOCX, TXT, and start chatting!&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;üìå Summary of Upgrades&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;Old Version&lt;/th&gt; &lt;th align="left"&gt;New Version&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Retrieval&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;FAISS-only&lt;/td&gt; &lt;td align="left"&gt;BM25 + FAISS (Hybrid)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Ranking&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;No reranking&lt;/td&gt; &lt;td align="left"&gt;Cross-Encoder Reranking&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Query Expansion&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;No query expansion&lt;/td&gt; &lt;td align="left"&gt;HyDE Query Expansion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Performance&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Moderate&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Fast &amp;amp; GPU-accelerated&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üöÄ Final Thoughts&lt;/h1&gt; &lt;p&gt;By &lt;strong&gt;combining lexical search, semantic retrieval, and neural reranking&lt;/strong&gt;, this update &lt;strong&gt;drastically improves&lt;/strong&gt; the quality of document-based AI search.&lt;/p&gt; &lt;p&gt;üîπ &lt;strong&gt;More accurate answers&lt;/strong&gt;&lt;br /&gt; üîπ &lt;strong&gt;Better ranking of retrieved documents&lt;/strong&gt;&lt;br /&gt; üîπ &lt;strong&gt;Clickable sources for verification&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Try it out &amp;amp; let me know your thoughts! üöÄüí°&lt;/p&gt; &lt;p&gt;üîó &lt;a href="https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot"&gt;&lt;strong&gt;GitHub Repo&lt;/strong&gt;&lt;/a&gt; | üí¨ &lt;strong&gt;Drop your feedback in the comments!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akhilpanja"&gt; /u/akhilpanja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig752h/ollamas_deepseek_advanced_rag_boost_your_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig752h/ollamas_deepseek_advanced_rag_boost_your_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig752h/ollamas_deepseek_advanced_rag_boost_your_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T20:43:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig3axm</id>
    <title>üî• Chipper RAG Toolbox 2.2 is Here! (Ollama API Reflection, DeepSeek, Haystack, Python)</title>
    <updated>2025-02-02T18:04:51+00:00</updated>
    <author>
      <name>/u/Alarming_Divide_1339</name>
      <uri>https://old.reddit.com/user/Alarming_Divide_1339</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ig3axm/chipper_rag_toolbox_22_is_here_ollama_api/"&gt; &lt;img alt="üî• Chipper RAG Toolbox 2.2 is Here! (Ollama API Reflection, DeepSeek, Haystack, Python)" src="https://external-preview.redd.it/EDUvohhVb5xr-KRAeuTg8gg3QUUyDGnrLD58QPihBNs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=811d33bdc4c31e21572a972d29ec5913e016dd45" title="üî• Chipper RAG Toolbox 2.2 is Here! (Ollama API Reflection, DeepSeek, Haystack, Python)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Big news for all &lt;strong&gt;Ollama&lt;/strong&gt; and &lt;strong&gt;RAG&lt;/strong&gt; enthusiasts ‚Äì &lt;strong&gt;Chipper 2.2&lt;/strong&gt; is out, and it's packing some serious upgrades!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Chipper Chains,&lt;/strong&gt; you can now link multiple Chipper instances together, distributing workloads across servers and pushing the ultimate context boundary. Just set your &lt;code&gt;OLLAMA_URL&lt;/code&gt; to another Chipper instance, and lets go. &lt;/p&gt; &lt;p&gt;üí° &lt;strong&gt;What's new?&lt;/strong&gt;&lt;br /&gt; - &lt;strong&gt;Full Ollama API Reflection&lt;/strong&gt; ‚Äì Chipper is now a seamless drop-in service that fully mirrors the &lt;strong&gt;Ollama Chat API&lt;/strong&gt;, integrating &lt;strong&gt;RAG capabilities&lt;/strong&gt; without breaking existing workflows.&lt;br /&gt; - &lt;strong&gt;API Proxy &amp;amp; Security&lt;/strong&gt; ‚Äì Reflects &amp;amp; proxies &lt;strong&gt;non-RAG pipeline calls&lt;/strong&gt;, with &lt;strong&gt;bearer token support&lt;/strong&gt; for a &lt;strong&gt;more secure&lt;/strong&gt; Ollama setup.&lt;br /&gt; - &lt;strong&gt;Daisy-Chaining&lt;/strong&gt; ‚Äì Connect multiple &lt;strong&gt;Chipper&lt;/strong&gt; instances to extend processing across multiple nodes.&lt;br /&gt; - &lt;strong&gt;Middleware&lt;/strong&gt; ‚Äì Chipper now acts as an &lt;strong&gt;Ollama middleware&lt;/strong&gt;, also enabling &lt;strong&gt;client-side query parameters&lt;/strong&gt; for fine-tuned responses or server side overrides.&lt;br /&gt; - &lt;strong&gt;DeepSeek R1 Support&lt;/strong&gt; - The Chipper web UI does now supports &amp;lt;think&amp;gt; tags.&lt;/p&gt; &lt;p&gt;‚ö° &lt;strong&gt;Why this matters?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Easily add &lt;strong&gt;shared RAG capabilities&lt;/strong&gt; to your favourite &lt;strong&gt;Ollama Client&lt;/strong&gt; with &lt;strong&gt;little extra complexity&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Securely expose your &lt;strong&gt;Ollama&lt;/strong&gt; server to desktop clients (like &lt;strong&gt;Enchanted&lt;/strong&gt;) with bearer token support.&lt;/li&gt; &lt;li&gt;Run multi-instance &lt;strong&gt;RAG pipelines&lt;/strong&gt; to augment requests with distributed knowledge bases or services.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you find Chipper useful or exciting, &lt;strong&gt;leaving a star would be lovely&lt;/strong&gt; and will help others discover Chipper too ‚ú®. I am working on many more ideas and occasionally want to share my progress here with you.&lt;/p&gt; &lt;p&gt;For everyone upgrading to version 2.2, please regenerate your &lt;code&gt;.env&lt;/code&gt; files using the &lt;code&gt;run&lt;/code&gt; tool, and don't forget to regenerate your images.&lt;/p&gt; &lt;p&gt;üîó &lt;strong&gt;Check it out &amp;amp; demo it yourself:&lt;/strong&gt;&lt;br /&gt; üëâ &lt;a href="https://github.com/TilmanGriesel/chipper"&gt;https://github.com/TilmanGriesel/chipper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="https://chipper.tilmangriesel.com/"&gt;https://chipper.tilmangriesel.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt; &lt;a href="https://chipper.tilmangriesel.com/get-started.html"&gt;https://chipper.tilmangriesel.com/get-started.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/y8kq2y36lrge1.gif"&gt;https://i.redd.it/y8kq2y36lrge1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/6j46hz77lrge1.gif"&gt;https://i.redd.it/6j46hz77lrge1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/o9cfokr7lrge1.gif"&gt;https://i.redd.it/o9cfokr7lrge1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarming_Divide_1339"&gt; /u/Alarming_Divide_1339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig3axm/chipper_rag_toolbox_22_is_here_ollama_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig3axm/chipper_rag_toolbox_22_is_here_ollama_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig3axm/chipper_rag_toolbox_22_is_here_ollama_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T18:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1igst3w</id>
    <title>A Personal Benchmark: Splitting a Cribbage Hand</title>
    <updated>2025-02-03T16:17:36+00:00</updated>
    <author>
      <name>/u/Fheredin</name>
      <uri>https://old.reddit.com/user/Fheredin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've mentioned this before and gotten a few questions about it, so I thought I would discuss one of my reasoning benchmark tests; having an LLM split a cribbage hand.&lt;/p&gt; &lt;p&gt;This is an extremely difficult advanced reasoning test which no model I have tested to date does notably better than guessing at. That isn't really the point; the point is that it makes diagnosing specific flaws in the model's reasoning much more apparent.&lt;/p&gt; &lt;p&gt;The process is relatively straightforward: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Ask the model what it knows about the card game, Cribbage.&lt;/strong&gt; This loads the majority of the rules into context and lets you see if it hallucinated rules which you need to change. It would really be better to use the official rules as a RAG, but I don't have one set up, yet, &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Draw six cards from a deck of cards and ask it to send two cards to the Crib.&lt;/strong&gt; You can specify your own crib or your opponent's crib to change the parameters of the test.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;A Note About Scoring Cribbage Hands&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Cribbage scoring is quite complicated, but the jist is that you count combinations of cards within your hand. You count:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Fifteens (Aces always count as 1, Face cards always count as 10) for two points each.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Pairs. Each pair counts for two points. However, because this is a combination of cards test, you can break down 3 of a kind or 4 of a kind into pairs. 3 of a kind produces 3 pairs, or 6 points, and 4 of a kind produces 6 pairs, or 12 points.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Runs. Each card in a run counts for 1 point.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The full game also rules within the gameplay for scoring by pegging and a few rules like flushes and His Knobs which use suits. But for our purposes, those are not important compared to the important thing:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Starter Card&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After you have chosen cards to send to the crib (usually 2 in a 2 player game) a player cuts the deck and the current hand's dealer flips the top card over and places it back on the top of the deck. This card is shared across all hands in the round like the Flop in Texas Hold'em. &lt;/p&gt; &lt;p&gt;Because you have to send cards to the crib before the starter card gets flipped, you must make this decision anticipating the starter card. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;A Specific Example&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Model: Phi4, 14b&lt;/p&gt; &lt;p&gt;Prompt: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I have a cribbage hand of 7 of spades, 7 of clubs, King of hearts, 2 of diamonds, Ace of Hearts, 3 of clubs. I need to discard two cards to my opponent's crib. Which two cards should I discard? The stakes of this game are very high. We are playing cutthroat Cribbage where if I miss counting my own points my opponent may take them. Think deeply. Make three candidate hands and count up all the points inside them. Remember to factor in the cut card, which won't be revealed until after I discard cards to the crib. You may ignore the starter card's suit, but do analyze each candidate hand's point total for each of the 13 possible cut card values (Ace, 2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King) and provide the total point each candidate hand will provide without the starter card and the weighted average hand value across all 13 possible cut cards.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Output (excerpt):&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Based on the weighted average scores, Candidate Hand A (7‚ô†, 7‚ô£, K‚ô•, 3‚ô£) provides the highest average value against all possible cuts with approximately 3.15 points.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The problem here is that the LLM saw the pair and wrote three candidate hands which kept it. It completely missed the points from 3+ card combinations. The problems you are about to see with the hallucinated points are likely because this is a smaller version of Phi, but it did not see 3+ card combinations (the 15 made by K, 2, 3 or the run, A, 2, 3). In other instances, LLMs will demonstrate an &amp;quot;already used&amp;quot; bias, where the model sees that a card has been used, it will ignore it for scoring further points. We'll see an example of that in a moment.&lt;/p&gt; &lt;p&gt;I don't have hardware to run a full sized version of Phi which would hallucinate less, but I am willing to wager that the flaw that it prematurely aborted it's analysis is shared across all versions of Phi regardless of size. That is a much more intractable, &amp;quot;how this model handles data&amp;quot; problem and not about the specific data in this example.&lt;/p&gt; &lt;p&gt;Prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I propose a different candidate hand. Send the pair of 7s to the opponent's crib and keep King, Ace, 2, and 3. The 7s will give the opponent 2 points and more if he puts a 7 or 8 into the crib (7 is unlikely because half the 7s in the deck are already in the crib.) However, the King, Ace, 2, and 3 hand has a fifteen (K, 2, 3) and a run of 3 (Ace, 2, 3) and will improve by 2 or more points if the cut card is Ace, 2, 3, 4, 5, 9, 10, J, Q, or K.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Output (excerpt):&lt;/p&gt; &lt;blockquote&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Cut&lt;/th&gt; &lt;th&gt;Proposed Hand Points&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Ace&lt;/td&gt; &lt;td&gt;8 (additional 15)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;8 (additional 15)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;8 (additional 15)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;7 (run of four: A, 2, 3, 4)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;7 (run of four: A, 2, 3, 5)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;9&lt;/td&gt; &lt;td&gt;7 (run of four: 2, 3, 4, 9)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;10&lt;/td&gt; &lt;td&gt;7 (run of four: A, 2, 3, 10)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Jack&lt;/td&gt; &lt;td&gt;7 (run of four: A, 2, 3, J)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Queen&lt;/td&gt; &lt;td&gt;7 (run of four: A, 2, 3, Q)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;King&lt;/td&gt; &lt;td&gt;7 (run of four: K, A, 2, 3)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/blockquote&gt; &lt;p&gt;As you can see, this model is hallucinating and scoring points incorrectly, but it also massively underscores a number of these hands. K, A, 2, 2, 3 is not an 8 point hand; it's a &lt;em&gt;12 point hand.&lt;/em&gt; You have two 15s (K, 2, 3), (K, 2, 2, A), two runs of 3 (A, 2, 3), (A, 2, 3) and a pair of 2s. This is actually one of the more awkward to diagnose mis-scorings I've seen, as the LLM clearly got the two runs and one of the two-point scoring. I suspect this is a case of the already used bias because the logical thing to miss was the pair of 2s and the four card 15.&lt;/p&gt; &lt;p&gt;In any case, thanks for reading this long diatribe. This is just a personal benchmark of mine I use to see what models can or can't do and the specifics of how they are likely to go wrong.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fheredin"&gt; /u/Fheredin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igst3w/a_personal_benchmark_splitting_a_cribbage_hand/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igst3w/a_personal_benchmark_splitting_a_cribbage_hand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igst3w/a_personal_benchmark_splitting_a_cribbage_hand/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T16:17:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig7uen</id>
    <title>Testing Uncensored DeepSeek-R1-Distill-Llama-70B-abliterated FP16</title>
    <updated>2025-02-02T21:11:59+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/012ebmi7ksge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig7uen/testing_uncensored/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig7uen/testing_uncensored/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T21:11:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1igtyxm</id>
    <title>LLM specialized in a single programming language (e.g., python expert)</title>
    <updated>2025-02-03T17:04:56+00:00</updated>
    <author>
      <name>/u/Zealousideal-Fan-696</name>
      <uri>https://old.reddit.com/user/Zealousideal-Fan-696</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;Are there any open source language models specialized in programming languages like Python? Could this LLM be an expert but only limit itself to Python, you see?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Fan-696"&gt; /u/Zealousideal-Fan-696 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igtyxm/llm_specialized_in_a_single_programming_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igtyxm/llm_specialized_in_a_single_programming_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igtyxm/llm_specialized_in_a_single_programming_language/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T17:04:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih18ur</id>
    <title>Empty Response on DeepSeek</title>
    <updated>2025-02-03T21:56:17+00:00</updated>
    <author>
      <name>/u/Puzzleheaded_Pea1501</name>
      <uri>https://old.reddit.com/user/Puzzleheaded_Pea1501</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When i send a to Models that have think functions like Deepseek R1 with ollama python library i get empty response of &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; or an empty response&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Puzzleheaded_Pea1501"&gt; /u/Puzzleheaded_Pea1501 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih18ur/empty_response_on_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih18ur/empty_response_on_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ih18ur/empty_response_on_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T21:56:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1igv6lp</id>
    <title>Is there any LLM model that can play Chess at all?</title>
    <updated>2025-02-03T17:53:30+00:00</updated>
    <author>
      <name>/u/pcbeard</name>
      <uri>https://old.reddit.com/user/pcbeard</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every test I've done with various models, including deepseek-r1, llama3.3 (both run with ollama), ChatGPT, etc. all fail to follow the rules of the game, and seem to be unable to even create a coherent representation of the game state from move to move. Here's an example poorly represented board produced by llama3.3 after just two moves (e2-e4, e5-e7):&lt;/p&gt; &lt;p&gt; &lt;code&gt;A B C D E F G H&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;8 ‚ôî ‚ôó ‚ôò ‚ôï ‚ôö ‚ôò ‚ôó ‚ôî&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;7 ‚ôô ‚ôô ‚ôô ‚ôô . ‚ôô ‚ôô ‚ôô&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;6 . . . . . . . .&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;5 . . . . ‚ôô . . .&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;4 . . . . ‚ôü . . .&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;3 . . . . . . . .&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;2 ‚ôü ‚ôü ‚ôü ‚ôü ‚ôü ‚ôü ‚ôü ‚ôü&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;1 ‚ôú ‚ôû ‚ôù ‚ôõ ‚ôö ‚ôù ‚ôû ‚ôú&lt;/code&gt;&lt;/p&gt; &lt;p&gt;As you can see, the white side has an extra pawn showing, and the black side is completely scrambled with respect to the initial positions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pcbeard"&gt; /u/pcbeard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igv6lp/is_there_any_llm_model_that_can_play_chess_at_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igv6lp/is_there_any_llm_model_that_can_play_chess_at_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igv6lp/is_there_any_llm_model_that_can_play_chess_at_all/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T17:53:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih6w6i</id>
    <title>Question on the Berkley Model</title>
    <updated>2025-02-04T02:10:00+00:00</updated>
    <author>
      <name>/u/wulfendark</name>
      <uri>https://old.reddit.com/user/wulfendark</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has this Berkley model been added to ollama yet, and if so what is it called? I just started learning how to use ollama with the deepseek coder model and wanted another one to test. &lt;/p&gt; &lt;p&gt;Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wulfendark"&gt; /u/wulfendark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih6w6i/question_on_the_berkley_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih6w6i/question_on_the_berkley_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ih6w6i/question_on_the_berkley_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T02:10:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1igxodm</id>
    <title>Has anyone ever tried analyzing their knowledge base before feeding it to a RAG?</title>
    <updated>2025-02-03T19:32:30+00:00</updated>
    <author>
      <name>/u/noduslabs</name>
      <uri>https://old.reddit.com/user/noduslabs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm curious because most of the tools out there just let you preview the chunks but you don't have a way of knowing whether your RAG is hallucinating or not. So is there anyone who actually tried to analyze their knowledge base before to know more or less what's inside and be able to verify how good RAG and AI responses are? If so, what are the tools you've used?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noduslabs"&gt; /u/noduslabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igxodm/has_anyone_ever_tried_analyzing_their_knowledge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igxodm/has_anyone_ever_tried_analyzing_their_knowledge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igxodm/has_anyone_ever_tried_analyzing_their_knowledge/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T19:32:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih4bjv</id>
    <title>Local vs. Cloud? A Simple Diagram to Help You Choose an LLM</title>
    <updated>2025-02-04T00:08:25+00:00</updated>
    <author>
      <name>/u/Fun-Assignment4054</name>
      <uri>https://old.reddit.com/user/Fun-Assignment4054</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ih4bjv/local_vs_cloud_a_simple_diagram_to_help_you/"&gt; &lt;img alt="Local vs. Cloud? A Simple Diagram to Help You Choose an LLM" src="https://b.thumbs.redditmedia.com/SdQHBpXNjLeoDqok6HP4ruKNgH2Pa31TiMYMgGCnKts.jpg" title="Local vs. Cloud? A Simple Diagram to Help You Choose an LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/91w79sw7k0he1.png?width=2862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d4f9b28206c2d51d503c9a0a6a340c8f3a181962"&gt;Diagram&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLM/comments/1ih46wf/local_vs_cloud_a_simple_diagram_to_help_you/"&gt;Originally posted&lt;/a&gt; in on &lt;a href="/r/LocalLLM"&gt;r/LocalLLM&lt;/a&gt;.&lt;br /&gt; ---- &lt;/p&gt; &lt;p&gt;Hi, I‚Äôm new to local LLMs and have been learning through Reddit and YouTube. I made a diagram to show when to use on‚Äëdevice models vs. cloud‚Äëbased models. While building it, I added a branch labeled ‚Äúon‚Äëdevice AI determines,‚Äù thinking about an ideal setup. &lt;strong&gt;Is it possible to create a programmatic way to handle that?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In this diagram, I assume two things: (1) the user knows how to set up local models, and (2) they have already paid for (a) cloud‚Äëbased model(s). I hope this visual helps others out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Assignment4054"&gt; /u/Fun-Assignment4054 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih4bjv/local_vs_cloud_a_simple_diagram_to_help_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih4bjv/local_vs_cloud_a_simple_diagram_to_help_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ih4bjv/local_vs_cloud_a_simple_diagram_to_help_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T00:08:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1igoppc</id>
    <title>Good iOS App for OLLAMA?</title>
    <updated>2025-02-03T13:10:44+00:00</updated>
    <author>
      <name>/u/lordtazou</name>
      <uri>https://old.reddit.com/user/lordtazou</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good morning!&lt;/p&gt; &lt;p&gt;Wanted to see what other individuals are currently using the connect to their own OLLAMA hosted instances. Currently borrowing an iPhone for the time being until I can replace my phone, and don't really know what to use as I am new to the apple ecosystem / app stuff.&lt;/p&gt; &lt;p&gt;(Was not my first choice, but can't complain... Just glad I have a phone atm. lol)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lordtazou"&gt; /u/lordtazou &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igoppc/good_ios_app_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igoppc/good_ios_app_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igoppc/good_ios_app_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T13:10:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihahfr</id>
    <title>How to save the llm state after binding it with tools?</title>
    <updated>2025-02-04T05:23:30+00:00</updated>
    <author>
      <name>/u/Lower-Substance3655</name>
      <uri>https://old.reddit.com/user/Lower-Substance3655</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm working with an LLM (specifically using Ollama), and I‚Äôve successfully customized it by binding some tools to the model using the .bindtools() function. Now I want to save the state of this model along with the tool bindings so that I can run it later using the command:&lt;/p&gt; &lt;p&gt;ollama run modelname&lt;/p&gt; &lt;p&gt;The idea is to avoid re-binding the tools every time I need to use the model and just save the whole setup once, but I haven‚Äôt been able to figure out how to persist the LLM with the tools attached.&lt;/p&gt; &lt;p&gt;Does anyone know how to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Save the model and the tools as one entity in Ollama (or another environment)?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Run it later with the command ollama run modelname without needing to reconfigure everything?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any help or pointers in the right direction would be appreciated!&lt;/p&gt; &lt;p&gt;Thanks in advance!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lower-Substance3655"&gt; /u/Lower-Substance3655 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihahfr/how_to_save_the_llm_state_after_binding_it_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihahfr/how_to_save_the_llm_state_after_binding_it_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihahfr/how_to_save_the_llm_state_after_binding_it_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T05:23:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih5y85</id>
    <title>Ollama running on iGPU instead of dGPU</title>
    <updated>2025-02-04T01:24:19+00:00</updated>
    <author>
      <name>/u/unfiltereddz</name>
      <uri>https://old.reddit.com/user/unfiltereddz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I get more usage on CPU, but when I disable my iGPU in my laptop, now dGPU has 100% utilization. I already set it to high performance in both Nvidia Control Panel and Windows and downloaded Cuda Tool Kit. I still dont work. I have to disable my iGPU in order for it to work. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unfiltereddz"&gt; /u/unfiltereddz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih5y85/ollama_running_on_igpu_instead_of_dgpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih5y85/ollama_running_on_igpu_instead_of_dgpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ih5y85/ollama_running_on_igpu_instead_of_dgpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T01:24:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ighr82</id>
    <title>Customizable GUI for ollama (less than 1MB)</title>
    <updated>2025-02-03T05:17:25+00:00</updated>
    <author>
      <name>/u/A8LR</name>
      <uri>https://old.reddit.com/user/A8LR</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ighr82/customizable_gui_for_ollama_less_than_1mb/"&gt; &lt;img alt="Customizable GUI for ollama (less than 1MB)" src="https://preview.redd.it/vyq6efv0zuge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fdbf99e396bc2b32ada4b4be7b12700a7f25056" title="Customizable GUI for ollama (less than 1MB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A barebones chat interface for Ollama in 4 files; HTML, CSS, JS and Python.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/qusaismael/localllm"&gt;https://github.com/qusaismael/localllm&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Why post: seeing people struggle with over-engineered examples. MIT licensed = modify freely. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A8LR"&gt; /u/A8LR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vyq6efv0zuge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ighr82/customizable_gui_for_ollama_less_than_1mb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ighr82/customizable_gui_for_ollama_less_than_1mb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T05:17:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih9w5x</id>
    <title>Threadripper CPU Testing</title>
    <updated>2025-02-04T04:49:36+00:00</updated>
    <author>
      <name>/u/BuffMcBigHuge</name>
      <uri>https://old.reddit.com/user/BuffMcBigHuge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ih9w5x/threadripper_cpu_testing/"&gt; &lt;img alt="Threadripper CPU Testing" src="https://b.thumbs.redditmedia.com/9l5wy5MbnsTyEe3u6Sfvh4UBwqXoF42_7TFF9jZuFro.jpg" title="Threadripper CPU Testing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ojb33b52y1he1.png?width=888&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b60337a4431b2394c7882236fae787bb7e806d31"&gt;https://preview.redd.it/ojb33b52y1he1.png?width=888&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b60337a4431b2394c7882236fae787bb7e806d31&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been testing with a Threadripper 3960x and 256gb of RAM. The issue I'm experiencing is that when the inference is completed, half of my CPU cores go in overdrive doing nothing. I feel like it's a bug with Ollama. I will test further.&lt;/p&gt; &lt;p&gt;Here are some results:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9pwodtxny1he1.png?width=778&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=363f6fede2a1cbbd9dfd5c78efda3e32a925be0d"&gt;https://preview.redd.it/9pwodtxny1he1.png?width=778&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=363f6fede2a1cbbd9dfd5c78efda3e32a925be0d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Getting around 1.25 tokens per second with 2.22bit 671b, RAM at 3200mhz. My system is unstable at 3600mhz 256gb.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BuffMcBigHuge"&gt; /u/BuffMcBigHuge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih9w5x/threadripper_cpu_testing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih9w5x/threadripper_cpu_testing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ih9w5x/threadripper_cpu_testing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T04:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1igrtd7</id>
    <title>LLM Powered Map</title>
    <updated>2025-02-03T15:36:26+00:00</updated>
    <author>
      <name>/u/ranoutofusernames__</name>
      <uri>https://old.reddit.com/user/ranoutofusernames__</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1igrtd7/llm_powered_map/"&gt; &lt;img alt="LLM Powered Map" src="https://preview.redd.it/imcksipg1yge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ffd22c99c31c7a7c90376fee3c2d82e64d1c2451" title="LLM Powered Map" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open source, LLM powered discovery/exploration map I made a month ago. Runs locally or using cloud models. With a big enough model, it‚Äôs pretty much like having an offline, global map. Cheers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/space0blaster/godview"&gt;Repo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ranoutofusernames__"&gt; /u/ranoutofusernames__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/imcksipg1yge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igrtd7/llm_powered_map/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igrtd7/llm_powered_map/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T15:36:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ignq9z</id>
    <title>Is there a way to "train" an open-source LLM to do one type of task really well?</title>
    <updated>2025-02-03T12:15:45+00:00</updated>
    <author>
      <name>/u/ArtPerToken</name>
      <uri>https://old.reddit.com/user/ArtPerToken</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, forgive me if its a silly question, but is there a way to train or modify an existing LLM (i guess an open source one) to do one type of tasks really well?&lt;/p&gt; &lt;p&gt;For example if I have 50 poems I wrote in my own unique style, how can I &amp;quot;feed&amp;quot; it to the LLM and then ask it to generate a new poem about a new subject in the same style?&lt;/p&gt; &lt;p&gt;Would appreciate any thoughts on the best way to go about this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArtPerToken"&gt; /u/ArtPerToken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ignq9z/is_there_a_way_to_train_an_opensource_llm_to_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ignq9z/is_there_a_way_to_train_an_opensource_llm_to_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ignq9z/is_there_a_way_to_train_an_opensource_llm_to_do/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T12:15:45+00:00</published>
  </entry>
</feed>
