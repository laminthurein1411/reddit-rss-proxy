<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-15T22:24:00+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jasvg8</id>
    <title>This looks interesting, breaking the guard rail.</title>
    <updated>2025-03-14T01:49:12+00:00</updated>
    <author>
      <name>/u/powerflower_khi</name>
      <uri>https://old.reddit.com/user/powerflower_khi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jasvg8/this_looks_interesting_breaking_the_guard_rail/"&gt; &lt;img alt="This looks interesting, breaking the guard rail." src="https://b.thumbs.redditmedia.com/VAucDCdJLm4V-HpqjMt3MuNq1WSbrVhVI1VjuYVI-zI.jpg" title="This looks interesting, breaking the guard rail." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/qcfm8ck89koe1.png?width=1084&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=063bdad1594d12a6b3ecc15bbd0c3781da664685"&gt;https://preview.redd.it/qcfm8ck89koe1.png?width=1084&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=063bdad1594d12a6b3ecc15bbd0c3781da664685&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Used via Ollama gemma3:27b. on certain topics, the safeguard rail still works. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/powerflower_khi"&gt; /u/powerflower_khi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jasvg8/this_looks_interesting_breaking_the_guard_rail/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jasvg8/this_looks_interesting_breaking_the_guard_rail/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jasvg8/this_looks_interesting_breaking_the_guard_rail/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T01:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaydvn</id>
    <title>Gemma3 12B uses excessive memory.</title>
    <updated>2025-03-14T07:30:02+00:00</updated>
    <author>
      <name>/u/RaviK99</name>
      <uri>https://old.reddit.com/user/RaviK99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried the new gemma models and while the 4B ran fine the 12B model just kept on eating my RAM until windows stepped in and the ollama server process was restarted and I get the error that an existing connection was forcibly closed by the remote host.&lt;/p&gt; &lt;p&gt;I have modest setup. A Ryzen 5 5600H, 16GB Ram and a 4 GB Nvidia Laptop GPU. Not the beefiest gig I know but I have run deepseek-r1 14B without any problem while multitasking at a respectable token/sec.&lt;/p&gt; &lt;p&gt;Is anyone else facing increased ram usage for the model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RaviK99"&gt; /u/RaviK99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaydvn/gemma3_12b_uses_excessive_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaydvn/gemma3_12b_uses_excessive_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jaydvn/gemma3_12b_uses_excessive_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T07:30:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1japxmx</id>
    <title>Running Gemma3 on a OnePlus 3!</title>
    <updated>2025-03-13T23:24:40+00:00</updated>
    <author>
      <name>/u/Parreirao2</name>
      <uri>https://old.reddit.com/user/Parreirao2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1japxmx/running_gemma3_on_a_oneplus_3/"&gt; &lt;img alt="Running Gemma3 on a OnePlus 3!" src="https://preview.redd.it/9uf5r0bmjjoe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=600964789c81aeaf1624128dc4dfbf20dfaa50d7" title="Running Gemma3 on a OnePlus 3!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Parreirao2"&gt; /u/Parreirao2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9uf5r0bmjjoe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1japxmx/running_gemma3_on_a_oneplus_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1japxmx/running_gemma3_on_a_oneplus_3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T23:24:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb9i1s</id>
    <title>Can I move Ollama models from PC to other PC (ubuntu)</title>
    <updated>2025-03-14T17:31:24+00:00</updated>
    <author>
      <name>/u/EssamGoda</name>
      <uri>https://old.reddit.com/user/EssamGoda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using Ollama on ubuntu and I downloaded some models can I copy these models to another PC? and how? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EssamGoda"&gt; /u/EssamGoda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jb9i1s/can_i_move_ollama_models_from_pc_to_other_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jb9i1s/can_i_move_ollama_models_from_pc_to_other_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jb9i1s/can_i_move_ollama_models_from_pc_to_other_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T17:31:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbae0v</id>
    <title>How does Ollama pick the CPU backend?</title>
    <updated>2025-03-14T18:16:03+00:00</updated>
    <author>
      <name>/u/PepperGrind</name>
      <uri>https://old.reddit.com/user/PepperGrind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jbae0v/how_does_ollama_pick_the_cpu_backend/"&gt; &lt;img alt="How does Ollama pick the CPU backend?" src="https://external-preview.redd.it/wyCM1fHzTa-IIqHgS1QTxdSYNXn668elDj0WmYMPf_k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0d58c9a49c1e9ce629e5b31dce17b727d8c6ab8" title="How does Ollama pick the CPU backend?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded one of the release packages for Linux and had a peek inside. In the &amp;quot;libs&amp;quot; folder, I see the following:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8es46n6e4poe1.png?width=249&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19643b589b010e39b5fd3dc8044a0033c8331949"&gt;https://preview.redd.it/8es46n6e4poe1.png?width=249&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19643b589b010e39b5fd3dc8044a0033c8331949&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This aligns nicely with llama.cpp's `GGML_CPU_ALL_VARIANTS` build option - &lt;a href="https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#L307"&gt;https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#L307&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Is Ollama automatically detecting my CPU under the hood, and deciding which is the best CPU backend to use, or does it rely on manual specification, and falls back to the &amp;quot;base&amp;quot; backend if nothing is specified?&lt;/p&gt; &lt;p&gt;As a bonus, it'd be great if someone could link me the Ollama code where it is deciding which CPU backend to link.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PepperGrind"&gt; /u/PepperGrind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbae0v/how_does_ollama_pick_the_cpu_backend/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbae0v/how_does_ollama_pick_the_cpu_backend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbae0v/how_does_ollama_pick_the_cpu_backend/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T18:16:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbqmxw</id>
    <title>Buying an M4 Macbook air for ollama</title>
    <updated>2025-03-15T08:30:08+00:00</updated>
    <author>
      <name>/u/Sad_Throat_5187</name>
      <uri>https://old.reddit.com/user/Sad_Throat_5187</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am considering buying a base model M4 MacBook Air with 16 GB of RAM for running ollama models. What models can it handle? Is Gemma3 27b possible? What is your opinion?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad_Throat_5187"&gt; /u/Sad_Throat_5187 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbqmxw/buying_an_m4_macbook_air_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbqmxw/buying_an_m4_macbook_air_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbqmxw/buying_an_m4_macbook_air_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T08:30:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbh7ah</id>
    <title>Unsharded 80GB Llama 3.3 model for Ollama?</title>
    <updated>2025-03-14T23:13:16+00:00</updated>
    <author>
      <name>/u/chiaplotter4u</name>
      <uri>https://old.reddit.com/user/chiaplotter4u</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As Ollama still doesn't support sharded models, are there any that would fit 2x A6000 and aren't sharded? Llama 3.3 is preferred, but other models can be used too. Looking for a model that works with Czech as best as possible.&lt;/p&gt; &lt;p&gt;For some reason, merged GGUF Llama 3.3 doesn't load (Error: Post &amp;quot;&lt;a href="http://127.0.0.1:11434/api/generate%22:"&gt;http://127.0.0.1:11434/api/generate&amp;quot;:&lt;/a&gt; EOF). If someone managed to solve that, I'd appreciate the steps.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chiaplotter4u"&gt; /u/chiaplotter4u &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbh7ah/unsharded_80gb_llama_33_model_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbh7ah/unsharded_80gb_llama_33_model_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbh7ah/unsharded_80gb_llama_33_model_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T23:13:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbifhz</id>
    <title>What happens if Context Length is set larger than the Model supports?</title>
    <updated>2025-03-15T00:11:07+00:00</updated>
    <author>
      <name>/u/digitalextremist</name>
      <uri>https://old.reddit.com/user/digitalextremist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If by &lt;code&gt;/set&lt;/code&gt; or environment variable or API argument, the context length is set higher than the maximum in the model definition from the library... what happens?&lt;/p&gt; &lt;p&gt;Does the model just stay within its own limits and silently spill context?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/digitalextremist"&gt; /u/digitalextremist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbifhz/what_happens_if_context_length_is_set_larger_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbifhz/what_happens_if_context_length_is_set_larger_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbifhz/what_happens_if_context_length_is_set_larger_than/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T00:11:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbfn18</id>
    <title>Ideas for prompting ollama for entity-relation extraction from text?</title>
    <updated>2025-03-14T22:02:12+00:00</updated>
    <author>
      <name>/u/Ok_Bad7992</name>
      <uri>https://old.reddit.com/user/Ok_Bad7992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have ollama running on an M1 Mac with Gemma3. It answers simple &amp;quot;Why is the sky blue?&amp;quot; prompts, but I need to figure out how to extract information, entities and their relationships at the very least. I'd be happy to hear from others and, if necessary, work together to co-evolve a powerful system.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Bad7992"&gt; /u/Ok_Bad7992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbfn18/ideas_for_prompting_ollama_for_entityrelation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbfn18/ideas_for_prompting_ollama_for_entityrelation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbfn18/ideas_for_prompting_ollama_for_entityrelation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T22:02:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb5014</id>
    <title>New RAG docs &amp; AI assistant make it easy for non-coders to build RAGs</title>
    <updated>2025-03-14T14:19:43+00:00</updated>
    <author>
      <name>/u/DonTizi</name>
      <uri>https://old.reddit.com/user/DonTizi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The documentation of rlama, including all available commands and detailed examples, is now live on our website! But that‚Äôs not all‚Äîwe‚Äôve also introduced Rlama Chat, an AI-powered assistant designed to help you with your RAG implementations. Whether you have questions, need guidance, or are brainstorming new RAG use cases, Rlama Chat is here to support your projects.Have an idea for a specific RAG? Build it.Check out the docs and start exploring today!&lt;/p&gt; &lt;p&gt;You can go throught here if you have interest to make RAGs: &lt;a href="https://rlama.dev/"&gt;Website&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can see a demo of Rlama Chat here: &lt;a href="https://x.com/LeDonTizi/status/1900544052107399573"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DonTizi"&gt; /u/DonTizi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jb5014/new_rag_docs_ai_assistant_make_it_easy_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jb5014/new_rag_docs_ai_assistant_make_it_easy_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jb5014/new_rag_docs_ai_assistant_make_it_easy_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T14:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb7f08</id>
    <title>What is your dream gpu specs for ollama that you wish it existed?</title>
    <updated>2025-03-14T16:04:03+00:00</updated>
    <author>
      <name>/u/Masterofironfist</name>
      <uri>https://old.reddit.com/user/Masterofironfist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mine would be rtx 5060 Ti 24GB due to compact size and probably great performance in LLMs and Flux and price around 500$.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Masterofironfist"&gt; /u/Masterofironfist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jb7f08/what_is_your_dream_gpu_specs_for_ollama_that_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jb7f08/what_is_your_dream_gpu_specs_for_ollama_that_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jb7f08/what_is_your_dream_gpu_specs_for_ollama_that_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T16:04:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbfnlx</id>
    <title>üì£ Just added multimodal support to Observer AI!</title>
    <updated>2025-03-14T22:02:54+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share a new update to my open-source project &lt;a href="https://app.observer-ai.com"&gt;Observer AI&lt;/a&gt; - it now fully supports multimodal vision models including Gemma 3 Vision through Ollama!&lt;/p&gt; &lt;h1&gt;What's new?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Full vision model support&lt;/strong&gt;: Your agents can now &amp;quot;see&amp;quot; and understand your screen beyond just text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Works with Gemma 3 Vision and Llava.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Some example use cases:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Create an agent that monitors dashboards and alerts you to visual anomalies&lt;/li&gt; &lt;li&gt;Build a desktop assistant that recognizes UI elements and helps navigate applications&lt;/li&gt; &lt;li&gt;Design a screen reader that can explain what's happening visually&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All of this runs completely locally through Ollama - no API keys, no cloud dependencies.&lt;/p&gt; &lt;p&gt;Check it out at &lt;a href="https://app.observer-ai.com"&gt;https://app.observer-ai.com&lt;/a&gt; or on &lt;a href="https://github.com/Roy3838/Observer"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love to hear your feedback or ideas for other features that would be useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbfnlx/just_added_multimodal_support_to_observer_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbfnlx/just_added_multimodal_support_to_observer_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbfnlx/just_added_multimodal_support_to_observer_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T22:02:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc253n</id>
    <title>Noob - GPU usage question - low while replying</title>
    <updated>2025-03-15T18:49:35+00:00</updated>
    <author>
      <name>/u/nraygun</name>
      <uri>https://old.reddit.com/user/nraygun</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got Ollama working on my main desktop PC(AMD Ryzen 5 3600X, 16GB, GTX 1050) running MX Linux with the UI hosted in a Docker container on my Unraid server. I'm using deepseek-R1. I'm surprised it works at all on my humble little system!&lt;/p&gt; &lt;p&gt;I watch nvidia-smi and I see that the GPU doesn't really get exercised when it's replying. Before it goes into &amp;quot;thinking&amp;quot; it spikes to 99%, then while &amp;quot;thinking&amp;quot; it only goes to 12-17%. When it's replying, it uses 4-8%.&lt;/p&gt; &lt;p&gt;Is this to be expected? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nraygun"&gt; /u/nraygun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jc253n/noob_gpu_usage_question_low_while_replying/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jc253n/noob_gpu_usage_question_low_while_replying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jc253n/noob_gpu_usage_question_low_while_replying/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T18:49:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbzfu7</id>
    <title>Gemma3:4b behaves differently with Langchain and Pydantic AI</title>
    <updated>2025-03-15T16:50:54+00:00</updated>
    <author>
      <name>/u/No-Comfort3958</name>
      <uri>https://old.reddit.com/user/No-Comfort3958</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am testing Gemma3:4b and PydanticAI, and I realised unlike Langchain's ChatOllama PydanticAI doesn't have Ollama specific class, it uses OpenAI's api calling system.&lt;/p&gt; &lt;p&gt;I was testing with the prompt &lt;code&gt;Where were the olympics held in 2012? Give answer in city, country format&lt;/code&gt; these responses from langchain were standard with 5 consecutive runs &lt;strong&gt;London, United Kingdom&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;However with PydanticAI it the answers are weird for some reason such as:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;LONDON, England üá¨Û†Å¢Û†Å≥Û†Å£ »õÛ†Åø&lt;/li&gt; &lt;li&gt;London, Great Great Britain (&lt;em&gt;officer Great Britain&lt;/em&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;London,&lt;/strong&gt; United Kingdom The Olympic events that year (Summer/XXIX Summer) were held primarily in and in the city and state of London and surrounding suburban areas.&lt;/li&gt; &lt;li&gt;ŒõŒ∏ŒÆ&amp;lt;0xE2&amp;gt;&amp;lt;0x80&amp;gt;&amp;lt;0xAF&amp;gt;ŒΩŒ± (Athens!), Greece&lt;/li&gt; &lt;li&gt;London, in United K√∂nigreich.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;london, UK&lt;/strong&gt; You can double-verify this on any Olympic Games webpage (official website or credible source like Wikipedia, ESPN).&lt;/li&gt; &lt;li&gt;‰º¶Êï¶, Ëã±Ê†ºÂÖ∞ (in the UnitedKingdom) Do you want to know about other Olympics too?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I thought it must be an issue with the way the model is being called so I tested the same with llama3.2 with PydanticAI. The answer is always &lt;strong&gt;London, United Kingdom&lt;/strong&gt;, nothing more nothing less.&lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Comfort3958"&gt; /u/No-Comfort3958 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbzfu7/gemma34b_behaves_differently_with_langchain_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbzfu7/gemma34b_behaves_differently_with_langchain_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbzfu7/gemma34b_behaves_differently_with_langchain_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T16:50:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbtjze</id>
    <title>Is there a guide about Ollama parameters and how to use them?</title>
    <updated>2025-03-15T12:00:37+00:00</updated>
    <author>
      <name>/u/Tehgamecat</name>
      <uri>https://old.reddit.com/user/Tehgamecat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm struggling to understand how to get any of the parameters to do anything in Ollama 0.6.0 or 0.6.1 (rc) on wsl2.&lt;/p&gt; &lt;p&gt;Does Ollama not have a config file or something? Or is it on the model or what? I've struggled to find any details or instructions (probably on me).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tehgamecat"&gt; /u/Tehgamecat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbtjze/is_there_a_guide_about_ollama_parameters_and_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbtjze/is_there_a_guide_about_ollama_parameters_and_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbtjze/is_there_a_guide_about_ollama_parameters_and_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T12:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbqvzy</id>
    <title>[Guide] How to Run Ollama-OCR on Google Colab (Free Tier!) üöÄ</title>
    <updated>2025-03-15T08:49:48+00:00</updated>
    <author>
      <name>/u/imanoop7</name>
      <uri>https://old.reddit.com/user/imanoop7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I recently built &lt;strong&gt;Ollama-OCR&lt;/strong&gt;, an AI-powered OCR tool that extracts text from &lt;strong&gt;PDFs, charts, and images&lt;/strong&gt; using advanced &lt;strong&gt;vision-language models&lt;/strong&gt;. Now, I‚Äôve written a step-by-step guide on how you can run it on &lt;strong&gt;Google Colab Free Tier!&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;What‚Äôs in the guide?&lt;/h1&gt; &lt;p&gt;‚úîÔ∏è &lt;strong&gt;Installing Ollama on Google Colab&lt;/strong&gt; (No GPU required!)&lt;br /&gt; ‚úîÔ∏è Running models like &lt;strong&gt;Granite3.2-Vision, LLaVA 7B&lt;/strong&gt; &amp;amp; more&lt;br /&gt; ‚úîÔ∏è Extracting text in &lt;strong&gt;Markdown, JSON, structured formats&lt;/strong&gt;&lt;br /&gt; ‚úîÔ∏è Using &lt;strong&gt;custom prompts for better accuracy&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey everyone, Detailed Guide &lt;strong&gt;Ollama-OCR&lt;/strong&gt;, an AI-powered OCR tool that extracts text from PDFs, charts, and images using advanced vision-language models. It works great for structured and unstructured data extraction!&lt;/p&gt; &lt;p&gt;Here's what you can do with it:&lt;br /&gt; ‚úîÔ∏è Install &amp;amp; run &lt;strong&gt;Ollama&lt;/strong&gt; on Google Colab (Free Tier)&lt;br /&gt; ‚úîÔ∏è Use models like &lt;strong&gt;Granite3.2-Vision&lt;/strong&gt; &amp;amp; &lt;strong&gt;llama-vision3.2&lt;/strong&gt; for better accuracy&lt;br /&gt; ‚úîÔ∏è Extract text in &lt;strong&gt;Markdown, JSON, structured data, or key-value formats&lt;/strong&gt;&lt;br /&gt; ‚úîÔ∏è Customize prompts for better results&lt;/p&gt; &lt;p&gt;üîó Check out &lt;a href="https://medium.com/@mauryaanoop3/how-to-run-ollama-ocr-on-google-colab-free-tier-9bd3aa86dfe2"&gt;Guide&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Check it out &amp;amp; contribute! üîó &lt;a href="https://github.com/imanoop7/Ollama-OCR"&gt;GitHub: Ollama-OCR&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear if anyone else is using &lt;strong&gt;Ollama-OCR&lt;/strong&gt; for document processing! Let‚Äôs discuss. üëá&lt;/p&gt; &lt;p&gt;#OCR #MachineLearning #AI #DeepLearning #GoogleColab #OllamaOCR #opensource&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/imanoop7"&gt; /u/imanoop7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbqvzy/guide_how_to_run_ollamaocr_on_google_colab_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbqvzy/guide_how_to_run_ollamaocr_on_google_colab_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbqvzy/guide_how_to_run_ollamaocr_on_google_colab_free/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T08:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbzs8m</id>
    <title>Open vs closed source: Real differences beyond cost?</title>
    <updated>2025-03-15T17:05:34+00:00</updated>
    <author>
      <name>/u/Every_Gold4726</name>
      <uri>https://old.reddit.com/user/Every_Gold4726</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a long time I've been using open-web-ui with CUDA and docker for my AI projects. Recently I've been looking into msty.app, and it got me thinking about the whole open source vs closed source thing.&lt;/p&gt; &lt;p&gt;I've noticed there's often this attitude that closed source is somehow inherently worse, but I'm trying to understand the real reasons beyond just the obvious &amp;quot;free vs paid&amp;quot; argument. The cost factor isn't what I'm concerned about - I'm more interested in the actual technical or philosophical differences.&lt;/p&gt; &lt;p&gt;Has anyone here used both approaches and can share what the actual practical differences were? What are the legitimate advantages or disadvantages of each that go beyond price?&lt;/p&gt; &lt;p&gt;Just trying to understand more of the reasoning behind these decisions as I consider msty.app and similar options.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Every_Gold4726"&gt; /u/Every_Gold4726 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbzs8m/open_vs_closed_source_real_differences_beyond_cost/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbzs8m/open_vs_closed_source_real_differences_beyond_cost/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbzs8m/open_vs_closed_source_real_differences_beyond_cost/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T17:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbzyby</id>
    <title>Ascii ability</title>
    <updated>2025-03-15T17:12:53+00:00</updated>
    <author>
      <name>/u/Sterling1989</name>
      <uri>https://old.reddit.com/user/Sterling1989</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to run a model locally that is capable of ascii art. It seems very hard to do for any LLM to do this. Even running the big boys in the browser (CHATGPT, Grok etc) they struggle to do this. Anyone know of any local models that are able to do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sterling1989"&gt; /u/Sterling1989 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbzyby/ascii_ability/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbzyby/ascii_ability/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbzyby/ascii_ability/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T17:12:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbvn6n</id>
    <title>Quantisation vs Parameters</title>
    <updated>2025-03-15T13:56:26+00:00</updated>
    <author>
      <name>/u/Glad-Process5955</name>
      <uri>https://old.reddit.com/user/Glad-Process5955</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is better less parameters with high quantisations or vice versa.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glad-Process5955"&gt; /u/Glad-Process5955 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbvn6n/quantisation_vs_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbvn6n/quantisation_vs_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbvn6n/quantisation_vs_parameters/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T13:56:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc2bwv</id>
    <title>Why is gemma3 27b-it-fp16 taking 64GB.</title>
    <updated>2025-03-15T18:58:09+00:00</updated>
    <author>
      <name>/u/Sanandaji</name>
      <uri>https://old.reddit.com/user/Sanandaji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 56GB of VRAM. Per &lt;a href="https://ollama.com/library/gemma3/tags"&gt;https://ollama.com/library/gemma3/tags&lt;/a&gt; 27b-it-fp16 should be 55GB but the size shows 64GB for me and it slows my machine down to almost a halt. I get 3 tokens per second in CLI, open webui cannot even run it, and this is the usage i see: &lt;a href="https://i.imgur.com/wPtFc2b.png"&gt;https://i.imgur.com/wPtFc2b.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Is this an issue between ollama and gemma3 or is this normal behavior?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sanandaji"&gt; /u/Sanandaji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jc2bwv/why_is_gemma3_27bitfp16_taking_64gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jc2bwv/why_is_gemma3_27bitfp16_taking_64gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jc2bwv/why_is_gemma3_27bitfp16_taking_64gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T18:58:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc3l5b</id>
    <title>How to use Rlama with Web UI?</title>
    <updated>2025-03-15T19:54:07+00:00</updated>
    <author>
      <name>/u/laurentbourrelly</name>
      <uri>https://old.reddit.com/user/laurentbourrelly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rlama &lt;a href="https://github.com/DonTizi/rlama?tab=readme-ov-file#rag---create-a-rag-system"&gt;https://github.com/DonTizi/rlama?tab=readme-ov-file#rag---create-a-rag-system&lt;/a&gt; Is a fantastic tool, but I would like to use it with &lt;a href="https://github.com/open-webui/open-webui"&gt;https://github.com/open-webui/open-webui&lt;/a&gt; or another Web interface instead of Terminal (OS X).&lt;/p&gt; &lt;p&gt;How do I proceed?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/laurentbourrelly"&gt; /u/laurentbourrelly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jc3l5b/how_to_use_rlama_with_web_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jc3l5b/how_to_use_rlama_with_web_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jc3l5b/how_to_use_rlama_with_web_ui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T19:54:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbkbai</id>
    <title>The Complete Guide to Building Your Free Local AI Assistant with Ollama and Open WebUI</title>
    <updated>2025-03-15T01:45:48+00:00</updated>
    <author>
      <name>/u/PeterHash</name>
      <uri>https://old.reddit.com/user/PeterHash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just published a no-BS step-by-step guide on Medium for anyone tired of paying monthly AI subscription fees or worried about privacy when using tools like ChatGPT. In my guide, I walk you through setting up your local AI environment using &lt;strong&gt;Ollama&lt;/strong&gt; and &lt;strong&gt;Open WebUI&lt;/strong&gt;‚Äîa setup that lets you run a custom ChatGPT entirely on your computer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What You'll Learn:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How to eliminate AI subscription costs (yes, zero monthly fees!)&lt;/li&gt; &lt;li&gt;Achieve complete privacy: your data stays local, with no third-party data sharing&lt;/li&gt; &lt;li&gt;Enjoy faster response times (no more waiting during peak hours)&lt;/li&gt; &lt;li&gt;Get complete customization to build specialized AI assistants for your unique needs&lt;/li&gt; &lt;li&gt;Overcome token limits with unlimited usage&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Setup Process:&lt;/strong&gt;&lt;br /&gt; With about 15 terminal commands, you can have everything up and running in under an hour. I included all the code, screenshots, and troubleshooting tips that helped me through the setup. The result is a clean web interface that feels like ChatGPT‚Äîentirely under your control.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A Sneak Peek at the Guide:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Toolstack Overview:&lt;/strong&gt; You'll need (Ollama, Open WebUI, a &lt;strong&gt;GPU-powered machine&lt;/strong&gt;, etc.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Environment Setup:&lt;/strong&gt; How to configure Python 3.11 and set up your system&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Installing &amp;amp; Configuring:&lt;/strong&gt; Detailed instructions for both Ollama and Open WebUI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced Features:&lt;/strong&gt; I also cover features like web search integration, a code interpreter, custom model creation, and even a preview of upcoming advanced RAG features for creating custom knowledge bases.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've been using this setup for two months, and it's completely replaced my paid AI subscriptions while boosting my workflow efficiency. Stay tuned for part two, which will cover advanced RAG implementation, complex workflows, and tool integration based on your feedback.&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@hautel.alex2000/build-your-local-ai-from-zero-to-a-custom-chatgpt-interface-with-ollama-open-webui-6bee2c5abba3"&gt;&lt;strong&gt;Read the complete guide here ‚Üí&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Let's Discuss:&lt;/strong&gt;&lt;br /&gt; What AI workflows would you most want to automate with your own customizable AI assistant? Are there specific use cases or features you're struggling with that you'd like to see in future guides? Share your thoughts below‚ÄîI'd love to incorporate popular requests in the upcoming instalment!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PeterHash"&gt; /u/PeterHash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbkbai/the_complete_guide_to_building_your_free_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbkbai/the_complete_guide_to_building_your_free_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbkbai/the_complete_guide_to_building_your_free_local_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T01:45:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbviqb</id>
    <title>An Open-Source AI Assistant for Chatting with Your Developer Docs</title>
    <updated>2025-03-15T13:50:04+00:00</updated>
    <author>
      <name>/u/eleven-five</name>
      <uri>https://old.reddit.com/user/eleven-five</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on &lt;strong&gt;Ragpi&lt;/strong&gt;, an open-source AI assistant that builds knowledge bases from docs, GitHub Issues and READMEs. It uses PostgreSQL with pgvector as a vector DB and leverages RAG to answer technical questions through an API. Ragpi also integrates with Discord and Slack, making it easy to interact with directly from those platforms.&lt;/p&gt; &lt;p&gt;Some things it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Creates knowledge bases from documentation websites, GitHub Issues and READMEs&lt;/li&gt; &lt;li&gt;Uses hybrid search (semantic + keyword) for retrieval&lt;/li&gt; &lt;li&gt;Uses tool calling to dynamically search and retrieve relevant information during conversations&lt;/li&gt; &lt;li&gt;Works with OpenAI, Ollama, DeepSeek, or any OpenAI-compatible API&lt;/li&gt; &lt;li&gt;Provides a simple REST API for querying and managing sources&lt;/li&gt; &lt;li&gt;Integrates with Discord and Slack for easy interaction&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Built with:&lt;/strong&gt; FastAPI, Celery and Postgres&lt;/p&gt; &lt;p&gt;It‚Äôs still a work in progress, but I‚Äôd love some feedback!&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ragpi/ragpi"&gt;https://github.com/ragpi/ragpi&lt;/a&gt;&lt;br /&gt; Docs: &lt;a href="https://docs.ragpi.io/"&gt;https://docs.ragpi.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eleven-five"&gt; /u/eleven-five &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbviqb/an_opensource_ai_assistant_for_chatting_with_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbviqb/an_opensource_ai_assistant_for_chatting_with_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbviqb/an_opensource_ai_assistant_for_chatting_with_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T13:50:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbwwxh</id>
    <title>Why didn't they design gemma3 to fit in GPU memory more efficiently?</title>
    <updated>2025-03-15T14:57:18+00:00</updated>
    <author>
      <name>/u/droxy429</name>
      <uri>https://old.reddit.com/user/droxy429</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma3 is advertised as the &amp;quot;most capable model that runs on a single GPU. So if they figure the target market for this model is people running on a single GPU, why wouldn't they make the size of each model scale up with typical GPU memory sizes: 4GB, 8GB, 16GB, 24GB... &lt;a href="https://ollama.com/library/gemma3/tags"&gt;Check out the sizes of these models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The 4b is 3.3GB which fits nicely in a 4GB memory GPU.&lt;/p&gt; &lt;p&gt;The 12b is 8.1GB which is a little too big to fit in an 8GB memory GPU.&lt;/p&gt; &lt;p&gt;The 27b is 17GB which is just a little too big to fit in a 16GB memory GPU.&lt;/p&gt; &lt;p&gt;This is frustrating since I have a 16GB GPU and need to run the 8.1GB model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/droxy429"&gt; /u/droxy429 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbwwxh/why_didnt_they_design_gemma3_to_fit_in_gpu_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbwwxh/why_didnt_they_design_gemma3_to_fit_in_gpu_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbwwxh/why_didnt_they_design_gemma3_to_fit_in_gpu_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T14:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc0yu2</id>
    <title>Tiny Ollama Chat: A Super Lightweight Alternative to OpenWebUI</title>
    <updated>2025-03-15T17:57:32+00:00</updated>
    <author>
      <name>/u/No-Carpet-211</name>
      <uri>https://old.reddit.com/user/No-Carpet-211</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone,&lt;/p&gt; &lt;p&gt;I created Tiny Ollama Chat after finding OpenWebUI too resource-heavy for my needs. It's a minimal but functional UI - just the essentials for interacting with your Ollama models. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Check out the repo&lt;/strong&gt; &lt;a href="https://github.com/anishgowda21/tiny-ollama-chat"&gt;https://github.com/anishgowda21/tiny-ollama-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;p&gt;Its,&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Incredibly lightweight (only 32MB Docker image!)&lt;/li&gt; &lt;li&gt;Real-time message streaming&lt;/li&gt; &lt;li&gt;Conversation history and multiple model support&lt;/li&gt; &lt;li&gt;Custom Ollama URL configuration&lt;/li&gt; &lt;li&gt;Persistent storage with SQLite&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It offers fast startup time, simple deployment (Docker or local build), and a clean UI focused on the chat experience.&lt;/p&gt; &lt;p&gt;Would love your feedback if you try it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Carpet-211"&gt; /u/No-Carpet-211 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jc0yu2/tiny_ollama_chat_a_super_lightweight_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jc0yu2/tiny_ollama_chat_a_super_lightweight_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jc0yu2/tiny_ollama_chat_a_super_lightweight_alternative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T17:57:32+00:00</published>
  </entry>
</feed>
