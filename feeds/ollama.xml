<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-15T08:49:03+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ioyiyw</id>
    <title>Ollama 0.5.9 Update make my CPU inference slower</title>
    <updated>2025-02-14T00:54:30+00:00</updated>
    <author>
      <name>/u/Signal_Kiwi_9737</name>
      <uri>https://old.reddit.com/user/Signal_Kiwi_9737</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;Just updated Ollama from 0.5.7 &amp;gt; 0.5.9 and run my favorite LLM and noticed major performance drop on my dual Xeon 6126 setup. Went from ~3 t/s down to ~2 t/s. This is not great for me... Just to be sure this is correct I downgraded Ollama back to 0.5.7 and performance is restored! &lt;/p&gt; &lt;p&gt;Both of my CPUs have AVX512 instructions however it seems that using those instructions can in fact slows down inference performance?? I'm confused on this one... can some one explain this to me :)&lt;/p&gt; &lt;p&gt;My system is a Fujitsu RM2530 M4 1U server, dual Xeon 6126 with 384GB ram, no GPU and NUMA disabled. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Signal_Kiwi_9737"&gt; /u/Signal_Kiwi_9737 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioyiyw/ollama_059_update_make_my_cpu_inference_slower/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioyiyw/ollama_059_update_make_my_cpu_inference_slower/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ioyiyw/ollama_059_update_make_my_cpu_inference_slower/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T00:54:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iofspy</id>
    <title>Possible 32GB AMD GPU</title>
    <updated>2025-02-13T10:01:44+00:00</updated>
    <author>
      <name>/u/GhostInThePudding</name>
      <uri>https://old.reddit.com/user/GhostInThePudding</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Well this is promising:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=NIUtyzuFFOM"&gt;https://www.youtube.com/watch?v=NIUtyzuFFOM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Leaks show the 9070XT may be a 32GB GPU for under US$1000. Which means if it works well with AI, it could be the ultimate home user GPU available, particularly for Linux users. I hope it doesn't suck!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GhostInThePudding"&gt; /u/GhostInThePudding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iofspy/possible_32gb_amd_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iofspy/possible_32gb_amd_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iofspy/possible_32gb_amd_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T10:01:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioot2m</id>
    <title>OpenThinker:32b</title>
    <updated>2025-02-13T17:45:08+00:00</updated>
    <author>
      <name>/u/GVDub2</name>
      <uri>https://old.reddit.com/user/GVDub2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just loaded up this one. Incredibly complex reasoning process, followed by an extraordinarily terse response. I'll have to go look at the GitHub to see what's going on, as it insists on referring to itself in the third person (&amp;quot;the assistant&amp;quot;). An interesting one, but not a fast response. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GVDub2"&gt; /u/GVDub2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioot2m/openthinker32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioot2m/openthinker32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ioot2m/openthinker32b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T17:45:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1iog9ky</id>
    <title>Challenge! Decode image to JSON</title>
    <updated>2025-02-13T10:35:55+00:00</updated>
    <author>
      <name>/u/dxcore_35</name>
      <uri>https://old.reddit.com/user/dxcore_35</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iog9ky/challenge_decode_image_to_json/"&gt; &lt;img alt="Challenge! Decode image to JSON" src="https://preview.redd.it/3fs8e0uxwvie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7de0101dea023e12442d9a1453e885505037768b" title="Challenge! Decode image to JSON" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dxcore_35"&gt; /u/dxcore_35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3fs8e0uxwvie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iog9ky/challenge_decode_image_to_json/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iog9ky/challenge_decode_image_to_json/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T10:35:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipb7w5</id>
    <title>Searching for LLM-Driven Web Development: Best Free &amp; Low-Cost Strategies ($0‚Äì30/Month)</title>
    <updated>2025-02-14T14:01:48+00:00</updated>
    <author>
      <name>/u/Silent-Technician-90</name>
      <uri>https://old.reddit.com/user/Silent-Technician-90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not a web developer, but I have some basic experience coding in HTML, PHP, Python, and Ruby, though only at a surface level. As a hobby, I wanted to create my own web application, and I built one using Flask + MongoDB, implementing minimal functionality with the help of ChatGPT and other LLM-based chats.&lt;/p&gt; &lt;p&gt;Currently, I have financial constraints and am looking for a way to continue development with LLM involvement, where I will act solely as a &lt;strong&gt;user, product owner, and tester&lt;/strong&gt;, while the LLM will serve as the &lt;strong&gt;architect and developer&lt;/strong&gt;. My goal is to &lt;strong&gt;request new features and receive a working version directly in the browser&lt;/strong&gt;, evaluating whether the functionality works as expected.&lt;/p&gt; &lt;p&gt;I plan to &lt;strong&gt;transition from Flask to FastAPI for the backend&lt;/strong&gt; and use &lt;strong&gt;Next.js, TailwindCSS, ShadcnUI, TypeScript, and MongoDB&lt;/strong&gt; for the frontend and database.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Are there more efficient development approaches with zero financial investment (i.e. local LLM inferences which may work on my hardware with cline)?&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Would using &lt;strong&gt;local 72B models&lt;/strong&gt; be a viable option?&lt;/li&gt; &lt;li&gt;I have an &lt;strong&gt;RTX 4090 and a MacBook Pro with 128GB&lt;/strong&gt; merged v&lt;strong&gt;RAM&lt;/strong&gt;, which should be capable of running &lt;strong&gt;70B models&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What LLM models can be used with Cline for local development? what are the best options at the moment?&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;For effective LLM-based development, I understand that Memory Bank + Repomix (some also mentioned usage of MCP Servers) is an optimal setup. Are there other solutions I should consider?&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;If free development options turn out to be insufficient&lt;/strong&gt;, my understanding is that the closest &lt;strong&gt;paid alternative within a $20‚Äì30/month budget is a Cursor subscription&lt;/strong&gt;. &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Are there other viable alternatives in this price range?&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My primary focus is on &lt;strong&gt;free development solutions&lt;/strong&gt;, but I am also open to considering &lt;strong&gt;paid options up to $30 per month&lt;/strong&gt; if they significantly improve the development process.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silent-Technician-90"&gt; /u/Silent-Technician-90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipb7w5/searching_for_llmdriven_web_development_best_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipb7w5/searching_for_llmdriven_web_development_best_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipb7w5/searching_for_llmdriven_web_development_best_free/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T14:01:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip54oj</id>
    <title>How do you use console AI?</title>
    <updated>2025-02-14T07:16:51+00:00</updated>
    <author>
      <name>/u/Big-Relative-349</name>
      <uri>https://old.reddit.com/user/Big-Relative-349</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I'm an aspiring comic artist. I‚Äôve been experimenting with various AI models on Ollama to manage my worldbuilding database, but so far, all I‚Äôve gotten are unpredictable responses rather than anything truly useful. The only real takeaway has been learning some basic CMD and PowerShell commands.&lt;/p&gt; &lt;p&gt;My PC can run AI models up to 14B smoothly, but anything from 32B onward starts to lag. I thought my &lt;strong&gt;4060 Ti&lt;/strong&gt; would be the perfect GPU for this, but apparently, I was wrong.&lt;/p&gt; &lt;p&gt;How can I use these AI models in a way that‚Äôs actually useful to me and ensures at least somewhat predictable responses?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big-Relative-349"&gt; /u/Big-Relative-349 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip54oj/how_do_you_use_console_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip54oj/how_do_you_use_console_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ip54oj/how_do_you_use_console_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T07:16:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip0rx5</id>
    <title>Best way to self host open source LLM‚Äôs on GCP</title>
    <updated>2025-02-14T02:52:52+00:00</updated>
    <author>
      <name>/u/addimo</name>
      <uri>https://old.reddit.com/user/addimo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have some free credit on google cloud, thinking about using google cloud run with ollama, or vertex ai as they seems to be the simplest to run. But I am not sure if there is a better way on GCP maybe less costly ones‚Ä¶does anyone have experience self hosting on gcp ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/addimo"&gt; /u/addimo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip0rx5/best_way_to_self_host_open_source_llms_on_gcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip0rx5/best_way_to_self_host_open_source_llms_on_gcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ip0rx5/best_way_to_self_host_open_source_llms_on_gcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T02:52:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip9i7c</id>
    <title>How Does a Local small 7b model Compare to Google's Gemini 2.0 flash ?</title>
    <updated>2025-02-14T12:29:36+00:00</updated>
    <author>
      <name>/u/Parenormale</name>
      <uri>https://old.reddit.com/user/Parenormale</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently tested &lt;strong&gt;Neura-Mini (7B) running locally on with Ollama&lt;/strong&gt; against &lt;strong&gt;Google's Gemini 2.0 Flash&lt;/strong&gt; to see how they handle complex topics like &lt;strong&gt;math, game theory, cryptography, and philosophy&lt;/strong&gt; .&lt;/p&gt; &lt;p&gt;Both models were evaluated by gpt4o based on &lt;strong&gt;accuracy, depth, clarity, and logical reasoning&lt;/strong&gt; , with a final score assigned per response.&lt;/p&gt; &lt;p&gt;The results were interesting‚Äî&lt;strong&gt;not necessarily what I expected&lt;/strong&gt; . 7b local mode despite running on my &lt;strong&gt;Intel Ultra 5 125H&lt;/strong&gt; , performed better in some areas than I thought possible.&lt;/p&gt; &lt;p&gt;Here‚Äôs the full test video:&lt;/p&gt; &lt;p&gt;here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/ta49JQWPGgY?si=hyfOQIhfqTG6pMH8"&gt;7b fine tuned model vs.Goolgle Gemini 2.0 Flash Compared &amp;amp; Evaluated by GPT-4o&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious to hear from others: &lt;strong&gt;Do you think local models can compete with cloud-based LLMs like Gemini ?&lt;/strong&gt; What trade-offs do you see between control, performance, and capability?&lt;/p&gt; &lt;p&gt;Also, considering the results, &lt;strong&gt;do you think a model like this could actually be suitable for serious, professional use?&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Parenormale"&gt; /u/Parenormale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip9i7c/how_does_a_local_small_7b_model_compare_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip9i7c/how_does_a_local_small_7b_model_compare_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ip9i7c/how_does_a_local_small_7b_model_compare_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T12:29:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioyxkm</id>
    <title>How to do proper function calling on Ollama models</title>
    <updated>2025-02-14T01:15:16+00:00</updated>
    <author>
      <name>/u/hervalfreire</name>
      <uri>https://old.reddit.com/user/hervalfreire</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fellow Llamas,&lt;/p&gt; &lt;p&gt;I've been spending some time trying to develop some fully-offline projects using local LLMs, and stumbled upon a bit of a wall. Essentially, I'm trying to use tool calling with a local model, and failing with pretty much all of them.&lt;/p&gt; &lt;p&gt;The test is simple:&lt;/p&gt; &lt;p&gt;- there's a function for listing files in a directory&lt;/p&gt; &lt;p&gt;- the question I ask the LLM is simply how many files exist in the current folder + its parent&lt;/p&gt; &lt;p&gt;I'm using litellm since it helps calling ollama + remote models with the same interface. It also automatically adds instructions around function calling to the system prompt.&lt;/p&gt; &lt;p&gt;The results I got so far:&lt;/p&gt; &lt;p&gt;- Claude got it right every time (there's 12 files total)&lt;/p&gt; &lt;p&gt;- GPT responded in half the time, but was wrong (it hallucinated the number of files and directories)&lt;/p&gt; &lt;p&gt;- tinyllama couldn't figure out how to call the function at all&lt;/p&gt; &lt;p&gt;- mistral hallucinated different functions to try to sum the numbers&lt;/p&gt; &lt;p&gt;- qwen2.5 hallucinated a calculate_total_files that doesn't exist in one run, and got in a loop on another&lt;/p&gt; &lt;p&gt;- llama3.2 get in an infinite loop, calling the same function forever, consistently&lt;/p&gt; &lt;p&gt;- llama3.3 hallucinated a count_files that doesn't exist and failed&lt;/p&gt; &lt;p&gt;- deepseek-r1 hallucinated a list_iles function and failed&lt;/p&gt; &lt;p&gt;I included the code as well as results in a gist here: &lt;a href="https://gist.github.com/herval/e341dfc73ecb42bc27efa1243aaeb69b"&gt;https://gist.github.com/herval/e341dfc73ecb42bc27efa1243aaeb69b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious about everyone's experiences. Has anyone managed to get these models consistently work with function calling?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hervalfreire"&gt; /u/hervalfreire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioyxkm/how_to_do_proper_function_calling_on_ollama_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioyxkm/how_to_do_proper_function_calling_on_ollama_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ioyxkm/how_to_do_proper_function_calling_on_ollama_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T01:15:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipg7ne</id>
    <title>I want to create a Jarvis-like Business Intelligence with Open Web-UI and flowise.</title>
    <updated>2025-02-14T17:42:20+00:00</updated>
    <author>
      <name>/u/Unlucky-Cup1043</name>
      <uri>https://old.reddit.com/user/Unlucky-Cup1043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey guys, sorry for my bad english. I am currently connecting OWUI functions to process automation tools like flowise or N8N to create a unified centralized business intelligence. &lt;/p&gt; &lt;p&gt;Now I Need solutions for collecting necessary business data and how to store them (VectorDB?)so that Multi-agent systems would be able to correctly collect them and then transfer the necessary data to a function in OWUI so that the models can access the ‚ÄûBusiness-Brain‚Äú. So that for example a model could search in a google drive folder for saved pdfs of an invoice etc and give me required info. Does anybody have experience with something similar or has ideas on how you can make this work? &lt;/p&gt; &lt;p&gt;Thanks in advance! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unlucky-Cup1043"&gt; /u/Unlucky-Cup1043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipg7ne/i_want_to_create_a_jarvislike_business/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipg7ne/i_want_to_create_a_jarvislike_business/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipg7ne/i_want_to_create_a_jarvislike_business/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T17:42:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipgrcj</id>
    <title>How can I know the max value of `num_layers` of each LM in ollama?</title>
    <updated>2025-02-14T18:05:01+00:00</updated>
    <author>
      <name>/u/Responsible-Sky8889</name>
      <uri>https://old.reddit.com/user/Responsible-Sky8889</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like the know whats the architecture of each LM that I'm running locally using Ollama, so that I can get the most out of running it locally, by setting the parameter num_layers. However, I checked the official documentation of each model in ollama and I didnt manage to find it.&lt;/p&gt; &lt;p&gt;Is is something that can vary depending on the cuantization? Still, how can I know whats tha max value that each model supports?&lt;/p&gt; &lt;p&gt;Thanks a lot and sorry if this question is stupid for expertised programmers in ollama!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible-Sky8889"&gt; /u/Responsible-Sky8889 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipgrcj/how_can_i_know_the_max_value_of_num_layers_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipgrcj/how_can_i_know_the_max_value_of_num_layers_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipgrcj/how_can_i_know_the_max_value_of_num_layers_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T18:05:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip9nnn</id>
    <title>Ollama building problem</title>
    <updated>2025-02-14T12:38:28+00:00</updated>
    <author>
      <name>/u/AdhesivenessLatter57</name>
      <uri>https://old.reddit.com/user/AdhesivenessLatter57</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using ollama since starting, but earlier building from source code was easy. Since it moved to cmake system some times it builds with nvidia and some times not.&lt;/p&gt; &lt;p&gt;I am using following: cmake -B build cmake -- build build go build .&lt;/p&gt; &lt;p&gt;Cuda toolkit for nvidia is installed, and cmake build detects it.&lt;/p&gt; &lt;p&gt;But when running ollama it doesn't use nvidia gpu.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdhesivenessLatter57"&gt; /u/AdhesivenessLatter57 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip9nnn/ollama_building_problem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip9nnn/ollama_building_problem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ip9nnn/ollama_building_problem/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T12:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipm76f</id>
    <title>What is the maximum value for OLLAMA_NUM_PARALLEL in Ollama?</title>
    <updated>2025-02-14T21:59:57+00:00</updated>
    <author>
      <name>/u/Responsible-Sky8889</name>
      <uri>https://old.reddit.com/user/Responsible-Sky8889</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm setting up Ollama and looking to optimize performance for handling multiple concurrent requests. I know that the OLLAMA_NUM_PARALLEL parameter controls the number of parallel inferences, but I haven‚Äôt found any clear documentation on its upper limit. ‚Ä¢ What is the maximum value that can be assigned to OLLAMA_NUM_PARALLEL? ‚Ä¢ Is it hardware-dependent, or does Ollama have an internal cap? ‚Ä¢ Has anyone experimented with increasing it, and what impact did it have on performance?&lt;/p&gt; &lt;p&gt;I‚Äôd like to build a personal server for a side project that allows multiple inferences at once. &lt;/p&gt; &lt;p&gt;In the case that this parameter has some limitations? Can it be done using some other software?&lt;/p&gt; &lt;p&gt;Any insights or recommendations would be greatly appreciated! Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible-Sky8889"&gt; /u/Responsible-Sky8889 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipm76f/what_is_the_maximum_value_for_ollama_num_parallel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipm76f/what_is_the_maximum_value_for_ollama_num_parallel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipm76f/what_is_the_maximum_value_for_ollama_num_parallel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T21:59:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipnx82</id>
    <title>Best Metrics for Evaluating Locally Running Models</title>
    <updated>2025-02-14T23:18:43+00:00</updated>
    <author>
      <name>/u/thegauravverma</name>
      <uri>https://old.reddit.com/user/thegauravverma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to run multiple models locally to find the best one for my RAG-based app in terms of speed and efficiency. What key metrics should I focus on? I've been looking at eval rate, prompt eval rate (which seems to keep increasing with context), load duration, etc. Any other important factors to consider?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thegauravverma"&gt; /u/thegauravverma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipnx82/best_metrics_for_evaluating_locally_running_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipnx82/best_metrics_for_evaluating_locally_running_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipnx82/best_metrics_for_evaluating_locally_running_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T23:18:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioscui</id>
    <title>This is pure genius! Thank you!</title>
    <updated>2025-02-13T20:13:52+00:00</updated>
    <author>
      <name>/u/Apprehensive_Row9873</name>
      <uri>https://old.reddit.com/user/Apprehensive_Row9873</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all. I'm new here, I'm a french engineer. I was searching for a solution to self-host Mistral for days and couldn‚Äôt find the right way to do it correctly with Python and llama.cpp. I just couldn‚Äôt manage to offload the model to the GPU without CUDA errors. After lots of digging, I discovered vLLM and then Ollama. Just want to say THANK YOU! üôå This program works flawlessly from scratch on Docker üê≥, and I‚Äôll now implement it to auto-start Mistral and run directly in memory üß†‚ö°. This is incredible, huge thanks to the devs! üöÄüî•&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apprehensive_Row9873"&gt; /u/Apprehensive_Row9873 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioscui/this_is_pure_genius_thank_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioscui/this_is_pure_genius_thank_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ioscui/this_is_pure_genius_thank_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T20:13:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iphebt</id>
    <title>Can LLM access git repo ?</title>
    <updated>2025-02-14T18:32:07+00:00</updated>
    <author>
      <name>/u/Mundane-Tree-9336</name>
      <uri>https://old.reddit.com/user/Mundane-Tree-9336</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I would like to setup a local LLM using Ollama to get some help for a specific project. I was wondering if it's possible to give the LLM access to a private git repo ?&lt;/p&gt; &lt;p&gt;I'm working on a large project with a team, and I'd like to be able to ask question about this specific project (i.e. what are the dependencies of that file?, where is this function called?) , or get some help coding (i.e. Optimize that function, and it will optimize the function but also the function called inside of it, that might be in different files, or refactor that code, allowing it to move some function to different files, etc...)&lt;/p&gt; &lt;p&gt;Although all of these features might not be possible, I'd still like to connect the LLM to a code base (git repo, or local code).&lt;/p&gt; &lt;p&gt;Is this possible ?&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mundane-Tree-9336"&gt; /u/Mundane-Tree-9336 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iphebt/can_llm_access_git_repo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iphebt/can_llm_access_git_repo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iphebt/can_llm_access_git_repo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T18:32:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipsa8l</id>
    <title>Uploading custom source book to ollama...?</title>
    <updated>2025-02-15T03:09:45+00:00</updated>
    <author>
      <name>/u/Croestalker</name>
      <uri>https://old.reddit.com/user/Croestalker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For my campaign I've made a custom source book, but would like to have llama create quests and additional NPC's etc. what's the best way to have it retain info (give it a memory?) and feed my source book and it's updates?&lt;/p&gt; &lt;p&gt;Also, what model do you suggest? I only have a 1080 (8gb VRAM) with 32 GB Ram. So huge models won't work obviously. :(&lt;/p&gt; &lt;p&gt;(Can't accept my first post with a typo so deleted and re posted, haha.) Edit: can't accept my two typos... Ugh.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Croestalker"&gt; /u/Croestalker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipsa8l/uploading_custom_source_book_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipsa8l/uploading_custom_source_book_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipsa8l/uploading_custom_source_book_to_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T03:09:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip9772</id>
    <title>Ollama in Docker reports 100% GPU, but runs on CPU instead</title>
    <updated>2025-02-14T12:10:28+00:00</updated>
    <author>
      <name>/u/AmphibianFrog</name>
      <uri>https://old.reddit.com/user/AmphibianFrog</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had everything running really nicely on a Debian linux server with 3 GPUs. I bought a new AMD Threadripper CPU and motherboard, reinstalled everything, and now I am getting weird behaviour.&lt;/p&gt; &lt;p&gt;I have everything running in docker. If I restart ollama, and then load up a model it will run in the GPU. I can see it working in nvtop and it's very fast.&lt;/p&gt; &lt;p&gt;However, the next time I try to run a model after some time has passed it runs completely in my CPU.&lt;/p&gt; &lt;p&gt;If I do &lt;code&gt;ollama ps&lt;/code&gt; I see the following:&lt;/p&gt; &lt;p&gt;&lt;code&gt; NAME ID SIZE PROCESSOR UNTIL mistral-small:22b-instruct-2409-q8_0 ebe30125ec3c 29 GB 100% GPU 29 minutes from now &lt;/code&gt;&lt;/p&gt; &lt;p&gt;But inference is really slow, my GPUs are at 0% VRAM usage and about half of my CPU cores go to 100%.&lt;/p&gt; &lt;p&gt;If I restart ollama it will work again for a while and then revert to this.&lt;/p&gt; &lt;p&gt;I can't even tell if this is a problem with docker or ollama. Has anyone seen this before and does anyone know how to fix it?&lt;/p&gt; &lt;p&gt;Here is my output to nvidia-smi:&lt;/p&gt; &lt;p&gt;``` Fri Feb 14 12:10:59 2025&lt;br /&gt; +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 535.216.01 Driver Version: 535.216.01 CUDA Version: 12.2 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 NVIDIA GeForce RTX 3090 On | 00000000:21:00.0 Off | N/A | | 0% 39C P8 23W / 370W | 3MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ | 1 NVIDIA GeForce RTX 3060 On | 00000000:49:00.0 Off | N/A | | 0% 54C P8 17W / 170W | 3MiB / 12288MiB | 0% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ | 2 NVIDIA GeForce RTX 3070 Ti On | 00000000:4A:00.0 Off | N/A | | 0% 45C P8 17W / 290W | 3MiB / 8192MiB | 0% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+&lt;/p&gt; &lt;p&gt;+---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=======================================================================================| | No running processes found | +---------------------------------------------------------------------------------------+ ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AmphibianFrog"&gt; /u/AmphibianFrog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip9772/ollama_in_docker_reports_100_gpu_but_runs_on_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip9772/ollama_in_docker_reports_100_gpu_but_runs_on_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ip9772/ollama_in_docker_reports_100_gpu_but_runs_on_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T12:10:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipmdu3</id>
    <title>Hardware question: Does anyone know what the AMD AI accelerator cores do ?</title>
    <updated>2025-02-14T22:07:50+00:00</updated>
    <author>
      <name>/u/Living-Cheek-2273</name>
      <uri>https://old.reddit.com/user/Living-Cheek-2273</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to upgrade my gtx 970 to be actually able to run AI and my options are:&lt;/p&gt; &lt;p&gt;-the 6700xt 12gb of vram (250‚Ç¨) faster and easy to get &lt;/p&gt; &lt;p&gt;-the 7600xt 16gb of vram (300‚Ç¨) hard to find but gets the AI accelerator cores and more vram&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So I guess I want to find out if the extra vram and the AI cores are worth it for AI&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Living-Cheek-2273"&gt; /u/Living-Cheek-2273 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipmdu3/hardware_question_does_anyone_know_what_the_amd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipmdu3/hardware_question_does_anyone_know_what_the_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipmdu3/hardware_question_does_anyone_know_what_the_amd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T22:07:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipekhy</id>
    <title>x2 RTX 3060 12GB VRAM</title>
    <updated>2025-02-14T16:32:47+00:00</updated>
    <author>
      <name>/u/VariousGrand</name>
      <uri>https://old.reddit.com/user/VariousGrand</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you think that having two RTX 360 with 12Gb VRAM each is enough to run deepseek-r1 32b?&lt;/p&gt; &lt;p&gt;Or there any other option you think it will have better performance?&lt;/p&gt; &lt;p&gt;Would be better maybe to have Titan RTX with 24gb of vram? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VariousGrand"&gt; /u/VariousGrand &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipekhy/x2_rtx_3060_12gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipekhy/x2_rtx_3060_12gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipekhy/x2_rtx_3060_12gb_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T16:32:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip99f5</id>
    <title>I built this GUI for Ollama, also have built-in knowledge base and note, hope you like it!</title>
    <updated>2025-02-14T12:14:12+00:00</updated>
    <author>
      <name>/u/w-zhong</name>
      <uri>https://old.reddit.com/user/w-zhong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ip99f5/i_built_this_gui_for_ollama_also_have_builtin/"&gt; &lt;img alt="I built this GUI for Ollama, also have built-in knowledge base and note, hope you like it!" src="https://preview.redd.it/242otrvdi4ie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7cac77d270c8ee5ddc11b42dfa6096d5d230fb61" title="I built this GUI for Ollama, also have built-in knowledge base and note, hope you like it!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w-zhong"&gt; /u/w-zhong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/242otrvdi4ie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip99f5/i_built_this_gui_for_ollama_also_have_builtin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ip99f5/i_built_this_gui_for_ollama_also_have_builtin/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T12:14:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipmpr0</id>
    <title>What's the best LLM I can run with at least 10 t/s on 24 cores, 215GB ram &amp; 8GB vram?</title>
    <updated>2025-02-14T22:22:15+00:00</updated>
    <author>
      <name>/u/MarinatedPickachu</name>
      <uri>https://old.reddit.com/user/MarinatedPickachu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an older workstation with plenty of ram and 2x12 2.9ghz cores, but only an rtx 2070 super. The memory is afaik also divided into two numa-nodes, not sure how this would affect LLM performance. Is there anything interesting I could run on this at reasonable speed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarinatedPickachu"&gt; /u/MarinatedPickachu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipmpr0/whats_the_best_llm_i_can_run_with_at_least_10_ts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipmpr0/whats_the_best_llm_i_can_run_with_at_least_10_ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipmpr0/whats_the_best_llm_i_can_run_with_at_least_10_ts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T22:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipla7v</id>
    <title>Promptable Video Redaction: Use Moondream to redact content with a prompt (open source)</title>
    <updated>2025-02-14T21:19:00+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ipla7v/promptable_video_redaction_use_moondream_to/"&gt; &lt;img alt="Promptable Video Redaction: Use Moondream to redact content with a prompt (open source)" src="https://external-preview.redd.it/Zng2d3BhbWc4NmplMZN2WL68RoAkfEFkGlg6y4sh7yXh5lDDNxO3LBLK1287.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=599aed209532bc7ea0e78aa6c93c55dd068b89e9" title="Promptable Video Redaction: Use Moondream to redact content with a prompt (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/djtn6gmg86je1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipla7v/promptable_video_redaction_use_moondream_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipla7v/promptable_video_redaction_use_moondream_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T21:19:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipw9og</id>
    <title>Is there a model that actually can examine and read all of a pdf?</title>
    <updated>2025-02-15T07:26:33+00:00</updated>
    <author>
      <name>/u/Aleilnonno</name>
      <uri>https://old.reddit.com/user/Aleilnonno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried Llama3.1 8b and Phi4 14b, but they just make a short summary and then they proceed to analyse. How can I solve?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aleilnonno"&gt; /u/Aleilnonno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipw9og/is_there_a_model_that_actually_can_examine_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipw9og/is_there_a_model_that_actually_can_examine_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipw9og/is_there_a_model_that_actually_can_examine_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T07:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipeutl</id>
    <title>I created a free, open source Web extension to run Ollama</title>
    <updated>2025-02-14T16:45:21+00:00</updated>
    <author>
      <name>/u/gerpann</name>
      <uri>https://old.reddit.com/user/gerpann</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey fellow developers! üëã I'm excited to introduce &lt;strong&gt;Ollamazing&lt;/strong&gt;, a browser extension that brings the power of local AI models directly into your browsing experience. Let me share why you might want to give it a try.&lt;/p&gt; &lt;h1&gt;What is Ollamazing?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Ollamazing&lt;/strong&gt; is a free, open-source browser extension that connects with &lt;strong&gt;Ollama&lt;/strong&gt; to run AI models locally on your machine. Think of it as having ChatGPT-like (or even Deepseek for newer) capabilities, but with complete privacy and no subscription fees.&lt;/p&gt; &lt;h1&gt;üåü Key Features&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;100% Free and Open Source &lt;ul&gt; &lt;li&gt;No hidden costs or subscription fees&lt;/li&gt; &lt;li&gt;Fully open-source codebase&lt;/li&gt; &lt;li&gt;Community-driven development&lt;/li&gt; &lt;li&gt;Transparent about how your data is handled&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Local AI Processing &lt;ul&gt; &lt;li&gt;Thanks to Ollama, we can run AI models directly on your machine&lt;/li&gt; &lt;li&gt;Complete privacy - your data never leaves your computer&lt;/li&gt; &lt;li&gt;Works offline once models are downloaded&lt;/li&gt; &lt;li&gt;Support for various open-source models (&lt;em&gt;llama3.3&lt;/em&gt;, &lt;em&gt;gemma&lt;/em&gt;, &lt;em&gt;phi4&lt;/em&gt;, &lt;em&gt;qwen&lt;/em&gt;, &lt;em&gt;mistral&lt;/em&gt;, &lt;em&gt;codellama&lt;/em&gt;, etc.) and specially &lt;strong&gt;&lt;em&gt;deepseek-r1&lt;/em&gt;&lt;/strong&gt; - the most popular open source model at current time.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Seamless Browser Integration &lt;ul&gt; &lt;li&gt;Chat with AI right from your browser sidebar&lt;/li&gt; &lt;li&gt;Text selection support for quick queries&lt;/li&gt; &lt;li&gt;Context-aware responses based on the current webpage&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Developer-Friendly Features &lt;ul&gt; &lt;li&gt;Code completion and explanation&lt;/li&gt; &lt;li&gt;Documentation generation&lt;/li&gt; &lt;li&gt;Code review assistance&lt;/li&gt; &lt;li&gt;Bug fixing suggestions&lt;/li&gt; &lt;li&gt;Multiple programming language support&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Easy Setup &lt;ul&gt; &lt;li&gt;Install Ollama on your machine or any remote server&lt;/li&gt; &lt;li&gt;Download your preferred models&lt;/li&gt; &lt;li&gt;Install the Ollamazing browser extension&lt;/li&gt; &lt;li&gt;Start chatting with AI!&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;üöÄ Getting Started&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# 1. Install Ollama curl -fsSL https://ollama.com/install.sh | sh # 2. Pull your first model (e.g., Deepseek R1 7 billion parameters) ollama pull deepseek-r1:7b &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then simply install the extension from your browser's extension store, and you're ready to go!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;For more information about Ollama, please visit the &lt;a href="https://ollama.com/"&gt;official website&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: If you run Ollama on local machine, ensure to setup the &lt;code&gt;OLLAMA_ORIGINS&lt;/code&gt; to allow the extension can connect to the server. For more details, read &lt;a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-configure-ollama-server"&gt;Ollama FAQ&lt;/a&gt;, set the &lt;code&gt;OLLAMA_ORIGINS&lt;/code&gt; to &lt;code&gt;*&lt;/code&gt; or &lt;code&gt;chrome-extension://*&lt;/code&gt; or the domain you want to allow.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;üí° Use Cases&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Code completion and explanation&lt;/li&gt; &lt;li&gt;Documentation generation&lt;/li&gt; &lt;li&gt;Code review assistance&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üîí Privacy First&lt;/h1&gt; &lt;p&gt;Unlike cloud-based AI assistants, Ollamazing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Keeps your data on your machine&lt;/li&gt; &lt;li&gt;Doesn't require an internet connection for inference&lt;/li&gt; &lt;li&gt;Gives you full control over which model to use&lt;/li&gt; &lt;li&gt;Allows you to audit the code and know exactly what's happening with your data&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üõ†Ô∏è Technical Stack&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Use framework &lt;a href="https://wxt.dev/"&gt;WXT&lt;/a&gt; to build the extension&lt;/li&gt; &lt;li&gt;Built with React and TypeScript&lt;/li&gt; &lt;li&gt;Uses Valtio for state management&lt;/li&gt; &lt;li&gt;Implements TanStack Query for efficient data fetching&lt;/li&gt; &lt;li&gt;Follows modern web extension best practices&lt;/li&gt; &lt;li&gt;Utilizes Shadcn/UI for a clean, modern interface&lt;/li&gt; &lt;li&gt;Use i18n for multi-language support&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;ü§ù Contributing&lt;/h1&gt; &lt;p&gt;We welcome contributions! Whether it's:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adding new features&lt;/li&gt; &lt;li&gt;Improving documentation&lt;/li&gt; &lt;li&gt;Reporting bugs&lt;/li&gt; &lt;li&gt;Suggesting enhancements&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out our GitHub repository &lt;a href="https://github.com/buiducnhat/ollamazing"&gt;https://github.com/buiducnhat/ollamazing&lt;/a&gt; to get started!&lt;/p&gt; &lt;h1&gt;üîÆ Future Plans&lt;/h1&gt; &lt;p&gt;We're working on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Enhanced context awareness&lt;/li&gt; &lt;li&gt;Custom model fine-tuning support&lt;/li&gt; &lt;li&gt;Improve UI/UX&lt;/li&gt; &lt;li&gt;Improved performance optimizations&lt;/li&gt; &lt;li&gt;Additional browser support&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try It Today!&lt;/h1&gt; &lt;p&gt;Ready to experience local AI in your browser? Get started with Ollamazing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Chrome web store: &lt;a href="https://chromewebstore.google.com/detail/ollamazing/bfndpdpimcehljfgjdacbpapgbkecahi"&gt;https://chromewebstore.google.com/detail/ollamazing/bfndpdpimcehljfgjdacbpapgbkecahi&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub repository: &lt;a href="https://github.com/buiducnhat/ollamazing"&gt;https://github.com/buiducnhat/ollamazing&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Product Hunt: &lt;a href="https://www.producthunt.com/posts/ollamazing"&gt;https://www.producthunt.com/posts/ollamazing&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know in the comments if you have any questions or feedback! Have you tried running AI models locally before? What features would you like to see in Ollamazing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gerpann"&gt; /u/gerpann &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipeutl/i_created_a_free_open_source_web_extension_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipeutl/i_created_a_free_open_source_web_extension_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipeutl/i_created_a_free_open_source_web_extension_to_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T16:45:21+00:00</published>
  </entry>
</feed>
