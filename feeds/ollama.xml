<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-05-21T22:06:07+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1kpwptc</id>
    <title>Why changing num_gpu has a much bigger impact on Gemma3 than Qwen3?</title>
    <updated>2025-05-18T22:47:47+00:00</updated>
    <author>
      <name>/u/S4lVin</name>
      <uri>https://old.reddit.com/user/S4lVin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, basically, was testing out some settings to have the best performance with each model.&lt;/p&gt; &lt;p&gt;I found out that by running the default num_gpu value (which i don't know what is it on Open WebUI) Gemma3 12B QAT runs at about 13-14T/s (Using ~40% GPU and ~95% CPU), while Qwen3 runs at about 60T/s (Using ~95% GPU and ~25% CPU).&lt;/p&gt; &lt;p&gt;If i increase the num_gpu value to 256, Gemma3 runs at about 60T/s (Using ~95% GPU and ~25% CPU), while Qwen3 runs the same as before.&lt;/p&gt; &lt;p&gt;Why does this happen? It's as if Qwen3 is already set with num_gpu maxed out, while Gemma3 does not. But i suppose num_gpu is set by default to all models, and it doesn't change from model to model, or am i wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/S4lVin"&gt; /u/S4lVin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpwptc/why_changing_num_gpu_has_a_much_bigger_impact_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpwptc/why_changing_num_gpu_has_a_much_bigger_impact_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kpwptc/why_changing_num_gpu_has_a_much_bigger_impact_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-18T22:47:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpzahp</id>
    <title>AI Model for Handwriting OCR Recognition?</title>
    <updated>2025-05-19T00:59:00+00:00</updated>
    <author>
      <name>/u/Wonderful-Truth-4849</name>
      <uri>https://old.reddit.com/user/Wonderful-Truth-4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm pretty new to using offline AI models and could really use some advice. I‚Äôm in the process of digitizing some old diaries, and I‚Äôm considering subscribing to Transkribus, but before committing, I want to test out some offline OCR models to see what works best.&lt;/p&gt; &lt;p&gt;I did give ChatGPT a try for handwriting recognition, and it actually did a solid job, but unfortunately, due to copyright and permissions, I can‚Äôt use it for this project. So now I‚Äôm on the hunt for other good offline options.&lt;/p&gt; &lt;p&gt;Any recommendations or experiences with OCR models that work well for handwritten text would be super helpful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wonderful-Truth-4849"&gt; /u/Wonderful-Truth-4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpzahp/ai_model_for_handwriting_ocr_recognition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpzahp/ai_model_for_handwriting_ocr_recognition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kpzahp/ai_model_for_handwriting_ocr_recognition/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T00:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqc20q</id>
    <title>Log auto analysis</title>
    <updated>2025-05-19T13:44:16+00:00</updated>
    <author>
      <name>/u/wizz772</name>
      <uri>https://old.reddit.com/user/wizz772</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SO I am working on a project and my aim is to figure out failures bases on error logs using AI,&lt;/p&gt; &lt;p&gt;I'm currently storing the logs with the manual analysis in a vector db&lt;/p&gt; &lt;p&gt;I plan on using ollama -&amp;gt; llama as a RAG for auto analysis how do I introduce RL and rate whether the output by RAG was good or not and better the output&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wizz772"&gt; /u/wizz772 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqc20q/log_auto_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqc20q/log_auto_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kqc20q/log_auto_analysis/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T13:44:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq5ake</id>
    <title>Clara ‚Äî A fully offline, Modular AI workspace (LLMs + Agents + Automation + Image Gen)</title>
    <updated>2025-05-19T06:56:03+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kq5ake/clara_a_fully_offline_modular_ai_workspace_llms/"&gt; &lt;img alt="Clara ‚Äî A fully offline, Modular AI workspace (LLMs + Agents + Automation + Image Gen)" src="https://preview.redd.it/u6niruxjqo1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92c4cac8e33b1fe68fdc0af3f66d45dcdcf1c55a" title="Clara ‚Äî A fully offline, Modular AI workspace (LLMs + Agents + Automation + Image Gen)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u6niruxjqo1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kq5ake/clara_a_fully_offline_modular_ai_workspace_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kq5ake/clara_a_fully_offline_modular_ai_workspace_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T06:56:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpr3nu</id>
    <title>My Godot game is using Ollama+LLama 3.1 to act as the Game Master</title>
    <updated>2025-05-18T18:40:49+00:00</updated>
    <author>
      <name>/u/According-Moose2931</name>
      <uri>https://old.reddit.com/user/According-Moose2931</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kpr3nu/my_godot_game_is_using_ollamallama_31_to_act_as/"&gt; &lt;img alt="My Godot game is using Ollama+LLama 3.1 to act as the Game Master" src="https://b.thumbs.redditmedia.com/fSFiY10TeECMbnJQvqKSTbwzh3rhXykErr_0HZIiO4Y.jpg" title="My Godot game is using Ollama+LLama 3.1 to act as the Game Master" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/According-Moose2931"&gt; /u/According-Moose2931 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kpr3nu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpr3nu/my_godot_game_is_using_ollamallama_31_to_act_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kpr3nu/my_godot_game_is_using_ollamallama_31_to_act_as/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-18T18:40:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqlolh</id>
    <title>How to store different models on multiple drives?</title>
    <updated>2025-05-19T20:05:34+00:00</updated>
    <author>
      <name>/u/__ThrowAway__123___</name>
      <uri>https://old.reddit.com/user/__ThrowAway__123___</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have my models stored on an NVMe drive (C drive) that is running out of storage space. I want to move &lt;em&gt;some&lt;/em&gt; of the models I use less frequently to a slower drive. From what I could find so far, I understand it is possible to create symlinks to specific models stored on a different drive, however my .ollama\models folder only contains a folder called &amp;quot;manifest&amp;quot; and a folder called &amp;quot;blobs&amp;quot;, with separate files in it with hashes as a name, &amp;quot;sha256-...&amp;quot;, with a few big files (weights) and files of a few KB. By sorting by date modified and looking at the size I can see which files belong together and which is which, however I have a feeling that moving those together and linking them may cause issues. &lt;/p&gt; &lt;p&gt;Is there a better way to do this? Or is creating symlinks for all of those individual files fine?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__ThrowAway__123___"&gt; /u/__ThrowAway__123___ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqlolh/how_to_store_different_models_on_multiple_drives/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqlolh/how_to_store_different_models_on_multiple_drives/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kqlolh/how_to_store_different_models_on_multiple_drives/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T20:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqgfk8</id>
    <title>High CPU and Low GPU?</title>
    <updated>2025-05-19T16:42:24+00:00</updated>
    <author>
      <name>/u/sandman_br</name>
      <uri>https://old.reddit.com/user/sandman_br</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using VSCODO, CLINE, OLLAMA + deepcoder, and the code generation is very slow. But my CPU is at 80% and my GPU is at 5%.&lt;/p&gt; &lt;p&gt;Any clues why it is so slow and why the CPU is way heavily used than the GPU (RTX4070)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandman_br"&gt; /u/sandman_br &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqgfk8/high_cpu_and_low_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqgfk8/high_cpu_and_low_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kqgfk8/high_cpu_and_low_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T16:42:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq8bcj</id>
    <title>Any lightweight AI model for ollama that can be trained to do queries and read software manuals?</title>
    <updated>2025-05-19T10:29:02+00:00</updated>
    <author>
      <name>/u/Palova98</name>
      <uri>https://old.reddit.com/user/Palova98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;I will explain myself better here. &lt;/p&gt; &lt;p&gt;I work for an IT company that integrates an accountability software with basically no public knowledge. &lt;/p&gt; &lt;p&gt;We would like to train an AI that we can feed all the internal PDF manuals and the database structure so we can ask him to make queries for us and troubleshoot problems with the software (ChatGPT found a way to give the model access to a Microsoft SQL server, though I just read this information, still have to actually try) . &lt;/p&gt; &lt;p&gt;Sadly we have a few servers in our datacenter but they are all classic old-ish Xeon CPUs with, of course, tens of other VMs running, so when i tried an ollama docker container with llama3 it takes several minutes for the engine to answer anything. (16 vCPUs and 24G RAM). &lt;/p&gt; &lt;p&gt;So, now that you know the contest, I'm here to ask:&lt;/p&gt; &lt;p&gt;1) Does Ollama have better, lighter models than llama3 to do read and learn pdf manuals and read data from a database via query? &lt;/p&gt; &lt;p&gt;2) What kind of hardware do i need to make it usable? any embedded board like Nvidia's Orin Nano Super Dev kit can work? a mini-pc with an i9? A freakin' 5090 or some other serious GPU? &lt;/p&gt; &lt;p&gt;Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Palova98"&gt; /u/Palova98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kq8bcj/any_lightweight_ai_model_for_ollama_that_can_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kq8bcj/any_lightweight_ai_model_for_ollama_that_can_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kq8bcj/any_lightweight_ai_model_for_ollama_that_can_be/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T10:29:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqdqy7</id>
    <title>Summarizing information in a database</title>
    <updated>2025-05-19T14:55:21+00:00</updated>
    <author>
      <name>/u/newz2000</name>
      <uri>https://old.reddit.com/user/newz2000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm not quite sure the right words to search for. I have a sqlite database with a record of important customer communication. I would like to attempt to search it with a local llm and have been using Ollama on other projects successfully.&lt;/p&gt; &lt;p&gt;I can run SQL queries on the data and I have created a python tool that can create a report. But I'd like to take it to the next level. For example:&lt;/p&gt; &lt;p&gt;* When was it that I talked to Jack about his pricing questions?&lt;/p&gt; &lt;p&gt;* Who was it that said they had a child graduating this spring?&lt;/p&gt; &lt;p&gt;* Have I missed any important follow-ups from the last week?&lt;/p&gt; &lt;p&gt;I have Gemini as part of Google Workspace and my first thought was that I can create a Google Doc per person and then use Gemini to query it. This is possible, but since the data is constantly changing, this is actually harder than it sounds.&lt;/p&gt; &lt;p&gt;Any tips on how to find relevant info?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newz2000"&gt; /u/newz2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqdqy7/summarizing_information_in_a_database/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqdqy7/summarizing_information_in_a_database/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kqdqy7/summarizing_information_in_a_database/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T14:55:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr6sl1</id>
    <title>I have deleted llama 3.1 now facing issue (urgent help)</title>
    <updated>2025-05-20T14:50:31+00:00</updated>
    <author>
      <name>/u/LostKaleidoscope278</name>
      <uri>https://old.reddit.com/user/LostKaleidoscope278</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;my Mac is glitching after I uninstalled llama3.1 so I went in to terminal and typed Ollama rm llama3.1 it was done then I went to application and deleted my Ollama right from next sec my Mac (M1) is glitching and all icons are bright I am getting too much white light what to do.....(I had done with software update and chatgpt instructions non of them are working )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostKaleidoscope278"&gt; /u/LostKaleidoscope278 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kr6sl1/i_have_deleted_llama_31_now_facing_issue_urgent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kr6sl1/i_have_deleted_llama_31_now_facing_issue_urgent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kr6sl1/i_have_deleted_llama_31_now_facing_issue_urgent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-20T14:50:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqlig3</id>
    <title>Wrapped up OllamaUI. Should I stop now or break it again?</title>
    <updated>2025-05-19T19:58:56+00:00</updated>
    <author>
      <name>/u/andreadev3d</name>
      <uri>https://old.reddit.com/user/andreadev3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kqlig3/wrapped_up_ollamaui_should_i_stop_now_or_break_it/"&gt; &lt;img alt="Wrapped up OllamaUI. Should I stop now or break it again?" src="https://b.thumbs.redditmedia.com/8OfSXmvENCvMZJHmj5Rn_wTMYLknhFT-8u5uPcUCwiI.jpg" title="Wrapped up OllamaUI. Should I stop now or break it again?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've run out of things to implement for now, at least until I figure out how to get an MCP agent working in vanilla JS without a backend.&lt;/p&gt; &lt;p&gt;That said, I'm considering adding a chat history feature, but I'm not sure how useful it would be for most users.&lt;/p&gt; &lt;p&gt;If you have ideas or want to see specific features added, I‚Äôd love to hear from you!&lt;/p&gt; &lt;p&gt;Feel free to&lt;a href="https://discord.gg/MBxDKkg6YK"&gt; join my Discord&lt;/a&gt; for a friendly chat and to share your thoughts.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/AndreaDev3D/OllamaChat"&gt;https://github.com/AndreaDev3D/OllamaChat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As usual any feedback is appreciated.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hajxpexyls1f1.png?width=1811&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=58cbf61220545f7124323a88fcd5aeb57fc44570"&gt;https://preview.redd.it/hajxpexyls1f1.png?width=1811&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=58cbf61220545f7124323a88fcd5aeb57fc44570&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andreadev3d"&gt; /u/andreadev3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqlig3/wrapped_up_ollamaui_should_i_stop_now_or_break_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqlig3/wrapped_up_ollamaui_should_i_stop_now_or_break_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kqlig3/wrapped_up_ollamaui_should_i_stop_now_or_break_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T19:58:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqp33a</id>
    <title>I added automatic language detection and text-to-speech response to AI Runner</title>
    <updated>2025-05-19T22:23:42+00:00</updated>
    <author>
      <name>/u/w00fl35</name>
      <uri>https://old.reddit.com/user/w00fl35</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kqp33a/i_added_automatic_language_detection_and/"&gt; &lt;img alt="I added automatic language detection and text-to-speech response to AI Runner" src="https://external-preview.redd.it/d2tuOWVtMnVkdDFmMTuTBNgqFywp7VxarWureDzUbFixi-3H8s4hiED7R6fh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a996d2a893cbd939affe5d9d6b34532354677cd7" title="I added automatic language detection and text-to-speech response to AI Runner" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w00fl35"&gt; /u/w00fl35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6zv0e6g7dt1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqp33a/i_added_automatic_language_detection_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kqp33a/i_added_automatic_language_detection_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T22:23:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr2ejr</id>
    <title>Need Terminal UI suggestions for Windows</title>
    <updated>2025-05-20T11:22:54+00:00</updated>
    <author>
      <name>/u/anon_e_mouse1</name>
      <uri>https://old.reddit.com/user/anon_e_mouse1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, can you suggest some terminal UIs for chatting with models through Ollama? They should be easy to set up. &lt;/p&gt; &lt;p&gt;I'm not a developer. I just want to try some terminal-style UIs for fun. I recently used Oterm. I like it, but there are a few things I wish it had. So, I wanted to see what other UIs are out there.&lt;br /&gt; I'm on Windows.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anon_e_mouse1"&gt; /u/anon_e_mouse1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kr2ejr/need_terminal_ui_suggestions_for_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kr2ejr/need_terminal_ui_suggestions_for_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kr2ejr/need_terminal_ui_suggestions_for_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-20T11:22:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr0xhv</id>
    <title>Not so Smart Agent (Ollama, Spring AI, MCP)</title>
    <updated>2025-05-20T09:51:18+00:00</updated>
    <author>
      <name>/u/DependentOk7737</name>
      <uri>https://old.reddit.com/user/DependentOk7737</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on a simple Spring AI agent that runs local LLMs via Ollama. It also acts as an MCP client with a couple of MCP server integrations (Web Content Fetching, Context7).&lt;/p&gt; &lt;p&gt;Right now, it's nothing special, but I plan to expand it gradually.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/nktltvnv/smart-agent"&gt;https://github.com/nktltvnv/smart-agent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DependentOk7737"&gt; /u/DependentOk7737 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kr0xhv/not_so_smart_agent_ollama_spring_ai_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kr0xhv/not_so_smart_agent_ollama_spring_ai_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kr0xhv/not_so_smart_agent_ollama_spring_ai_mcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-20T09:51:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqo8ze</id>
    <title>Observer Micro Agents with Ollama demo!</title>
    <updated>2025-05-19T21:48:02+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kqo8ze/observer_micro_agents_with_ollama_demo/"&gt; &lt;img alt="Observer Micro Agents with Ollama demo!" src="https://external-preview.redd.it/Y3hqdjV4aDY3dDFmMUseoVY8fQTbYJfjqlW4w2NBhsFRYZKCiBtmbkUNYsUI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3a649ad3d73eb21cc664740a837af5243bd5ff2" title="Observer Micro Agents with Ollama demo!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/slzlvvh67t1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqo8ze/observer_micro_agents_with_ollama_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kqo8ze/observer_micro_agents_with_ollama_demo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T21:48:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1krk2ur</id>
    <title>Also new to OLLAMA .... have installed msty 19.2.9 not working</title>
    <updated>2025-05-20T23:59:30+00:00</updated>
    <author>
      <name>/u/Reasonable-Watch-497</name>
      <uri>https://old.reddit.com/user/Reasonable-Watch-497</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have installed msty x64 19.2.0 first use even though I told it local model would not let me do any work and wanted an authorization key. the next set of installs the icons are created but no gui screen comes up. OS is windows 10 (I will update soon)...really need help on this issue....thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable-Watch-497"&gt; /u/Reasonable-Watch-497 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1krk2ur/also_new_to_ollama_have_installed_msty_1929_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1krk2ur/also_new_to_ollama_have_installed_msty_1929_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1krk2ur/also_new_to_ollama_have_installed_msty_1929_not/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-20T23:59:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1krgihs</id>
    <title>ClipAI: connect your clipboard to Ollama</title>
    <updated>2025-05-20T21:18:30+00:00</updated>
    <author>
      <name>/u/Comprehensive-Bird59</name>
      <uri>https://old.reddit.com/user/Comprehensive-Bird59</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1krgihs/clipai_connect_your_clipboard_to_ollama/"&gt; &lt;img alt="ClipAI: connect your clipboard to Ollama" src="https://a.thumbs.redditmedia.com/I2aeHPP-fP7SsR_oc_u_JXSDUVljd1JeHx3YG6gwct4.jpg" title="ClipAI: connect your clipboard to Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;CLAIM:&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;ClipAI&lt;/strong&gt; is a simple but powerful utility to connect your &lt;strong&gt;clipboard&lt;/strong&gt; üìã directly to a &lt;strong&gt;Local LLM&lt;/strong&gt; ü§ñ (Ollama-based) such as &lt;strong&gt;Gemma 3&lt;/strong&gt;, &lt;strong&gt;Phi 4&lt;/strong&gt;, &lt;strong&gt;Deepseek-V3&lt;/strong&gt;, &lt;strong&gt;Qwen&lt;/strong&gt;, &lt;strong&gt;Llama 3.x&lt;/strong&gt;, etc. It is a &lt;strong&gt;clipboard viewer&lt;/strong&gt; and &lt;strong&gt;text transformer&lt;/strong&gt; application built using &lt;strong&gt;Python&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It is your &lt;strong&gt;daily companion&lt;/strong&gt; for any &lt;strong&gt;writing-related job&lt;/strong&gt; ‚úèÔ∏èüìÑ. Easy peasy.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/91btjuf7802f1.png?width=204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9c86ccbc98810e91d85b75d530e67c6f1f51e9a"&gt;https://preview.redd.it/91btjuf7802f1.png?width=204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9c86ccbc98810e91d85b75d530e67c6f1f51e9a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;REALITY:&lt;/strong&gt;&lt;br /&gt; So, it‚Äôs the 100th application that implements a chat/interaction with an LLM, but I aimed for something really simple to &amp;quot;drag and drop&amp;quot; while working, obviously focused on writing.&lt;/p&gt; &lt;p&gt;I was having trouble translating text on the fly and now I use ClipAI, which is working well for me. It‚Äôs at least solved one of my problems!&lt;/p&gt; &lt;p&gt;Feedback are appreciated.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/markod0925/ClipAI"&gt;https://github.com/markod0925/ClipAI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pi0mjx3f802f1.png?width=702&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0589c3969f9381e6bdeb21fb65cec54a82266390"&gt;https://preview.redd.it/pi0mjx3f802f1.png?width=702&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0589c3969f9381e6bdeb21fb65cec54a82266390&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comprehensive-Bird59"&gt; /u/Comprehensive-Bird59 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1krgihs/clipai_connect_your_clipboard_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1krgihs/clipai_connect_your_clipboard_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1krgihs/clipai_connect_your_clipboard_to_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-20T21:18:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1krbg6i</id>
    <title>IDEA: Record your voice prompts, copy them straight into Ollama (100% local)</title>
    <updated>2025-05-20T17:54:03+00:00</updated>
    <author>
      <name>/u/lukerm_zl</name>
      <uri>https://old.reddit.com/user/lukerm_zl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've integrated a simple voice recorder with Ollama.&lt;/p&gt; &lt;p&gt;Hopefully useful. Let me know if you have any ideas to improve. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lukerm_zl"&gt; /u/lukerm_zl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/lukerm/vosk-dictation?tab=readme-ov-file#ollama-"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1krbg6i/idea_record_your_voice_prompts_copy_them_straight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1krbg6i/idea_record_your_voice_prompts_copy_them_straight/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-20T17:54:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1krnzxi</id>
    <title>Improvement in the ollama-python tool system: refactoring, organization and better support for AI context</title>
    <updated>2025-05-21T03:20:35+00:00</updated>
    <author>
      <name>/u/chavomodder</name>
      <uri>https://old.reddit.com/user/chavomodder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1krnzxi/improvement_in_the_ollamapython_tool_system/"&gt; &lt;img alt="Improvement in the ollama-python tool system: refactoring, organization and better support for AI context" src="https://external-preview.redd.it/d0pyMuGXhi1u8xGfnFw3LaCoEAGKKUnQIwS1Kup-IcU.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4761175bca0491db83b3f8f2f316c63c1e171664" title="Improvement in the ollama-python tool system: refactoring, organization and better support for AI context" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys!&lt;/p&gt; &lt;p&gt;Previously, I took the initiative to create decorators to facilitate tool registration in ollama-python, but I realized that some parts of the system were still poorly organized or unclear. So I decided to refactor and improve several points. Here are the main changes:&lt;/p&gt; &lt;p&gt;I created the _tools.py module to centralize everything related to tools&lt;/p&gt; &lt;p&gt;I renamed functions to clearer names&lt;/p&gt; &lt;p&gt;Fixed bugs and improved registration and tool search&lt;/p&gt; &lt;p&gt;I added support for extracting the name and description of tools, useful for the AI ‚Äã‚Äãcontext (example: you are an assistant and have access to the following tools {get_ollama_tool_description})&lt;/p&gt; &lt;p&gt;Docstrings are now used as description automatically&lt;/p&gt; &lt;p&gt;It will return something like: ({ &amp;quot;Calculator&amp;quot;: &amp;quot;calculates numbers&amp;quot; &amp;quot;search_web&amp;quot;: Performs searches on the web })&lt;/p&gt; &lt;p&gt;More modular and tested code with new test suite&lt;/p&gt; &lt;p&gt;These changes make the use of tools simpler and more efficient for those who develop with the library.&lt;/p&gt; &lt;p&gt;commit link: &lt;a href="https://github.com/ollama/ollama-python/pull/516/commits/49ed36bf4789c754102fc05d2f911bbec5ea9cc6"&gt;https://github.com/ollama/ollama-python/pull/516/commits/49ed36bf4789c754102fc05d2f911bbec5ea9cc6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chavomodder"&gt; /u/chavomodder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ollama/ollama-python/pull/516/commits/49ed36bf4789c754102fc05d2f911bbec5ea9cc6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1krnzxi/improvement_in_the_ollamapython_tool_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1krnzxi/improvement_in_the_ollamapython_tool_system/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-21T03:20:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks1358</id>
    <title>Anyone else getting garbage output from models after updating to 0.7?</title>
    <updated>2025-05-21T15:43:51+00:00</updated>
    <author>
      <name>/u/aaronr_90</name>
      <uri>https://old.reddit.com/user/aaronr_90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am on Ubuntu 22.04 and was using Codestral, Mistral Small and Qwen 2.5. All models responded as if a large needy can was prancing all over the keyboard. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aaronr_90"&gt; /u/aaronr_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ks1358/anyone_else_getting_garbage_output_from_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ks1358/anyone_else_getting_garbage_output_from_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ks1358/anyone_else_getting_garbage_output_from_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-21T15:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks5860</id>
    <title>Advice on the AI/LLM "GPU triangle" - the tradeoffs between Price/Cost, Size (VRAM), and Speed</title>
    <updated>2025-05-21T18:28:23+00:00</updated>
    <author>
      <name>/u/TorrentRover</name>
      <uri>https://old.reddit.com/user/TorrentRover</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To begin with, I'm poor. I'm running a Lenovo PowerStation P520 with Xeon W-2145 and 1000w power supply with 2x PCIe x16 slots and 2x GPU (or EPS 12v) power drops.&lt;/p&gt; &lt;p&gt;Here are my current options:&lt;/p&gt; &lt;p&gt;2x RTX 3060 12GB cards (newish, lower spec, 24GB VRAM total)&lt;/p&gt; &lt;p&gt;or&lt;/p&gt; &lt;p&gt;2x Tesla K80 cards (old, low spec, 48GB VRAM total)&lt;/p&gt; &lt;p&gt;The tradeoffs are pretty obvious here. I have tested both. The 3060s gives me better inference speed but limit what models I can run due to lower VRAM. The K80s allow me to run larger models, but the performance is abismal.&lt;/p&gt; &lt;p&gt;Oh, and the power draw on the K80s is pretty insane. Resting with no model(s) loaded has 4x dies/chips (2x per card) hovering around 20-30w each (up to 120w) just idling. When a model is held in RAM, it can easily be 50-70w per chip/die. When running inference, it does hit the TDP of 149w each (nearly 600w total).&lt;/p&gt; &lt;p&gt;What would you choose? Why? Are there any similarly priced options I should be considering?&lt;/p&gt; &lt;p&gt;EDIT: I should have mentioned the software environment. I'm running Proxmox, and my ollama/Open Webui system is setup as a VM with Ubuntu 24.04.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TorrentRover"&gt; /u/TorrentRover &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ks5860/advice_on_the_aillm_gpu_triangle_the_tradeoffs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ks5860/advice_on_the_aillm_gpu_triangle_the_tradeoffs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ks5860/advice_on_the_aillm_gpu_triangle_the_tradeoffs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-21T18:28:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqz8gn</id>
    <title>I trapped LLama3.2B into an art installation and made it question its own existence endlessly</title>
    <updated>2025-05-20T07:48:24+00:00</updated>
    <author>
      <name>/u/Dull-Pressure9628</name>
      <uri>https://old.reddit.com/user/Dull-Pressure9628</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kqz8gn/i_trapped_llama32b_into_an_art_installation_and/"&gt; &lt;img alt="I trapped LLama3.2B into an art installation and made it question its own existence endlessly" src="https://preview.redd.it/gnnlh2fh6w1f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbc376b11d27165a314895138fd608df2741cb0e" title="I trapped LLama3.2B into an art installation and made it question its own existence endlessly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dull-Pressure9628"&gt; /u/Dull-Pressure9628 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gnnlh2fh6w1f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqz8gn/i_trapped_llama32b_into_an_art_installation_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kqz8gn/i_trapped_llama32b_into_an_art_installation_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-20T07:48:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1krkhr7</id>
    <title>Parking Analysis with Object Detection and Ollama models for Report Generation</title>
    <updated>2025-05-21T00:19:22+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1krkhr7/parking_analysis_with_object_detection_and_ollama/"&gt; &lt;img alt="Parking Analysis with Object Detection and Ollama models for Report Generation" src="https://external-preview.redd.it/N2J0OW5oMjkzMTJmMeMfnSo893myclMRvg1dOF4kmROzcG9sBbtv4hMJoM_m.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2887d668e49b0e0dd586dbd35f4bab59f2c79dd4" title="Parking Analysis with Object Detection and Ollama models for Report Generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Reddit!&lt;/p&gt; &lt;p&gt;Been tinkering with a fun project combining computer vision and LLMs, and wanted to share the progress.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The gist:&lt;/strong&gt;&lt;br /&gt; It uses a YOLO model (via Roboflow) to do real-time object detection on a video feed of a parking lot, figuring out which spots are taken and which are free. You can see the little red/green boxes doing their thing in the video.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;But here's the (IMO) coolest part:&lt;/strong&gt; The system then takes that occupancy data and feeds it to an open-source LLM (running locally with Ollama, tried models like Phi-3 for this). The LLM then generates a surprisingly detailed &amp;quot;Parking Lot Analysis Report&amp;quot; in Markdown.&lt;/p&gt; &lt;p&gt;This report isn't just &amp;quot;X spots free.&amp;quot; It calculates occupancy percentages, assesses current demand (e.g., &amp;quot;moderately utilized&amp;quot;), flags potential risks (like overcrowding if it gets too full), and even suggests actionable improvements like dynamic pricing strategies or better signage.&lt;/p&gt; &lt;p&gt;It's all automated ‚Äì from seeing the car park to getting a mini-management consultant report.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech Stack Snippets:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CV:&lt;/strong&gt; YOLO model from Roboflow for spot detection.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM:&lt;/strong&gt; Ollama for local LLM inference (e.g., Phi-3).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output:&lt;/strong&gt; Markdown reports.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The video shows it in action, including the report being generated.&lt;/p&gt; &lt;p&gt;Github Code: &lt;a href="https://github.com/Pavankunchala/LLM-Learn-PK/tree/main/ollama/parking_analysis"&gt;https://github.com/Pavankunchala/LLM-Learn-PK/tree/main/ollama/parking_analysis&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also if in this code you have to draw the polygons manually I built a separate app for it you can check that code here: &lt;a href="https://github.com/Pavankunchala/LLM-Learn-PK/tree/main/polygon-zone-app"&gt;https://github.com/Pavankunchala/LLM-Learn-PK/tree/main/polygon-zone-app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Self-promo note: If you find the code useful, a star on GitHub would be awesome!)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I'm thinking next:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time alerts for lot managers.&lt;/li&gt; &lt;li&gt;Predictive analysis for peak hours.&lt;/li&gt; &lt;li&gt;Maybe a simple web dashboard.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; On a related note, I'm actively looking for new opportunities in Computer Vision and LLM engineering. If your team is hiring or you know of any openings, I'd be grateful if you'd reach out!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Email:&lt;/strong&gt; [&lt;a href="mailto:pavankunchalaofficial@gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;](mailto:&lt;a href="mailto:pavankunchalaofficial@gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My other projects on GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Resume:&lt;/strong&gt; &lt;a href="https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view"&gt;https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i9tkek29312f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1krkhr7/parking_analysis_with_object_detection_and_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1krkhr7/parking_analysis_with_object_detection_and_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-21T00:19:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kru7b4</id>
    <title>12-&gt;16GB VRAM worth the upgrade?</title>
    <updated>2025-05-21T10:11:14+00:00</updated>
    <author>
      <name>/u/rddz48</name>
      <uri>https://old.reddit.com/user/rddz48</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is an upgrade to f.i. RTX2000ADA with 16GB VRAM from RTX4070 with 12GB worth the money?&lt;/p&gt; &lt;p&gt;Just asking because from models available for download only a few more seem to fit in the extra 4GB, a couple of 24b models to be specific. &lt;/p&gt; &lt;p&gt;If a model is only a bit bigger than available VRAM Ollama will fall back to CPU/RAM from CUDA/VRAM I think...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rddz48"&gt; /u/rddz48 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kru7b4/1216gb_vram_worth_the_upgrade/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kru7b4/1216gb_vram_worth_the_upgrade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kru7b4/1216gb_vram_worth_the_upgrade/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-21T10:11:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1krywit</id>
    <title>Did llama3.2-vision:11b go blind?</title>
    <updated>2025-05-21T14:14:57+00:00</updated>
    <author>
      <name>/u/Comfortable_Ad_8117</name>
      <uri>https://old.reddit.com/user/Comfortable_Ad_8117</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1krywit/did_llama32vision11b_go_blind/"&gt; &lt;img alt="Did llama3.2-vision:11b go blind?" src="https://preview.redd.it/fqme1xjy752f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb2b9b1187f5501e79b343474c72d2f55771cd26" title="Did llama3.2-vision:11b go blind?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable_Ad_8117"&gt; /u/Comfortable_Ad_8117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fqme1xjy752f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1krywit/did_llama32vision11b_go_blind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1krywit/did_llama32vision11b_go_blind/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-21T14:14:57+00:00</published>
  </entry>
</feed>
