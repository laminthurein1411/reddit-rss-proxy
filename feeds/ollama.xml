<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-01T23:23:37+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1izqfxm</id>
    <title>An AI agent, using Ollama and mistral, in 16 lines of code</title>
    <updated>2025-02-27T21:04:07+00:00</updated>
    <author>
      <name>/u/Excellent-Suit2150</name>
      <uri>https://old.reddit.com/user/Excellent-Suit2150</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1izqfxm/an_ai_agent_using_ollama_and_mistral_in_16_lines/"&gt; &lt;img alt="An AI agent, using Ollama and mistral, in 16 lines of code" src="https://preview.redd.it/hp46r94jxqle1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c648dc6adec2ba2f9480cafdd512db628a35caad" title="An AI agent, using Ollama and mistral, in 16 lines of code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent-Suit2150"&gt; /u/Excellent-Suit2150 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hp46r94jxqle1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izqfxm/an_ai_agent_using_ollama_and_mistral_in_16_lines/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izqfxm/an_ai_agent_using_ollama_and_mistral_in_16_lines/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T21:04:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1izlgdn</id>
    <title>Leveraging Ollama to maximise home/work/life quality</title>
    <updated>2025-02-27T17:35:57+00:00</updated>
    <author>
      <name>/u/StrayaSpiders</name>
      <uri>https://old.reddit.com/user/StrayaSpiders</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry in advance for the long thread - I love this thing! Huge props to the Ollama community, open-webui, and this subreddit! I wouldn't have got this far without you!&lt;/p&gt; &lt;p&gt;I got an Nvidia Jetsgon AGX Orin (64gb) from work - I don't work in AI and want to use it to run LLMs that will make my life easier. I really like the concept of &amp;quot;offline&amp;quot; AI that's private and I can feed more context than I would be comfortable giving to a tech company (maybe my tinfoil hat is too tight).&lt;/p&gt; &lt;p&gt;I added a 1tb NVMe and flashed the Jetson - it's now running Ubuntu 22.04. I've so far managed to get Ollama with open-webui running. I've tried to get Stable diffusion running, but can't get it to see the GPU yet.&lt;/p&gt; &lt;p&gt;In terms of LLMs. PHI4 &amp;amp; Mistral Nemo seem to give the most useful content and not take forever to reply.&lt;/p&gt; &lt;p&gt;This thread is a huge huge &amp;quot;thank you&amp;quot; as I've used lots of comments here to help me get all of this going, but also an ask for recommended next steps! I want to go down the local/offline wormhole more and really create a system that makes my life easier maybe home automation? I work in statistics and there's a few things I'd like to achieve;&lt;/p&gt; &lt;p&gt;- IDE support for coding&lt;br /&gt; - Financial data parsing (really great if it can read financial reports and distill so I can get info quicker) [web page/pdf/doc]&lt;br /&gt; - Generic PDF/DOC reading (generic distilling information - this would save me 100s of hours in deciding if I should bother reading something further)&lt;br /&gt; - Is there a way I can make LLMs &amp;quot;remember&amp;quot; things? I found the &amp;quot;personalisation&amp;quot; area in Open webui, but can I solve this more programmatically?&lt;/p&gt; &lt;p&gt;Any other recommendations for making my day-to-day life easier (yes, I'll spend 50 hours tinkering to save 10 minutes).&lt;/p&gt; &lt;p&gt;Side note: was putting Ubuntu 22 on the Jetson a mistake? It was a pain to get to the point ollama would use GPU (drivers). Maybe I should revert to NVidia's image?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StrayaSpiders"&gt; /u/StrayaSpiders &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izlgdn/leveraging_ollama_to_maximise_homeworklife_quality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izlgdn/leveraging_ollama_to_maximise_homeworklife_quality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izlgdn/leveraging_ollama_to_maximise_homeworklife_quality/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T17:35:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1izqb0c</id>
    <title>How can I make Ollama serve a preloaded model so I can call it directly like an API?</title>
    <updated>2025-02-27T20:58:31+00:00</updated>
    <author>
      <name>/u/CellObvious3943</name>
      <uri>https://old.reddit.com/user/CellObvious3943</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Right now, when I make a request, it seems to load the model first, which slows down the response time. Is there a way to keep the model loaded and ready for faster responses?&lt;/p&gt; &lt;p&gt;this example takes: 3.62 seconds&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import requests import json url = &amp;quot;http://localhost:11434/api/generate&amp;quot; data = { &amp;quot;model&amp;quot;: &amp;quot;llama3.2&amp;quot;, &amp;quot;prompt&amp;quot;: &amp;quot;tell me a short story and make it funny.&amp;quot;, } &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CellObvious3943"&gt; /u/CellObvious3943 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izqb0c/how_can_i_make_ollama_serve_a_preloaded_model_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izqb0c/how_can_i_make_ollama_serve_a_preloaded_model_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izqb0c/how_can_i_make_ollama_serve_a_preloaded_model_so/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T20:58:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1izh66i</id>
    <title>which AIs are you using?</title>
    <updated>2025-02-27T14:35:37+00:00</updated>
    <author>
      <name>/u/fantasy-owl</name>
      <uri>https://old.reddit.com/user/fantasy-owl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Want to try a local AI but not sure which one. I know that an AI can be good for a task but not that good for other tasks, so which AIs are you using and how is your experience with them? And Which AI is your favorite for a specif task? &lt;/p&gt; &lt;p&gt;My PC specs:&lt;br /&gt; GPU - NVIDIA 12VRAM&lt;br /&gt; CPU - AMD Ryzen 7&lt;br /&gt; RAM - 64GB&lt;/p&gt; &lt;p&gt;I‚Äôd really appreciate any advice or suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fantasy-owl"&gt; /u/fantasy-owl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izh66i/which_ais_are_you_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izh66i/which_ais_are_you_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izh66i/which_ais_are_you_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T14:35:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1izkh7c</id>
    <title>[Release] ScribePal - An Open Source Browser Extension for Private AI Chat Using Your Local Ollama Models</title>
    <updated>2025-02-27T16:56:18+00:00</updated>
    <author>
      <name>/u/Code-Forge-Temple</name>
      <uri>https://old.reddit.com/user/Code-Forge-Temple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;ScribePal - A Privacy-Focused Browser Extension for Ollama&lt;/h1&gt; &lt;p&gt;ScribePal is an Open Source intelligent browser extension that leverages AI to empower your web experience by providing contextual insights, efficient content summarization, and seamless interaction while you browse.&lt;/p&gt; &lt;h2&gt;Privacy &amp;amp; Compatibility&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Works with local Ollama models - all AI processing stays within your network&lt;/li&gt; &lt;li&gt;Compatible with Chrome, Firefox, Vivaldi, Opera, Edge, Brave, etc.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Key Features&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI-powered assistance:&lt;/strong&gt; Uses your local Ollama models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;100% Private:&lt;/strong&gt; All data stays within your LAN&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Theming:&lt;/strong&gt; Supports light and dark themes&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat Interface:&lt;/strong&gt; Draggable chat box for easy interaction&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Management:&lt;/strong&gt; Select, refresh, download, and delete models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Capture Tool:&lt;/strong&gt; Highlight and capture webpage content&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt Customization:&lt;/strong&gt; Customize how the AI responds&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;&lt;em&gt;Note: Requires a running Ollama instance on your local machine or LAN&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I have provided the full Ollama intructions in &lt;a href="https://github.com/code-forge-temple/scribe-pal?tab=readme-ov-file#prerequisites"&gt;prerequisites&lt;/a&gt; section of the README repo.&lt;/p&gt; &lt;h2&gt;Installation&lt;/h2&gt; &lt;p&gt;Please check the &lt;a href="https://github.com/code-forge-temple/scribe-pal?tab=readme-ov-file#installing"&gt;installing&lt;/a&gt; section of the README repo.&lt;/p&gt; &lt;h2&gt;How to Use&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Open the Extension:&lt;/strong&gt; Click the extension icon in your toolbar&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Configure:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Set your Ollama Server URL&lt;/li&gt; &lt;li&gt;Choose your preferred theme&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat Interface:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Click &amp;quot;Show ScribePal chat&amp;quot;&lt;/li&gt; &lt;li&gt;Drag the chat box anywhere on the page&lt;/li&gt; &lt;li&gt;Capture webpage content with &lt;code&gt;@captured&lt;/code&gt; tag&lt;/li&gt; &lt;li&gt;Customize prompts for better responses&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interact:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Type queries and get markdown-formatted responses&lt;/li&gt; &lt;li&gt;Manage your Ollama models directly from the interface&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Quick Demo&lt;/h2&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=IR7Jufc0zxo"&gt;Watch the tutorial video&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;GitHub Repository: &lt;a href="https://github.com/code-forge-temple/scribe-pal"&gt;https://github.com/code-forge-temple/scribe-pal&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Contributing&lt;/h2&gt; &lt;p&gt;Found a bug or have a suggestion? I'd love to hear from you! Please open an issue on the &lt;a href="https://github.com/code-forge-temple/scribe-pal/issues"&gt;GitHub repository&lt;/a&gt; with: - A clear description of the issue/suggestion - Your browser and version - Steps to reproduce (for bugs) - Your Ollama version and setup&lt;/p&gt; &lt;p&gt;Your feedback helps make ScribePal better for everyone!&lt;/p&gt; &lt;p&gt;&lt;em&gt;Note: When opening issues, please check if a similar issue already exists to avoid duplicates.&lt;/em&gt;&lt;/p&gt; &lt;h2&gt;License&lt;/h2&gt; &lt;p&gt;This project is licensed under the GNU General Public License v3.0.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Code-Forge-Temple"&gt; /u/Code-Forge-Temple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izkh7c/release_scribepal_an_open_source_browser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izkh7c/release_scribepal_an_open_source_browser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izkh7c/release_scribepal_an_open_source_browser/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T16:56:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1izm8fh</id>
    <title>Best llm for coding!</title>
    <updated>2025-02-27T18:07:50+00:00</updated>
    <author>
      <name>/u/Potential_Chip4708</name>
      <uri>https://old.reddit.com/user/Potential_Chip4708</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am angular and nodejs developer. I am using copilot with claude sonnet 3.5 which is free. Additionally i have some experience on Mistral Codestral. (Cline). UI standpoint codestral is not good. But if you specify a bug or feature with files relative path, it gives perfect solution. Apart from that am missing any good llm? Any suggestions for a local llm. That can be better than this setup? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Potential_Chip4708"&gt; /u/Potential_Chip4708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izm8fh/best_llm_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izm8fh/best_llm_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izm8fh/best_llm_for_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T18:07:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j03jbm</id>
    <title>phi4-mini model can't run properly and spitting gibberish</title>
    <updated>2025-02-28T09:03:42+00:00</updated>
    <author>
      <name>/u/Maleficent_Repair359</name>
      <uri>https://old.reddit.com/user/Maleficent_Repair359</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j03jbm/phi4mini_model_cant_run_properly_and_spitting/"&gt; &lt;img alt="phi4-mini model can't run properly and spitting gibberish" src="https://b.thumbs.redditmedia.com/YADAreVtRAdIjZ63qeru9qaZ_j2rzfHyAaVCkYJD80c.jpg" title="phi4-mini model can't run properly and spitting gibberish" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/f5povrg2iule1.png?width=1881&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8619bdaccdbfa07b2da0a77c8780cfe68d1b7a53"&gt;log of the phi4 mini model output&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent_Repair359"&gt; /u/Maleficent_Repair359 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j03jbm/phi4mini_model_cant_run_properly_and_spitting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j03jbm/phi4mini_model_cant_run_properly_and_spitting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j03jbm/phi4mini_model_cant_run_properly_and_spitting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-28T09:03:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0dz37</id>
    <title>Definir modelo para exclusivamente utilizar Portugu√™s do Brasil (PT-BR)</title>
    <updated>2025-02-28T18:00:59+00:00</updated>
    <author>
      <name>/u/Antique-Deal4769</name>
      <uri>https://old.reddit.com/user/Antique-Deal4769</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;H√° alguma forma de alterar a linguagem para todos os prompts novos serem nativamente em portugu√™s do brasil? &lt;/p&gt; &lt;p&gt;Tentei de todas as formas tentar setar para que jamais houvesse mistura de l√≠nguas nas intera√ß√µes, mas isso n√£o persiste. Na Open WebUI tamb√©m defini o idioma para portugu√™s, mas claramente isso √© sobre o Docker. &lt;/p&gt; &lt;p&gt;J√° procurei em todas op√ß√µes, mas n√£o encontro. H√° algum lugar espec√≠fico para eu definir o isso direto no modelo? Estou usando o Ollama com o deepseek-r1:70b&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Antique-Deal4769"&gt; /u/Antique-Deal4769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0dz37/definir_modelo_para_exclusivamente_utilizar/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0dz37/definir_modelo_para_exclusivamente_utilizar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j0dz37/definir_modelo_para_exclusivamente_utilizar/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-28T18:00:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j041n8</id>
    <title>llama3.3:70b-instruct-q4_K_M with Ollama is running mainly on the CPU with RTX 3090</title>
    <updated>2025-02-28T09:42:31+00:00</updated>
    <author>
      <name>/u/No_Poet3183</name>
      <uri>https://old.reddit.com/user/No_Poet3183</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j041n8/llama3370binstructq4_k_m_with_ollama_is_running/"&gt; &lt;img alt="llama3.3:70b-instruct-q4_K_M with Ollama is running mainly on the CPU with RTX 3090" src="https://a.thumbs.redditmedia.com/FfHL09v2wyIKKhR1pQUvob9Xau_gcOVzPXpyZMxsmF4.jpg" title="llama3.3:70b-instruct-q4_K_M with Ollama is running mainly on the CPU with RTX 3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPU usage is very low while CPU is spinning on max. I have 24GB VRAM.&lt;/p&gt; &lt;p&gt;Shouldn't q4_K_M quantized llama3.3 should fit into this VRAM?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bp74l5gpoule1.png?width=1897&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34d3a21c101bef1600851f9a0df157bd8033d059"&gt;https://preview.redd.it/bp74l5gpoule1.png?width=1897&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34d3a21c101bef1600851f9a0df157bd8033d059&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Poet3183"&gt; /u/No_Poet3183 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j041n8/llama3370binstructq4_k_m_with_ollama_is_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j041n8/llama3370binstructq4_k_m_with_ollama_is_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j041n8/llama3370binstructq4_k_m_with_ollama_is_running/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-28T09:42:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1izmfn7</id>
    <title>Building a robot that can see, hear, talk, and dance. Powered by on-device AI with the Jetson Orin NX, Moondream &amp; Whisper (open source)</title>
    <updated>2025-02-27T18:16:23+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1izmfn7/building_a_robot_that_can_see_hear_talk_and_dance/"&gt; &lt;img alt="Building a robot that can see, hear, talk, and dance. Powered by on-device AI with the Jetson Orin NX, Moondream &amp;amp; Whisper (open source)" src="https://external-preview.redd.it/dWlhd3A2OHYzcWxlMe__omCO_n66cYU7Fe7wXFz05iYznG-U5sQ5kSodSfXF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cff4f9933b92052bd2cd458e4172868ca7c62431" title="Building a robot that can see, hear, talk, and dance. Powered by on-device AI with the Jetson Orin NX, Moondream &amp;amp; Whisper (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9kwfq88v3qle1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izmfn7/building_a_robot_that_can_see_hear_talk_and_dance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izmfn7/building_a_robot_that_can_see_hear_talk_and_dance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T18:16:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j041o7</id>
    <title>Granite 3.2 and the meta-strawberry: dynamic inference scaling seems to work? [Details in the comments]</title>
    <updated>2025-02-28T09:42:34+00:00</updated>
    <author>
      <name>/u/mmmgggmmm</name>
      <uri>https://old.reddit.com/user/mmmgggmmm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j041o7/granite_32_and_the_metastrawberry_dynamic/"&gt; &lt;img alt="Granite 3.2 and the meta-strawberry: dynamic inference scaling seems to work? [Details in the comments]" src="https://b.thumbs.redditmedia.com/_STeH13T_O8Wx_VUxnPaJJOtZv77nihKZhL2cX4295Y.jpg" title="Granite 3.2 and the meta-strawberry: dynamic inference scaling seems to work? [Details in the comments]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mmmgggmmm"&gt; /u/mmmgggmmm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j041o7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j041o7/granite_32_and_the_metastrawberry_dynamic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j041o7/granite_32_and_the_metastrawberry_dynamic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-28T09:42:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1j02fuq</id>
    <title>beast arrived</title>
    <updated>2025-02-28T07:41:31+00:00</updated>
    <author>
      <name>/u/_ggsa</name>
      <uri>https://old.reddit.com/user/_ggsa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j02fuq/beast_arrived/"&gt; &lt;img alt="beast arrived" src="https://b.thumbs.redditmedia.com/xOVgu2nqy6sy_S39KOfageCUHpUyWG05i4_kEy_CAcw.jpg" title="beast arrived" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/bejtfc6s3ule1.jpg?width=1636&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f1aa15fb942f8bc67acc150686cfc06d8ec5bd23"&gt;https://preview.redd.it/bejtfc6s3ule1.jpg?width=1636&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f1aa15fb942f8bc67acc150686cfc06d8ec5bd23&lt;/a&gt;&lt;/p&gt; &lt;p&gt;got his monster for $3k, can't wait to see what i can do with it! spec: m1 ultra, 20/64, 128gb&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_ggsa"&gt; /u/_ggsa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j02fuq/beast_arrived/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j02fuq/beast_arrived/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j02fuq/beast_arrived/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-28T07:41:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1j08s9u</id>
    <title>I built an open-source chat playground UI for Ollama</title>
    <updated>2025-02-28T14:22:38+00:00</updated>
    <author>
      <name>/u/CountlessFlies</name>
      <uri>https://old.reddit.com/user/CountlessFlies</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;! &lt;/p&gt; &lt;p&gt;I've been experimenting with local models to generate data for fine-tuning, and so I built a custom UI for creating conversations with local models served via Ollama. Almost a clone of OpenAI's playground, but for local models.&lt;/p&gt; &lt;p&gt;Thought others might find it useful, so I open-sourced it: &lt;a href="https://github.com/prvnsmpth/open-playground"&gt;https://github.com/prvnsmpth/open-playground&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The playground gives you more control over the conversation - you can add, remove, edit messages in the chat at any point, switch between models mid-conversation, etc.&lt;/p&gt; &lt;p&gt;My ultimate goal with this project is to build a tool that can simplify the process of building datasets for fine-tuning local models. Eventually I'd like to be able to trigger the fine-tuning job via this tool too.&lt;/p&gt; &lt;p&gt;If you're interested in fine-tuning LLMs for specific tasks, please let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CountlessFlies"&gt; /u/CountlessFlies &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j08s9u/i_built_an_opensource_chat_playground_ui_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j08s9u/i_built_an_opensource_chat_playground_ui_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j08s9u/i_built_an_opensource_chat_playground_ui_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-28T14:22:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0m7mp</id>
    <title>Can Ollama do post requests for external ai models?</title>
    <updated>2025-03-01T00:01:31+00:00</updated>
    <author>
      <name>/u/Good-Path-1204</name>
      <uri>https://old.reddit.com/user/Good-Path-1204</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says I have a external server with a few ai models on runpod, I basically want to know if there is a way to make a post request to them from ollama (or even load the models for ollama). this is mainly for me to use it for flowiseAI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Good-Path-1204"&gt; /u/Good-Path-1204 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0m7mp/can_ollama_do_post_requests_for_external_ai_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0m7mp/can_ollama_do_post_requests_for_external_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j0m7mp/can_ollama_do_post_requests_for_external_ai_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-01T00:01:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0lxrz</id>
    <title>Any small model without restriction?</title>
    <updated>2025-02-28T23:48:42+00:00</updated>
    <author>
      <name>/u/ivkemilioner</name>
      <uri>https://old.reddit.com/user/ivkemilioner</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivkemilioner"&gt; /u/ivkemilioner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0lxrz/any_small_model_without_restriction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0lxrz/any_small_model_without_restriction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j0lxrz/any_small_model_without_restriction/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-28T23:48:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j10u99</id>
    <title>The ai is funny.</title>
    <updated>2025-03-01T14:37:49+00:00</updated>
    <author>
      <name>/u/AnaverageuserX</name>
      <uri>https://old.reddit.com/user/AnaverageuserX</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j10u99/the_ai_is_funny/"&gt; &lt;img alt="The ai is funny." src="https://b.thumbs.redditmedia.com/Gm7ebq-kNgUi_EslWtvP_iunwlDxL7b2s6aPSm-w6zs.jpg" title="The ai is funny." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All I did was ask for a description on dogs and it began lying to me. It obviously can't shut down&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8ubksugla3me1.png?width=917&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ce88225ede597045d3ceffccec777a1f04aadb5"&gt;https://preview.redd.it/8ubksugla3me1.png?width=917&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ce88225ede597045d3ceffccec777a1f04aadb5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kv59p54ma3me1.png?width=1136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f330d85fbfc293f3db6d73760cf1c9045bf9231e"&gt;https://preview.redd.it/kv59p54ma3me1.png?width=1136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f330d85fbfc293f3db6d73760cf1c9045bf9231e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I raged a bit...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnaverageuserX"&gt; /u/AnaverageuserX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j10u99/the_ai_is_funny/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j10u99/the_ai_is_funny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j10u99/the_ai_is_funny/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-01T14:37:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0suks</id>
    <title>3D printing prosthetics</title>
    <updated>2025-03-01T06:05:20+00:00</updated>
    <author>
      <name>/u/Choice_Complaint9171</name>
      <uri>https://old.reddit.com/user/Choice_Complaint9171</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have been searching lately about prosthetics due to a family member that has to undergo surgery on the foot &lt;em&gt;diabetic amputation&lt;/em&gt; it burns my heart to imagine the emotions my loved one is going through I want so much to try and soften the hurt and possible depression from this outcome I‚Äôve lost sleep all week trying to think for lack of better words how can I somehow better the resulting reality of what my loved one has to bear. &lt;/p&gt; &lt;p&gt;To get to the point I‚Äôm thinking about trying to have llama vision map out dimensions of the foot from a photo and take those dimensions to a cad editor like tinkercad then print a prototype on a Ender 3 this is an idea but I can only imagine that there‚Äôs other people that share somewhat of the same experience as me wanting to make a difference and I feel I‚Äôm just at a exhaustive pace at the moment &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Choice_Complaint9171"&gt; /u/Choice_Complaint9171 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0suks/3d_printing_prosthetics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0suks/3d_printing_prosthetics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j0suks/3d_printing_prosthetics/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-01T06:05:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0pgcd</id>
    <title>8xMi50 Server Faster than 8xMi60 Server -&gt; (37 - 41 t/s) - OpenThinker-32B-abliterated.Q8_0</title>
    <updated>2025-03-01T02:48:30+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/02qy1teqqzle1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0pgcd/8xmi50_server_faster_than_8xmi60_server_37_41_ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j0pgcd/8xmi50_server_faster_than_8xmi60_server_37_41_ts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-01T02:48:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0fd05</id>
    <title>RAG on documents</title>
    <updated>2025-02-28T18:59:29+00:00</updated>
    <author>
      <name>/u/Morphos91</name>
      <uri>https://old.reddit.com/user/Morphos91</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RAG on documents&lt;/p&gt; &lt;p&gt;Hi all&lt;/p&gt; &lt;p&gt;I started my first deepdive into AI models and RAG.&lt;/p&gt; &lt;p&gt;One of our customers has technical manuals about cars (how to fix what error codes, replacement parts you name it).&lt;br /&gt; His question was if we could implement an AI chat so he can 'chat' with the documents.&lt;/p&gt; &lt;p&gt;I know I have to vector the text on the documents and run a similarity search when they prompt. After the similarity search, I need to run the text (of that vector) through An AI to create a response. &lt;/p&gt; &lt;p&gt;I'm just wondering if this will actually work. He gave me an example prompt: &amp;quot;What does errorcode e29 mean on a XXX brand with lot number e19b?&amp;quot; &lt;/p&gt; &lt;p&gt;He expects a response which says 'On page 119 of document X errorcode e29 means... '&lt;/p&gt; &lt;p&gt;I have yet to decide how to chunk the documents, but If I would chunk they by paragraph for example I guess my vector would find the errorcode but the vector will have no knowledge about the brand of car or the lot number. That's information which is in an other vector (the one of page 1 for example). &lt;/p&gt; &lt;p&gt;These documents can be hundreds of pages long. Am I missing something about these vector searches? or do I need to send the complete document content to the assistant after the similarity search? That would be alot of input tokens.&lt;/p&gt; &lt;p&gt;Help!&lt;br /&gt; And thanks in advance :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Morphos91"&gt; /u/Morphos91 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0fd05/rag_on_documents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0fd05/rag_on_documents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j0fd05/rag_on_documents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-28T18:59:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0zz9v</id>
    <title>"Ollama serve" get's stuck</title>
    <updated>2025-03-01T13:55:17+00:00</updated>
    <author>
      <name>/u/Citizen-of-Denmark</name>
      <uri>https://old.reddit.com/user/Citizen-of-Denmark</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run Ollama in Linux Mint 22.1. When I run &amp;quot;Ollama serve&amp;quot;, I get the below respose and I'm not returned to the command prompt. What's happening?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama serve 2025/03/01 14:50:21 routes.go:1205: INFO server config env=&amp;quot;map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/jakob/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]&amp;quot; time=2025-03-01T14:50:21.632+01:00 level=INFO source=images.go:432 msg=&amp;quot;total blobs: 0&amp;quot; time=2025-03-01T14:50:21.632+01:00 level=INFO source=images.go:439 msg=&amp;quot;total unused blobs removed: 0&amp;quot; time=2025-03-01T14:50:21.632+01:00 level=INFO source=routes.go:1256 msg=&amp;quot;Listening on 127.0.0.1:11434 (version 0.5.12)&amp;quot; time=2025-03-01T14:50:21.632+01:00 level=INFO source=gpu.go:217 msg=&amp;quot;looking for compatible GPUs&amp;quot; time=2025-03-01T14:50:21.689+01:00 level=INFO source=types.go:130 msg=&amp;quot;inference compute&amp;quot; id=GPU-27e1fc7b-f051-1aaf-4545-7af2f6f47ea0 library=cuda variant=v12 compute=8.9 driver=12.4 name=&amp;quot;NVIDIA GeForce RTX 4080 SUPER&amp;quot; total=&amp;quot;15.7 GiB&amp;quot; available=&amp;quot;9.0 GiB&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Citizen-of-Denmark"&gt; /u/Citizen-of-Denmark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0zz9v/ollama_serve_gets_stuck/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0zz9v/ollama_serve_gets_stuck/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j0zz9v/ollama_serve_gets_stuck/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-01T13:55:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0f5zj</id>
    <title>Introducing LLMule: A P2P network for Ollama users to share and discover models</title>
    <updated>2025-02-28T18:51:01+00:00</updated>
    <author>
      <name>/u/micupa</name>
      <uri>https://old.reddit.com/user/micupa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; community!&lt;/p&gt; &lt;p&gt;I'm excited to share a project I've been working on that I think many of you will find useful. It's called &lt;strong&gt;LLMule&lt;/strong&gt; - an open-source desktop client that not only works with your local Ollama setup but also lets you connect to a P2P network of shared models.&lt;/p&gt; &lt;h1&gt;What is LLMule?&lt;/h1&gt; &lt;p&gt;LLMule is inspired by the old-school P2P networks like eMule and Napster, but for AI models. I built it to democratize AI access and create a community-powered alternative to corporate AI services.&lt;/p&gt; &lt;h1&gt;Key features:&lt;/h1&gt; &lt;p&gt;üîí &lt;strong&gt;True Privacy&lt;/strong&gt;: Your conversations stay on your device. Network conversations are anonymous, and we never store prompts or responses.&lt;/p&gt; &lt;p&gt;üíª &lt;strong&gt;Works with Ollama:&lt;/strong&gt; Automatically detects and integrate with Ollama models (also compatible with LM Studio, vLLM, and EXO)&lt;/p&gt; &lt;p&gt;üåê &lt;strong&gt;P2P Model Sharing:&lt;/strong&gt; Share your Ollama models with others and discover models shared by the community&lt;/p&gt; &lt;p&gt;üîß &lt;strong&gt;Open Source&lt;/strong&gt; - MIT licensed, fully transparent code&lt;/p&gt; &lt;h1&gt;Why I built this?&lt;/h1&gt; &lt;p&gt;I believe AI should be accessible to everyone, not just controlled by big tech. By creating a decentralized network where we can all share our models and compute resources, we can build something that's owned by the community.&lt;/p&gt; &lt;p&gt;Get involved!&lt;/p&gt; &lt;p&gt;- GitHub: [LLMule-desktop-client](&lt;a href="https://github.com/cm64-studio/LLMule-desktop-client"&gt;https://github.com/cm64-studio/LLMule-desktop-client&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;- Website: [llmule.xyz](&lt;a href="https://llmule.xyz"&gt;https://llmule.xyz&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;- Download for: Windows, macOS, and Linux&lt;/p&gt; &lt;p&gt;I'd love to hear your thoughts, feedback, and ideas. This is an early version, so there's a lot of room for community input to shape where it goes.&lt;/p&gt; &lt;p&gt;Let's decentralize AI together!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/micupa"&gt; /u/micupa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0f5zj/introducing_llmule_a_p2p_network_for_ollama_users/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0f5zj/introducing_llmule_a_p2p_network_for_ollama_users/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j0f5zj/introducing_llmule_a_p2p_network_for_ollama_users/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-28T18:51:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0pls3</id>
    <title>When the context window is exceeded, what happens to the data fed into the model?</title>
    <updated>2025-03-01T02:56:33+00:00</updated>
    <author>
      <name>/u/DelosBoard2052</name>
      <uri>https://old.reddit.com/user/DelosBoard2052</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running llama3.2:3b and I developed a conversational memory for it that pre-pends the conversation history to the current query. Llama has a context window of 2048 tokens. When the memory plus n√®w query exceeds 2048 tokens, does it just lose the oldest part of the memory dump, or does any other odd behavior happen? I also have a custom modelfile - does that data survive any context window overflow, or would that be the first thing to go? Asking because I suspect something I observe happening may be related to a context window overflow.... Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DelosBoard2052"&gt; /u/DelosBoard2052 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0pls3/when_the_context_window_is_exceeded_what_happens/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0pls3/when_the_context_window_is_exceeded_what_happens/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j0pls3/when_the_context_window_is_exceeded_what_happens/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-01T02:56:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0cwah</id>
    <title>Mac Studio Server Guide: Run Ollama with optimized memory usage (11GB ‚Üí 3GB)</title>
    <updated>2025-02-28T17:16:01+00:00</updated>
    <author>
      <name>/u/_ggsa</name>
      <uri>https://old.reddit.com/user/_ggsa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama community! &lt;/p&gt; &lt;p&gt;I created a guide to run Mac Studio (or any Apple Silicon Mac) as a dedicated Ollama server. Here's what it does:&lt;/p&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Reduces system memory usage from 11GB to 3GB&lt;/li&gt; &lt;li&gt;Runs automatically on startup&lt;/li&gt; &lt;li&gt;Optimizes for headless operation (SSH access)&lt;/li&gt; &lt;li&gt;Allows more GPU memory allocation&lt;/li&gt; &lt;li&gt;Includes proper logging setup&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Perfect for you if:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You want to use Mac Studio/Mini as a dedicated LLM server&lt;/li&gt; &lt;li&gt;You need to run multiple large models&lt;/li&gt; &lt;li&gt;You want to access models remotely&lt;/li&gt; &lt;li&gt;You care about resource optimization&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Setup includes scripts to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Disable unnecessary services&lt;/li&gt; &lt;li&gt;Configure automatic startup&lt;/li&gt; &lt;li&gt;Set optimal Ollama parameters&lt;/li&gt; &lt;li&gt;Enable remote access&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/anurmatov/mac-studio-server"&gt;https://github.com/anurmatov/mac-studio-server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you're running Ollama on Mac, I'd love to hear about your setup and what tweaks you use! üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_ggsa"&gt; /u/_ggsa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0cwah/mac_studio_server_guide_run_ollama_with_optimized/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0cwah/mac_studio_server_guide_run_ollama_with_optimized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j0cwah/mac_studio_server_guide_run_ollama_with_optimized/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-28T17:16:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0by7r</id>
    <title>Tested local LLMs on a maxed out M4 Macbook Pro so you don't have to</title>
    <updated>2025-02-28T16:37:44+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently own a MacBook M1 Pro (32GB RAM, 16-core GPU) and now a maxed-out MacBook M4 Max (128GB RAM, 40-core GPU) and ran some inference speed tests. I kept the context size at the default 4096. Out of curiosity, I compared MLX-optimized models vs. GGUF. Here are my initial results!&lt;/p&gt; &lt;h4&gt;Ollama&lt;/h4&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;GGUF models&lt;/th&gt; &lt;th&gt;M4 Max (128 GB RAM, 40-core GPU)&lt;/th&gt; &lt;th&gt;M1 Pro (32GB RAM, 16-core GPU)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:7B (4bit)&lt;/td&gt; &lt;td&gt;72.50 tokens/s&lt;/td&gt; &lt;td&gt;26.85 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:14B (4bit)&lt;/td&gt; &lt;td&gt;38.23 tokens/s&lt;/td&gt; &lt;td&gt;14.66 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:32B (4bit)&lt;/td&gt; &lt;td&gt;19.35 tokens/s&lt;/td&gt; &lt;td&gt;6.95 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:72B (4bit)&lt;/td&gt; &lt;td&gt;8.76 tokens/s&lt;/td&gt; &lt;td&gt;Didn't Test&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h4&gt;LM Studio&lt;/h4&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;MLX models&lt;/th&gt; &lt;th&gt;M4 Max (128 GB RAM, 40-core GPU)&lt;/th&gt; &lt;th&gt;M1 Pro (32GB RAM, 16-core GPU)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-7B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;101.87 tokens/s&lt;/td&gt; &lt;td&gt;38.99 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-14B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;52.22 tokens/s&lt;/td&gt; &lt;td&gt;18.88 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-32B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;24.46 tokens/s&lt;/td&gt; &lt;td&gt;9.10 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-32B-Instruct (8bit)&lt;/td&gt; &lt;td&gt;13.75 tokens/s&lt;/td&gt; &lt;td&gt;Won‚Äôt Complete (Crashed)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-72B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;10.86 tokens/s&lt;/td&gt; &lt;td&gt;Didn't Test&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;GGUF models&lt;/th&gt; &lt;th&gt;M4 Max (128 GB RAM, 40-core GPU)&lt;/th&gt; &lt;th&gt;M1 Pro (32GB RAM, 16-core GPU)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-7B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;71.73 tokens/s&lt;/td&gt; &lt;td&gt;26.12 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-14B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;39.04 tokens/s&lt;/td&gt; &lt;td&gt;14.67 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-32B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;19.56 tokens/s&lt;/td&gt; &lt;td&gt;4.53 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-72B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;8.31 tokens/s&lt;/td&gt; &lt;td&gt;Didn't Test&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Some thoughts:&lt;/p&gt; &lt;p&gt;- I chose Qwen2.5 simply because its currently my favorite local model to work with. It seems to perform better than the distilled DeepSeek models (my opinion). But I'm open to testing other models if anyone has any suggestions.&lt;/p&gt; &lt;p&gt;- Even though there's a big performance difference between the two, I'm still not sure if its worth the even bigger price difference. I'm still debating whether to keep it and sell my M1 Pro or return it.&lt;/p&gt; &lt;p&gt;- I'm curious to know when MLX based models are released on Ollama, will they be faster than the ones on LM Studio? Based on these results, the base models on Ollama are slightly faster than the instruct models in LM Studio. I'm under the impression that instruct models are overall more performant than the base models.&lt;/p&gt; &lt;p&gt;Let me know your thoughts!&lt;/p&gt; &lt;p&gt;EDIT: Added test results for 72B and 7B variants&lt;/p&gt; &lt;p&gt;UPDATE: I decided to add a github repo so we can document various inference speeds from different devices. Feel free to contribute here: &lt;a href="https://github.com/itsmostafa/inference-speed-tests"&gt;https://github.com/itsmostafa/inference-speed-tests&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0by7r/tested_local_llms_on_a_maxed_out_m4_macbook_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0by7r/tested_local_llms_on_a_maxed_out_m4_macbook_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j0by7r/tested_local_llms_on_a_maxed_out_m4_macbook_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-28T16:37:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j14ah2</id>
    <title>Long context and multiple GPUs</title>
    <updated>2025-03-01T17:08:54+00:00</updated>
    <author>
      <name>/u/Daemonero</name>
      <uri>https://old.reddit.com/user/Daemonero</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm curious how context is split with multiple GPUs. Let's say I use codestral 22b and it fits entirely on one 16gb GPU. I then keep chatting and eventually the context overfills. Does it then split to the second GPU or would it overflow to system ram, leaving the second GPU unused? &lt;/p&gt; &lt;p&gt;If so, one way to combat this would be to use a higher quant so that it splits between GPUs from the start I suppose. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daemonero"&gt; /u/Daemonero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j14ah2/long_context_and_multiple_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j14ah2/long_context_and_multiple_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j14ah2/long_context_and_multiple_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-01T17:08:54+00:00</published>
  </entry>
</feed>
