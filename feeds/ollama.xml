<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-27T08:37:28+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1iao84u</id>
    <title>[Deepseek r1] REPLACETheseStringsWithCorrespondingValues()</title>
    <updated>2025-01-26T19:45:26+00:00</updated>
    <author>
      <name>/u/vsarunhah</name>
      <uri>https://old.reddit.com/user/vsarunhah</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I just tried running Deepseek r1 1.5b and was just messing around with it and got some weird output that I haven't seen before. Anyone what this is about/where this could've come from? Not sure if it's an ollama thing or an r1 thing.&lt;/p&gt; &lt;p&gt;``` ollama run deepseek-r1:1.5b&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;hello &amp;lt;think&amp;gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;Hello! How can I assist you today? üòä&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;What are you good at &amp;lt;think&amp;gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;Hello! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. For the best results, always do your own research before using AI tools.&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;What are you optimized for? `&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;Hi! I'm DeepSeek-R1, an AI assistant created to help with various tasks and provide information. I optimize my performance based on extensive training data and advanced algorithms to deliver accurate and helpful responses. For more details about our products or services, please visit our official website.&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;What is the most recent data you have been trained on `&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;REPLACETheseStringsWithCorrespondingValues()&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;What is the most recent data you have been trained on? &amp;lt;think&amp;gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;Hi! I'm DeepSeek-R1, an AI assistant created to help with various tasks and provide information. For the best results, always do your own research before using AI tools. If you have any questions or need assistance, feel free to ask!&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;What is the most recent data you have been trained on `&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;REPLACETheseStringsWithCorrespondingValues() ```&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Seems reproducible too, which is pretty weird.&lt;/p&gt; &lt;p&gt;ollama version is 0.5.7&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vsarunhah"&gt; /u/vsarunhah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iao84u/deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iao84u/deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iao84u/deepseek_r1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T19:45:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9yv89</id>
    <title>Introducing Awesome Open Source AI: A list for tracking great open source models</title>
    <updated>2025-01-25T22:28:29+00:00</updated>
    <author>
      <name>/u/SuccessIsHardWork</name>
      <uri>https://old.reddit.com/user/SuccessIsHardWork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i9yv89/introducing_awesome_open_source_ai_a_list_for/"&gt; &lt;img alt="Introducing Awesome Open Source AI: A list for tracking great open source models" src="https://external-preview.redd.it/GXmJ82vxdZ1DvaPMBQ6TD4TQLzEhsWVdtuXgso5_c_Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce8f5a1c97427023bef612109fcd4409305118f3" title="Introducing Awesome Open Source AI: A list for tracking great open source models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuccessIsHardWork"&gt; /u/SuccessIsHardWork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/suncloudsmoon/awesome-open-source-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9yv89/introducing_awesome_open_source_ai_a_list_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9yv89/introducing_awesome_open_source_ai_a_list_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T22:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iatne4</id>
    <title>Ollama in a conda venv</title>
    <updated>2025-01-26T23:23:24+00:00</updated>
    <author>
      <name>/u/Big_Scholar_3358</name>
      <uri>https://old.reddit.com/user/Big_Scholar_3358</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to install Ollama latest inside a conda virtual env so that I can experiment and keeping my system clean. Any way to achieve this? The packages available in conda-forge and pypi are not the latest. &lt;/p&gt; &lt;p&gt;TIA &amp;amp; cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big_Scholar_3358"&gt; /u/Big_Scholar_3358 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iatne4/ollama_in_a_conda_venv/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iatne4/ollama_in_a_conda_venv/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iatne4/ollama_in_a_conda_venv/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T23:23:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1iap6ot</id>
    <title>running Ollama on Intet Arc iGPU</title>
    <updated>2025-01-26T20:21:33+00:00</updated>
    <author>
      <name>/u/JV_info</name>
      <uri>https://old.reddit.com/user/JV_info</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a Windows 11 on a mini PC (Geekom GT series G1 Mega) and it has the Intel Arc iGPU, + NPU&lt;br /&gt; I also have a local AI chat and my setup is this (Ollama + Docker + Openwebui)&lt;br /&gt; now I want to use my Intel ARC iGPU but no matter what I do, Ollama ends up running on CPU.&lt;/p&gt; &lt;p&gt;I tried the instruction from Intel, the PDF you shared as well but the problem is that the Intel oneAPI Base Toolkit they use in the instructions is version 2024.1.0 but the version available to download now is 2025.1 and it is not working, I mean when I run the &amp;quot;Ollama serve&amp;quot; it is missing some DLLs that I have but in a higher version... and it's a dead end.&lt;/p&gt; &lt;p&gt;So, I wanted to see if there is a way:&lt;/p&gt; &lt;p&gt;1- downloading the 2024.1.0 version.&lt;/p&gt; &lt;p&gt;2- or any other way to use for my case, Ollama + Docker + Openwebui, so that the intel Arc iGPU or NPU will be used when I use Ollama in OpenwebUi.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JV_info"&gt; /u/JV_info &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iap6ot/running_ollama_on_intet_arc_igpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iap6ot/running_ollama_on_intet_arc_igpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iap6ot/running_ollama_on_intet_arc_igpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T20:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1iajtdg</id>
    <title>SQL DB to RAG</title>
    <updated>2025-01-26T17:03:23+00:00</updated>
    <author>
      <name>/u/PaymentDramatic6154</name>
      <uri>https://old.reddit.com/user/PaymentDramatic6154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any good tutorials on how to do this? Looking for some help, everything ive been seeing about RAG is with pdfs or documents. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaymentDramatic6154"&gt; /u/PaymentDramatic6154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iajtdg/sql_db_to_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iajtdg/sql_db_to_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iajtdg/sql_db_to_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T17:03:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib0onc</id>
    <title>jacksonville Florida Ollama AI Entrepeneurs</title>
    <updated>2025-01-27T05:19:56+00:00</updated>
    <author>
      <name>/u/Choice_Complaint9171</name>
      <uri>https://old.reddit.com/user/Choice_Complaint9171</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;who in jax is looking to collaborate on creative AI opportunities. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Choice_Complaint9171"&gt; /u/Choice_Complaint9171 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib0onc/jacksonville_florida_ollama_ai_entrepeneurs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib0onc/jacksonville_florida_ollama_ai_entrepeneurs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib0onc/jacksonville_florida_ollama_ai_entrepeneurs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T05:19:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaybfp</id>
    <title>Running Deepseek R1 on M3 Pro 18GB?</title>
    <updated>2025-01-27T03:14:24+00:00</updated>
    <author>
      <name>/u/creativegoof96</name>
      <uri>https://old.reddit.com/user/creativegoof96</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone knows what version of Deepseek should I install locally? Trying to check if 32b is enough.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/creativegoof96"&gt; /u/creativegoof96 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaybfp/running_deepseek_r1_on_m3_pro_18gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaybfp/running_deepseek_r1_on_m3_pro_18gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaybfp/running_deepseek_r1_on_m3_pro_18gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T03:14:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaodk3</id>
    <title>Issues running deepseek</title>
    <updated>2025-01-26T19:51:08+00:00</updated>
    <author>
      <name>/u/crazyT8nn1s</name>
      <uri>https://old.reddit.com/user/crazyT8nn1s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded deepseek-r1 (1.5b) version via ollama and tried running it on the terminal in MacOS.&lt;/p&gt; &lt;p&gt;I typed &amp;quot;Hi&amp;quot; at the prompt and I get nothing back. I just get another prompt. &amp;quot;&amp;gt;&amp;gt;&amp;gt;&amp;quot;&lt;/p&gt; &lt;p&gt;I don't see any &amp;quot;think&amp;quot; or anything of that sort. This is only isolated to deepseek. Other models I don't have this issue.&lt;/p&gt; &lt;p&gt;Any suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crazyT8nn1s"&gt; /u/crazyT8nn1s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaodk3/issues_running_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaodk3/issues_running_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaodk3/issues_running_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T19:51:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaqxrv</id>
    <title>Ollama on cpu never releases cpu?</title>
    <updated>2025-01-26T21:26:30+00:00</updated>
    <author>
      <name>/u/SweetumsTheMuppet</name>
      <uri>https://old.reddit.com/user/SweetumsTheMuppet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've got a server with 32 cpu cores and no real GPU on it. Performance is ok on 7-14b models so I'm playing around with it.&lt;/p&gt; &lt;p&gt;However, after a prompt (which reports back, for example, that it takes about 14 minutes to generate), the CPU will remain 100% of assigned cores indefinitely. Not all the time, but about 30% of the time. No idea what it's doing for tens of minutes or hours after releasing the prompt. I have to kill the process.&lt;/p&gt; &lt;p&gt;Any thoughts or tweaks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SweetumsTheMuppet"&gt; /u/SweetumsTheMuppet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaqxrv/ollama_on_cpu_never_releases_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaqxrv/ollama_on_cpu_never_releases_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaqxrv/ollama_on_cpu_never_releases_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T21:26:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iatzsq</id>
    <title>4x AMD Instinct Mi60 Server + vLLM + unsloth/DeepSeek-R1-Distill-Qwen-32B FP16</title>
    <updated>2025-01-26T23:39:12+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0u9az5iybffe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iatzsq/4x_amd_instinct_mi60_server_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iatzsq/4x_amd_instinct_mi60_server_vllm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T23:39:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iazjyi</id>
    <title>Help please! Ollama install for windows is not coming up as a zip or exe</title>
    <updated>2025-01-27T04:15:01+00:00</updated>
    <author>
      <name>/u/Extension_Focus_8803</name>
      <uri>https://old.reddit.com/user/Extension_Focus_8803</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im trying to download ollama to start delving into AI stuff, but when i try to download, it is coming up as a file that isnt a zip or exe. &lt;/p&gt; &lt;p&gt;36ca2c6f-e290-4af2-b650-f89c1158514f&lt;/p&gt; &lt;p&gt;that's the file name that is downloading for me. &lt;/p&gt; &lt;p&gt;any help would be awesome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Extension_Focus_8803"&gt; /u/Extension_Focus_8803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iazjyi/help_please_ollama_install_for_windows_is_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iazjyi/help_please_ollama_install_for_windows_is_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iazjyi/help_please_ollama_install_for_windows_is_not/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T04:15:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaqlk5</id>
    <title>GPU crashes after ~1 day of running phi4</title>
    <updated>2025-01-26T21:13:59+00:00</updated>
    <author>
      <name>/u/Admirral</name>
      <uri>https://old.reddit.com/user/Admirral</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;For starters, my system specs:&lt;br /&gt; i9 14900k&lt;br /&gt; AMD RX 6950XT&lt;br /&gt; 64gb DDR5 ram&lt;br /&gt; windows 11&lt;/p&gt; &lt;p&gt;I am running phi4 in conjunction with a data analytics bot I wrote. It computes batches of prompts roughly every 30 minutes or so, about 5 to 20 per batch (just a very rough ballpark). The issue I experience is that after about 24 hours of usage (give or take), my GPU driver starts to crash out, occasionally freezing the entire system. &lt;/p&gt; &lt;p&gt;What I have noticed is that when I first start the model, my GPU utilization stays around 0% until a prompt is sent in, at which point it spikes to 95% (along with power consumption going to 300 watts) and then drops back down to 0% and 40 watts after completing the prompt. However, after some time, I notice that the utilization trends up to 99% at idle (without any prompt processing) and power usage at 80%. This persists even when I /bye the model, and I need to go into task manager, locate &amp;quot;ollama&amp;quot; processes and end task before my card drops back down to its usual idle.&lt;/p&gt; &lt;p&gt;I am just wondering if there is anything I can do here to prevent this, or if this is just a symptom of using AMD. Am also wondering if it would be better to just use CPU processing instead, although I have this card and would hate to not have it be productive. &lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirral"&gt; /u/Admirral &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaqlk5/gpu_crashes_after_1_day_of_running_phi4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaqlk5/gpu_crashes_after_1_day_of_running_phi4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaqlk5/gpu_crashes_after_1_day_of_running_phi4/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T21:13:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib0hak</id>
    <title>creative ideas</title>
    <updated>2025-01-27T05:07:42+00:00</updated>
    <author>
      <name>/u/Choice_Complaint9171</name>
      <uri>https://old.reddit.com/user/Choice_Complaint9171</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;has anyone attempted to process medical x-rays through ollama vision &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Choice_Complaint9171"&gt; /u/Choice_Complaint9171 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib0hak/creative_ideas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib0hak/creative_ideas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib0hak/creative_ideas/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T05:07:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib0osq</id>
    <title>jacksonville Florida Ollama AI Entrepeneurs</title>
    <updated>2025-01-27T05:20:11+00:00</updated>
    <author>
      <name>/u/Choice_Complaint9171</name>
      <uri>https://old.reddit.com/user/Choice_Complaint9171</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;who in jax is looking to collaborate on creative AI opportunities. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Choice_Complaint9171"&gt; /u/Choice_Complaint9171 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib0osq/jacksonville_florida_ollama_ai_entrepeneurs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib0osq/jacksonville_florida_ollama_ai_entrepeneurs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib0osq/jacksonville_florida_ollama_ai_entrepeneurs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T05:20:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaoigf</id>
    <title>Does deepseek r1 locally support reading images?</title>
    <updated>2025-01-26T19:56:28+00:00</updated>
    <author>
      <name>/u/PawanAgarwal</name>
      <uri>https://old.reddit.com/user/PawanAgarwal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iaoigf/does_deepseek_r1_locally_support_reading_images/"&gt; &lt;img alt="Does deepseek r1 locally support reading images?" src="https://preview.redd.it/ywwq5z6j8efe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc1f5bc8e418faa002c27dfce87f3f7daa503ccb" title="Does deepseek r1 locally support reading images?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PawanAgarwal"&gt; /u/PawanAgarwal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ywwq5z6j8efe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaoigf/does_deepseek_r1_locally_support_reading_images/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaoigf/does_deepseek_r1_locally_support_reading_images/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T19:56:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib17tv</id>
    <title>Gibberish output a fair amount of the time?</title>
    <updated>2025-01-27T05:54:15+00:00</updated>
    <author>
      <name>/u/ysaric</name>
      <uri>https://old.reddit.com/user/ysaric</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run the ipex version of Ollama (0.5.1-ipexllm-20250123 using instructions from &lt;a href="https://ipex-llm-latest.readthedocs.io/en/latest/doc/LLM/Quickstart/ollama%5C_quickstart.html"&gt;https://ipex-llm-latest.readthedocs.io/en/latest/doc/LLM/Quickstart/ollama\_quickstart.html&lt;/a&gt;) for my Intel a770 16GB card, I have an older Ryzen 5 3600x CPU and 64 GB RAM, although I'm moving this week hopefully to a Ryzen 9 5950X. My OS is Windows 11, I'm running Ollama on a current version of Miniforge3. On the front end I'm using open Webui running via Docker Desktop. For the most part I run models in the 8B range (7B-8B, LLama 3 or 3.1, Deepseek-R1) or in the 13-14B range (Phi4, Qwen 2.5).&lt;/p&gt; &lt;p&gt;The issue I'm having is that a fair amount of time the output from even a pretty simple prompt is basically gibberish, ex:&lt;/p&gt; &lt;p&gt;&amp;quot;List ten awesome bird watching locations in North America&lt;/p&gt; &lt;p&gt;ollama.com/library/deepseek-r1:7bToday at 00:48&lt;/p&gt; &lt;p&gt;search as the0 to jQuery as &amp;gt;&amp;gt; in as as0: in&amp;gt; as the implode you&amp;gt; The on them you your1***&amp;gt; their. we0. &amp;lt; They**:&lt;br /&gt; as .&lt;/p&gt; &lt;p&gt;0&amp;gt;&lt;br /&gt; }}`&amp;lt;/&amp;lt;&amp;gt;&amp;gt; they} they and1.&lt;br /&gt; 13 as0:&lt;br /&gt; your, and&amp;quot;&lt;/p&gt; &lt;p&gt;What's weird is that sometimes these models will work fine, and then sometimes they output gibberish. Some models seem more likely to spout gibberish, but honestly that's all anecdote. None seem immune. My miniforge window is showing no obvious errors or crashes, nothing like I am exceeding memory capacity.&lt;/p&gt; &lt;p&gt;Just thought I would see if anyone else has run into or runs into issues like this, and certainly if there was a way to stop it from happening or make it happen less often I would be all ears.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ysaric"&gt; /u/ysaric &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib17tv/gibberish_output_a_fair_amount_of_the_time/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib17tv/gibberish_output_a_fair_amount_of_the_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib17tv/gibberish_output_a_fair_amount_of_the_time/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T05:54:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1iapk5s</id>
    <title>Can I restrict a model to only run on CPU?</title>
    <updated>2025-01-26T20:35:25+00:00</updated>
    <author>
      <name>/u/mindsetFPS</name>
      <uri>https://old.reddit.com/user/mindsetFPS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mindsetFPS"&gt; /u/mindsetFPS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iapk5s/can_i_restrict_a_model_to_only_run_on_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iapk5s/can_i_restrict_a_model_to_only_run_on_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iapk5s/can_i_restrict_a_model_to_only_run_on_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T20:35:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib1ryi</id>
    <title>Difference in Llama 3.2 1B instruct and Llama 3.1 8B</title>
    <updated>2025-01-27T06:31:39+00:00</updated>
    <author>
      <name>/u/lonesomhelme</name>
      <uri>https://old.reddit.com/user/lonesomhelme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all, noob LLM tweaker here. I wanted to know the difference when working with these models. &lt;/p&gt; &lt;p&gt;For context, I deployed both these models on a cloud provider using vLLM inference. From what I noticed it was not easy to get proper responses from 3.2 1b model and the response were short. When I switched to 3.1 8b I could get better responses.&lt;/p&gt; &lt;p&gt;I want to understand what could've been the issue - is it my setup or the model?&lt;/p&gt; &lt;p&gt;More Context: I tried Llama 3.2 1B from Ollama and everything worked great. But when I switched to pulling the model from HuggingFace it messed up everything. Essentially, it's the same model, so it should work the same. What could be wrong here??&lt;/p&gt; &lt;p&gt;Appreciate any help in understanding this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lonesomhelme"&gt; /u/lonesomhelme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib1ryi/difference_in_llama_32_1b_instruct_and_llama_31_8b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib1ryi/difference_in_llama_32_1b_instruct_and_llama_31_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib1ryi/difference_in_llama_32_1b_instruct_and_llama_31_8b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T06:31:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaq51g</id>
    <title>UI-Tars vs Brower-Use Web-UI</title>
    <updated>2025-01-26T20:57:18+00:00</updated>
    <author>
      <name>/u/DelPrive235</name>
      <uri>https://old.reddit.com/user/DelPrive235</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone know which agent is performing better and more stable at the moment? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DelPrive235"&gt; /u/DelPrive235 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaq51g/uitars_vs_broweruse_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaq51g/uitars_vs_broweruse_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaq51g/uitars_vs_broweruse_webui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T20:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib3m97</id>
    <title>The Problem with Chinese based AI Companies - DeepSeek-R1</title>
    <updated>2025-01-27T08:34:55+00:00</updated>
    <author>
      <name>/u/DayshareLP</name>
      <uri>https://old.reddit.com/user/DayshareLP</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ib3m97/the_problem_with_chinese_based_ai_companies/"&gt; &lt;img alt="The Problem with Chinese based AI Companies - DeepSeek-R1" src="https://b.thumbs.redditmedia.com/g0r7i3-gONdYCsYFY9YKNDUPLTlS9Bq9NDlDbyUx6zM.jpg" title="The Problem with Chinese based AI Companies - DeepSeek-R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/h90nn5rizhfe1.png?width=1481&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=778c2898c231e94c8283820bd135f7953a8d8783"&gt;https://preview.redd.it/h90nn5rizhfe1.png?width=1481&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=778c2898c231e94c8283820bd135f7953a8d8783&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Deep Seek is a Chinese company and as such it has to censor their model according to the Chinese Communist Party's views. This is a very bad thing and we should boycott this AI model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DayshareLP"&gt; /u/DayshareLP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib3m97/the_problem_with_chinese_based_ai_companies/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib3m97/the_problem_with_chinese_based_ai_companies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib3m97/the_problem_with_chinese_based_ai_companies/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T08:34:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1iacoys</id>
    <title>Is there a pure, quantised version of DeekSeekr1 for Ollama?</title>
    <updated>2025-01-26T11:55:10+00:00</updated>
    <author>
      <name>/u/john_alan</name>
      <uri>https://old.reddit.com/user/john_alan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see this: &lt;a href="https://ollama.com/library/deepseek-r1"&gt;https://ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but it appears to be Llama3.2/Qwen trained with DeepSeek r1.&lt;/p&gt; &lt;p&gt;I essentially want an 8bit 70bn version of pure r1?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/john_alan"&gt; /u/john_alan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iacoys/is_there_a_pure_quantised_version_of_deekseekr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iacoys/is_there_a_pure_quantised_version_of_deekseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iacoys/is_there_a_pure_quantised_version_of_deekseekr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T11:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaht3c</id>
    <title>DeepSeek-R1's Bias</title>
    <updated>2025-01-26T15:47:14+00:00</updated>
    <author>
      <name>/u/cov_id19</name>
      <uri>https://old.reddit.com/user/cov_id19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iaht3c/deepseekr1s_bias/"&gt; &lt;img alt="DeepSeek-R1's Bias" src="https://preview.redd.it/41tjd3220dfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d3331ed008c8f650aba9895a60c4c649bd8ab47d" title="DeepSeek-R1's Bias" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cov_id19"&gt; /u/cov_id19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/41tjd3220dfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaht3c/deepseekr1s_bias/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaht3c/deepseekr1s_bias/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T15:47:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iavfc6</id>
    <title>8x AMD Instinct Mi60 Server + vLLM + unsloth/DeepSeek-R1-Distill-Qwen-32B FP16</title>
    <updated>2025-01-27T00:46:36+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vpaab64pkffe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iavfc6/8x_amd_instinct_mi60_server_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iavfc6/8x_amd_instinct_mi60_server_vllm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T00:46:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib0ddn</id>
    <title>ollama server</title>
    <updated>2025-01-27T05:01:35+00:00</updated>
    <author>
      <name>/u/Choice_Complaint9171</name>
      <uri>https://old.reddit.com/user/Choice_Complaint9171</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i want to setup ollama to be able to receive request from me in other words i want to be able to communicate with my local ollama on my desktop pc remotely through my iphone can someone confirm if this is possible i would assume you can make this possible with webui something like assigning authentication through ip address for remote use &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Choice_Complaint9171"&gt; /u/Choice_Complaint9171 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib0ddn/ollama_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib0ddn/ollama_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib0ddn/ollama_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T05:01:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1iawxvm</id>
    <title>Deepseek-R1:8b</title>
    <updated>2025-01-27T02:02:52+00:00</updated>
    <author>
      <name>/u/Choice_Complaint9171</name>
      <uri>https://old.reddit.com/user/Choice_Complaint9171</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone please help me from my ignorance is it me or does deepseek R1 :8b seem to resemble a young adult with high IQ and a lot of self doubt &lt;/p&gt; &lt;p&gt;To confirm I‚Äôm not ignorant myself try asking ‚Äúis buffalo a part of New York?‚Äù In your setup. &lt;/p&gt; &lt;h1&gt;just your Average AI Contributor&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Choice_Complaint9171"&gt; /u/Choice_Complaint9171 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iawxvm/deepseekr18b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iawxvm/deepseekr18b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iawxvm/deepseekr18b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T02:02:52+00:00</published>
  </entry>
</feed>
