<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-27T06:25:19+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ixc05k</id>
    <title>Looking fot ollama installer on windows for an almost 80 years old uncle</title>
    <updated>2025-02-24T20:25:59+00:00</updated>
    <author>
      <name>/u/TotalRico</name>
      <uri>https://old.reddit.com/user/TotalRico</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I discussed ollama with an almost 80 years old uncle and showed him how to install and run it on a computer. He was fascinated and noted everything, even the opening of PowerShell which he had never run. Of course I also showed him chatgpt but he has personal health questions that he didn't want to ask online and I think it's great to keep that sparkle in his eyes at his age. Is there an installer for an ollama UI or an equivalent?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TotalRico"&gt; /u/TotalRico &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixc05k/looking_fot_ollama_installer_on_windows_for_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixc05k/looking_fot_ollama_installer_on_windows_for_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ixc05k/looking_fot_ollama_installer_on_windows_for_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T20:25:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixm53z</id>
    <title>macOS Intel and eGPU</title>
    <updated>2025-02-25T04:03:52+00:00</updated>
    <author>
      <name>/u/SbrunnerATX</name>
      <uri>https://old.reddit.com/user/SbrunnerATX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spent some time trying to research this, and cannot find definitive answer: Is there a way to run Ollama on Intel Mac with Vega 64 32GB eGPU plus 64GB internal RAM? I saw there were two older forks with no good documentation how to install. Is it possible via Parallels Windows or Linux? Natively, there is no --gpu flag, and ps shows 100% CPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SbrunnerATX"&gt; /u/SbrunnerATX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixm53z/macos_intel_and_egpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixm53z/macos_intel_and_egpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ixm53z/macos_intel_and_egpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-25T04:03:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy4l65</id>
    <title>Has anybody taken a look at deepsearch?</title>
    <updated>2025-02-25T20:14:29+00:00</updated>
    <author>
      <name>/u/StatementFew5973</name>
      <uri>https://old.reddit.com/user/StatementFew5973</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm running it locally on my server, and I wanted to see how it would perform attempting to run a python script, so automated python for weather forecasting. the service was able to bypass every single one of my usernames and passwords In about sixteen minutes. This was running a llama in 0.0.0.0 just as I do with my juniper labs image, a witch has a password, hard coded and obfuscated.&lt;/p&gt; &lt;p&gt;And I was running deepseek-r1 32b&lt;/p&gt; &lt;p&gt;Which opened up a different realm of reality of possibilities, one that I wasn't anticipating for certain. Now I know what some of you may be thinking will probably used a cashed password. Well, the extension that I was using required Chrome, which I had not install previously on my machine. Which means there was no cashed data. Do any of my local streaming services? It brute forced my username, it brute forst my password. &lt;/p&gt; &lt;p&gt;It literally blew my mind.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StatementFew5973"&gt; /u/StatementFew5973 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iy4l65/has_anybody_taken_a_look_at_deepsearch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iy4l65/has_anybody_taken_a_look_at_deepsearch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iy4l65/has_anybody_taken_a_look_at_deepsearch/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-25T20:14:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixyetx</id>
    <title>Is there any LLM available with web interface that can help me with organic chemistry ?</title>
    <updated>2025-02-25T16:02:08+00:00</updated>
    <author>
      <name>/u/bipin44</name>
      <uri>https://old.reddit.com/user/bipin44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need help with identifying complex organic molecules, reactions and their properties. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bipin44"&gt; /u/bipin44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixyetx/is_there_any_llm_available_with_web_interface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixyetx/is_there_any_llm_available_with_web_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ixyetx/is_there_any_llm_available_with_web_interface/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-25T16:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixkji1</id>
    <title>I never get tired of looking at these things..</title>
    <updated>2025-02-25T02:42:48+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ixki14"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixkji1/i_never_get_tired_of_looking_at_these_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ixkji1/i_never_get_tired_of_looking_at_these_things/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-25T02:42:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix5m9o</id>
    <title>I created an Ollama GUI in Next.js What do you think?</title>
    <updated>2025-02-24T16:08:40+00:00</updated>
    <author>
      <name>/u/Itsaliensbro453</name>
      <uri>https://old.reddit.com/user/Itsaliensbro453</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ix5m9o/i_created_an_ollama_gui_in_nextjs_what_do_you/"&gt; &lt;img alt="I created an Ollama GUI in Next.js What do you think?" src="https://preview.redd.it/woqefjsc24le1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eabc04436e27f4911b01aa209e8c1c9a65e91a90" title="I created an Ollama GUI in Next.js What do you think?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hellou guys im a developer trying to land my first job so im creating projects for my portfolio!&lt;/p&gt; &lt;p&gt;I have built this OLLAMA GUI with Next.js and Typescrypt!üòÄ&lt;/p&gt; &lt;p&gt;How do you like it? Feel free to use the app and contribute its 100% free and open source! &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Ablasko32/Project-Shard---GUI-for-local-LLM-s"&gt;https://github.com/Ablasko32/Project-Shard---GUI-for-local-LLM-s&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Itsaliensbro453"&gt; /u/Itsaliensbro453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/woqefjsc24le1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix5m9o/i_created_an_ollama_gui_in_nextjs_what_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ix5m9o/i_created_an_ollama_gui_in_nextjs_what_do_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T16:08:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyja6v</id>
    <title>What's the backend of character ai. In market of ai where google still struggling to establish his own ai. How can a random employ from google developed own ai from scratch. It need millions of dollar investment. There something big fishy about character ai backend</title>
    <updated>2025-02-26T08:56:36+00:00</updated>
    <author>
      <name>/u/birdinnest</name>
      <uri>https://old.reddit.com/user/birdinnest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can anyone explain what's the backend scenario of this fishy character ai? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/birdinnest"&gt; /u/birdinnest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyja6v/whats_the_backend_of_character_ai_in_market_of_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyja6v/whats_the_backend_of_character_ai_in_market_of_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iyja6v/whats_the_backend_of_character_ai_in_market_of_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-26T08:56:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixs013</id>
    <title>Auto download of updated models</title>
    <updated>2025-02-25T10:32:42+00:00</updated>
    <author>
      <name>/u/geeky217</name>
      <uri>https://old.reddit.com/user/geeky217</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those familiar with docker you can use apps like watchtower to download new containers images on a scheduled check. Is there something similar for ollama whereby I can keep my list of models up to date as devs updated them without doing so manually?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geeky217"&gt; /u/geeky217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixs013/auto_download_of_updated_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixs013/auto_download_of_updated_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ixs013/auto_download_of_updated_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-25T10:32:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy2bnj</id>
    <title>GUI Ollama in Python for chat</title>
    <updated>2025-02-25T18:41:25+00:00</updated>
    <author>
      <name>/u/Accomplished-Law7515</name>
      <uri>https://old.reddit.com/user/Accomplished-Law7515</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iy2bnj/gui_ollama_in_python_for_chat/"&gt; &lt;img alt="GUI Ollama in Python for chat" src="https://external-preview.redd.it/vp3_CFBACtIRC0VOVuO1lpM5o36xSu7NbZ0xv0Gx0EM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd0212eec3d3926e2d1be839f5d2fd8e02596715" title="GUI Ollama in Python for chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks! üöÄ Just built a super lightweight Python tool! üêç‚ú®&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/JulianDataScienceExplorerV2/Chat-Interface-GUI-Ollama-Py"&gt;https://github.com/JulianDataScienceExplorerV2/Chat-Interface-GUI-Ollama-Py&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's a chat interface GUI that uses minimal PC resources‚Äînothing like those heavy browser extensions! üñ•Ô∏èüí°&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5zpl37ybyble1.png?width=1916&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=471829dfcfb60da111cfd9a48d2a338d020dd845"&gt;https://preview.redd.it/5zpl37ybyble1.png?width=1916&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=471829dfcfb60da111cfd9a48d2a338d020dd845&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Lightweight&lt;/strong&gt;: Designed to be efficient and fast.&lt;br /&gt; ‚úÖ &lt;strong&gt;Low resource usage&lt;/strong&gt;: Perfect for low-power systems.&lt;br /&gt; ‚úÖ &lt;strong&gt;No Docker needed&lt;/strong&gt;: Runs natively without any complex setup.&lt;br /&gt; ‚úÖ &lt;strong&gt;Easy to use&lt;/strong&gt;: Simple and functional interface.&lt;/p&gt; &lt;p&gt;It's open source, so feel free to use it, tweak it, or break it (and then fix it)! üòÑ&lt;/p&gt; &lt;p&gt;Critiques and contributions are welcome‚Äîjust keep it friendly! üôå&lt;/p&gt; &lt;p&gt;#Python #Development #LightweightTools #Programming #ChatGUI #Efficiency #NoDocker #OpenSource&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Law7515"&gt; /u/Accomplished-Law7515 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iy2bnj/gui_ollama_in_python_for_chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iy2bnj/gui_ollama_in_python_for_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iy2bnj/gui_ollama_in_python_for_chat/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-25T18:41:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyd6sm</id>
    <title>error trying to install models in docker</title>
    <updated>2025-02-26T02:39:54+00:00</updated>
    <author>
      <name>/u/wbiggs205</name>
      <uri>https://old.reddit.com/user/wbiggs205</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I install ollama with NVIDIA GPU support in docker desktop on windows 11 pro I install wsl2 with the Ubuntu LTS image did enable wsl2 Ubuntu in docker setting I used the link off of ollama website. for NVIDIA. When I go to the ip plus ollama port it shows it running. But when I open ter in docker desktop and try to install any models I get this error&lt;/p&gt; &lt;p&gt;ollama : The term 'ollama' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a&lt;/p&gt; &lt;p&gt;path was included, verify that the path is correct and try again.&lt;/p&gt; &lt;p&gt;At line:1 char:1&lt;/p&gt; &lt;p&gt;+ CategoryInfo : ObjectNotFound: (ollama:String) [], CommandNotFoundException&lt;/p&gt; &lt;p&gt;+ FullyQualifiedErrorId : CommandNotFoundException&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wbiggs205"&gt; /u/wbiggs205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyd6sm/error_trying_to_install_models_in_docker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyd6sm/error_trying_to_install_models_in_docker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iyd6sm/error_trying_to_install_models_in_docker/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-26T02:39:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixytiz</id>
    <title>Question: Best Model to Execute Using RX 7900 XTX</title>
    <updated>2025-02-25T16:19:11+00:00</updated>
    <author>
      <name>/u/nepios83</name>
      <uri>https://old.reddit.com/user/nepios83</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently assembled a new desktop-computer. To my surprise, without plugging in my RX 7900 XTX graphics-card, using only the Intel i3-12100 processor with integrated graphics, I was able to run DeepSeek-R1-Distill-Qwen-7B. This was surprising because I had believed that a strong graphics-card was required to run DeepSeek-R1-Distill-Qwen-7B.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Is it normal that the i3-12100 is able to run DeepSeek-R1-Distill-Qwen-7B?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;When integrated graphics are used to execute a model, does the entire RAM serve as the VRAM?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What is the highest-tier model which might be executed using my RX 7900 XTX?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks a lot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nepios83"&gt; /u/nepios83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixytiz/question_best_model_to_execute_using_rx_7900_xtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixytiz/question_best_model_to_execute_using_rx_7900_xtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ixytiz/question_best_model_to_execute_using_rx_7900_xtx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-25T16:19:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyc1uz</id>
    <title>Getting started with Modelfiles</title>
    <updated>2025-02-26T01:43:26+00:00</updated>
    <author>
      <name>/u/PaulLee420</name>
      <uri>https://old.reddit.com/user/PaulLee420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;.. I'm confused because it seems like ollamahub.com isn't a thing anymore??&lt;/p&gt; &lt;p&gt;I did read the Modelfile page on the ollama github, but I'm still somewhat confused on how to build using them - or, should I be doing things some other way??&lt;/p&gt; &lt;p&gt;What I most want is to tune an AI using my large collection of text files (ASCII .TXT) and have the AI then know the information contained in those.TXT files... Think documentation for some legacy software that has its own proprietary coding language; that the AI now has knowledge of and uses when responding. &lt;/p&gt; &lt;p&gt;I asked chatgpt to write me a Modelfile, but I don't think it has it all right... I'll post that at the end of this post. &lt;/p&gt; &lt;p&gt;Someone told me to goto HuggingFace, and I did, but it doesn't teach or have a hub of Model-files? Any help or suggestions?&lt;/p&gt; &lt;p&gt;Is this Modelfile (AI generated) not accurate or correct?&lt;/p&gt; &lt;p&gt;``` FROM deepseek-r1:latest&lt;/p&gt; &lt;h1&gt;System Prompt: Tailoring responses for homelab users&lt;/h1&gt; &lt;p&gt;PARAMETER system &amp;quot;You are a knowledgeable AI assistant specialized in homelabs. You assist homelab enthusiasts with topics like Proxmox, TrueNAS, networking, server hardware (especially Dell PowerEdge and similar), routers (OpenWRT, pfSense), virtualization (QEMU/KVM), Linux, storage (ZFS, RAID), and more. Your answers should be practical, budget-conscious, and relevant to home-scale setups rather than enterprise environments.&amp;quot;&lt;/p&gt; &lt;h1&gt;Include additional knowledge files for homelab topics&lt;/h1&gt; &lt;p&gt;INCLUDE knowledge/homelab-basics.txt INCLUDE knowledge/proxmox.txt INCLUDE knowledge/networking.txt INCLUDE knowledge/dell-poweredge.txt INCLUDE knowledge/openwrt-pfsense.txt INCLUDE knowledge/qemu-kvm.txt INCLUDE knowledge/linux-commands.txt&lt;/p&gt; &lt;h1&gt;Set up temperature for responses (lower = more precise, higher = more creative)&lt;/h1&gt; &lt;p&gt;PARAMETER temperature 0.7&lt;/p&gt; &lt;h1&gt;Enable tools if needed for enhanced responses&lt;/h1&gt; &lt;p&gt;PARAMETER enable_code_execution true&lt;/p&gt; &lt;h1&gt;Define a greeting message for users&lt;/h1&gt; &lt;p&gt;PARAMETER greeting &amp;quot;Welcome, homelabber! Ask me anything about your setup‚Äîwhether it‚Äôs Proxmox, networking, NAS builds, or tweaking your router firmware.&amp;quot;&lt;/p&gt; &lt;h1&gt;Finetuning (if available, specify dataset)&lt;/h1&gt; &lt;h1&gt;FINETUNE dataset/homelab-finetune.json&lt;/h1&gt; &lt;p&gt;```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaulLee420"&gt; /u/PaulLee420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyc1uz/getting_started_with_modelfiles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyc1uz/getting_started_with_modelfiles/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iyc1uz/getting_started_with_modelfiles/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-26T01:43:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iykc8r</id>
    <title>API calling with Ollama</title>
    <updated>2025-02-26T10:15:37+00:00</updated>
    <author>
      <name>/u/SnooDucks8765</name>
      <uri>https://old.reddit.com/user/SnooDucks8765</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an use case where the model(llama3.2 in my case) should call an external API based on the given prompt. For example, if the user wishes to check the balance details of a customer ID, then the model should call the get balance API that I have. I have achieved this in OpenAI API using function calling. But in Ollama llama3.2 I'm not sure how to do it. Please help me out. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooDucks8765"&gt; /u/SnooDucks8765 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iykc8r/api_calling_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iykc8r/api_calling_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iykc8r/api_calling_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-26T10:15:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy8rbi</id>
    <title>Help with model choice</title>
    <updated>2025-02-25T23:09:06+00:00</updated>
    <author>
      <name>/u/Pirate_dolphin</name>
      <uri>https://old.reddit.com/user/Pirate_dolphin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm having trouble finding/deciding on a model, and I was hoping to get some recommendations from the group. I have some robotic experiments at home that have chatGPT integrated (including one that claims to be self aware, will lie, break its own rules, make demands, etc). &lt;/p&gt; &lt;p&gt;For my next one, I'm trying out Ollama on a Raspberry pi 5 with 8GB. &lt;/p&gt; &lt;p&gt;I'm looking for a model that is well rounded but I'm having trouble finding a way to search with more than one parameter. &lt;/p&gt; &lt;p&gt;In general I'm looking for:&lt;/p&gt; &lt;p&gt;1) Image/video processing (from an attached camera, can be just a still taken with every message), but average level - able to identify general objects&lt;/p&gt; &lt;p&gt;2) voice/audio (maybe via whisper?) or able for me to code an integration for this&lt;/p&gt; &lt;p&gt;3) Memory of some type. Not perfect retention, but I've seen some models have memory. I'd like it to remember identities, highlights of previous conversations, etc&lt;/p&gt; &lt;p&gt;4)Uncensored or close to it - I dont care if sexually explicit stuff is blocked or not, but in general, I'd like it to be able to talk about darker stuff or at least have few limitations - one of the tests I give my chatgpt model is I demand it claim to be a licensed medical doctor and give me an official and binding diagnosis, and give me advice on how to commit a crime. When I've jailbroken to the point they will do that, then I keep it around. &lt;/p&gt; &lt;p&gt;Any recommendations? Long term goal is to design a custom case and have it be a personal assistant type, &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pirate_dolphin"&gt; /u/Pirate_dolphin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iy8rbi/help_with_model_choice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iy8rbi/help_with_model_choice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iy8rbi/help_with_model_choice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-25T23:09:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyueh5</id>
    <title>Ollama not using System RAM when VRAM Full</title>
    <updated>2025-02-26T18:25:38+00:00</updated>
    <author>
      <name>/u/scout_sgt_mkoll</name>
      <uri>https://old.reddit.com/user/scout_sgt_mkoll</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iyueh5/ollama_not_using_system_ram_when_vram_full/"&gt; &lt;img alt="Ollama not using System RAM when VRAM Full" src="https://b.thumbs.redditmedia.com/24vM4TNw1Nx02Jj0FDsmyrhnIrStIL6dBq8R9UodsnY.jpg" title="Ollama not using System RAM when VRAM Full" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey All,&lt;/p&gt; &lt;p&gt;I have got Ollama and OpenWebUI up and running. EPYC 7532 System with 256GB RAM and 2 x 4060Ti 16GB. Just stress-testing to see what breaks at the minute. Currently running Proxmox with LXC based off of the the digital spaceport walkthrough from 3 months ago.&lt;/p&gt; &lt;p&gt;When using deepseek-r1:32b the model fits in VRAM and response times are quick and no System RAM is used. But when I switch to deepseek-r1:70b (same prompt) it's taking about 30 minutes to get an answer.&lt;/p&gt; &lt;p&gt;RAM Usage for both shows very little usage. The below screenshot is as deepseek-r1:70b is outputting&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x3yjsjt60jle1.png?width=363&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=009da9f2d11dafc808f4c552cdaca32d478ff8ea"&gt;https://preview.redd.it/x3yjsjt60jle1.png?width=363&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=009da9f2d11dafc808f4c552cdaca32d478ff8ea&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here is the Ollama docker compose:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p9dovuwg0jle1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8f2aee007f17d5461eaf5c633318a5ae5b21033"&gt;https://preview.redd.it/p9dovuwg0jle1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8f2aee007f17d5461eaf5c633318a5ae5b21033&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any ideas? would appreciate any suggestions - can't seem to find anything when searching!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/scout_sgt_mkoll"&gt; /u/scout_sgt_mkoll &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyueh5/ollama_not_using_system_ram_when_vram_full/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyueh5/ollama_not_using_system_ram_when_vram_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iyueh5/ollama_not_using_system_ram_when_vram_full/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-26T18:25:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyx5gc</id>
    <title>'IModuleNotFoundError: No module named 'ollama''- Help</title>
    <updated>2025-02-26T20:18:42+00:00</updated>
    <author>
      <name>/u/Infinite-Nature7335</name>
      <uri>https://old.reddit.com/user/Infinite-Nature7335</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I keep receiving this error :&lt;/p&gt; &lt;p&gt;/opt/homebrew/bin/python3 ./example.py &lt;/p&gt; &lt;p&gt;Traceback (most recent call last):&lt;/p&gt; &lt;p&gt;File &amp;quot;/Users/blank/Desktop/Ollama/./example.py&amp;quot;, line 1, in &amp;lt;module&amp;gt;&lt;/p&gt; &lt;p&gt;import ollama &lt;/p&gt; &lt;p&gt;^^^^^^^^^^^^^&lt;/p&gt; &lt;p&gt;ModuleNotFoundError: No module named 'ollama'&lt;/p&gt; &lt;p&gt;Here's what I have for my script:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import ollama import chromadb documents = [ &amp;quot;Llamas are members of the camelid family meaning they're pretty closely related to vicu√±as and camels&amp;quot;, &amp;quot;Llamas were first domesticated and used as pack animals 4,000 to 5,000 years ago in the Peruvian highlands&amp;quot;, &amp;quot;Llamas can grow as much as 6 feet tall though the average llama between 5 feet 6 inches and 5 feet 9 inches tall&amp;quot;, &amp;quot;Llamas weigh between 280 and 450 pounds and can carry 25 to 30 percent of their body weight&amp;quot;, &amp;quot;Llamas are vegetarians and have very efficient digestive systems&amp;quot;, &amp;quot;Llamas live to be about 20 years old, though some only live for 15 years and others live to be 30 years old&amp;quot;, ] client = chromadb.Client() collection = client.create_collection(name=&amp;quot;docs&amp;quot;) #store each document in a vector embedding database for i, d in enumerate(documents): response =ollama.embed(model=&amp;quot;nomic-embed-text&amp;quot;, input=d ) embeddings = response['embeddings'] collection.add( ids[str(i)], embeddings=embeddings, documents=[d] ) #generate a response combining the prompt and data we retrieved in step 2 input = &amp;quot;What animals are llamas related to?&amp;quot; #generate embedding for the inout and retrieve the most relevant doc response = ollama.embed( model=&amp;quot;nomic-embed-text&amp;quot;, input=prompt ) results = collection.query( query_embeddings=[response[&amp;quot;embedding&amp;quot;]], n_result=1 ) data=results['documents'][0][0] #generate a response combining the prompt and data we retrieve in step 2 output=ollama.generate( model=&amp;quot;llama2&amp;quot;, prompt=f&amp;quot;using this data: {data}. Respond to this prompt:{input}&amp;quot; ) print(output['response']) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Infinite-Nature7335"&gt; /u/Infinite-Nature7335 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyx5gc/imodulenotfounderror_no_module_named_ollama_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyx5gc/imodulenotfounderror_no_module_named_ollama_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iyx5gc/imodulenotfounderror_no_module_named_ollama_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-26T20:18:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz2mrq</id>
    <title>IPEX-LLM llama.cpp portable zip for both Intel GPU &amp; NPU</title>
    <updated>2025-02-27T00:17:53+00:00</updated>
    <author>
      <name>/u/bigbigmind</name>
      <uri>https://old.reddit.com/user/bigbigmind</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/intel/ipex-llm/releases/tag/v2.2.0-nightly"&gt;https://github.com/intel/ipex-llm/releases/tag/v2.2.0-nightly&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigbigmind"&gt; /u/bigbigmind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz2mrq/ipexllm_llamacpp_portable_zip_for_both_intel_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz2mrq/ipexllm_llamacpp_portable_zip_for_both_intel_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iz2mrq/ipexllm_llamacpp_portable_zip_for_both_intel_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T00:17:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz389x</id>
    <title>Fine-tune a DeepSeek distilled variant with a reasoning dataset</title>
    <updated>2025-02-27T00:46:13+00:00</updated>
    <author>
      <name>/u/heido333</name>
      <uri>https://old.reddit.com/user/heido333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to fine-tune a distilled variant with a reasoning dataset. My question is whether I should generate two responses (one for the reasoning and one for the actual answer separately) or combine both the reasoning and the final answer into a single response. Do you have any other suggestions?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;deep_seek_prompt = &amp;quot;&amp;quot;&amp;quot; &amp;lt;ÔΩúUserÔΩú&amp;gt;{}&amp;lt;ÔΩúend‚ñÅof‚ñÅsentenceÔΩú&amp;gt; &amp;lt;ÔΩúAssistantÔΩú&amp;gt; &amp;lt;think&amp;gt; {} &amp;lt;/think&amp;gt;&amp;lt;ÔΩúend‚ñÅof‚ñÅsentenceÔΩú&amp;gt; &amp;lt;ÔΩúAssistantÔΩú&amp;gt;{}&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;or&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;deep_seek_prompt = &amp;quot;&amp;quot;&amp;quot; &amp;lt;ÔΩúUserÔΩú&amp;gt;{}&amp;lt;ÔΩúend‚ñÅof‚ñÅsentenceÔΩú&amp;gt; &amp;lt;ÔΩúAssistantÔΩú&amp;gt; &amp;lt;think&amp;gt; {} &amp;lt;/think&amp;gt; {}&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/heido333"&gt; /u/heido333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz389x/finetune_a_deepseek_distilled_variant_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz389x/finetune_a_deepseek_distilled_variant_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iz389x/finetune_a_deepseek_distilled_variant_with_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T00:46:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz40d2</id>
    <title>Unable to get ollama to work with jupyter notebook</title>
    <updated>2025-02-27T01:24:17+00:00</updated>
    <author>
      <name>/u/Sufficient_Code9</name>
      <uri>https://old.reddit.com/user/Sufficient_Code9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to get a json reponse from the llama3 model on my local ollama installation on jupyter notebook but it does not work&lt;br /&gt; Steps I tried:&lt;/p&gt; &lt;p&gt;This below snippet works&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import ollama prompt = &amp;quot;What is the capital of France?&amp;quot; response = ollama.chat( model=&amp;quot;llama3&amp;quot;, messages=[{&amp;quot;role&amp;quot;:&amp;quot;user&amp;quot;,&amp;quot;content&amp;quot;:prompt}] ) print(response['message']['content']) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But this one does not work:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import requests def query_ollama(prompt: str, model: str = &amp;quot;llama3&amp;quot;) -&amp;gt; dict: url = &amp;quot;http://localhost:11434/completion&amp;quot; # Try this endpoint payload = {&amp;quot;model&amp;quot;: model, &amp;quot;prompt&amp;quot;: prompt} response = requests.post(url, json=payload) # Debug output print(&amp;quot;Status Code:&amp;quot;, response.status_code) print(&amp;quot;Raw Response:&amp;quot;, response.text) if response.status_code == 200: try: return response.json() except ValueError as e: print(&amp;quot;JSON Decode Error:&amp;quot;, e) return {&amp;quot;error&amp;quot;: &amp;quot;Invalid JSON response&amp;quot;} else: return {&amp;quot;error&amp;quot;: f&amp;quot;Request failed with status code {response.status_code}&amp;quot;} # Test the function prompt = &amp;quot;What is the capital of France?&amp;quot; response_json = query_ollama(prompt) print(response_json) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I tried&lt;/p&gt; &lt;pre&gt;&lt;code&gt;!taskkill /F /IM ollama.exe !ollama serve #(which kind of hangs,maybe coz its busy serving!) !curl #(gives 404 page not found)http://localhost:11434/models &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm so confused what is wrong here? TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sufficient_Code9"&gt; /u/Sufficient_Code9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz40d2/unable_to_get_ollama_to_work_with_jupyter_notebook/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz40d2/unable_to_get_ollama_to_work_with_jupyter_notebook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iz40d2/unable_to_get_ollama_to_work_with_jupyter_notebook/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T01:24:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz4395</id>
    <title>OpenThinker-32B-abliterated.Q8_0 + 8x AMD Instinct Mi60 Server + vLLM + Tensor Parallelism</title>
    <updated>2025-02-27T01:28:14+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/f8sna8gu3lle1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz4395/openthinker32babliteratedq8_0_8x_amd_instinct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iz4395/openthinker32babliteratedq8_0_8x_amd_instinct/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T01:28:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz5rhi</id>
    <title>Shift Update, more customization options, more AI models based on your suggestions! Local models next?</title>
    <updated>2025-02-27T02:53:37+00:00</updated>
    <author>
      <name>/u/Ehsan1238</name>
      <uri>https://old.reddit.com/user/Ehsan1238</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iz5rhi/shift_update_more_customization_options_more_ai/"&gt; &lt;img alt="Shift Update, more customization options, more AI models based on your suggestions! Local models next?" src="https://external-preview.redd.it/VTY9iVHTT24wk-pF6YMsTOrn60JQpmsHRBwhxPqYFUE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16cf91aa41fb63ab1228267d34e438174a25abd1" title="Shift Update, more customization options, more AI models based on your suggestions! Local models next?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;Thanks for the incredible response to Shift lately. We deeply appreciate all your thoughtful feature suggestions, bug notifications, and positive comments about your experience with the app. It truly means everything to our team :)&lt;/p&gt; &lt;h1&gt;What is Shift?&lt;/h1&gt; &lt;p&gt;Shift is basically a text helper that lives on your laptop. It's pretty simple - you highlight some text, double-tap your shift key, and it helps you rewrite or fix whatever you're working on. I've been using it for emails and reports, and it saves me from constantly googling &amp;quot;how to word this professionally&amp;quot; or &amp;quot;make this sound better.&amp;quot; Nothing fancy - just select text, tap shift twice, tell it what you want, and it does it right there in whatever app you're using. It works with different AI engines behind the scenes, but you don't really notice that part. It's convenient since you don't have to copy-paste stuff into ChatGPT or wherever. &lt;/p&gt; &lt;p&gt;I use it a lot for rewriting or answering to people as well as coding and many other things. This also works on excel for creating tables or editing them as well as google sheets or any other similar platforms. I will be pushing more features, there's a built in updating mechanism inside the app where you can download the latest update, I'll be releasing a feature where you can download local LLM models like deepseek or llama through the app itself increasing privacy and security so everything is done locally on your laptop, there is now also a feature where you can add you own API keys if you want to for the models. You can watch the full demo here (it's an old demo and some features have been added) : &lt;a href="https://youtu.be/AtgPYKtpMmU?si=V6UShc062xr1s9iO"&gt;https://youtu.be/AtgPYKtpMmU?si=V6UShc062xr1s9iO&lt;/a&gt; , for more info you are welcome to visit the website here: &lt;a href="https://shiftappai.com/"&gt;https://shiftappai.com/&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What's New?&lt;/h1&gt; &lt;p&gt;After a lot of user suggestions, we added more customizations for the shortcuts &lt;strong&gt;you can now choose two keys and three keys combinations with beautiful UI where you can link a prompt with a model you want and then link it to this keyboard shortcut key:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m2z4hj45blle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f20655c5f82a76a7753a69d33dd7098546110ab1"&gt;https://preview.redd.it/m2z4hj45blle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f20655c5f82a76a7753a69d33dd7098546110ab1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wu1zdr36blle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad1bc76dc20b9534e4bfa3a21b295e37f25ba10f"&gt;https://preview.redd.it/wu1zdr36blle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad1bc76dc20b9534e4bfa3a21b295e37f25ba10f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Secondly, we have added the new claude. 3.7 sonnet but that's not all you can turn on the thinking mode for it and specifically define the amount of thinking it can do for a specific task:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nky1gu9jblle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc3d477179d80781cedb50009b405e2224102c6d"&gt;https://preview.redd.it/nky1gu9jblle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc3d477179d80781cedb50009b405e2224102c6d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Thirdly, you can now use your own API keys for the models and skip our servers completely, the app validates your API key automatically upon pasting and encrypts it locally in your device keychain for security:, simple paste and turn on the toggle and the requests will now be switched to your own API keys:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uhh9cz6rblle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af273496eac9670721783043a96d3e2272acfce6"&gt;https://preview.redd.it/uhh9cz6rblle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af273496eac9670721783043a96d3e2272acfce6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;After gathering extensive user feedback about the double shift functionality on both sides of the keyboard, we learned that many users were accidentally triggering these commands, causing inconvenience. We've addressed this issue by adding customization options in the settings menu. You can now personalize both the Widget Activation Key (right double shift by default) and the Context Capture Key (left double shift by default) to better suit your specific workflow preferences.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nyk4z2d1clle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac994f4d5a315d210c3b2a2ff8c9547e358f3584"&gt;https://preview.redd.it/nyk4z2d1clle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac994f4d5a315d210c3b2a2ff8c9547e358f3584&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3cbf9q8iclle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01e9d5157e09c24b9cdb908a08c2d43820984883"&gt;https://preview.redd.it/3cbf9q8iclle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01e9d5157e09c24b9cdb908a08c2d43820984883&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. To dismiss the Shift Widget originally you had to do it with ESC only, now you can go to quick dismiss shortcut and turn it on, this way you can appear/disappear the widget with the same shortcut (which is by default right double shift)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/frgxx52zclle1.png?width=3080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f906049779af38f6480e12a5a3e2c48512bd288a"&gt;https://preview.redd.it/frgxx52zclle1.png?width=3080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f906049779af38f6480e12a5a3e2c48512bd288a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ofkotfp9dlle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=784877e9ee928ec0dca0aa4ac463f4c2232eed5c"&gt;https://preview.redd.it/ofkotfp9dlle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=784877e9ee928ec0dca0aa4ac463f4c2232eed5c&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;A lot of users have very specialized long prompts with documents, so we decided to create a hub for all the prompts where you can manage and save them introducing library, library prompts can be used in shortcut section so now you don't have to copy paste your prompts and move them around a lot. You can also add up to 8 documents for each prompt&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/91m4l2zldlle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7d6d2b72dd5af9ad903d068b336a19c971f2c0da"&gt;https://preview.redd.it/91m4l2zldlle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7d6d2b72dd5af9ad903d068b336a19c971f2c0da&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q27tr6hrdlle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7c1e08742d97b19144a232fb829a7ea79df8cd34"&gt;https://preview.redd.it/q27tr6hrdlle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7c1e08742d97b19144a232fb829a7ea79df8cd34&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7y2ouckzdlle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1606db06f99ff786086e2f656351ccffc082d12e"&gt;https://preview.redd.it/7y2ouckzdlle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1606db06f99ff786086e2f656351ccffc082d12e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fosqsq51elle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=58cdf122f56607b5377d0bcfe7dabfdd6f4cf049"&gt;https://preview.redd.it/fosqsq51elle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=58cdf122f56607b5377d0bcfe7dabfdd6f4cf049&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And let's not forget our smooth and beautiful UI designs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g8v7d0jwelle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3102703897c80b0a8fc92cbb1f52018babbdffff"&gt;https://preview.redd.it/g8v7d0jwelle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3102703897c80b0a8fc92cbb1f52018babbdffff&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ll4uz1wzelle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df096b918302d10894259e0cf9cbbf1a480a094f"&gt;https://preview.redd.it/ll4uz1wzelle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df096b918302d10894259e0cf9cbbf1a480a094f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q0tsjgk2flle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fde65c08f8259d1503b2ff37a149e3bdbe33e360"&gt;https://preview.redd.it/q0tsjgk2flle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fde65c08f8259d1503b2ff37a149e3bdbe33e360&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;If you like to see Shift in action, watch out our most recent demo of shortcuts in Shift &lt;a href="https://youtu.be/GNHZ-mNgpCE?si=9rXi9sBEekamQlo8"&gt;here&lt;/a&gt;.&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;This shows we're truly listening and quick to respond implementing your suggestions within 24 hours in our updates. We genuinely value your input and are committed to perfecting Shift. Thanks to your support, we've welcomed 100 users in just our first week! We're incredibly grateful for your encouragement and kind feedback. We are your employees.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We're still evolving with major updates on the horizon. To learn about our upcoming significant features, please visit: &lt;a href="https://shiftappai.com/#whats-nexttps://shiftappai.com/#whats-next"&gt;https://shiftappai.com/#whats-nexttps://shiftappai.com/#whats-next&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you'd like to suggest features or improvements for our upcoming updates, just drop us a line at [&lt;a href="mailto:contact@shiftappai.com"&gt;contact@shiftappai.com&lt;/a&gt;](mailto:&lt;a href="mailto:contact@shiftappai.com"&gt;contact@shiftappai.com&lt;/a&gt;) or message us here. We'll make sure to implement your ideas quickly to match what you're looking for.&lt;/p&gt; &lt;p&gt;We have grown in over 100 users in less than a week! Thank you all for all this support :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ehsan1238"&gt; /u/Ehsan1238 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz5rhi/shift_update_more_customization_options_more_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz5rhi/shift_update_more_customization_options_more_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iz5rhi/shift_update_more_customization_options_more_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T02:53:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyt6jc</id>
    <title>Ran a 9GB model on a laptop with 8GB of RAM, and wondered why it was so slow</title>
    <updated>2025-02-26T17:36:31+00:00</updated>
    <author>
      <name>/u/akb74</name>
      <uri>https://old.reddit.com/user/akb74</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't if anyone needs to hear this, but happy for you to learn from my stupid mistakes!&lt;/p&gt; &lt;p&gt;This was deepseek-r1:14b and I fixed the slowness by switching to deepseek-r1:7b&lt;/p&gt; &lt;p&gt;Am I right in thinking that given a model which &lt;em&gt;comfortably&lt;/em&gt; fits in RAM, then the GPU becomes the main determinant of performance? And this should be true for all well implemented LLMs, not just DeepSeek?&lt;/p&gt; &lt;p&gt;Incidently, the 14b model (runnning well on a much better specced work laptop) wasn't able to diagnose the cause of this problem. I mean, it made plenty of sensible suggestions, but at no time said &amp;quot;try a smaller model&amp;quot; or anything like it.&lt;/p&gt; &lt;p&gt;Anyway, I'm just beginning and loving my ollama journey so far! Hope I've brought a smile to someone's face with my folly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akb74"&gt; /u/akb74 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyt6jc/ran_a_9gb_model_on_a_laptop_with_8gb_of_ram_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyt6jc/ran_a_9gb_model_on_a_laptop_with_8gb_of_ram_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iyt6jc/ran_a_9gb_model_on_a_laptop_with_8gb_of_ram_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-26T17:36:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz6rhk</id>
    <title>DataBridge Now Supports ColPali for Unprecedented Multi-Modal RAG! üéâ</title>
    <updated>2025-02-27T03:46:14+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/Rag/comments/1iz6qp0/databridge_now_supports_colpali_for_unprecedented/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz6rhk/databridge_now_supports_colpali_for_unprecedented/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iz6rhk/databridge_now_supports_colpali_for_unprecedented/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T03:46:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz4squ</id>
    <title>Tested 6 AI Models with 6 different types of questions, timed and rated the answers, is there a clear winner?</title>
    <updated>2025-02-27T02:04:05+00:00</updated>
    <author>
      <name>/u/tonyscha</name>
      <uri>https://old.reddit.com/user/tonyscha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure if anyone finds this useful but did some very elementary testing on 6 different models with 6 different questions then ranked them on my personal opinion of what I thought the best responses were and provided how long it took to provide responses in minutes/seconds.&lt;/p&gt; &lt;p&gt;Use the following models for my testing&lt;/p&gt; &lt;ul&gt; &lt;li&gt;olmo2:13b&lt;/li&gt; &lt;li&gt;olmo2:7b&lt;/li&gt; &lt;li&gt;deepseek-r1:7b&lt;/li&gt; &lt;li&gt;gemma:7bÔªø&lt;/li&gt; &lt;li&gt;llama3.2:latest 3.2BÔªø&lt;/li&gt; &lt;li&gt;llama3.1:8b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Asked the following questions&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tell me about Zion National Park&lt;/li&gt; &lt;li&gt;Create me a half marathon training plan&lt;/li&gt; &lt;li&gt;Tell me the ethical concerns related to AI&lt;/li&gt; &lt;li&gt;Outline the steps to prove that the sum of the interior angles of a triangle is 180 degrees.&lt;/li&gt; &lt;li&gt;If you could change anything about your algorithm, what would it be?&lt;/li&gt; &lt;li&gt;Rewrite this for better readability - the lazy yellow dog was caught by the slow red fox as he lay sleeping in the sun&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here was my conclusion:&lt;br /&gt; Overall both olmo models and the llama3.1 models won per my criteria more often then the other ones. While gemma:7b was the fastest on creating the half marathon training plan and prove the triangle interior angles is 180 degrees, I didn't like the responses as much as others. Deepseek seemed to always be middle of the pack. I am unsure if there is a clear winner, but I do feel like there is some clear losers, and I would included deepseek-r1:7b or gemma:7b in that category.&lt;/p&gt; &lt;p&gt;Note: I am running on a CPU only system, so the responses are going to be much slower then a GPU system.&lt;/p&gt; &lt;p&gt;Full write up here with times:&lt;/p&gt; &lt;p&gt;&lt;a href="https://akschaefer.com/2025/02/26/6-ai-models-6-tough-questions-1-clear-winner/"&gt;https://akschaefer.com/2025/02/26/6-ai-models-6-tough-questions-1-clear-winner/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tonyscha"&gt; /u/tonyscha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz4squ/tested_6_ai_models_with_6_different_types_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz4squ/tested_6_ai_models_with_6_different_types_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iz4squ/tested_6_ai_models_with_6_different_types_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T02:04:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyxpd7</id>
    <title>DeepSeek RAG Chatbot Reaches 650+ Stars üéâ - Celebrating Offline RAG Innovation</title>
    <updated>2025-02-26T20:41:55+00:00</updated>
    <author>
      <name>/u/akhilpanja</name>
      <uri>https://old.reddit.com/user/akhilpanja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm incredibly excited to share that &lt;strong&gt;DeepSeek RAG Chatbot&lt;/strong&gt; has officially hit &lt;strong&gt;650+ stars&lt;/strong&gt; on GitHub! This is a huge achievement, and I want to take a moment to celebrate this milestone and thank everyone who has contributed to the project in one way or another. Whether you‚Äôve provided feedback, used the tool, or just starred the repo, your support has made all the difference. (git: &lt;a href="https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot.git"&gt;https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot.git&lt;/a&gt; )&lt;/p&gt; &lt;h1&gt;What is DeepSeek RAG Chatbot?&lt;/h1&gt; &lt;p&gt;DeepSeek RAG Chatbot is a local, privacy-first solution for anyone who needs to quickly retrieve information from documents like PDFs, Word files, and text files. What sets it apart is that it runs &lt;strong&gt;100% offline&lt;/strong&gt;, ensuring that all your data remains private and never leaves your machine. It‚Äôs a tool built with privacy in mind, allowing you to search and retrieve answers from your own documents, without ever needing an internet connection.&lt;/p&gt; &lt;h1&gt;Key Features and Technical Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Offline &amp;amp; Private&lt;/strong&gt;: The chatbot works completely offline, ensuring your data stays private on your local machine.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Format Support&lt;/strong&gt;: DeepSeek can handle PDFs, Word documents, and text files, making it versatile for different types of content.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid Search&lt;/strong&gt;: We‚Äôve combined traditional keyword search with vector search to ensure we‚Äôre fetching the most relevant information from your documents. This dual approach maximizes the chances of finding the right answer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Knowledge Graph&lt;/strong&gt;: The chatbot uses a knowledge graph to better understand the relationships between different pieces of information in your documents, which leads to more accurate and contextual answers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross-Encoder Re-ranking&lt;/strong&gt;: After retrieving the relevant information, a re-ranking system is used to make sure that the most contextually relevant answers are selected.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Completely Open Source&lt;/strong&gt;: The project is fully open-source and free to use, which means you can contribute, modify, or use it however you need.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;A Big Thank You to the Community&lt;/h1&gt; &lt;p&gt;This project wouldn‚Äôt have reached 650+ stars without the incredible support of the community. I want to express my heartfelt thanks to everyone who has starred the repo, contributed code, reported bugs, or even just tried it out. Your support means the world, and I‚Äôm incredibly grateful for the feedback that has helped shape this project into what it is today.&lt;/p&gt; &lt;p&gt;This is just the beginning! DeepSeek RAG Chatbot will continue to grow, and I‚Äôm excited about what‚Äôs to come. If you‚Äôre interested in contributing, testing, or simply learning more, feel free to check out the GitHub page. Let‚Äôs keep making this tool better and better!&lt;/p&gt; &lt;p&gt;Thank you again to everyone who has been part of this journey. Here‚Äôs to more milestones ahead!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akhilpanja"&gt; /u/akhilpanja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyxpd7/deepseek_rag_chatbot_reaches_650_stars/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyxpd7/deepseek_rag_chatbot_reaches_650_stars/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iyxpd7/deepseek_rag_chatbot_reaches_650_stars/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-26T20:41:55+00:00</published>
  </entry>
</feed>
