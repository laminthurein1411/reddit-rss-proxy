<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-06-12T11:23:12+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1l805il</id>
    <title>THE best model ?</title>
    <updated>2025-06-10T14:28:15+00:00</updated>
    <author>
      <name>/u/Livid_Molasses_5824</name>
      <uri>https://old.reddit.com/user/Livid_Molasses_5824</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys for a RX7800XT &amp;amp; a ryzen5600x what's the perfect model ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Livid_Molasses_5824"&gt; /u/Livid_Molasses_5824 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l805il/the_best_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l805il/the_best_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l805il/the_best_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-10T14:28:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7rgkv</id>
    <title>running ollma on vsphere without GPU</title>
    <updated>2025-06-10T06:10:34+00:00</updated>
    <author>
      <name>/u/emaayan</name>
      <uri>https://old.reddit.com/user/emaayan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi , trying to run ollama with qwen 2.5 7b model on a vsphere , gave it a vm with os proton,128 gb memory about 16 cpus and that thing is still slow and unusable than my desktop i9900 with 64gb memory and 4060 16gb vram, &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emaayan"&gt; /u/emaayan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7rgkv/running_ollma_on_vsphere_without_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7rgkv/running_ollma_on_vsphere_without_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l7rgkv/running_ollma_on_vsphere_without_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-10T06:10:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7kc8k</id>
    <title>Multi-Config Switching UI</title>
    <updated>2025-06-09T23:49:31+00:00</updated>
    <author>
      <name>/u/PleasantCandidate785</name>
      <uri>https://old.reddit.com/user/PleasantCandidate785</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw a UI or UI for UIs mentioned in a thread earlier. It was called Multi-&amp;lt;something&amp;gt; but I can't remember what the something was.&lt;/p&gt; &lt;p&gt;As I remember it allowed sharing models between multiple backends like Ollama and ExllamaV2 and also switching UIs. &lt;/p&gt; &lt;p&gt;I've been googling off and on for it all day, but am coming up empty.&lt;/p&gt; &lt;p&gt;Anyone know what I'm talking about?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PleasantCandidate785"&gt; /u/PleasantCandidate785 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7kc8k/multiconfig_switching_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7kc8k/multiconfig_switching_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l7kc8k/multiconfig_switching_ui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-09T23:49:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6x83v</id>
    <title>Use Ollama to make agents watch your screen!</title>
    <updated>2025-06-09T05:56:48+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l6x83v/use_ollama_to_make_agents_watch_your_screen/"&gt; &lt;img alt="Use Ollama to make agents watch your screen!" src="https://external-preview.redd.it/NHZ4YnNpcXZjdTVmMZ0cZOsTXi-ThTayE7iEfGGYXF4Z17hX-7dpetBO2beo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a3632ea76a07d26b7e0ad2747798f130b121668" title="Use Ollama to make agents watch your screen!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zbl1cgqvcu5f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l6x83v/use_ollama_to_make_agents_watch_your_screen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l6x83v/use_ollama_to_make_agents_watch_your_screen/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-09T05:56:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7fhb5</id>
    <title>best option for personal private and local RAG with Ollama ?</title>
    <updated>2025-06-09T20:25:33+00:00</updated>
    <author>
      <name>/u/LivingSignificant452</name>
      <uri>https://old.reddit.com/user/LivingSignificant452</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; I would like to set up a private , local notebooklm alternative. Using documents I prepare in PDF mainly ( up to 50 very long document 500pages each ). Also !! I need it to work correctly with french language.&lt;br /&gt; for the hardward part, I have a RTX 3090, so I can choose any ollama model working with up to 24Mb of vram.&lt;/p&gt; &lt;p&gt;I have &lt;strong&gt;openwebui&lt;/strong&gt;, and started to make some test with the integrated document feature, but for the option or improve it, it's difficult to understand the impact of each option&lt;/p&gt; &lt;p&gt;I have tested briefly &lt;strong&gt;PageAssist&lt;/strong&gt; in chrome, but honestly, it's like it doesn't work, despite I followed a youtube tutorial.&lt;/p&gt; &lt;p&gt;is there anything else I should try ? I saw a mention to LightRag ?&lt;br /&gt; as things are moving so fast, it's hard to know where to start, and even when it works, you don't know if you are not missing an option or a tip. thanks by advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LivingSignificant452"&gt; /u/LivingSignificant452 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7fhb5/best_option_for_personal_private_and_local_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7fhb5/best_option_for_personal_private_and_local_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l7fhb5/best_option_for_personal_private_and_local_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-09T20:25:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7orop</id>
    <title>How to Install Open WebUI with Bundled Ollama Support</title>
    <updated>2025-06-10T03:30:51+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l7orop/how_to_install_open_webui_with_bundled_ollama/"&gt; &lt;img alt="How to Install Open WebUI with Bundled Ollama Support" src="https://external-preview.redd.it/WjowGLQbVnk9UeHmNSsY5EBOABBmLH2XCrrr8yydG4o.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8573656176199f099b9ec629705960b89fdad66b" title="How to Install Open WebUI with Bundled Ollama Support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/6oOVZEU_36c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7orop/how_to_install_open_webui_with_bundled_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l7orop/how_to_install_open_webui_with_bundled_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-10T03:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7jzqk</id>
    <title>Built coexistAI, building blocks for your own deep research at scale</title>
    <updated>2025-06-09T23:33:43+00:00</updated>
    <author>
      <name>/u/Optimalutopic</name>
      <uri>https://old.reddit.com/user/Optimalutopic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/SPThole/CoexistAI"&gt;https://github.com/SPThole/CoexistAI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi all! I‚Äôm excited to share CoexistAI, a modular open-source framework designed to help you streamline and automate your research workflows‚Äîright on your own machine. &lt;/p&gt; &lt;h3&gt;What is CoexistAI?&lt;/h3&gt; &lt;p&gt;CoexistAI brings together web, YouTube, and Reddit search, flexible summarization, and geospatial analysis‚Äîall powered by LLMs and embedders you choose (local or cloud). It‚Äôs built for researchers, students, and anyone who wants to organize, analyze, and summarize information efficiently. &lt;/p&gt; &lt;h3&gt;Key Features&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Open-source and modular:&lt;/strong&gt; Fully open-source and designed for easy customization. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-LLM and embedder support:&lt;/strong&gt; Connect with various LLMs and embedding models, including local and cloud providers (OpenAI, Google, Ollama, and more coming soon). &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unified search:&lt;/strong&gt; Perform web, YouTube, and Reddit searches directly from the framework. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Notebook and API integration:&lt;/strong&gt; Use CoexistAI seamlessly in Jupyter notebooks or via FastAPI endpoints. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible summarization:&lt;/strong&gt; Summarize content from web pages, YouTube videos, and Reddit threads by simply providing a link. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM-powered at every step:&lt;/strong&gt; Language models are integrated throughout the workflow for enhanced automation and insights. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local model compatibility:&lt;/strong&gt; Easily connect to and use local LLMs for privacy and control. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Modular tools:&lt;/strong&gt; Use each feature independently or combine them to build your own research assistant. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Geospatial capabilities:&lt;/strong&gt; Generate and analyze maps, with more enhancements planned. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;On-the-fly RAG:&lt;/strong&gt; Instantly perform Retrieval-Augmented Generation (RAG) on web content. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deploy on your own PC or server:&lt;/strong&gt; Set up once and use across your devices at home or work. &lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;How you might use it&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Research any topic by searching, aggregating, and summarizing from multiple sources &lt;/li&gt; &lt;li&gt;Summarize and compare papers, videos, and forum discussions &lt;/li&gt; &lt;li&gt;Build your own research assistant for any task &lt;/li&gt; &lt;li&gt;Use geospatial tools for location-based research or mapping projects &lt;/li&gt; &lt;li&gt;Automate repetitive research tasks with notebooks or API calls &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt; CoexistAI on GitHub&lt;/p&gt; &lt;p&gt;&lt;em&gt;Free for non-commercial research &amp;amp; educational use.&lt;/em&gt; &lt;/p&gt; &lt;p&gt;Would love feedback from anyone interested in local-first, modular research tools! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Optimalutopic"&gt; /u/Optimalutopic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7jzqk/built_coexistai_building_blocks_for_your_own_deep/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7jzqk/built_coexistai_building_blocks_for_your_own_deep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l7jzqk/built_coexistai_building_blocks_for_your_own_deep/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-09T23:33:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1l88vdj</id>
    <title>Instant shutdown and restart when using deepseek-r1:70b</title>
    <updated>2025-06-10T20:06:05+00:00</updated>
    <author>
      <name>/u/Ok_Musician_4872</name>
      <uri>https://old.reddit.com/user/Ok_Musician_4872</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have ollama version is 0.9.0, I tried to play with a few different models, everything works correctly. But when I'm trying to use deepseek-r1:70b it behaves very strangely. I've managed to load the model from cmd line, and enter simple prompt. It worked slowly, but worked. But every time when I'm trying to use it with bigger prompt through API, my PC shutdowns completely (LEDs are off, HDD stops, fans stops), and then after 2-3 seconds it boots normally. Anyone had something like that? What can be the reason? It happens almost immediately when I hit enter...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Musician_4872"&gt; /u/Ok_Musician_4872 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l88vdj/instant_shutdown_and_restart_when_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l88vdj/instant_shutdown_and_restart_when_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l88vdj/instant_shutdown_and_restart_when_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-10T20:06:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1l82z0y</id>
    <title>What‚Äôs the Best Method to Determine Cable Length from a Scaled PDF Drawing?</title>
    <updated>2025-06-10T16:19:11+00:00</updated>
    <author>
      <name>/u/ElegantSherbet3945</name>
      <uri>https://old.reddit.com/user/ElegantSherbet3945</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l82z0y/whats_the_best_method_to_determine_cable_length/"&gt; &lt;img alt="What‚Äôs the Best Method to Determine Cable Length from a Scaled PDF Drawing?" src="https://b.thumbs.redditmedia.com/5ABubQUjQPHOgMUXmVPEqHhcG39CIgyGFVp4ci0Zc-U.jpg" title="What‚Äôs the Best Method to Determine Cable Length from a Scaled PDF Drawing?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a working drawing that was created in AutoCAD and exported as a PDF. The drawing includes a legend and, as shown in the screenshot, a line marked from point A to point B. This line, represented by a purple dotted line, indicates the path of a cable.&lt;/p&gt; &lt;p&gt;Using the scale provided in the drawing, I want to calculate the total length of cable needed to run from point A to point B.&lt;/p&gt; &lt;p&gt;What method or model can I use to determine this?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u6m5vekok46f1.png?width=555&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa096cdd15e93a4f23b6875edeb3ade91e052b2b"&gt;https://preview.redd.it/u6m5vekok46f1.png?width=555&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa096cdd15e93a4f23b6875edeb3ade91e052b2b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElegantSherbet3945"&gt; /u/ElegantSherbet3945 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l82z0y/whats_the_best_method_to_determine_cable_length/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l82z0y/whats_the_best_method_to_determine_cable_length/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l82z0y/whats_the_best_method_to_determine_cable_length/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-10T16:19:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8nz26</id>
    <title>Thank you very much for the harmony of beautiful moments</title>
    <updated>2025-06-11T09:10:05+00:00</updated>
    <author>
      <name>/u/Electronic_Hat_7519</name>
      <uri>https://old.reddit.com/user/Electronic_Hat_7519</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l8nz26/thank_you_very_much_for_the_harmony_of_beautiful/"&gt; &lt;img alt="Thank you very much for the harmony of beautiful moments" src="https://external-preview.redd.it/fDZYiI4uAZFL0TXT29rrw-VG9UeFD332Z2q_PtKtCx4.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07c7662b37cffdf535f0e7d0c7ab4be755af2306" title="Thank you very much for the harmony of beautiful moments" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic_Hat_7519"&gt; /u/Electronic_Hat_7519 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://suno.com/s/tXFtMXgWrRV947iu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8nz26/thank_you_very_much_for_the_harmony_of_beautiful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l8nz26/thank_you_very_much_for_the_harmony_of_beautiful/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T09:10:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8gbzq</id>
    <title>GPU ollama docker</title>
    <updated>2025-06-11T01:33:01+00:00</updated>
    <author>
      <name>/u/Informal_Catch_4688</name>
      <uri>https://old.reddit.com/user/Informal_Catch_4688</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm currently using ollama through WLS for my assistant on windows what I noticed is that it only uses 28% of my GPU but the reply from questions take long time 15secods how can I speed it up ? I was using llama.cpp before that and it was taking around 1-4 seconds to generate answer , I could not use llama.cpp because of hallucinations assistant would day the prompt my question and answer and hashtags etc &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Informal_Catch_4688"&gt; /u/Informal_Catch_4688 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8gbzq/gpu_ollama_docker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8gbzq/gpu_ollama_docker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l8gbzq/gpu_ollama_docker/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T01:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8vtdj</id>
    <title>Name the Llm that can do this</title>
    <updated>2025-06-11T15:35:55+00:00</updated>
    <author>
      <name>/u/matthewstevensdotorg</name>
      <uri>https://old.reddit.com/user/matthewstevensdotorg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Write a strictly rhyming poem where the words increase in syllable length according to ANY segment of the Fibonacci sequence&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matthewstevensdotorg"&gt; /u/matthewstevensdotorg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8vtdj/name_the_llm_that_can_do_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8vtdj/name_the_llm_that_can_do_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l8vtdj/name_the_llm_that_can_do_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T15:35:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1l85fh8</id>
    <title>Ollama Frontend/GUI</title>
    <updated>2025-06-10T17:53:30+00:00</updated>
    <author>
      <name>/u/Ok_Most9659</name>
      <uri>https://old.reddit.com/user/Ok_Most9659</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for an Ollama frontend/GUI. Preferably can be used offline, is private, works in Linux, and open source.&lt;br /&gt; Any recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Most9659"&gt; /u/Ok_Most9659 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l85fh8/ollama_frontendgui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l85fh8/ollama_frontendgui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l85fh8/ollama_frontendgui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-10T17:53:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8pyb4</id>
    <title>Are we supposed to always wrap content text with special tokens?</title>
    <updated>2025-06-11T11:14:43+00:00</updated>
    <author>
      <name>/u/SeaworthinessLeft160</name>
      <uri>https://old.reddit.com/user/SeaworthinessLeft160</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using Ollama and Pydantic for my structured output. It's pretty bare bones. However, in my system message content, the text lacks special tokens; the user role content is the same.&lt;/p&gt; &lt;p&gt;I've seen tutorials in video and article formats, and sometimes authors use special tokens, sometimes not.&lt;/p&gt; &lt;p&gt;Is it that the framework they use already creates the special tokens to wrap the text, specific to the model being used? If I use Ollama and Pydantic, am I supposed to manually add those special tokens?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SeaworthinessLeft160"&gt; /u/SeaworthinessLeft160 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8pyb4/are_we_supposed_to_always_wrap_content_text_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8pyb4/are_we_supposed_to_always_wrap_content_text_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l8pyb4/are_we_supposed_to_always_wrap_content_text_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T11:14:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1l92dox</id>
    <title>üéôÔ∏è Looking for Beta Testers ‚Äì Get 24 Hours of Free TTS Audio</title>
    <updated>2025-06-11T19:52:48+00:00</updated>
    <author>
      <name>/u/mythicinfinity</name>
      <uri>https://old.reddit.com/user/mythicinfinity</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm launching a new TTS (text-to-speech) service and I'm looking for a few early users to help test it out. If you're into AI voices, audio content, or just want to convert a lot of text to audio, this is a great chance to try it for free.&lt;/p&gt; &lt;p&gt;‚úÖ Beta testers get &lt;strong&gt;24 hours of audio generation&lt;/strong&gt; (no strings attached)&lt;br /&gt; ‚úÖ Supports multiple voices and formats&lt;br /&gt; ‚úÖ Ideal for podcasts, audiobooks, screenreaders, etc.&lt;/p&gt; &lt;p&gt;If you're interested, &lt;strong&gt;DM me&lt;/strong&gt; and I'll get you set up with access. Feedback is optional but appreciated!&lt;/p&gt; &lt;p&gt;Thanks! üôå&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mythicinfinity"&gt; /u/mythicinfinity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l92dox/looking_for_beta_testers_get_24_hours_of_free_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l92dox/looking_for_beta_testers_get_24_hours_of_free_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l92dox/looking_for_beta_testers_get_24_hours_of_free_tts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T19:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8tyzb</id>
    <title>giving deepseek R1 a new chance, model-choice, gguf import</title>
    <updated>2025-06-11T14:22:17+00:00</updated>
    <author>
      <name>/u/Impossible_Art9151</name>
      <uri>https://old.reddit.com/user/Impossible_Art9151</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;hopefully someone can give me a few hints.&lt;br /&gt; I once tested deepseek r1:70b when released. But I was fine with qwen2.5 and llama3.3 and deleted deepseek after a while.&lt;/p&gt; &lt;p&gt;I would like to give it a new chance. I own a Dual AMD workstation with 320GB RAM and a nvidia A6000 - 48GB VRAM&lt;br /&gt; Further I am using ubuntu, ollama (non-docker) and openwebui (non-docker).&lt;/p&gt; &lt;p&gt;I want to test highest quality, not on speed!&lt;br /&gt; Any quant recommendations for my hardware? unsloth, bartowski?&lt;br /&gt; Does for example run a hf.co/unsloth/DeepSeek-R1-0528-GGUF:Q3_K_S in my setup? Since I haven't used hf-gguf for a long time, can someone provide a step-by-step description, tutorial?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impossible_Art9151"&gt; /u/Impossible_Art9151 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8tyzb/giving_deepseek_r1_a_new_chance_modelchoice_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8tyzb/giving_deepseek_r1_a_new_chance_modelchoice_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l8tyzb/giving_deepseek_r1_a_new_chance_modelchoice_gguf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T14:22:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8r68f</id>
    <title>Ollama not releasing VRAM after running a model</title>
    <updated>2025-06-11T12:18:24+00:00</updated>
    <author>
      <name>/u/Siderox</name>
      <uri>https://old.reddit.com/user/Siderox</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been using Ollama (without Docker) to run a few models (mainly Gemma3:12b) for a couple months and noticed that it often does not release VRAM after it runs the model. For example, the VRAM usage will be at, say, 0.5GB before running the model, then 5.5GB while running, then remaining at 5.5GB. If you run the model again the usage will drop back down to 0.5GB for a second then back up to 5.5GB, suggesting it only clears the memory right before reloading the model. Seems to work that way regardless of whether I‚Äôm using the model on vanilla settings in powershell or on customised settings in OpenWebUI. Culling Ollama will bring GPU usage back to baseline, though, so it‚Äôs not a fatal issue, just a bit odd. Anyone else had this issue? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Siderox"&gt; /u/Siderox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8r68f/ollama_not_releasing_vram_after_running_a_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8r68f/ollama_not_releasing_vram_after_running_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l8r68f/ollama_not_releasing_vram_after_running_a_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T12:18:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1l912g0</id>
    <title>Local LLM and Agentic Use Cases?</title>
    <updated>2025-06-11T19:01:05+00:00</updated>
    <author>
      <name>/u/Ok_Most9659</name>
      <uri>https://old.reddit.com/user/Ok_Most9659</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do the smaller distilled and quantized models have capability for agentic use cases given their limits?&lt;br /&gt; If so, what are some of the use cases you are employing your local AI for and model are you using (including parameter/bits)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Most9659"&gt; /u/Ok_Most9659 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l912g0/local_llm_and_agentic_use_cases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l912g0/local_llm_and_agentic_use_cases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l912g0/local_llm_and_agentic_use_cases/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T19:01:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1l90c7r</id>
    <title>i made a commit message generator that can be used offline and for free</title>
    <updated>2025-06-11T18:32:30+00:00</updated>
    <author>
      <name>/u/mehmetflix_</name>
      <uri>https://old.reddit.com/user/mehmetflix_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i made a commit message generator by finetuning qwen2.5 coder 7b instruct, it is quantized to 8bits so it has a 8.1gb size. if anyone wants to try it here is the link &lt;a href="https://pypi.org/project/ezcmt/"&gt;https://pypi.org/project/ezcmt/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;if you try it out tell me if theres anything that can be added or a bug that can be fixed&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehmetflix_"&gt; /u/mehmetflix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l90c7r/i_made_a_commit_message_generator_that_can_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l90c7r/i_made_a_commit_message_generator_that_can_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l90c7r/i_made_a_commit_message_generator_that_can_be/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T18:32:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9d3h3</id>
    <title>chat with mysql using ollama</title>
    <updated>2025-06-12T04:08:00+00:00</updated>
    <author>
      <name>/u/Specialist_Figure_31</name>
      <uri>https://old.reddit.com/user/Specialist_Figure_31</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there any open source github that can be used to chat with my mysql &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist_Figure_31"&gt; /u/Specialist_Figure_31 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9d3h3/chat_with_mysql_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9d3h3/chat_with_mysql_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l9d3h3/chat_with_mysql_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-12T04:08:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1l95vfw</id>
    <title>Keeping Ollama chats persistent (Docker, Web UI)</title>
    <updated>2025-06-11T22:14:52+00:00</updated>
    <author>
      <name>/u/redpandafire</name>
      <uri>https://old.reddit.com/user/redpandafire</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New. Able to install and launch a container of Ollama running gemma3. It works, great. Shut down the computer. Everything is gone. Starting an image creates a brand new container. Unable to launch previous containers, it gets stuck on downloading 30/30 files. I believe the command is:&lt;/p&gt; &lt;p&gt;Docker ps -a Docker start (container id) [options]&lt;/p&gt; &lt;p&gt;Everytime I do this, Docker runs in command interface a bunch of lines and gets stuck downloading files 30/30.&lt;/p&gt; &lt;p&gt;TL;DR I just want to stop and start a specific container, that I believe, contains all my work and chats.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redpandafire"&gt; /u/redpandafire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l95vfw/keeping_ollama_chats_persistent_docker_web_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l95vfw/keeping_ollama_chats_persistent_docker_web_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l95vfw/keeping_ollama_chats_persistent_docker_web_ui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T22:14:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9bxf2</id>
    <title>What is the best model to help with writing?</title>
    <updated>2025-06-12T03:04:49+00:00</updated>
    <author>
      <name>/u/VajraXL</name>
      <uri>https://old.reddit.com/user/VajraXL</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What model would you recommend as a writing assistant for a writer who is not a native English speaker and needs help with grammar and style corrections, and perhaps suggestions for alternative phrasing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VajraXL"&gt; /u/VajraXL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9bxf2/what_is_the_best_model_to_help_with_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9bxf2/what_is_the_best_model_to_help_with_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l9bxf2/what_is_the_best_model_to_help_with_writing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-12T03:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1l906f4</id>
    <title>Why use docker with ollama and Open WebuI?</title>
    <updated>2025-06-11T18:26:13+00:00</updated>
    <author>
      <name>/u/Ok_Most9659</name>
      <uri>https://old.reddit.com/user/Ok_Most9659</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have seen people recommend using Docker with Ollama and Open WebUI. I am not a programmer and new to local LLM, but my understanding is that its to ensure both programs run well on your system as it avoids potential local environment issues your system may have that could impede running Ollama or Open Webui. I have installed Ollama directly from their website without Docker and it runs without issue on my system. I have yet to download Open Webui and debating on downloading Docker first. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Is ensuring the program will run on any system the sole reason to run Ollama and Open WebUI through Docker container?&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Are there any benefits to running a program in a container for security or privacy?&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Any benefits to GPU efficiency for running a program in a container?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Most9659"&gt; /u/Ok_Most9659 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l906f4/why_use_docker_with_ollama_and_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l906f4/why_use_docker_with_ollama_and_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l906f4/why_use_docker_with_ollama_and_open_webui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T18:26:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9gvu5</id>
    <title>Run Ollama in your documents with Writeopia. Windows app now available!</title>
    <updated>2025-06-12T08:05:54+00:00</updated>
    <author>
      <name>/u/lehen01</name>
      <uri>https://old.reddit.com/user/lehen01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l9gvu5/run_ollama_in_your_documents_with_writeopia/"&gt; &lt;img alt="Run Ollama in your documents with Writeopia. Windows app now available!" src="https://external-preview.redd.it/dTdpM2pnNGhlZzZmMVhZcvYgeFNt2CRk4Y47xoCdZZ4Xui688StqpxFWpMYy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e9e4f38f67b08aa746c08a2a69827378720736a" title="Run Ollama in your documents with Writeopia. Windows app now available!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello hello. &lt;/p&gt; &lt;p&gt;Sometime ago, I shared my project Writeopia in &lt;a href="https://www.reddit.com/r/ollama/comments/1jdb4zm/i_created_a_text_editor_that_integrates_with/"&gt;this post&lt;/a&gt; and it had a super nice reception. Many users asked about the Windows app, because at that time, only macOS and Linux were available. &lt;/p&gt; &lt;p&gt;We are happy to announce that the Windows app is finally available. You can download it from the &lt;a href="https://apps.microsoft.com/detail/9NW5WL8NRM4H?hl=en-us&amp;amp;gl=NL&amp;amp;ocid=pdpshare"&gt;Windows Store&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;If you like the project, don't forget to star us on Github: &lt;a href="https://github.com/Writeopia/Writeopia"&gt;https://github.com/Writeopia/Writeopia&lt;/a&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lehen01"&gt; /u/lehen01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1i0njg4heg6f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9gvu5/run_ollama_in_your_documents_with_writeopia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l9gvu5/run_ollama_in_your_documents_with_writeopia/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-12T08:05:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8otdn</id>
    <title>Finally ChatGPT did it!!</title>
    <updated>2025-06-11T10:06:06+00:00</updated>
    <author>
      <name>/u/theMonarch776</name>
      <uri>https://old.reddit.com/user/theMonarch776</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l8otdn/finally_chatgpt_did_it/"&gt; &lt;img alt="Finally ChatGPT did it!!" src="https://preview.redd.it/tc3mi0b5v96f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40cf4649c9497de9ae34f27c933a9d28b7018f53" title="Finally ChatGPT did it!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;finally it told there are 3 'r's in Strawberry&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theMonarch776"&gt; /u/theMonarch776 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tc3mi0b5v96f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8otdn/finally_chatgpt_did_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l8otdn/finally_chatgpt_did_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T10:06:06+00:00</published>
  </entry>
</feed>
