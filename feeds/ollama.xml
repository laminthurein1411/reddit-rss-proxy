<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-22T13:08:17+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jfb2s1</id>
    <title>What are the uses for small models ( below 7b )?</title>
    <updated>2025-03-19T23:28:10+00:00</updated>
    <author>
      <name>/u/digitalextremist</name>
      <uri>https://old.reddit.com/user/digitalextremist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Right now I have ~500gb of models and I am trying to find a way to prioritize them and shed some. Seems like it would be wise to have a ~1TB M.2 drive just for LLMs, going forward. But while I feel the squeeze...&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Understanding that anything under &lt;code&gt;32b&lt;/code&gt; is the actual small point ( perhaps &lt;code&gt;70b&lt;/code&gt; ) ... what are the real uses for the teeny tiny models?&lt;/p&gt; &lt;p&gt;My go-to use-cases are code ( non-vibe-coding ) tending toward local-only multi-model agentic soon, document review and analysis in various ways, devops tending toward VPC management by MCP soon, and general information analysis and knowledge-base development. Most of the time I see myself headed into tool-dependence also, but am not there yet.&lt;/p&gt; &lt;p&gt;How do small models fit there? And where do the mid-range models fit? It seems like if something can be thrown at a &lt;code&gt;&amp;gt;8b&lt;/code&gt; model for the same time-cost, why not?&lt;/p&gt; &lt;p&gt;The only real use I can see right now is for conversation for its own sake, for example in the case of psychological interventions where an LLM can supplement companionship, once properly prepared not to death-spiral if someone is in a compromised mental state and will be for years at a time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/digitalextremist"&gt; /u/digitalextremist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfb2s1/what_are_the_uses_for_small_models_below_7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfb2s1/what_are_the_uses_for_small_models_below_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jfb2s1/what_are_the_uses_for_small_models_below_7b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-19T23:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jft4n6</id>
    <title>What model can I run locally on old server - Xeon 2.1 GHz x 2, 96 GB ECC ram, Nvidia Grid K2</title>
    <updated>2025-03-20T16:37:01+00:00</updated>
    <author>
      <name>/u/Chintan124</name>
      <uri>https://old.reddit.com/user/Chintan124</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have this 5-6 years old server which I‚Äôm not using anymore. It has Intel Xeon 2.1 GHz Octacore x 2 processors, 96 GB ECC ram, Nvidia Grid K2 graphics card. Can I run a 70b model locally on this with usable tokens/second output? Potentially custom trained? If yes, can anyone tell me which version of Linux can I use which would detect the Grid K2 for use? How to train models on custom data?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chintan124"&gt; /u/Chintan124 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jft4n6/what_model_can_i_run_locally_on_old_server_xeon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jft4n6/what_model_can_i_run_locally_on_old_server_xeon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jft4n6/what_model_can_i_run_locally_on_old_server_xeon/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-20T16:37:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfwsfh</id>
    <title>How to send images to vision models via http request</title>
    <updated>2025-03-20T19:08:12+00:00</updated>
    <author>
      <name>/u/s3bastienb</name>
      <uri>https://old.reddit.com/user/s3bastienb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, it is really possible to send images as base64 to ollama via openai style api calls? i keep hitting token limits and if i resize the image down more or compress it the llm's can't identify the images. I feel like i'm doing something wrong.&lt;/p&gt; &lt;p&gt;What i'm currently doing is taking an image and resizing it down to 500x500 then converting that to base64 then including it in my message under the image section as shown in the docs on github. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/s3bastienb"&gt; /u/s3bastienb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfwsfh/how_to_send_images_to_vision_models_via_http/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfwsfh/how_to_send_images_to_vision_models_via_http/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jfwsfh/how_to_send_images_to_vision_models_via_http/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-20T19:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfwtop</id>
    <title>How to pass data to ollama python app</title>
    <updated>2025-03-20T19:09:40+00:00</updated>
    <author>
      <name>/u/PankajRepswal</name>
      <uri>https://old.reddit.com/user/PankajRepswal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jfwtop/how_to_pass_data_to_ollama_python_app/"&gt; &lt;img alt="How to pass data to ollama python app" src="https://b.thumbs.redditmedia.com/-nmgY5imUKtV3Ix8gECBWTXlq3Hj6bHPDnytzvxpizs.jpg" title="How to pass data to ollama python app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/26j7arku6wpe1.png?width=1676&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a57fcf584420f7276bc2c459fa7fac8eaad8f126"&gt;https://preview.redd.it/26j7arku6wpe1.png?width=1676&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a57fcf584420f7276bc2c459fa7fac8eaad8f126&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a part of my code in which the first function returns the scraped data, and second function returns the assistant response. I want to pass the 'scraped_data' to the LLM as the default/system prompt, but whenever I ask the assistant questions related to the data then its response is that I don't have any data.&lt;br /&gt; How to fix it?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from wow import * import ollama import icecream as ic from tiktoken import encoding_for_model import streamlit as st @st.cache_data def data_input(): results = scrape_scholarships() data = main(results) if isinstance(data, list): data = data # print(data) print(f&amp;quot;Data length: {len(data)}&amp;quot;) # print(f&amp;quot;First 100 characters: {data[:100]}&amp;quot;) encoder = encoding_for_model(&amp;quot;gpt-4o&amp;quot;) tokens = encoder.encode(data) print(f&amp;quot;Number of tokens: {len(tokens)}&amp;quot;) print(&amp;quot;Type of data: &amp;quot;, type(data)) return data scraped_data = data_input() if &amp;quot;messages&amp;quot; not in st.session_state: st.session_state[&amp;quot;messages&amp;quot;] = [ { &amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: f&amp;quot;You are given some data and you have to only analyze the data correctly if the user asks for any output then give the output as per the data and user's question otherwise don't give answer. Here is the data: \n\n{scraped_data}&amp;quot; }] def chat_with_data(): try: ollama_response = ollama.chat(model='llama3.2', messages=st.session_state[&amp;quot;messages&amp;quot;], stream=False) # ic.ic(ollama_response) assistant_message = ollama_response['message']['content'] return assistant_message except Exception as e: ic(e) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is the info related to the data:&lt;br /&gt; Data length: 177754&lt;/p&gt; &lt;p&gt;Number of tokens: 34812&lt;/p&gt; &lt;p&gt;Type of data: &amp;lt;class 'str'&amp;gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PankajRepswal"&gt; /u/PankajRepswal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfwtop/how_to_pass_data_to_ollama_python_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfwtop/how_to_pass_data_to_ollama_python_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jfwtop/how_to_pass_data_to_ollama_python_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-20T19:09:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfy45v</id>
    <title>Need some help integrating MCP with LLM apis</title>
    <updated>2025-03-20T20:03:09+00:00</updated>
    <author>
      <name>/u/Street_Climate_9890</name>
      <uri>https://old.reddit.com/user/Street_Climate_9890</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wish to intergrate the playwright mcp with my openai api or calude 3.5sonnet usage somehow.....&lt;br /&gt; Any guidance is highly appreciated.... i wish to make a solution for my mom and dad to help them easily order groceries from online platforms using simple instructions on their end and automate and save them with some kind of self healing nature...&lt;/p&gt; &lt;p&gt;Based on their day to day, i will update the required requirments and prompts flow for the mcp...&lt;/p&gt; &lt;p&gt;ANy blogs or tutorial links would be super useful too.&lt;/p&gt; &lt;h1&gt;Thanks a ton.&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Street_Climate_9890"&gt; /u/Street_Climate_9890 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfy45v/need_some_help_integrating_mcp_with_llm_apis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfy45v/need_some_help_integrating_mcp_with_llm_apis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jfy45v/need_some_help_integrating_mcp_with_llm_apis/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-20T20:03:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jflnxl</id>
    <title>Structured Outputs in Ollama - What's Your Recipe for Success?</title>
    <updated>2025-03-20T10:19:41+00:00</updated>
    <author>
      <name>/u/RMCPhoto</name>
      <uri>https://old.reddit.com/user/RMCPhoto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with Ollama's structured output feature (using JSON schemas via Pydantic models) and wanted to hear how others are implementing this in their projects. My results have been a bit mixed with Gemma3 and Phi4.&lt;/p&gt; &lt;p&gt;My goal has been information extraction from text.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Questions:&lt;/strong&gt; 1. &lt;strong&gt;Model Performance&lt;/strong&gt;: Which local models (e.g. llama3.1, mixtral, Gemma, phi) have you found most reliable for structured output generation? And for what use case? 2. &lt;strong&gt;Schema Design&lt;/strong&gt;: How are you leveraging Pydantic's field labels/descriptions in your JSON schemas? Are you including semantic descriptions to guide the model? 3. &lt;strong&gt;Prompt Engineering&lt;/strong&gt;: Do you explicitly restate the desired output structure in your prompts &lt;em&gt;in addition&lt;/em&gt; to passing the schema, or rely solely on the schema definition? 4. &lt;strong&gt;Validation Patterns&lt;/strong&gt;: What error handling strategies work best when parsing model responses?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discussion Points:&lt;/strong&gt; - Have you found certain schema structures (nested objects vs flat) work better? - Any clever uses of enums or constrained types? - How does structured output performance compare between models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RMCPhoto"&gt; /u/RMCPhoto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jflnxl/structured_outputs_in_ollama_whats_your_recipe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jflnxl/structured_outputs_in_ollama_whats_your_recipe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jflnxl/structured_outputs_in_ollama_whats_your_recipe/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-20T10:19:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfnspn</id>
    <title>Connect to your self-hosted LLMs. From anywhere.</title>
    <updated>2025-03-20T12:30:53+00:00</updated>
    <author>
      <name>/u/smilulilu</name>
      <uri>https://old.reddit.com/user/smilulilu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jfnspn/connect_to_your_selfhosted_llms_from_anywhere/"&gt; &lt;img alt="Connect to your self-hosted LLMs. From anywhere." src="https://external-preview.redd.it/w7-9BozDXx3jvu1jDs1gQZsTB8GxOXeHgT1T6uUjNfk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e744cb352536eba2767bc6bd474ad0d58dccfceb" title="Connect to your self-hosted LLMs. From anywhere." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to share a small hobby project of mine which I have been building for a couple of months now. I'm looking for some early development test users for some feedback. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Project name&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Reititin&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Reititin connects to your self-hosted LLMs seamlessly. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You create a new agent from Reititin UI and run a simple script on your LLM host machine that connects your Reititin account to your self-hosted LLM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it's built&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;To allow ease of access to self-hosted LLMs and agents from anywhere. No need for custom VPCs, Tunnels, Proxys, and SSH stuff.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who it's for&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Reititin is built for people who want to self-host their LLMs and are looking for a simple way to connect to their LLMs from anywhere.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smilulilu"&gt; /u/smilulilu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://test.reititin.com"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfnspn/connect_to_your_selfhosted_llms_from_anywhere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jfnspn/connect_to_your_selfhosted_llms_from_anywhere/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-20T12:30:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfjose</id>
    <title>Where‚Äôs Mistral Small 3.1?</title>
    <updated>2025-03-20T07:48:33+00:00</updated>
    <author>
      <name>/u/tjevns</name>
      <uri>https://old.reddit.com/user/tjevns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm surprised to see that there‚Äôs still no sign of Mistral Small 3.1 available from Ollama. New open models usually have usually appeared by now from official model release. It‚Äôs been a couple of days now. Any ideas why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tjevns"&gt; /u/tjevns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfjose/wheres_mistral_small_31/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfjose/wheres_mistral_small_31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jfjose/wheres_mistral_small_31/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-20T07:48:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg5c99</id>
    <title>CodeGPT Autocomplete Issues with Ollama</title>
    <updated>2025-03-21T01:25:34+00:00</updated>
    <author>
      <name>/u/ReverendRocky</name>
      <uri>https://old.reddit.com/user/ReverendRocky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I'm running Ollama on a linux machine running the deepseek-coder:base model. I'm trying to set it up with CodeGPT to do autocomplete but each request is logged in Ollama as a 500 error with the following issue: &lt;/p&gt; &lt;p&gt;[GIN] 2025/03/20 - 21:22:35 | 500 | 635.767461ms | &lt;a href="http://127.0.0.1"&gt;127.0.0.1&lt;/a&gt; | POST &amp;quot;/api/generate&amp;quot;&lt;br /&gt; time=2025-03-20T21:22:35.416-04:00 level=INFO source=runner.go:600 msg=&amp;quot;aborting completion request due to client closing the connection&amp;quot; &lt;/p&gt; &lt;p&gt;I'm relatively new to this though have not been able to find many talking about this issue and I wonder if anyone might be able to shed some light or point me in the right direction ^_^&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ReverendRocky"&gt; /u/ReverendRocky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jg5c99/codegpt_autocomplete_issues_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jg5c99/codegpt_autocomplete_issues_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jg5c99/codegpt_autocomplete_issues_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-21T01:25:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg1dzj</id>
    <title>Am I doing something wrong? Ollama never gives answers longer than one sentence.</title>
    <updated>2025-03-20T22:19:44+00:00</updated>
    <author>
      <name>/u/typhoon90</name>
      <uri>https://old.reddit.com/user/typhoon90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I installed Ollama the other day and have been playing around with it, so far I have tried Llama 3.2 as well as Wizard Vicuna Uncensored and have been getting very poor responses on both. No matter what I prompt I only ever get one around one sentence as a response and there doesnt appear to be any context in future messages. I have tried setting system prompts /set system and can see them being saved but they appear to have no impact on the replies that I am getting out of the model. I am just running it out of powershell. Am I doing something wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/typhoon90"&gt; /u/typhoon90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jg1dzj/am_i_doing_something_wrong_ollama_never_gives/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jg1dzj/am_i_doing_something_wrong_ollama_never_gives/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jg1dzj/am_i_doing_something_wrong_ollama_never_gives/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-20T22:19:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg7n7x</id>
    <title>8x Mi60 AI Server Doing Actual Work!</title>
    <updated>2025-03-21T03:26:53+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zl35kmbtoype1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jg7n7x/8x_mi60_ai_server_doing_actual_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jg7n7x/8x_mi60_ai_server_doing_actual_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-21T03:26:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfv0fz</id>
    <title>Did Docker just screwed Ollama?</title>
    <updated>2025-03-20T17:54:53+00:00</updated>
    <author>
      <name>/u/Intrepid_Snoo</name>
      <uri>https://old.reddit.com/user/Intrepid_Snoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Docker just announced at Java One that they now support hosting and running models natively with a OPEN AI API compatible to interact with them.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/mk_2MIWxLI0?t=1544"&gt;https://youtu.be/mk_2MIWxLI0?t=1544&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Intrepid_Snoo"&gt; /u/Intrepid_Snoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfv0fz/did_docker_just_screwed_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfv0fz/did_docker_just_screwed_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jfv0fz/did_docker_just_screwed_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-20T17:54:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg3inn</id>
    <title>How ollama uses GPUs in parallel tasks</title>
    <updated>2025-03-20T23:54:40+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I have 3 7900 xtx and I use gemma3 27b model. I use it as a server which needs to serve as many requests as possible. I have decided that the parallel option could be maybe 15. So the system could serve the model for 15 users simultaneously, and the rest would wait in the que. My question is, I know that when inferencing 1 request, it uses 1 GPU at a time, but what happens when inferencing 15 simultaneous requests, and they all come not exactly at same moment but like inside a 3 second period. Will ollama use more tha 1 GPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jg3inn/how_ollama_uses_gpus_in_parallel_tasks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jg3inn/how_ollama_uses_gpus_in_parallel_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jg3inn/how_ollama_uses_gpus_in_parallel_tasks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-20T23:54:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgrsma</id>
    <title>How many models are listed in Ollama library?</title>
    <updated>2025-03-21T21:16:09+00:00</updated>
    <author>
      <name>/u/Hedgehog_Dapper</name>
      <uri>https://old.reddit.com/user/Hedgehog_Dapper</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to count the number of models listed in Ollama is there any way to get it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hedgehog_Dapper"&gt; /u/Hedgehog_Dapper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jgrsma/how_many_models_are_listed_in_ollama_library/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jgrsma/how_many_models_are_listed_in_ollama_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jgrsma/how_many_models_are_listed_in_ollama_library/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-21T21:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgs3wu</id>
    <title>A simple HTML UI local Chatbot through VBScript</title>
    <updated>2025-03-21T21:30:09+00:00</updated>
    <author>
      <name>/u/f4lc0n_3416</name>
      <uri>https://old.reddit.com/user/f4lc0n_3416</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there folks,&lt;/p&gt; &lt;p&gt;I am no professional programmer, neither its my major field. However, due to my hobby of making silly things in VBscript in my spare time, I made a simple script, that installs Ollama, creates an HTML User Interface, where you can talk with LLM as if you're chatting inside a chatbox.&lt;/p&gt; &lt;p&gt;I took help from ChatGPT, for most of the HTML part (pardon my ignorance :p)&lt;/p&gt; &lt;p&gt;FEATURES:&lt;/p&gt; &lt;p&gt;&amp;gt; It can handle custom bots,&lt;/p&gt; &lt;p&gt;&amp;gt; has memory retention, that stores chats and memory, even if the browser is closed. (memory stays until browser cache is removed)&lt;/p&gt; &lt;p&gt;&amp;gt; It ofcourse runs on localhost, without internet.&lt;/p&gt; &lt;p&gt;&amp;gt; the one in video supports Llama3.2 3b parameter, a small model, however can easily integrate bigger models, by a little change in script.&lt;/p&gt; &lt;p&gt;&amp;gt; easy installation, no need of CLI commands. Just run a file and it will install Ollama first, on next run it will simply run the UI&lt;/p&gt; &lt;p&gt;&amp;gt; planning to add more features and improving UI&lt;/p&gt; &lt;p&gt;LIMITATIONS:&lt;/p&gt; &lt;p&gt;&amp;gt; The bot needs to type the whole message, only then it is send, instead of being printed word by word like conventional GPTs. Which ofcourse, takes a while for big responses.&lt;br /&gt; &amp;gt; few other bugs and stuff possibly I don't know&lt;/p&gt; &lt;p&gt;I wanted to show something I made out of fun, as I was simply bored, incase of any suggestion or improvements, kindly tell me below.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1jgs3wu/video/jemtquun24qe1/player"&gt;https://reddit.com/link/1jgs3wu/video/jemtquun24qe1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/f4lc0n_3416"&gt; /u/f4lc0n_3416 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jgs3wu/a_simple_html_ui_local_chatbot_through_vbscript/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jgs3wu/a_simple_html_ui_local_chatbot_through_vbscript/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jgs3wu/a_simple_html_ui_local_chatbot_through_vbscript/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-21T21:30:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh3cis</id>
    <title>How to host OLLAM? My laptop can't handle LLMs‚Äîany cheap hosting providers you recommend?</title>
    <updated>2025-03-22T07:47:09+00:00</updated>
    <author>
      <name>/u/PixelPioneer-001</name>
      <uri>https://old.reddit.com/user/PixelPioneer-001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been trying to run OLLAM on my laptop, but it keeps hitting 100% memory usage and is super slow. It‚Äôs just not able to handle the LLMs properly. I‚Äôm looking for a cheap but reliable hosting provider to run it.&lt;/p&gt; &lt;p&gt;Does anyone have suggestions for affordable hosting options that can handle OLLAM without breaking the bank?&lt;/p&gt; &lt;p&gt;Appreciate any help or recommendations!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PixelPioneer-001"&gt; /u/PixelPioneer-001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh3cis/how_to_host_ollam_my_laptop_cant_handle_llmsany/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh3cis/how_to_host_ollam_my_laptop_cant_handle_llmsany/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jh3cis/how_to_host_ollam_my_laptop_cant_handle_llmsany/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-22T07:47:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgte9s</id>
    <title>Mistral Small 22b only 40% gpu</title>
    <updated>2025-03-21T22:27:46+00:00</updated>
    <author>
      <name>/u/Inner-End7733</name>
      <uri>https://old.reddit.com/user/Inner-End7733</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just tried mistral small 22b for the first time and I was getting about 10t/s at only 40% gpu. That's strange to me since Mistral-Nemo get me up to 80-90% GPU. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inner-End7733"&gt; /u/Inner-End7733 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jgte9s/mistral_small_22b_only_40_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jgte9s/mistral_small_22b_only_40_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jgte9s/mistral_small_22b_only_40_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-21T22:27:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgbxot</id>
    <title>Built an app for Mac and Windows. Its alternative for openwebui or librechat</title>
    <updated>2025-03-21T08:18:15+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jgbxot/built_an_app_for_mac_and_windows_its_alternative/"&gt; &lt;img alt="Built an app for Mac and Windows. Its alternative for openwebui or librechat" src="https://external-preview.redd.it/wBMT-jmCl5XYkWR1AWvetODsl1A2V4fj59LTyPe1WpI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b8331475460b9074f076a36ad33a972de04b5fe" title="Built an app for Mac and Windows. Its alternative for openwebui or librechat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recent i made a post in ollama sub saying im working on a app and got a lot of insights and today i added all those features and released it to public Its not native app by the way its electron app. Its completely private not connection to the internet needed once model is downloaded&lt;/p&gt; &lt;p&gt;What can it do, 1. Image Generation, 2. Tiny Agent Builders (you can use it like apps) 3. Chat with ollama and manage models in app for beginners&lt;/p&gt; &lt;p&gt;Feel free to comment if something I can improve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/badboysm890/ClaraVerse"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jgbxot/built_an_app_for_mac_and_windows_its_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jgbxot/built_an_app_for_mac_and_windows_its_alternative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-21T08:18:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgxjk7</id>
    <title>Looking for a chatbot with the functionalities of chatgpt/claude but is private (my data will not be reported back or recorded), can ollama provide that?</title>
    <updated>2025-03-22T01:46:45+00:00</updated>
    <author>
      <name>/u/DALLAVID</name>
      <uri>https://old.reddit.com/user/DALLAVID</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DALLAVID"&gt; /u/DALLAVID &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jgxjk7/looking_for_a_chatbot_with_the_functionalities_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jgxjk7/looking_for_a_chatbot_with_the_functionalities_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jgxjk7/looking_for_a_chatbot_with_the_functionalities_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-22T01:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh353c</id>
    <title>RTX 5070 and RTX 3060TI</title>
    <updated>2025-03-22T07:31:29+00:00</updated>
    <author>
      <name>/u/Kirtap01</name>
      <uri>https://old.reddit.com/user/Kirtap01</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently have a RTX 3060 ti, and despite the little vram (8gb) it works well. I know it is generally possible to run ollama utilising 2 gpus. But i wonder how well it would work with an rtx 5070 and rtx 3060ti. Im considering the rtx 5070 because the card would give me also sufficient gaming performance. In Germany i can buy a rtx 5070 for 649‚Ç¨ instead of 1000‚Ç¨+ for an rtx 5070ti. I know the 5070ti has 16gb vram but wouldn‚Äòt it be better to have 20 gb with the two cards combined. Please correct me if im wrong.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kirtap01"&gt; /u/Kirtap01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh353c/rtx_5070_and_rtx_3060ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh353c/rtx_5070_and_rtx_3060ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jh353c/rtx_5070_and_rtx_3060ti/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-22T07:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh6ixi</id>
    <title>LangChain or Pydantic AI or else ?</title>
    <updated>2025-03-22T11:39:07+00:00</updated>
    <author>
      <name>/u/Responsible-Tart-964</name>
      <uri>https://old.reddit.com/user/Responsible-Tart-964</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible-Tart-964"&gt; /u/Responsible-Tart-964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/aiagents/comments/1jh6ii2/langchain_or_pydantic_ai_or_else/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh6ixi/langchain_or_pydantic_ai_or_else/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jh6ixi/langchain_or_pydantic_ai_or_else/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-22T11:39:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh45kb</id>
    <title>how to force qwq to use both GPUs?</title>
    <updated>2025-03-22T08:48:37+00:00</updated>
    <author>
      <name>/u/caetydid</name>
      <uri>https://old.reddit.com/user/caetydid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;I run QwQ on dual rtx 3090. What I see is that the model is being loaded fully on one rtx and that the CPU utilization spikes to 100%. If I disable one GPU the performance and the behavior is almost the same, I yield around 19-22t/s.&lt;/p&gt; &lt;p&gt;Is there a way to force ollama to use both GPUs? As soon as I have increased context 24Gb VRAM will not suffice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/caetydid"&gt; /u/caetydid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh45kb/how_to_force_qwq_to_use_both_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh45kb/how_to_force_qwq_to_use_both_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jh45kb/how_to_force_qwq_to_use_both_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-22T08:48:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh6l7b</id>
    <title>PyChat</title>
    <updated>2025-03-22T11:43:11+00:00</updated>
    <author>
      <name>/u/mspamnamem</name>
      <uri>https://old.reddit.com/user/mspamnamem</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve seen a few posts recently about chat clients that people have been building. They‚Äôre great! &lt;/p&gt; &lt;p&gt;I‚Äôve been working on one of my own context aware chat clients. It is written in python and has a few unique things:&lt;/p&gt; &lt;p&gt;(1) can import and export chats. I think this so I can export a ‚Äústarter‚Äù chat. I sort of think of this like a sourdough starter. Share it with your friends. Can be useful for coding if you don‚Äôt want to start from scratch every time.&lt;/p&gt; &lt;p&gt;(2) context aware and can switch provider and model in the chat window. &lt;/p&gt; &lt;p&gt;(3) search and archive threads. &lt;/p&gt; &lt;p&gt;(4) allow two AIs to communicate with one another. Also useful for coding: make one strong coding model the developer and a strong language model the manager. Can also simulate debates and stuff. &lt;/p&gt; &lt;p&gt;(5) attempts to highlight code into code blocks and allows you to easily copy them. &lt;/p&gt; &lt;p&gt;I have this working at home with a Mac on my network hosting ollama and running this client on a PC. I haven‚Äôt tested it with localhost ollama running on the same machine but it should still work. Just make sure that ollama is listening on 0.0.0.0 not just html server. &lt;/p&gt; &lt;p&gt;Note: - API keys are optional to OpenAI and Anthropic. They are stored locally but not encrypted. Same with the chat database. Maybe in the future I‚Äôll work to encrypt these. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;There are probably some bugs because I‚Äôm just one person. Willing to fix. Let me know! &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/Magnetron85/PyChat"&gt;https://github.com/Magnetron85/PyChat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mspamnamem"&gt; /u/mspamnamem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh6l7b/pychat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh6l7b/pychat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jh6l7b/pychat/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-22T11:43:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgsx00</id>
    <title>Built an open source mock interviews platform powered by ollama</title>
    <updated>2025-03-21T22:06:03+00:00</updated>
    <author>
      <name>/u/Boring_Rabbit2275</name>
      <uri>https://old.reddit.com/user/Boring_Rabbit2275</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jgsx00/built_an_open_source_mock_interviews_platform/"&gt; &lt;img alt="Built an open source mock interviews platform powered by ollama" src="https://preview.redd.it/2vz41jpw84qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf6a54b98a23125724ae5c087864384d0b41aa85" title="Built an open source mock interviews platform powered by ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Come practice your interviews for free using our project on GitHub here: &lt;a href="https://github.com/Azzedde/aiva_mock_interviews"&gt;https://github.com/Azzedde/aiva_mock_interviews&lt;/a&gt; We are two junior AI engineers, and we would really appreciate feedback on our work. Please star it if you like it.&lt;/p&gt; &lt;p&gt;We find that the junior era is full of uncertainty, and we want to know if we are doing good work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Boring_Rabbit2275"&gt; /u/Boring_Rabbit2275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2vz41jpw84qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jgsx00/built_an_open_source_mock_interviews_platform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jgsx00/built_an_open_source_mock_interviews_platform/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-21T22:06:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgsqci</id>
    <title>Observer AI - AI Agent creation!</title>
    <updated>2025-03-21T21:58:00+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jgsqci/observer_ai_ai_agent_creation/"&gt; &lt;img alt="Observer AI - AI Agent creation!" src="https://external-preview.redd.it/YW1jZTUxa3E2NHFlMUoR_eYKE5mqj2V4XhjfJPn1tdQGxAnERB4_DcKntuZ4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e50911a0eba38062d5cdfed3f34aa80c7b8e0893" title="Observer AI - AI Agent creation!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama community!&lt;/p&gt; &lt;p&gt;Just dropped possibly the coolest feature yet for Observer AI - a natural language &lt;strong&gt;Agent Generator&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;I made a quick (admittedly janky üòÖ) demo video showing how it works&lt;/p&gt; &lt;p&gt;This turns Observer AI into a no-code platform for creating AI agents that can monitor your screen, run Python via Jupyter, and take actions - all powered by your local Ollama models!&lt;/p&gt; &lt;p&gt;Give it a try at &lt;a href="https://app.observer-ai.com"&gt;https://app.observer-ai.com&lt;/a&gt; and let me know what kind of agents you end up creating!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/g63nxzjq64qe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jgsqci/observer_ai_ai_agent_creation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jgsqci/observer_ai_ai_agent_creation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-21T21:58:00+00:00</published>
  </entry>
</feed>
