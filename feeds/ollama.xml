<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-31T04:48:52+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1idq0ou</id>
    <title>How do I set context size when using openai compatible API?</title>
    <updated>2025-01-30T15:52:48+00:00</updated>
    <author>
      <name>/u/sobolanul11</name>
      <uri>https://old.reddit.com/user/sobolanul11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using openai library (I already had it setup in the project, I just added another url). I added &lt;/p&gt; &lt;pre&gt;&lt;code&gt;options: { num_ctx: 16384 } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;to the request with no result &lt;/p&gt; &lt;p&gt;I also run the ollama with /set parameter num_ctx 8192, but with no luck&lt;/p&gt; &lt;p&gt;Every request is truncated at 2048, I see this in logs:&lt;br /&gt; &lt;code&gt;level=WARN source=runner.go:129 msg=&amp;quot;truncating input prompt&amp;quot; limit=2048 prompt=3246 keep=5 new=2048&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sobolanul11"&gt; /u/sobolanul11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idq0ou/how_do_i_set_context_size_when_using_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idq0ou/how_do_i_set_context_size_when_using_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idq0ou/how_do_i_set_context_size_when_using_openai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T15:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1idjwkh</id>
    <title>how to use Ollama with C++</title>
    <updated>2025-01-30T10:15:50+00:00</updated>
    <author>
      <name>/u/Reasonable-Falcon470</name>
      <uri>https://old.reddit.com/user/Reasonable-Falcon470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi i am bad with python i just cant i tried and i cant find any C++ instructions for Ollama&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable-Falcon470"&gt; /u/Reasonable-Falcon470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idjwkh/how_to_use_ollama_with_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idjwkh/how_to_use_ollama_with_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idjwkh/how_to_use_ollama_with_c/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T10:15:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1idf5ez</id>
    <title>How can I force Ollama to use the cpu instead of gpu?</title>
    <updated>2025-01-30T05:06:19+00:00</updated>
    <author>
      <name>/u/ResponsibleTruck4717</name>
      <uri>https://old.reddit.com/user/ResponsibleTruck4717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know I can force it use cpu instead of gpu when I'm using docker but I'm looking for solution without docker.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleTruck4717"&gt; /u/ResponsibleTruck4717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idf5ez/how_can_i_force_ollama_to_use_the_cpu_instead_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idf5ez/how_can_i_force_ollama_to_use_the_cpu_instead_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idf5ez/how_can_i_force_ollama_to_use_the_cpu_instead_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T05:06:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1idmtrv</id>
    <title>Deepseek on an RTX 4060 ti 8gb</title>
    <updated>2025-01-30T13:22:25+00:00</updated>
    <author>
      <name>/u/whymeimbusysleeping</name>
      <uri>https://old.reddit.com/user/whymeimbusysleeping</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi gang. Can I get a recommendation on which model to use that would suit my GPU? And how to install it?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/whymeimbusysleeping"&gt; /u/whymeimbusysleeping &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idmtrv/deepseek_on_an_rtx_4060_ti_8gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idmtrv/deepseek_on_an_rtx_4060_ti_8gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idmtrv/deepseek_on_an_rtx_4060_ti_8gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T13:22:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1idlbhw</id>
    <title>What's the best model that can be comfortably run on a normal PC?</title>
    <updated>2025-01-30T11:55:30+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've installed llama3.2 as this is what Ollama itself recommend at the start, so I just installed the default option, but it seems kind of bad, especially in comparison to current popular models that we access online. Not only that, its skills in non-English languages are abysmal. In English, it's sort of decent, not that bad, but I tried it in Serbian, it doesn't even properly speak the language.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idlbhw/whats_the_best_model_that_can_be_comfortably_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idlbhw/whats_the_best_model_that_can_be_comfortably_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idlbhw/whats_the_best_model_that_can_be_comfortably_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T11:55:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1id6y97</id>
    <title>I feel bad for the AI lol after seeing its chain of thought. üò≠</title>
    <updated>2025-01-29T22:44:12+00:00</updated>
    <author>
      <name>/u/Tricky_Reflection_75</name>
      <uri>https://old.reddit.com/user/Tricky_Reflection_75</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1id6y97/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt; &lt;img alt="I feel bad for the AI lol after seeing its chain of thought. üò≠" src="https://b.thumbs.redditmedia.com/enUEH3eKOb0e3umQhJmhNKG1mhLeM6dFReIv_8oKkzc.jpg" title="I feel bad for the AI lol after seeing its chain of thought. üò≠" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/910r117yg0ge1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c699f0b744adc486372072f5a73072f6d893f97e"&gt;https://preview.redd.it/910r117yg0ge1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c699f0b744adc486372072f5a73072f6d893f97e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tricky_Reflection_75"&gt; /u/Tricky_Reflection_75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id6y97/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id6y97/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id6y97/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T22:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1idu6cm</id>
    <title>System String from previous instance wont go away</title>
    <updated>2025-01-30T18:46:30+00:00</updated>
    <author>
      <name>/u/Murphys_Project</name>
      <uri>https://old.reddit.com/user/Murphys_Project</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1idu6cm/system_string_from_previous_instance_wont_go_away/"&gt; &lt;img alt="System String from previous instance wont go away" src="https://a.thumbs.redditmedia.com/1tJzsgo66gwtkOMQQRXjstDacuZ80N9Ryz8EbFBG3V8.jpg" title="System String from previous instance wont go away" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I was messing around with ollama /set system &amp;lt;string&amp;gt; and made the deepseek-r1:7b model type code with ``` at the beginning and end of code blocks for implementation as a discord bot. I cleared the system string for the setting of ``` and ``` at the beginning and end of the code but the issue still persisted. It seems that when I /set system, it applied it globally to all models including the 12b and 8b model. I tried messing with the python library for asking the deepseek model a prompt via that api and the code it gives me still has ``` attached to it. I tried uninstalling and reinstalling ollama and the models to try to fix this and it still does not work. I even set the system parameters to remove ``` from code and it ignores it. When I uninstalled ollama, I stared at its file directories to make sure it was removed and it was, even the temp file. I genuinely don't know what up with my issue. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zx1rs1ynf6ge1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=285dc1c3b92b9a5d7189cfccfa5a7dffa9778f0c"&gt;https://preview.redd.it/zx1rs1ynf6ge1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=285dc1c3b92b9a5d7189cfccfa5a7dffa9778f0c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f06cpl0nf6ge1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c101f7ca53c3cf8b18ce0d4fa6e9ff1c6722a0a"&gt;https://preview.redd.it/f06cpl0nf6ge1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c101f7ca53c3cf8b18ce0d4fa6e9ff1c6722a0a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Murphys_Project"&gt; /u/Murphys_Project &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idu6cm/system_string_from_previous_instance_wont_go_away/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idu6cm/system_string_from_previous_instance_wont_go_away/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idu6cm/system_string_from_previous_instance_wont_go_away/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T18:46:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1idvl47</id>
    <title>Model for language learning?</title>
    <updated>2025-01-30T19:44:33+00:00</updated>
    <author>
      <name>/u/KiRa937</name>
      <uri>https://old.reddit.com/user/KiRa937</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm currently learning Japanese and I want to use AI in my studies. The most obvious way is to ask ‚Äúexplain word A in context of sentence B‚Äù, but I also thought maybe it is possible to analyze the material in my target language. Something like ‚Äúhere‚Äôs a bunch of texts, order them by grammar difficulty‚Äù. Or maybe ‚ÄúI‚Äôm interested in vocabulary of topic X, so order texts by level of use of said vocabulary‚Äù. So what model should I use for language learning? And for future reference what models would work for different languages if there‚Äôs no ‚Äúone fits all‚Äù model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KiRa937"&gt; /u/KiRa937 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idvl47/model_for_language_learning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idvl47/model_for_language_learning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idvl47/model_for_language_learning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T19:44:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1idsvfd</id>
    <title>Open WebUI not working after 0.5.6, "Network Problem" 400 Bad Request (Docker)</title>
    <updated>2025-01-30T17:52:08+00:00</updated>
    <author>
      <name>/u/ucffool</name>
      <uri>https://old.reddit.com/user/ucffool</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Even since I pulled down 0.5.6 or 0.5.7, the container runs and seems fine, but talking to Ollama or with Together.ai both fail. It can retrieve the list of models just fine, but it seems to be an issue with completions.&lt;/p&gt; &lt;p&gt;Anyone else having this problem? Do I need to blow it away and do a completely clean install, loading settings for exports?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;2025-01-30 10:50:03 INFO: connection rejected (400 Bad Request) 2025-01-30 10:50:03 INFO: connection closed &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ucffool"&gt; /u/ucffool &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idsvfd/open_webui_not_working_after_056_network_problem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idsvfd/open_webui_not_working_after_056_network_problem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idsvfd/open_webui_not_working_after_056_network_problem/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T17:52:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie1jzx</id>
    <title>Remove &lt;think&gt; tags?</title>
    <updated>2025-01-31T00:01:59+00:00</updated>
    <author>
      <name>/u/midlivecrisis</name>
      <uri>https://old.reddit.com/user/midlivecrisis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apologies in advance if this has already been answered. Is there a way to force the smaller Deepseek models (7b, 14b running on Ollama) to NOT return the &amp;lt;think&amp;gt; tags and thinking content - and to only return the end response? Is there a parameter I missed where you can disable that content? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/midlivecrisis"&gt; /u/midlivecrisis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1jzx/remove_think_tags/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1jzx/remove_think_tags/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie1jzx/remove_think_tags/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T00:01:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie1s1h</id>
    <title>Running DeepSeek-R1-GGUF from Unsloth with Ollama</title>
    <updated>2025-01-31T00:12:35+00:00</updated>
    <author>
      <name>/u/WitcherSanek</name>
      <uri>https://old.reddit.com/user/WitcherSanek</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How to run this model on Windows with ollama &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2%5C_K%5C_XL"&gt;https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2\_K\_XL&lt;/a&gt;?&lt;/p&gt; &lt;p&gt;Huggingface has instruction &amp;quot;ollama run hf.co/unsloth/DeepSeek-R1-GGUF:Q2_K_XL&amp;quot; which leads to error &amp;quot;Error: pull model manifest: 400: The specified repository contains sharded GGUF. Ollama does not support this yet. Follow this issue for more info:&amp;quot;.&lt;/p&gt; &lt;p&gt;Downloading this files locally and running &amp;quot;llama-gguf-split --merge&amp;quot; leads to instant creation of empty file without any error or info message.&lt;/p&gt; &lt;p&gt;Manual concatenation with copy -b allows model to be imported, but &amp;quot;ollama run R1:latest&amp;quot; leads to error &amp;quot;Error: llama runner process has terminated: error loading model: invalid split file: D:\Ollama\blobs\sha256-311b7e2b72da29daffbac5e5f5df9353b1b3be9879d22d1dc498ece99529cfe5&amp;quot;.&lt;/p&gt; &lt;p&gt;What is wrong with my attempts? Am i missing something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WitcherSanek"&gt; /u/WitcherSanek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1s1h/running_deepseekr1gguf_from_unsloth_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1s1h/running_deepseekr1gguf_from_unsloth_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie1s1h/running_deepseekr1gguf_from_unsloth_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T00:12:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie42ld</id>
    <title>Deepseek-r1:8b is 4 times slower than llama3.2:8b on Ollama running locally</title>
    <updated>2025-01-31T02:03:05+00:00</updated>
    <author>
      <name>/u/PawanAgarwal</name>
      <uri>https://old.reddit.com/user/PawanAgarwal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am running both llama3.2:8b and deepseek-r1:8b locally on my mac. I noticed than latency per token for deepseek-r1:8b model is 4x llama3.2:8b. Since both are 8b versions, I was hoping latency would be similar. Anyone else also seeing that? Any configs needed for ollama for deepseek serving?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PawanAgarwal"&gt; /u/PawanAgarwal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie42ld/deepseekr18b_is_4_times_slower_than_llama328b_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie42ld/deepseekr18b_is_4_times_slower_than_llama328b_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie42ld/deepseekr18b_is_4_times_slower_than_llama328b_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T02:03:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie4nbl</id>
    <title>Uninstalling deepseek LLM</title>
    <updated>2025-01-31T02:32:40+00:00</updated>
    <author>
      <name>/u/CraftyOwl21</name>
      <uri>https://old.reddit.com/user/CraftyOwl21</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi - Via Terminal, I installed locally at least two different versions of DeepSeek via Ollama. How do I now uninstall everything?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CraftyOwl21"&gt; /u/CraftyOwl21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie4nbl/uninstalling_deepseek_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie4nbl/uninstalling_deepseek_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie4nbl/uninstalling_deepseek_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T02:32:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie4s8g</id>
    <title>Running ollama locally on my phone, very weird responses</title>
    <updated>2025-01-31T02:39:55+00:00</updated>
    <author>
      <name>/u/NorthropChicken</name>
      <uri>https://old.reddit.com/user/NorthropChicken</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ie4s8g/running_ollama_locally_on_my_phone_very_weird/"&gt; &lt;img alt="Running ollama locally on my phone, very weird responses" src="https://preview.redd.it/sjp5jds6s8ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08630c765c18cf9a40aa0f10dcb2b7c621b68c1e" title="Running ollama locally on my phone, very weird responses" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tinyllama on Pixel 5&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NorthropChicken"&gt; /u/NorthropChicken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sjp5jds6s8ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie4s8g/running_ollama_locally_on_my_phone_very_weird/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie4s8g/running_ollama_locally_on_my_phone_very_weird/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T02:39:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1idpjtj</id>
    <title>What would be the simplest way to train deepseek model on self hosted server.</title>
    <updated>2025-01-30T15:31:52+00:00</updated>
    <author>
      <name>/u/shaxadhere</name>
      <uri>https://old.reddit.com/user/shaxadhere</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to train model on specific set of data, I have the data in raw text and I have questions and answers from that raw text.&lt;/p&gt; &lt;p&gt;Is there any way I can train my model on all of that data.&lt;/p&gt; &lt;p&gt;Documents in total are 400,000 pages, and questions and answers are around 1 million+&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shaxadhere"&gt; /u/shaxadhere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idpjtj/what_would_be_the_simplest_way_to_train_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idpjtj/what_would_be_the_simplest_way_to_train_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idpjtj/what_would_be_the_simplest_way_to_train_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T15:31:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1idv02o</id>
    <title>Has anyone been using the base M4 Mac Mini as an Ollama server and want to share their mileage?</title>
    <updated>2025-01-30T19:20:09+00:00</updated>
    <author>
      <name>/u/_AdamWTF</name>
      <uri>https://old.reddit.com/user/_AdamWTF</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently have a home lab setup that spans a few different machines but I don‚Äôt currently have anything capable of running AI at decent speeds, I was going to Frankenstein together a machine using spare parts I have laying around, but getting a decent GPU with acceptable vram in the UK is so expensive right now.&lt;/p&gt; &lt;p&gt;This brings me to the Mac Mini M4, looking at just the base model and getting it for around ¬£550, it seems like a lot of power for a decent price. I‚Äôm just curious how good the performance really is, I‚Äôm expecting with something like a 3b model of say llama 3.2 the response for 5k context tokens to be &amp;lt; 3 seconds to be suitable for me. &lt;/p&gt; &lt;p&gt;If anyone is willing to share their experiences and some examples of the kind of speeds you‚Äôre getting I‚Äôd really appreciate it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_AdamWTF"&gt; /u/_AdamWTF &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idv02o/has_anyone_been_using_the_base_m4_mac_mini_as_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idv02o/has_anyone_been_using_the_base_m4_mac_mini_as_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idv02o/has_anyone_been_using_the_base_m4_mac_mini_as_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T19:20:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1idxa19</id>
    <title>Fine tuning for small models</title>
    <updated>2025-01-30T20:55:48+00:00</updated>
    <author>
      <name>/u/jcrowe</name>
      <uri>https://old.reddit.com/user/jcrowe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an app that pulls specific information from a website. Right now I scrape the page, convert it to text and upload it to chatgpt to get a json file of data. Works great‚Ä¶&lt;/p&gt; &lt;p&gt;What I would like to do is switch over to a small LLM model (llama3.2:1b for example). But this model doesn‚Äôt return the results I want.&lt;/p&gt; &lt;p&gt;I would like to fine tune this model to make it work for my use case.&lt;/p&gt; &lt;p&gt;Can anyone recommend a tutorial for this or tell me I‚Äôm an idiot if it‚Äôs not the right way to use the technology?&lt;/p&gt; &lt;p&gt;ETA: to clarify, I don‚Äôt want to fine tune for the model to have the new information, I want to fine tune so that it knows how to deal with this information. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jcrowe"&gt; /u/jcrowe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idxa19/fine_tuning_for_small_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idxa19/fine_tuning_for_small_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idxa19/fine_tuning_for_small_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T20:55:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie58r3</id>
    <title>Is it safe to follow this guide and install Deep Seek with Ollama through the CMD?</title>
    <updated>2025-01-31T03:04:32+00:00</updated>
    <author>
      <name>/u/Scuzyfuzywuzy</name>
      <uri>https://old.reddit.com/user/Scuzyfuzywuzy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://qwen-ai.com/deepseek-r1-distill-1-5b/"&gt;https://qwen-ai.com/deepseek-r1-distill-1-5b/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Its the legit website right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Scuzyfuzywuzy"&gt; /u/Scuzyfuzywuzy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie58r3/is_it_safe_to_follow_this_guide_and_install_deep/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie58r3/is_it_safe_to_follow_this_guide_and_install_deep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie58r3/is_it_safe_to_follow_this_guide_and_install_deep/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T03:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie5k9y</id>
    <title>Facing errors while installing deepseek r1 1.5b through ollama in windows</title>
    <updated>2025-01-31T03:21:10+00:00</updated>
    <author>
      <name>/u/AndreoBee100</name>
      <uri>https://old.reddit.com/user/AndreoBee100</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ie5k9y/facing_errors_while_installing_deepseek_r1_15b/"&gt; &lt;img alt="Facing errors while installing deepseek r1 1.5b through ollama in windows" src="https://preview.redd.it/dmv3g0miz8ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f773d3a58be867c97bec65c82e0a6ef252cea3e6" title="Facing errors while installing deepseek r1 1.5b through ollama in windows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AndreoBee100"&gt; /u/AndreoBee100 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dmv3g0miz8ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie5k9y/facing_errors_while_installing_deepseek_r1_15b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie5k9y/facing_errors_while_installing_deepseek_r1_15b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T03:21:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1idvw46</id>
    <title>Recommended deepseek model for coding tasks for an average PC?</title>
    <updated>2025-01-30T19:57:31+00:00</updated>
    <author>
      <name>/u/Upset_Hippo_5304</name>
      <uri>https://old.reddit.com/user/Upset_Hippo_5304</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Want to join the gang and try this stuff for coding tasks.&lt;/p&gt; &lt;p&gt;Py, dart, js, c#&lt;/p&gt; &lt;p&gt;32GB ram RTX 3070 Ryzen 5 5600x&lt;/p&gt; &lt;p&gt;Cheers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Upset_Hippo_5304"&gt; /u/Upset_Hippo_5304 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idvw46/recommended_deepseek_model_for_coding_tasks_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idvw46/recommended_deepseek_model_for_coding_tasks_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idvw46/recommended_deepseek_model_for_coding_tasks_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T19:57:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie6fzy</id>
    <title>Empty response for Deepseek Models</title>
    <updated>2025-01-31T04:09:36+00:00</updated>
    <author>
      <name>/u/TheHarinator</name>
      <uri>https://old.reddit.com/user/TheHarinator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone have a problem when trying to interact with the Deepseek-r1 models? The response always is empty for me. I have tried removing and re-pulling these models too. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheHarinator"&gt; /u/TheHarinator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie6fzy/empty_response_for_deepseek_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie6fzy/empty_response_for_deepseek_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie6fzy/empty_response_for_deepseek_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T04:09:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1idh8ft</id>
    <title>Deepseek r1 671b on my local PC</title>
    <updated>2025-01-30T07:03:27+00:00</updated>
    <author>
      <name>/u/Geschirrtuch</name>
      <uri>https://old.reddit.com/user/Geschirrtuch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;Two days ago, I turned night into day, and in the end, I managed to get R1 running on my local PC. Yesterday, I uploaded a video on YouTube showing how I did it: &lt;a href="https://www.youtube.com/watch?v=O3Lk3xSkAdk"&gt;https://www.youtube.com/watch?v=O3Lk3xSkAdk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I don't post here often, so I'm not sure if sharing the link is okay‚ÄîI hope it is.&lt;/p&gt; &lt;p&gt;The video is in German, but with subtitles, everyone should be able to understand it.&lt;br /&gt; Be careful if you want to try this yourself! ;)&lt;/p&gt; &lt;p&gt;Update:&lt;/p&gt; &lt;p&gt;For those who don't feel like watching the video: The &amp;quot;trick&amp;quot; was using Windows' pagefile. I set up three of them on three different SSDs, which gave me around 750GB of virtual memory in total.&lt;/p&gt; &lt;p&gt;Loading the model and answering a question took my PC about 90 minutes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Geschirrtuch"&gt; /u/Geschirrtuch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T07:03:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie1aui</id>
    <title>Does `ollama create` actually build a new model?</title>
    <updated>2025-01-30T23:50:04+00:00</updated>
    <author>
      <name>/u/homelab2946</name>
      <uri>https://old.reddit.com/user/homelab2946</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded a GGUF file and create a Modelfile to extend it. Then I run `ollama create model_name -f Modelfile`, a `model_name:latest` model is created and shows in `ollama list` 10 GB. The GGUF file is also around the same 10 GB. Does `ollama create` not just add instruction on a base model but actually acting more like `docker build`? Would it then be fine to remove the GGUF file after the build?&lt;/p&gt; &lt;p&gt;Another scenario is through a supported ollama model, like `llama3`. Does Ollama &amp;quot;build&amp;quot; a new image if I create a new Modelfile from `llama3`, so it takes double the storage?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/homelab2946"&gt; /u/homelab2946 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1aui/does_ollama_create_actually_build_a_new_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1aui/does_ollama_create_actually_build_a_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie1aui/does_ollama_create_actually_build_a_new_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T23:50:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie210z</id>
    <title>Got Deepseek R1 1.5b running locally on Pixel 8 pro</title>
    <updated>2025-01-31T00:24:19+00:00</updated>
    <author>
      <name>/u/Teradyyne</name>
      <uri>https://old.reddit.com/user/Teradyyne</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"&gt; &lt;img alt="Got Deepseek R1 1.5b running locally on Pixel 8 pro" src="https://external-preview.redd.it/cTk0emwwZ3ozOGdlMYbCJM1MQLfOpw8fF1FnxkqAh7visCMP7lFjyVXppg7i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=263215164e10efcd203b94bae4672c47e11c63d8" title="Got Deepseek R1 1.5b running locally on Pixel 8 pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Teradyyne"&gt; /u/Teradyyne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cha0fplz38ge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T00:24:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1idqxto</id>
    <title>Why Are All Local AI Models So Bad? No One Talks About This!</title>
    <updated>2025-01-30T16:31:59+00:00</updated>
    <author>
      <name>/u/NikkEvan</name>
      <uri>https://old.reddit.com/user/NikkEvan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with local AI models, even &amp;quot;high-end ones&amp;quot; like the recent DeepSeek-R1 32B, using Open WebUI.&lt;br /&gt; I expected them to be weaker than online models, but the gap is just ridiculous.&lt;br /&gt; Even for the simplest questions, they either fail, give nonsense answers, or completely misunderstand the input.&lt;/p&gt; &lt;p&gt;I‚Äôve set the parameters and all the settings at the best i could, tried different setups, system prompts, and still , even after parsing a basic document just a few pages long, is a struggle.&lt;br /&gt; If it already fails here, how am I supposed to use it for hundreds of internal company documents?&lt;/p&gt; &lt;p&gt;The crazy part? No one talks about this!&lt;br /&gt; Instead, i see every video in youtube saying :&lt;br /&gt; &amp;quot;How to run locally (modelname) much better than chat-gpt&amp;quot;&lt;br /&gt; &amp;quot;Local Deepseek beats Chat-gpt&amp;quot;&lt;br /&gt; Than the question they ask to those local models are : How many 'R' are in the word Strawberry and the model answer: 2 ... lol &lt;/p&gt; &lt;p&gt;Why is the performance so bad, even on 32B models?&lt;/p&gt; &lt;p&gt;Why are there no proper guides to get the best out of local AI?&lt;br /&gt; Having a big hardware such as the Nvidia project DIGITS will make a big model work close to the online Chat-gpt 3 or 4 ? I see those has 175b parameters. &lt;/p&gt; &lt;p&gt;What are we missing?&lt;/p&gt; &lt;p&gt;I really want to make local AI work as close as the online models, even buying bigger and stronger hardware, but, right now, it just feels like a waste of time.&lt;br /&gt; Has anyone actually succeeded in making these models work well? If so, how? And , what do you intend for Working Well for a local Model ? &lt;/p&gt; &lt;p&gt;Let‚Äôs discuss this! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NikkEvan"&gt; /u/NikkEvan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T16:31:59+00:00</published>
  </entry>
</feed>
