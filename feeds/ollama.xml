<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-08T14:05:18+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ijspg3</id>
    <title>Is it normal for ollama to use CPU when OLLAMA_KEEP_ALIVE=-1</title>
    <updated>2025-02-07T11:15:47+00:00</updated>
    <author>
      <name>/u/gmetothemoongodspeed</name>
      <uri>https://old.reddit.com/user/gmetothemoongodspeed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm using the Windows client and when setting OLLAMA_KEEP_ALIVE=-1 my CPU usage doesn‚Äôt stop at the end of the query. Is this normal? I would say it uses CPU for approximately 5 minutes after the query ends. Then the CPU drops to minimal as expected.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gmetothemoongodspeed"&gt; /u/gmetothemoongodspeed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijspg3/is_it_normal_for_ollama_to_use_cpu_when_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijspg3/is_it_normal_for_ollama_to_use_cpu_when_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijspg3/is_it_normal_for_ollama_to_use_cpu_when_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T11:15:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijxthe</id>
    <title>Ollama setup: GPU load fails.</title>
    <updated>2025-02-07T15:39:30+00:00</updated>
    <author>
      <name>/u/kayakyakr</name>
      <uri>https://old.reddit.com/user/kayakyakr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Final update, for posterity: If you copy/paste a docker_compose.yml file off of the internet and are using an nvidia GPU, make sure you are using the ollama/ollama docker image instead of ollama/ollama:rcom. Hope that this helps someone searching for this issue discover the fix.&lt;/p&gt; &lt;p&gt;&lt;del&gt;Local LLM newb, but not server newb. Been trying to bring ollama up on my server to mess around with. Have it running in a proxmox LXC container, docker hosted, with nvidia-container-toolkit working as expected. I've tested the easy nvidia-smi container, as well as put it through its paces using the dockerized gpu_burn project. Same setup works as a gaming server with the same GPU.&lt;/del&gt;&lt;/p&gt; &lt;p&gt;edit2: a ha. I had copied a compose that was installing rocm, which is for amd processors &amp;gt;_&amp;lt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;edit: I found something that seems weird:&lt;/del&gt; &lt;code&gt; time=2025-02-07T17:00:57.303Z level=INFO source=routes.go:1267 msg=&amp;quot;Dynamic LLM libraries&amp;quot; runners=&amp;quot;[cpu cpu_avx cpu_avx2 rocm_avx]&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;returns only CPU runners, there's no cuda_vXX runner available there like I've seen in other logs&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;old:&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;Ollama finds the GPU and &lt;code&gt;ollama ps&lt;/code&gt; even gives a result of &lt;code&gt;100% GPU&lt;/code&gt; for the loaded model.&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;Best I can tell, these are the relevant lines where it fails to load into GPU and instead switches to CPU:&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt; ollama | time=2025-02-07T05:51:38.953Z level=INFO source=memory.go:356 msg=&amp;quot;offload to cuda&amp;quot; layers.requested=-1 layers.model=29 layers.offload=29 layers.split=&amp;quot;&amp;quot; memory.available=&amp;quot;\[7.7 GiB\]&amp;quot; memory.gpu\_overhead=&amp;quot;0 B&amp;quot; memory.required.full=&amp;quot;2.5 GiB&amp;quot; memory.required.partial=&amp;quot;2.5 GiB&amp;quot; memory.required.kv=&amp;quot;224.0 MiB&amp;quot; memory.required.allocations=&amp;quot;\[2.5 GiB\]&amp;quot; memory.weights.total=&amp;quot;1.5 GiB&amp;quot; memory.weights.repeating=&amp;quot;1.3 GiB&amp;quot; memory.weights.nonrepeating=&amp;quot;236.5 MiB&amp;quot; memory.graph.full=&amp;quot;299.8 MiB&amp;quot; memory.graph.partial=&amp;quot;482.3 MiB&amp;quot; ollama | time=2025-02-07T05:51:38.954Z level=INFO source=server.go:376 msg=&amp;quot;starting llama server&amp;quot; cmd=&amp;quot;/usr/lib/ollama/runners/cpu\_avx2/ollama\_llama\_server runner --model /root/.ollama/models/blobs/sha256-4c132839f93a189e3d8fa196e3324adf94335971104a578470197ea7e11d8e70 --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 28 --parallel 4 --port 39375&amp;quot; ollama | time=2025-02-07T05:51:38.955Z level=INFO source=sched.go:449 msg=&amp;quot;loaded runners&amp;quot; count=2 ollama | time=2025-02-07T05:51:38.955Z level=INFO source=server.go:555 msg=&amp;quot;waiting for llama runner to start responding&amp;quot; ollama | time=2025-02-07T05:51:38.956Z level=INFO source=server.go:589 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server error&amp;quot; ollama | time=2025-02-07T05:51:38.966Z level=INFO source=runner.go:936 msg=&amp;quot;starting go runner&amp;quot; ollama | time=2025-02-07T05:51:38.971Z level=INFO source=runner.go:937 msg=system info=&amp;quot;CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | AARCH64\_REPACK = 1 | cgo(gcc)&amp;quot; threads=28 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;I see the line with &amp;quot;llm server error&amp;quot; but for the life of me, I haven't been able to figure out where I might find that error. Adding OLLAMA_DEBUG doesn't add anything illuminating:&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt; ollama | time=2025-02-07T15:31:26.233Z level=DEBUG source=gpu.go:713 msg=&amp;quot;no filter required for library cpu&amp;quot; ollama | time=2025-02-07T15:31:26.234Z level=INFO source=server.go:376 msg=&amp;quot;starting llama server&amp;quot; cmd=&amp;quot;/usr/lib/ollama/runners/cpu\_avx2/ollama\_llama\_server runner --model /root/.ollama/models/blobs/sha256-4c132839f93a189e3d8fa196e3324adf94335971104a578470197ea7e11d8e70 --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --verbose --threads 28 --parallel 4 --port 41131&amp;quot; ollama | time=2025-02-07T15:31:26.234Z level=DEBUG source=server.go:393 msg=subprocess environment=&amp;quot;\[PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HSA\_OVERRIDE\_GFX\_VERSION='9.0.0' CUDA\_ERROR\_LEVEL=50 LD\_LIBRARY\_PATH=/usr/lib/ollama:/usr/lib/ollama:/usr/lib/ollama/runners/cpu\_avx2\]&amp;quot; ollama | time=2025-02-07T15:31:26.235Z level=INFO source=sched.go:449 msg=&amp;quot;loaded runners&amp;quot; count=1 ollama | time=2025-02-07T15:31:26.235Z level=DEBUG source=sched.go:575 msg=&amp;quot;evaluating already loaded&amp;quot; model=/root/.ollama/models/blobs/sha256-4c132839f93a189e3d8fa196e3324adf94335971104a578470197ea7e11d8e70 ollama | time=2025-02-07T15:31:26.235Z level=INFO source=server.go:555 msg=&amp;quot;waiting for llama runner to start responding&amp;quot; ollama | time=2025-02-07T15:31:26.235Z level=INFO source=server.go:589 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server error&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;host dmesg doesn't contain any error messages. /dev/nvidia-uvm is passed through to all levels.&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;Open to any suggestions that might shed light on the mystery error that's keeping me from using my GPU.&lt;/del&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kayakyakr"&gt; /u/kayakyakr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijxthe/ollama_setup_gpu_load_fails/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijxthe/ollama_setup_gpu_load_fails/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijxthe/ollama_setup_gpu_load_fails/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T15:39:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijye71</id>
    <title>Some good options for a deepseek r1 local interface?</title>
    <updated>2025-02-07T16:03:22+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im using windows sadly, i need something with a Internet search and is light but is also totally free/local.&lt;/p&gt; &lt;p&gt;What are some local choices i should look into that would work great with r1?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijye71/some_good_options_for_a_deepseek_r1_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijye71/some_good_options_for_a_deepseek_r1_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijye71/some_good_options_for_a_deepseek_r1_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T16:03:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij7nuo</id>
    <title>UNCENSORED AI MODELS</title>
    <updated>2025-02-06T17:12:07+00:00</updated>
    <author>
      <name>/u/yng_kydd</name>
      <uri>https://old.reddit.com/user/yng_kydd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some months ago i tried for the first time wizard vicuna and i was ok with it being a lil slow and not that optimized, i wasn't even complaining cause at least i had an AI uncensored, something i could ask for everything.&lt;/p&gt; &lt;p&gt;This week i've seen a post talking about other new models that are pretty much better like tiger gemma, dolphin and others&lt;/p&gt; &lt;p&gt;i've been searching about this for quite a lot and i'd want to ask y'all what is the best uncensored AI model right now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yng_kydd"&gt; /u/yng_kydd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij7nuo/uncensored_ai_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij7nuo/uncensored_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij7nuo/uncensored_ai_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T17:12:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikb7nq</id>
    <title>New Proyect...</title>
    <updated>2025-02-08T01:11:57+00:00</updated>
    <author>
      <name>/u/tech215</name>
      <uri>https://old.reddit.com/user/tech215</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Would it be a good idea to create something lightweight and would you use it?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1ikb7nq"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tech215"&gt; /u/tech215 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikb7nq/new_proyect/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikb7nq/new_proyect/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ikb7nq/new_proyect/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T01:11:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij2pw7</id>
    <title>üéâ Being Thankful for Everyone Who Made This Project a Super Hit! üöÄ</title>
    <updated>2025-02-06T13:37:59+00:00</updated>
    <author>
      <name>/u/akhilpanja</name>
      <uri>https://old.reddit.com/user/akhilpanja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are thrilled to announce that our project, DeepSeek-RAG-Chatbot, has officially hit 100 stars on GitHub repo: &lt;a href="https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot.git"&gt;https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot.git&lt;/a&gt; üåü‚ú® &lt;/p&gt; &lt;p&gt;This journey has been incredible, and we couldn‚Äôt have achieved this milestone without the support of our amazing community. Your contributions, feedback, and enthusiasm have helped shape this project into what it is today!&lt;/p&gt; &lt;p&gt;üîç Performance Boost The graph above showcases the significant improvements in Graph Context Relevancy and Graph Context Recall after integrating GraphRAG and further advancements. Our system is now more accurate, contextually aware, and efficient in retrieving relevant information.&lt;/p&gt; &lt;p&gt;We are committed to making this project even better and look forward to the next milestones! üöÄ&lt;/p&gt; &lt;p&gt;Thank you all once again for being part of this journey. Let‚Äôs keep building together! üí°üî• Ôøº&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akhilpanja"&gt; /u/akhilpanja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij2pw7/being_thankful_for_everyone_who_made_this_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij2pw7/being_thankful_for_everyone_who_made_this_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij2pw7/being_thankful_for_everyone_who_made_this_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T13:37:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijs34u</id>
    <title>LLMs as Embeddings?</title>
    <updated>2025-02-07T10:33:09+00:00</updated>
    <author>
      <name>/u/Better-Designer-8904</name>
      <uri>https://old.reddit.com/user/Better-Designer-8904</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ijs34u/llms_as_embeddings/"&gt; &lt;img alt="LLMs as Embeddings?" src="https://b.thumbs.redditmedia.com/ytaJOU70XQ_OdGIs6ff3q5cDHpWWu6pRAi8DYLo9CwY.jpg" title="LLMs as Embeddings?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/pu81il0r2phe1.png?width=832&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=539f8ec6328553c328a6bced0c730e87952fb5ba"&gt;https://preview.redd.it/pu81il0r2phe1.png?width=832&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=539f8ec6328553c328a6bced0c730e87952fb5ba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been using LangChain to run LLMs as embeddings through Ollama, and it actually works pretty well. But I‚Äôm kinda wondering‚Ä¶ how does it actually work? And does it even make sense to use an LLM for embeddings instead of a dedicated model?&lt;/p&gt; &lt;p&gt;If anyone understands the details, I‚Äôd love an explanation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Better-Designer-8904"&gt; /u/Better-Designer-8904 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijs34u/llms_as_embeddings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijs34u/llms_as_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijs34u/llms_as_embeddings/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T10:33:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik4grv</id>
    <title>Is there something similar to operator what runs locally?</title>
    <updated>2025-02-07T20:13:25+00:00</updated>
    <author>
      <name>/u/Anyusername7294</name>
      <uri>https://old.reddit.com/user/Anyusername7294</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would love to try operator, but $200/month is too much for me. Also I don't want to give access to my entire computer to OpenAI. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anyusername7294"&gt; /u/Anyusername7294 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik4grv/is_there_something_similar_to_operator_what_runs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik4grv/is_there_something_similar_to_operator_what_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ik4grv/is_there_something_similar_to_operator_what_runs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T20:13:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijm8m0</id>
    <title>Dora - Local Drive Semantic Search</title>
    <updated>2025-02-07T03:59:52+00:00</updated>
    <author>
      <name>/u/ranoutofusernames__</name>
      <uri>https://old.reddit.com/user/ranoutofusernames__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Sharing Dora, an alternative to the Mac Explorer app that I wrote today so you can retrieve files using natural language. It runs a local crawler at the target directory to index file names and paths recursively, embeds them and then lets you retrieve them using a chat window (semantic search). You can then open the files directly from the results as well.&lt;/p&gt; &lt;p&gt;It runs completely local and no data is sent out.&lt;/p&gt; &lt;p&gt;Adding file content embedding for plaintext, PDFs and images on the next update for even better results. The goal is to do deep-research with local files eventually.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/space0blaster/dora"&gt;https://github.com/space0blaster/dora&lt;/a&gt;&lt;/p&gt; &lt;p&gt;License: MIT&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ranoutofusernames__"&gt; /u/ranoutofusernames__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijm8m0/dora_local_drive_semantic_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijm8m0/dora_local_drive_semantic_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijm8m0/dora_local_drive_semantic_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T03:59:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik4xrz</id>
    <title>Help picking a GPU</title>
    <updated>2025-02-07T20:33:38+00:00</updated>
    <author>
      <name>/u/phantom6047</name>
      <uri>https://old.reddit.com/user/phantom6047</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking to start messing around with llms and ollama and need to purchase a gpu for my machine. I am running a Precision t7810 with dual E5-2690 cpus and 256gb 2400 ECC ram. The psu in this machine has only one free 8 pin connector and I originally hoped to purchase a 4070 as that seemed to be my best option, but I've realized that getting ahold of a 4070 is practically impossible. There's no used market around me with anything nvidia for sale so that's out too. I'm hoping to get something with lots of vram that will also hold up well for some light 2k gaming, and I've pretty much settled on a 7800xt. &lt;/p&gt; &lt;p&gt;I run arch on my systems and whatever gpu I get will be passed through to a windows vm for gaming or another arch vm/docker configuration for llms. &lt;/p&gt; &lt;p&gt;At this point I'm about to pull the trigger on a newegg deal for a 7800xt and psu for $550, pretty much maxing out my budget. I'm looking to hear your thoughts on how well this would or wouldn't work and if I should consider something else. Look forward to your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phantom6047"&gt; /u/phantom6047 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik4xrz/help_picking_a_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik4xrz/help_picking_a_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ik4xrz/help_picking_a_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T20:33:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikik9q</id>
    <title>Buy GeForce RTX 5090 BLACK POWER X3 32GB for coding yes no ??</title>
    <updated>2025-02-08T08:15:34+00:00</updated>
    <author>
      <name>/u/Lumpy_Part_1767</name>
      <uri>https://old.reddit.com/user/Lumpy_Part_1767</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey if I buy Pc with rtx 4090 I will run good models locally ? For coding Or even MEGA GeForce RTX 5090 BLACK POWER X3 32GB I ü§∑ dk&lt;/p&gt; &lt;p&gt;Also What if I can use the gpu other things like rendering montage videos with after effects or training models like Moe and try to create dedicated model for specific language or pattern coding like svelte lang?&lt;/p&gt; &lt;p&gt;Or create YouTube videos about using llm and trying different things about AI?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lumpy_Part_1767"&gt; /u/Lumpy_Part_1767 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikik9q/buy_geforce_rtx_5090_black_power_x3_32gb_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikik9q/buy_geforce_rtx_5090_black_power_x3_32gb_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ikik9q/buy_geforce_rtx_5090_black_power_x3_32gb_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T08:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijwoxn</id>
    <title>Exposing ollamas 11434 port for api use</title>
    <updated>2025-02-07T14:50:30+00:00</updated>
    <author>
      <name>/u/epigen01</name>
      <uri>https://old.reddit.com/user/epigen01</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys ive been using ngrok (free) for use on my homelab but the monthly limit for http requests was just hit ( i didnt know about that).&lt;/p&gt; &lt;p&gt;Any free alternatives to ngrok? Ideally something easy (otherwise i might have to use tailscale)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/epigen01"&gt; /u/epigen01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijwoxn/exposing_ollamas_11434_port_for_api_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijwoxn/exposing_ollamas_11434_port_for_api_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijwoxn/exposing_ollamas_11434_port_for_api_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T14:50:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik8i7x</id>
    <title>What type of models can my machine run, any coding models?</title>
    <updated>2025-02-07T23:06:14+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ik8i7x/what_type_of_models_can_my_machine_run_any_coding/"&gt; &lt;img alt="What type of models can my machine run, any coding models?" src="https://b.thumbs.redditmedia.com/4U4r8A_jnxDMPM5aPlZjCwocQ_o7ifejWkeDLm6HnWQ.jpg" title="What type of models can my machine run, any coding models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im mostly looking for a model for coding and general questions. im just not sure what the largest model i can run is while still having a ok speed. any suggestions would be great, also i know my machine isint the greatest thing out there. Ive tried some 7b models or less but i feel they are not powerful enough.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5mftuja6tshe1.png?width=373&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f94fc8466e34c449a1f649fa53d8b28ebe023503"&gt;https://preview.redd.it/5mftuja6tshe1.png?width=373&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f94fc8466e34c449a1f649fa53d8b28ebe023503&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z3s2cyi6tshe1.png?width=374&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cff5d8bdb2d9f0fc303a8f90c7d06320159943ca"&gt;https://preview.redd.it/z3s2cyi6tshe1.png?width=374&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cff5d8bdb2d9f0fc303a8f90c7d06320159943ca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5ojhh3q6tshe1.png?width=403&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e5bbcb30bd63218d5bc0d482c877a66309f9aae3"&gt;https://preview.redd.it/5ojhh3q6tshe1.png?width=403&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e5bbcb30bd63218d5bc0d482c877a66309f9aae3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik8i7x/what_type_of_models_can_my_machine_run_any_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik8i7x/what_type_of_models_can_my_machine_run_any_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ik8i7x/what_type_of_models_can_my_machine_run_any_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T23:06:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik1p0d</id>
    <title>Local Cursor.ai</title>
    <updated>2025-02-07T18:18:19+00:00</updated>
    <author>
      <name>/u/Kind_Ad_2866</name>
      <uri>https://old.reddit.com/user/Kind_Ad_2866</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since cursor only supports online models such as Claude and OpenAI, I‚Äôm surprised no one has created an alternative for local models yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind_Ad_2866"&gt; /u/Kind_Ad_2866 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik1p0d/local_cursorai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik1p0d/local_cursorai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ik1p0d/local_cursorai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T18:18:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik6nml</id>
    <title>My Chat App Supports DeepSeek-R1 &amp; Works on All Platforms Now [Open Source]</title>
    <updated>2025-02-07T21:46:10+00:00</updated>
    <author>
      <name>/u/pozitronx</name>
      <uri>https://old.reddit.com/user/pozitronx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ik6nml/my_chat_app_supports_deepseekr1_works_on_all/"&gt; &lt;img alt="My Chat App Supports DeepSeek-R1 &amp;amp; Works on All Platforms Now [Open Source]" src="https://external-preview.redd.it/kifZ0Ldmdn6gGLz2rDPo3mLeb0SO0q-89k2FTXEJFUo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=865102b36718797f6b74ce3c7a0bbe6bfb735df2" title="My Chat App Supports DeepSeek-R1 &amp;amp; Works on All Platforms Now [Open Source]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, last month I share my app &lt;strong&gt;Reins: Chat for Ollama&lt;/strong&gt;. It simplifies configurations of conversations like customizing system prompt per chat or tweaking advanced options. Now, it shows DeepSeek-R1 thought messages separately and runs on iOS, Android, macOS and Windows. You can learn more from &lt;a href="https://github.com/ibrahimcetin/reins"&gt;GitHub&lt;/a&gt; and the &lt;a href="https://www.reddit.com/r/ollama/comments/1hv1vlh/opensource_app_for_easy_ollama_chat_configuration/"&gt;previous post&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://apps.apple.com/tr/app/reins-chat-for-ollama/id6739738501"&gt;iOS App&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ibrahimcetin/reins/releases/tag/1.2.0"&gt;Android App&lt;/a&gt; (I need testers for Google Play. If you send me your Google Play email, I will add you testers and send the link of the app. Your help is much appreciated.)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ibrahimcetin/reins/releases/tag/1.2.0"&gt;Windows App&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ibrahimcetin/reins/releases/tag/1.2.0"&gt;MacOS App&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ibrahimcetin/reins"&gt;GitHub Link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1mpito51fshe1.png?width=3618&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09b0d28f7e5c9dd3586733fd1048c135bcc1edca"&gt;https://preview.redd.it/1mpito51fshe1.png?width=3618&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09b0d28f7e5c9dd3586733fd1048c135bcc1edca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j7d859g0cshe1.png?width=2544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3238936ae201b52c15ca39de17ac01ff3c109625"&gt;https://preview.redd.it/j7d859g0cshe1.png?width=2544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3238936ae201b52c15ca39de17ac01ff3c109625&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/453hndz0cshe1.png?width=2546&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31c585b337a6dbaf7071132b3ffad73334da3013"&gt;https://preview.redd.it/453hndz0cshe1.png?width=2546&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31c585b337a6dbaf7071132b3ffad73334da3013&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pozitronx"&gt; /u/pozitronx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik6nml/my_chat_app_supports_deepseekr1_works_on_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik6nml/my_chat_app_supports_deepseekr1_works_on_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ik6nml/my_chat_app_supports_deepseekr1_works_on_all/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T21:46:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikjrmn</id>
    <title>Does the RTX 4090 in EOF 2025 price will decrease??</title>
    <updated>2025-02-08T09:40:50+00:00</updated>
    <author>
      <name>/u/Lumpy_Part_1767</name>
      <uri>https://old.reddit.com/user/Lumpy_Part_1767</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lumpy_Part_1767"&gt; /u/Lumpy_Part_1767 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikjrmn/does_the_rtx_4090_in_eof_2025_price_will_decrease/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikjrmn/does_the_rtx_4090_in_eof_2025_price_will_decrease/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ikjrmn/does_the_rtx_4090_in_eof_2025_price_will_decrease/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T09:40:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik3m98</id>
    <title>How do I make chatting about documents not suck?</title>
    <updated>2025-02-07T19:37:51+00:00</updated>
    <author>
      <name>/u/cunasmoker69420</name>
      <uri>https://old.reddit.com/user/cunasmoker69420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;: Asking various 22b-32b sized models questions about an insurance policy document that is about 40 pages long&lt;/p&gt; &lt;p&gt;The various models I've tried mostly fail miserably, often telling me the information I'm looking for is not in the document (it is) or returning incomplete information. &lt;/p&gt; &lt;p&gt;I'm assuming I'm doing something wrong since other people rave about using their local LLMs for document analysis. I guess its probably not as simple as uploading documents and asking away, so I'm grateful for any advice&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models tried&lt;/strong&gt;: gemma2, mistral, &amp;quot;deepseek-r1&amp;quot;, qwen2-5, and more&lt;/p&gt; &lt;p&gt;EDIT: I am learning now that there is indeed a lot more to this than just using Open WebUI and uploading documents and chatting about them. I have learned the defaults in Open WebUI with Ollama are very basic and more research needs to be done on on my part configuring embedding models and reranking models beyond the default settings.&lt;/p&gt; &lt;p&gt;If anyone has a guide they can point me to that would be great&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cunasmoker69420"&gt; /u/cunasmoker69420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik3m98/how_do_i_make_chatting_about_documents_not_suck/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik3m98/how_do_i_make_chatting_about_documents_not_suck/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ik3m98/how_do_i_make_chatting_about_documents_not_suck/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T19:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikkd12</id>
    <title>Context window syntax per ollama model?</title>
    <updated>2025-02-08T10:23:33+00:00</updated>
    <author>
      <name>/u/Private-Citizen</name>
      <uri>https://old.reddit.com/user/Private-Citizen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;The Setup&lt;/h1&gt; &lt;p&gt;I am using ollama to pull and run models on linux. I wrote my own Web UI (httpd/php) to prompt the models instead of using the cli. The webpage connects to the API using:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$data = json_encode([&amp;quot;model&amp;quot; =&amp;gt; $llm, &amp;quot;prompt&amp;quot; =&amp;gt; $prompt, &amp;quot;stream&amp;quot; =&amp;gt; true]); $ch = curl_init(&amp;quot;http://localhost:11434/api/generate&amp;quot;); ... curl_setopt($ch, CURLOPT_POSTFIELDS, $data); ... curl_exec($ch); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I tried feeding the context window into the &amp;quot;messages&amp;quot; parameter like the following which didn't work, the model kept seeing every prompt as new with no history.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# $history['role'] = &amp;quot;User&amp;quot;; # $history['content'] = &amp;quot;prompt text&amp;quot;; $data = json_encode([&amp;quot;model&amp;quot; =&amp;gt; $llm, &amp;quot;messages&amp;quot; =&amp;gt; $history &amp;quot;prompt&amp;quot; =&amp;gt; $prompt, &amp;quot;stream&amp;quot; =&amp;gt; true]); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I asked GPT who said ollama requires the context windows to be fed into the prompt. So why does &amp;quot;messages&amp;quot; exist?&lt;/p&gt; &lt;p&gt;I started appending the $history to each &amp;quot;prompt&amp;quot; instead of sending it in &amp;quot;messages&amp;quot;. GPT gave me two formats to try, each with different degrees of success depending on the model.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;System: This is a rule to follow. User: prompt question? Assistant: response. User: Next question? Assistant: next response. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And also this format:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;|begin_of_text|&amp;gt; &amp;lt;|start_header_id|&amp;gt;system&amp;lt;|end_header_id|&amp;gt; This is a system message. &amp;lt;|eot_id|&amp;gt; &amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt; Question? &amp;lt;|eot_id|&amp;gt; &amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt; Response. &amp;lt;|eot_id|&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Neither of these are perfect as some of the times the models will start putting some of the tags in the raw response like Putting &lt;code&gt;Assistant:&lt;/code&gt; before its response or adding &lt;code&gt;&amp;lt;|eot_id|&amp;gt;&lt;/code&gt; to the end. Or sometimes including the entire &lt;code&gt;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;&lt;/code&gt; header in the response.&lt;/p&gt; &lt;h1&gt;Question:&lt;/h1&gt; &lt;p&gt;How do i use use the info provided on ollama to structure the context window for the selected model? I see the params and template links, is that it? I wasn't able to make sense of how to use them. Is &amp;quot;messages&amp;quot; really useless? Is appending to the &amp;quot;prompt&amp;quot; the way&amp;quot;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Private-Citizen"&gt; /u/Private-Citizen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikkd12/context_window_syntax_per_ollama_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikkd12/context_window_syntax_per_ollama_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ikkd12/context_window_syntax_per_ollama_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T10:23:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikkzt2</id>
    <title>Does this Pc worthy 4.6m??</title>
    <updated>2025-02-08T11:07:49+00:00</updated>
    <author>
      <name>/u/Lumpy_Part_1767</name>
      <uri>https://old.reddit.com/user/Lumpy_Part_1767</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MSI PROSPECT 700R Gaming Case / Black - Intel Core i9-14900KF 14th Gen Processor (up to 5.8 GHz, 36 MB cache) - 128 GB (4x 32 GB) DDR5 Memory - 2 TB M.2 SSD - MSI GeForce RTX 4090 SUPRIM LIQUID X Graphics Card, 24 GB Dedicated GDDR6X Memory - MSI MAG Z790 TOMAHAWK WIFI Motherboard / ATX / DDR5 - MSI MEG CORELIQUID S360 Water Cooling Kit - MSI MAG A1250GL PCIE5 Power Supply / 80 PLUS Gold / 1250 W / Black - 1-Year Warranty&lt;/p&gt; &lt;p&gt;Price : 4.6 million USD.&lt;/p&gt; &lt;p&gt;Why ollama ? I need good pc for various tasks like coding, rendering videos, running models via ollama and openwebui locally...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lumpy_Part_1767"&gt; /u/Lumpy_Part_1767 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikkzt2/does_this_pc_worthy_46m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikkzt2/does_this_pc_worthy_46m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ikkzt2/does_this_pc_worthy_46m/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T11:07:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijrwas</id>
    <title>Best LLM for Coding</title>
    <updated>2025-02-07T10:20:01+00:00</updated>
    <author>
      <name>/u/anshul2k</name>
      <uri>https://old.reddit.com/user/anshul2k</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for LLM for coding i got 32GB ram and 4080&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anshul2k"&gt; /u/anshul2k &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijrwas/best_llm_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijrwas/best_llm_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijrwas/best_llm_for_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T10:20:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ika79l</id>
    <title>Help with testing the 0.5.8 pre-release</title>
    <updated>2025-02-08T00:23:25+00:00</updated>
    <author>
      <name>/u/jmorganca</name>
      <uri>https://old.reddit.com/user/jmorganca</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;The next version of Ollama has overhauled how acceleration libraries are packaged. This adds support for non-AVX+GPU and AVX2+GPU combos. It also adds AVX512 instruction support for high-end CPUs like the AMD threadripper processors.&lt;/p&gt; &lt;p&gt;We'd love your help testing it out before marking it as a final release to work out any kinks with GPU support. You can download it here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.5.8-rc11"&gt;https://github.com/ollama/ollama/releases/tag/v0.5.8-rc11&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On Linux, you can run:&lt;/p&gt; &lt;p&gt;&lt;code&gt; curl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.5.8-rc11 sh &lt;/code&gt;&lt;/p&gt; &lt;p&gt;For Docker, you can use the following command to pull the new version:&lt;/p&gt; &lt;p&gt;&lt;code&gt; docker pull ollama/ollama:0.5.8-rc11 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;or for ROCm users:&lt;/p&gt; &lt;p&gt;&lt;code&gt; docker pull ollama/ollama:0.5.8-rc11-rocm &lt;/code&gt;&lt;/p&gt; &lt;p&gt;If you hit any issues feel free to DM me or create a GitHub issue letting us know you are on the 0.5.8 RC version. Thanks so much!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jmorganca"&gt; /u/jmorganca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ika79l/help_with_testing_the_058_prerelease/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ika79l/help_with_testing_the_058_prerelease/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ika79l/help_with_testing_the_058_prerelease/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T00:23:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijx0ll</id>
    <title>PDF to JSON</title>
    <updated>2025-02-07T15:04:34+00:00</updated>
    <author>
      <name>/u/hotdone</name>
      <uri>https://old.reddit.com/user/hotdone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ijx0ll/pdf_to_json/"&gt; &lt;img alt="PDF to JSON" src="https://b.thumbs.redditmedia.com/XWID2_aUBd2Bw1zOtjvRHQY1gy9OSXlnPe9v7pfEUUc.jpg" title="PDF to JSON" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, i am looking for guidance on how i can upload a pdf file and get the contents in JSON code. For example, a new patient form that asks for name, address and phone number. This is a pdf that I will feed to the LLM and then I would like it to analyze the content of the form and output code. Like in the picture. That code will output a fillable textbox call First Name. Currently i am looking at a pdf form that was provided to me and I have to translate it into code, like the example in the picture. For each item in the new patient form. First name, last name, address etc. This is a very time consuming process at the moment and I would like to see if i can use AI to optimize it. Thank you, any help/advise is appreciated &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hotdone"&gt; /u/hotdone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ijx0ll"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijx0ll/pdf_to_json/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijx0ll/pdf_to_json/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T15:04:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikk762</id>
    <title>Roadmap for supervised fine tuning</title>
    <updated>2025-02-08T10:11:51+00:00</updated>
    <author>
      <name>/u/Forward_Friend_2078</name>
      <uri>https://old.reddit.com/user/Forward_Friend_2078</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am planning to fine tune a model for using it on an insurance domain. Please provide some tutorials that I can learn and follow while working on it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Forward_Friend_2078"&gt; /u/Forward_Friend_2078 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikk762/roadmap_for_supervised_fine_tuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikk762/roadmap_for_supervised_fine_tuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ikk762/roadmap_for_supervised_fine_tuning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T10:11:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikmb2i</id>
    <title>Single GPU with more VRAM or split between two?</title>
    <updated>2025-02-08T12:34:39+00:00</updated>
    <author>
      <name>/u/Fluffy_Sun1498</name>
      <uri>https://old.reddit.com/user/Fluffy_Sun1498</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I am relatively new to LLMs and local AI. But it seems to me like a lot of fun and a potential to have a fully locally running assistant that would be enough for simple tasks (like assisting in smart home). I have read a lot of articles about it and ran a few models myself, so I am not a total newbie to the topic, but there is one thing I cannot find info about anywhere. I am putting myself a workstation for different tasks, kinda &amp;quot;home server&amp;quot; and I am planning on buying GPU (s) for LLM. Now after this rather long introduction, let me share the &lt;strong&gt;main problem&lt;/strong&gt;, I have two choices: Nvidia Quadro M6000 with 24GB of VRAM and Nvidia RTX Quadro A2000 with 12GB of VRAM. Why those two? Because when it comes to the price, it is roughly that one M6000 costs the same as two A2000 (Used) and overall they are relatively cheap (affordable by me). I know you can split the model between two GPUS, but I've read somewhere that this is not efficient if you don't have nvlink. The A2000 is much newer and has higher compute capability than the M6000, but to get &amp;quot;the same&amp;quot; amount of VRAM I should buy two A2000. Also, A2000 has much lower TDP. I planned on buying a single A2000 to run models no bigger than 10b, and later, if I'd ever love to run bigger models, just buy a second one. The question based on all previous text is, if I buy a single A2000 and later buy another one, will this be worse\simmilar\better experience than buying a single M6000?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluffy_Sun1498"&gt; /u/Fluffy_Sun1498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikmb2i/single_gpu_with_more_vram_or_split_between_two/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikmb2i/single_gpu_with_more_vram_or_split_between_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ikmb2i/single_gpu_with_more_vram_or_split_between_two/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T12:34:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikjn89</id>
    <title>For those wondering about instruct models vs text models. The difference is huge!</title>
    <updated>2025-02-08T09:31:53+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've heard some say it's not too important whether you use instruct or text model. I asked many questions about this and all answers were sort of confusing. So I decided to test it for myself. And, oh boy, the difference is huge. Basically instruct models behave like you'd expect an AI assistant to behave. Text models, on the other hand, literally try to continue what you were writing, literally predicting next word.&lt;/p&gt; &lt;p&gt;So in my case, it understood my prompt to be the beginning of a dialogue, and what it did was simulate the rest of the dialogue, in rather incoherent way. Anyway, here's how it went:&lt;/p&gt; &lt;p&gt;I used the same prompt which is: &lt;strong&gt;Hello! Do you know of some psychological experiments that I could do on myself?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I tested it on the same model, of the same size, with the same quantization, the only difference being instruct vs text.&lt;/p&gt; &lt;p&gt;So here's how &lt;strong&gt;llama3.2:3b-instruct-q8_0 replied&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;em&gt;&amp;quot;While it's essential to consult with a mental health professional before conducting any experiment, especially if&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;you have any underlying conditions or concerns. That being said, here are some simple and relatively safe&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;self-experiments you can try:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;1. \&lt;/em&gt;*The Forer Effect**: Have someone describe you in vague terms, saying things like &amp;quot;You're imaginative, sensitive, and enjoy exploring new ideas.&amp;quot; Then, ask yourself how many of those traits resonate with you. This experiment demonstrates the power of confirmation bias.*&lt;/p&gt; &lt;p&gt;&lt;em&gt;2. \&lt;/em&gt;*The Baader-Meinhof Phenomenon**: Start noticing a specific word or concept (e.g., &amp;quot;sustainable living&amp;quot;) in your daily life. Write it down whenever you see it, and track its frequency over time. This experiment explores how our brains become attuned to patterns and concepts that interest us.*&lt;/p&gt; &lt;p&gt;&lt;em&gt;3. \&lt;/em&gt;*The Stroop Task**: Look at a series of words on the screen, such as &amp;quot;red,&amp;quot; &amp;quot;blue,&amp;quot; or &amp;quot;green.&amp;quot; Then, quickly try to name the color each word is associated with (e.g., &amp;quot;red&amp;quot; would be answered with &amp;quot;red&amp;quot;). This experiment assesses how our brains process conflicting information.*&lt;/p&gt; &lt;p&gt;&lt;em&gt;4. \&lt;/em&gt;*The Zeigarnik Effect**: Write a story with an unresolved plot point (e.g., &amp;quot;As I walked into the room, I saw my best friend standing alone...&amp;quot;). Then, try to remember where you left off. Later, write down what happened next. This experiment shows how our brains tend to retain information that's incomplete or unfinished.*&lt;/p&gt; &lt;p&gt;&lt;em&gt;5. \&lt;/em&gt;*The Placebo Effect**: Take a pain reliever without knowing if it's actually effective (a placebo). Rate the level of discomfort or pain before and after taking the &amp;quot;medication.&amp;quot; If you experience a reduction in symptoms, this might demonstrate the power of our brains' expectations on our physical well-being.*&lt;/p&gt; &lt;p&gt;&lt;em&gt;Remember to:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;\&lt;/em&gt; Only conduct these experiments if you're feeling physically and mentally well.*&lt;/p&gt; &lt;p&gt;&lt;em&gt;\&lt;/em&gt; Be aware of any underlying conditions that may be affected by these self-experiments (e.g., anxiety, depression).*&lt;/p&gt; &lt;p&gt;&lt;em&gt;\&lt;/em&gt; Don't push yourself too hard or try to force specific outcomes.*&lt;/p&gt; &lt;p&gt;&lt;em&gt;\&lt;/em&gt; Consider discussing your results with a mental health professional for interpretation and support. Keep in mind that while self-experiments can be interesting and insightful, they shouldn't replace professional psychological assessments or treatment.&amp;quot;*&lt;/p&gt; &lt;p&gt;And this is how &lt;strong&gt;llama3.2:3b-text-q8_0&lt;/strong&gt; replied:&lt;/p&gt; &lt;p&gt;&lt;em&gt;I'm just curious, but i think it would be interesting to me. I don't want to hurt myself though.. so please give good suggestions. Thanks!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Have you ever been in a state of flow, where time stops and all of your senses are completely present? If not, try doing the following exercises.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;This is the first exercise that I recommend because this will allow you to discover more about yourself.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;1. Close your eyes and think of something that makes you happy. Try thinking of it as intensely as possible (don't worry if the thought isn't positive or doesn't make sense). Do this for 2-3 minutes.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;2. Now, try doing a free-form association exercise. Think of something that's bothering you. As soon as an image pops up in your head, write down what it is immediately, without thinking about it too much (like the image I wrote above).&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;What you just did was experience a flow state. A flow state is like a trance-like state where time seems to stop and all of your senses are present.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I also recommend reading Flow: The Psychology of Optimal Experience by Mihaly Csikszentmihalyi. It's an easy read that will allow you to understand why people get into flow states and how to achieve them more often (I've already finished the book, and I'm sure you'll love it).&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Let me know if this helped! If not, please let me know in the comments.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Thank you for your post. I find it very interesting because I am currently doing a project on flow theory and have been researching into this area of psychology for some time now. I can tell that you are very interested in flow theories, as well as other areas of psychology. It's nice to see someone else sharing their knowledge and interests!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I'm glad you liked the article! I really enjoy flow experiences. It makes me feel like nothing else matters for a little while.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I am doing research on positive psychology too - my major is psychology and I was wondering if you could recommend some online resources that would be helpful to me as I prepare for an exam? Also, do you know of any famous psychologists who have contributed in the field?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Hi there! It's nice to meet you. First off, it's great that you're interested in positive psychology!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I don't know much about positive psychology (apart from what is presented in my university's curriculum), but I will definitely try to find some online resources for you.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;You can check out this Wikipedia article on famous psychologists who contributed in the field. I'm sure it contains a lot of names that you may not have heard before, so give it a look!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Also, here are some more useful links (this one focuses on famous positive psychologist).&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I hope these help! Let me know if they don't work for you. Thanks again.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;You're welcome! Feel free to leave any questions you might have about positive psychology in the comments section below. I'll do my best to answer them!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;P.S. - If anyone else is interested, please feel free to share this article with your friends and/or family members!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I really like this page and the info it contains. It's very helpful! Thanks for posting this.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;You're welcome! Feel free to leave any other comments or questions you may have in the comment section below. I'll do my best to respond as quickly as possible!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;P.S. - If anyone else is interested, feel free to share this page with your friends and/or family members.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Wow, thank you for sharing this great article on flow psychology! This site has some interesting information about the effects of flow experiences on one's life. It's been a while since I've read anything from this perspective so it was nice to come across something like that again. Keep up the good work!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Hi there! I'm glad that you found my article helpful.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Thanks for sharing your thoughts as well!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;P.S. &lt;/p&gt; &lt;p&gt;Another thing that I noticed with text models is that they sometimes just start with prompt, but then they take their thoughts in completely unpredictable directions, and they can keep going on, and on, generating extremely long replies. So when I asked &lt;strong&gt;llama3.2:3b-text-q8_0&lt;/strong&gt; &amp;quot;What is the solution for the war in Ukraine&amp;quot;, &lt;strong&gt;it generated 2124 words long answer!&lt;/strong&gt; And only first paragraph had something to do with war. I'll copy just a part of it here so that you can see:&lt;/p&gt; &lt;p&gt;&lt;em&gt;For that, we need to ask ourselves a more fundamental question. Why do people hate each other so much? Many of our political leaders have been saying ‚ÄúIt‚Äôs not the war; it‚Äôs the weapons‚Äù as if weapons are just something we use and get rid of when they‚Äôre no longer useful. We also see this with the United States‚Äô response to the invasion of Ukraine by Russia ‚Äì many Americans, including politicians, say that President Biden has done a great job in supporting our allies. But what is really going on here? Why do people hate each other so much?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Let‚Äôs look at how hatred works. If you think about it, all humans have emotions: anger, fear, love and happiness. But we also see these feelings expressed as being ‚Äúgood‚Äù or ‚Äúbad.‚Äù This means that in order to be able to love someone else (or something), we need first of all to have a negative feeling towards them ‚Äì either hate or fear.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Now let‚Äôs take another look at hatred ‚Äì what it is and how it works. Hatred is actually more complex than anger or fear; indeed, there are many different types of hatred! For example:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;1) You may feel that someone has hurt you in some way (e.g., by not being fair to others), and then you want revenge.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;2) You may see yourself as better than another person because of how successful they‚Äôve been at achieving something ‚Äì e.g., making money or having a good career ‚Äì and therefore think negatively about them when faced with challenges themselves; this kind of hatred can lead people into thinking things like: ‚ÄúI know I‚Äôm doing the right thing, but he‚Äôs getting away with it!‚Äù&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;3) You may be jealous because someone else seems to have more than you do (e.g., money or status), and so want them to suffer as a result ‚Äì e.g., taking revenge on them by stealing their possessions!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;It‚Äôs interesting that there are actually two different ways in which people express hatred: through violence and through words. The former is what we call physical aggression, while the latter is verbal abuse.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;As an example of how this works in practice, imagine if someone had been mistreated by their parents as a child ‚Äì e.g., being physically abused or not allowed to do anything they wanted because it was ‚Äúwrong‚Äù...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I just copied the first page out of 7. It just kept on discussing different kinds of love and hate, for seven pages of text in MS Word.&lt;/p&gt; &lt;p&gt;In a way, text models are more like humans, they have their own will, they don't obey the prompt, they talk about what they want to talk about, and they are less politically correct.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikjn89/for_those_wondering_about_instruct_models_vs_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikjn89/for_those_wondering_about_instruct_models_vs_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ikjn89/for_those_wondering_about_instruct_models_vs_text/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T09:31:53+00:00</published>
  </entry>
</feed>
