<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-18T15:35:56+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1irzuyw</id>
    <title>Exposing ollama to internet</title>
    <updated>2025-02-18T00:42:01+00:00</updated>
    <author>
      <name>/u/Right_Positive5886</name>
      <uri>https://old.reddit.com/user/Right_Positive5886</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have managed to run ollama with open-webgui on a Linode VM. 32gb ram 1 tb memory and 8 core cpu. I have fronted it with nginx proxy with let‚Äôs encrypt certs. The application is up unfortunately it works for only small prompts bigger prompts the app is erroring out . It doesn‚Äôt matter whether I‚Äôm running a large model or a small one ( atm Deepseek 1.5 B ) would anyone know what is missing ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Right_Positive5886"&gt; /u/Right_Positive5886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irzuyw/exposing_ollama_to_internet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irzuyw/exposing_ollama_to_internet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irzuyw/exposing_ollama_to_internet/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T00:42:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1irjave</id>
    <title>OpenThinker-32B-FP16 + 8x AMD Instinct Mi60 Server + vLLM + Tensor Parallelism</title>
    <updated>2025-02-17T13:01:13+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/klcjcd1d6pje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irjave/openthinker32bfp16_8x_amd_instinct_mi60_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irjave/openthinker32bfp16_8x_amd_instinct_mi60_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T13:01:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1irnuqw</id>
    <title>Ollama shouldn't spam update fetch requests every second or two when it already has the update downloaded...</title>
    <updated>2025-02-17T16:29:29+00:00</updated>
    <author>
      <name>/u/ZdrytchX</name>
      <uri>https://old.reddit.com/user/ZdrytchX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;time=2025-02-16T03:53:53.276+08:00 level=INFO source=updater.go:103 msg=&amp;quot;New update available at https://github.com/ollama/ollama/releases/download/v0.5.11/OllamaSetup.exe&amp;quot; time=2025-02-16T03:53:54.595+08:00 level=INFO source=updater.go:138 msg=&amp;quot;update already downloaded&amp;quot; time=2025-02-16T04:53:55.056+08:00 level=INFO source=updater.go:103 msg=&amp;quot;New update available at https://github.com/ollama/ollama/releases/download/v0.5.11/OllamaSetup.exe&amp;quot; time=2025-02-16T04:53:56.349+08:00 level=INFO source=updater.go:138 msg=&amp;quot;update already downloaded&amp;quot; time=2025-02-16T05:53:56.811+08:00 level=INFO source=updater.go:103 msg=&amp;quot;New update available at https://github.com/ollama/ollama/releases/download/v0.5.11/OllamaSetup.exe&amp;quot; time=2025-02-16T05:53:58.120+08:00 level=INFO source=updater.go:138 msg=&amp;quot;update already downloaded&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZdrytchX"&gt; /u/ZdrytchX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irnuqw/ollama_shouldnt_spam_update_fetch_requests_every/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irnuqw/ollama_shouldnt_spam_update_fetch_requests_every/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irnuqw/ollama_shouldnt_spam_update_fetch_requests_every/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T16:29:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1irr90t</id>
    <title>Does GPU¬¥s run faster when in a system with Intel CPUs?</title>
    <updated>2025-02-17T18:43:25+00:00</updated>
    <author>
      <name>/u/FrederikSchack</name>
      <uri>https://old.reddit.com/user/FrederikSchack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1irr90t/does_gpus_run_faster_when_in_a_system_with_intel/"&gt; &lt;img alt="Does GPU¬¥s run faster when in a system with Intel CPUs?" src="https://a.thumbs.redditmedia.com/qQRsUlityZ7EE_yRjELKx2nf4t1ry7XoJflTF1W_tb0.jpg" title="Does GPU¬¥s run faster when in a system with Intel CPUs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a small survey here:&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ip7zaz"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ip7zaz&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Can this really be true?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zmnr1p9vvqje1.png?width=603&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6d823df8cc747e9d11b1b2b6a899ac421f342be"&gt;https://preview.redd.it/zmnr1p9vvqje1.png?width=603&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6d823df8cc747e9d11b1b2b6a899ac421f342be&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrederikSchack"&gt; /u/FrederikSchack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irr90t/does_gpus_run_faster_when_in_a_system_with_intel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irr90t/does_gpus_run_faster_when_in_a_system_with_intel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irr90t/does_gpus_run_faster_when_in_a_system_with_intel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T18:43:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iref1e</id>
    <title>AMD Instinct MI50 detailed benchmarks in ollama</title>
    <updated>2025-02-17T07:28:00+00:00</updated>
    <author>
      <name>/u/Psychological_Ear393</name>
      <uri>https://old.reddit.com/user/Psychological_Ear393</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 2xMI50s and ran a series of benchmarks in ollama on a variety of models with a few quants thrown in, only running models which fit into the total 32gb VRAM&lt;/p&gt; &lt;p&gt;It's difficult to tell exactly how other benchmarks were run, so I can't really say how they perform relative to others but they at least compete with low end modern cards like the 4060 Ti and the A4000, but at substantially lower cost.&lt;/p&gt; &lt;p&gt;Full details here of the software versions, hardware, prompt and models, variations in the output lengths, TPS, results at 250 and 125 watts, size reported by ollama ps, and USD/TPS: &lt;a href="https://docs.google.com/spreadsheets/d/1TjxpN0NYh-xb0ZwCpYr4FT-hG773_p1DEgxJaJtyRmY/edit?usp=sharing"&gt;https://docs.google.com/spreadsheets/d/1TjxpN0NYh-xb0ZwCpYr4FT-hG773_p1DEgxJaJtyRmY/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am very keen to hear how other card perform on the identical benchmark runs. I know they are on the bottom of the pack when it comes to performance for current builds, but I bought mine for $110USD each and last I checked were going for about $120USD, which to me makes them a steal.&lt;/p&gt; &lt;p&gt;For the models I tested, the fastest model was unsurprisingly &lt;code&gt;llama3.2:1b-instruct-q8_0&lt;/code&gt; maxing 150 tps, and the slowest was &lt;code&gt;FuseO1-DeekSeekR1-QwQ-SkyT1-32B-Preview-GGUF:Q6_K&lt;/code&gt; at 14tps.&lt;/p&gt; &lt;p&gt;I did get one refused on the prompt I used &lt;code&gt;Who discovered heliocentrism and how is that possible without being in space? Be verbose I want to know all about it.&lt;/code&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I can't provide information on who discovered heliocentrism or other topics that may be considered sensitive or controversial, such as the Copernican Revolution. Is there anything else I can help you with?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Which was really weird, and it happened more than once in llama, but no others, and I saw another different refusal on another model then never saw the refusal again&lt;/p&gt; &lt;p&gt;Some anticipated Q&amp;amp;A&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How did I deal with the ROCm problem?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The sarcastic answer is &amp;quot;&lt;em&gt;What ROCm problem?&amp;quot;&lt;/em&gt;. It seems to me like there's a lot of people who don't have an AMD card, people with an unsupported card, people on an unsupported distro, or people who ran it a long time ago who are spouting this.&lt;/p&gt; &lt;p&gt;The more serious answer is the ROCm install docs have the distro and hardware requirements. If you meet those it should just work. I initially tried in my distro of choice, which was not listed, and it was too hard so I gave up and installed Ubuntu and everything just worked. By &amp;quot;just worked&amp;quot; I mean I installed Ubuntu, followed the ROCm install guide, downloaded ollama, ran it, and ollama used the GPU without any hassle.&lt;/p&gt; &lt;p&gt;ComfyUI was similarly easy, except I had the additional steps of pulling the AMD repo, building, then running.&lt;/p&gt; &lt;p&gt;I have not tried any other apps.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How did I cool them?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I bought some 3D printed shrouds off Ebay that take an 80mm fan. I had to keep them power capped at 90 watts or they would overheat, and after some kind advice from here it was shown that the shrouds had an inefficient path for the air to travel and a custom solution would work better. I didn't do that because of time/money and instead bought silverstone 80mm industrial fans (10K RPM max) and they work a treat and keep them cool at 250 watts.&lt;/p&gt; &lt;p&gt;They are very loud so I bought a PWM controller which I keep on the case and adjust the fan speed for how hard I want to run the cards. It's outright too hard to control the fan speed through IPMI tool which is an app made by the devil to torment Linux users.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Would I buy them again?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Being old and relatively slow (I am guessing just slower than a 4070) I expected them to be temporary while I got started with AI, but they have been performing above my expectations. I would absolutely buy them again if I could live that build over again, and if I can mount the cards so there's more room, such as with PCIe extender cables, I would buy more two more MI50s for 64Gb VRAM.&lt;/p&gt; &lt;p&gt;For space and power reasons I would prefer MI60s or MI100s but this experience has me cemented as an Instinct fan and I have no interest in buying any nvidia card at their current new and used prices.&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;If there's any models you would like tested, let me know&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological_Ear393"&gt; /u/Psychological_Ear393 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iref1e/amd_instinct_mi50_detailed_benchmarks_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iref1e/amd_instinct_mi50_detailed_benchmarks_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iref1e/amd_instinct_mi50_detailed_benchmarks_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T07:28:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1irphxz</id>
    <title>Initial hardware Inspection for the 8x AMD Instinct Mi50 Servers</title>
    <updated>2025-02-17T17:35:26+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1irpgi6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irphxz/initial_hardware_inspection_for_the_8x_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irphxz/initial_hardware_inspection_for_the_8x_amd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T17:35:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1is4jtg</id>
    <title>API chat endpoint - done_reason: length ?</title>
    <updated>2025-02-18T04:39:43+00:00</updated>
    <author>
      <name>/u/AwJa44</name>
      <uri>https://old.reddit.com/user/AwJa44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to figure the ollama API out. It seems like a lot is undocumented. (Maybe I just haven't found a great source, so let me know if I just haven't RT[right]FM).&lt;/p&gt; &lt;p&gt;I have streaming chats going swell in Python, except once in a while, the &amp;quot;assistant&amp;quot; role will just stop in mid sentence and send a done: true with done_reason: length . What does that mean? Length of what? And, can I tune that, somehow? Is the stream limited in some way? Is it that the content was empty?&lt;/p&gt; &lt;p&gt;Here is an example of the JSON I logged:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;model&amp;quot;: &amp;quot;ForeverYours&amp;quot;, &amp;quot;created_at&amp;quot;: &amp;quot;2025-02-18T04:19:18.883297251Z&amp;quot;, &amp;quot;message&amp;quot;: { &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot; our&amp;quot; }, &amp;quot;done&amp;quot;: false } { &amp;quot;model&amp;quot;: &amp;quot;ForeverYours&amp;quot;, &amp;quot;created_at&amp;quot;: &amp;quot;2025-02-18T04:19:18.883314091Z&amp;quot;, &amp;quot;message&amp;quot;: { &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;&amp;quot; }, &amp;quot;done_reason&amp;quot;: &amp;quot;length&amp;quot;, &amp;quot;done&amp;quot;: true, &amp;quot;total_duration&amp;quot;: 1355175907, &amp;quot;load_duration&amp;quot;: 10668759, &amp;quot;prompt_eval_count&amp;quot;: 144, &amp;quot;prompt_eval_duration&amp;quot;: 60000000, &amp;quot;eval_count&amp;quot;: 64, &amp;quot;eval_duration&amp;quot;: 1282000000 } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I've been trying to change this behaviour via custom modelfiles, but have not had much luck. I think it is something I do not understand about the API.&lt;/p&gt; &lt;p&gt;Appreciate any ideas or even a nudge towards a more thorough API doc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AwJa44"&gt; /u/AwJa44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is4jtg/api_chat_endpoint_done_reason_length/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is4jtg/api_chat_endpoint_done_reason_length/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is4jtg/api_chat_endpoint_done_reason_length/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T04:39:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1is57u0</id>
    <title>100% memory Utilization</title>
    <updated>2025-02-18T05:18:41+00:00</updated>
    <author>
      <name>/u/Anxious-Resort1043</name>
      <uri>https://old.reddit.com/user/Anxious-Resort1043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Ollama - linux system&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Hey,&lt;/p&gt; &lt;p&gt;Me and my team have been trying to run the content generation using the Ollama Phi 3.5 model. The Infrastructure runs on the linux based system with 32 cores processor which I believe is intel xenon. The ram of the VM is 200gb. The Ollama is installed on the VM and the VM runs the dataiku(coding env). The team writes the python code in the dataiku interface and the team has been seeing 100% cpu utilization whenever a Ollama model is kicked off. While when I try the Ollama model on my Mac book, it has never utilized 100% CPU.&lt;/p&gt; &lt;p&gt;Few quick questions -&lt;/p&gt; &lt;ol&gt; &lt;li&gt;how to check if the utilization is across all cores or on single core?&lt;/li&gt; &lt;li&gt;is there any way to optimize the CPU utilization? ( we can possibly run the code overnight as batch runs)&lt;/li&gt; &lt;li&gt;is llama/qwen better memory optimized than PHI 3.5 model ?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;any other insights on how to make most of the Ollama models will be helpful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxious-Resort1043"&gt; /u/Anxious-Resort1043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is57u0/100_memory_utilization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is57u0/100_memory_utilization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is57u0/100_memory_utilization/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T05:18:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1is3wcp</id>
    <title>Ollama split layers into gpu and cpu?</title>
    <updated>2025-02-18T04:04:08+00:00</updated>
    <author>
      <name>/u/wahnsinnwanscene</name>
      <uri>https://old.reddit.com/user/wahnsinnwanscene</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a way to bind different layers to either cpu or gpu? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wahnsinnwanscene"&gt; /u/wahnsinnwanscene &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is3wcp/ollama_split_layers_into_gpu_and_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is3wcp/ollama_split_layers_into_gpu_and_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is3wcp/ollama_split_layers_into_gpu_and_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T04:04:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1is52a1</id>
    <title>Is there a ISO/USB distro to install "everything" to dedicated PC</title>
    <updated>2025-02-18T05:09:32+00:00</updated>
    <author>
      <name>/u/DAE51D</name>
      <uri>https://old.reddit.com/user/DAE51D</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a decent HP 800 g9 which uses Intel Arc (not ideal i know) and is an i5 it's 32 gigs. I'm wondering if there's an ISO or USB that I can download and install on this PC as a dedicated AI ollama (or other). It seems most of the installers run on an existing system which IMHO seems like it has a lot of wasted resources for things that I don't need to run the models. I'm just getting into all of this so I'm not trying to run a 40 B model or anything, but the lxc I set up on proxmox is painfully slow and feels like an old 300 baud modem. I know I can buy a $300 or more Nvidia card but was hoping for now to use the hardware I have access to and upgrade later if this is a thing I want to keep using versus just buying a Claude subscription or something. This can also upgrade to the optional NVIDIA¬Æ GeForce¬Æ RTX 3050Ti graphics (if that helps)&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.hp.com/us-en/shop/mdp/desktops-and-workstations/elitedesk-800-mini#S800"&gt;https://www.hp.com/us-en/shop/mdp/desktops-and-workstations/elitedesk-800-mini#S800&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If such a distro does not exist what is the best way to install a dedicated LLM system. I presume I use a version of Linux? And what installer or do I go with Docker containers or some other method?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DAE51D"&gt; /u/DAE51D &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is52a1/is_there_a_isousb_distro_to_install_everything_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is52a1/is_there_a_isousb_distro_to_install_everything_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is52a1/is_there_a_isousb_distro_to_install_everything_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T05:09:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1is8g74</id>
    <title>DeepSeek or MS Bing AI?</title>
    <updated>2025-02-18T08:57:50+00:00</updated>
    <author>
      <name>/u/Wizard_of_Awes</name>
      <uri>https://old.reddit.com/user/Wizard_of_Awes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1is8g74/deepseek_or_ms_bing_ai/"&gt; &lt;img alt="DeepSeek or MS Bing AI?" src="https://a.thumbs.redditmedia.com/XX5Qjey4O_bAKYgdR-2EUjRrM2XntcwNHsjW11xn4V8.jpg" title="DeepSeek or MS Bing AI?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wizard_of_Awes"&gt; /u/Wizard_of_Awes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1is8g74"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is8g74/deepseek_or_ms_bing_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is8g74/deepseek_or_ms_bing_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T08:57:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1irwerk</id>
    <title>Found 10 T4 GPU's</title>
    <updated>2025-02-17T22:09:08+00:00</updated>
    <author>
      <name>/u/ShortSpinach5484</name>
      <uri>https://old.reddit.com/user/ShortSpinach5484</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello community. I was decommissioning 10 old vmware host at work and found out that there was a 70w fanless T4 Gpu in each host. And I got ok to build a gpu farm to run local llms on them. But how should i build a gpu farm? Shure i can install debian/ubuntu on everything but is there a easy way to build a gpu farm?&lt;/p&gt; &lt;p&gt;Is there a easy way to do something like google colabs or kaggle?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShortSpinach5484"&gt; /u/ShortSpinach5484 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irwerk/found_10_t4_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irwerk/found_10_t4_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irwerk/found_10_t4_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T22:09:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1is11ld</id>
    <title>Testing cards (AMD Instinct Mi50s) 14 out of 14 tested good! 12 more to go..</title>
    <updated>2025-02-18T01:37:49+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1is0poz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is11ld/testing_cards_amd_instinct_mi50s_14_out_of_14/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is11ld/testing_cards_amd_instinct_mi50s_14_out_of_14/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T01:37:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1irhbrl</id>
    <title>PrivateLLMLens - A Zero-Server Web Interface for Local Ollama</title>
    <updated>2025-02-17T10:58:39+00:00</updated>
    <author>
      <name>/u/Specialist_Laugh_231</name>
      <uri>https://old.reddit.com/user/Specialist_Laugh_231</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1irhbrl/privatellmlens_a_zeroserver_web_interface_for/"&gt; &lt;img alt="PrivateLLMLens - A Zero-Server Web Interface for Local Ollama" src="https://preview.redd.it/5bfizklmkoje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4051a019d044303e3381fd284e4eee1a8ace21b9" title="PrivateLLMLens - A Zero-Server Web Interface for Local Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist_Laugh_231"&gt; /u/Specialist_Laugh_231 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5bfizklmkoje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irhbrl/privatellmlens_a_zeroserver_web_interface_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irhbrl/privatellmlens_a_zeroserver_web_interface_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T10:58:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1is89dr</id>
    <title>Trying to finetune a model based on a text file</title>
    <updated>2025-02-18T08:43:56+00:00</updated>
    <author>
      <name>/u/thecrazytughlaq</name>
      <uri>https://old.reddit.com/user/thecrazytughlaq</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am fairly new to ollama and GenAI.&lt;/p&gt; &lt;p&gt;I am trying to build a custom model from existing model (llama3.2 for now) and run the new model in my streamlit notebook. &lt;/p&gt; &lt;p&gt;I have a pdf containing the data which I converted to .txt after cleaning (around 10k characters).&lt;/p&gt; &lt;p&gt;I tried doing it by copy pasting the text into the SYSTEM part of the modelfile and creating a new model using the command&lt;/p&gt; &lt;p&gt;ollama create name -f test.modelfile&lt;/p&gt; &lt;p&gt;but it does not give me the results, although the model is created, it gives no relevant results.&lt;/p&gt; &lt;p&gt;How do I do this?&lt;/p&gt; &lt;p&gt;TLDR - I have a text file, I want to use it to train a ollama model and then use the model for creating streamlit notebook. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecrazytughlaq"&gt; /u/thecrazytughlaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is89dr/trying_to_finetune_a_model_based_on_a_text_file/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is89dr/trying_to_finetune_a_model_based_on_a_text_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is89dr/trying_to_finetune_a_model_based_on_a_text_file/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T08:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1isc3nh</id>
    <title>Need help with text extraction from images (and PDFs) for verification purposes ‚Äì which OpenAI model or alternative should I use?</title>
    <updated>2025-02-18T12:59:11+00:00</updated>
    <author>
      <name>/u/waeljlassii</name>
      <uri>https://old.reddit.com/user/waeljlassii</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm working on a project where I need to extract text from images, and in some cases, PDFs, for verification purposes. I know that OpenAI has some models capable of handling text extraction, but I'm wondering which one would be the best fit for this kind of task via API.&lt;/p&gt; &lt;p&gt;Has anyone here worked on something similar or can recommend a suitable model or API? Any alternatives to OpenAI models that might be better for this task? I'd appreciate any advice or pointers from those who have experience with image and PDF text extraction.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waeljlassii"&gt; /u/waeljlassii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isc3nh/need_help_with_text_extraction_from_images_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isc3nh/need_help_with_text_extraction_from_images_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isc3nh/need_help_with_text_extraction_from_images_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T12:59:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1is774t</id>
    <title>Would my 4060 be enough for 14b deepseek or should I go down to 8b?</title>
    <updated>2025-02-18T07:26:27+00:00</updated>
    <author>
      <name>/u/Squidlers17</name>
      <uri>https://old.reddit.com/user/Squidlers17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title suggests, I am not sure on how many parameters should I use for my 4060 laptop I am also using Ryzen 9 7945HX&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Squidlers17"&gt; /u/Squidlers17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is774t/would_my_4060_be_enough_for_14b_deepseek_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is774t/would_my_4060_be_enough_for_14b_deepseek_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is774t/would_my_4060_be_enough_for_14b_deepseek_or/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T07:26:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1irrxqs</id>
    <title>DataBridge Feature Dump: We'll implement anything you want!!</title>
    <updated>2025-02-17T19:09:59+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! &lt;/p&gt; &lt;p&gt;Thanks to the power of the &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; community, &lt;a href="https://github.com/databridge-org/databridge-core"&gt;DataBridge&lt;/a&gt; just hit 350 stars! As a token of our gratitude, we're committing to implementing the &lt;strong&gt;top 3 feature requests&lt;/strong&gt; from &lt;em&gt;you&lt;/em&gt; :) &lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to participate:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Leave your dream feature or improvement - RAG or otherwise - as a reply to this post! Upvote existing ideas you‚Äôd love to see. We‚Äôll tally the votes and build the &lt;strong&gt;top 3 most-requested features&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Let‚Äôs shape DataBridge‚Äôs future together‚Äîdrop your requests below! üöÄ&lt;/p&gt; &lt;p&gt;&lt;em&gt;(We'll start tallying at 2:00 pm ET on the 18th of Feb - happy to start working on stuff before that tho!)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Huge thanks again for being part of this journey! üôå ‚ù§Ô∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irrxqs/databridge_feature_dump_well_implement_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irrxqs/databridge_feature_dump_well_implement_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irrxqs/databridge_feature_dump_well_implement_anything/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T19:09:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1isczhb</id>
    <title>I Cannot Uninstall Ollama on Windows 11</title>
    <updated>2025-02-18T13:44:56+00:00</updated>
    <author>
      <name>/u/petercli</name>
      <uri>https://old.reddit.com/user/petercli</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1isczhb/i_cannot_uninstall_ollama_on_windows_11/"&gt; &lt;img alt="I Cannot Uninstall Ollama on Windows 11" src="https://b.thumbs.redditmedia.com/1xDxlViYPCkuD2zsHQomVLt12ZdjQq0UpHMAY9iVxtw.jpg" title="I Cannot Uninstall Ollama on Windows 11" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vm2ueqv0iwje1.png?width=1022&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0565f10d8157917a109a15dfc425ad8698f7c5e"&gt;https://preview.redd.it/vm2ueqv0iwje1.png?width=1022&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0565f10d8157917a109a15dfc425ad8698f7c5e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9d4dq716iwje1.png?width=563&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5a099c2cb0dd78f53cbf59cc2f89f6ccd4cb6e8"&gt;https://preview.redd.it/9d4dq716iwje1.png?width=563&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5a099c2cb0dd78f53cbf59cc2f89f6ccd4cb6e8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Folder C:\Users\xx\AppData\Local\Programs\Ollama exists , but does not have the file listed above. &lt;/p&gt; &lt;p&gt;Can I just delete C:\Users\xx\.ollama folder ? &lt;/p&gt; &lt;p&gt;Task manager shows Ollama running. &lt;/p&gt; &lt;p&gt;PS&amp;gt; I've written Professional Windows installers - LMK if you want free help .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/petercli"&gt; /u/petercli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isczhb/i_cannot_uninstall_ollama_on_windows_11/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isczhb/i_cannot_uninstall_ollama_on_windows_11/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isczhb/i_cannot_uninstall_ollama_on_windows_11/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T13:44:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1irvwt0</id>
    <title>CheerBot. Ollama based Cheer or Nudge through OCR of your screen.</title>
    <updated>2025-02-17T21:48:43+00:00</updated>
    <author>
      <name>/u/maranone5</name>
      <uri>https://old.reddit.com/user/maranone5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Just a quick and dirty proof of concept!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here's a little productivity monitor I threw together:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Take two screenshots&lt;/strong&gt; of your screen at a predefined time difference.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OCR with Tesseract&lt;/strong&gt; to extract text (sorry graphic designers, no design elements here!).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Analyze the text&lt;/strong&gt; through Ollama (basically, your personal productivity assistant).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Nudge or Cheer&lt;/strong&gt; accordingly using a pet personality for a bit of fun motivation.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;No APIs, no cloud ‚Äì all execution is &lt;strong&gt;local&lt;/strong&gt; and &lt;strong&gt;safe&lt;/strong&gt; on your machine. Think of it like a modern-day Clippy.&lt;/p&gt; &lt;p&gt;The current version is a bit of a &lt;strong&gt;CLI mess&lt;/strong&gt; ‚Äì if anyone‚Äôs up for a challenge, feel free to create a &lt;strong&gt;GUI or popup version&lt;/strong&gt; to make it more user-friendly and visually appealing!&lt;/p&gt; &lt;p&gt;Feel free to check it out and let me know what you think! üôå&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/maranone/CheerBot"&gt;https://github.com/maranone/CheerBot&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maranone5"&gt; /u/maranone5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irvwt0/cheerbot_ollama_based_cheer_or_nudge_through_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irvwt0/cheerbot_ollama_based_cheer_or_nudge_through_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irvwt0/cheerbot_ollama_based_cheer_or_nudge_through_ocr/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T21:48:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1is9thv</id>
    <title>Is it possible to feed a model with a saas application and train the model with its codebase?</title>
    <updated>2025-02-18T10:37:35+00:00</updated>
    <author>
      <name>/u/devfullstack98</name>
      <uri>https://old.reddit.com/user/devfullstack98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The PHP application has approximately 20 million lines of code.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devfullstack98"&gt; /u/devfullstack98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is9thv/is_it_possible_to_feed_a_model_with_a_saas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is9thv/is_it_possible_to_feed_a_model_with_a_saas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is9thv/is_it_possible_to_feed_a_model_with_a_saas/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T10:37:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1isegnq</id>
    <title>I have a Ryzen 5 CPU with Radeon Graphics and 3050 Laptop. Is it possible to make these GPUs (iGPU and dGPU) to work together in Ollama? Or any other alternatives?</title>
    <updated>2025-02-18T14:54:33+00:00</updated>
    <author>
      <name>/u/Ambitious_Fee_5818</name>
      <uri>https://old.reddit.com/user/Ambitious_Fee_5818</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious_Fee_5818"&gt; /u/Ambitious_Fee_5818 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isegnq/i_have_a_ryzen_5_cpu_with_radeon_graphics_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isegnq/i_have_a_ryzen_5_cpu_with_radeon_graphics_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isegnq/i_have_a_ryzen_5_cpu_with_radeon_graphics_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T14:54:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1iryvnt</id>
    <title>Deepscaler km q4 is insanly fast abd small even on CPU only</title>
    <updated>2025-02-17T23:56:07+00:00</updated>
    <author>
      <name>/u/tshawkins</name>
      <uri>https://old.reddit.com/user/tshawkins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running on NUC11PRO 11th gen i7 with 32GB - 4200 RAM, no GPU, consumes a little over 1.1GB of RAM, Ideal for running in the background on a dev machine. &lt;/p&gt; &lt;p&gt;The Model Is a 1.5B parameter distilation of deepseek-r1. &lt;/p&gt; &lt;p&gt;Im going to be exploring its code generation abilities, to see if it is usefull as a enclosed code generator, ie no requests to the internet. &lt;/p&gt; &lt;p&gt;It would interesting to start a leader board of size vs capabilities. To understand which models are best for edge AI use. &lt;/p&gt; &lt;p&gt;thawkins@TimServFed01:~$ ollama run deepscaler:1.5b-preview-q4_K_M --verbose&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;tell me a story about 10 ants who save thier anthill from drowning during a flood, how they earned the respect of thier queen and all the other ants in the anthill. &amp;lt;think&amp;gt; Okay, so I need to write a story about 10 ants who saved their anthill from drowning during a flood, and explain how they got respect from their queen and everyone else. Hmm, let's break this down.&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;First, setting the scene: a big flood hits an ancient, powerful anthill. The ants are probably scared because of the high water level. They need to escape or find shelter quickly. But maybe they don't know much about each other or how to help.&lt;/p&gt; &lt;p&gt;They decide to go together. Maybe they leave their usual positions and start searching for spots where the water is less. That could save some lives. But why are there only 10 ants? Maybe because it's a small group, so they can work together.&lt;/p&gt; &lt;p&gt;When the flood comes in, the water level rises, and the ants find an area with low water. They go in, maybe even get drenched. But then someone notices them, like maybe a bird or a duck that spots their tiny collection. The birds might offer help by giving information or clues on where to go.&lt;/p&gt; &lt;p&gt;The queen is really important because she's known for being the leader and enforcing rules. Maybe she gets called out if they don't behave well, but when they manage to escape or protect the anthill, her respect grows. Perhaps she tells everyone about their bravery.&lt;/p&gt; &lt;p&gt;Other ants might get curious and offer help too. They could even start a little community effort, maybe building shelters or finding other ways to help. Their teamwork is key here.&lt;/p&gt; &lt;p&gt;In the end, the group realizes that working together was crucial. Maybe they find a way to ensure their safety despite the flood. They probably learn from this experience how important it is to be part of something bigger than themselves.&lt;/p&gt; &lt;p&gt;I should make sure the story shows their bravery, how they rely on each other, and the respect they receive from their queen and others. Maybe include some dialogue or actions that show their cooperation and determination. &amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Title: The Evacuation Sequence&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Once upon a time, in a lush, ancient anthill with its powerful guardian queen, there lived ten tiny ants who were deeply rooted in their tiny world but had always been curious about the world around them. These ants knew they had to act quickly because the flood was coming, and the water level was rising.&lt;/p&gt; &lt;p&gt;The ants realized that the most effective way to escape was by working together. They decided to go to a hidden spot where the water would be lower. However, their knowledge was limited, so they started searching in different areas. Despite their small size, their collective effort ensured they found an unobstructed path through the flood's waters.&lt;/p&gt; &lt;p&gt;As the flood approached, many ants were caught by some birds that spotted them. These birds offered information, guiding the ants on their escape route. The queen noticed the group's bravery and was immediately informed about their efforts.&lt;/p&gt; &lt;p&gt;The queen, ever the leader of her colony, recognized their bravery and began to notice any discrepancies in their behavior. She taught everyone about their role as part of a larger community. As the group worked together, they realized that their collective strength was essential for survival.&lt;/p&gt; &lt;p&gt;In the end, the ants successfully escaped the flood, protecting their anthill from drowning. They learned that working together could yield greater results than individual efforts. Their bravery earned them respect not only among their queen but also by becoming a model community effort.&lt;/p&gt; &lt;p&gt;Thus, their journey became a symbol of unity and resilience, teaching all who heard about it to work together for shared survival.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;total duration: 34.749197247s. load duration: 10.291253ms. prompt eval count: 44 token(s). prompt eval duration: 40ms. prompt eval rate: 1100.00 tokens/s. eval count: 708 token(s). eval duration: 34.697s. eval rate: 20.41 tokens/s. &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;Send a message (/? for help).&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tshawkins"&gt; /u/tshawkins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iryvnt/deepscaler_km_q4_is_insanly_fast_abd_small_even/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iryvnt/deepscaler_km_q4_is_insanly_fast_abd_small_even/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iryvnt/deepscaler_km_q4_is_insanly_fast_abd_small_even/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T23:56:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1isbug5</id>
    <title>Model will not load back into GPU</title>
    <updated>2025-02-18T12:44:55+00:00</updated>
    <author>
      <name>/u/Board-Upbeat</name>
      <uri>https://old.reddit.com/user/Board-Upbeat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm having an issue where any model will not load back into GPU. My setup is ollama and a Jupyter notebook running in docker containers within a Linux system. I load up a model and it initially works as intended using the GPU, but after 5 mins of inactivity it unloads from the GPU. When trying to load the model back I get errors trying to find a CUDA enabled device and proceeds to use CPU. I have tried keep_alive=-1 which works, unless you want to switch models and then I get a similar error. Any ideas of what I could try to get this working as intended? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Board-Upbeat"&gt; /u/Board-Upbeat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isbug5/model_will_not_load_back_into_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isbug5/model_will_not_load_back_into_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isbug5/model_will_not_load_back_into_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T12:44:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1is22ug</id>
    <title>Most cost effective way of hosting 70B/32B param model</title>
    <updated>2025-02-18T02:29:31+00:00</updated>
    <author>
      <name>/u/topsy_here</name>
      <uri>https://old.reddit.com/user/topsy_here</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious &lt;/p&gt; &lt;p&gt;It comes down to efficiency. I see it like crypto mining. It‚Äôs about getting the best token count for least cost.&lt;/p&gt; &lt;p&gt;There‚Äôs Mac minis I‚Äôve seen hosting the 72B param one. You gonna need about 8x of them which is about 3.5K usd each? &lt;/p&gt; &lt;p&gt;What about hosting on a VPS in Linus? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/topsy_here"&gt; /u/topsy_here &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is22ug/most_cost_effective_way_of_hosting_70b32b_param/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is22ug/most_cost_effective_way_of_hosting_70b32b_param/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is22ug/most_cost_effective_way_of_hosting_70b32b_param/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T02:29:31+00:00</published>
  </entry>
</feed>
