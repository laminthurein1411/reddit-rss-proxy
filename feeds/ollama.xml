<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-17T20:23:59+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ircqzg</id>
    <title>Managed to Secure my Ollama/Whisper Ubuntu Server</title>
    <updated>2025-02-17T05:38:35+00:00</updated>
    <author>
      <name>/u/ManyInteresting3969</name>
      <uri>https://old.reddit.com/user/ManyInteresting3969</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ManyInteresting3969"&gt; /u/ManyInteresting3969 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/u_ManyInteresting3969/comments/1ircqeg/managed_to_secure_my_ollamawhisper_ubuntu_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ircqzg/managed_to_secure_my_ollamawhisper_ubuntu_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ircqzg/managed_to_secure_my_ollamawhisper_ubuntu_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T05:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ireu17</id>
    <title>Any body got the chance to run Ollama with Deepseek R1 671B version?</title>
    <updated>2025-02-17T07:58:54+00:00</updated>
    <author>
      <name>/u/ChemicalExcellent463</name>
      <uri>https://old.reddit.com/user/ChemicalExcellent463</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;Average stats: (Running on dual 3090 Ti GPU, Epyc 7763 CPU in Ubuntu 22.04) ---------------------------------------------------- Model: deepseek-r1:70b Performance Metrics: Prompt Processing: 336.73 tokens/sec Generation Speed: 17.65 tokens/sec Combined Speed: 18.01 tokens/sec Workload Stats: Input Tokens: 165 Generated Tokens: 7673 Model Load Time: 6.11s Processing Time: 0.49s Generation Time: 434.70s Total Time: 441.31s ---------------------------------------------------- Average stats: (Running on single 3090 GPU, 13900KS CPU in WSL2(Ubuntu 22.04) in Windows 11) ---------------------------------------------------- Model: deepseek-r1:32b Performance Metrics: Prompt Processing: 399.05 tokens/sec Generation Speed: 27.18 tokens/sec Combined Speed: 27.58 tokens/sec Workload Stats: Input Tokens: 168 Generated Tokens: 10601 Model Load Time: 15.44s Processing Time: 0.42s Generation Time: 390.00s Total Time: 405.87s ---------------------------------------------------- &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Github code: &lt;a href="https://github.com/larryhopecode/ollama-benchmark"&gt;larryhopecode/ollama-benchmark: Ollama based Benchmark with detail I/O token per second. Python with Deepseek R1 example.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChemicalExcellent463"&gt; /u/ChemicalExcellent463 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ireu17/any_body_got_the_chance_to_run_ollama_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ireu17/any_body_got_the_chance_to_run_ollama_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ireu17/any_body_got_the_chance_to_run_ollama_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T07:58:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir72jb</id>
    <title>Funny AI response</title>
    <updated>2025-02-17T00:29:20+00:00</updated>
    <author>
      <name>/u/ZealousidealBee8299</name>
      <uri>https://old.reddit.com/user/ZealousidealBee8299</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am testing out models for tool calling, and this one model just never worked right to trigger on country. So as the input to the query I finally used:&lt;/p&gt; &lt;p&gt;String simulatedInput = &amp;quot;Do you have a tool that shows what country i am in? if not what were you expecting the tool to be called so i can tell the programmer.&amp;quot;;&lt;/p&gt; &lt;p&gt;And it responded:&lt;/p&gt; &lt;p&gt;It looks like I've got it! The tool is called \getCurrentCountry` and it returns a string indicating the country you are currently in. In this case, the output indicates that you are in the United States (US). If you'd like to know more about your location or would like to search for specific countries, feel free to ask!`&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZealousidealBee8299"&gt; /u/ZealousidealBee8299 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir72jb/funny_ai_response/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir72jb/funny_ai_response/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ir72jb/funny_ai_response/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T00:29:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqqq0e</id>
    <title>ollama-remote: Make local ollama run models on remote server (colab, kaggle, ...)</title>
    <updated>2025-02-16T11:58:12+00:00</updated>
    <author>
      <name>/u/amitness</name>
      <uri>https://old.reddit.com/user/amitness</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a package for the gpu-poor/mac-poor to run ollama models via remote servers (colab, kaggle, paid inference etc.)&lt;/p&gt; &lt;p&gt;Just 2 lines and the local ollama cli can access all models which actually run on the server-side GPU/CPU:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install ollama-remote ollama-remote &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I wrote it to speed up prompt engineering and synthetic data generation for a personal project which ran too slowly with local models on my mac. Once the results are good, we switch back to running locally.&lt;/p&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;The tool downloads and sets up ollama on the server side and exposes a port&lt;/li&gt; &lt;li&gt;Cloudflare tunnel is automatically downloaded and setup to expose ollama's port to a random domain&lt;/li&gt; &lt;li&gt;We parse the domain and then provide code for setting&lt;code&gt;OLLAMA_HOST&lt;/code&gt; as well as usage in OpenAI SDK for local use.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Source code: &lt;a href="https://github.com/amitness/ollama-remote"&gt;https://github.com/amitness/ollama-remote&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amitness"&gt; /u/amitness &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqqq0e/ollamaremote_make_local_ollama_run_models_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqqq0e/ollamaremote_make_local_ollama_run_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqqq0e/ollamaremote_make_local_ollama_run_models_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T11:58:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1irg7ox</id>
    <title>I made a simple python library to create a bridge between real and simulated python interpreters</title>
    <updated>2025-02-17T09:38:37+00:00</updated>
    <author>
      <name>/u/rb9_3b</name>
      <uri>https://old.reddit.com/user/rb9_3b</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rb9_3b"&gt; /u/rb9_3b &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1ipyivw/i_made_a_simple_python_library_to_create_a_bridge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irg7ox/i_made_a_simple_python_library_to_create_a_bridge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irg7ox/i_made_a_simple_python_library_to_create_a_bridge/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T09:38:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir69xk</id>
    <title>seeking a fast local LLM solution using only CPU</title>
    <updated>2025-02-16T23:50:34+00:00</updated>
    <author>
      <name>/u/Orleans007</name>
      <uri>https://old.reddit.com/user/Orleans007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I have a development server connected via SSH with the following specs: 64GB RAM, 16 CPU cores, no GPU, no TPU.&lt;/p&gt; &lt;p&gt;I‚Äôm looking for a way to connect an open source model locally to my Langchain application.&lt;/p&gt; &lt;p&gt;I don't want to use cloud hosted inference endpoints, i have tried using Llamafile/Ollama with 3/4 bit quantized models but the response times are extremely slow, especially when integrating a SQL or Pandas agent.&lt;/p&gt; &lt;p&gt;I'm seeking an open source, local solution that avoids latency and slow responses.&lt;/p&gt; &lt;p&gt;is it possible?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Orleans007"&gt; /u/Orleans007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir69xk/seeking_a_fast_local_llm_solution_using_only_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir69xk/seeking_a_fast_local_llm_solution_using_only_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ir69xk/seeking_a_fast_local_llm_solution_using_only_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T23:50:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1irjdtm</id>
    <title>100% utilization using the Ollama with phi 3.5 model</title>
    <updated>2025-02-17T13:05:19+00:00</updated>
    <author>
      <name>/u/Anxious-Resort1043</name>
      <uri>https://old.reddit.com/user/Anxious-Resort1043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, &lt;/p&gt; &lt;p&gt;I have been using the phi3 .5 model using Ollama on a linux machine which has 32 core CPU and 200Gb ram, no GPU though. The LLM is used to generate response using the chunks, in my Mac which is 8gb M1 mini, it has no issues but facing issues with the Linux machine. Am I doing anything wrong here ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxious-Resort1043"&gt; /u/Anxious-Resort1043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irjdtm/100_utilization_using_the_ollama_with_phi_35_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irjdtm/100_utilization_using_the_ollama_with_phi_35_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irjdtm/100_utilization_using_the_ollama_with_phi_35_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T13:05:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1irkej7</id>
    <title>adjusting RoPE scaling and NTK-aware scaling</title>
    <updated>2025-02-17T13:57:34+00:00</updated>
    <author>
      <name>/u/Useful-Skill6241</name>
      <uri>https://old.reddit.com/user/Useful-Skill6241</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I guys im running deepseek-r1:70b -its a distilled Q4_K_M 0c1615a8ca32 (42GB)&lt;/p&gt; &lt;p&gt;I have changed the paramters in ollma for a 15,000 token context window:&lt;/p&gt; &lt;p&gt;&amp;quot; Model&lt;/p&gt; &lt;p&gt;architecture llama&lt;/p&gt; &lt;p&gt;parameters 70.6B&lt;/p&gt; &lt;p&gt;context length 131072&lt;/p&gt; &lt;p&gt;embedding length 8192&lt;/p&gt; &lt;p&gt;quantization Q4_K_M&lt;/p&gt; &lt;p&gt;Parameters&lt;/p&gt; &lt;p&gt;num_ctx 15000&amp;quot;&lt;/p&gt; &lt;p&gt;is there anywhere you guys know of where I can adjusting RoPE scaling and NTK-aware scaling. I have the ram for it, it just spits out gibberish or doesnt take into account the text after around i would say 1200 tokens of output&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Useful-Skill6241"&gt; /u/Useful-Skill6241 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irkej7/adjusting_rope_scaling_and_ntkaware_scaling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irkej7/adjusting_rope_scaling_and_ntkaware_scaling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irkej7/adjusting_rope_scaling_and_ntkaware_scaling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T13:57:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1irn0f9</id>
    <title>Command-r7b</title>
    <updated>2025-02-17T15:54:46+00:00</updated>
    <author>
      <name>/u/J0Mo_o</name>
      <uri>https://old.reddit.com/user/J0Mo_o</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can command-r7b be run on ollama 0.5.7 It says it needs ollama 0.5.5 but i don't know if that means it needs THIS specific version or it needs at least this version&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/J0Mo_o"&gt; /u/J0Mo_o &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irn0f9/commandr7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irn0f9/commandr7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irn0f9/commandr7b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T15:54:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1irek2w</id>
    <title>How do you find the right AI model for your use case?</title>
    <updated>2025-02-17T07:38:22+00:00</updated>
    <author>
      <name>/u/uber_men</name>
      <uri>https://old.reddit.com/user/uber_men</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where can I search for AI models that fit my use case? For example, if I want to restore old blurred photos but don‚Äôt know which AI model to use, I should be able to find one like &lt;a href="https://github.com/TencentARC/GFPGAN"&gt;GFPGAN&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uber_men"&gt; /u/uber_men &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irek2w/how_do_you_find_the_right_ai_model_for_your_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irek2w/how_do_you_find_the_right_ai_model_for_your_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irek2w/how_do_you_find_the_right_ai_model_for_your_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T07:38:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1iro9kd</id>
    <title>I Built My Own AI Code Assistant with DeepSeek &amp; LangChain!</title>
    <updated>2025-02-17T16:46:08+00:00</updated>
    <author>
      <name>/u/Special_Community179</name>
      <uri>https://old.reddit.com/user/Special_Community179</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iro9kd/i_built_my_own_ai_code_assistant_with_deepseek/"&gt; &lt;img alt="I Built My Own AI Code Assistant with DeepSeek &amp;amp; LangChain!" src="https://external-preview.redd.it/hnt7poXU-xnfeXFZfxsMOiSfirurfRIotwJUPYEsx90.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=655a950bdbfba394f2efb20d1be0901f5db6f196" title="I Built My Own AI Code Assistant with DeepSeek &amp;amp; LangChain!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special_Community179"&gt; /u/Special_Community179 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=7GAKB21lxyg&amp;amp;list=PLp01ObP3udmq2quR-RfrX4zNut_t_kNot"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iro9kd/i_built_my_own_ai_code_assistant_with_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iro9kd/i_built_my_own_ai_code_assistant_with_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T16:46:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1irr90t</id>
    <title>Does GPU¬¥s run faster when in a system with Intel CPUs?</title>
    <updated>2025-02-17T18:43:25+00:00</updated>
    <author>
      <name>/u/FrederikSchack</name>
      <uri>https://old.reddit.com/user/FrederikSchack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1irr90t/does_gpus_run_faster_when_in_a_system_with_intel/"&gt; &lt;img alt="Does GPU¬¥s run faster when in a system with Intel CPUs?" src="https://a.thumbs.redditmedia.com/qQRsUlityZ7EE_yRjELKx2nf4t1ry7XoJflTF1W_tb0.jpg" title="Does GPU¬¥s run faster when in a system with Intel CPUs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a small survey here:&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ip7zaz"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ip7zaz&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Can this really be true?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zmnr1p9vvqje1.png?width=603&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6d823df8cc747e9d11b1b2b6a899ac421f342be"&gt;https://preview.redd.it/zmnr1p9vvqje1.png?width=603&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6d823df8cc747e9d11b1b2b6a899ac421f342be&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrederikSchack"&gt; /u/FrederikSchack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irr90t/does_gpus_run_faster_when_in_a_system_with_intel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irr90t/does_gpus_run_faster_when_in_a_system_with_intel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irr90t/does_gpus_run_faster_when_in_a_system_with_intel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T18:43:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1irn6o9</id>
    <title>Any fix to this?</title>
    <updated>2025-02-17T16:01:55+00:00</updated>
    <author>
      <name>/u/Beneficial-Cup2969</name>
      <uri>https://old.reddit.com/user/Beneficial-Cup2969</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1irn6o9/any_fix_to_this/"&gt; &lt;img alt="Any fix to this?" src="https://preview.redd.it/iaqslgar2qje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f8c6aa9074fc89619a7bbd47b7412491bf027552" title="Any fix to this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry am really new to all these private ai stuff but I can't get ollama to analyse this image I used a tutorial to use llama 3 using docker and local host for convenience am i missing something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beneficial-Cup2969"&gt; /u/Beneficial-Cup2969 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iaqslgar2qje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irn6o9/any_fix_to_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irn6o9/any_fix_to_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T16:01:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1irnuqw</id>
    <title>Ollama shouldn't spam update fetch requests every second or two when it already has the update downloaded...</title>
    <updated>2025-02-17T16:29:29+00:00</updated>
    <author>
      <name>/u/ZdrytchX</name>
      <uri>https://old.reddit.com/user/ZdrytchX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;time=2025-02-16T03:53:53.276+08:00 level=INFO source=updater.go:103 msg=&amp;quot;New update available at https://github.com/ollama/ollama/releases/download/v0.5.11/OllamaSetup.exe&amp;quot; time=2025-02-16T03:53:54.595+08:00 level=INFO source=updater.go:138 msg=&amp;quot;update already downloaded&amp;quot; time=2025-02-16T04:53:55.056+08:00 level=INFO source=updater.go:103 msg=&amp;quot;New update available at https://github.com/ollama/ollama/releases/download/v0.5.11/OllamaSetup.exe&amp;quot; time=2025-02-16T04:53:56.349+08:00 level=INFO source=updater.go:138 msg=&amp;quot;update already downloaded&amp;quot; time=2025-02-16T05:53:56.811+08:00 level=INFO source=updater.go:103 msg=&amp;quot;New update available at https://github.com/ollama/ollama/releases/download/v0.5.11/OllamaSetup.exe&amp;quot; time=2025-02-16T05:53:58.120+08:00 level=INFO source=updater.go:138 msg=&amp;quot;update already downloaded&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZdrytchX"&gt; /u/ZdrytchX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irnuqw/ollama_shouldnt_spam_update_fetch_requests_every/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irnuqw/ollama_shouldnt_spam_update_fetch_requests_every/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irnuqw/ollama_shouldnt_spam_update_fetch_requests_every/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T16:29:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir1y64</id>
    <title>Web Search for Ollama</title>
    <updated>2025-02-16T20:38:37+00:00</updated>
    <author>
      <name>/u/Pure-Caramel1216</name>
      <uri>https://old.reddit.com/user/Pure-Caramel1216</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ir1y64/web_search_for_ollama/"&gt; &lt;img alt="Web Search for Ollama" src="https://external-preview.redd.it/cTZzejBhcDdia2plMX2nSzWAqWPwL7GaLWhWgmcM0HgC18EKBhh979hmLHax.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02f06424f67deec37bb163c15ea8dec1329da736" title="Web Search for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Although I love Ollama, I've found it nearly impossible to get up-to-date information. Existing solutions are either too complex or simply don't work well. That‚Äôs why I built an anonymous web search tool for Ollama (check out the video above).&lt;/p&gt; &lt;p&gt;Since I‚Äôm not the only one facing this issue, I‚Äôm considering launching it as a full-fledged tool. I‚Äôd really appreciate your suggestions on how to improve it‚Äîplease leave your ideas in the comments. And if you like what you see, consider upvoting so this project reaches a wider audience.&lt;/p&gt; &lt;p&gt;About the Video:&lt;/p&gt; &lt;p&gt;Model: Small 3B model&lt;/p&gt; &lt;p&gt;Hardware: Running on an NVIDIA A10 GPU&lt;/p&gt; &lt;p&gt;Demo: I ran the same prompt twice‚Äîfirst without web search, and then with web search.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pure-Caramel1216"&gt; /u/Pure-Caramel1216 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9wsgtkx7bkje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir1y64/web_search_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ir1y64/web_search_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T20:38:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1irtpy2</id>
    <title>Can I store the inputs and outputs of my conversations?</title>
    <updated>2025-02-17T20:20:23+00:00</updated>
    <author>
      <name>/u/BaggiPonte</name>
      <uri>https://old.reddit.com/user/BaggiPonte</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run ollama in my CLI and I would like to store my completions for personal analysis/fine tuning. How can I do that? Or perhaps ollama can store them by default locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BaggiPonte"&gt; /u/BaggiPonte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irtpy2/can_i_store_the_inputs_and_outputs_of_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irtpy2/can_i_store_the_inputs_and_outputs_of_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irtpy2/can_i_store_the_inputs_and_outputs_of_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T20:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqvypa</id>
    <title>Ollama Vs. LM Studio</title>
    <updated>2025-02-16T16:29:24+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iqvypa/ollama_vs_lm_studio/"&gt; &lt;img alt="Ollama Vs. LM Studio" src="https://external-preview.redd.it/9twFf-ndeg92DJQ4VUhbGT49y8GQ4M4VnDD5QQ3aM0g.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d603d6ac771df4cac0cb2b45b29e921c3ab5dc19" title="Ollama Vs. LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/QGtkaDWJZlA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqvypa/ollama_vs_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqvypa/ollama_vs_lm_studio/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T16:29:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1irjave</id>
    <title>OpenThinker-32B-FP16 + 8x AMD Instinct Mi60 Server + vLLM + Tensor Parallelism</title>
    <updated>2025-02-17T13:01:13+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/klcjcd1d6pje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irjave/openthinker32bfp16_8x_amd_instinct_mi60_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irjave/openthinker32bfp16_8x_amd_instinct_mi60_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T13:01:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1irsdl2</id>
    <title>Can not use ollama in VS Code on remote server</title>
    <updated>2025-02-17T19:27:25+00:00</updated>
    <author>
      <name>/u/wbiggs205</name>
      <uri>https://old.reddit.com/user/wbiggs205</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a remote GPU server run Windows 11 pro with a NVIDIA t1000 8 g I installed ollama. And the web GUI. Enable. I did change the default port. I can use ollama with the GUI just fine. And I have AnythingLLM installed on my desktop I can get it to use the server but when I set up continue to use it. It will not connect. I did add this to the config file then I port the address and the new port where the endpoint is but nothing. Any ides ?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;models&amp;quot;: [ { &amp;quot;title&amp;quot;: &amp;quot;Llama3.1 8B&amp;quot;, &amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;, &amp;quot;model&amp;quot;: &amp;quot;llama3.1:8b&amp;quot;, &amp;quot;apiBase&amp;quot;: &amp;quot;http://&amp;lt;my endpoint&amp;gt;:11434&amp;quot; } ] } &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wbiggs205"&gt; /u/wbiggs205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irsdl2/can_not_use_ollama_in_vs_code_on_remote_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irsdl2/can_not_use_ollama_in_vs_code_on_remote_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irsdl2/can_not_use_ollama_in_vs_code_on_remote_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T19:27:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1irt83f</id>
    <title>Is it possible to access the API from other machines on network?</title>
    <updated>2025-02-17T20:01:02+00:00</updated>
    <author>
      <name>/u/mshriver2</name>
      <uri>https://old.reddit.com/user/mshriver2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I am running Ollama on windows 10. I can use the API just fine locally but it is inaccessible from other machines on the network. I have confirmed that the port is allowed through the windows firewall. I did some research and it seems Ollama API is only set to bind to localhost by default. It seems you can change that in windows with a -- command flag. However there doesn't seem to be an equivalent in Windows. Is there any workaround to being able to access the API from other machines?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mshriver2"&gt; /u/mshriver2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irt83f/is_it_possible_to_access_the_api_from_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irt83f/is_it_possible_to_access_the_api_from_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irt83f/is_it_possible_to_access_the_api_from_other/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T20:01:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1irtewe</id>
    <title>Jarvis at Home</title>
    <updated>2025-02-17T20:08:17+00:00</updated>
    <author>
      <name>/u/AutomaticRadish5</name>
      <uri>https://old.reddit.com/user/AutomaticRadish5</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1irtewe/jarvis_at_home/"&gt; &lt;img alt="Jarvis at Home" src="https://external-preview.redd.it/dTExNzZsM3BhcmplMTt6sXG7ygBufWbZGyKocKVKKjpDeQAd8T0jjbX8B6f0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acdd6591a4ab8dc0b4906d2af1c4be8021a23cea" title="Jarvis at Home" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Neat little work in progress &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AutomaticRadish5"&gt; /u/AutomaticRadish5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6e2njv6parje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irtewe/jarvis_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irtewe/jarvis_at_home/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T20:08:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1irphxz</id>
    <title>Initial hardware Inspection for the 8x AMD Instinct Mi50 Servers</title>
    <updated>2025-02-17T17:35:26+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1irpgi6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irphxz/initial_hardware_inspection_for_the_8x_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irphxz/initial_hardware_inspection_for_the_8x_amd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T17:35:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iref1e</id>
    <title>AMD Instinct MI50 detailed benchmarks in ollama</title>
    <updated>2025-02-17T07:28:00+00:00</updated>
    <author>
      <name>/u/Psychological_Ear393</name>
      <uri>https://old.reddit.com/user/Psychological_Ear393</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 2xMI50s and ran a series of benchmarks in ollama on a variety of models with a few quants thrown in, only running models which fit into the total 32gb VRAM&lt;/p&gt; &lt;p&gt;It's difficult to tell exactly how other benchmarks were run, so I can't really say how they perform relative to others but they at least compete with low end modern cards like the 4060 Ti and the A4000, but at substantially lower cost.&lt;/p&gt; &lt;p&gt;Full details here of the software versions, hardware, prompt and models, variations in the output lengths, TPS, results at 250 and 125 watts, size reported by ollama ps, and USD/TPS: &lt;a href="https://docs.google.com/spreadsheets/d/1TjxpN0NYh-xb0ZwCpYr4FT-hG773_p1DEgxJaJtyRmY/edit?usp=sharing"&gt;https://docs.google.com/spreadsheets/d/1TjxpN0NYh-xb0ZwCpYr4FT-hG773_p1DEgxJaJtyRmY/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am very keen to hear how other card perform on the identical benchmark runs. I know they are on the bottom of the pack when it comes to performance for current builds, but I bought mine for $110USD each and last I checked were going for about $120USD, which to me makes them a steal.&lt;/p&gt; &lt;p&gt;For the models I tested, the fastest model was unsurprisingly &lt;code&gt;llama3.2:1b-instruct-q8_0&lt;/code&gt; maxing 150 tps, and the slowest was &lt;code&gt;FuseO1-DeekSeekR1-QwQ-SkyT1-32B-Preview-GGUF:Q6_K&lt;/code&gt; at 14tps.&lt;/p&gt; &lt;p&gt;I did get one refused on the prompt I used &lt;code&gt;Who discovered heliocentrism and how is that possible without being in space? Be verbose I want to know all about it.&lt;/code&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I can't provide information on who discovered heliocentrism or other topics that may be considered sensitive or controversial, such as the Copernican Revolution. Is there anything else I can help you with?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Which was really weird, and it happened more than once in llama, but no others, and I saw another different refusal on another model then never saw the refusal again&lt;/p&gt; &lt;p&gt;Some anticipated Q&amp;amp;A&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How did I deal with the ROCm problem?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The sarcastic answer is &amp;quot;&lt;em&gt;What ROCm problem?&amp;quot;&lt;/em&gt;. It seems to me like there's a lot of people who don't have an AMD card, people with an unsupported card, people on an unsupported distro, or people who ran it a long time ago who are spouting this.&lt;/p&gt; &lt;p&gt;The more serious answer is the ROCm install docs have the distro and hardware requirements. If you meet those it should just work. I initially tried in my distro of choice, which was not listed, and it was too hard so I gave up and installed Ubuntu and everything just worked. By &amp;quot;just worked&amp;quot; I mean I installed Ubuntu, followed the ROCm install guide, downloaded ollama, ran it, and ollama used the GPU without any hassle.&lt;/p&gt; &lt;p&gt;ComfyUI was similarly easy, except I had the additional steps of pulling the AMD repo, building, then running.&lt;/p&gt; &lt;p&gt;I have not tried any other apps.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How did I cool them?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I bought some 3D printed shrouds off Ebay that take an 80mm fan. I had to keep them power capped at 90 watts or they would overheat, and after some kind advice from here it was shown that the shrouds had an inefficient path for the air to travel and a custom solution would work better. I didn't do that because of time/money and instead bought silverstone 80mm industrial fans (10K RPM max) and they work a treat and keep them cool at 250 watts.&lt;/p&gt; &lt;p&gt;They are very loud so I bought a PWM controller which I keep on the case and adjust the fan speed for how hard I want to run the cards. It's outright too hard to control the fan speed through IPMI tool which is an app made by the devil to torment Linux users.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Would I buy them again?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Being old and relatively slow (I am guessing just slower than a 4070) I expected them to be temporary while I got started with AI, but they have been performing above my expectations. I would absolutely buy them again if I could live that build over again, and if I can mount the cards so there's more room, such as with PCIe extender cables, I would buy more two more MI50s for 64Gb VRAM.&lt;/p&gt; &lt;p&gt;For space and power reasons I would prefer MI60s or MI100s but this experience has me cemented as an Instinct fan and I have no interest in buying any nvidia card at their current new and used prices.&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;If there's any models you would like tested, let me know&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological_Ear393"&gt; /u/Psychological_Ear393 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iref1e/amd_instinct_mi50_detailed_benchmarks_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iref1e/amd_instinct_mi50_detailed_benchmarks_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iref1e/amd_instinct_mi50_detailed_benchmarks_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T07:28:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1irrxqs</id>
    <title>DataBridge Feature Dump: We'll implement anything you want!!</title>
    <updated>2025-02-17T19:09:59+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! &lt;/p&gt; &lt;p&gt;Thanks to the power of the &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; community, &lt;a href="https://github.com/databridge-org/databridge-core"&gt;DataBridge&lt;/a&gt; just hit 350 stars! As a token of our gratitude, we're committing to implementing the &lt;strong&gt;top 3 feature requests&lt;/strong&gt; from &lt;em&gt;you&lt;/em&gt; :) &lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to participate:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Leave your dream feature or improvement - RAG or otherwise - as a reply to this post! Upvote existing ideas you‚Äôd love to see. We‚Äôll tally the votes and build the &lt;strong&gt;top 3 most-requested features&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Let‚Äôs shape DataBridge‚Äôs future together‚Äîdrop your requests below! üöÄ&lt;/p&gt; &lt;p&gt;&lt;em&gt;(We'll start tallying at 2:00 pm ET on the 18th of Feb - happy to start working on stuff before that tho!)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Huge thanks again for being part of this journey! üôå ‚ù§Ô∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irrxqs/databridge_feature_dump_well_implement_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irrxqs/databridge_feature_dump_well_implement_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irrxqs/databridge_feature_dump_well_implement_anything/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T19:09:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1irhbrl</id>
    <title>PrivateLLMLens - A Zero-Server Web Interface for Local Ollama</title>
    <updated>2025-02-17T10:58:39+00:00</updated>
    <author>
      <name>/u/Specialist_Laugh_231</name>
      <uri>https://old.reddit.com/user/Specialist_Laugh_231</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1irhbrl/privatellmlens_a_zeroserver_web_interface_for/"&gt; &lt;img alt="PrivateLLMLens - A Zero-Server Web Interface for Local Ollama" src="https://preview.redd.it/5bfizklmkoje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4051a019d044303e3381fd284e4eee1a8ace21b9" title="PrivateLLMLens - A Zero-Server Web Interface for Local Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist_Laugh_231"&gt; /u/Specialist_Laugh_231 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5bfizklmkoje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irhbrl/privatellmlens_a_zeroserver_web_interface_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irhbrl/privatellmlens_a_zeroserver_web_interface_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T10:58:39+00:00</published>
  </entry>
</feed>
