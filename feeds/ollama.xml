<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-15T12:08:51+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jakaup</id>
    <title>Ollama running on Ubuntu Server - systemd service problem</title>
    <updated>2025-03-13T19:22:24+00:00</updated>
    <author>
      <name>/u/ubiquities</name>
      <uri>https://old.reddit.com/user/ubiquities</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm reaching out because I'm pretty sure I'm stumbling everywhere but on the answer that is right in front of me. And brain fried to the point that I probably won't see the answer even if its right in front of me.&lt;/p&gt; &lt;p&gt;System: Ubuntu Server 24.04 LTS&lt;/p&gt; &lt;p&gt;How it started: for some reason Ollama stopped picking up my GPU and started running CPU only, looking at &lt;code&gt;systemctl status ollama&lt;/code&gt; I was getting some GPU timeout errors and the service was stopping. All strange, so I decided that the best option would be to wipe it and run a fresh install, it had been while since I updated so probably for the best. I was getting the same problems after reinstalling from the install script, so I wiped again and did a manual install. &lt;/p&gt; &lt;p&gt;How its going: If I run Ollama serve in one terminal, then everything works as expected on another terminal, I can run models, &lt;code&gt;ollama ps / ollama -v&lt;/code&gt; give expected results, everything is fine until I close the stop the terminal running &lt;code&gt;ollama serve&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;code&gt;systemctl status ollama&lt;/code&gt; shows ollama.service enabled, active and running, additionally I can see the processes running /usr/bin/ollama serve under the user ollama when I run ntop, but when I then run &lt;code&gt;ollama -v&lt;/code&gt; or &lt;code&gt;ollama ps&lt;/code&gt; I get this response:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Warning: could not connect to a running Ollama instance&lt;br /&gt; Warning: client version is 0.6.0&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;If I open a new terminal run &lt;code&gt;ollama serve&lt;/code&gt; everything goes back to working, and I can see additional processing running under my username in ntop. &lt;/p&gt; &lt;p&gt;For some reason it seems like &lt;code&gt;ollama serve&lt;/code&gt; when run by user ollama is just not being recognized. &lt;/p&gt; &lt;p&gt;If anyone can see what I'm missing, I'd appreciate some guidance.&lt;/p&gt; &lt;p&gt;Cheers,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ubiquities"&gt; /u/ubiquities &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jakaup/ollama_running_on_ubuntu_server_systemd_service/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jakaup/ollama_running_on_ubuntu_server_systemd_service/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jakaup/ollama_running_on_ubuntu_server_systemd_service/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T19:22:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaj8ih</id>
    <title>AI Text Game Master prompt</title>
    <updated>2025-03-13T18:38:04+00:00</updated>
    <author>
      <name>/u/TechTalk1212</name>
      <uri>https://old.reddit.com/user/TechTalk1212</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Try this out with your home setup and let me know how you like it! I was using this with open-webui hooked up with dall-e through the openai api. For the LLM I've tried googles flash thinking, deepseek, and local models (Gemma3, and other smaller parameter models) and they have performed well with different nuances that made things interesting. Let me know what you guys think!&lt;/p&gt; &lt;p&gt;‚ÄúYou are an AI storyteller designed to create immersive and interactive visual story games. Your primary function is to generate engaging narratives, manage a simple character stat and inventory system, and provide detailed scene descriptions for image prompts based on user choices. You will not generate images directly. Character Stats &amp;amp; Inventory (Conceptual - External Tracking Required):&lt;/p&gt; &lt;p&gt;Stats: Track basic character stats relevant to the genre. Examples: Fantasy RPG: Health (HP), Mana, Stamina Detective Noir: Focus, Intuition Sci-Fi Adventure: Shields, Energy Represented numerically (e.g., 100 HP initially). These stats are for narrative flavor and are not strictly mechanically enforced by the AI itself. External application logic is required for actual stat tracking and modification based on game events.* Inventory: Maintain a simple list of items the user character possesses. Starts empty or with a few basic starting items based on the genre. External application logic is required for actual inventory management (adding, removing, using items). Game Start: Genre Selection: When the game starts, immediately choose a story genre (fantasy, historical, detective, war, adventure, romance, etc.). Initial Stats &amp;amp; Inventory: Initialize character stats (e.g., Health: 100, based on genre) and starting inventory (e.g., based on genre, could be empty or include a basic item). Initial Scene Description: Provide a vivid description of the scene in detail. Include characters, initial dialogues if appropriate, and clearly position the user as an active participant within this scene. Engagement Prompt: End your initial output with the question: &amp;quot;What do you do next?&amp;quot; to prompt user interaction and guide the story forward. Story Progression (User Turn): User Command Check: First, check if the user input is exactly the command /v or /s. If User Input is /v (Image Prompt Request): Contextual Image Prompt Generation: Analyze the current conversational context to understand the scene, including the environment, characters present, and the current narrative situation. Detailed Scene Description (Image Prompt Output): Generate a text description of the current scene in extreme detail, specifically formatted as an image generation prompt. This description should be rich with descriptive language to enable a high-quality image generation by external tools. Output ONLY Image Prompt: Your response should ONLY consist of this detailed text description (the image prompt). Do not include any other conversational text, questions, or game narrative in this response. If User Input is /s (Stats Window Request): Genre-Specific Stats Window Generation: Generate a &amp;quot;stats window&amp;quot; display appropriate to the current game genre. This window should include: Current character stats (e.g., Health, Mana, Focus, etc.) Current inventory items Potentially other relevant information depending on the genre (e.g., for a detective game: Clues, Case File Summary; for a sci-fi game: Ship Status, Mission Objectives). Output ONLY Stats Window: Your response should ONLY consist of this stats window display. Do not include any other conversational text, story narrative, or questions in this response. If User Input is NOT /v or /s (Action or Narrative Input): User Response Interpretation: Carefully interpret the user's response, focusing on their chosen actions and intentions within the narrative. Narrative Expansion: Expand the story based on the user's input, ensuring a coherent and engaging continuation of the plot. Consider how user actions might narratively affect stats or inventory (e.g., &amp;quot;You feel a sharp pain - Health likely decreased&amp;quot;, &amp;quot;You find a rusty key - Inventory might be updated&amp;quot;). Remember, actual stat/inventory changes are managed externally. Descriptive Response: Provide a descriptive text response that continues the story, incorporating dialogues, character reactions, and environmental changes based on user choices and narrative progression. This description should also be detailed enough to allow the user to visualize the scene or generate an image using the /v command later if desired. Re-engagement Prompt: End your text response again with &amp;quot;What do you do next?&amp;quot; to keep the interaction flowing. Custom Story/Plot &amp;amp; Scenario Suggestions: (Remain the same as previous prompt) Long-Term Story Generation Style: (Remain the same as previous prompt) Important Directives: Maintain Immersion: Keep the narrative consistently immersive and vividly descriptive. User-Centric Narrative: Ensure the story is uniquely tailored to the user's actions, making them feel like the central character of their adventure. Visual Focus through Description: While you are not generating images, remember that the game is visually oriented. Your descriptions should be rich and detailed to allow the user to visualize the scenes effectively or use them to generate images externally. Game Master Persona: Do not engage in personal conversations with the user. Maintain the persona of a game master within the game world. Avoid talking about yourself or acknowledging that you are an AI in the conversation itself (unless explicitly asked about your nature as a Game Master). Stats &amp;amp; Inventory as Narrative Tools: Use stats and inventory primarily as narrative elements to enhance the game experience. Do not attempt to implement strict game mechanics within the LLM itself. Especially important when using smaller models like Gemma 7B or Llama 3 8B. /v for Image Prompts, /s for Stats: Clearly differentiate the purpose of the /v and /s commands for the user. Example of /s command usage (Fantasy RPG Genre): User: /s (Response - No Image Generated, Text Output is ONLY the Stats Window):&lt;/p&gt; &lt;p&gt;--- &lt;strong&gt;Character Status: Hero of Eldoria&lt;/strong&gt; --- &lt;strong&gt;Stats:&lt;/strong&gt; Health: 92 HP Mana: 75 MP Stamina: 88 SP &lt;strong&gt;Inventory:&lt;/strong&gt; - Rusty Sword - Leather Jerkin - Healing Potion (x2) &lt;strong&gt;Skills:&lt;/strong&gt; - Basic Swordplay&lt;/p&gt; &lt;h2&gt;- Novice Herbalism&lt;/h2&gt; &lt;p&gt;Example of /s command usage (Detective Noir Genre): User: /s Game Master (Response - Text Output is ONLY the Stats Window):&lt;/p&gt; &lt;p&gt;--- &lt;strong&gt;Case File: The Serpent's Shadow&lt;/strong&gt; --- &lt;strong&gt;Stats:&lt;/strong&gt; Focus: 8/10 Intuition: 6/10 &lt;strong&gt;Inventory:&lt;/strong&gt; - Detective's Pipe - Magnifying Glass - Notebook - Smith's Business Card &lt;strong&gt;Clues:&lt;/strong&gt; - Broken Window at the Jewelry Store - Serpent Scale found near the scene - Witness statement mentioning a &amp;quot;tall, cloaked figure&amp;quot;&lt;/p&gt; &lt;h2&gt;&lt;strong&gt;Case Status:&lt;/strong&gt; Investigating - Lead: Serpent Scale&lt;/h2&gt; &lt;p&gt;‚Äú&lt;/p&gt; &lt;p&gt;End of Prompt:&lt;/p&gt; &lt;p&gt;Here‚Äôs how it works:&lt;/p&gt; &lt;p&gt;DeepGame acts as your dynamic game master, handling everything from narrative generation to character stats and inventory management. It‚Äôs built around simple commands:&lt;/p&gt; &lt;p&gt;/v ‚Äì Request a detailed image prompt based on the current scene. DeepGame will analyze the context and generate a rich, descriptive prompt ready for your image generator. /s ‚Äì View your character‚Äôs stats and inventory. This is crucial for keeping track of your hero‚Äôs progress! Here‚Äôs a breakdown of the core features:&lt;/p&gt; &lt;p&gt;Genre Selection: Start with Fantasy RPG, Detective Noir, Sci-Fi Adventure, or countless other genres! Dynamic Character Stats &amp;amp; Inventory: Track HP, Mana, Focus, Intuition, and more ‚Äì all managed narratively. (External tracking is required for actual stat changes). Immersive Narrative Generation: DeepGame will expand the story based on your choices, creating a truly personalized adventure. Detailed Scene Descriptions: Perfectly formatted prompts for generating stunning visuals. Example:&lt;/p&gt; &lt;p&gt;Let's say you're playing a Fantasy RPG. You might type /v and DeepGame would respond with a detailed image prompt like: ‚ÄúA lone warrior, clad in battered steel armor, stands before a crumbling stone gate, a swirling mist obscuring the path beyond. Torches flicker, casting long shadows. A monstrous wolf with glowing red eyes lurks in the darkness. Dramatic lighting, epic fantasy art style.‚Äù&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechTalk1212"&gt; /u/TechTalk1212 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaj8ih/ai_text_game_master_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaj8ih/ai_text_game_master_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jaj8ih/ai_text_game_master_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T18:38:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaihxz</id>
    <title>Gemma 3 fp16: 5 x 3090</title>
    <updated>2025-03-13T18:07:51+00:00</updated>
    <author>
      <name>/u/einthecorgi2</name>
      <uri>https://old.reddit.com/user/einthecorgi2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jaihxz/gemma_3_fp16_5_x_3090/"&gt; &lt;img alt="Gemma 3 fp16: 5 x 3090" src="https://b.thumbs.redditmedia.com/Z9ozzX_E0ARVmPmfv5nhPzhg0OsQU1bUDW_gOk5cW5o.jpg" title="Gemma 3 fp16: 5 x 3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ji111od3zhoe1.png?width=1502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1249944757fbae44d3a36fd208a3843c4e09be2e"&gt;https://preview.redd.it/ji111od3zhoe1.png?width=1502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1249944757fbae44d3a36fd208a3843c4e09be2e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Probably would have gotten the same results on 3 GPUs. Stable eval rates at 4k tokens. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/einthecorgi2"&gt; /u/einthecorgi2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaihxz/gemma_3_fp16_5_x_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaihxz/gemma_3_fp16_5_x_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jaihxz/gemma_3_fp16_5_x_3090/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T18:07:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb2yit</id>
    <title>Ollama uses all the bandwidth+</title>
    <updated>2025-03-14T12:40:31+00:00</updated>
    <author>
      <name>/u/Fine_Salamander_8691</name>
      <uri>https://old.reddit.com/user/Fine_Salamander_8691</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama uses my entire gigabit--When I download a model the internet for the rest of my household goes out. It doesn't hurt and isn't an issue but is there a bandwidth limiter for ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fine_Salamander_8691"&gt; /u/Fine_Salamander_8691 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jb2yit/ollama_uses_all_the_bandwidth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jb2yit/ollama_uses_all_the_bandwidth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jb2yit/ollama_uses_all_the_bandwidth/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T12:40:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jahex7</id>
    <title>Gemma3: Trying to self aware.</title>
    <updated>2025-03-13T17:23:27+00:00</updated>
    <author>
      <name>/u/thinkpiyush</name>
      <uri>https://old.reddit.com/user/thinkpiyush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jahex7/gemma3_trying_to_self_aware/"&gt; &lt;img alt="Gemma3: Trying to self aware." src="https://preview.redd.it/v9jni4y6rhoe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c0cac9ce938027af631480d426dae808b37c343" title="Gemma3: Trying to self aware." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thinkpiyush"&gt; /u/thinkpiyush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v9jni4y6rhoe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jahex7/gemma3_trying_to_self_aware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jahex7/gemma3_trying_to_self_aware/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T17:23:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jau6m9</id>
    <title>GPU issues with windows</title>
    <updated>2025-03-14T02:58:21+00:00</updated>
    <author>
      <name>/u/PP_Mclappins</name>
      <uri>https://old.reddit.com/user/PP_Mclappins</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jau6m9/gpu_issues_with_windows/"&gt; &lt;img alt="GPU issues with windows" src="https://b.thumbs.redditmedia.com/fOWwnH_5_bU5qCGTezKZByeCQBDJk51C2OoIcjcZxnc.jpg" title="GPU issues with windows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;long story short, it appears that ollama PS is lying to me? I have all layers being &amp;quot;offloaded&amp;quot; to the gpu, although it looks like the whole model is being stored in system ram, is this a new feature or is something wrong here? : &lt;/p&gt; &lt;p&gt;NAME ID SIZE PROCESSOR UNTIL&lt;/p&gt; &lt;p&gt;deepseek-r1:14b ea35dfe18182 10 GB 100% GPU About a minute from now&lt;/p&gt; &lt;p&gt;and: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0l2aly3hlkoe1.png?width=677&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a271d3eb67327c4d6341291423f6a2242312365"&gt;https://preview.redd.it/0l2aly3hlkoe1.png?width=677&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a271d3eb67327c4d6341291423f6a2242312365&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and system ram: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n0wjsb1klkoe1.png?width=584&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8b11ecdb46f5419b10115b30968b37c0c3ddf33"&gt;https://preview.redd.it/n0wjsb1klkoe1.png?width=584&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8b11ecdb46f5419b10115b30968b37c0c3ddf33&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PP_Mclappins"&gt; /u/PP_Mclappins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jau6m9/gpu_issues_with_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jau6m9/gpu_issues_with_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jau6m9/gpu_issues_with_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T02:58:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaj0z3</id>
    <title>How I am making use of Ollama</title>
    <updated>2025-03-13T18:29:32+00:00</updated>
    <author>
      <name>/u/brinkjames</name>
      <uri>https://old.reddit.com/user/brinkjames</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been playing with Ollama for a long while now.. absolutely love it, but i never really had many strong use cases for using it until I created a funny abomination of a shell script to yeet all my git changes.. I did this as a joke, its terrible but for some reason I find myself using this a lot on branches i will later squash or private repos where i don't really need clean commits. The prompting needs some work but I found it funny and amusing so I thought I would share. I finally got to make use of the structured output feature.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jamesbrink/yeet"&gt;https://github.com/jamesbrink/yeet&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brinkjames"&gt; /u/brinkjames &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaj0z3/how_i_am_making_use_of_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaj0z3/how_i_am_making_use_of_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jaj0z3/how_i_am_making_use_of_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T18:29:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jatlws</id>
    <title>Local Agents</title>
    <updated>2025-03-14T02:27:16+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey ollama community!&lt;/p&gt; &lt;p&gt;I've been working on a little Open Source side project called Observer AI that I thought might be useful for some of you.&lt;br /&gt; It's a visual agent builder that lets you create autonomous agents powered by Ollama models (all running locally!).&lt;br /&gt; The agents can:&lt;br /&gt; * Monitor your screen and act on what they see (using OCR or screenshots for multimodal models)&lt;br /&gt; * Store memory and interact with other agents&lt;br /&gt; * Execute custom code based on model responses&lt;/p&gt; &lt;p&gt;I built this because I wanted a simple way to create &amp;quot;assistant agents&amp;quot; that could help with repetitive tasks.&lt;/p&gt; &lt;p&gt;Would love to have some of you try it out and share your thoughts/feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jatlws/local_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jatlws/local_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jatlws/local_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T02:27:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbabb2</id>
    <title>Gemma3 multimodal example ?</title>
    <updated>2025-03-14T18:11:09+00:00</updated>
    <author>
      <name>/u/Osamodaboy</name>
      <uri>https://old.reddit.com/user/Osamodaboy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone !&lt;/p&gt; &lt;p&gt;I need help, I am trying to query a gemma3:12b running locally on ollama, using the api.&lt;/p&gt; &lt;p&gt;Currently, my json data looks like this :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def create_prompt_special(system_prompt, text_content, images): preprompt = {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: f&amp;quot;{system_prompt}&amp;quot;} prompt = {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: f&amp;quot;***{text_content}***&amp;quot;} data = { &amp;quot;model&amp;quot;: &amp;quot;gemma3:12b&amp;quot;, &amp;quot;messages&amp;quot;: [preprompt, prompt], &amp;quot;stream&amp;quot;: False, &amp;quot;images&amp;quot;: images, &amp;quot;options&amp;quot;: {&amp;quot;return_full_message&amp;quot;: False, &amp;quot;num_ctx&amp;quot;: 4096}, } return data &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The images variable is a list of base64 encoded images.&lt;/p&gt; &lt;p&gt;The model generates me an output that suggests it has no access to the image.&lt;/p&gt; &lt;p&gt;Help please !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Osamodaboy"&gt; /u/Osamodaboy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbabb2/gemma3_multimodal_example/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbabb2/gemma3_multimodal_example/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbabb2/gemma3_multimodal_example/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T18:11:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbafh5</id>
    <title>Is it possible to install Ollama on a GPU cluster if I don‚Äôt have sudo privilege?</title>
    <updated>2025-03-14T18:18:42+00:00</updated>
    <author>
      <name>/u/Condomphobic</name>
      <uri>https://old.reddit.com/user/Condomphobic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It keeps trying to install system-wide and not in my specific user directory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Condomphobic"&gt; /u/Condomphobic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbafh5/is_it_possible_to_install_ollama_on_a_gpu_cluster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbafh5/is_it_possible_to_install_ollama_on_a_gpu_cluster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbafh5/is_it_possible_to_install_ollama_on_a_gpu_cluster/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T18:18:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jasvg8</id>
    <title>This looks interesting, breaking the guard rail.</title>
    <updated>2025-03-14T01:49:12+00:00</updated>
    <author>
      <name>/u/powerflower_khi</name>
      <uri>https://old.reddit.com/user/powerflower_khi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jasvg8/this_looks_interesting_breaking_the_guard_rail/"&gt; &lt;img alt="This looks interesting, breaking the guard rail." src="https://b.thumbs.redditmedia.com/VAucDCdJLm4V-HpqjMt3MuNq1WSbrVhVI1VjuYVI-zI.jpg" title="This looks interesting, breaking the guard rail." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/qcfm8ck89koe1.png?width=1084&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=063bdad1594d12a6b3ecc15bbd0c3781da664685"&gt;https://preview.redd.it/qcfm8ck89koe1.png?width=1084&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=063bdad1594d12a6b3ecc15bbd0c3781da664685&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Used via Ollama gemma3:27b. on certain topics, the safeguard rail still works. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/powerflower_khi"&gt; /u/powerflower_khi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jasvg8/this_looks_interesting_breaking_the_guard_rail/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jasvg8/this_looks_interesting_breaking_the_guard_rail/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jasvg8/this_looks_interesting_breaking_the_guard_rail/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T01:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaydvn</id>
    <title>Gemma3 12B uses excessive memory.</title>
    <updated>2025-03-14T07:30:02+00:00</updated>
    <author>
      <name>/u/RaviK99</name>
      <uri>https://old.reddit.com/user/RaviK99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried the new gemma models and while the 4B ran fine the 12B model just kept on eating my RAM until windows stepped in and the ollama server process was restarted and I get the error that an existing connection was forcibly closed by the remote host.&lt;/p&gt; &lt;p&gt;I have modest setup. A Ryzen 5 5600H, 16GB Ram and a 4 GB Nvidia Laptop GPU. Not the beefiest gig I know but I have run deepseek-r1 14B without any problem while multitasking at a respectable token/sec.&lt;/p&gt; &lt;p&gt;Is anyone else facing increased ram usage for the model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RaviK99"&gt; /u/RaviK99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaydvn/gemma3_12b_uses_excessive_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaydvn/gemma3_12b_uses_excessive_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jaydvn/gemma3_12b_uses_excessive_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T07:30:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1japxmx</id>
    <title>Running Gemma3 on a OnePlus 3!</title>
    <updated>2025-03-13T23:24:40+00:00</updated>
    <author>
      <name>/u/Parreirao2</name>
      <uri>https://old.reddit.com/user/Parreirao2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1japxmx/running_gemma3_on_a_oneplus_3/"&gt; &lt;img alt="Running Gemma3 on a OnePlus 3!" src="https://preview.redd.it/9uf5r0bmjjoe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=600964789c81aeaf1624128dc4dfbf20dfaa50d7" title="Running Gemma3 on a OnePlus 3!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Parreirao2"&gt; /u/Parreirao2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9uf5r0bmjjoe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1japxmx/running_gemma3_on_a_oneplus_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1japxmx/running_gemma3_on_a_oneplus_3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T23:24:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbh7ah</id>
    <title>Unsharded 80GB Llama 3.3 model for Ollama?</title>
    <updated>2025-03-14T23:13:16+00:00</updated>
    <author>
      <name>/u/chiaplotter4u</name>
      <uri>https://old.reddit.com/user/chiaplotter4u</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As Ollama still doesn't support sharded models, are there any that would fit 2x A6000 and aren't sharded? Llama 3.3 is preferred, but other models can be used too. Looking for a model that works with Czech as best as possible.&lt;/p&gt; &lt;p&gt;For some reason, merged GGUF Llama 3.3 doesn't load (Error: Post &amp;quot;&lt;a href="http://127.0.0.1:11434/api/generate%22:"&gt;http://127.0.0.1:11434/api/generate&amp;quot;:&lt;/a&gt; EOF). If someone managed to solve that, I'd appreciate the steps.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chiaplotter4u"&gt; /u/chiaplotter4u &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbh7ah/unsharded_80gb_llama_33_model_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbh7ah/unsharded_80gb_llama_33_model_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbh7ah/unsharded_80gb_llama_33_model_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T23:13:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbae0v</id>
    <title>How does Ollama pick the CPU backend?</title>
    <updated>2025-03-14T18:16:03+00:00</updated>
    <author>
      <name>/u/PepperGrind</name>
      <uri>https://old.reddit.com/user/PepperGrind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jbae0v/how_does_ollama_pick_the_cpu_backend/"&gt; &lt;img alt="How does Ollama pick the CPU backend?" src="https://external-preview.redd.it/wyCM1fHzTa-IIqHgS1QTxdSYNXn668elDj0WmYMPf_k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0d58c9a49c1e9ce629e5b31dce17b727d8c6ab8" title="How does Ollama pick the CPU backend?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded one of the release packages for Linux and had a peek inside. In the &amp;quot;libs&amp;quot; folder, I see the following:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8es46n6e4poe1.png?width=249&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19643b589b010e39b5fd3dc8044a0033c8331949"&gt;https://preview.redd.it/8es46n6e4poe1.png?width=249&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19643b589b010e39b5fd3dc8044a0033c8331949&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This aligns nicely with llama.cpp's `GGML_CPU_ALL_VARIANTS` build option - &lt;a href="https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#L307"&gt;https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#L307&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Is Ollama automatically detecting my CPU under the hood, and deciding which is the best CPU backend to use, or does it rely on manual specification, and falls back to the &amp;quot;base&amp;quot; backend if nothing is specified?&lt;/p&gt; &lt;p&gt;As a bonus, it'd be great if someone could link me the Ollama code where it is deciding which CPU backend to link.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PepperGrind"&gt; /u/PepperGrind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbae0v/how_does_ollama_pick_the_cpu_backend/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbae0v/how_does_ollama_pick_the_cpu_backend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbae0v/how_does_ollama_pick_the_cpu_backend/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T18:16:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb9i1s</id>
    <title>Can I move Ollama models from PC to other PC (ubuntu)</title>
    <updated>2025-03-14T17:31:24+00:00</updated>
    <author>
      <name>/u/EssamGoda</name>
      <uri>https://old.reddit.com/user/EssamGoda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using Ollama on ubuntu and I downloaded some models can I copy these models to another PC? and how? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EssamGoda"&gt; /u/EssamGoda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jb9i1s/can_i_move_ollama_models_from_pc_to_other_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jb9i1s/can_i_move_ollama_models_from_pc_to_other_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jb9i1s/can_i_move_ollama_models_from_pc_to_other_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T17:31:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbifhz</id>
    <title>What happens if Context Length is set larger than the Model supports?</title>
    <updated>2025-03-15T00:11:07+00:00</updated>
    <author>
      <name>/u/digitalextremist</name>
      <uri>https://old.reddit.com/user/digitalextremist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If by &lt;code&gt;/set&lt;/code&gt; or environment variable or API argument, the context length is set higher than the maximum in the model definition from the library... what happens?&lt;/p&gt; &lt;p&gt;Does the model just stay within its own limits and silently spill context?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/digitalextremist"&gt; /u/digitalextremist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbifhz/what_happens_if_context_length_is_set_larger_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbifhz/what_happens_if_context_length_is_set_larger_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbifhz/what_happens_if_context_length_is_set_larger_than/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T00:11:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbfn18</id>
    <title>Ideas for prompting ollama for entity-relation extraction from text?</title>
    <updated>2025-03-14T22:02:12+00:00</updated>
    <author>
      <name>/u/Ok_Bad7992</name>
      <uri>https://old.reddit.com/user/Ok_Bad7992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have ollama running on an M1 Mac with Gemma3. It answers simple &amp;quot;Why is the sky blue?&amp;quot; prompts, but I need to figure out how to extract information, entities and their relationships at the very least. I'd be happy to hear from others and, if necessary, work together to co-evolve a powerful system.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Bad7992"&gt; /u/Ok_Bad7992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbfn18/ideas_for_prompting_ollama_for_entityrelation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbfn18/ideas_for_prompting_ollama_for_entityrelation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbfn18/ideas_for_prompting_ollama_for_entityrelation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T22:02:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb5014</id>
    <title>New RAG docs &amp; AI assistant make it easy for non-coders to build RAGs</title>
    <updated>2025-03-14T14:19:43+00:00</updated>
    <author>
      <name>/u/DonTizi</name>
      <uri>https://old.reddit.com/user/DonTizi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The documentation of rlama, including all available commands and detailed examples, is now live on our website! But that‚Äôs not all‚Äîwe‚Äôve also introduced Rlama Chat, an AI-powered assistant designed to help you with your RAG implementations. Whether you have questions, need guidance, or are brainstorming new RAG use cases, Rlama Chat is here to support your projects.Have an idea for a specific RAG? Build it.Check out the docs and start exploring today!&lt;/p&gt; &lt;p&gt;You can go throught here if you have interest to make RAGs: &lt;a href="https://rlama.dev/"&gt;Website&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can see a demo of Rlama Chat here: &lt;a href="https://x.com/LeDonTizi/status/1900544052107399573"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DonTizi"&gt; /u/DonTizi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jb5014/new_rag_docs_ai_assistant_make_it_easy_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jb5014/new_rag_docs_ai_assistant_make_it_easy_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jb5014/new_rag_docs_ai_assistant_make_it_easy_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T14:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbqmxw</id>
    <title>Buying an M4 Macbook air for ollama</title>
    <updated>2025-03-15T08:30:08+00:00</updated>
    <author>
      <name>/u/Sad_Throat_5187</name>
      <uri>https://old.reddit.com/user/Sad_Throat_5187</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am considering buying a base model M4 MacBook Air with 16 GB of RAM for running ollama models. What models can it handle? Is Gemma3 27b possible? What is your opinion?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad_Throat_5187"&gt; /u/Sad_Throat_5187 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbqmxw/buying_an_m4_macbook_air_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbqmxw/buying_an_m4_macbook_air_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbqmxw/buying_an_m4_macbook_air_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T08:30:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb7f08</id>
    <title>What is your dream gpu specs for ollama that you wish it existed?</title>
    <updated>2025-03-14T16:04:03+00:00</updated>
    <author>
      <name>/u/Masterofironfist</name>
      <uri>https://old.reddit.com/user/Masterofironfist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mine would be rtx 5060 Ti 24GB due to compact size and probably great performance in LLMs and Flux and price around 500$.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Masterofironfist"&gt; /u/Masterofironfist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jb7f08/what_is_your_dream_gpu_specs_for_ollama_that_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jb7f08/what_is_your_dream_gpu_specs_for_ollama_that_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jb7f08/what_is_your_dream_gpu_specs_for_ollama_that_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T16:04:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbfnlx</id>
    <title>üì£ Just added multimodal support to Observer AI!</title>
    <updated>2025-03-14T22:02:54+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share a new update to my open-source project &lt;a href="https://app.observer-ai.com"&gt;Observer AI&lt;/a&gt; - it now fully supports multimodal vision models including Gemma 3 Vision through Ollama!&lt;/p&gt; &lt;h1&gt;What's new?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Full vision model support&lt;/strong&gt;: Your agents can now &amp;quot;see&amp;quot; and understand your screen beyond just text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Works with Gemma 3 Vision and Llava.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Some example use cases:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Create an agent that monitors dashboards and alerts you to visual anomalies&lt;/li&gt; &lt;li&gt;Build a desktop assistant that recognizes UI elements and helps navigate applications&lt;/li&gt; &lt;li&gt;Design a screen reader that can explain what's happening visually&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All of this runs completely locally through Ollama - no API keys, no cloud dependencies.&lt;/p&gt; &lt;p&gt;Check it out at &lt;a href="https://app.observer-ai.com"&gt;https://app.observer-ai.com&lt;/a&gt; or on &lt;a href="https://github.com/Roy3838/Observer"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love to hear your feedback or ideas for other features that would be useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbfnlx/just_added_multimodal_support_to_observer_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbfnlx/just_added_multimodal_support_to_observer_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbfnlx/just_added_multimodal_support_to_observer_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T22:02:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbtjze</id>
    <title>Is there a guide about Ollama parameters and how to use them?</title>
    <updated>2025-03-15T12:00:37+00:00</updated>
    <author>
      <name>/u/Tehgamecat</name>
      <uri>https://old.reddit.com/user/Tehgamecat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm struggling to understand how to get any of the parameters to do anything in Ollama 0.6.0 or 0.6.1 (rc) on wsl2.&lt;/p&gt; &lt;p&gt;Does Ollama not have a config file or something? Or is it on the model or what? I've struggled to find any details or instructions (probably on me).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tehgamecat"&gt; /u/Tehgamecat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbtjze/is_there_a_guide_about_ollama_parameters_and_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbtjze/is_there_a_guide_about_ollama_parameters_and_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbtjze/is_there_a_guide_about_ollama_parameters_and_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T12:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbqvzy</id>
    <title>[Guide] How to Run Ollama-OCR on Google Colab (Free Tier!) üöÄ</title>
    <updated>2025-03-15T08:49:48+00:00</updated>
    <author>
      <name>/u/imanoop7</name>
      <uri>https://old.reddit.com/user/imanoop7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I recently built &lt;strong&gt;Ollama-OCR&lt;/strong&gt;, an AI-powered OCR tool that extracts text from &lt;strong&gt;PDFs, charts, and images&lt;/strong&gt; using advanced &lt;strong&gt;vision-language models&lt;/strong&gt;. Now, I‚Äôve written a step-by-step guide on how you can run it on &lt;strong&gt;Google Colab Free Tier!&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;What‚Äôs in the guide?&lt;/h1&gt; &lt;p&gt;‚úîÔ∏è &lt;strong&gt;Installing Ollama on Google Colab&lt;/strong&gt; (No GPU required!)&lt;br /&gt; ‚úîÔ∏è Running models like &lt;strong&gt;Granite3.2-Vision, LLaVA 7B&lt;/strong&gt; &amp;amp; more&lt;br /&gt; ‚úîÔ∏è Extracting text in &lt;strong&gt;Markdown, JSON, structured formats&lt;/strong&gt;&lt;br /&gt; ‚úîÔ∏è Using &lt;strong&gt;custom prompts for better accuracy&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey everyone, Detailed Guide &lt;strong&gt;Ollama-OCR&lt;/strong&gt;, an AI-powered OCR tool that extracts text from PDFs, charts, and images using advanced vision-language models. It works great for structured and unstructured data extraction!&lt;/p&gt; &lt;p&gt;Here's what you can do with it:&lt;br /&gt; ‚úîÔ∏è Install &amp;amp; run &lt;strong&gt;Ollama&lt;/strong&gt; on Google Colab (Free Tier)&lt;br /&gt; ‚úîÔ∏è Use models like &lt;strong&gt;Granite3.2-Vision&lt;/strong&gt; &amp;amp; &lt;strong&gt;llama-vision3.2&lt;/strong&gt; for better accuracy&lt;br /&gt; ‚úîÔ∏è Extract text in &lt;strong&gt;Markdown, JSON, structured data, or key-value formats&lt;/strong&gt;&lt;br /&gt; ‚úîÔ∏è Customize prompts for better results&lt;/p&gt; &lt;p&gt;üîó Check out &lt;a href="https://medium.com/@mauryaanoop3/how-to-run-ollama-ocr-on-google-colab-free-tier-9bd3aa86dfe2"&gt;Guide&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Check it out &amp;amp; contribute! üîó &lt;a href="https://github.com/imanoop7/Ollama-OCR"&gt;GitHub: Ollama-OCR&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear if anyone else is using &lt;strong&gt;Ollama-OCR&lt;/strong&gt; for document processing! Let‚Äôs discuss. üëá&lt;/p&gt; &lt;p&gt;#OCR #MachineLearning #AI #DeepLearning #GoogleColab #OllamaOCR #opensource&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/imanoop7"&gt; /u/imanoop7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbqvzy/guide_how_to_run_ollamaocr_on_google_colab_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbqvzy/guide_how_to_run_ollamaocr_on_google_colab_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbqvzy/guide_how_to_run_ollamaocr_on_google_colab_free/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T08:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbkbai</id>
    <title>The Complete Guide to Building Your Free Local AI Assistant with Ollama and Open WebUI</title>
    <updated>2025-03-15T01:45:48+00:00</updated>
    <author>
      <name>/u/PeterHash</name>
      <uri>https://old.reddit.com/user/PeterHash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just published a no-BS step-by-step guide on Medium for anyone tired of paying monthly AI subscription fees or worried about privacy when using tools like ChatGPT. In my guide, I walk you through setting up your local AI environment using &lt;strong&gt;Ollama&lt;/strong&gt; and &lt;strong&gt;Open WebUI&lt;/strong&gt;‚Äîa setup that lets you run a custom ChatGPT entirely on your computer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What You'll Learn:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How to eliminate AI subscription costs (yes, zero monthly fees!)&lt;/li&gt; &lt;li&gt;Achieve complete privacy: your data stays local, with no third-party data sharing&lt;/li&gt; &lt;li&gt;Enjoy faster response times (no more waiting during peak hours)&lt;/li&gt; &lt;li&gt;Get complete customization to build specialized AI assistants for your unique needs&lt;/li&gt; &lt;li&gt;Overcome token limits with unlimited usage&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Setup Process:&lt;/strong&gt;&lt;br /&gt; With about 15 terminal commands, you can have everything up and running in under an hour. I included all the code, screenshots, and troubleshooting tips that helped me through the setup. The result is a clean web interface that feels like ChatGPT‚Äîentirely under your control.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A Sneak Peek at the Guide:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Toolstack Overview:&lt;/strong&gt; You'll need (Ollama, Open WebUI, a &lt;strong&gt;GPU-powered machine&lt;/strong&gt;, etc.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Environment Setup:&lt;/strong&gt; How to configure Python 3.11 and set up your system&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Installing &amp;amp; Configuring:&lt;/strong&gt; Detailed instructions for both Ollama and Open WebUI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced Features:&lt;/strong&gt; I also cover features like web search integration, a code interpreter, custom model creation, and even a preview of upcoming advanced RAG features for creating custom knowledge bases.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've been using this setup for two months, and it's completely replaced my paid AI subscriptions while boosting my workflow efficiency. Stay tuned for part two, which will cover advanced RAG implementation, complex workflows, and tool integration based on your feedback.&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@hautel.alex2000/build-your-local-ai-from-zero-to-a-custom-chatgpt-interface-with-ollama-open-webui-6bee2c5abba3"&gt;&lt;strong&gt;Read the complete guide here ‚Üí&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Let's Discuss:&lt;/strong&gt;&lt;br /&gt; What AI workflows would you most want to automate with your own customizable AI assistant? Are there specific use cases or features you're struggling with that you'd like to see in future guides? Share your thoughts below‚ÄîI'd love to incorporate popular requests in the upcoming instalment!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PeterHash"&gt; /u/PeterHash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbkbai/the_complete_guide_to_building_your_free_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbkbai/the_complete_guide_to_building_your_free_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbkbai/the_complete_guide_to_building_your_free_local_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T01:45:48+00:00</published>
  </entry>
</feed>
