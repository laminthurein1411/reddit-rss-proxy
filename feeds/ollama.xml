<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-01T15:34:48+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1iem982</id>
    <title>RAG and Perplexity like Search</title>
    <updated>2025-01-31T18:56:29+00:00</updated>
    <author>
      <name>/u/atomique90</name>
      <uri>https://old.reddit.com/user/atomique90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wonder which stack you guys use to solve the following tasks: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;RAG (Pipeline): How do you guys get your data chunked and stored into a vectorized database? Especially what do you do to upsert or update the data? I am interested in solutions that are local only and dont rely on a cloud service. &lt;/li&gt; &lt;li&gt;How do you realize perplexity like chat search with ollama? I tried perplexica but especially with k8s I had many problems to kickstart it. I would love to have it in an UI like open webui. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I already tried to play a bit with ollama, perplexica, open webui, flowise, n8n, qdrant and psql (chat memory). &lt;/p&gt; &lt;p&gt;I am just curious what would be the best way to solve this and improve my journey! &lt;/p&gt; &lt;p&gt;Thanks a lot&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atomique90"&gt; /u/atomique90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iem982/rag_and_perplexity_like_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iem982/rag_and_perplexity_like_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iem982/rag_and_perplexity_like_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T18:56:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ievb8g</id>
    <title>Running ollama with Llama3.2:3b on a Raspberry Pi 5 8 Gig - runs, but "unimportable"?</title>
    <updated>2025-02-01T01:38:56+00:00</updated>
    <author>
      <name>/u/DelosBoard2052</name>
      <uri>https://old.reddit.com/user/DelosBoard2052</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've got Llama3.2:3b running well on a Raspberry Pi, and I'm running Python version 3.11.2. I have a small script to capture the output from Llama, and the first line of the script is &amp;quot;import ollama&amp;quot;. I get the old ModuleNotFoundError.&lt;/p&gt; &lt;p&gt;I installed ollama via curl.&lt;/p&gt; &lt;p&gt;I am running a virtual environment, and installed ollama in that environment.&lt;/p&gt; &lt;p&gt;When I try, in either the VE or the Non-VE windows, to find where ollama is installed, via either &amp;quot;pip show ollama&amp;quot; or the string &amp;quot;python -c 'import ollama; print(ollama.__file__)'&amp;quot;, it says &amp;quot;no module named ollama&amp;quot;.&lt;/p&gt; &lt;p&gt;Where does ollama install? It's not in my /usr/lib/python3.11/site-packages directory in either VE or Non-VE windows. And yet it runs perfectly. &lt;/p&gt; &lt;p&gt;I feel dumb. Guidance greatly appreciated here, thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DelosBoard2052"&gt; /u/DelosBoard2052 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ievb8g/running_ollama_with_llama323b_on_a_raspberry_pi_5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ievb8g/running_ollama_with_llama323b_on_a_raspberry_pi_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ievb8g/running_ollama_with_llama323b_on_a_raspberry_pi_5/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T01:38:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iegbea</id>
    <title>Why is open-webui this size?</title>
    <updated>2025-01-31T14:43:53+00:00</updated>
    <author>
      <name>/u/DevuDixit</name>
      <uri>https://old.reddit.com/user/DevuDixit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iegbea/why_is_openwebui_this_size/"&gt; &lt;img alt="Why is open-webui this size?" src="https://external-preview.redd.it/Erly_C0iblWMXf_1Jomyy7GuVEMgel-rmrf80xwRk9I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa064b742fd7f951e2c00ee5d40558b26fb7d7d2" title="Why is open-webui this size?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/87a620piccge1.png?width=1102&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=824a6c45bcd69a837b8fe314f55546e743df3eee"&gt;https://preview.redd.it/87a620piccge1.png?width=1102&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=824a6c45bcd69a837b8fe314f55546e743df3eee&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/id1h4wgudcge1.png?width=1089&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f88f10529c414c7eaa7e3b463f062079af32aeb1"&gt;https://preview.redd.it/id1h4wgudcge1.png?width=1089&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f88f10529c414c7eaa7e3b463f062079af32aeb1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I was just trying to install and use the &lt;a href="https://github.com/open-webui/open-webui"&gt;open-webui&lt;/a&gt; for using the gui and I was curious about these files (marked by red) it is pulling. I have used some local web based guis in the past and all of them were under 100 MBs of size. So why is this comparatively bulky ? ( just curious ; )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DevuDixit"&gt; /u/DevuDixit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegbea/why_is_openwebui_this_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegbea/why_is_openwebui_this_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iegbea/why_is_openwebui_this_size/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T14:43:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1iegog2</id>
    <title>Would x2 RTX 3060 12GB suffice advanced models like DeepSeek R1 14B or higher?</title>
    <updated>2025-01-31T15:00:54+00:00</updated>
    <author>
      <name>/u/GamerGuy95953</name>
      <uri>https://old.reddit.com/user/GamerGuy95953</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am determining if I should buy 2 RTX 3060 12GB (around $270 US each.) wondering if it‚Äôs even worth it over other options. Based on my research AI models perform better with lots of VRAM over clock speeds. &lt;/p&gt; &lt;p&gt;My server PC setup currently is a GTX 1050 and a Ryzen 7 2700X. I am planning to also upgrade the CPU to R9 something. It depending on the price for the R9 models. &lt;/p&gt; &lt;p&gt;Any suggestions are appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GamerGuy95953"&gt; /u/GamerGuy95953 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegog2/would_x2_rtx_3060_12gb_suffice_advanced_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegog2/would_x2_rtx_3060_12gb_suffice_advanced_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iegog2/would_x2_rtx_3060_12gb_suffice_advanced_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T15:00:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1iewe66</id>
    <title>Ollama for Knowledge base</title>
    <updated>2025-02-01T02:35:52+00:00</updated>
    <author>
      <name>/u/Videodad</name>
      <uri>https://old.reddit.com/user/Videodad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spun up Ollama on a docker and was goofing with it, having it read some mark down files to see what it could do. Got me thinking, maybe I could use it for a Knowledge Base for the helpdesk, but if I do this, do I need it to do anything but use my files to give output, and can I present images as part of this. &lt;/p&gt; &lt;p&gt;So, question is, has anyone else spun up Ollama with just the data that would be needed for something like this. If so, what did you do, what did you learn, and what pitfalls can you share to avoid?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Videodad"&gt; /u/Videodad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iewe66/ollama_for_knowledge_base/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iewe66/ollama_for_knowledge_base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iewe66/ollama_for_knowledge_base/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T02:35:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iewzve</id>
    <title>I'm having trouble getting my local Chrome extension to work with the Ollama local on ubuntu. Need assistance</title>
    <updated>2025-02-01T03:08:25+00:00</updated>
    <author>
      <name>/u/hasan_py</name>
      <uri>https://old.reddit.com/user/hasan_py</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using ubuntu 24. In my chrome extension I'm trying to call this two api. But always giving me CORS error. Tried out to add OLLAMA_HOST=&amp;quot;0.0.0.0&amp;quot; OLLAMA_ORIGINS=&amp;quot;chrome-extension://*&amp;quot; in my bashrc and also zshrc but not worked. Anyone can help me?? &lt;/p&gt; &lt;pre&gt;&lt;code&gt;http://127.0.0.1:11434/api/chat http://127.0.0.1:11434/api/generate &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hasan_py"&gt; /u/hasan_py &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iewzve/im_having_trouble_getting_my_local_chrome/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iewzve/im_having_trouble_getting_my_local_chrome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iewzve/im_having_trouble_getting_my_local_chrome/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T03:08:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iet8lx</id>
    <title>Place Olama model on external drive</title>
    <updated>2025-01-31T23:58:07+00:00</updated>
    <author>
      <name>/u/jaserjsk</name>
      <uri>https://old.reddit.com/user/jaserjsk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it possible to download a model from Olama and place it on external harddrive?&lt;br /&gt; I'm searhing for a way, but I cannot figure out how!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jaserjsk"&gt; /u/jaserjsk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iet8lx/place_olama_model_on_external_drive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iet8lx/place_olama_model_on_external_drive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iet8lx/place_olama_model_on_external_drive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T23:58:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1if1g12</id>
    <title>Deepseek version vs ChatGPT</title>
    <updated>2025-02-01T07:47:59+00:00</updated>
    <author>
      <name>/u/jaserjsk</name>
      <uri>https://old.reddit.com/user/jaserjsk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are many versions of Deepseek R1 to download, but if I now wanted to have a model installed locally&lt;br /&gt; on my PC and if wanted to pick the model that would be as powerful as ChatGPT, which version of Deepseek would I need!&lt;/p&gt; &lt;p&gt;I tried the 32B model and it was way to slow on my RTX 3070 8GB.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jaserjsk"&gt; /u/jaserjsk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if1g12/deepseek_version_vs_chatgpt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if1g12/deepseek_version_vs_chatgpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if1g12/deepseek_version_vs_chatgpt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T07:47:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iey7g7</id>
    <title>WSL2 says not enough memory but Nivida control panel says plenty.</title>
    <updated>2025-02-01T04:15:43+00:00</updated>
    <author>
      <name>/u/azimuth79b</name>
      <uri>https://old.reddit.com/user/azimuth79b</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iey7g7/wsl2_says_not_enough_memory_but_nivida_control/"&gt; &lt;img alt="WSL2 says not enough memory but Nivida control panel says plenty." src="https://preview.redd.it/3gazoiw6egge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba94ac15ddf35d2b7dd393095dc8ec04ff82ed62" title="WSL2 says not enough memory but Nivida control panel says plenty." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I should be able to use llama3.3 given I have 81.5 GB VRAM&lt;/p&gt; &lt;p&gt;How to fix please?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/azimuth79b"&gt; /u/azimuth79b &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3gazoiw6egge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iey7g7/wsl2_says_not_enough_memory_but_nivida_control/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iey7g7/wsl2_says_not_enough_memory_but_nivida_control/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T04:15:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie210z</id>
    <title>Got Deepseek R1 1.5b running locally on Pixel 8 pro</title>
    <updated>2025-01-31T00:24:19+00:00</updated>
    <author>
      <name>/u/Teradyyne</name>
      <uri>https://old.reddit.com/user/Teradyyne</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"&gt; &lt;img alt="Got Deepseek R1 1.5b running locally on Pixel 8 pro" src="https://external-preview.redd.it/cTk0emwwZ3ozOGdlMYbCJM1MQLfOpw8fF1FnxkqAh7visCMP7lFjyVXppg7i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=263215164e10efcd203b94bae4672c47e11c63d8" title="Got Deepseek R1 1.5b running locally on Pixel 8 pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Teradyyne"&gt; /u/Teradyyne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cha0fplz38ge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T00:24:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1if24tv</id>
    <title>AI OCR</title>
    <updated>2025-02-01T08:39:23+00:00</updated>
    <author>
      <name>/u/Tricky_Event5031</name>
      <uri>https://old.reddit.com/user/Tricky_Event5031</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need to create some ai system that can recreate scanned documents using maybe some vision model only or maybe like combination of some OCR with some vision model or chat model. But few things to consider is that i need to deploy the model later on locally so no chance of using apis , right now i can test the solution using api of that model but not like in production.&lt;/p&gt; &lt;p&gt;The documents structure keeps changing , no fix scanned docs, no fix quality, nothing is fix , some might have tables , some might not have those , some are poor quality scanned &lt;/p&gt; &lt;p&gt;Also i want to extract all information and return in json&lt;/p&gt; &lt;p&gt;So help me guys open to all recommendations and solutions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tricky_Event5031"&gt; /u/Tricky_Event5031 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if24tv/ai_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if24tv/ai_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if24tv/ai_ocr/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T08:39:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1if2wy6</id>
    <title>How can I change the model storage path?</title>
    <updated>2025-02-01T09:38:44+00:00</updated>
    <author>
      <name>/u/Ayaouniya</name>
      <uri>https://old.reddit.com/user/Ayaouniya</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run Ollama on a server. There are many people in this server. I have no Sudo permissions. After the inspection, I found that the model I downloaded is in the directory of others (maybe others are running Ollama). I tried to change the environment The variable is not effective, and Ollama.service I can't find this file&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ayaouniya"&gt; /u/Ayaouniya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if2wy6/how_can_i_change_the_model_storage_path/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if2wy6/how_can_i_change_the_model_storage_path/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if2wy6/how_can_i_change_the_model_storage_path/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T09:38:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ietdh4</id>
    <title>How to train your dragon? (dragon = deepseek R1 on Ollama)</title>
    <updated>2025-02-01T00:04:04+00:00</updated>
    <author>
      <name>/u/Flying_Motorbike</name>
      <uri>https://old.reddit.com/user/Flying_Motorbike</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm a complete beginner here. I have a large collection of PDFs (around 400) that I want the AI to read. Once the ‚Äòlearning‚Äô phase is complete, I want it to be able to answer questions and provide references to the PDFs where it found the information. I‚Äôd like this process to be repeated without the need for ‚Äòrelearning‚Äô. I‚Äôve searched through the existing posts, but I could only find information on RAG, which I‚Äôm not sure can be used for the read-once-use-later approach that I‚Äôm looking for. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flying_Motorbike"&gt; /u/Flying_Motorbike &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ietdh4/how_to_train_your_dragon_dragon_deepseek_r1_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ietdh4/how_to_train_your_dragon_dragon_deepseek_r1_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ietdh4/how_to_train_your_dragon_dragon_deepseek_r1_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T00:04:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iem5it</id>
    <title>AI Tools as a Hiring Requirement? Just Saw a Job Post That Blew My Mind</title>
    <updated>2025-01-31T18:52:03+00:00</updated>
    <author>
      <name>/u/Far_Flamingo5333</name>
      <uri>https://old.reddit.com/user/Far_Flamingo5333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just came across a job listing that explicitly requires experience with Cursor and Windsurf as part of the stack. Not ‚Äúnice to have‚Äù it‚Äôs actually listed as a preference for hiring.&lt;/p&gt; &lt;p&gt;The post reads:&lt;/p&gt; &lt;p&gt;‚ÄúWe‚Äôre hiring our first engineer(s)!&lt;/p&gt; &lt;p&gt;üëæ Prefer AI-native (you use Cursor, Windsurf, or built your own setup)&lt;/p&gt; &lt;p&gt;üëæ Can showcase past projects/work&lt;/p&gt; &lt;p&gt;üëæ Based in the Bay Area &amp;amp; down for 3 days/week in-person&lt;/p&gt; &lt;p&gt;‚Ä¶we don‚Äôt technically have an office yet, so you can help us decide where to go‚Äù&lt;/p&gt; &lt;p&gt;I‚Äôm honestly amazed and astonished. This is the first time I‚Äôve seen AI coding tools being treated as a must-have skill rather than just a productivity boost. It makes me wonder:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Are we at the point where AI-assisted coding is a hard requirement for top tech jobs&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Will future engineers be judged not just on their raw coding ability but how well they integrate AI into their workflow&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;How long until AI-native workflows become the default expectation everywhere?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Would love to hear thoughts from others, are you seeing this trend in hiring, or is this just an early sign of what‚Äôs to come?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far_Flamingo5333"&gt; /u/Far_Flamingo5333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iem5it/ai_tools_as_a_hiring_requirement_just_saw_a_job/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iem5it/ai_tools_as_a_hiring_requirement_just_saw_a_job/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iem5it/ai_tools_as_a_hiring_requirement_just_saw_a_job/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T18:52:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1if1hrg</id>
    <title>Did anyone host Deepseek R1 671B locally and got cloud like t/s?</title>
    <updated>2025-02-01T07:51:44+00:00</updated>
    <author>
      <name>/u/Orange-Hokage</name>
      <uri>https://old.reddit.com/user/Orange-Hokage</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What hardware is required to host Deepseek R1 671B so that it can give responses at a speed comparable to the cloud version? Has anyone done it? I found some people who have done it using CPUs extensively rather than GPUs and they got 4-5 t/s approx. Is there anyone who used GPUs extensively or got t/s comparable to the cloud?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Orange-Hokage"&gt; /u/Orange-Hokage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if1hrg/did_anyone_host_deepseek_r1_671b_locally_and_got/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if1hrg/did_anyone_host_deepseek_r1_671b_locally_and_got/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if1hrg/did_anyone_host_deepseek_r1_671b_locally_and_got/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T07:51:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1if5ljk</id>
    <title>Ollama version for given Laptop Specs</title>
    <updated>2025-02-01T12:48:21+00:00</updated>
    <author>
      <name>/u/Ok_Butterscotch_2313</name>
      <uri>https://old.reddit.com/user/Ok_Butterscotch_2313</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I think this is a FAQ in this thread but, you know, different configs may allow more/less robust OLlama versions to run flawlessly. In my vast ignorance about OLlama HW requirements, I installed the latest big model version which just got stuck by simply saying hello. My laptops specs are as follow, please help me to determine which is the best version for me:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU: Intel Core i5 11400H @ 2.70GHz. 6 cores, 12 Threads. Max Turbo Frequency 4.50 GHz. Cache 12 MB.&lt;/li&gt; &lt;li&gt;RAM: 32GB DDR4.&lt;/li&gt; &lt;li&gt;GPU: NVIDIA GeForce RTX 3050. 4GB. 2048 @ 1.24 - 1.5 GHz 128 Bit @ 12000 MHz.&lt;/li&gt; &lt;li&gt;OS: Windows 11 Pro.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Butterscotch_2313"&gt; /u/Ok_Butterscotch_2313 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if5ljk/ollama_version_for_given_laptop_specs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if5ljk/ollama_version_for_given_laptop_specs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if5ljk/ollama_version_for_given_laptop_specs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T12:48:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1if6kg8</id>
    <title>Llama3.2 is actually quite good, sometimes it surprises me with good answers, consider this example</title>
    <updated>2025-02-01T13:43:23+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1if6kg8/llama32_is_actually_quite_good_sometimes_it/"&gt; &lt;img alt="Llama3.2 is actually quite good, sometimes it surprises me with good answers, consider this example" src="https://b.thumbs.redditmedia.com/MxXX6Y3BoBNgiH8bWnNceG42dEH3xNVY9mQfK3f32YE.jpg" title="Llama3.2 is actually quite good, sometimes it surprises me with good answers, consider this example" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0trxou3b7jge1.png?width=1246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8df4977b6eda27fe04df0fc355a3289525d2630d"&gt;https://preview.redd.it/0trxou3b7jge1.png?width=1246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8df4977b6eda27fe04df0fc355a3289525d2630d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if6kg8/llama32_is_actually_quite_good_sometimes_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if6kg8/llama32_is_actually_quite_good_sometimes_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if6kg8/llama32_is_actually_quite_good_sometimes_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T13:43:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1if6q94</id>
    <title>Modal requires more ram, will swap memory work ?</title>
    <updated>2025-02-01T13:51:55+00:00</updated>
    <author>
      <name>/u/Significant_Cap13</name>
      <uri>https://old.reddit.com/user/Significant_Cap13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SO i wanted to try running deepseek r1 smallest 1.5Billion parameters modal on 1gb ram. I took it as a challenge and got slapped with the error message saying -&lt;br /&gt; &amp;quot;modal requires more system memory&amp;quot;&lt;/p&gt; &lt;p&gt;now i can obviously add one more ram stick but since it's a challenge i can't do that. So i was wondering is there any way to run it just like this ?&lt;br /&gt; I mean by using swap memory ?&lt;br /&gt; I am using bodhi linux to run it and the main goal here is not the performance I will get but just to run it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant_Cap13"&gt; /u/Significant_Cap13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if6q94/modal_requires_more_ram_will_swap_memory_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if6q94/modal_requires_more_ram_will_swap_memory_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if6q94/modal_requires_more_ram_will_swap_memory_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T13:51:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1iewdmc</id>
    <title>Why is the ollama file size smaller than the models on hugging face?</title>
    <updated>2025-02-01T02:35:04+00:00</updated>
    <author>
      <name>/u/Any_Dot769</name>
      <uri>https://old.reddit.com/user/Any_Dot769</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I'm downloading the R1 32GB model from Ollama and it looks like the total size in GB is much smaller (20GB) than the one on hugging face (~65GB). How is this possible? Am I misunderstanding the GB figure on ollama? Is it a guide on how much VRAM is needed for the model? I'm new to Ollama so not sure how it works, any advice is much appreciated! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Dot769"&gt; /u/Any_Dot769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iewdmc/why_is_the_ollama_file_size_smaller_than_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iewdmc/why_is_the_ollama_file_size_smaller_than_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iewdmc/why_is_the_ollama_file_size_smaller_than_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T02:35:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iezx7o</id>
    <title>Ollama Model Benchmark Tool for Development</title>
    <updated>2025-02-01T06:01:18+00:00</updated>
    <author>
      <name>/u/binoy_manoj</name>
      <uri>https://old.reddit.com/user/binoy_manoj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iezx7o/ollama_model_benchmark_tool_for_development/"&gt; &lt;img alt="Ollama Model Benchmark Tool for Development" src="https://external-preview.redd.it/oBe7rH-Bc5AYJcp-6yFnxIyDF9OJYJwLuLN_Ozvb8uM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=295cac930833f6c08e012f4e6a8d0bbe5b53c6c0" title="Ollama Model Benchmark Tool for Development" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey community! üëã&lt;/p&gt; &lt;p&gt;I just released an open-source tool that helps developers benchmark different Ollama models specifically for development tasks. If you're using Ollama for coding assistance, this might be useful for finding the best model for your needs.&lt;/p&gt; &lt;h1&gt;What it does:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Runs your installed models through real-world coding scenarios&lt;/li&gt; &lt;li&gt;Measures response times and generates detailed performance reports&lt;/li&gt; &lt;li&gt;Tests things like component creation, API routes, and data fetching patterns&lt;/li&gt; &lt;li&gt;Runs in an isolated environment (so it's easy to test and remove)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Sample output:&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/azcz96flwgge1.png?width=1921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30ff786ca58617260d7c65f2518a27ce01c66045"&gt;https://preview.redd.it/azcz96flwgge1.png?width=1921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30ff786ca58617260d7c65f2518a27ce01c66045&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why I built this:&lt;/h1&gt; &lt;p&gt;I was running multiple models locally and wanted a systematic way to compare their:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Speed (response times)&lt;/li&gt; &lt;li&gt;Code quality&lt;/li&gt; &lt;li&gt;Understanding of modern development framework&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;github: &lt;a href="https://github.com/binoymanoj/ollama-benchmark/"&gt;https://github.com/binoymanoj/ollama-benchmark/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Post your results here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/binoy_manoj"&gt; /u/binoy_manoj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iezx7o/ollama_model_benchmark_tool_for_development/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iezx7o/ollama_model_benchmark_tool_for_development/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iezx7o/ollama_model_benchmark_tool_for_development/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T06:01:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1if8nav</id>
    <title>Build a Research Agent with Deepseek, LangGraph, and Streamlit</title>
    <updated>2025-02-01T15:26:40+00:00</updated>
    <author>
      <name>/u/Special_Community179</name>
      <uri>https://old.reddit.com/user/Special_Community179</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1if8nav/build_a_research_agent_with_deepseek_langgraph/"&gt; &lt;img alt="Build a Research Agent with Deepseek, LangGraph, and Streamlit" src="https://external-preview.redd.it/Nr7dYgUYitfYQy_0koUx1hriUqDWeqL3Ujq5Y1KRrMg.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c55b1c22ce35f10f6b7c57d4d18215de642e104" title="Build a Research Agent with Deepseek, LangGraph, and Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special_Community179"&gt; /u/Special_Community179 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=nRBiD_7l2Mg&amp;amp;list=PLp01ObP3udmq2quR-RfrX4zNut_t_kNot"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if8nav/build_a_research_agent_with_deepseek_langgraph/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if8nav/build_a_research_agent_with_deepseek_langgraph/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T15:26:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1iey40e</id>
    <title>What is the best way to setup a local LLM to use a local PDF</title>
    <updated>2025-02-01T04:10:09+00:00</updated>
    <author>
      <name>/u/Outrageous-Win-3244</name>
      <uri>https://old.reddit.com/user/Outrageous-Win-3244</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How would you create a local AI chatbot that uses one or more local PDFs as information source? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Win-3244"&gt; /u/Outrageous-Win-3244 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iey40e/what_is_the_best_way_to_setup_a_local_llm_to_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iey40e/what_is_the_best_way_to_setup_a_local_llm_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iey40e/what_is_the_best_way_to_setup_a_local_llm_to_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T04:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1if26o7</id>
    <title>I need an LLM that hallucinates a lot and generates garbage responses.</title>
    <updated>2025-02-01T08:43:17+00:00</updated>
    <author>
      <name>/u/dumbPotatoPot</name>
      <uri>https://old.reddit.com/user/dumbPotatoPot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I'm building a proof-of-concept to implement the &lt;a href="https://www.anthropic.com/research/building-effective-agents#:%7E:text=Workflow%3A%20Evaluator%2Doptimizer"&gt;evalutator-optimizer workflow&lt;/a&gt; in Java and need a primary model that hallucinates a lot for testing.&lt;/p&gt; &lt;p&gt;One approach would be to use a system prompt to specifically ask the LLM to produce irrelevant responses, but I can't do that currently.&lt;/p&gt; &lt;p&gt;Is there an LLM that's specifically available for such purpose? Also I've heard about increasing the context window to get quality responses, so in such case would decreasing the context window work? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dumbPotatoPot"&gt; /u/dumbPotatoPot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if26o7/i_need_an_llm_that_hallucinates_a_lot_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if26o7/i_need_an_llm_that_hallucinates_a_lot_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if26o7/i_need_an_llm_that_hallucinates_a_lot_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T08:43:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ietwx3</id>
    <title>Built my own discord bot using ollama and uh...</title>
    <updated>2025-02-01T00:29:41+00:00</updated>
    <author>
      <name>/u/GlitchPhoenix98</name>
      <uri>https://old.reddit.com/user/GlitchPhoenix98</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ietwx3/built_my_own_discord_bot_using_ollama_and_uh/"&gt; &lt;img alt="Built my own discord bot using ollama and uh..." src="https://preview.redd.it/xokxk8pu9fge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=902c2a44c4ab57ad1a35490d4766d3c979bf1edf" title="Built my own discord bot using ollama and uh..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GlitchPhoenix98"&gt; /u/GlitchPhoenix98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xokxk8pu9fge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ietwx3/built_my_own_discord_bot_using_ollama_and_uh/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ietwx3/built_my_own_discord_bot_using_ollama_and_uh/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T00:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1if4p38</id>
    <title>Been messing around with DeepSeek R1 + Ollama, and honestly, it's kinda wild how much you can do locally with free open-source tools. No cloud, no API keys, just your machine and some cool AI magic.</title>
    <updated>2025-02-01T11:50:04+00:00</updated>
    <author>
      <name>/u/hasan_py</name>
      <uri>https://old.reddit.com/user/hasan_py</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;Page-Assist Chrome Extension - &lt;a href="https://github.com/n4ze3m/page-assist"&gt;https://github.com/n4ze3m/page-assist&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Open Web-UI LLM Wrapper - &lt;a href="https://github.com/open-webui/open-webui"&gt;https://github.com/open-webui/open-webui&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Browser use ‚Äì &lt;a href="https://github.com/browser-use/browser-use"&gt;https://github.com/browser-use/browser-use&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Roo-Code (VS Code Extension) ‚Äì &lt;a href="https://github.com/RooVetGit/Roo-Code"&gt;https://github.com/RooVetGit/Roo-Code&lt;/a&gt;&lt;/li&gt; &lt;li&gt;n8n ‚Äì &lt;a href="https://github.com/n8n-io/n8n"&gt;https://github.com/n8n-io/n8n&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A simple RAG app: &lt;a href="https://github.com/hasan-py/chat-with-pdf-RAG"&gt;https://github.com/hasan-py/chat-with-pdf-RAG&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ai assistant Chrome extension: &lt;a href="https://github.com/hasan-py/Ai-Assistant-Chrome-Extension"&gt;https://github.com/hasan-py/Ai-Assistant-Chrome-Extension&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Anyone exploring something else? Please share- it would be highly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hasan_py"&gt; /u/hasan_py &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if4p38/been_messing_around_with_deepseek_r1_ollama_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if4p38/been_messing_around_with_deepseek_r1_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if4p38/been_messing_around_with_deepseek_r1_ollama_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T11:50:04+00:00</published>
  </entry>
</feed>
