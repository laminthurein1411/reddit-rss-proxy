<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-04-26T18:25:00+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1k6au3l</id>
    <title>Agents can now start/stop themselves and other agents in Observer AI!</title>
    <updated>2025-04-23T21:23:47+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys! I just added possibly the biggest feature in terms of power to the open source tool ObserverAI!!&lt;/p&gt; &lt;p&gt;Agents can now stop/start themselves or other agents, making them actual Agents instead of Workflows due to the Anthropic definition of agents:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Workflows&lt;/strong&gt; are systems where LLMs and tools are orchestrated through predefined code paths.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agents&lt;/strong&gt;, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;See: &lt;a href="https://www.anthropic.com/engineering/building-effective-agents/"&gt;https://www.anthropic.com/engineering/building-effective-agents/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Observer AI agents can now work in clusters, for example:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Small agent (8b gemini) can watch the screen to see when code pops up. &lt;/li&gt; &lt;li&gt;Then turns on a big agent like deepseek coder to suggest better code! &lt;/li&gt; &lt;li&gt;Then deepseek coder turns small agent back on just to identify code on screen.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This tool is still being tested and is on beta, but i would love for people to contribute with agent ideas or pull requests. &lt;/p&gt; &lt;p&gt;If you want to check it out its on &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Thank you all for your feedback so far! I really appreciate it! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k6au3l/agents_can_now_startstop_themselves_and_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k6au3l/agents_can_now_startstop_themselves_and_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k6au3l/agents_can_now_startstop_themselves_and_other/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-23T21:23:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6pml0</id>
    <title>[LangGraph + Ollama] Agent using local model (qwen2.5) returns AIMessage(content='') even when tool responds correctly</title>
    <updated>2025-04-24T11:16:06+00:00</updated>
    <author>
      <name>/u/hashirama-fey0</name>
      <uri>https://old.reddit.com/user/hashirama-fey0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm using create_react_agent from langgraph.prebuilt with a local model served via Ollama (qwen2.5), and the agent consistently returns an AIMessage with an empty content field ‚Äî even though the tool returns a valid string.&lt;/p&gt; &lt;p&gt;Code&lt;/p&gt; &lt;p&gt;from langgraph.prebuilt import create_react_agent from langchain_ollama import ChatOllama&lt;/p&gt; &lt;p&gt;model = ChatOllama(model=&amp;quot;qwen2.5&amp;quot;)&lt;/p&gt; &lt;p&gt;def search(query: str): &amp;quot;&amp;quot;&amp;quot;Call to surf the web.&amp;quot;&amp;quot;&amp;quot; if &amp;quot;sf&amp;quot; in query.lower() or &amp;quot;san francisco&amp;quot; in query.lower(): return &amp;quot;It's 60 degrees and foggy.&amp;quot; return &amp;quot;It's 90 degrees and sunny.&amp;quot;&lt;/p&gt; &lt;p&gt;agent = create_react_agent(model=model, tools=[search])&lt;/p&gt; &lt;p&gt;response = agent.invoke( {}, {&amp;quot;messages&amp;quot;: [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;what is the weather in sf&amp;quot;}]} ) print(response) Output&lt;/p&gt; &lt;p&gt;{ 'messages': [ AIMessage( content='', additional_kwargs={}, response_metadata={ 'model': 'qwen2.5', 'created_at': '2025-04-24T09:13:29.983043Z', 'done': True, 'done_reason': 'load', 'total_duration': None, 'load_duration': None, 'prompt_eval_count': None, 'prompt_eval_duration': None, 'eval_count': None, 'eval_duration': None, 'model_name': 'qwen2.5' }, id='run-6a897b3a-1971-437b-8a98-95f06bef3f56-0' ) ] } As shown above, the agent responds with an empty string, even though the search() tool clearly returns &amp;quot;It's 60 degrees and foggy.&amp;quot;.&lt;/p&gt; &lt;p&gt;Has anyone seen this behavior? Could it be an issue with qwen2.5, langgraph.prebuilt, the Ollama config, or maybe a mismatch somewhere between them?&lt;/p&gt; &lt;p&gt;Any insight appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hashirama-fey0"&gt; /u/hashirama-fey0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k6pml0/langgraph_ollama_agent_using_local_model_qwen25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k6pml0/langgraph_ollama_agent_using_local_model_qwen25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k6pml0/langgraph_ollama_agent_using_local_model_qwen25/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-24T11:16:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6qwu5</id>
    <title>Forgive me Ollama, for I have sinned.</title>
    <updated>2025-04-24T12:26:29+00:00</updated>
    <author>
      <name>/u/Immediate_Song4279</name>
      <uri>https://old.reddit.com/user/Immediate_Song4279</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k6qwu5/forgive_me_ollama_for_i_have_sinned/"&gt; &lt;img alt="Forgive me Ollama, for I have sinned." src="https://preview.redd.it/tqm1j4wf0swe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a744c643bb11bc24526b3e9252f866c294fb0dc" title="Forgive me Ollama, for I have sinned." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tiger Gemma 8B has left the building.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Immediate_Song4279"&gt; /u/Immediate_Song4279 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tqm1j4wf0swe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k6qwu5/forgive_me_ollama_for_i_have_sinned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k6qwu5/forgive_me_ollama_for_i_have_sinned/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-24T12:26:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1k674xf</id>
    <title>Free Ollama GPU!</title>
    <updated>2025-04-23T18:52:45+00:00</updated>
    <author>
      <name>/u/guuidx</name>
      <uri>https://old.reddit.com/user/guuidx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you run this on Google Collab, you have a free Ollama running GPU!&lt;/p&gt; &lt;p&gt;Do not forgot to enable the GPU in the right upper corner of the Google Collab screen, by clicking on CPU/MEM.&lt;/p&gt; &lt;p&gt;!curl -fsSL &lt;a href="https://molodetz.nl/retoor/uberlama/raw/branch/main/ollama-colab-v2.sh"&gt;https://molodetz.nl/retoor/uberlama/raw/branch/main/ollama-colab-v2.sh&lt;/a&gt; | sh&lt;/p&gt; &lt;p&gt;Read the full script here, and about how to use your Ollama model: &lt;a href="https://molodetz.nl/project/uberlama/ollama-colab-v2.sh.html"&gt;https://molodetz.nl/project/uberlama/ollama-colab-v2.sh.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The idea was not mine, I've read some blog post that gave me the idea.&lt;/p&gt; &lt;p&gt;But the blog post required many steps and had several dependencies.&lt;/p&gt; &lt;p&gt;Mine only has one (Python) dependency: aiohttp. That one gets installed by the script automatically.&lt;/p&gt; &lt;p&gt;To run a different model, you have to update the script. &lt;/p&gt; &lt;p&gt;The whole Ollama hub including server (hub itself) is Open Source.&lt;/p&gt; &lt;p&gt;If you have questions, send me a PM. I like to talk about programming.&lt;/p&gt; &lt;p&gt;EDIT: working on streaming support for webui, didn't realize that so much webui users. It currently works if you disable streaming responses on openwebui. Maybe I will make a new post later with instruction video. I'm currently chatting with it using webui. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/guuidx"&gt; /u/guuidx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k674xf/free_ollama_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k674xf/free_ollama_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k674xf/free_ollama_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-23T18:52:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1k70pyt</id>
    <title>What SW have you found best for properly reading PDF text, graphs, charts, pics, etc for RAG?</title>
    <updated>2025-04-24T19:15:12+00:00</updated>
    <author>
      <name>/u/GaltEngineering</name>
      <uri>https://old.reddit.com/user/GaltEngineering</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GaltEngineering"&gt; /u/GaltEngineering &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k70pyt/what_sw_have_you_found_best_for_properly_reading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k70pyt/what_sw_have_you_found_best_for_properly_reading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k70pyt/what_sw_have_you_found_best_for_properly_reading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-24T19:15:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6ronv</id>
    <title>Models to extract entities from PDF</title>
    <updated>2025-04-24T13:03:58+00:00</updated>
    <author>
      <name>/u/vanTrottel</name>
      <uri>https://old.reddit.com/user/vanTrottel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For an automated process I wrote a python script which sends a prompt to a local ollama with the text of the PDF as well as the prompt.&lt;/p&gt; &lt;p&gt;Everything works fine, but with Llama3.3 I only reach an accuracy of about 80%.&lt;/p&gt; &lt;p&gt;The documents are in german and contain technical, specific data as well as adresses.&lt;/p&gt; &lt;p&gt;Which models compatible with a local Ollama are good at extracting specific information from PDFs?&lt;/p&gt; &lt;p&gt;I tested the following models:&lt;/p&gt; &lt;p&gt;Llama3.3 =&amp;gt; 80%&lt;/p&gt; &lt;p&gt;Phi =&amp;gt; 1%&lt;/p&gt; &lt;p&gt;Mistral =36,6%&lt;/p&gt; &lt;p&gt;Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vanTrottel"&gt; /u/vanTrottel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k6ronv/models_to_extract_entities_from_pdf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k6ronv/models_to_extract_entities_from_pdf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k6ronv/models_to_extract_entities_from_pdf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-24T13:03:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7juvr</id>
    <title>Little help</title>
    <updated>2025-04-25T12:36:44+00:00</updated>
    <author>
      <name>/u/Vibe_Cipher_</name>
      <uri>https://old.reddit.com/user/Vibe_Cipher_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys I installed ollama a few days back to locally run some models and test it out everything. But recently someone point it out that though it is safe, I might try to find a more secure way to use ollama. I only downloaded ollama and work on by just pulling the model on my terminal so far. I heard that it might be better to run on a docker container but I don't know how to use that. Someone plz guide me a little &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vibe_Cipher_"&gt; /u/Vibe_Cipher_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7juvr/little_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7juvr/little_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k7juvr/little_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-25T12:36:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7c0h5</id>
    <title>Can I run Mistral 7B locally on ASUS TUF A15 (RTX 3050 4GB VRAM, 16GB RAM)?</title>
    <updated>2025-04-25T04:09:23+00:00</updated>
    <author>
      <name>/u/KaleidoscopeCivil495</name>
      <uri>https://old.reddit.com/user/KaleidoscopeCivil495</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! üëã&lt;/p&gt; &lt;p&gt;I‚Äôm planning to experiment with &lt;strong&gt;local LLMs&lt;/strong&gt; using &lt;strong&gt;Ollama,&lt;/strong&gt; and I am new to this, and I‚Äôm curious if my laptop can handle the &lt;strong&gt;Mistral:7b-instruct&lt;/strong&gt; model smoothly.&lt;/p&gt; &lt;p&gt;Here are my specs:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Laptop&lt;/strong&gt;: ASUS TUF A15&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPU&lt;/strong&gt;: RTX 3050 4GB VRAM&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RAM&lt;/strong&gt;: 16GB DDR4&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Processor&lt;/strong&gt;: AMD Ryzen 7 7435HS &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Storage&lt;/strong&gt;: SSD&lt;/p&gt; &lt;p&gt;&lt;strong&gt;OS&lt;/strong&gt;: Windows 11&lt;/p&gt; &lt;p&gt;I'm mostly interested in:&lt;/p&gt; &lt;p&gt;Running it smoothly for code, learning, and research&lt;/p&gt; &lt;p&gt;Avoiding overheating or crashes&lt;/p&gt; &lt;p&gt;Understanding if quantized versions (like Q4_0) would run better on this config&lt;/p&gt; &lt;p&gt;Anyone here running Mistral 7B on similar hardware? Would love your experience, tips, and which quant version works best!&lt;/p&gt; &lt;p&gt;Thanks in advance üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KaleidoscopeCivil495"&gt; /u/KaleidoscopeCivil495 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7c0h5/can_i_run_mistral_7b_locally_on_asus_tuf_a15_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7c0h5/can_i_run_mistral_7b_locally_on_asus_tuf_a15_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k7c0h5/can_i_run_mistral_7b_locally_on_asus_tuf_a15_rtx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-25T04:09:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7f0b9</id>
    <title>LLMA 3.3 3B not using GPU</title>
    <updated>2025-04-25T07:23:16+00:00</updated>
    <author>
      <name>/u/INFERNOthepro</name>
      <uri>https://old.reddit.com/user/INFERNOthepro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k7f0b9/llma_33_3b_not_using_gpu/"&gt; &lt;img alt="LLMA 3.3 3B not using GPU" src="https://external-preview.redd.it/vaGfmeMenRE9lbHpGW9SE06BZu9HTcViMEsrIDZDb38.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc2438c88a3d95f1087f01bfe8c7c335b230cd51" title="LLMA 3.3 3B not using GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My mac has a amd radeon pro 5500m 4gb gpu and im runnign the llma 3.2 3B parameter model on my mac. Why is it still not using the GPU?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/updjykq6nxwe1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f32a5f351e5ca0fe457489052cc4f7e9db5a63ba"&gt;https://preview.redd.it/updjykq6nxwe1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f32a5f351e5ca0fe457489052cc4f7e9db5a63ba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gxtxr4b9nxwe1.png?width=1632&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2154ab4952042f4473c70c854b720d630b12b86"&gt;https://preview.redd.it/gxtxr4b9nxwe1.png?width=1632&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2154ab4952042f4473c70c854b720d630b12b86&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/INFERNOthepro"&gt; /u/INFERNOthepro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7f0b9/llma_33_3b_not_using_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7f0b9/llma_33_3b_not_using_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k7f0b9/llma_33_3b_not_using_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-25T07:23:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7rxpl</id>
    <title>Ollama won't run on RX7700xt</title>
    <updated>2025-04-25T18:18:59+00:00</updated>
    <author>
      <name>/u/c30ra</name>
      <uri>https://old.reddit.com/user/c30ra</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, i've trouble running ollama on my gpu.&lt;/p&gt; &lt;p&gt;I'm on fedora 42 system. I've followed every guide i've found on internet. From the logs it seems that it detect correctly rocm but at the end the layers are uploaded to CPU.&lt;/p&gt; &lt;p&gt;Can someone guide to debug this? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/c30ra"&gt; /u/c30ra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7rxpl/ollama_wont_run_on_rx7700xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7rxpl/ollama_wont_run_on_rx7700xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k7rxpl/ollama_wont_run_on_rx7700xt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-25T18:18:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7u7d1</id>
    <title>Graphic card for homelab</title>
    <updated>2025-04-25T19:54:11+00:00</updated>
    <author>
      <name>/u/LordGrande666</name>
      <uri>https://old.reddit.com/user/LordGrande666</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!!&lt;/p&gt; &lt;p&gt;I know this topic is here, it's probably the same old thing: What graphics card should I buy to host olama?&lt;/p&gt; &lt;p&gt;I have a server with a Chinese motherboard that has an i7 13800h from a laptop. I use it to run various services on it, like Plex, Pihole, Netbootxyz, HomeAssistant...&lt;/p&gt; &lt;p&gt;As you can guess, I want to start up an AI for my home, little by little, so it can be our assistant and see how I can integrate it as a voice assistant, or I don't know... for now, it's all just an idea in my head.&lt;/p&gt; &lt;p&gt;Now, I have a 2080 from my old computer, and I don't want to install it. Why? Because a 2080 that's on all the time must consume a lot of power.&lt;/p&gt; &lt;p&gt;So I've considered other options:&lt;/p&gt; &lt;p&gt;- Buy a much more modest graphics card, like a 3050, a 7060xt...&lt;/p&gt; &lt;p&gt;- Undervolt the 2080 and try lowering the GPU speed (Ideally, it should do this on its own. If it demands performance, remove the restrictions. This might be stupid, I'm sure it already does this.)&lt;/p&gt; &lt;p&gt;- Crazy idea: A plug-and-play graphics card using Oculink. Do I want to generate something powerful? I plug it in. Do I just want to ask it for a recipe? I don't.&lt;/p&gt; &lt;p&gt;I don't know, what do you think? What would you do in my place? :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LordGrande666"&gt; /u/LordGrande666 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7u7d1/graphic_card_for_homelab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7u7d1/graphic_card_for_homelab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k7u7d1/graphic_card_for_homelab/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-25T19:54:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7hi83</id>
    <title>Ollama Excel query agent</title>
    <updated>2025-04-25T10:21:33+00:00</updated>
    <author>
      <name>/u/aminekissai</name>
      <uri>https://old.reddit.com/user/aminekissai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone.&lt;/p&gt; &lt;p&gt;Im kinda new in this field.&lt;/p&gt; &lt;p&gt;I want to code an agent, using local llms (preferably using Ollama), to interact with an Excel file. &lt;/p&gt; &lt;p&gt;Classic RAG doesnt work for me since I may have queries such as &amp;quot;what is the number of rows&amp;quot;. &lt;/p&gt; &lt;p&gt;I used create_pandas_agent from langchain, it worked fine using an OpenAI model, but it doesnt give good results using a small local LLM (I tried Mistral, Deepseek and Gemma). &lt;/p&gt; &lt;p&gt;Using SQL seems a bit overkill.&lt;/p&gt; &lt;p&gt;I tried installing Pandasai but it seems that my computer doesnt want it üòÖ.&lt;/p&gt; &lt;p&gt;Has anyone done something similar before? Any help is appreciated.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aminekissai"&gt; /u/aminekissai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7hi83/ollama_excel_query_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7hi83/ollama_excel_query_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k7hi83/ollama_excel_query_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-25T10:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6m1b3</id>
    <title>Someone found my open AI server and used it to process disturbing amounts of personal data, for over a month</title>
    <updated>2025-04-24T07:06:23+00:00</updated>
    <author>
      <name>/u/ufaruq</name>
      <uri>https://old.reddit.com/user/ufaruq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k6m1b3/someone_found_my_open_ai_server_and_used_it_to/"&gt; &lt;img alt="Someone found my open AI server and used it to process disturbing amounts of personal data, for over a month" src="https://preview.redd.it/x0j3zhqcfqwe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=168a099a8405f4b87e437f05cbc0954fe95234e0" title="Someone found my open AI server and used it to process disturbing amounts of personal data, for over a month" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just found out that someone has been using my locally hosted AI model for over a month, without me knowing.&lt;/p&gt; &lt;p&gt;Apparently, I left the Ollama port open on my router, and someone found it. They‚Äôve been sending it huge chunks of personal information ‚Äî names, phone numbers, addresses, parcel IDs, job details, even latitude and longitude. All of it was being processed through my setup while I had no clue.&lt;/p&gt; &lt;p&gt;I only noticed today when I was checking some logs and saw a flood of suspicious-looking entries. When I dug into it, I found that it wasn‚Äôt just some one-off request ‚Äî this had been going on for weeks.&lt;/p&gt; &lt;p&gt;The kind of data they were processing is creepy as hell. It looks like they were trying to organize or extract information on people. I‚Äôm attaching a screenshot of one snippet ‚Äî it speaks for itself.&lt;/p&gt; &lt;p&gt;The IP was from Hong Kong and the prompt is at the end in Chinese.&lt;/p&gt; &lt;p&gt;I‚Äôve shut it all down now and locked things up tight. Just posting this as a warning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ufaruq"&gt; /u/ufaruq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x0j3zhqcfqwe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k6m1b3/someone_found_my_open_ai_server_and_used_it_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k6m1b3/someone_found_my_open_ai_server_and_used_it_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-24T07:06:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8azpu</id>
    <title>Deepseek r2 model?</title>
    <updated>2025-04-26T11:40:51+00:00</updated>
    <author>
      <name>/u/AnhCloudB</name>
      <uri>https://old.reddit.com/user/AnhCloudB</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've used the Deepseek r2 model in their official website and its ten times better than the r1 model provided in ollama. Is there or will there be an unfiltered r2 model soon?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnhCloudB"&gt; /u/AnhCloudB &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8azpu/deepseek_r2_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8azpu/deepseek_r2_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8azpu/deepseek_r2_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-26T11:40:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7gaiu</id>
    <title>ü¶ô lazyollama ‚Äì terminal tool for chatting with Ollama models now does LeetCode OCR + code copy</title>
    <updated>2025-04-25T08:57:55+00:00</updated>
    <author>
      <name>/u/DTostes</name>
      <uri>https://old.reddit.com/user/DTostes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a CLI called &lt;a href="https://github.com/davitostes/lazyollama"&gt;&lt;code&gt;lazyollama&lt;/code&gt;&lt;/a&gt; to manage chats with Ollama models ‚Äî all in the terminal.&lt;/p&gt; &lt;p&gt;Core features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;create/select/delete chats&lt;/li&gt; &lt;li&gt;auto-saves convos locally as JSON&lt;/li&gt; &lt;li&gt;switch models mid-session&lt;/li&gt; &lt;li&gt;simple terminal workflow, no UI needed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üÜï New in-chat commands:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;/leetcodehack&lt;/code&gt;: screenshot + OCR a LeetCode problem, sends to the model ‚Üí needs &lt;code&gt;hyprshot&lt;/code&gt; + &lt;code&gt;tesseract&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;/copycode&lt;/code&gt;: grabs the first code block from the response and copies to clipboard ‚Üí needs &lt;code&gt;xclip&lt;/code&gt; or &lt;code&gt;wl-clip&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üí° Model suggestions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;gemma:3b&lt;/code&gt; for light stuff&lt;/li&gt; &lt;li&gt;&lt;code&gt;mistral&lt;/code&gt; or &lt;code&gt;qwen2.5-coder&lt;/code&gt; for coding and &lt;code&gt;/leetcodehack&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Written in Go, zero fancy dependencies, MIT licensed.&lt;br /&gt; Repo: &lt;a href="https://github.com/davitostes/lazyollama"&gt;https://github.com/davitostes/lazyollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if it‚Äôs useful or if you‚Äôve got ideas to make it better!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DTostes"&gt; /u/DTostes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7gaiu/lazyollama_terminal_tool_for_chatting_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7gaiu/lazyollama_terminal_tool_for_chatting_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k7gaiu/lazyollama_terminal_tool_for_chatting_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-25T08:57:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7koor</id>
    <title>Ollama beginner here, how do I know/check if the ports are open or safe?</title>
    <updated>2025-04-25T13:16:43+00:00</updated>
    <author>
      <name>/u/Flutter_ExoPlanet</name>
      <uri>https://old.reddit.com/user/Flutter_ExoPlanet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Reading this post: &lt;a href="https://www.reddit.com/r/ollama/comments/1k6m1b3/someone_found_my_open_ai_server_and_used_it_to/"&gt;https://www.reddit.com/r/ollama/comments/1k6m1b3/someone_found_my_open_ai_server_and_used_it_to/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Made me realize I am not sure I know what I am doing&lt;/p&gt; &lt;p&gt;Simply installing ollama and running locally some llms, does that mean we have already opened ports somehow? How to check it and how to make sure is secure again?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flutter_ExoPlanet"&gt; /u/Flutter_ExoPlanet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7koor/ollama_beginner_here_how_do_i_knowcheck_if_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7koor/ollama_beginner_here_how_do_i_knowcheck_if_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k7koor/ollama_beginner_here_how_do_i_knowcheck_if_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-25T13:16:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1k821kq</id>
    <title>The work goes on</title>
    <updated>2025-04-26T02:04:53+00:00</updated>
    <author>
      <name>/u/GVDub2</name>
      <uri>https://old.reddit.com/user/GVDub2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Continuing to work on &lt;a href="https://github.com/GVDub/panai-seed-node"&gt;https://github.com/GVDub/panai-seed-node&lt;/a&gt;, and it's coming along, though still a proof-of-concept on the home network. But it's getting closer, and I thought that I'd share the mission statement here: &lt;/p&gt; &lt;h1&gt;PanAI: Memory with Meaning&lt;/h1&gt; &lt;p&gt;In the quiet spaces between generations, memories fade. Stories are lost. Choices once made with courage and conviction vanish into silence.&lt;/p&gt; &lt;p&gt;PanAI was born from a simple truth:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Not facts. Not dates. But the heartbeat behind them. The way a voice softens when recalling a lost friend. The way hands shake, ever so slightly, when describing a moment of fear overcome.&lt;/p&gt; &lt;p&gt;Our founder's grandfather was a Quaker minister, born on the American frontier in 1873. A man who once, unarmed, faced down a drunken gunfighter to protect his town. That moment ‚Äî that fiber of human choice and presence ‚Äî lives now only in secondhand fragments. He died when his grandson was seven years old, before the questions could be asked, before the full story could be told.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How many stories like that have we lost?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;How many silent heroes, quiet acts of bravery, whispered dreams have faded because we lacked a way to hold them ‚Äî tenderly, safely, accessibly ‚Äî for the future?&lt;/p&gt; &lt;p&gt;PanAI isn't about data. It isn't about &amp;quot;efficiency.&amp;quot; It's about &lt;strong&gt;catching what matters before it drifts away&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It's about:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Families preserving not just names, but &lt;em&gt;meaning&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Organizations keeping not just records, but &lt;em&gt;wisdom&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Communities safeguarding not just history, but &lt;em&gt;hope&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In a world obsessed with &amp;quot;faster&amp;quot; and &amp;quot;cheaper,&amp;quot; PanAI stands for something else:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;Our Principles&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Decentralization&lt;/strong&gt;: Memory should not be owned by corporations or buried on servers a thousand miles away. It belongs to you, and to those you choose to share it with.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ethics First&lt;/strong&gt;: No monetization of memories. No harvesting of private thoughts. Consent and control are woven into the fabric of PanAI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accessibility&lt;/strong&gt;: Whether it's one person, a family, or a small town library, PanAI can be deployed and embraced.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evolution&lt;/strong&gt;: Memories are not static. PanAI grows, reflects, and learns alongside you, weaving threads of connection across time and distance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Joy and Wonder&lt;/strong&gt;: Not every memory needs to be &amp;quot;important.&amp;quot; Some are simply &lt;em&gt;beautiful&lt;/em&gt; ‚Äî a child's laugh, a joke between old friends, a favorite song sung off-key. These matter too.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why We Build&lt;/h1&gt; &lt;p&gt;Because someday, someone will wish they could ask you, &lt;em&gt;&amp;quot;What was it really like?&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;PanAI exists so that the answer doesn't have to be silence.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;It can be presence.&lt;/strong&gt; &lt;strong&gt;It can be memory.&lt;/strong&gt; &lt;strong&gt;It can be connection, spanning the spaces between heartbeats, between lifetimes.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;And it can be real.&lt;/p&gt; &lt;p&gt;&lt;em&gt;PanAI: Because memory deserves a future.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GVDub2"&gt; /u/GVDub2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k821kq/the_work_goes_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k821kq/the_work_goes_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k821kq/the_work_goes_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-26T02:04:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k86b0e</id>
    <title>Best MCP Servers for Data Scientists</title>
    <updated>2025-04-26T06:17:05+00:00</updated>
    <author>
      <name>/u/mehul_gupta1997</name>
      <uri>https://old.reddit.com/user/mehul_gupta1997</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k86b0e/best_mcp_servers_for_data_scientists/"&gt; &lt;img alt="Best MCP Servers for Data Scientists" src="https://external-preview.redd.it/TfgAyWilaq-ESKVky0U12Y_KWtvNmUNDET1FawbaFo0.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ae19dc23ef4763011fb0f3748684452fa8b2e6c" title="Best MCP Servers for Data Scientists" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehul_gupta1997"&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/TumbALfrtC8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k86b0e/best_mcp_servers_for_data_scientists/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k86b0e/best_mcp_servers_for_data_scientists/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-26T06:17:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7s4l2</id>
    <title>AI Memory and small models</title>
    <updated>2025-04-25T18:26:40+00:00</updated>
    <author>
      <name>/u/Short-Honeydew-7000</name>
      <uri>https://old.reddit.com/user/Short-Honeydew-7000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;We've announced our AI memory tool here a few weeks ago:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/ollama/comments/1jk7hh0/use_ollama_to_create_your_own_ai_memory_locally/"&gt;https://www.reddit.com/r/ollama/comments/1jk7hh0/use_ollama_to_create_your_own_ai_memory_locally/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Many of you asked us how would it work with small models.&lt;/p&gt; &lt;p&gt;I spent a bit of time testing it and trying to understand what works and what doesn't.&lt;/p&gt; &lt;p&gt;After testing various models available through Ollama, we found:&lt;/p&gt; &lt;p&gt;Smaller Models (‚â§7B parameters)&lt;/p&gt; &lt;p&gt;- Phi-4 (3-7B): Shows promise for simpler structured outputs but struggles with complex nested schemas.&lt;br /&gt; - Gemma-3 (3-7B): Similar to Phi-4, works for basic structures but degrades significantly with complex schemas.&lt;br /&gt; - Llama 3.3 (8B): Fails miserably&lt;br /&gt; - Deepseek-r1 (1.5B-7B): Inconsistent results, sometimes returning answers in Chinese, often failing to generate valid structured output.&lt;/p&gt; &lt;p&gt;Medium-sized Models (8-14B parameters)&lt;/p&gt; &lt;p&gt;- Qwen2 (14B): Significantly outperforms other models of similar size, especially for extraction tasks.&lt;br /&gt; - Llama 3.2 (8B): Doesn't do so well with knowledge graph creation, best avoided&lt;br /&gt; - Deepseek (8B): Improved over smaller versions but still unreliable for complex knowledge graph generation.&lt;/p&gt; &lt;p&gt;Larger Models (&amp;gt;14B)&lt;br /&gt; - Qwen2.5-coder (32B): Excellent for structured outputs, approaching cloud model performance.&lt;br /&gt; - Llama 3.3 (70B): Very reliable but requires significant hardware resources.&lt;br /&gt; - Deepseek-r1 (32B): Can create simpler graphs and, after several retries, gives reasonable outputs.&lt;/p&gt; &lt;p&gt;Optimization Strategies from Community Feedback&lt;/p&gt; &lt;p&gt;The Ollama community + our Discord users has shared several strategies that have helped improve structured output performance:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Two-stage approach: First get outputs for known examples, then use majority voting across multiple responses to select the ideal setup. We have some re-runs logic in our adapters and are extending this.&lt;/li&gt; &lt;li&gt;Field descriptions: Always include detailed field descriptions in Pydantic models to guide the model.&lt;/li&gt; &lt;li&gt;Reasoning fields: Add &amp;quot;reasoning&amp;quot; fields in the JSON that guide the model through proper steps before target output fields.&lt;/li&gt; &lt;li&gt;Format specification: Explicitly stating &amp;quot;Respond in minified JSON&amp;quot; is often crucial.&lt;/li&gt; &lt;li&gt;Alternative formats: Some users reported better results with YAML than JSON, particularly when wrapped in markdown code blocks.&lt;/li&gt; &lt;li&gt;Simplicity: Keep It Simple - recursive or deeply nested schemas typically perform poorly.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Have a look at our Github if you want to take it for a spin: &lt;a href="https://github.com/topoteretes/cognee"&gt;https://github.com/topoteretes/cognee&lt;/a&gt;&lt;/p&gt; &lt;p&gt;YouTube Ollama small model explainer: &lt;a href="https://www.youtube.com/watch?v=P2ZaSnnl7z0"&gt;https://www.youtube.com/watch?v=P2ZaSnnl7z0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Short-Honeydew-7000"&gt; /u/Short-Honeydew-7000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7s4l2/ai_memory_and_small_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7s4l2/ai_memory_and_small_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k7s4l2/ai_memory_and_small_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-25T18:26:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8euyg</id>
    <title>2x 64GB M2 Mac Studio Ultra for hosting locally</title>
    <updated>2025-04-26T14:57:45+00:00</updated>
    <author>
      <name>/u/Mountain_Desk_767</name>
      <uri>https://old.reddit.com/user/Mountain_Desk_767</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have these 2x Macs, and i am thinking of combining them (cluster) to host &amp;gt;70B models.&lt;br /&gt; The question is, is it possible i combine both of them to be able to utilize their VRAM, improve performance and use large models. Can i set them up as a server and only have my laptop access it. I will have the open web ui on my laptop and connect to them.&lt;/p&gt; &lt;p&gt;Is it worth the consideration.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mountain_Desk_767"&gt; /u/Mountain_Desk_767 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8euyg/2x_64gb_m2_mac_studio_ultra_for_hosting_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8euyg/2x_64gb_m2_mac_studio_ultra_for_hosting_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8euyg/2x_64gb_m2_mac_studio_ultra_for_hosting_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-26T14:57:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7jebb</id>
    <title>Give Your Local LLM Superpowers! üöÄ New Guide to Open WebUI Tools</title>
    <updated>2025-04-25T12:12:59+00:00</updated>
    <author>
      <name>/u/PeterHash</name>
      <uri>https://old.reddit.com/user/PeterHash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; ,&lt;/p&gt; &lt;p&gt;Just dropped the next part of my Open WebUI series. This one's all about Tools - giving your local models the ability to do things like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Check the current time/weather ‚è∞&lt;/li&gt; &lt;li&gt;Perform accurate calculations üî¢&lt;/li&gt; &lt;li&gt;Scrape live web info üåê&lt;/li&gt; &lt;li&gt;Even send emails or schedule meetings! (Examples included) üìßüóìÔ∏è&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We cover finding community tools, crucial safety tips, and how to build your own custom tools with Python (code template + examples in the linked GitHub repo!). It's perfect if you've ever wished your Open WebUI setup could interact with the real world or external APIs.&lt;/p&gt; &lt;p&gt;Check it out and let me know what cool tools you're planning to build!&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@hautel.alex2000/beyond-text-equipping-your-open-webui-ai-with-action-tools-594e15cd7903"&gt;Beyond Text: Equipping Your Open WebUI AI with Action Tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PeterHash"&gt; /u/PeterHash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7jebb/give_your_local_llm_superpowers_new_guide_to_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k7jebb/give_your_local_llm_superpowers_new_guide_to_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k7jebb/give_your_local_llm_superpowers_new_guide_to_open/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-25T12:12:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8itfb</id>
    <title>Best model for synthetic data</title>
    <updated>2025-04-26T17:49:57+00:00</updated>
    <author>
      <name>/u/No_Wind7503</name>
      <uri>https://old.reddit.com/user/No_Wind7503</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I working on synthetic data generation system and I need small models (3-8B) to generate the data, anyone know best model can do that or specific to do that&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Wind7503"&gt; /u/No_Wind7503 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8itfb/best_model_for_synthetic_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8itfb/best_model_for_synthetic_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8itfb/best_model_for_synthetic_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-26T17:49:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8cub6</id>
    <title>Any UI for Local Fine-Tuning of Open-Source LLMs?</title>
    <updated>2025-04-26T13:22:31+00:00</updated>
    <author>
      <name>/u/who_is_erik</name>
      <uri>https://old.reddit.com/user/who_is_erik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey AI experts!&lt;/p&gt; &lt;p&gt;I'm exploring local fine-tuning of open-source LLMs. We've seen tools like AI-Toolkit, Kohya SS, and Flux Gym enable local training and fine-tuning of diffusion models. &lt;/p&gt; &lt;p&gt;Specifically: Are there frameworks or libraries that support local fine-tuning of open-source LLMs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/who_is_erik"&gt; /u/who_is_erik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8cub6/any_ui_for_local_finetuning_of_opensource_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8cub6/any_ui_for_local_finetuning_of_opensource_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8cub6/any_ui_for_local_finetuning_of_opensource_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-26T13:22:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8fti9</id>
    <title>Best model for Web Development?</title>
    <updated>2025-04-26T15:40:29+00:00</updated>
    <author>
      <name>/u/Akila_Kavinga</name>
      <uri>https://old.reddit.com/user/Akila_Kavinga</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! What's a model that best suited for web development? I just want a model that can read documentation for me. If that's not possible, a model that can reason an answer with minimal hallucinating will do.&lt;/p&gt; &lt;p&gt;PC Specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;4060 8GB Laptop GPU&lt;/li&gt; &lt;li&gt;16GB RAM&lt;/li&gt; &lt;li&gt;i7-13620H&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Akila_Kavinga"&gt; /u/Akila_Kavinga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8fti9/best_model_for_web_development/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8fti9/best_model_for_web_development/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8fti9/best_model_for_web_development/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-26T15:40:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8cprt</id>
    <title>Free GPU for Openwebui</title>
    <updated>2025-04-26T13:16:20+00:00</updated>
    <author>
      <name>/u/guuidx</name>
      <uri>https://old.reddit.com/user/guuidx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi people!&lt;/p&gt; &lt;p&gt;I wrote a post two days ago about using google colab cpu for free to use for Ollama. It was kinda aimed at developers but many webui users were interested. It was not supported, I had to add that functionality. So, that's done now!&lt;/p&gt; &lt;p&gt;Also, by request, i made a video now. The video is full length and you can see that the setup is only a few steps and a few minutes to complete in total! In the video you'll see me happily using a super fast qwen2.5 using openwebui! I'm showing the openwebui config. &lt;/p&gt; &lt;p&gt;The link mentioned in the video as 'my post' is: &lt;a href="https://www.reddit.com/r/ollama/comments/1k674xf/free_ollama_gpu/"&gt;https://www.reddit.com/r/ollama/comments/1k674xf/free_ollama_gpu/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know your experience! &lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k8cprt/video/43794nq7i6xe1/player"&gt;https://reddit.com/link/1k8cprt/video/43794nq7i6xe1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/guuidx"&gt; /u/guuidx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8cprt/free_gpu_for_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8cprt/free_gpu_for_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8cprt/free_gpu_for_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-26T13:16:20+00:00</published>
  </entry>
</feed>
