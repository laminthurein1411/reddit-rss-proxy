<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-06T12:09:38+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1iipt22</id>
    <title>how to force ollama to use more GPU ram</title>
    <updated>2025-02-06T00:41:05+00:00</updated>
    <author>
      <name>/u/sivri</name>
      <uri>https://old.reddit.com/user/sivri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to run deepseek-r1:70b on 4090 and it's a bit slow. 0.5 sec per word I guess.&lt;br /&gt; ollama ps returns 49%/51% CPU/GPU as utilisation.&lt;br /&gt; Is there a way to make it use most of the GPU?&lt;br /&gt; When I run smaller models it runs %100 on GPU. As I understand 4090's memory is not enough to run 70b but then why isn't it using %90 of gpu and %10 of cpu or something like that. But instead it just uses half half?&lt;br /&gt; Sorry I'm very noob. I can not find any configuration or anything other then what &amp;quot;ollama --help&amp;quot; provides.&lt;br /&gt; Also I've checked ollamas github docs and searched the .md files in repo but can't find any answer. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sivri"&gt; /u/sivri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iipt22/how_to_force_ollama_to_use_more_gpu_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iipt22/how_to_force_ollama_to_use_more_gpu_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iipt22/how_to_force_ollama_to_use_more_gpu_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T00:41:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiklyz</id>
    <title>Quantization help</title>
    <updated>2025-02-05T20:56:04+00:00</updated>
    <author>
      <name>/u/vagaliki</name>
      <uri>https://old.reddit.com/user/vagaliki</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm trying to quantize a model (not LLM yet - that's step 2, just Yolo initially) to run on a DSP. The things I've tried so far (yolo's export with tf's quantizer, qualcomm's AI hub, qualcomm's AIMET) have all not produced very good results (or in the case of AIMET, am getting errors when I try to convert the onnx + encodings to int8). &lt;/p&gt; &lt;p&gt;Would really appreciate if one of you quantization wizards was willing to show me what exactly you are doing and how you're getting the results to be accurate. Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vagaliki"&gt; /u/vagaliki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iiklyz/quantization_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iiklyz/quantization_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iiklyz/quantization_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T20:56:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iimkmf</id>
    <title>LLM origins</title>
    <updated>2025-02-05T22:16:42+00:00</updated>
    <author>
      <name>/u/ApprehensiveFault741</name>
      <uri>https://old.reddit.com/user/ApprehensiveFault741</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was making notes on some llm models i have on Olama and i can't find ANYTHING on this one:&lt;/p&gt; &lt;p&gt;mysterious/opus-creative:latest (4.1gb)&lt;/p&gt; &lt;p&gt;I think i got it from ollama.com or maibe huggingface.co&lt;/p&gt; &lt;p&gt;Does anyone know where it came from? What model is it based on, who is the creator, what does it specialize in?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveFault741"&gt; /u/ApprehensiveFault741 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iimkmf/llm_origins/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iimkmf/llm_origins/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iimkmf/llm_origins/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T22:16:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii6zut</id>
    <title>Optimizing Local LLMs on Mac Mini M4: Seeking Advice for Better Performance</title>
    <updated>2025-02-05T10:20:42+00:00</updated>
    <author>
      <name>/u/Killtec_Gaming</name>
      <uri>https://old.reddit.com/user/Killtec_Gaming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt; community! &lt;/p&gt; &lt;p&gt;We recently purchased a Mac Mini M4 (base model) for our office to run local AI operations. Our primary setup involves n8n for automation workflows integrated with Ollama, using mainly 7B and 14B models. &lt;/p&gt; &lt;p&gt;However, we've noticed that the results from these quantized models are significantly less impressive compared to cloud-based solutions. &lt;/p&gt; &lt;p&gt;We're looking for guidance on: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Are there specific optimization techniques or fine-tuning approaches we should consider? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What settings have you found most effective for 7B/14B models on Apple Silicon? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Would investing in more powerful hardware for running larger models be the only way to achieve cloud-like quality? &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any insights from those running similar setups would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Killtec_Gaming"&gt; /u/Killtec_Gaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6zut/optimizing_local_llms_on_mac_mini_m4_seeking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6zut/optimizing_local_llms_on_mac_mini_m4_seeking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii6zut/optimizing_local_llms_on_mac_mini_m4_seeking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T10:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iijdrc</id>
    <title>num_ctx parameter does not work</title>
    <updated>2025-02-05T20:05:17+00:00</updated>
    <author>
      <name>/u/Captain21_aj</name>
      <uri>https://old.reddit.com/user/Captain21_aj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried running deepseek-r1:32b but no matter the context length (above 2048) i keep getting input truncate warning with the &lt;code&gt;n_ctx&lt;/code&gt; value stuck to &lt;code&gt;2048&lt;/code&gt;&lt;/p&gt; &lt;p&gt;LLM server log:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llm_load_print_meta: general.name = DeepSeek R1 Distill Qwen 32B llm_load_print_meta: BOS token = 151646 '&amp;lt;ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú&amp;gt;' llm_load_print_meta: EOS token = 151643 '&amp;lt;ÔΩúend‚ñÅof‚ñÅsentence ÔΩú&amp;gt;' llm_load_print_meta: PAD token = 151643 '&amp;lt;ÔΩúend‚ñÅof‚ñÅsentence ÔΩú&amp;gt;' llm_load_print_meta: LF token = 148848 '√Ñƒ¨' llm_load_print_meta: EOG token = 151643 '&amp;lt;ÔΩúend‚ñÅof‚ñÅsentence ÔΩú&amp;gt;' llm_load_print_meta: max token length = 256 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 CUDA devices: Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes llm_load_tensors: ggml ctx size = 0.68 MiB llm_load_tensors: offloading 49 repeating layers to GPU llm_load_tensors: offloaded 49/65 layers to GPU llm_load_tensors: CUDA_Host buffer size = 5312.13 MiB llm_load_tensors: CUDA0 buffer size = 13613.88 MiB llama_new_context_with_model: n_ctx = 32000 llama_new_context_with_model: n_batch = 512 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: flash_attn = 0 llama_new_context_with_model: freq_base = 1000000.0 llama_new_context_with_model: freq_scale = 1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;LLM request log:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;INFO [update_slots] input truncated | n_ctx=2048 n_erase=6660 n_keep=4 n_left=2044 n_shift=1022 tid=&amp;quot;140586800865280&amp;quot; timestamp=1738784475 INFO [update_slots] input truncated | n_ctx=2048 n_erase=3299 n_keep=4 n_left=2044 n_shift=1022 tid=&amp;quot;140586800865280&amp;quot; timestamp=1738784485 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It seems that this issue has been raised since July last year on this &lt;a href="https://github.com/ollama/ollama/issues/5661"&gt;GitHub Issue&lt;/a&gt;. From what I experienced I feel that the model is actually running high context length but the warning still persist, Is there any way to overcome this? Running on arch linux.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Captain21_aj"&gt; /u/Captain21_aj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iijdrc/num_ctx_parameter_does_not_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iijdrc/num_ctx_parameter_does_not_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iijdrc/num_ctx_parameter_does_not_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T20:05:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iip9ss</id>
    <title>why is macos reporting 1gb ram used instead of +5?</title>
    <updated>2025-02-06T00:15:42+00:00</updated>
    <author>
      <name>/u/Avansay</name>
      <uri>https://old.reddit.com/user/Avansay</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running llama3.1:8b q4 on my m3max. activity monitory says it's using 1ish gb of ram. &lt;/p&gt; &lt;p&gt;the model page at ollama says this model is 4.9 gb.&lt;/p&gt; &lt;p&gt;Can someone explain why it's not showing at least the size of the model as what's used by ram?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Avansay"&gt; /u/Avansay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iip9ss/why_is_macos_reporting_1gb_ram_used_instead_of_5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iip9ss/why_is_macos_reporting_1gb_ram_used_instead_of_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iip9ss/why_is_macos_reporting_1gb_ram_used_instead_of_5/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T00:15:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iipuqg</id>
    <title>Variables to force more GPU usage?</title>
    <updated>2025-02-06T00:43:20+00:00</updated>
    <author>
      <name>/u/kovnev</name>
      <uri>https://old.reddit.com/user/kovnev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm running into an issue where context is getting dumped into RAM, while there still seems to be enough VRAM available.&lt;/p&gt; &lt;p&gt;Is there any variables, commands or settings to help resolve this?&lt;/p&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;p&gt;6.7gb / 8gb VRAM used while running by model (including all overheads, and everything else running).&lt;/p&gt; &lt;p&gt;0.7gb RAM used. As I increase context further, RAM usage increases, and VRAM usage remains the same.&lt;/p&gt; &lt;p&gt;It seems like it could run it all on VRAM but it just won't use much of that last 1gb+.&lt;/p&gt; &lt;p&gt;Ollama backend, SillyTavern frontend.&lt;/p&gt; &lt;p&gt;Using KV cache at Q4.&lt;/p&gt; &lt;p&gt;Any help appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kovnev"&gt; /u/kovnev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iipuqg/variables_to_force_more_gpu_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iipuqg/variables_to_force_more_gpu_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iipuqg/variables_to_force_more_gpu_usage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T00:43:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiflm1</id>
    <title>Using DeepSeek To Make A Game</title>
    <updated>2025-02-05T17:33:01+00:00</updated>
    <author>
      <name>/u/sveennn</name>
      <uri>https://old.reddit.com/user/sveennn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iiflm1/using_deepseek_to_make_a_game/"&gt; &lt;img alt="Using DeepSeek To Make A Game" src="https://external-preview.redd.it/Jsnuak6K7miLugvD2wowYcZVRD06OvCXYvnQr335CGY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7128c34bc49c26ff56a9bf1a8250bb7747e29153" title="Using DeepSeek To Make A Game" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sveennn"&gt; /u/sveennn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@sveennn/using-deepseek-to-make-a-game-db6932157495"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iiflm1/using_deepseek_to_make_a_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iiflm1/using_deepseek_to_make_a_game/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T17:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1iilv2d</id>
    <title>Anyone successfully used ollama with Cline?</title>
    <updated>2025-02-05T21:47:11+00:00</updated>
    <author>
      <name>/u/boxabirds</name>
      <uri>https://old.reddit.com/user/boxabirds</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rapidly tiring of the rate limits that various endpoints having for AI coding, and did some brief experiments trying to substitute with a local llama with models that fit into 24 GB.&lt;/p&gt; &lt;p&gt;No luck so fat: actually not even using deep seek cloud R1 with Cline has worked. &lt;/p&gt; &lt;p&gt;I guess it‚Äôs one of those ‚Äúwait 12 months and it‚Äôll all be sorted out‚Äú situations‚Ä¶ ü§î&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boxabirds"&gt; /u/boxabirds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iilv2d/anyone_successfully_used_ollama_with_cline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iilv2d/anyone_successfully_used_ollama_with_cline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iilv2d/anyone_successfully_used_ollama_with_cline/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T21:47:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1iirjfx</id>
    <title>Ai</title>
    <updated>2025-02-06T02:05:12+00:00</updated>
    <author>
      <name>/u/prettytjts</name>
      <uri>https://old.reddit.com/user/prettytjts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been playing around with Ai llms for a couple days now, but struggling to run them on my server. I need a gpu that's good for running deepseek r1 32b and hopefully 70b. What are the requirements for that ? I want my own local Ai. I have seen a lot of people recommending the rtx series of gpus. I also want the 32b model to be fast and hopefully able to run the 70b model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prettytjts"&gt; /u/prettytjts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iirjfx/ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iirjfx/ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iirjfx/ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T02:05:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iishul</id>
    <title>Monitors freeze on prompt</title>
    <updated>2025-02-06T02:53:47+00:00</updated>
    <author>
      <name>/u/rogerfin</name>
      <uri>https://old.reddit.com/user/rogerfin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running Debian Bookworm with KDE plasma x11, Nvidia RTX 3060 12g (primary) + Nvidia p620, with 4 monitors to each card, 228 GB system RAM, running latest stable drivers/kernel, no backports. Tried various small/large models, but the moment I press enter on the ollama prompt, all the monitors connected to primary GPU freeze, while secondary monitors keep working, and prompt responds normally. Have to restart sddm to claim back the frozen monitors.&lt;/p&gt; &lt;p&gt;Found similar issue here, but no solution: &lt;a href="https://forums.developer.nvidia.com/t/550-76-running-a-prompt-on-ollama-instantly-freezes-all-external-monitors/291606"&gt;https://forums.developer.nvidia.com/t/550-76-running-a-prompt-on-ollama-instantly-freezes-all-external-monitors/291606&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has someone managed to solve or any pointers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rogerfin"&gt; /u/rogerfin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iishul/monitors_freeze_on_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iishul/monitors_freeze_on_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iishul/monitors_freeze_on_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T02:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1iikoqb</id>
    <title>Handy scripts for local use</title>
    <updated>2025-02-05T20:59:09+00:00</updated>
    <author>
      <name>/u/Diligent_Property_39</name>
      <uri>https://old.reddit.com/user/Diligent_Property_39</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last few days i created some scripts that might also be interesting for this subreddit users. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;A local chat script for ollama that uses the available llm's from your local install.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Link: &lt;a href="https://github.com/xdep/Ollama-Chat-Client"&gt;https://github.com/xdep/Ollama-Chat-Client&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Another script to test the llm security for any flaws left by its creators. It will try multiple promps to see if it can bypass its default security measures.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Link: &lt;a href="https://github.com/xdep/llm-security-checks"&gt;https://github.com/xdep/llm-security-checks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;*** Screenshots available for both scripts to get an impression of the functionality.&lt;/p&gt; &lt;p&gt;Hope you guys like it :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diligent_Property_39"&gt; /u/Diligent_Property_39 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iikoqb/handy_scripts_for_local_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iikoqb/handy_scripts_for_local_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iikoqb/handy_scripts_for_local_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T20:59:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii6y3n</id>
    <title>Which model is best for RAG or chatting document?</title>
    <updated>2025-02-05T10:16:59+00:00</updated>
    <author>
      <name>/u/Interesting_Music464</name>
      <uri>https://old.reddit.com/user/Interesting_Music464</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to train a model locally on my Macbook Pro M1 32GB based on a technical standard/specifications that is written in a document format like PDF. Which model would you recommend for this case? I saw that MLX is best for Apple Silicon so that is my only lead on how to properly choose a model aside from choosing the number of parameters and available unified memory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting_Music464"&gt; /u/Interesting_Music464 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6y3n/which_model_is_best_for_rag_or_chatting_document/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6y3n/which_model_is_best_for_rag_or_chatting_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii6y3n/which_model_is_best_for_rag_or_chatting_document/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T10:16:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iioy7r</id>
    <title>Another Ollama+OpenWebUI Docker post...</title>
    <updated>2025-02-06T00:00:45+00:00</updated>
    <author>
      <name>/u/PaulLee420</name>
      <uri>https://old.reddit.com/user/PaulLee420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So... I'm on MacOS and can install Ollama using the MacOS download. Then I can spin up an Open WebUI docker container and it works perfectly.&lt;/p&gt; &lt;p&gt;However, I'd like to have ALL my AI data in docker containers - but when I install Ollama via docker, Open WebUI can see the LLMs but is gets some 500 error when I try to send a prompt...&lt;/p&gt; &lt;p&gt;I have followed the troubleshooting stuff on Open WebUI about this issue, changing the Docker run commands - but it simply doesn't work. Has there been any movement on this issue?&lt;/p&gt; &lt;p&gt;Do I need to post more details??? Will update if needed...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaulLee420"&gt; /u/PaulLee420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iioy7r/another_ollamaopenwebui_docker_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iioy7r/another_ollamaopenwebui_docker_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iioy7r/another_ollamaopenwebui_docker_post/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T00:00:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1iivo62</id>
    <title>Why are ollama package downloads at NPM zero?</title>
    <updated>2025-02-06T05:50:45+00:00</updated>
    <author>
      <name>/u/quantum-aey-ai</name>
      <uri>https://old.reddit.com/user/quantum-aey-ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iivo62/why_are_ollama_package_downloads_at_npm_zero/"&gt; &lt;img alt="Why are ollama package downloads at NPM zero?" src="https://external-preview.redd.it/sr7XqdeKF73E4m8CFm57jK-VSCmixf5xr3cX1tdw1SY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ac9ce6a0d25fae7f6d6d78c177a7289c0eb8c68" title="Why are ollama package downloads at NPM zero?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/1d9c22cijghe1.png?width=1348&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf8fde4921fb9e3b1436e30d19089a2f55119ffb"&gt;ollama downloads are 0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Is it NPM? What's happening?&lt;/p&gt; &lt;p&gt;[SOLVED] It is a bug with NPM. Download count is 0 for every package on &lt;a href="http://npmjs.com"&gt;npmjs.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantum-aey-ai"&gt; /u/quantum-aey-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iivo62/why_are_ollama_package_downloads_at_npm_zero/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iivo62/why_are_ollama_package_downloads_at_npm_zero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iivo62/why_are_ollama_package_downloads_at_npm_zero/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T05:50:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1iixbpm</id>
    <title>Roast/critique/improve my plan: Pre-training a model for a specific JSON structure</title>
    <updated>2025-02-06T07:44:31+00:00</updated>
    <author>
      <name>/u/anderssewerin</name>
      <uri>https://old.reddit.com/user/anderssewerin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to implement an agent that can help generate, edit and enhance what's basically a graph defined in a JSON structure with a given syntax. I have access to a pretty large chunk of existing examples.&lt;/p&gt; &lt;p&gt;My thinking is that the best way to go would be to specialize a model to understand this structure, so I won't have to include a description in every prompt. Downside would be cost and coding effort to train, but running costs would be reduced. Format is not likely to change significantly.&lt;/p&gt; &lt;p&gt;My thinking was to generate training pairs by knockong our nodes or chunks of the existing examples and generate completion pairs for training this way.&lt;/p&gt; &lt;p&gt;Does that make sense, and are there any good examples of this out there that I can learn from?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anderssewerin"&gt; /u/anderssewerin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iixbpm/roastcritiqueimprove_my_plan_pretraining_a_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iixbpm/roastcritiqueimprove_my_plan_pretraining_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iixbpm/roastcritiqueimprove_my_plan_pretraining_a_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T07:44:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1iixtss</id>
    <title>As a beginner, where to start to setup &amp; run AI locally on your system?</title>
    <updated>2025-02-06T08:22:13+00:00</updated>
    <author>
      <name>/u/ExtremePresence3030</name>
      <uri>https://old.reddit.com/user/ExtremePresence3030</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are dozens of different apps for client and server side or some doing both, and different AI models as well. It makes me confused where to start. Is there any guideline ever written anywhere about how to choose the right app for you and how to install it through step-by-step guideline?&lt;/p&gt; &lt;p&gt;(My system is 6GB GPU, and 40GB Ram . My usage is research, brainstorming and research)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExtremePresence3030"&gt; /u/ExtremePresence3030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iixtss/as_a_beginner_where_to_start_to_setup_run_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iixtss/as_a_beginner_where_to_start_to_setup_run_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iixtss/as_a_beginner_where_to_start_to_setup_run_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T08:22:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1iijkmy</id>
    <title>Step-by-Step Guide to Running Open Deep Research with smolagents</title>
    <updated>2025-02-05T20:13:05+00:00</updated>
    <author>
      <name>/u/KonradFreeman</name>
      <uri>https://old.reddit.com/user/KonradFreeman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I had heard something about OpenAI's Deep Research&lt;/p&gt; &lt;p&gt;OpenAI‚Äôs Deep Research represents a leap toward AGI by enabling AI to independently discover and synthesize knowledge. While still evolving, its ability to automate expert-level research has transformative potential across industries. For users, however, its current instability and access limitations temper immediate utility, signaling a need for ongoing refinement.&lt;/p&gt; &lt;p&gt;Then I got an email this morning about this new open source project which reverse engineered how Deep Research works:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/blog/open-deep-research"&gt;https://huggingface.co/blog/open-deep-research&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I ran it and wrote a guide on how to run it:&lt;/p&gt; &lt;p&gt;&lt;a href="https://danielkliewer.com/2025/02/05/open-deep-research"&gt;https://danielkliewer.com/2025/02/05/open-deep-research&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You just run this command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;smolagent &amp;quot;{PROMPT}&amp;quot; \ --model-type &amp;quot;HfApiModel&amp;quot; \ --model-id &amp;quot;Qwen/Qwen2.5-Coder-32B-Instruct&amp;quot; \ --imports &amp;quot;pandas numpy&amp;quot; \ --tools &amp;quot;web_search translation&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I was surprised by some of what you can do with it and am interested in applying it and using it as a starting point for some other project.&lt;/p&gt; &lt;p&gt;The interesting thing about it is how it interacts with code instead of JSON and how this increases its accuracy considerably on the benchmarks.&lt;/p&gt; &lt;p&gt;I am planning on adapting this framework to work with Ollama and run local models. I am organizing a Hackathon on the 13th to do just that and more.&lt;/p&gt; &lt;p&gt;The idea is to develop software that benefits humanity using the reverse engineering of ClosedAI's latest model locally using Ollama.&lt;/p&gt; &lt;p&gt;Or whichever inference engine you like.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KonradFreeman"&gt; /u/KonradFreeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iijkmy/stepbystep_guide_to_running_open_deep_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iijkmy/stepbystep_guide_to_running_open_deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iijkmy/stepbystep_guide_to_running_open_deep_research/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T20:13:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1iihpxs</id>
    <title>Which small models are better than the original ChatGPT (based on GPT 3.5 released in November 2022) ?</title>
    <updated>2025-02-05T18:59:06+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm wondering about those small models that we can run on our PCs. How many parameters do you need to have better performance than the first version of ChatGPT that was released in November 2022?&lt;/p&gt; &lt;p&gt;Are parameters the only measures that count? &lt;strong&gt;&lt;em&gt;Perhaps newer models can achieve the same performance with less parameters?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm asking this because I consider the original ChatGPT to be kind of first serious model. Everything below it, seems like a toy model.&lt;/p&gt; &lt;p&gt;So I'm wondering now in 2025, do we have any models that we can run on PC as good as the first ChatGPT?&lt;/p&gt; &lt;p&gt;If I recall, GPT 3, a predecessor to GPT 3.5, already had 175 billion parameters when it was released in 2020! And it was a long time ago!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iihpxs/which_small_models_are_better_than_the_original/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iihpxs/which_small_models_are_better_than_the_original/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iihpxs/which_small_models_are_better_than_the_original/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T18:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1iifmpr</id>
    <title>qwen 2.5 VL on Ollama</title>
    <updated>2025-02-05T17:34:19+00:00</updated>
    <author>
      <name>/u/mans-987</name>
      <uri>https://old.reddit.com/user/mans-987</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any way to use qwen 2.5 VL with Ollama? The model is open source and can be found here on hugging face: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"&gt;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;also on github: &lt;a href="https://github.com/QwenLM/Qwen2.5-VL"&gt;https://github.com/QwenLM/Qwen2.5-VL&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mans-987"&gt; /u/mans-987 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iifmpr/qwen_25_vl_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iifmpr/qwen_25_vl_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iifmpr/qwen_25_vl_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T17:34:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij0kq6</id>
    <title>Ollama commands: How to use Ollama in the command line [Part 2]</title>
    <updated>2025-02-06T11:38:54+00:00</updated>
    <author>
      <name>/u/geshan</name>
      <uri>https://old.reddit.com/user/geshan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ij0kq6/ollama_commands_how_to_use_ollama_in_the_command/"&gt; &lt;img alt="Ollama commands: How to use Ollama in the command line [Part 2]" src="https://external-preview.redd.it/jlW-oWEX_HQ97HQZemLXUJ798r1KAbkE9X8R5aBgHBs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6af01b870b8ee04789b7d38be82e984b4d9b80c0" title="Ollama commands: How to use Ollama in the command line [Part 2]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geshan"&gt; /u/geshan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://geshan.com.np/blog/2025/02/ollama-commands/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij0kq6/ollama_commands_how_to_use_ollama_in_the_command/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij0kq6/ollama_commands_how_to_use_ollama_in_the_command/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T11:38:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1iitfiv</id>
    <title>Function Calling in Terminal + DeepSeek-R1-Distill-Llama-70B-Q_8 + vLLM -&gt; Sometimes...</title>
    <updated>2025-02-06T03:43:17+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7h5utciiwfhe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iitfiv/function_calling_in_terminal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iitfiv/function_calling_in_terminal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T03:43:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiwunj</id>
    <title>Hosting ollama on a Proxmox LXC Container with GPU Passthrough.</title>
    <updated>2025-02-06T07:09:34+00:00</updated>
    <author>
      <name>/u/ninja-con-gafas</name>
      <uri>https://old.reddit.com/user/ninja-con-gafas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently hosted the DeepSeek-R1 14b model on a LXC container. I am sharing some key lessons that I learnt during the process. You can find the original article &lt;a href="https://www.linkedin.com/pulse/lessons-learned-setting-up-lxc-container-proxmox-gpu-atharv-darekar-s1xef/?trackingId=UP4SkEJNtMEehqiXXIxzqw%3D%3D"&gt;hear&lt;/a&gt;. I have used AI to compose my thoughts.&lt;/p&gt; &lt;h1&gt;1. Preparation is Everything&lt;/h1&gt; &lt;p&gt;Before diving into the setup, it‚Äôs essential to refer to a well-documented guide or tutorial to avoid potential pitfalls. I found the following resources particularly useful:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Video Tutorial:&lt;/strong&gt; &lt;a href="https://youtu.be/lNGNRIJ708k?feature=shared"&gt;Proxmox LXC GPU Passthrough&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Guide:&lt;/strong&gt; &lt;a href="https://digitalspaceport.com/proxmox-lxc-gpu-passthru-setup-guide/"&gt;Digital Spaceport‚Äôs Setup Guide&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. Get the Right Drivers&lt;/h1&gt; &lt;p&gt;Ensure you download and install the correct and latest GPU drivers for both the host and the container. Any version mismatch can cause frustrating errors down the line.&lt;/p&gt; &lt;h1&gt;3. Beware of Dynamic Device Numbers&lt;/h1&gt; &lt;p&gt;Running the command `ls -al /dev/nvidia*` will give you the major and minor device numbers, but these numbers are not static‚Äîthey can change after a system reboot. To mitigate this issue, list all known device numbers in the container‚Äôs configuration file. This way, the container can automatically pick the relevant one without requiring manual intervention post-reboot.&lt;/p&gt; &lt;h1&gt;4. Keep Host and Container Drivers in Sync&lt;/h1&gt; &lt;p&gt;The host and container must have &lt;strong&gt;identical driver versions&lt;/strong&gt;. If you‚Äôre installing CUDA, be sure to uncheck the bundled driver option‚Äîinstalling it could cause a version mismatch error in the container.&lt;/p&gt; &lt;h1&gt;5. Host Changes Can Break the Container&lt;/h1&gt; &lt;p&gt;Since an LXC container shares the kernel with the host, any updates to the host (such as a driver update or kernel upgrade) may break the container. Also, use the -dkms flag when installing drivers on the host (ensure dkms is installed first) and when installing drivers inside the container, use the --no-kernel-modules option to prevent conflicts.&lt;/p&gt; &lt;h1&gt;6. Backup, Backup, Backup...!&lt;/h1&gt; &lt;p&gt;Before making any major system changes‚Äîlike installing new packages, updating drivers, or modifying kernel modules‚Äî&lt;strong&gt;create a system backup&lt;/strong&gt;. Having a rollback option can be a lifesaver if something goes wrong.&lt;/p&gt; &lt;h1&gt;Final Thoughts&lt;/h1&gt; &lt;p&gt;Setting up an LXC container on Proxmox with GPU passthrough requires attention to detail, but with proper planning, the process becomes much smoother. If you're tackling this for the first time, take it slow, document your steps, and always have a backup ready.&lt;/p&gt; &lt;p&gt;Have you encountered any challenges while setting up GPU passthrough in LXC? I‚Äôd love to hear about your experiences and solutions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninja-con-gafas"&gt; /u/ninja-con-gafas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iiwunj/hosting_ollama_on_a_proxmox_lxc_container_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iiwunj/hosting_ollama_on_a_proxmox_lxc_container_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iiwunj/hosting_ollama_on_a_proxmox_lxc_container_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T07:09:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1iilug6</id>
    <title>Ollama + DataBridge: Creating an interactive learning platform under 2 minutes!</title>
    <updated>2025-02-05T21:46:26+00:00</updated>
    <author>
      <name>/u/yes-no-maybe_idk</name>
      <uri>https://old.reddit.com/user/yes-no-maybe_idk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=tfqIa_6lqQU"&gt;https://www.youtube.com/watch?v=tfqIa_6lqQU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Learn how to turn any video into an interactive learning tool with Databridge! In this demo, we'll show you how to ingest a lecture video and generate engaging questions with DataBridge, all locally Using Ollama and DataBridge.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/databridge-org/databridge-core"&gt;https://github.com/databridge-org/databridge-core&lt;/a&gt;&lt;br /&gt; Docs: &lt;a href="https://databridge.gitbook.io/databridge-docs"&gt;https://databridge.gitbook.io/databridge-docs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear comments, see you build cool stuff (or maybe even contribute to our OSS library).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yes-no-maybe_idk"&gt; /u/yes-no-maybe_idk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iilug6/ollama_databridge_creating_an_interactive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iilug6/ollama_databridge_creating_an_interactive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iilug6/ollama_databridge_creating_an_interactive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T21:46:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiycpb</id>
    <title>How to Ollama on-demand?</title>
    <updated>2025-02-06T09:02:35+00:00</updated>
    <author>
      <name>/u/RamenKomplex</name>
      <uri>https://old.reddit.com/user/RamenKomplex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;We would like to run a ollama server for our team and let team members use it via the openweb ui. We can create a new VM on Aws and run it there but since it won't be used all the time, this would be a waste of the rather expensive resources. &lt;/p&gt; &lt;p&gt;Are there any smarter ways of deploying ollama based openwebui for team use in a way that we only pay for the GPU/CPU only when chat is being used?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RamenKomplex"&gt; /u/RamenKomplex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iiycpb/how_to_ollama_ondemand/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iiycpb/how_to_ollama_ondemand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iiycpb/how_to_ollama_ondemand/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T09:02:35+00:00</published>
  </entry>
</feed>
