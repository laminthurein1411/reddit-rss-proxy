<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-24T23:35:12+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jhpxz3</id>
    <title>Enough resources for local AI?</title>
    <updated>2025-03-23T03:10:41+00:00</updated>
    <author>
      <name>/u/JagerAntlerite7</name>
      <uri>https://old.reddit.com/user/JagerAntlerite7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for advice on running Ollama locally on my outdated Dell Precision 3630. I do not need amazing performance, just hoping for coding assistance.&lt;/p&gt; &lt;p&gt;Here are the workstation specs: * OS: Ubuntu 24.04.01 LTS * CPU: Intel Core Processor i7 (8 cores) * RAM: 128GB * GPU: Nvidia Quadro P2000 5GB * Storage: 1TB NVMe * IDEs: VSCode and JetBrains&lt;/p&gt; &lt;p&gt;If those resources sound reasonable for my use case, what library is suggested?&lt;/p&gt; &lt;p&gt;EDITS: Added Dell model number &amp;quot;3630&amp;quot;, corrected storage size, added GPU memory.&lt;/p&gt; &lt;p&gt;UPDATES: * 2025-03-24: Ollama install was painless, yet prompt responses are painfully slow. Needs to be faster. I tried using multiple 0.5B and 1B models. My 5GB GPU memory seems to be the bottle neck. With only a single PCIe x16 I cannot add additional cards and I do not have the PS wattage for a single bigger card. Appears I am stuck. Additonally, none played well with &lt;a href="https://github.com/block/goose"&gt;Codename Goose&lt;/a&gt;'s MCP extensions. Sadness. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JagerAntlerite7"&gt; /u/JagerAntlerite7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhpxz3/enough_resources_for_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhpxz3/enough_resources_for_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jhpxz3/enough_resources_for_local_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T03:10:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhv79c</id>
    <title>Budget GPU for Deepseek</title>
    <updated>2025-03-23T09:08:36+00:00</updated>
    <author>
      <name>/u/BillGRC</name>
      <uri>https://old.reddit.com/user/BillGRC</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I need a budget GPU for an old Z77 system (ReBar enabled BIOS patch) to try some small Deepseek distilled models. I can find RX 5500XT 8GB and ARC A380 near the same price under 100$. Which card will perform better (t/s)? My main OS is Linux Ubuntu 22.04. I'm a really casual gamer playing here and there some CS2 and maybe some PUBG. I know RX 5500XT is better for games but ARC is way better for transcoding. Thanks for your time! Really appreciate.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BillGRC"&gt; /u/BillGRC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhv79c/budget_gpu_for_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhv79c/budget_gpu_for_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jhv79c/budget_gpu_for_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T09:08:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji7009</id>
    <title>How to get attention scores in ollama models?</title>
    <updated>2025-03-23T19:08:45+00:00</updated>
    <author>
      <name>/u/Ok_Company6990</name>
      <uri>https://old.reddit.com/user/Ok_Company6990</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am writing a research paper and for that I need the attention scores of the output generated by the llm. Is there any way that I can access the scores in ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Company6990"&gt; /u/Ok_Company6990 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji7009/how_to_get_attention_scores_in_ollama_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji7009/how_to_get_attention_scores_in_ollama_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ji7009/how_to_get_attention_scores_in_ollama_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T19:08:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji60du</id>
    <title>Branching out from the Ollama library</title>
    <updated>2025-03-23T18:27:07+00:00</updated>
    <author>
      <name>/u/Inner-End7733</name>
      <uri>https://old.reddit.com/user/Inner-End7733</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've pretty much exhausted my options for models in the official library that I'm interested in running. I'm looking for recs on stuff I could get on huggingface or github that you've had success with. I think 14b q4 seems to be the ideal size/quant for my set up, but I'm interested in seeing what the limits of other quants are on my machine too. I'm a big fan of Phi4 at the moment, it's got some decent techincal hardware knowledge, and I'm also a pretty big fan of mistral-nemo, and to an extent gemma3:12b from the library. What your favorite model in this specification range to run? anything with more than 14b parameters but under 20 that you like?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inner-End7733"&gt; /u/Inner-End7733 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji60du/branching_out_from_the_ollama_library/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji60du/branching_out_from_the_ollama_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ji60du/branching_out_from_the_ollama_library/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T18:27:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhpdgf</id>
    <title>üöÄ AI Terminal v0.1 ‚Äî A Modern, Open-Source Terminal with Local AI Assistance!</title>
    <updated>2025-03-23T02:38:28+00:00</updated>
    <author>
      <name>/u/Macsdeve</name>
      <uri>https://old.reddit.com/user/Macsdeve</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're excited to announce AI Terminal, an open-source, Rust-powered terminal that's designed to simplify your command-line experience through the power of local AI.&lt;/p&gt; &lt;p&gt;Key features include:&lt;/p&gt; &lt;p&gt;Local AI Assistant: Interact directly in your terminal with a locally running, fine-tuned LLM for command suggestions, explanations, or automatic execution.&lt;/p&gt; &lt;p&gt;Git Repository Visualization: Easily view and navigate your Git repositories.&lt;/p&gt; &lt;p&gt;Smart Autocomplete: Quickly autocomplete commands and paths to boost productivity.&lt;/p&gt; &lt;p&gt;Real-time Stream Output: Instant display of streaming command outputs.&lt;/p&gt; &lt;p&gt;Keyboard-First Design: Navigate smoothly with intuitive shortcuts and resizable panels‚Äîno mouse required!&lt;/p&gt; &lt;p&gt;What's next on our roadmap:&lt;/p&gt; &lt;p&gt;üõ†Ô∏è Community-driven development: Your feedback shapes our direction!&lt;/p&gt; &lt;p&gt;üìå Session persistence: Keep your workflow intact across terminal restarts.&lt;/p&gt; &lt;p&gt;üîç Automatic AI reasoning &amp;amp; error detection: Let AI handle troubleshooting seamlessly.&lt;/p&gt; &lt;p&gt;üåê Ollama independence: Developing our own lightweight embedded AI model.&lt;/p&gt; &lt;p&gt;üé® Enhanced UI experience: Continuous UI improvements while keeping it clean and intuitive.&lt;/p&gt; &lt;p&gt;We'd love to hear your thoughts, ideas, or even better‚Äîhave you contribute!&lt;/p&gt; &lt;p&gt;‚≠ê GitHub repo: &lt;a href="https://github.com/MicheleVerriello/ai-terminal"&gt;https://github.com/MicheleVerriello/ai-terminal&lt;/a&gt; üëâ Try it out: &lt;a href="https://ai-terminal.dev/"&gt;https://ai-terminal.dev/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Contributors warmly welcomed! Join us in redefining the terminal experience.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Macsdeve"&gt; /u/Macsdeve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhpdgf/ai_terminal_v01_a_modern_opensource_terminal_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhpdgf/ai_terminal_v01_a_modern_opensource_terminal_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jhpdgf/ai_terminal_v01_a_modern_opensource_terminal_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T02:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji6xt2</id>
    <title>Ollama not using my Gpu</title>
    <updated>2025-03-23T19:06:03+00:00</updated>
    <author>
      <name>/u/Key_Appointment_7582</name>
      <uri>https://old.reddit.com/user/Key_Appointment_7582</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ji6xt2/ollama_not_using_my_gpu/"&gt; &lt;img alt="Ollama not using my Gpu" src="https://b.thumbs.redditmedia.com/LA2n2GtvgCXvf0CsCZW0bYhAMoS_G7afSJX7TQLr3Lw.jpg" title="Ollama not using my Gpu" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/emmo9002mhqe1.png?width=451&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d14c89f4e57705487097c1ae6a19ccedad4697a2"&gt;https://preview.redd.it/emmo9002mhqe1.png?width=451&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d14c89f4e57705487097c1ae6a19ccedad4697a2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My computer will not use my GPU when running llama 3.1 8b. I was working perfectly yesterday and now it doesn't. Has anyone had this problem? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Key_Appointment_7582"&gt; /u/Key_Appointment_7582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji6xt2/ollama_not_using_my_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji6xt2/ollama_not_using_my_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ji6xt2/ollama_not_using_my_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T19:06:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jif4v0</id>
    <title>iOS Apps with Vision and Voice</title>
    <updated>2025-03-24T01:14:40+00:00</updated>
    <author>
      <name>/u/simracerman</name>
      <uri>https://old.reddit.com/user/simracerman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for an iOS App that connects directly to Ollama (currently using Open WebUI, but it's clinky in Safari on iOS). I tried Reins and Enchanted but they are too barebone (can't even adjust font size).&lt;/p&gt; &lt;p&gt;There are plenty of Apps on App Store but they are either all subscriptions based, or collect every last info they can to justify their existence.&lt;/p&gt; &lt;p&gt;I don't mind paying $10-$20 one time for something more customizable than Enchanted, supports vision, read aloud (not necessary but nice), and keyboard extension.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simracerman"&gt; /u/simracerman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jif4v0/ios_apps_with_vision_and_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jif4v0/ios_apps_with_vision_and_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jif4v0/ios_apps_with_vision_and_voice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-24T01:14:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji5tk2</id>
    <title>Added web search to my ollama Discord bot.</title>
    <updated>2025-03-23T18:19:04+00:00</updated>
    <author>
      <name>/u/4500vcel</name>
      <uri>https://old.reddit.com/user/4500vcel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ji5tk2/added_web_search_to_my_ollama_discord_bot/"&gt; &lt;img alt="Added web search to my ollama Discord bot." src="https://preview.redd.it/gtqojwh8ehqe1.gif?width=640&amp;amp;crop=smart&amp;amp;s=81031a8860b48b78fc3ad1a9af9958c95a585c96" title="Added web search to my ollama Discord bot." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I‚Äôve shared my Discord bot a couple times here. I just added a RAG pipeline for web search. It gathers info from Wikipedia and DuckDuckGo search result. It‚Äôs enabled by sending &lt;code&gt;+ search&lt;/code&gt; and disabled by sending &lt;code&gt;+ model&lt;/code&gt;. It can be found here: &lt;a href="https://github.com/jake83741/vnc-lm"&gt;https://github.com/jake83741/vnc-lm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If anyone ends up trying and has any feedback, I‚Äôd love to hear it. Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/4500vcel"&gt; /u/4500vcel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gtqojwh8ehqe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji5tk2/added_web_search_to_my_ollama_discord_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ji5tk2/added_web_search_to_my_ollama_discord_bot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T18:19:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji8vlm</id>
    <title>Codename Goose agentic AI</title>
    <updated>2025-03-23T20:28:26+00:00</updated>
    <author>
      <name>/u/JagerAntlerite7</name>
      <uri>https://old.reddit.com/user/JagerAntlerite7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using Block's open source &lt;a href="https://block.github.io/goose/"&gt;Codename Goose&lt;/a&gt; CLI paired with Google Gemini 1.5 Pro and other LLMs for a couple months now. Goose runs locally, keeping control in my hands, allowing me to perform all the same coding tasks from a terminal that I would normally do from a browser session.&lt;/p&gt; &lt;p&gt;While a CLI is a welcome convenience, the real power is the ability to use any Model Context Protocol (MCP) server extension. Goose is agentic AI, the next step beyond LLMs, and these &lt;a href="https://block.github.io/goose/v1/extensions/"&gt;extensions&lt;/a&gt; are the really exciting part. &lt;/p&gt; &lt;p&gt;There are four built-in extensions that can be enabled right away: * &amp;quot;Memory&amp;quot; provides additional context for future prompt responses * &amp;quot;Developer Tools&amp;quot; allows editing and shell command execution * &amp;quot;JetBrains&amp;quot; for IDE integration and enhanced context * &amp;quot;Computer Controls&amp;quot; make webscraping, file caching, and automations possible &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JagerAntlerite7"&gt; /u/JagerAntlerite7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji8vlm/codename_goose_agentic_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji8vlm/codename_goose_agentic_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ji8vlm/codename_goose_agentic_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T20:28:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jigshi</id>
    <title>Need Feedback - LLM based commit message generator</title>
    <updated>2025-03-24T02:41:10+00:00</updated>
    <author>
      <name>/u/Tangoua</name>
      <uri>https://old.reddit.com/user/Tangoua</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I hope this post is appropriate for this sub. I was assigned a task as part of an assignment. I had to use the gemma3:1b model to create a tool. I made this commit message generator which takes in the output of git diff to generate messages. I know this has been done many times before but I took this upon myself to learn more about Ollama and LLMs in general.&lt;/p&gt; &lt;p&gt;It can be found here: &lt;a href="https://github.com/Git-Uzair/ez-commit"&gt;https://github.com/Git-Uzair/ez-commit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The assignment requires me to gather feedback from at least 1 potential user. I would be very thankful for any!&lt;/p&gt; &lt;p&gt;Also, I am aware it is far from perfect and will give wrong commit messages and for that, I needed a few answers from you guys.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How do we modify the system message for gemma3:1b model? Is there an example I can follow?&lt;/li&gt; &lt;li&gt;Can we adjust the temperature for the model through the Ollama library, I tried passing in different values through the generate function but it didn't seem to fix/break anything.&lt;/li&gt; &lt;li&gt;Has anyone made a custom model file for this specific model?&lt;/li&gt; &lt;li&gt;Is there a rule of thumb for a system message for LLMs in general that I should follow?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tangoua"&gt; /u/Tangoua &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jigshi/need_feedback_llm_based_commit_message_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jigshi/need_feedback_llm_based_commit_message_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jigshi/need_feedback_llm_based_commit_message_generator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-24T02:41:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jimz35</id>
    <title>Rag - context/length of response settings</title>
    <updated>2025-03-24T09:51:40+00:00</updated>
    <author>
      <name>/u/xUaScalp</name>
      <uri>https://old.reddit.com/user/xUaScalp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have tested this RAG (&lt;a href="https://github.com/paquino11/chatpdf-rag-deepseek-r1"&gt;https://github.com/paquino11/chatpdf-rag-deepseek-r1&lt;/a&gt; )interaction with documentation for Xcode , but mostly return is very short , is there some way increase length of response ? &lt;/p&gt; &lt;p&gt;Model used deepseek-r1:32b . &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xUaScalp"&gt; /u/xUaScalp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jimz35/rag_contextlength_of_response_settings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jimz35/rag_contextlength_of_response_settings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jimz35/rag_contextlength_of_response_settings/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-24T09:51:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji4p5v</id>
    <title>GPU &amp; Ollama Recommendations</title>
    <updated>2025-03-23T17:32:20+00:00</updated>
    <author>
      <name>/u/BenjaminForggoti</name>
      <uri>https://old.reddit.com/user/BenjaminForggoti</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've read through numerous similar posts, but as a complete beginner I'm not sure what difference do specific ollama models provide. &lt;/p&gt; &lt;p&gt;As a copywriter I would like to train an LLM locally to automate my tasks. The idea is to train it based on my writing style (which requires numerous prompts on ChatGPT &amp;amp; Grok that I need to input every single time). &lt;/p&gt; &lt;p&gt;I'm planning on building a first machine and as I understand GPU is the most important factor. &lt;/p&gt; &lt;p&gt;What model of GPU &amp;amp; Ollama would you recommend for this type of work? My budget for building a PC would be around $1000-$1200. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BenjaminForggoti"&gt; /u/BenjaminForggoti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji4p5v/gpu_ollama_recommendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji4p5v/gpu_ollama_recommendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ji4p5v/gpu_ollama_recommendations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T17:32:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jiouk4</id>
    <title>Reset parameters do default ?</title>
    <updated>2025-03-24T11:54:55+00:00</updated>
    <author>
      <name>/u/xUaScalp</name>
      <uri>https://old.reddit.com/user/xUaScalp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How can we reset parameters to default in models in CLI ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xUaScalp"&gt; /u/xUaScalp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jiouk4/reset_parameters_do_default/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jiouk4/reset_parameters_do_default/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jiouk4/reset_parameters_do_default/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-24T11:54:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jik116</id>
    <title>Does Gemma3 have some optimization to make more use of the GPU in Ollama?</title>
    <updated>2025-03-24T06:01:36+00:00</updated>
    <author>
      <name>/u/matthewcasperson</name>
      <uri>https://old.reddit.com/user/matthewcasperson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Ollama for a while now with a 16GB 4060 Ti and models split between the GPU and CPU. CPU and GPU usage follow a fairly predictable pattern: there is a brief burst of GPU activity and a longer sustained period of high CPU usage. This makes sense to me as the GPU finishes its work quickly, and the CPU takes longer to finish the layers it has been assigned.&lt;/p&gt; &lt;p&gt;Then I tried gemma3 and I am seeing high and consistent GPU usage and very little CPU usage. This is despite the fact that &amp;quot;ollama ps&amp;quot; clearly shows &amp;quot;73%/27% CPU/GPU&amp;quot;.&lt;/p&gt; &lt;p&gt;Did Google do some optimization that allowed Gemma3 to run in the GPU despite being split between the GPU and CPU? I don't understand how a model with a 73%/27% CPU/GPU split manages to execute (by all appearances) in the GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matthewcasperson"&gt; /u/matthewcasperson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jik116/does_gemma3_have_some_optimization_to_make_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jik116/does_gemma3_have_some_optimization_to_make_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jik116/does_gemma3_have_some_optimization_to_make_more/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-24T06:01:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jimv74</id>
    <title>Limitations of Coding Assistants: Seeking Feedback and Collaborators</title>
    <updated>2025-03-24T09:43:25+00:00</updated>
    <author>
      <name>/u/visdalal</name>
      <uri>https://old.reddit.com/user/visdalal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm diving back into coding after a long hiatus (like, a decade!) and have been tinkering with various coding assistants. While they‚Äôre cool for basic boilerplate stuff, I‚Äôve noticed some consistent gripes that I‚Äôm curious if anyone else has run into:&lt;/p&gt; &lt;p&gt;‚Ä¢ Cost: I‚Äôve tried tools like Cline and Replit at scale. Basic templates work fine, but when it comes to refining code, the costs just balloon. Anyone else feeling this pain? &lt;/p&gt; &lt;p&gt;‚Ä¢ Local LLM Support: Some assistants claim to support local LLMs, but they struggle with models in the 3b/7b range. I rarely get meaningful completions with these smaller parameter models. &lt;/p&gt; &lt;p&gt;‚Ä¢ Code Reusability: I‚Äôm all about reusing common modules (logging, DB management, queue management, etc.). Yet, starting a new project feels like reinventing the wheel every time. &lt;/p&gt; &lt;p&gt;‚Ä¢ Verification &amp;amp; Planning: A lot of these tools just assume and dive straight into code without proper verification. Cline‚Äôs Planning mode is a cool step, but I‚Äôd love a more structured approach to validate what‚Äôs about to be coded. &lt;/p&gt; &lt;p&gt;‚Ä¢ Testing: Ensuring that every module is unit tested feels like an uphill battle with the current state of these assistants. &lt;/p&gt; &lt;p&gt;‚Ä¢ Output Refinement: The models typically spit out code in one go. I‚Äôd prefer an iterative approach‚Äîevaluate the output against standard practices, then refine it if needed. &lt;/p&gt; &lt;p&gt;‚Ä¢ Learning User Preferences: It‚Äôs a big gap that these tools don‚Äôt learn from my previous projects. I‚Äôd love if they could pick up on my preferred frameworks and coding styles automatically. &lt;/p&gt; &lt;p&gt;‚Ä¢ Dummy Code &amp;amp; Error Handling: I often see dummy functions or error handling that just wraps issues in try/catch blocks without really solving the underlying problem. &lt;/p&gt; &lt;p&gt;‚Ä¢ Iterative Development: In a real dev cycle, you start small (an MVP, perhaps) and then build iteratively. These assistants seem to miss that iterative, modular approach. &lt;/p&gt; &lt;p&gt;‚Ä¢ Context overruns: Again, solvable through modularizing the project, refactoring to small files to keep context small but needs manual effort&lt;/p&gt; &lt;p&gt;I‚Äôve seen some interesting discussions around prompt enforcement and breaking down tasks into smaller modules, but none of the assistants seem to tackle these core issues autonomously.&lt;/p&gt; &lt;p&gt;Has anyone come across a tool or built an agent that addresses some (or all!) of these pain points? I‚Äôm planning to try out &lt;a href="http://refact.ai"&gt;refact.ai&lt;/a&gt; soon‚Äîit looks like it might be geared towards these challenges‚Äîbut I‚Äôd love to share notes Or collaborate, or get feedback on any obvious blindspots in my take as I'm constantly thinking that wouldn't it be better for me to make my own multi-agent framework which is able to do some or all of these things rather than trying to make them work manually. I've already started building something custom with Local LLMs and would like to get a sense if others are in the same boat.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/visdalal"&gt; /u/visdalal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jimv74/limitations_of_coding_assistants_seeking_feedback/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jimv74/limitations_of_coding_assistants_seeking_feedback/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jimv74/limitations_of_coding_assistants_seeking_feedback/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-24T09:43:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jiwxlr</id>
    <title>Unable to Get Ollama to Work with GPU Passthrough on Proxmox - Docker Recognizes GPU, but Web UI Doesn't Load</title>
    <updated>2025-03-24T17:50:09+00:00</updated>
    <author>
      <name>/u/lowriskcork</name>
      <uri>https://old.reddit.com/user/lowriskcork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm currently trying to set up &lt;em&gt;Ollama&lt;/em&gt; (using the official &lt;code&gt;ollama/ollama&lt;/code&gt; Docker image) on my Proxmox setup, with GPU passthrough. However, I'm running into some issues with the GPU not being recognized properly within the &lt;em&gt;Ollama&lt;/em&gt;container, and I can't get the web UI to load.&lt;/p&gt; &lt;h1&gt;Setup Overview:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Proxmox Version&lt;/strong&gt;: Latest stable&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Host System&lt;/strong&gt;: Debian (LXC container) with GPU passthrough&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: NVIDIA Quadro P2000&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Docker Version&lt;/strong&gt;: Latest stable&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NVIDIA Driver&lt;/strong&gt;: 535.216.01&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CUDA Version&lt;/strong&gt;: 12.2&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Container Image&lt;/strong&gt;: &lt;code&gt;ollama/ollama&lt;/code&gt; from Docker Hub&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Current Setup:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I have successfully set up &lt;strong&gt;GPU passthrough&lt;/strong&gt; via Proxmox to a &lt;strong&gt;Debian LXC container (&lt;/strong&gt;unprivileged&lt;strong&gt;)&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Inside the container, I installed Docker, and the NVIDIA container runtime (&lt;code&gt;nvidia-docker2&lt;/code&gt;) is set up correctly.&lt;/li&gt; &lt;li&gt;The &lt;strong&gt;GPU&lt;/strong&gt; is passed through to the Docker container via the &lt;code&gt;--runtime=nvidia&lt;/code&gt; option, and Docker recognizes the GPU correctly.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Outputs:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;docker info | grep -i nvidia&lt;/code&gt;:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Runtimes: runc io.containerd.runc.v2 nvidia &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;2.&lt;code&gt;docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu20.04 nvidia-smi&lt;/code&gt;: This command correctly detects the GPU:&lt;/p&gt; &lt;p&gt;3.&lt;code&gt;docker run --rm --runtime=nvidia --gpus all ollama/ollama&lt;/code&gt;: The container runs, but it fails to initialize the GPU properly&lt;/p&gt; &lt;pre&gt;&lt;code&gt;2025/03/24 17:42:16 routes.go:1230: INFO server config env=... 2025/03/24 17:42:16.952Z level=WARN source=gpu.go:605 msg=&amp;quot;unknown error initializing cuda driver library /usr/lib/x86_64-linux-gnu/libcuda.so.535.216.01: cuda driver library init failure: 999. see https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md for more information&amp;quot; 2025/03/24 17:42:16.973Z level=INFO source=gpu.go:377 msg=&amp;quot;no compatible GPUs were discovered&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;4&lt;code&gt;nvidia-container-cli info&lt;/code&gt;: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;NVRM version: 535.216.01 CUDA version: 12.2 Device Index: 0 Model: Quadro P2000 Brand: Quadro GPU UUID: GPU-7c8d85e4-eb4f-40b7-c416-0b3fb8f867f6 Bus Location: 00000000:c1:00.0 Architecture: 6.1 +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 535.216.01 Driver Version: 535.216.01 CUDA Version: 12.2 | |-----------------------------------------+----------------------+----------------------| | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | 0 Quadro P2000 On | 00000000:C1:00.0 Off | N/A | | 47% 36C P8 5W / 75W | 1MiB / 5120MiB | 0% Default | +-----------------------------------------+----------------------+----------------------+ &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Issues:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Ollama does not recognize the GPU&lt;/strong&gt;: When trying to run &lt;code&gt;ollama/ollama&lt;/code&gt; via Docker, it reports an error with the CUDA driver and states that no compatible GPUs are discovered, even though other containers (like &lt;code&gt;nvidia/cuda&lt;/code&gt;) can access the GPU correctly.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Permissions issue with&lt;/strong&gt; &lt;code&gt;/dev/nvidia*&lt;/code&gt; &lt;strong&gt;devices&lt;/strong&gt;: I tried to set permissions using &lt;code&gt;chmod 666 /dev/nvidia*&lt;/code&gt;, but encountered &amp;quot;Operation not permitted&amp;quot; errors.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Steps I've Taken:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;NVIDIA Container Runtime&lt;/strong&gt;: I verified that &lt;code&gt;nvidia-docker2&lt;/code&gt; and &lt;code&gt;nvidia-container-runtime&lt;/code&gt; are installed and configured properly.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CUDA Installation&lt;/strong&gt;: I ensured that CUDA is properly installed and that the correct driver (&lt;code&gt;535.216.01&lt;/code&gt;) is running.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Running Docker with GPU&lt;/strong&gt;: I ran the Docker container with &lt;code&gt;--runtime=nvidia&lt;/code&gt; and &lt;code&gt;--gpus all&lt;/code&gt; to pass through the GPU to the container.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Testing with CUDA container&lt;/strong&gt;: The &lt;code&gt;nvidia/cuda&lt;/code&gt; container works perfectly, but &lt;code&gt;ollama/ollama&lt;/code&gt; does not.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Things I've Tried:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Using&lt;/strong&gt; &lt;code&gt;--privileged&lt;/code&gt; &lt;strong&gt;flag&lt;/strong&gt;: I ran the Docker container with the &lt;code&gt;--privileged&lt;/code&gt; flag to give it full access to the system's devices:bashCopyEditsudo docker run --rm --runtime=nvidia --gpus all --privileged ollama/ollama &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Checking Logs&lt;/strong&gt;: I looked into the logs for the &lt;code&gt;ollama/ollama&lt;/code&gt; container, but nothing stood out as a clear issue beyond the CUDA driver failure.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;What I'm Looking For:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Has anyone faced a similar issue with Ollama and GPU passthrough in Docker?&lt;/li&gt; &lt;li&gt;Is there any specific configuration required to make Ollama detect the GPU correctly?&lt;/li&gt; &lt;li&gt;Any insights into how I can get the web UI to load successfully?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thank you in advance for any help or suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lowriskcork"&gt; /u/lowriskcork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jiwxlr/unable_to_get_ollama_to_work_with_gpu_passthrough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jiwxlr/unable_to_get_ollama_to_work_with_gpu_passthrough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jiwxlr/unable_to_get_ollama_to_work_with_gpu_passthrough/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-24T17:50:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jiy9ua</id>
    <title>How to run Ollama on Runpod with multiple GPUs</title>
    <updated>2025-03-24T18:41:57+00:00</updated>
    <author>
      <name>/u/Accurate_Daikon_5972</name>
      <uri>https://old.reddit.com/user/Accurate_Daikon_5972</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, is anyone using runpod with multiple GPUs to run ollama?&lt;/p&gt; &lt;p&gt;I spent a few hours on it and did not achieve to leverage a second GPU on the same instance.&lt;/p&gt; &lt;p&gt;- I used a template with and without CUDA.&lt;br /&gt; - I installed CUDA toolkit.&lt;br /&gt; - I set CUDA_VISIBLE_DEVICES=0,1 environment variable before serving ollama.&lt;/p&gt; &lt;p&gt;But yet, I only see my first GPU going to 100% utilization and the second one at 0%.&lt;/p&gt; &lt;p&gt;Is there something else I should do? Or a specific Runpod template that is ready to use with ollama + open-webui + multiple GPUs?&lt;/p&gt; &lt;p&gt;Any help is greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accurate_Daikon_5972"&gt; /u/Accurate_Daikon_5972 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jiy9ua/how_to_run_ollama_on_runpod_with_multiple_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jiy9ua/how_to_run_ollama_on_runpod_with_multiple_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jiy9ua/how_to_run_ollama_on_runpod_with_multiple_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-24T18:41:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jj0dp6</id>
    <title>Dockerized Ollama Not Using GPU (CUDA init error 999)</title>
    <updated>2025-03-24T20:04:22+00:00</updated>
    <author>
      <name>/u/lowriskcork</name>
      <uri>https://old.reddit.com/user/lowriskcork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I'm running Ollama in Docker with GPU support, but it‚Äôs not using my GPU. My host and container both show my Quadro P2000 correctly via &lt;code&gt;nvidia-smi&lt;/code&gt; (Driver 535.216.01, CUDA 12.2). However, Ollama logs display:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;unknown error initializing cuda driver library /usr/lib/x86_64-linux-gnu/libcuda.so.535.216.01: cuda driver library init failure: 999 no compatible GPUs were discovered &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I‚Äôve tried setting the environment variable:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run --rm -it --gpus all -e LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu -p 11434:11434 ollama/ollama &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and ensured the NVIDIA container toolkit is installed. According to the Ollama GPU docs, GPUs with compute capability 5.0+ are supported (my GPU is 6.1).&lt;/p&gt; &lt;p&gt;Has anyone encountered this issue or have suggestions on how to resolve the CUDA initialization error inside Ollama? Thanks!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Advanced details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Host: Quadro P2000, nvidia-smi confirms GPU is detected.&lt;/li&gt; &lt;li&gt;Docker test with nvidia/cuda image works as expected.&lt;/li&gt; &lt;li&gt;Ollama falls back to CPU inference despite the GPU being visible.&lt;/li&gt; &lt;li&gt;Any troubleshooting tips or fixes would be appreciated.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lowriskcork"&gt; /u/lowriskcork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jj0dp6/dockerized_ollama_not_using_gpu_cuda_init_error/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jj0dp6/dockerized_ollama_not_using_gpu_cuda_init_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jj0dp6/dockerized_ollama_not_using_gpu_cuda_init_error/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-24T20:04:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jibdcg</id>
    <title>I built a Local AI Voice Assistant with Ollama + gTTS</title>
    <updated>2025-03-23T22:16:31+00:00</updated>
    <author>
      <name>/u/typhoon90</name>
      <uri>https://old.reddit.com/user/typhoon90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a local voice assistant that integrates Ollama for AI responses, it uses gTTS for text-to-speech, and pygame for audio playback. It queues and plays responses asynchronously, supports FFmpeg for audio speed adjustments, and maintains conversation history in a lightweight JSON-based memory system. Google also recently released their CHIRP voice models recently which sound a lot more natural however you need to modify the code slightly and add in your own API key/ json file.&lt;/p&gt; &lt;p&gt;Some key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Local AI Processing ‚Äì Uses Ollama to generate responses.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Audio Handling ‚Äì Queues and prioritizes TTS chunks to ensure smooth playback.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;FFmpeg Integration ‚Äì Speed mod TTS output if FFmpeg is installed (optional). I added this as I think google TTS sounds better at around x1.1 speed.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Memory System ‚Äì Retains past interactions for contextual responses.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Instructions: 1.Have ollama installed 2.Clone repo 3.Install requirements 4.Run app&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I figured others might find it useful or want to tinker with it. Repo is here if you want to check it out and would love any feedback:&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/ExoFi-Labs/OllamaGTTS"&gt;https://github.com/ExoFi-Labs/OllamaGTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;*Edit: I'm testing out TTS with faster whisper and Silero VAD at the moment, it seems to be working pretty well so far. I'll be testing it a bit more and try to push an update today or tomorrow.&lt;/p&gt; &lt;p&gt;*Edit2: Just pushed out an updated featuring speech to text using faster whisper and Silero VAD, so it is essentially fully voice enabled with voice interruption.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/typhoon90"&gt; /u/typhoon90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jibdcg/i_built_a_local_ai_voice_assistant_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jibdcg/i_built_a_local_ai_voice_assistant_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jibdcg/i_built_a_local_ai_voice_assistant_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T22:16:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jj0hoo</id>
    <title>Ollama same question with 4GB vs 8GB vs 12GB GPUs</title>
    <updated>2025-03-24T20:08:36+00:00</updated>
    <author>
      <name>/u/shanereaume</name>
      <uri>https://old.reddit.com/user/shanereaume</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1jj0hoo/video/i2z38rodwoqe1/player"&gt;https://reddit.com/link/1jj0hoo/video/i2z38rodwoqe1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I just updated an old Dell Precision M6600 that I was about to scrap, adding Kali and installing a Nvidia Quadro M3000M 4GB video card ( top left ) and have been looking for use as an MCP server or crawler, but not so excited about the performance for offloading work to just yet, so curious what others think. Here I am comparing to an 8GB Nvidia GeForce RTX 2070S ( top right ) and a 12GB Nvidia GeForce RTX 3060. You can see I used the same exaone-deep:2.4b Model, but found completion of the same task in this order:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Time&lt;/th&gt; &lt;th align="left"&gt;Graphics Card&lt;/th&gt; &lt;th align="left"&gt;CPU&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;4:16&lt;/td&gt; &lt;td align="left"&gt;Quadro M3000M 4GB&lt;/td&gt; &lt;td align="left"&gt;i7-2820QM Thread(s) per core: 2 Core(s) per socket: 4 Socket(s): 1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1:47&lt;/td&gt; &lt;td align="left"&gt;GeForce RTX 2070S 8GB&lt;/td&gt; &lt;td align="left"&gt;i9-10900K Thread(s) per core: 2 Core(s) per socket: 10 Socket(s): 1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;0:33&lt;/td&gt; &lt;td align="left"&gt;GeForce RTX 3060 12GB&lt;/td&gt; &lt;td align="left"&gt;i7-10700 Thread(s) per core: 2 Core(s) per socket: 8 Socket(s): 1&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Anyone have some recommendations for continued testing of the results in a way that can directly point to the bottlenecks? I am interested in learning not only the bottlenecks in the OS, but also in the design of the Model, so in the future I could understand how to optimize a model for the weaker GPU/CPU and get KPI's that tell me the optimization is working.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shanereaume"&gt; /u/shanereaume &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jj0hoo/ollama_same_question_with_4gb_vs_8gb_vs_12gb_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jj0hoo/ollama_same_question_with_4gb_vs_8gb_vs_12gb_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jj0hoo/ollama_same_question_with_4gb_vs_8gb_vs_12gb_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-24T20:08:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jip27s</id>
    <title>Open-source locally running vibe voice - code with your voice</title>
    <updated>2025-03-24T12:06:31+00:00</updated>
    <author>
      <name>/u/OkRide2660</name>
      <uri>https://old.reddit.com/user/OkRide2660</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using this repo you can setup a locally running whisper model which you can invoke any time using the Ctrl key. Whatever you speak is transcribed and typed into your keyboard as if you typed it yourself, so you can use it anywhere, eg in Cursor or Windsurf to instruct the AI or to type with your voice in a text document. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mpaepper/vibevoice"&gt;https://github.com/mpaepper/vibevoice&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OkRide2660"&gt; /u/OkRide2660 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jip27s/opensource_locally_running_vibe_voice_code_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jip27s/opensource_locally_running_vibe_voice_code_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jip27s/opensource_locally_running_vibe_voice_code_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-24T12:06:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jifav0</id>
    <title>Creating a decentralized AI network to challenge OpenAI's centralized model - Our open-source project Second Me</title>
    <updated>2025-03-24T01:23:16+00:00</updated>
    <author>
      <name>/u/LikeHerstory</name>
      <uri>https://old.reddit.com/user/LikeHerstory</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've just released &lt;a href="https://github.com/Mindverse/Second-Me"&gt;Second Me&lt;/a&gt;, an open-source project that creates a decentralized network of personalized AI entities as an alternative to centralized AI systems.The technology allows individuals to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Build an AI representation of themselves that learns their unique patterns&lt;/li&gt; &lt;li&gt;Deploy this AI to handle tasks autonomously&lt;/li&gt; &lt;li&gt;Connect with other user-created AIs for collaboration and exchange&lt;/li&gt; &lt;li&gt;Maintain authentic privacy through local execution and peer-to-peer communication&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This approach fundamentally differs from the current AI paradigm where a single large model serves millions of users with standardized responses.We believe the future of AI should amplify individual human capabilities rather than homogenize them, and we're making the code available to everyone, feel free to explore!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LikeHerstory"&gt; /u/LikeHerstory &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jifav0/creating_a_decentralized_ai_network_to_challenge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jifav0/creating_a_decentralized_ai_network_to_challenge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jifav0/creating_a_decentralized_ai_network_to_challenge/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-24T01:23:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jiva9a</id>
    <title>OpenArc: OpenVINO benchmarks, six models tested on Arc A770 and CPU-only, 3B-24B</title>
    <updated>2025-03-24T16:45:15+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Note: OpenArc has OpenWebUI support.OpenArc: OpenVINO benchmarks, six models tested on Arc A770 and CPU-only, 3B-24B&lt;/p&gt; &lt;p&gt;OpenArc: OpenVINO benchmarks, six models tested on Arc A770 and CPU-only, 3B-24B&lt;/p&gt; &lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I saw some performance discussion earlier today and decided it was time to weigh in with some OpenVINO benchmarks. Right now &lt;a href="https://github.com/SearchSavior/OpenArc"&gt;OpenArc&lt;/a&gt; doesn't have robust enough performance tracking integrated into the API so I used code &amp;quot;closer&amp;quot; to the OpenVINO Gen AI runtime than the implementation through Transformers; however, &lt;em&gt;performance should be similar&lt;/em&gt;&lt;/p&gt; &lt;p&gt;More benchmarks will follow. This was done ad-hoc; OpenArc will have a robust evaluation suite soon so more benchmarks will follow, including an HF space for sharing &lt;/p&gt; &lt;p&gt;Notes on the test: - No advanced openvino parameters were chosen - I didn't vary input length or anything - Multi-turn scenarios were not evaluated i.e, I ran the basic prompt without follow ups - Quant strategies for models are not considered - I converted each of these models myself (I'm working on standardizing model cards to share this information more directly) - OpenVINO generates a cache on first inference so metrics are on second generation - Seconds were used for readability&lt;/p&gt; &lt;p&gt;&lt;strong&gt;System&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;CPU: Xeon W-2255 (10c, 20t) @3.7ghz GPU: 3x Arc A770 16GB Asrock Phantom RAM: 128gb DDR4 ECC 2933 mhz Disk: 4tb ironwolf, 1tb 970 Evo&lt;/p&gt; &lt;p&gt;Total cost: ~$1700 US (Pretty good!)&lt;/p&gt; &lt;p&gt;OS: Ubuntu 24.04 Kernel: 6.9.4-060904-generic&lt;/p&gt; &lt;p&gt;Prompt: We don't even have a chat template so strap in and let it ride!&lt;/p&gt; &lt;p&gt;GPU: A770 (one was used)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Prompt Processing (sec)&lt;/th&gt; &lt;th&gt;Throughput (t/sec)&lt;/th&gt; &lt;th&gt;Duration (sec)&lt;/th&gt; &lt;th&gt;Size (GB)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Phi-4-mini-instruct-int4_asym-gptq-ov&lt;/td&gt; &lt;td&gt;0.41&lt;/td&gt; &lt;td&gt;47.25&lt;/td&gt; &lt;td&gt;3.10&lt;/td&gt; &lt;td&gt;2.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hermes-3-Llama-3.2-3B-int4_sym-awq-se-ov&lt;/td&gt; &lt;td&gt;0.27&lt;/td&gt; &lt;td&gt;64.18&lt;/td&gt; &lt;td&gt;0.98&lt;/td&gt; &lt;td&gt;1.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama-3.1-Nemotron-Nano-8B-v1-int4_sym-awq-se-ov&lt;/td&gt; &lt;td&gt;0.32&lt;/td&gt; &lt;td&gt;47.99&lt;/td&gt; &lt;td&gt;2.96&lt;/td&gt; &lt;td&gt;4.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi-4-int4_asym-awq-se-ov&lt;/td&gt; &lt;td&gt;0.30&lt;/td&gt; &lt;td&gt;25.27&lt;/td&gt; &lt;td&gt;5.32&lt;/td&gt; &lt;td&gt;8.1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek-R1-Distill-Qwen-14B-int4_sym-awq-se-ov&lt;/td&gt; &lt;td&gt;0.42&lt;/td&gt; &lt;td&gt;25.23&lt;/td&gt; &lt;td&gt;1.56&lt;/td&gt; &lt;td&gt;8.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral-Small-24B-Instruct-2501-int4_asym-ov&lt;/td&gt; &lt;td&gt;0.36&lt;/td&gt; &lt;td&gt;18.81&lt;/td&gt; &lt;td&gt;7.11&lt;/td&gt; &lt;td&gt;12.9&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;CPU: Xeon W-2255&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Prompt Processing (sec)&lt;/th&gt; &lt;th&gt;Throughput (t/sec)&lt;/th&gt; &lt;th&gt;Duration (sec)&lt;/th&gt; &lt;th&gt;Size (GB)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Phi-4-mini-instruct-int4_asym-gptq-ov&lt;/td&gt; &lt;td&gt;1.02&lt;/td&gt; &lt;td&gt;20.44&lt;/td&gt; &lt;td&gt;7.23&lt;/td&gt; &lt;td&gt;2.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hermes-3-Llama-3.2-3B-int4_sym-awq-se-ov&lt;/td&gt; &lt;td&gt;1.06&lt;/td&gt; &lt;td&gt;23.66&lt;/td&gt; &lt;td&gt;3.01&lt;/td&gt; &lt;td&gt;1.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama-3.1-Nemotron-Nano-8B-v1-int4_sym-awq-se-ov&lt;/td&gt; &lt;td&gt;2.53&lt;/td&gt; &lt;td&gt;13.22&lt;/td&gt; &lt;td&gt;12.14&lt;/td&gt; &lt;td&gt;4.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi-4-int4_asym-awq-se-ov&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;6.63&lt;/td&gt; &lt;td&gt;23.14&lt;/td&gt; &lt;td&gt;8.1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek-R1-Distill-Qwen-14B-int4_sym-awq-se-ov&lt;/td&gt; &lt;td&gt;5.02&lt;/td&gt; &lt;td&gt;7.25&lt;/td&gt; &lt;td&gt;11.09&lt;/td&gt; &lt;td&gt;8.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral-Small-24B-Instruct-2501-int4_asym-ov&lt;/td&gt; &lt;td&gt;6.88&lt;/td&gt; &lt;td&gt;4.11&lt;/td&gt; &lt;td&gt;37.5&lt;/td&gt; &lt;td&gt;12.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Nous-Hermes-2-Mixtral-8x7B-DPO-int4-sym-se-ov&lt;/td&gt; &lt;td&gt;15.56&lt;/td&gt; &lt;td&gt;6.67&lt;/td&gt; &lt;td&gt;34.60&lt;/td&gt; &lt;td&gt;24.2&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Analysis&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt processing on CPU and GPU are absolutely insane. We need more benchmarks though to compare... anecdotally it &lt;em&gt;shreds&lt;/em&gt; llama.cpp &lt;/li&gt; &lt;li&gt;Throughput is fantastic for models under 8B on CPU. Results will vary across devices but smaller models have absolutely phenomenal usability at scale&lt;/li&gt; &lt;li&gt;These results are early tests but I am confident this &lt;strong&gt;proves the value of Intel technology for inference. IF you are on a budget, already have Intel tech, using serverless or whatever, send it and send it hard.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;You can expect &lt;em&gt;better performance&lt;/em&gt; by tinkering with OpenVINO optimizations on CPU and GPU. These are available in the OpenArc dashboard and were excluded from this test purposefully. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For now OpenArc does not support benchmarking as part of it's API. Instead, use test &lt;a href="https://github.com/SearchSavior/OpenArc/blob/main/scripts/benchmark/ov_simple_bench.py"&gt;scripts in the repo&lt;/a&gt; to replicate these results. For this, use the OpenArc conda environment.&lt;/p&gt; &lt;p&gt;What do you guys think? What kinds of eval speed/throughput are you seeing with other frameworks for Intel CPU/GPU?&lt;/p&gt; &lt;p&gt;Join the offical &lt;a href="https://discord.gg/Bzz9hax9Jq"&gt;Discord&lt;/a&gt;!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jiva9a/openarc_openvino_benchmarks_six_models_tested_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jiva9a/openarc_openvino_benchmarks_six_models_tested_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jiva9a/openarc_openvino_benchmarks_six_models_tested_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-24T16:45:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jj1ukn</id>
    <title>ObserverAI demo video!</title>
    <updated>2025-03-24T21:01:45+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jj1ukn/observerai_demo_video/"&gt; &lt;img alt="ObserverAI demo video!" src="https://external-preview.redd.it/c2Z6czE5YWVicHFlMSzX04uPkutVqaVB3RNDmFdJfygXornQmolDDmaeMUuj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12c6e8b4c5e7f19d09d212fdcf6218c522b1f3d4" title="ObserverAI demo video!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey ollama community!&lt;/p&gt; &lt;p&gt;This is a better demo video than the one I uploaded a few days ago, it shows the flow of the application better!&lt;/p&gt; &lt;p&gt;The Observer AI agents can:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Observe your screen (via OCR or screenshots with vision models)&lt;/li&gt; &lt;li&gt;Process what they see with LLMs running locally through Ollama&lt;/li&gt; &lt;li&gt;Execute JS in the browser or Python code to perform actions on your system!!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Looking for feedback:&lt;br /&gt; I'd love your thoughts on:&lt;br /&gt; * What kinds of agents would you build with Python execution capabilities?&lt;br /&gt; Examples:&lt;br /&gt; - Stock buying bot (would be very bad at it's job hahaha)&lt;br /&gt; - Dashboard watching agent with custom hooks to react to information&lt;br /&gt; - Process registration agent, (would describe step by step a process you do on your computer)(I can help you through discord or dm's)&lt;br /&gt; * Feature requests or improvements to the UX?&lt;/p&gt; &lt;p&gt;Observer AI remains 100% open source and local-first - try it at &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com&lt;/a&gt; or check out the code at &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;br /&gt; Thanks for all the support and feedback so far!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/allep4aebpqe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jj1ukn/observerai_demo_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jj1ukn/observerai_demo_video/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-24T21:01:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jisfbt</id>
    <title>Just Built an Interactive AI-Powered CrewAI Documentation Assistant with Langchain and Ollama</title>
    <updated>2025-03-24T14:48:48+00:00</updated>
    <author>
      <name>/u/Maleficent-Penalty50</name>
      <uri>https://old.reddit.com/user/Maleficent-Penalty50</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jisfbt/just_built_an_interactive_aipowered_crewai/"&gt; &lt;img alt="Just Built an Interactive AI-Powered CrewAI Documentation Assistant with Langchain and Ollama" src="https://external-preview.redd.it/eXNyMnNyY2xobnFlMZe9ybkIgIdvv6PPmgWnShlnht23JofJEdJa4TdKHoaX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0fec3b3117d586dead26223ecc60881554fe0bf" title="Just Built an Interactive AI-Powered CrewAI Documentation Assistant with Langchain and Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent-Penalty50"&gt; /u/Maleficent-Penalty50 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/je2heyclhnqe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jisfbt/just_built_an_interactive_aipowered_crewai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jisfbt/just_built_an_interactive_aipowered_crewai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-24T14:48:48+00:00</published>
  </entry>
</feed>
