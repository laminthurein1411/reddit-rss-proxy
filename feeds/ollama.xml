<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-06-18T07:24:36+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1lbvd02</id>
    <title>ollama's 8b is only 5gb while hugging face is near 16gb, is it quantized?, if yes how to use the full unquantized llama 8b?</title>
    <updated>2025-06-15T08:23:56+00:00</updated>
    <author>
      <name>/u/Beyond_Birthday_13</name>
      <uri>https://old.reddit.com/user/Beyond_Birthday_13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lbvd02/ollamas_8b_is_only_5gb_while_hugging_face_is_near/"&gt; &lt;img alt="ollama's 8b is only 5gb while hugging face is near 16gb, is it quantized?, if yes how to use the full unquantized llama 8b?" src="https://external-preview.redd.it/wZte_idxzrfltl70eEbLpGXYidrhFHY38JLhA3vpbNc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abc79b312a92dc4f6f679bb710a084d0512a3c12" title="ollama's 8b is only 5gb while hugging face is near 16gb, is it quantized?, if yes how to use the full unquantized llama 8b?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beyond_Birthday_13"&gt; /u/Beyond_Birthday_13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lbvd02"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lbvd02/ollamas_8b_is_only_5gb_while_hugging_face_is_near/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lbvd02/ollamas_8b_is_only_5gb_while_hugging_face_is_near/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-15T08:23:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lc5fmy</id>
    <title>Not Allowed</title>
    <updated>2025-06-15T17:09:46+00:00</updated>
    <author>
      <name>/u/marketlurker</name>
      <uri>https://old.reddit.com/user/marketlurker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When my application tries to access the API endpoint &amp;quot;&lt;a href="http://localhost:11434/api/generate"&gt;localhost:11434/api/generate&lt;/a&gt;&amp;quot; I get an error, &amp;quot;405 method not allowed&amp;quot; error. Obviously, something is not quite right. Anyone have an idea what I am missing? I am running ollama in a docker container with the port exposed.&lt;/p&gt; &lt;p&gt;For those familiar with it, I am trying to run the python app marker-pdf. I am passing&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--ollama_base_url &amp;quot;http://localhost:11434&amp;quot; --ollama_model=&amp;quot;llama3.2&amp;quot; --llm_service=marker.services.ollama.OllamaService &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;per the instructions &lt;a href="https://pypi.org/project/marker-pdf/"&gt;here&lt;/a&gt;. I am running ollama 0.9.0.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marketlurker"&gt; /u/marketlurker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lc5fmy/not_allowed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lc5fmy/not_allowed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lc5fmy/not_allowed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-15T17:09:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lc7ej5</id>
    <title>changeish - manage your code's changelog using ollama</title>
    <updated>2025-06-15T18:31:18+00:00</updated>
    <author>
      <name>/u/Kitchen_Fix1464</name>
      <uri>https://old.reddit.com/user/Kitchen_Fix1464</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was working on a large application and struggling to keep up with the change log updates. So, I created this script that will update the change log file by generating a git history and prompt to feed to ollama. It appends the output to the top of the changelog file. The script is written in bash to reduce dependency/package management. It only requires git and Ollama. You can skip the generation step if Ollama is not available and it will return a prompt.md file that can be used with other LLM interfaces.&lt;/p&gt; &lt;p&gt;This is still VERY rough and makes some assumptions that need to he customizable. With that said, I wanted to post what I have so far and see if there is any interest in a tool like this. If so, I will spend some time making it more flexible and documenting the default workflow assumptions.&lt;/p&gt; &lt;p&gt;Any feedback is welcomed. Also happy to have PRs for missing features, fixes, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kitchen_Fix1464"&gt; /u/Kitchen_Fix1464 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/itlackey/changeish/tree/main"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lc7ej5/changeish_manage_your_codes_changelog_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lc7ej5/changeish_manage_your_codes_changelog_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-15T18:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcs53a</id>
    <title>what is the biggest LLM i can use with a Arda 4000 20gb vram</title>
    <updated>2025-06-16T12:59:06+00:00</updated>
    <author>
      <name>/u/Better-Barnacle-1990</name>
      <uri>https://old.reddit.com/user/Better-Barnacle-1990</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, which LL-Model is the biggest i can stil use on my Arda 4000 20gb VRAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Better-Barnacle-1990"&gt; /u/Better-Barnacle-1990 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcs53a/what_is_the_biggest_llm_i_can_use_with_a_arda/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcs53a/what_is_the_biggest_llm_i_can_use_with_a_arda/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lcs53a/what_is_the_biggest_llm_i_can_use_with_a_arda/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T12:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcf4tf</id>
    <title>System specs for ollama on proxmox</title>
    <updated>2025-06-16T00:16:57+00:00</updated>
    <author>
      <name>/u/CombatRaccoons</name>
      <uri>https://old.reddit.com/user/CombatRaccoons</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So i have a fresh pc build.&lt;/p&gt; &lt;p&gt;Intrel i7 20 core 14700k. 192 gb ddr5 ram 2x rtx 5060ti 16gb vram (total 32gb) 4 tb HDD Asus z790 motherboard 1x 10gb nic&lt;/p&gt; &lt;p&gt;Looking to build an ollama (or alternative) LLM server for application API and function calling. I would like to run a VMs within proxmox to include a ubuntu server vm with ollama (or alternative).&lt;/p&gt; &lt;p&gt;Is this sufficient? What are the recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombatRaccoons"&gt; /u/CombatRaccoons &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcf4tf/system_specs_for_ollama_on_proxmox/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcf4tf/system_specs_for_ollama_on_proxmox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lcf4tf/system_specs_for_ollama_on_proxmox/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T00:16:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcu3q8</id>
    <title>How to install Docker on Windows?</title>
    <updated>2025-06-16T14:21:59+00:00</updated>
    <author>
      <name>/u/Ok_Most9659</name>
      <uri>https://old.reddit.com/user/Ok_Most9659</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Struggling to find a clear and concise guide to installing Docker on Windows. Also, some say you must register a Docker account to use even for personal use on Windows, is this correct?&lt;br /&gt; Can any one link a clear concise installation guide for Docker on Windows?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Most9659"&gt; /u/Ok_Most9659 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcu3q8/how_to_install_docker_on_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcu3q8/how_to_install_docker_on_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lcu3q8/how_to_install_docker_on_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T14:21:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbt4zg</id>
    <title>iDoNotHaveThatMuchRam</title>
    <updated>2025-06-15T05:55:27+00:00</updated>
    <author>
      <name>/u/Virtual4P</name>
      <uri>https://old.reddit.com/user/Virtual4P</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lbt4zg/idonothavethatmuchram/"&gt; &lt;img alt="iDoNotHaveThatMuchRam" src="https://preview.redd.it/f5q7ded8ew6f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=395856e68d91cf6ad7f9706067350bde131156f3" title="iDoNotHaveThatMuchRam" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Virtual4P"&gt; /u/Virtual4P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f5q7ded8ew6f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lbt4zg/idonothavethatmuchram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lbt4zg/idonothavethatmuchram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-15T05:55:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcwjea</id>
    <title>What TUI interfaces for Ollama do you know?</title>
    <updated>2025-06-16T15:55:55+00:00</updated>
    <author>
      <name>/u/q-admin007</name>
      <uri>https://old.reddit.com/user/q-admin007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for something i can install on all my Linux servers that then connects to the ollama server.&lt;/p&gt; &lt;p&gt;I want to be able to pick a model and maybe have a history of previous chats. Maybe rerun a prompt with another model would be nice, but optional.&lt;/p&gt; &lt;p&gt;Anything that comes to mind?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/q-admin007"&gt; /u/q-admin007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcwjea/what_tui_interfaces_for_ollama_do_you_know/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcwjea/what_tui_interfaces_for_ollama_do_you_know/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lcwjea/what_tui_interfaces_for_ollama_do_you_know/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T15:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lczeww</id>
    <title>Local Open Source VScode Copilot model with MCP</title>
    <updated>2025-06-16T17:43:24+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1lcud8j/local_open_source_vscode_copilot_model_with_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lczeww/local_open_source_vscode_copilot_model_with_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lczeww/local_open_source_vscode_copilot_model_with_mcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T17:43:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lctg3s</id>
    <title>Looking for recommendations for a GPU</title>
    <updated>2025-06-16T13:55:35+00:00</updated>
    <author>
      <name>/u/Limitless83</name>
      <uri>https://old.reddit.com/user/Limitless83</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Right now I'm running some smaller LLMs on my CPU (intel i5 11500, 64GB DDR4) on my server. But i would like to run/experiment with some larger ones.&lt;br /&gt; EDIT: i'm running Ollama and Open WebUI in a docker on Debian 12 &lt;/p&gt; &lt;p&gt;I'm looking to buy a new GPU for either my server or my gaming PC.&lt;br /&gt; My gaming PC has a NVIDIA 4070 (non TI, 12GB VRAM).&lt;br /&gt; Budget wise I'm looking at either AMD RX 7600 XT, AMD RX 9060 XT or a NVIDIA RTX 5060 TI. (between 360‚Ç¨ - 480‚Ç¨)&lt;br /&gt; So the question is: which one of these 3 cards is the best for AI or to upgrade my PC so the 4070 goes into the server. Or is there a card that I'm overlooking in the same price range?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Limitless83"&gt; /u/Limitless83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lctg3s/looking_for_recommendations_for_a_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lctg3s/looking_for_recommendations_for_a_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lctg3s/looking_for_recommendations_for_a_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T13:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcxkrw</id>
    <title>Ollama: Powering Privacy-Focused AI for My WhatsApp Chat Mimicry!</title>
    <updated>2025-06-16T16:35:16+00:00</updated>
    <author>
      <name>/u/Basic_Regular_3100</name>
      <uri>https://old.reddit.com/user/Basic_Regular_3100</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I'm excited to share how &lt;strong&gt;Ollama&lt;/strong&gt; has been instrumental in my &lt;strong&gt;Chat Mimicry AI&lt;/strong&gt; project. This tool allows users to use WhatsApp chats history file, and then an AI mimics the personalities within. It's a powerful example of what local LLMs can achieve!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ollama was indispensable during development.&lt;/strong&gt; Its simplicity for running models locally allowed for rapid iteration and testing.&lt;/p&gt; &lt;p&gt;A key advantage of &lt;strong&gt;Ollama&lt;/strong&gt; is its role in &lt;strong&gt;data privacy for the local version of my project&lt;/strong&gt;. When users run the AI locally with Ollama, their chat data never leaves their device, which builds immense &lt;strong&gt;user trust&lt;/strong&gt;. While I currently have a hosted version online, my strong preference is to eventually &lt;strong&gt;self-host the AI for potential unlimited usage&lt;/strong&gt;, and to explore how Ollama can best support that while maintaining as much privacy as possible.&lt;/p&gt; &lt;p&gt;btw: You can explore hosted version and the &lt;strong&gt;Ollama-powered local version&lt;/strong&gt; &lt;a href="https://bitwattr.pages.dev/projects/chat-mimicry-ai"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ollama is truly democratizing AI and enabling new possibilities for user control and data handling. What are your thoughts on building private AI experiences with Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Basic_Regular_3100"&gt; /u/Basic_Regular_3100 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcxkrw/ollama_powering_privacyfocused_ai_for_my_whatsapp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcxkrw/ollama_powering_privacyfocused_ai_for_my_whatsapp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lcxkrw/ollama_powering_privacyfocused_ai_for_my_whatsapp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T16:35:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcy7qr</id>
    <title>MCP llm tool calls are sky-rocketing my token usage - travel agency example</title>
    <updated>2025-06-16T16:59:32+00:00</updated>
    <author>
      <name>/u/benxben13</name>
      <uri>https://old.reddit.com/user/benxben13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wish to know if im doing something wrong or maybe missing the obvious when building pipelines with mcp llm tool calls. &lt;/p&gt; &lt;p&gt;so I've built a basic pipeline (&lt;a href="https://github.com/benx13/basic-travel-agency"&gt;GitHub repo&lt;/a&gt;) for an llm travel agency to compare: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;classical tool calling&lt;/strong&gt;: fixed pipeline where we are asking the llm to generate the parameters of some function and manually call it&lt;/li&gt; &lt;li&gt;&lt;strong&gt;mcp llm tool calling&lt;/strong&gt;: dynamic loop where the llm decides sequentially which function to call &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I found out a couple interesting things about mcp tool calls: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;at some point the llm will decide to generate a &lt;strong&gt;tool_usage_token&lt;/strong&gt; for example search_hotels_token when it decides to look up hotels&lt;/li&gt; &lt;li&gt;the engine will cancel the request execute the tool and append its output to the prompt and makes a new llm call and keeps doing that for every tool call&lt;/li&gt; &lt;li&gt;&lt;strong&gt;by calling multiple tools it means that we are going to make multiple request in which the input prompt will probably be cached but the amount of tokens will pile-up, even at a 50% discount the input tokens are only increasing exponentially because basically you will be calling the same request multiple times. especially if a tool returns a big output eg: top-20 hotels so you will call those same 20 hotels for each request you make (number of tools used).&lt;/strong&gt; &lt;/li&gt; &lt;li&gt;you can't run multiple tools in async mode for example search tools because the llm can't generate multiple tool usage stop tokens at the same time (im not sure about this) but you will probably end up doing a routing tool and run your tools manually &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;as a result of the points above I checked my openrouter usage and found a significant difference for this basic travel agency example (using 4 sonnet):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;mcp approach&lt;/strong&gt; used: &lt;ul&gt; &lt;li&gt;total input tokens: &lt;strong&gt;3415&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;total output tokens: &lt;strong&gt;1491&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Total cost: &lt;strong&gt;0.02848$&lt;/strong&gt; (and it failed at the end)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Manuel approach&lt;/strong&gt; used: &lt;ul&gt; &lt;li&gt;total input tokens: &lt;strong&gt;381&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;total output tokens: &lt;strong&gt;175&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Total cost: &lt;strong&gt;0.00201$&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I understand the benefits of having a dynamic conversation using mcp tool calls methodology but &lt;strong&gt;is it worth the extra tokens?&lt;/strong&gt; as it would be cool if you actually can pause the request instead of canceling and launching a new one but that's impossible due to infrastructure purposes. &lt;/p&gt; &lt;p&gt;below is link to the comparison GitHub repo let me know guys if I'm missing something obvious.&lt;br /&gt; &lt;a href="https://github.com/benx13/basic-travel-agency"&gt;https://github.com/benx13/basic-travel-agency&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/benxben13"&gt; /u/benxben13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcy7qr/mcp_llm_tool_calls_are_skyrocketing_my_token/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcy7qr/mcp_llm_tool_calls_are_skyrocketing_my_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lcy7qr/mcp_llm_tool_calls_are_skyrocketing_my_token/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T16:59:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld9tvd</id>
    <title>[Update] Serene Pub v0.2.0-alpha - Added group chats, LM Studio, OpenAI support and more</title>
    <updated>2025-06-17T00:49:31+00:00</updated>
    <author>
      <name>/u/doolijb</name>
      <uri>https://old.reddit.com/user/doolijb</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doolijb"&gt; /u/doolijb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1ld8phi/update_serene_pub_v020alpha_added_group_chats_lm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ld9tvd/update_serene_pub_v020alpha_added_group_chats_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ld9tvd/update_serene_pub_v020alpha_added_group_chats_lm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T00:49:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcyhxk</id>
    <title>Alternatives to Apple Studio, preferably mini-pcs</title>
    <updated>2025-06-16T17:09:37+00:00</updated>
    <author>
      <name>/u/TwitchTv_SosaJacobb</name>
      <uri>https://old.reddit.com/user/TwitchTv_SosaJacobb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been wanting to run LLM locally by using external hardware with linux os. and I often saw that people here recommend Apple Studio. &lt;/p&gt; &lt;p&gt;However are there other alternatives? I've been thinking about BeeLink or Dell Thin mini-pcs.&lt;/p&gt; &lt;p&gt;My goal was to run 7b, 14b or maybe even 32b deepseek or other models efficiently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TwitchTv_SosaJacobb"&gt; /u/TwitchTv_SosaJacobb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcyhxk/alternatives_to_apple_studio_preferably_minipcs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcyhxk/alternatives_to_apple_studio_preferably_minipcs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lcyhxk/alternatives_to_apple_studio_preferably_minipcs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T17:09:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldgux6</id>
    <title>Expose ollama internally with https</title>
    <updated>2025-06-17T07:21:19+00:00</updated>
    <author>
      <name>/u/oturais</name>
      <uri>https://old.reddit.com/user/oturais</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. &lt;/p&gt; &lt;p&gt;I have an application that consumes openai api but only allows https endpoints. &lt;/p&gt; &lt;p&gt;Is there any easy way to configure ollama to expose the api on https?&lt;/p&gt; &lt;p&gt;I've seen some posts about creating a reverse pricy with nginx, but I'm struggling with that. Any other approach? &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oturais"&gt; /u/oturais &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldgux6/expose_ollama_internally_with_https/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldgux6/expose_ollama_internally_with_https/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ldgux6/expose_ollama_internally_with_https/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T07:21:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldon8y</id>
    <title>UI and tools for multiuser RAG with central knowledge base</title>
    <updated>2025-06-17T14:30:44+00:00</updated>
    <author>
      <name>/u/TommyWolfheart</name>
      <uri>https://old.reddit.com/user/TommyWolfheart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi.&lt;/p&gt; &lt;p&gt;I am developing an LLM system for an organisation's documentation with Ollama and would like, when everyone in the organisation chats with the system, for it to do RAG with a central/global knowledge base.&lt;/p&gt; &lt;p&gt;Open WebUl‚Äôs documentation on RAG seems to suggest that an individual has to upload their own documents to do RAG with them.&lt;/p&gt; &lt;p&gt;I would appreciate guidance on what UI to use to achieve what I want to do. I‚Äôm very happy to use LangChain but not sure how I would go about integrating the resulting system with Open WebUI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TommyWolfheart"&gt; /u/TommyWolfheart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldon8y/ui_and_tools_for_multiuser_rag_with_central/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldon8y/ui_and_tools_for_multiuser_rag_with_central/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ldon8y/ui_and_tools_for_multiuser_rag_with_central/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T14:30:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld1bj1</id>
    <title>I made a macos MCP client</title>
    <updated>2025-06-16T18:53:55+00:00</updated>
    <author>
      <name>/u/SandwichConscious336</name>
      <uri>https://old.reddit.com/user/SandwichConscious336</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ld1bj1/i_made_a_macos_mcp_client/"&gt; &lt;img alt="I made a macos MCP client" src="https://preview.redd.it/q0m3hohr5c7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94ec86313f925cd3386804c2445f1b9f706a53c3" title="I made a macos MCP client" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am working on adding MCP support for my native macos Ollama client app. I am looking for people currently using Ollama locally (with a client or not) who are curious about MCP and would like a way to easy use MCP servers (local and remote).&lt;/p&gt; &lt;p&gt;Reply and DM me if you're interested in testing my MCP integration.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SandwichConscious336"&gt; /u/SandwichConscious336 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q0m3hohr5c7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ld1bj1/i_made_a_macos_mcp_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ld1bj1/i_made_a_macos_mcp_client/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T18:53:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1le1wur</id>
    <title>Iphone app</title>
    <updated>2025-06-17T23:14:22+00:00</updated>
    <author>
      <name>/u/Zealousideal_Neck317</name>
      <uri>https://old.reddit.com/user/Zealousideal_Neck317</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, i just downloaded the app and i need help First i will tell you why i want to use this ai. From my understanding these types of bots, feel free to correct me (just please do it nicely) are better for uncensored, unfiltered chat. What i want to use it for is RP. I like to chat with ai bots to creat a story, and naturally stories get to a NSFW point, sexual or violent. The bots i am currently usually (idk if i can say the name) has bee insane with the guidlimes as it calls it. Like it won‚Äôt do a simple scene of teasing! So please help me and tell me if this is a better option &lt;/p&gt; &lt;p&gt;And to my important question I opened the app and it showed me that i needed to choose a server. From your knowledge which would be best for my case, knowing what i use it for and that it is on the app not a pc&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal_Neck317"&gt; /u/Zealousideal_Neck317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1le1wur/iphone_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1le1wur/iphone_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1le1wur/iphone_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T23:14:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldts6f</id>
    <title>My AI Interview Prep Side Project Now Has an "AI Coach" to Pinpoint Your Weak Skills!</title>
    <updated>2025-06-17T17:47:35+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ldts6f/my_ai_interview_prep_side_project_now_has_an_ai/"&gt; &lt;img alt="My AI Interview Prep Side Project Now Has an &amp;quot;AI Coach&amp;quot; to Pinpoint Your Weak Skills!" src="https://external-preview.redd.it/cXJkNHN3cnl5aTdmMUx0vRJ7fJWQFnAwyVuCziX8lbU-1zIfwzcA4VGtmYcd.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9ea99d70802d659868b41360d51236101f26349" title="My AI Interview Prep Side Project Now Has an &amp;quot;AI Coach&amp;quot; to Pinpoint Your Weak Skills!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Been working hard on my personal project, an AI-powered interview preparer, and just rolled out a new core feature I'm pretty excited about: the &lt;strong&gt;AI Coach&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;The main idea is to go beyond just giving you mock interview questions. After you do a practice interview in the app, this new AI Coach (which uses &lt;strong&gt;Agno agents&lt;/strong&gt; to orchestrate a local LLM like Llama/Mistral via Ollama) actually analyzes your answers to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tell you which skills you demonstrated well.&lt;/li&gt; &lt;li&gt;More importantly, &lt;strong&gt;pinpoint specific skills where you might need more work.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;It even gives you an overall score and a breakdown by criteria like accuracy, clarity, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Plus, you're not just limited to feedback after an interview. You can also &lt;strong&gt;tell the AI Coach which specific skills you want to learn or improve on&lt;/strong&gt;, and it can offer guidance or track your focus there.&lt;/p&gt; &lt;p&gt;The frontend for displaying all this feedback is built with &lt;strong&gt;React and TypeScript&lt;/strong&gt; (loving TypeScript for managing the data structures here!).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech Stack for this feature &amp;amp; the broader app:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI Coach Logic:&lt;/strong&gt; Agno agents, local LLMs (Ollama)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; Python, FastAPI, SQLAlchemy&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; React, TypeScript, Zustand, Framer Motion&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This has been a super fun challenge, especially the prompt engineering to get nuanced skill-based feedback from the LLMs and making sure the Agno agents handle the analysis flow correctly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I built this because I always wished I had more targeted feedback after practice interviews ‚Äì not just &amp;quot;good job&amp;quot; but &amp;quot;you need to work on X skill specifically.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What do you guys think?&lt;/li&gt; &lt;li&gt;What kind of skill-based feedback would be most useful to you from an AI coach?&lt;/li&gt; &lt;li&gt;Anyone else playing around with Agno agents or local LLMs for complex analysis tasks?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear your thoughts, suggestions, or if you're working on something similar!&lt;/p&gt; &lt;p&gt;You can check out my previous post about the main app here: &lt;a href="https://www.reddit.com/r/ollama/comments/1ku0b3j/im_building_an_ai_interview_prep_tool_to_get_real/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;https://www.reddit.com/r/ollama/comments/1ku0b3j/im_building_an_ai_interview_prep_tool_to_get_real/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üöÄ P.S. I am looking for new roles , If you like my work and have any Opportunites in Computer Vision or LLM Domain do contact me&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;My Email:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=mailto%3Apavankunchalaofficial%40gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My GitHub Profile (for more projects):&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My Resume:&lt;/strong&gt; &lt;a href="https://drive.google.com/file/d/1LVMVgAPKGUJbnrfE09OLJ0MrEZlBccOT/view"&gt;https://drive.google.com/file/d/1LVMVgAPKGUJbnrfE09OLJ0MrEZlBccOT/view&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dv6mwgtyyi7f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldts6f/my_ai_interview_prep_side_project_now_has_an_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ldts6f/my_ai_interview_prep_side_project_now_has_an_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T17:47:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldu0p0</id>
    <title>Trying to connect Ollama with WhatsApp using Node.js but no response ‚Äî Where is the clear documentation?</title>
    <updated>2025-06-17T17:56:37+00:00</updated>
    <author>
      <name>/u/Oz_Ar4L</name>
      <uri>https://old.reddit.com/user/Oz_Ar4L</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am completely new to this and have no formal programming experience, but I am trying a simple personal project:&lt;br /&gt; I want a bot to read messages coming through WhatsApp (using whatsapp-web.js) and respond using a local Ollama model that I have customized (called &amp;quot;Nergal&amp;quot;).&lt;/p&gt; &lt;p&gt;The WhatsApp part already works. The bot responds to simple commands like &amp;quot;Hi Nergal&amp;quot; and &amp;quot;Bye Nergal.&amp;quot;&lt;br /&gt; What I can‚Äôt get to work is connecting to Ollama so it responds based on the user‚Äôs message.&lt;/p&gt; &lt;p&gt;I have been searching for days but can‚Äôt find clear and straightforward documentation on how to integrate Ollama into a Node.js bot.&lt;/p&gt; &lt;p&gt;Does anyone have a working example or know where I can read documentation that explains how to do it?&lt;/p&gt; &lt;p&gt;I really appreciate any guidance. üôè&lt;/p&gt; &lt;pre&gt;&lt;code&gt;const qrcode = require('qrcode-terminal'); const { Client, LocalAuth } = require('whatsapp-web.js'); const ollama = require('ollama') const client = new Client({ authStrategy: new LocalAuth() }); client.on('qr', qr =&amp;gt; { qrcode.generate(qr, {small: true}); }); client.on('ready', () =&amp;gt; { console.log('Nergal is Awake!'); }); client.on('message_create', message =&amp;gt; { if (message.body === 'Hi N') { // send back &amp;quot;pong&amp;quot; to the chat the message was sent in client.sendMessage(message.from, 'Hello User'); } if (message.body === 'Bye N') { // send back &amp;quot;pong&amp;quot; to the chat the message was sent in client.sendMessage(message.from, 'Bye User'); } if (message.body.toLowerCase().includes('Nergal')) { async function generarTexto() { const response = await ollama.chat({ model: 'Nergal', messages: [{ role: 'user', content: 'What is Nergal?' }] }) console.log(response.message.content) } generarTexto() } }); client.initialize(); &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Oz_Ar4L"&gt; /u/Oz_Ar4L &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldu0p0/trying_to_connect_ollama_with_whatsapp_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldu0p0/trying_to_connect_ollama_with_whatsapp_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ldu0p0/trying_to_connect_ollama_with_whatsapp_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T17:56:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldva49</id>
    <title>Blog: You Can‚Äôt Have an AI Strategy Without a Data Strategy</title>
    <updated>2025-06-17T18:44:47+00:00</updated>
    <author>
      <name>/u/UnderstandingTop1424</name>
      <uri>https://old.reddit.com/user/UnderstandingTop1424</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for feedback for the blog -- &lt;a href="https://quarklabs.substack.com/p/you-cant-have-an-ai-strategy-without"&gt;https://quarklabs.substack.com/p/you-cant-have-an-ai-strategy-without&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UnderstandingTop1424"&gt; /u/UnderstandingTop1424 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldva49/blog_you_cant_have_an_ai_strategy_without_a_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldva49/blog_you_cant_have_an_ai_strategy_without_a_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ldva49/blog_you_cant_have_an_ai_strategy_without_a_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T18:44:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldse04</id>
    <title>Help with Llama (fairly new to this sorry)</title>
    <updated>2025-06-17T16:55:03+00:00</updated>
    <author>
      <name>/u/AdventurousReturn316</name>
      <uri>https://old.reddit.com/user/AdventurousReturn316</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can I run LLaMA 3 8B Q4 locally using Ollama or a similar tool. My laptop is a 2019 Lenovo with Windows 11 (64-bit), an Intel i5-9300H (4 cores, 8 threads), 16 GB DDR4 RAM, and an NVIDIA GTX 1650 (4GB VRAM). I‚Äôve got a 256 GB SSD and a 1 TB HDD. Virtualization is enabled, GPU idles at ~45¬∞C, and CPU usage sits around 8‚Äì10% when idle.&lt;/p&gt; &lt;p&gt;Can I run LLaMA 3 8B Q4 on this setup reliably? Is 16GB Ram good enough? Thank you in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdventurousReturn316"&gt; /u/AdventurousReturn316 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldse04/help_with_llama_fairly_new_to_this_sorry/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldse04/help_with_llama_fairly_new_to_this_sorry/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ldse04/help_with_llama_fairly_new_to_this_sorry/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T16:55:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldupri</id>
    <title>üöÄ I built a lightweight web UI for Ollama ‚Äì great for local LLMs!</title>
    <updated>2025-06-17T18:22:37+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ldupri/i_built_a_lightweight_web_ui_for_ollama_great_for/"&gt; &lt;img alt="üöÄ I built a lightweight web UI for Ollama ‚Äì great for local LLMs!" src="https://external-preview.redd.it/1DSkBtE8nncEw-aEby0cNCk5U9TnOLRhDdejd1tL2ak.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16e59db3ddf1a21ab36d3f484534eedc264f0b8a" title="üöÄ I built a lightweight web UI for Ollama ‚Äì great for local LLMs!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1ldupay/i_built_a_lightweight_web_ui_for_ollama_great_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldupri/i_built_a_lightweight_web_ui_for_ollama_great_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ldupri/i_built_a_lightweight_web_ui_for_ollama_great_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T18:22:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldkx9x</id>
    <title>40 GPU Cluster Concurrency Test</title>
    <updated>2025-06-17T11:42:58+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ldkx9x/40_gpu_cluster_concurrency_test/"&gt; &lt;img alt="40 GPU Cluster Concurrency Test" src="https://external-preview.redd.it/5oL5EdtwMwlXnOo15ZpuIiB4WAkPdKcGlE7lZy0SHHk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e578354f37d691c35448462a42256a439ce14a9" title="40 GPU Cluster Concurrency Test" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/aq4y2p9e5h7f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldkx9x/40_gpu_cluster_concurrency_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ldkx9x/40_gpu_cluster_concurrency_test/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T11:42:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldsl00</id>
    <title>Sadly the truth</title>
    <updated>2025-06-17T17:02:05+00:00</updated>
    <author>
      <name>/u/Virtual4P</name>
      <uri>https://old.reddit.com/user/Virtual4P</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ldsl00/sadly_the_truth/"&gt; &lt;img alt="Sadly the truth" src="https://preview.redd.it/551hwa6yfi7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=360466aeae57613ec984fa66508c12bfca7f9e4e" title="Sadly the truth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Virtual4P"&gt; /u/Virtual4P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/551hwa6yfi7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldsl00/sadly_the_truth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ldsl00/sadly_the_truth/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T17:02:05+00:00</published>
  </entry>
</feed>
