<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-11T08:50:14+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1j7iso1</id>
    <title>I can't make a rag system with fastapi</title>
    <updated>2025-03-09T21:54:57+00:00</updated>
    <author>
      <name>/u/Ok_Impact4403</name>
      <uri>https://old.reddit.com/user/Ok_Impact4403</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to make a small project but i can't make the rag system, I had one made with python for the console, but for a website I can't seem to be able to do it, I asked chatgpt, gemini, claude 3.7, none of them could help me out, the code made sense but the response that i was hoping to get never came. I eliminate the code that was really not doing anything, and if anyone knows anything I would be really appreciated, I send here the code that was for the website and also the modified version that I had for the terminal.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;the html&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&amp;lt;!DOCTYPE html&amp;gt;&lt;br /&gt; &amp;lt;html lang=&amp;quot;pt-pt&amp;quot;&amp;gt;&lt;br /&gt; &amp;lt;head&amp;gt;&lt;br /&gt; &amp;lt;meta charset=&amp;quot;UTF-8&amp;quot;&amp;gt;&lt;br /&gt; &amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=device-width, initial-scale=1.0&amp;quot;&amp;gt;&lt;br /&gt; &amp;lt;title&amp;gt;OficinaStudy&amp;lt;/title&amp;gt;&lt;br /&gt; &amp;lt;style&amp;gt;&lt;br /&gt; body {&lt;br /&gt; font-family: Arial, sans-serif;&lt;br /&gt; margin: 20px;&lt;br /&gt; } &lt;/p&gt; &lt;p&gt;/* STYLE CHAT BOX */&lt;br /&gt; #chat-box {&lt;br /&gt; margin: 20px 0;&lt;br /&gt; padding: 10px;&lt;br /&gt; border: 1px solid #ccc;&lt;br /&gt; max-width: 100%;&lt;br /&gt; min-height: 300px;&lt;br /&gt; overflow-y: auto;&lt;br /&gt; } &lt;/p&gt; &lt;p&gt;/* STYLE INPUT BOX */&lt;br /&gt; #input-box {&lt;br /&gt; width: calc(100% - 20px);&lt;br /&gt; padding: 10px;&lt;br /&gt; margin-bottom: 20px;&lt;br /&gt; }&lt;br /&gt; #box {&lt;br /&gt; width: calc(100% - 20px);&lt;br /&gt; padding: 10px;&lt;br /&gt; margin-bottom: 20px;&lt;br /&gt; }&lt;br /&gt; &amp;lt;/style&amp;gt;&lt;br /&gt; &amp;lt;/head&amp;gt;&lt;br /&gt; &amp;lt;body&amp;gt;&lt;br /&gt; &amp;lt;h1&amp;gt;OficinaStudy AI&amp;lt;/h1&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;lt;!-- CHATBOX --&amp;gt;&lt;br /&gt; &amp;lt;div id=&amp;quot;chat-box&amp;quot;&amp;gt;&amp;lt;/div&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;lt;!-- INPUT BOX --&amp;gt;&lt;br /&gt; &amp;lt;input type=&amp;quot;text&amp;quot; id=&amp;quot;input-box&amp;quot; placeholder=&amp;quot;Type your message here...&amp;quot; /&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;lt;!-- SEND BUTTON --&amp;gt;&lt;br /&gt; &amp;lt;button id=&amp;quot;send-button&amp;quot;&amp;gt;Send&amp;lt;/button&amp;gt;&lt;br /&gt; &amp;lt;button id=&amp;quot;rag&amp;quot;&amp;gt;RAG&amp;lt;/button&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;lt;script&amp;gt;&lt;br /&gt; // ATRIBUIR UM VALOR AOS IDS&lt;br /&gt; const chatBox = document.getElementById(&amp;quot;chat-box&amp;quot;);&lt;br /&gt; const inputBox = document.getElementById(&amp;quot;input-box&amp;quot;);&lt;br /&gt; const sendButton = document.getElementById(&amp;quot;send-button&amp;quot;);&lt;br /&gt; const rag = document.getElementById(&amp;quot;RAG&amp;quot;); &lt;/p&gt; &lt;p&gt;// ADICIONAR ACAO AO BOTAO&lt;br /&gt; sendButton.addEventListener(&amp;quot;click&amp;quot;, async () =&amp;gt; {&lt;br /&gt; const userInput = inputBox.value; &lt;/p&gt; &lt;p&gt;// DEFINIR AS PALAVRAS CHAVE&lt;br /&gt; const keywordList = [&amp;quot;exercicio&amp;quot;, &amp;quot;escolhas&amp;quot;, &amp;quot;multiplas&amp;quot;, &amp;quot;exerc√≠cio&amp;quot;, &amp;quot;m√∫ltiplas&amp;quot;, &amp;quot;escolha&amp;quot;]; &lt;/p&gt; &lt;p&gt;function checkKeywords() {&lt;br /&gt; userInputLower = userInput.toLowerCase();&lt;br /&gt; const hasKeyword = keywordList.some(keyword =&amp;gt; userInputLower.includes(keyword)); &lt;/p&gt; &lt;p&gt;if (hasKeyword) {&lt;br /&gt; alert(&amp;quot;sim!!! c:&amp;quot;);&lt;br /&gt; const newInput = document.createElement(&amp;quot;input&amp;quot;);&lt;br /&gt; newInput.type = &amp;quot;text&amp;quot;;&lt;br /&gt; &lt;a href="http://newInput.id"&gt;newInput.id&lt;/a&gt; = &amp;quot;box&amp;quot;;&lt;br /&gt; newInput.placeholder = &amp;quot;Type your message here...&amp;quot;; &lt;/p&gt; &lt;p&gt;document.body.appendChild(newInput);&lt;br /&gt; } else {&lt;br /&gt; alert(&amp;quot;nao :c&amp;quot;);&lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; checkKeywords(); &lt;/p&gt; &lt;p&gt;// RETIRAR OS ESPACOS EM BRANCO&lt;br /&gt; if (!userInput.trim()) return; &lt;/p&gt; &lt;p&gt;// ADICIONAR O USERINPUT √Ä CHATBOX&lt;br /&gt; chatBox.innerHTML += `&amp;lt;div&amp;gt;&amp;lt;strong&amp;gt;You:&amp;lt;/strong&amp;gt; ${userInput}&amp;lt;/div&amp;gt;`;&lt;br /&gt; inputBox.value = &amp;quot;&amp;quot;; &lt;/p&gt; &lt;p&gt;// ESTABELECER LIGACAO COM O &lt;a href="http://SERVER.PY"&gt;SERVER.PY&lt;/a&gt; E TRANSFORMAR EM JSON&lt;br /&gt; try { &lt;/p&gt; &lt;p&gt;const response = await fetch(&amp;quot;http://localhost:5000/generate&amp;quot;, {&lt;br /&gt; method: &amp;quot;POST&amp;quot;,&lt;br /&gt; headers: {&lt;br /&gt; &amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot;&lt;br /&gt; },&lt;br /&gt; body: JSON.stringify({ input: userInput })&lt;br /&gt; });&lt;br /&gt; const data = await response.json(); &lt;/p&gt; &lt;p&gt;// ADICIONAR A RESPOSTA DA IA √Ä CHATBOX&lt;br /&gt; if (data.response) {&lt;br /&gt; chatBox.innerHTML += `&amp;lt;div&amp;gt;&amp;lt;strong&amp;gt;Buddy:&amp;lt;/strong&amp;gt; ${data.response}&amp;lt;/div&amp;gt;`;&lt;br /&gt; } else {&lt;br /&gt; // DIZER QUE H√Å UM ERRO SE FOR O CASO&lt;br /&gt; chatBox.innerHTML += `&amp;lt;div&amp;gt;&amp;lt;strong&amp;gt;Buddy:&amp;lt;/strong&amp;gt; Error: ${data.error || &amp;quot;Erro desconhecido :(&amp;quot;}&amp;lt;/div&amp;gt;`;&lt;br /&gt; }&lt;br /&gt; } catch (error) {&lt;br /&gt; // DIZER SE HOUVE UM ERRO AO CONECTAR COM O SERVIDOR&lt;br /&gt; chatBox.innerHTML += `&amp;lt;div&amp;gt;&amp;lt;strong&amp;gt;Buddy:&amp;lt;/strong&amp;gt; Ops! Houve um erro ao conectar com o servidor! :( &amp;lt;/div&amp;gt;`;&lt;br /&gt; } &lt;/p&gt; &lt;p&gt;chatBox.scrollTop = chatBox.scrollHeight;&lt;br /&gt; }); &lt;/p&gt; &lt;p&gt;rag.addEventListener(&amp;quot;click&amp;quot;, async () =&amp;gt; { &lt;/p&gt; &lt;p&gt;})&lt;br /&gt; &amp;lt;/script&amp;gt;&lt;br /&gt; &amp;lt;/body&amp;gt;&lt;br /&gt; &amp;lt;/html&amp;gt;&lt;br /&gt; &amp;lt;!DOCTYPE html&amp;gt;&lt;br /&gt; &amp;lt;html lang=&amp;quot;pt-pt&amp;quot;&amp;gt;&lt;br /&gt; &amp;lt;head&amp;gt;&lt;br /&gt; &amp;lt;meta charset=&amp;quot;UTF-8&amp;quot;&amp;gt;&lt;br /&gt; &amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=device-width, initial-scale=1.0&amp;quot;&amp;gt;&lt;br /&gt; &amp;lt;title&amp;gt;OficinaStudy&amp;lt;/title&amp;gt;&lt;br /&gt; &amp;lt;style&amp;gt;&lt;br /&gt; body {&lt;br /&gt; font-family: Arial, sans-serif;&lt;br /&gt; margin: 20px;&lt;br /&gt; } &lt;/p&gt; &lt;p&gt;/* STYLE CHAT BOX */&lt;br /&gt; #chat-box {&lt;br /&gt; margin: 20px 0;&lt;br /&gt; padding: 10px;&lt;br /&gt; border: 1px solid #ccc;&lt;br /&gt; max-width: 100%;&lt;br /&gt; min-height: 300px;&lt;br /&gt; overflow-y: auto;&lt;br /&gt; } &lt;/p&gt; &lt;p&gt;/* STYLE INPUT BOX */&lt;br /&gt; #input-box {&lt;br /&gt; width: calc(100% - 20px);&lt;br /&gt; padding: 10px;&lt;br /&gt; margin-bottom: 20px;&lt;br /&gt; }&lt;br /&gt; #box {&lt;br /&gt; width: calc(100% - 20px);&lt;br /&gt; padding: 10px;&lt;br /&gt; margin-bottom: 20px;&lt;br /&gt; }&lt;br /&gt; &amp;lt;/style&amp;gt;&lt;br /&gt; &amp;lt;/head&amp;gt;&lt;br /&gt; &amp;lt;body&amp;gt;&lt;br /&gt; &amp;lt;h1&amp;gt;OficinaStudy AI&amp;lt;/h1&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;lt;!-- CHATBOX --&amp;gt;&lt;br /&gt; &amp;lt;div id=&amp;quot;chat-box&amp;quot;&amp;gt;&amp;lt;/div&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;lt;!-- INPUT BOX --&amp;gt;&lt;br /&gt; &amp;lt;input type=&amp;quot;text&amp;quot; id=&amp;quot;input-box&amp;quot; placeholder=&amp;quot;Type your message here...&amp;quot; /&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;lt;!-- SEND BUTTON --&amp;gt;&lt;br /&gt; &amp;lt;button id=&amp;quot;send-button&amp;quot;&amp;gt;Send&amp;lt;/button&amp;gt;&lt;br /&gt; &amp;lt;button id=&amp;quot;rag&amp;quot;&amp;gt;RAG&amp;lt;/button&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;lt;script&amp;gt;&lt;br /&gt; // ATRIBUIR UM VALOR AOS IDS&lt;br /&gt; const chatBox = document.getElementById(&amp;quot;chat-box&amp;quot;);&lt;br /&gt; const inputBox = document.getElementById(&amp;quot;input-box&amp;quot;);&lt;br /&gt; const sendButton = document.getElementById(&amp;quot;send-button&amp;quot;);&lt;br /&gt; const rag = document.getElementById(&amp;quot;RAG&amp;quot;); &lt;/p&gt; &lt;p&gt;// ADICIONAR ACAO AO BOTAO&lt;br /&gt; sendButton.addEventListener(&amp;quot;click&amp;quot;, async () =&amp;gt; {&lt;br /&gt; const userInput = inputBox.value; &lt;/p&gt; &lt;p&gt;// DEFINIR AS PALAVRAS CHAVE&lt;br /&gt; const keywordList = [&amp;quot;exercicio&amp;quot;, &amp;quot;escolhas&amp;quot;, &amp;quot;multiplas&amp;quot;, &amp;quot;exerc√≠cio&amp;quot;, &amp;quot;m√∫ltiplas&amp;quot;, &amp;quot;escolha&amp;quot;]; &lt;/p&gt; &lt;p&gt;function checkKeywords() {&lt;br /&gt; userInputLower = userInput.toLowerCase();&lt;br /&gt; const hasKeyword = keywordList.some(keyword =&amp;gt; userInputLower.includes(keyword)); &lt;/p&gt; &lt;p&gt;if (hasKeyword) {&lt;br /&gt; alert(&amp;quot;sim!!! c:&amp;quot;);&lt;br /&gt; const newInput = document.createElement(&amp;quot;input&amp;quot;);&lt;br /&gt; newInput.type = &amp;quot;text&amp;quot;;&lt;br /&gt; &lt;a href="http://newInput.id"&gt;newInput.id&lt;/a&gt; = &amp;quot;box&amp;quot;;&lt;br /&gt; newInput.placeholder = &amp;quot;Type your message here...&amp;quot;; &lt;/p&gt; &lt;p&gt;document.body.appendChild(newInput);&lt;br /&gt; } else {&lt;br /&gt; alert(&amp;quot;nao :c&amp;quot;);&lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; checkKeywords(); &lt;/p&gt; &lt;p&gt;// RETIRAR OS ESPACOS EM BRANCO&lt;br /&gt; if (!userInput.trim()) return; &lt;/p&gt; &lt;p&gt;// ADICIONAR O USERINPUT √Ä CHATBOX&lt;br /&gt; chatBox.innerHTML += `&amp;lt;div&amp;gt;&amp;lt;strong&amp;gt;You:&amp;lt;/strong&amp;gt; ${userInput}&amp;lt;/div&amp;gt;`;&lt;br /&gt; inputBox.value = &amp;quot;&amp;quot;; &lt;/p&gt; &lt;p&gt;// ESTABELECER LIGACAO COM O &lt;a href="http://SERVER.PY"&gt;SERVER.PY&lt;/a&gt; E TRANSFORMAR EM JSON&lt;br /&gt; try { &lt;/p&gt; &lt;p&gt;const response = await fetch(&amp;quot;http://localhost:5000/generate&amp;quot;, {&lt;br /&gt; method: &amp;quot;POST&amp;quot;,&lt;br /&gt; headers: {&lt;br /&gt; &amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot;&lt;br /&gt; },&lt;br /&gt; body: JSON.stringify({ input: userInput })&lt;br /&gt; });&lt;br /&gt; const data = await response.json(); &lt;/p&gt; &lt;p&gt;// ADICIONAR A RESPOSTA DA IA √Ä CHATBOX&lt;br /&gt; if (data.response) {&lt;br /&gt; chatBox.innerHTML += `&amp;lt;div&amp;gt;&amp;lt;strong&amp;gt;Buddy:&amp;lt;/strong&amp;gt; ${data.response}&amp;lt;/div&amp;gt;`;&lt;br /&gt; } else {&lt;br /&gt; // DIZER QUE H√Å UM ERRO SE FOR O CASO&lt;br /&gt; chatBox.innerHTML += `&amp;lt;div&amp;gt;&amp;lt;strong&amp;gt;Buddy:&amp;lt;/strong&amp;gt; Error: ${data.error || &amp;quot;Erro desconhecido :(&amp;quot;}&amp;lt;/div&amp;gt;`;&lt;br /&gt; }&lt;br /&gt; } catch (error) {&lt;br /&gt; // DIZER SE HOUVE UM ERRO AO CONECTAR COM O SERVIDOR&lt;br /&gt; chatBox.innerHTML += `&amp;lt;div&amp;gt;&amp;lt;strong&amp;gt;Buddy:&amp;lt;/strong&amp;gt; Ops! Houve um erro ao conectar com o servidor! :( &amp;lt;/div&amp;gt;`;&lt;br /&gt; } &lt;/p&gt; &lt;p&gt;chatBox.scrollTop = chatBox.scrollHeight;&lt;br /&gt; }); &lt;/p&gt; &lt;p&gt;rag.addEventListener(&amp;quot;click&amp;quot;, async () =&amp;gt; { &lt;/p&gt; &lt;p&gt;})&lt;br /&gt; &amp;lt;/script&amp;gt;&lt;br /&gt; &amp;lt;/body&amp;gt;&lt;br /&gt; &amp;lt;/html&amp;gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;the&lt;/strong&gt; &lt;a href="http://server.py"&gt;&lt;strong&gt;server.py&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;from typing import Dict&lt;br /&gt; from fastapi import FastAPI, HTTPException&lt;br /&gt; from fastapi.middleware.cors import CORSMiddleware&lt;br /&gt; from pydantic import BaseModel&lt;br /&gt; import ollama &lt;/p&gt; &lt;p&gt;app = FastAPI() &lt;/p&gt; &lt;p&gt;# Enable CORS&lt;br /&gt; app.add_middleware(&lt;br /&gt; CORSMiddleware,&lt;br /&gt; allow_origins=[&amp;quot;*&amp;quot;], # Adjust this for security in production&lt;br /&gt; allow_credentials=True,&lt;br /&gt; allow_methods=[&amp;quot;*&amp;quot;],&lt;br /&gt; allow_headers=[&amp;quot;*&amp;quot;],&lt;br /&gt; ) &lt;/p&gt; &lt;p&gt;model = &amp;quot;gemma2mod3&amp;quot; # Model name&lt;br /&gt; conversation_history = [] # Store conversation history &lt;/p&gt; &lt;p&gt;# Define request body model&lt;br /&gt; class UserInput(BaseModel):&lt;br /&gt; input: str &lt;/p&gt; &lt;p&gt;&lt;a href="/u/app"&gt;u/app&lt;/a&gt;.post(&amp;quot;/generate&amp;quot;)&lt;br /&gt; async def generate_response(user_input: UserInput) -&amp;gt; Dict[str, str]:&lt;br /&gt; try:&lt;br /&gt; global conversation_history &lt;/p&gt; &lt;p&gt;if not user_input.input:&lt;br /&gt; raise HTTPException(status_code=400, detail=&amp;quot;No input provided&amp;quot;) &lt;/p&gt; &lt;p&gt;# Add user message to history&lt;br /&gt; conversation_history.append({&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: user_input.input}) &lt;/p&gt; &lt;p&gt;# Format conversation history&lt;br /&gt; formatted_history = &amp;quot;\n&amp;quot;.join(&lt;br /&gt; [f&amp;quot;{msg['role'].capitalize()}: {msg['content']}&amp;quot; for msg in conversation_history]&lt;br /&gt; ) &lt;/p&gt; &lt;p&gt;# Generate response&lt;br /&gt; response = ollama.generate(model=model, prompt=formatted_history)&lt;br /&gt; assistant_response = response.get('response', &amp;quot;&amp;quot;) &lt;/p&gt; &lt;p&gt;# Add assistant response to history&lt;br /&gt; conversation_history.append({&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: assistant_response}) &lt;/p&gt; &lt;p&gt;return {&amp;quot;response&amp;quot;: assistant_response} &lt;/p&gt; &lt;p&gt;except Exception as e:&lt;br /&gt; raise HTTPException(status_code=500, detail=str(e)) &lt;/p&gt; &lt;p&gt;if __name__ == &amp;quot;__main__&amp;quot;:&lt;br /&gt; import uvicorn&lt;br /&gt; uvicorn.run(app, host=&amp;quot;0.0.0.0&amp;quot;, port=5000)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The rag for terminal&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;import torch&lt;br /&gt; from sentence_transformers import SentenceTransformer, util&lt;br /&gt; import os&lt;br /&gt; from openai import OpenAI&lt;br /&gt; import argparse &lt;/p&gt; &lt;p&gt;# Function to open a file and return its contents as a string&lt;br /&gt; def open_file(filepath):&lt;br /&gt; with open(filepath, 'r', encoding='utf-8') as infile:&lt;br /&gt; return infile.read() &lt;/p&gt; &lt;p&gt;# Function to get relevant context from the vault based on user input&lt;br /&gt; def get_relevant_context(user_input, vault_embeddings, vault_content, model, top_k=2):&lt;br /&gt; if vault_embeddings.nelement() == 0: # Check if the tensor has any elements&lt;br /&gt; return []&lt;br /&gt; # Encode the user input&lt;br /&gt; input_embedding = model.encode([user_input])&lt;br /&gt; # Compute cosine similarity between the input and vault embeddings&lt;br /&gt; cos_scores = util.cos_sim(input_embedding, vault_embeddings)[0]&lt;br /&gt; # Adjust top_k if it's greater than the number of available scores&lt;br /&gt; top_k = min(top_k, len(cos_scores))&lt;br /&gt; # Sort the scores and get the top-k indices&lt;br /&gt; top_indices = torch.topk(cos_scores, k=top_k)[1].tolist()&lt;br /&gt; # Get the corresponding context from the vault&lt;br /&gt; relevant_context = [vault_content[idx].strip() for idx in top_indices]&lt;br /&gt; return relevant_context &lt;/p&gt; &lt;p&gt;# Function to interact with the Ollama model&lt;br /&gt; def ollama_chat(user_input, system_message, vault_embeddings, vault_content, model, ollama_model, conversation_history):&lt;br /&gt; relevant_context = []&lt;br /&gt; user_input = user_input.replace(&amp;quot;search_vault&amp;quot;, &amp;quot;&amp;quot;).strip()&lt;br /&gt; relevant_context = get_relevant_context(user_input, vault_embeddings, vault_content, model) &lt;/p&gt; &lt;p&gt;if relevant_context:&lt;br /&gt; context_str = &amp;quot;\n&amp;quot;.join(relevant_context)&lt;br /&gt; print(&amp;quot;Context Pulled from Documents: \n\n&amp;quot; + context_str)&lt;br /&gt; user_input_with_context = context_str + &amp;quot;\n\n&amp;quot; + user_input&lt;br /&gt; else:&lt;br /&gt; user_input_with_context = user_input &lt;/p&gt; &lt;p&gt;conversation_history.append({&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: user_input_with_context})&lt;br /&gt; messages = [&lt;br /&gt; {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: system_message},&lt;br /&gt; *conversation_history&lt;br /&gt; ] &lt;/p&gt; &lt;p&gt;response = client.chat.completions.create(&lt;br /&gt; model=ollama_model,&lt;br /&gt; messages=messages&lt;br /&gt; ) &lt;/p&gt; &lt;p&gt;response_text = response.choices[0].message.content&lt;br /&gt; conversation_history.append({&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: response_text})&lt;br /&gt; return response_text &lt;/p&gt; &lt;p&gt;# Configuration for the Ollama API client&lt;br /&gt; client = OpenAI(&lt;br /&gt; base_url='http://localhost:11434/v1',&lt;br /&gt; api_key='llama3'&lt;br /&gt; ) &lt;/p&gt; &lt;p&gt;# Parse command-line arguments&lt;br /&gt; parser = argparse.ArgumentParser(description=&amp;quot;Ollama Chat&amp;quot;)&lt;br /&gt; parser.add_argument(&amp;quot;--model&amp;quot;, default=&amp;quot;Oficina-AI&amp;quot;, help=&amp;quot;Ollama model to use (default: Oficina-AI)&amp;quot;)&lt;br /&gt; args = parser.parse_args() &lt;/p&gt; &lt;p&gt;# Load the model and vault content&lt;br /&gt; model = SentenceTransformer(&amp;quot;all-MiniLM-L6-v2&amp;quot;)&lt;br /&gt; vault_content = []&lt;br /&gt; if os.path.exists(&amp;quot;vault.txt&amp;quot;):&lt;br /&gt; with open(&amp;quot;vault.txt&amp;quot;, &amp;quot;r&amp;quot;, encoding='utf-8') as vault_file:&lt;br /&gt; vault_content = vault_file.readlines()&lt;br /&gt; vault_embeddings = model.encode(vault_content) if vault_content else [] &lt;/p&gt; &lt;p&gt;vault_embeddings_tensor = torch.tensor(vault_embeddings)&lt;br /&gt; conversation_history = []&lt;br /&gt; system_message = &amp;quot;You are a helpful assistant that helps students by providing exercises and explanations using available resources. If information is found in the vault, it must be considered absolute truth. You should base your reasoning and opinions strictly on what is written in the &lt;a href="http://vault.You"&gt;vault.You&lt;/a&gt; are also an artificial inteligence helping students from all around the world study and have better grades, you should try to get used to any user that talks to you by imitating their behaviour, humor, and the way they talk to you, your principal job is to give students exercises when those are asked, those exercises could be for an example, true or false with or without justificating the falses, multiple choice, writting an answer or any other type of exercise that they ask. You should try to make them feel confortable, and when they ask you to explain something, you will explaint it.&amp;quot; &lt;/p&gt; &lt;p&gt;while True:&lt;br /&gt; user_input = input(&amp;quot;&amp;gt;&amp;gt;&amp;gt; &amp;quot;)&lt;br /&gt; if user_input.lower() == 'quit':&lt;br /&gt; break &lt;/p&gt; &lt;p&gt;response = ollama_chat(user_input, system_message, vault_embeddings_tensor, vault_content, model, conversation_history)&lt;br /&gt; #response = traduzir_para_pt_pt(response)&lt;br /&gt; print(&amp;quot;Response: \n\n&amp;quot; + response)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Impact4403"&gt; /u/Ok_Impact4403 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7iso1/i_cant_make_a_rag_system_with_fastapi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7iso1/i_cant_make_a_rag_system_with_fastapi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7iso1/i_cant_make_a_rag_system_with_fastapi/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T21:54:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j780k8</id>
    <title>Instructions in python SDK to use models as translators.</title>
    <updated>2025-03-09T13:49:14+00:00</updated>
    <author>
      <name>/u/nosumable</name>
      <uri>https://old.reddit.com/user/nosumable</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, new in this beautiful community!&lt;/p&gt; &lt;p&gt;Some days ago restarted a project to translate Chinese text from table tennis videos with my 16 GB vram gpu. In the past I used gCloud API to do the OCR and translation, the OCR was good but the translation was horrible.&lt;/p&gt; &lt;p&gt;I decided to go OpenSource. For the OCR I chose to use paddleocr (it works great) and for the translation I have found models as chatgpt Claude or deepseek works extremely good. So I decided to try a local approach with deepseek. The problem here arises, I cannot control what the model output gives, even if I order it to give the translation in a specific format to parse it after. Several question arises:&lt;/p&gt; &lt;p&gt;1) How do you handle this, I have read some other SDK have more methods that might me suitable for this&lt;/p&gt; &lt;p&gt;2) are there specific models that work better with translations? I was using 32b deepseek R1, but it might be overkill as speed translating is slow (performance is not a must, but if I can get some lighter model it would be nice)&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nosumable"&gt; /u/nosumable &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j780k8/instructions_in_python_sdk_to_use_models_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j780k8/instructions_in_python_sdk_to_use_models_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j780k8/instructions_in_python_sdk_to_use_models_as/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T13:49:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j775a2</id>
    <title>Ollama is not compatible with GPU anymore</title>
    <updated>2025-03-09T13:01:24+00:00</updated>
    <author>
      <name>/u/Inevitable_Cut_1309</name>
      <uri>https://old.reddit.com/user/Inevitable_Cut_1309</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j775a2/ollama_is_not_compatible_with_gpu_anymore/"&gt; &lt;img alt="Ollama is not compatible with GPU anymore" src="https://b.thumbs.redditmedia.com/6l83o2IL-Lp4AkXI1iPGr_0O0PU_zT-R0iXTjYtnbgI.jpg" title="Ollama is not compatible with GPU anymore" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have recently reinstalled cuda toolkit(12.5) and torch (11.8)&lt;br /&gt; I have NVIDIA GeForce RTX 4070, and my driver version is 572.60&lt;br /&gt; I am using Cuda 12.5 for Ollama compatibility, but every time I run my Ollama instead of the GPU, it starts running on the CPU.&lt;/p&gt; &lt;p&gt;The GPU used to be utilized 100% before the reinstallation, but now it doesn't consume more than 10% of the GPU.&lt;br /&gt; I have set the GPU for Olama to RTX 4070.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tl1naxrmsnne1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b0b3ef7f3db480b2833d82273a55bfc13c41829"&gt;https://preview.redd.it/tl1naxrmsnne1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b0b3ef7f3db480b2833d82273a55bfc13c41829&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mpan0v4aunne1.png?width=839&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4691fc4961bc11d9fe4bd90d1007e6df23a16235"&gt;https://preview.redd.it/mpan0v4aunne1.png?width=839&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4691fc4961bc11d9fe4bd90d1007e6df23a16235&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When I use the command ollama ps, it shows that it consumes 100% GPU.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4q73hr8runne1.png?width=725&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e696b64fde7a4cd744de5a54c977dd18e2706329"&gt;The GPU while running the ollama instance &lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have tried changing my Cuda version to 11.8, 12.3 and 12.8, but it doesn't make a difference. I am using cudnn 8.9.7.&lt;/p&gt; &lt;p&gt;I am doing this on a Windows 11. The models used to run at a 100% efficiency and now don't cross the 5-10% mark.&lt;br /&gt; I have tried reinstalling ollama as well.&lt;/p&gt; &lt;p&gt;These are the issues I see in ollama log file :&lt;/p&gt; &lt;p&gt;Key not found: llama.attention.key_length&lt;/p&gt; &lt;p&gt;key not found: llama.attention.value_length&lt;/p&gt; &lt;p&gt;ggml_backend_load_best: failed to load ... ggml-cpu-alderlake.dll&lt;/p&gt; &lt;p&gt;Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address is normally permitted.&lt;/p&gt; &lt;p&gt;Can someone tell me what to do here?&lt;/p&gt; &lt;p&gt;Edit:&lt;/p&gt; &lt;p&gt;I ran a code using my torch, and it is able to use 100% of the GPU:&lt;br /&gt; The code is :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import torch import time device = torch.device(&amp;quot;cuda&amp;quot; if torch.cuda.is_available() else &amp;quot;cpu&amp;quot;) print(f&amp;quot;Using device: {device}&amp;quot;) # Large matrix size for heavy computation size = 30000 # Increase this for more load iterations = 10 # Number of multiplications a = torch.randn(size, size, device=device) b = torch.randn(size, size, device=device) print(&amp;quot;Starting matrix multiplications...&amp;quot;) start_time = time.time() for i in range(iterations): c = torch.mm(a, b) # Matrix multiplication torch.cuda.synchronize() # Ensure GPU finishes before timing end_time = time.time() print(f&amp;quot;Completed {iterations} multiplications in {end_time - start_time:.2f} seconds&amp;quot;) print(&amp;quot;Final value from matrix:&amp;quot;, c[0, 0].item()) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable_Cut_1309"&gt; /u/Inevitable_Cut_1309 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j775a2/ollama_is_not_compatible_with_gpu_anymore/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j775a2/ollama_is_not_compatible_with_gpu_anymore/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j775a2/ollama_is_not_compatible_with_gpu_anymore/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T13:01:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7et4q</id>
    <title>Finetuning Llama 3.2 to Generate ASCII Cats (Full Tutorial)</title>
    <updated>2025-03-09T19:01:00+00:00</updated>
    <author>
      <name>/u/YungMixtape2004</name>
      <uri>https://old.reddit.com/user/YungMixtape2004</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j7et4q/finetuning_llama_32_to_generate_ascii_cats_full/"&gt; &lt;img alt="Finetuning Llama 3.2 to Generate ASCII Cats (Full Tutorial)" src="https://external-preview.redd.it/wZRAp0Z9Mp9JFxo0ZyDa-g6jwhUluZPSeazeek3nw58.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9aa8adb1463dff4004332fd167e93588aafe3ea2" title="Finetuning Llama 3.2 to Generate ASCII Cats (Full Tutorial)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YungMixtape2004"&gt; /u/YungMixtape2004 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/-H1-lr_sIZk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7et4q/finetuning_llama_32_to_generate_ascii_cats_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7et4q/finetuning_llama_32_to_generate_ascii_cats_full/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T19:01:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7fqdu</id>
    <title>Best model for questions about PC hardware</title>
    <updated>2025-03-09T19:40:44+00:00</updated>
    <author>
      <name>/u/haemakatus</name>
      <uri>https://old.reddit.com/user/haemakatus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering if there is a Ollama model trained on PC components such as motherboard chipsets, memory, GPUs etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/haemakatus"&gt; /u/haemakatus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7fqdu/best_model_for_questions_about_pc_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7fqdu/best_model_for_questions_about_pc_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7fqdu/best_model_for_questions_about_pc_hardware/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T19:40:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6ydpa</id>
    <title>Latest qwq thinking model with unsloth parameters</title>
    <updated>2025-03-09T03:15:12+00:00</updated>
    <author>
      <name>/u/DanielUpsideDown</name>
      <uri>https://old.reddit.com/user/DanielUpsideDown</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unsloth published an article on how to run qwq with optimized parameters &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"&gt;here&lt;/a&gt;. I made a modelfile and uploaded it to ollama - &lt;a href="https://ollama.com/driftfurther/qwq-unsloth"&gt;https://ollama.com/driftfurther/qwq-unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It fits perfectly into 24 GB VRAM and it is amazing at its performance. Coding in particular has been incredible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanielUpsideDown"&gt; /u/DanielUpsideDown &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j6ydpa/latest_qwq_thinking_model_with_unsloth_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j6ydpa/latest_qwq_thinking_model_with_unsloth_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j6ydpa/latest_qwq_thinking_model_with_unsloth_parameters/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T03:15:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7ci9z</id>
    <title>Apple specs in the future</title>
    <updated>2025-03-09T17:21:06+00:00</updated>
    <author>
      <name>/u/jamboman_</name>
      <uri>https://old.reddit.com/user/jamboman_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Started to use ollama about a week ago. I use a Mac mini M2 and have 256gb, with 24gb ram.&lt;/p&gt; &lt;p&gt;It works great, and I have no complaints.&lt;/p&gt; &lt;p&gt;But it made me think...we know that AI is going to rapidly improve, and things are going to change wildly. So...with that in mind, and with apple making machines with everything on one chip, it's going to mean that we could be wanting to upgrade machines more and more frequently in the future.&lt;/p&gt; &lt;p&gt;I want to upgrade today, but I also want to know that should better LLMs come out, with more demands, that I can upgrade to maintain performance.&lt;/p&gt; &lt;p&gt;Sorry of this has been asked before &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jamboman_"&gt; /u/jamboman_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7ci9z/apple_specs_in_the_future/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7ci9z/apple_specs_in_the_future/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7ci9z/apple_specs_in_the_future/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T17:21:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7rit5</id>
    <title>What PSU for dual 3090</title>
    <updated>2025-03-10T05:27:44+00:00</updated>
    <author>
      <name>/u/Timziito</name>
      <uri>https://old.reddit.com/user/Timziito</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey fellow humans üôÇ I have been able to get two 3090 msi cards with three 8 pins per gpu. &lt;/p&gt; &lt;p&gt;What would be an reasonable power supply? And atx3.0 or atx3.1&lt;/p&gt; &lt;p&gt;Best regards Tim&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Timziito"&gt; /u/Timziito &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7rit5/what_psu_for_dual_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7rit5/what_psu_for_dual_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7rit5/what_psu_for_dual_3090/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T05:27:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7p4sg</id>
    <title>Learning question</title>
    <updated>2025-03-10T03:05:02+00:00</updated>
    <author>
      <name>/u/tshawkins</name>
      <uri>https://old.reddit.com/user/tshawkins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What would be the problems associated having a RAG based AI, self update. &lt;/p&gt; &lt;p&gt;Often when conversing with an AI, it will say something outright false, would it be feasable to determine a command with a corrective intent, and then insert the correction into the RAG database as a high weight fact. &lt;/p&gt; &lt;p&gt;Something like. &lt;/p&gt; &lt;p&gt;AI: the eiffel tower is is in london. &lt;/p&gt; &lt;p&gt;Me: that is not correct, the eiffel tower is in paris. &lt;/p&gt; &lt;p&gt;AI: sorry, do you want me to remember that the eiffel tower is in paris?&lt;/p&gt; &lt;p&gt;Me: yes&lt;/p&gt; &lt;p&gt;AI: the location of the eiffel tower has been updated.&lt;/p&gt; &lt;p&gt;Me: where is the eiffle tower.&lt;/p&gt; &lt;p&gt;AI: the eiffle tower is in paris. &lt;/p&gt; &lt;p&gt;Note: that an AI will appear to do this right now, but as soon as the session ends, all facts learned are forgotten. with a self updating RAG system it will becom part of its permenant memory. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tshawkins"&gt; /u/tshawkins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7p4sg/learning_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7p4sg/learning_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7p4sg/learning_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T03:05:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7kp2l</id>
    <title>Best model for text summarization (2025)</title>
    <updated>2025-03-09T23:20:53+00:00</updated>
    <author>
      <name>/u/Unhappy_Bunch</name>
      <uri>https://old.reddit.com/user/Unhappy_Bunch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run Ollama on my desktop with 64GB ram and an RTX4080. I currently use llama3.1 8B for summarization text of all types. &lt;/p&gt; &lt;p&gt;What other models do you guys suggest that might be more accurate? &lt;/p&gt; &lt;p&gt;What other tips do you have for accuracy? &lt;/p&gt; &lt;p&gt;TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unhappy_Bunch"&gt; /u/Unhappy_Bunch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7kp2l/best_model_for_text_summarization_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7kp2l/best_model_for_text_summarization_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7kp2l/best_model_for_text_summarization_2025/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T23:20:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j72bsk</id>
    <title>MY JARVIS PROJECT</title>
    <updated>2025-03-09T07:20:44+00:00</updated>
    <author>
      <name>/u/cython_boy</name>
      <uri>https://old.reddit.com/user/cython_boy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! So I‚Äôve been messing around with AI and ended up building Jarvis , my own personal assistant. It listens for ‚ÄúHey Jarvis‚Äù understands what I need, and does things like sending emails, making calls, checking the weather, and more. It‚Äôs all powered by Gemini AI and ollama . with some smart intent handling using LangChain. (using ibm granite-dense models with gemini.)&lt;/p&gt; &lt;p&gt;# All three versions of project started with version 0 and latest is version 2.&lt;/p&gt; &lt;p&gt;version 2 (jarvis2.0): &lt;a href="https://github.com/ganeshnikhil/J.A.R.V.I.S.2.0"&gt;Github&lt;/a&gt; &lt;/p&gt; &lt;p&gt;version 1 (jarvis 1.0): &lt;a href="https://github.com/ganeshnikhil/Desktop_AI"&gt;v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;version 0 (jarvis 0.0): &lt;a href="https://github.com/ganeshnikhil/JARVIS"&gt;v0&lt;/a&gt; &lt;/p&gt; &lt;p&gt;all new versions are updated version of previous , with added new functionalities and new approach.&lt;/p&gt; &lt;p&gt;- Listens to my voice üéôÔ∏è&lt;/p&gt; &lt;p&gt;- Figures out if it needs AI, a function call , agentic modes , or a quick response&lt;/p&gt; &lt;p&gt;- Executes tasks like emailing, news updates, rag knowledge base or even making calls (adb).&lt;/p&gt; &lt;p&gt;- Handles errors without breaking (because trust me, it broke a lot at first)&lt;/p&gt; &lt;p&gt;- **Wake word chaos** ‚Äì It kept activating randomly, had to fine-tune that&lt;/p&gt; &lt;p&gt;- **Task confusion** ‚Äì Balancing AI responses with simple predefined actions , mixed approach.&lt;/p&gt; &lt;p&gt;- **Complex queries** ‚Äì Ended up using ML to route requests properly&lt;/p&gt; &lt;p&gt;Review my project , I want a feedback to improve it furthure , i am open for all kind of suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cython_boy"&gt; /u/cython_boy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j72bsk/my_jarvis_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j72bsk/my_jarvis_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j72bsk/my_jarvis_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T07:20:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7t6pk</id>
    <title>How to run Ollama on CPU</title>
    <updated>2025-03-10T07:27:43+00:00</updated>
    <author>
      <name>/u/Badincomputer</name>
      <uri>https://old.reddit.com/user/Badincomputer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a workstation with dual xeon gold 6154 cpu and 192 gb ram. I want to test how best it run CPU and RAM only and then i want to see how it will run on quadro p620 gpu. I could not find any resource to do so. My plan is to test first on workstation and with GPU and then i will install more RAM on it to see if it helps in any way. Basically it will be a comparison at last&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Badincomputer"&gt; /u/Badincomputer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7t6pk/how_to_run_ollama_on_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7t6pk/how_to_run_ollama_on_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7t6pk/how_to_run_ollama_on_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T07:27:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7tyb2</id>
    <title>I want to create a personal project using LLMs</title>
    <updated>2025-03-10T08:27:54+00:00</updated>
    <author>
      <name>/u/Spiritual_Piccolo793</name>
      <uri>https://old.reddit.com/user/Spiritual_Piccolo793</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do I need to use Azure or AWS for this? Because I want to use something along the lines of RAG + Database usage. Hence, what is the cheapest resource that I could use to try and build something? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiritual_Piccolo793"&gt; /u/Spiritual_Piccolo793 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7tyb2/i_want_to_create_a_personal_project_using_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7tyb2/i_want_to_create_a_personal_project_using_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7tyb2/i_want_to_create_a_personal_project_using_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T08:27:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1j82gjx</id>
    <title>Cannot save model after /set parameter num_ctx 32768</title>
    <updated>2025-03-10T16:18:25+00:00</updated>
    <author>
      <name>/u/HeadGr</name>
      <uri>https://old.reddit.com/user/HeadGr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So i found that ollama truncating input prompt (according to console output, and want to save altered model with forced num_ctx, but ollama keeps saying things like &amp;quot;The model name 'bahaslama32' is invalid&amp;quot; for any name given. Any hint or workaround?&lt;/p&gt; &lt;p&gt;UPDATE: Or maybe some hints how to avoid truncating prompt? I'm making requests from n8n using mysql agent and after few iterations LLM losing user question it had to answer.&lt;/p&gt; &lt;p&gt;&lt;code&gt;level=WARN source=runner.go:130 msg=&amp;quot;truncating input prompt&amp;quot; limit=2048 prompt=7159 keep=5 new=2048&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HeadGr"&gt; /u/HeadGr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j82gjx/cannot_save_model_after_set_parameter_num_ctx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j82gjx/cannot_save_model_after_set_parameter_num_ctx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j82gjx/cannot_save_model_after_set_parameter_num_ctx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T16:18:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7vtto</id>
    <title>Possible to quantize a model pulled from Ollama.com yourself?</title>
    <updated>2025-03-10T10:48:19+00:00</updated>
    <author>
      <name>/u/Lodurr242</name>
      <uri>https://old.reddit.com/user/Lodurr242</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Say I poke around on &lt;a href="http://ollama.com"&gt;ollama.com&lt;/a&gt;, and find a model I want to try (mistral-small). But there are only these quantized models availiable to pull:&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/mistral-small:24b-instruct-2501-q4_K_M"&gt;24b-instruct-2501-q4_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/mistral-small:24b-instruct-2501-q8_0"&gt;24b-instruct-2501-q8_0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If I would like something else, say, q5_K_M or &lt;code&gt;q6_K can I just pull the full model mistral-small:24b-instruct-2501-fp16 , create a 'Model file' with FROM ... and then run:&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama create --quantize q5_K_M mymodelfile&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I saw some documentation talking about the source model to be quantized should be in 'safe tensors' format, which makes me think the above simple approach is not valid. What do you say? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lodurr242"&gt; /u/Lodurr242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7vtto/possible_to_quantize_a_model_pulled_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7vtto/possible_to_quantize_a_model_pulled_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7vtto/possible_to_quantize_a_model_pulled_from/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T10:48:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7v5ia</id>
    <title>Using Ollama with Spring AI - Piotr's TechBlog</title>
    <updated>2025-03-10T10:00:14+00:00</updated>
    <author>
      <name>/u/piotr_minkowski</name>
      <uri>https://old.reddit.com/user/piotr_minkowski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j7v5ia/using_ollama_with_spring_ai_piotrs_techblog/"&gt; &lt;img alt="Using Ollama with Spring AI - Piotr's TechBlog" src="https://external-preview.redd.it/YgjB-PvLFD1RrOTuhDNKK1_bD47pcMM4PCECfwkxQxQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3a008d9cc9ac8d68eefb1343878edcc118d6d70" title="Using Ollama with Spring AI - Piotr's TechBlog" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/piotr_minkowski"&gt; /u/piotr_minkowski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://piotrminkowski.com/2025/03/10/using-ollama-with-spring-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7v5ia/using_ollama_with_spring_ai_piotrs_techblog/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7v5ia/using_ollama_with_spring_ai_piotrs_techblog/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T10:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7upat</id>
    <title>OLLAMA + TTS + STT, no cloud or API paying keys</title>
    <updated>2025-03-10T09:25:47+00:00</updated>
    <author>
      <name>/u/Brandu33</name>
      <uri>https://old.reddit.com/user/Brandu33</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm an eye impaired writer, I use UBUNTU.&lt;/p&gt; &lt;p&gt;Would you happen to know a chatbot or webui, which could be run locally without cloud or a paying API, even if internet is down. If you do not, and would like to work on one, I'm here, I'm not good at coding, but have basic (very basic knowledge!) and time.&lt;/p&gt; &lt;p&gt;Compatible with OLLAMA.&lt;/p&gt; &lt;p&gt;STT: a FOSS whisper.&lt;/p&gt; &lt;p&gt;TTS: even if gTTS.&lt;/p&gt; &lt;p&gt;RAG: embeded Ollama model.&lt;/p&gt; &lt;p&gt;Scrollable window, big font, darkmode, easy to copy what LLM says. Possibility to save chats, good prompt system to let the LLM know what is expected.&lt;/p&gt; &lt;p&gt;What would be over the board would be a User info, where one could provide LLM with one's name, preferred language, and tone of conversation.&lt;/p&gt; &lt;p&gt;And the possibility to add json file to create a json for the project the LLM is helping, or fool proofing. Yesterday QwQ suggested to me that a good way to fool proof a text in a collaborative way would look like this: ### **3. Foolproofing UI Ideas for Language Precision** &lt;/p&gt; &lt;p&gt;To handle dialects/characters/neologisms interactively: &lt;/p&gt; &lt;p&gt;- **Tier 1:** A simple JSON-style &amp;quot;style sheet&amp;quot; you maintain with rules &lt;/p&gt; &lt;p&gt;(e.g., *&amp;quot;[Character X] says 'gonna' instead of 'going to'; avoids &lt;/p&gt; &lt;p&gt;contractions&amp;quot;*). Share this once, and I‚Äôll reference it. &lt;/p&gt; &lt;p&gt;- **Tier 2:** Use a markdown-based feedback loop: &lt;/p&gt; &lt;p&gt;```markdown&lt;/p&gt; &lt;p&gt;## Character Profile &lt;/p&gt; &lt;p&gt;- Name: Zara &lt;/p&gt; &lt;p&gt;- Dialect: Bostonian accent (&amp;quot;parkin‚Äô lot&amp;quot;) &lt;/p&gt; &lt;p&gt;- Neologism: &amp;quot;frizzle&amp;quot; = chaotic excitement &lt;/p&gt; &lt;p&gt;## Your Text: &lt;/p&gt; &lt;p&gt;&amp;quot;[Zara] said, 'Let‚Äôs frizzle at the parkin‚Äô lot!'&amp;quot; &lt;/p&gt; &lt;p&gt;## My Suggestion? &lt;/p&gt; &lt;p&gt;[Yes/No/Adjust: ________________________]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brandu33"&gt; /u/Brandu33 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7upat/ollama_tts_stt_no_cloud_or_api_paying_keys/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7upat/ollama_tts_stt_no_cloud_or_api_paying_keys/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7upat/ollama_tts_stt_no_cloud_or_api_paying_keys/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T09:25:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j87bzd</id>
    <title>How to fix Ollama outputting responses with bad spacing?</title>
    <updated>2025-03-10T19:38:19+00:00</updated>
    <author>
      <name>/u/mccow67</name>
      <uri>https://old.reddit.com/user/mccow67</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j87bzd/how_to_fix_ollama_outputting_responses_with_bad/"&gt; &lt;img alt="How to fix Ollama outputting responses with bad spacing?" src="https://b.thumbs.redditmedia.com/J7l-GeJIkrcj0thHiB1YtkrxRWt9xO5KIZ0qq3kFiEw.jpg" title="How to fix Ollama outputting responses with bad spacing?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically, I have started a project. It's an AI interface to chat with Ollama models, but it all goes via my self-made GPU :D. Sadly, the responses from the LLM in the HTML code are terrible. They look like (given screenshot)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/woz6ttb00xne1.png?width=1246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=475e9ddaa12b00cb2e2c18c0ba1addd9aa2cbe54"&gt;https://preview.redd.it/woz6ttb00xne1.png?width=1246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=475e9ddaa12b00cb2e2c18c0ba1addd9aa2cbe54&lt;/a&gt;&lt;/p&gt; &lt;p&gt;2 bullet points I want to know:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;How do I fix proper spacing in between of bullet points etc? In the CLI version of Ollama, the spacing DOES exist.&lt;/li&gt; &lt;li&gt;How do I render markdown if the text is not initally there? I am aware that this might not be the right channel, but still: if you know it, please tell me! That includes LaTeX Math Equation rendering. Because the text is of course getting rendered in chunks.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any help would be greatly appreciated!&lt;/p&gt; &lt;p&gt;P.S. I'm 14 years old and just got obsessed with AI's. Please don't expect me to know everything already.&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; I'm using Node.js. This might change the thing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mccow67"&gt; /u/mccow67 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j87bzd/how_to_fix_ollama_outputting_responses_with_bad/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j87bzd/how_to_fix_ollama_outputting_responses_with_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j87bzd/how_to_fix_ollama_outputting_responses_with_bad/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T19:38:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j87nei</id>
    <title>Basic LLM performance testing of A100, RTX A6000, H100, H200 Spot GPU instances from DataCrunch</title>
    <updated>2025-03-10T19:52:13+00:00</updated>
    <author>
      <name>/u/olegsmith7</name>
      <uri>https://old.reddit.com/user/olegsmith7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j87nei/basic_llm_performance_testing_of_a100_rtx_a6000/"&gt; &lt;img alt="Basic LLM performance testing of A100, RTX A6000, H100, H200 Spot GPU instances from DataCrunch" src="https://b.thumbs.redditmedia.com/U6Cl4e55WuO1awTEvPao4sFK8IYedHOPJq7lu7eU8gA.jpg" title="Basic LLM performance testing of A100, RTX A6000, H100, H200 Spot GPU instances from DataCrunch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I benchmarked Rackspace Spot Kubernetes nodes with A30 and H100 GPUs for self-hosting LLMs last month. Yesterday, I conducted a similar assessment of A100, RTX A6000, H100, and H200 GPU-powered VMs from DataCrunch. Performance test results indicate the following findings:&lt;/p&gt; &lt;p&gt;- Based on cost per token per second (tps) per hour, the most cost-effective options are: Nvidia A100 40GB VRAM for 32b models (‚Ç¨0.1745/hour) and Nvidia H100 80GB VRAM for 70b models (‚Ç¨0.5180/hour)&lt;/p&gt; &lt;p&gt;- Token throughput (tokens per second) scales almost proportionally with model size: a 32b model (20GB size) yields twice the number of tokens per second compared to a 70b model (43GB size).&lt;/p&gt; &lt;p&gt;- H200 doesn't provide better single-conversation performance than H100, but it should show better overall throughput performance for multi-conversation load across multiple NVLinked H200 (e.g. 4x 8H200).&lt;/p&gt; &lt;p&gt;- New qwq:32b model a bit slower than qwen2.5-coder:32b in terms of token throughput.&lt;/p&gt; &lt;p&gt;- DataCrunch offers better prices than Rackspace Spot&lt;/p&gt; &lt;p&gt;read more &lt;a href="https://oleg.smetan.in/posts/2025-03-09-datacrunch-spot-llm-performance-test"&gt;https://oleg.smetan.in/posts/2025-03-09-datacrunch-spot-llm-performance-test&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ps8n078z2xne1.png?width=3644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=110130fb3c9d7706472c9d322337208279663e5a"&gt;https://preview.redd.it/ps8n078z2xne1.png?width=3644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=110130fb3c9d7706472c9d322337208279663e5a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/olegsmith7"&gt; /u/olegsmith7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j87nei/basic_llm_performance_testing_of_a100_rtx_a6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j87nei/basic_llm_performance_testing_of_a100_rtx_a6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j87nei/basic_llm_performance_testing_of_a100_rtx_a6000/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T19:52:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7ntrn</id>
    <title>I Fine-Tuned a Tiny LLM to Write Git Commits Offline‚ÄîCheck It Out!</title>
    <updated>2025-03-10T01:56:46+00:00</updated>
    <author>
      <name>/u/VictorCTavernari</name>
      <uri>https://old.reddit.com/user/VictorCTavernari</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good evening, Ollama community!&lt;/p&gt; &lt;p&gt;I've been an enthusiast of local open-source LLMs for about a year now. Typically, I prefer keeping my git commits small with clear, meaningful messages, especially when working with others. When ChatGPT launched GPTs, I created a dedicated model for writing commit messages: &lt;a href="https://chatgpt.com/g/g-1RdmhTAHg-git-commit-message-pro"&gt;Git Commit Message Pro&lt;/a&gt;. However, I encountered some privacy limitations, which led me to explore fine-tuning my own local LLM that could produce an initial draft requiring minimal edits. Using Ollama, I built &lt;a href="https://ollama.com/tavernari/git-commit-message"&gt;tavernari/git-commit-message&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;tavernari/git-commit-message&lt;/h1&gt; &lt;p&gt;In my first version, I used the 7B Mistral model, which occupies about 4.4 GB. While functional, it was resource-intensive and often produced slow and unsatisfactory responses.&lt;/p&gt; &lt;p&gt;Recently, there has been considerable hype around DeepSeekR1, a smaller model trained to &amp;quot;think&amp;quot; more effectively. Inspired by this, I created a smaller, reasoning-focused version dedicated specifically to writing commit messages.&lt;/p&gt; &lt;p&gt;This was my first attempt at fine-tuning. Although the results aren't perfect yet, I believe that with further training and refinement, I can achieve better outcomes.&lt;/p&gt; &lt;p&gt;Hence, I introduced the &amp;quot;reasoning&amp;quot; version: &lt;a href="https://ollama.com/tavernari/git-commit-message:reasoning"&gt;tavernari/git-commit-message:reasoning&lt;/a&gt;. This version uses a small 3B model (1.9 GB) optimized for enhanced reasoning capabilities. Additionally, I developed another version leveraging Chain of Thought (&lt;a href="https://arxiv.org/pdf/2502.18600"&gt;Chain of Thought&lt;/a&gt;), which also showed promising results, though it hasn't been deeply explored yet.&lt;/p&gt; &lt;h1&gt;Agentic Git Commit Message&lt;/h1&gt; &lt;p&gt;Despite its decent performance, the model struggled with larger contexts. To address this, I created an agentic bash script that incrementally evaluates git diffs, helping the LLM generate commits without losing context.&lt;/p&gt; &lt;p&gt;Script functionalities include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adding context to improve commit message quality.&lt;/li&gt; &lt;li&gt;Editing the generated message before committing.&lt;/li&gt; &lt;li&gt;Generating only the message with the --only-message option.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Installation is straightforward and explained on the model‚Äôs profile page: &lt;a href="https://ollama.com/tavernari/git-commit-message:reasoning"&gt;tavernari/git-commit-message:reasoning&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Project Goal&lt;/h1&gt; &lt;p&gt;My goal is to provide commit messages that are sufficiently good, needing only minor manual adjustments, and most importantly, functioning completely offline to ensure your intellectual work remains secure and private.&lt;/p&gt; &lt;p&gt;I've invested some financial resources into the fine-tuning process, aiming ultimately to create something beneficial for the community. In the future, I'll continue dedicating time to training and refining the model to enhance its quality.&lt;/p&gt; &lt;p&gt;The idea is to offer a practical, efficient tool that prioritizes the security and privacy of your work.&lt;/p&gt; &lt;p&gt;Feel free to use, suggest improvements, and collaborate!&lt;/p&gt; &lt;p&gt;My HuggingFace: &lt;a href="https://huggingface.co/Tavernari/git-commit-message"&gt;https://huggingface.co/Tavernari/git-commit-message&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VictorCTavernari"&gt; /u/VictorCTavernari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7ntrn/i_finetuned_a_tiny_llm_to_write_git_commits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7ntrn/i_finetuned_a_tiny_llm_to_write_git_commits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7ntrn/i_finetuned_a_tiny_llm_to_write_git_commits/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T01:56:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8a6z5</id>
    <title>How to test an AMD Instinct Mi50/Mi60 GPU</title>
    <updated>2025-03-10T21:37:39+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x91gel5sfxne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8a6z5/how_to_test_an_amd_instinct_mi50mi60_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j8a6z5/how_to_test_an_amd_instinct_mi50mi60_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T21:37:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j883k5</id>
    <title>Fine tuning ollama model</title>
    <updated>2025-03-10T20:11:16+00:00</updated>
    <author>
      <name>/u/Snoo_44191</name>
      <uri>https://old.reddit.com/user/Snoo_44191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys I am using QWQ 32B with crew ai locally on my RTX A6000 48GB Vram GPU. The crew hallucinates a lot at most of the times , mainly while tool calling and also sometimes in normal tasks . I have edited the model file and set num ctx to 16000 , still i dont get a stable streamlined output , it changes after each iteration ! (My prompts are perfect as they work awesome with open ai or Gemini api&amp;quot;s) I was suggested by one redditor to fine tune the model for crew ai , but i am not able to understand how to craft the dataset , what should it exactly be ? So that the model learns to call tools better and interact with crewai better ? &lt;/p&gt; &lt;p&gt;Any help on this would be extremely relieving!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snoo_44191"&gt; /u/Snoo_44191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j883k5/fine_tuning_ollama_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j883k5/fine_tuning_ollama_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j883k5/fine_tuning_ollama_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T20:11:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j82dfc</id>
    <title>Ollama + Apple Notes - I built ChatGPT for Apple Notes</title>
    <updated>2025-03-10T16:14:58+00:00</updated>
    <author>
      <name>/u/arne226</name>
      <uri>https://old.reddit.com/user/arne226</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j82dfc/ollama_apple_notes_i_built_chatgpt_for_apple_notes/"&gt; &lt;img alt="Ollama + Apple Notes - I built ChatGPT for Apple Notes" src="https://external-preview.redd.it/MnNtYmp0NDUwd25lMR9ljqoPJ2qXFOb1yIG9LCnvszZJql-YtrXAxdd8XVVd.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7be12c0a6c664fe32deb128ec7777f0e93665081" title="Ollama + Apple Notes - I built ChatGPT for Apple Notes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arne226"&gt; /u/arne226 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9yo1vx450wne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j82dfc/ollama_apple_notes_i_built_chatgpt_for_apple_notes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j82dfc/ollama_apple_notes_i_built_chatgpt_for_apple_notes/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T16:14:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8ef9z</id>
    <title>How do I train an untrained AI?</title>
    <updated>2025-03-11T00:45:30+00:00</updated>
    <author>
      <name>/u/AnaverageuserX</name>
      <uri>https://old.reddit.com/user/AnaverageuserX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With untrained AIs do I just feed them random Text-Based datasets with the desired language/intel I want? Or do I feed them other stuff like random numbers? I'm using the Msty App with the Model &amp;quot;untrained-suave-789.IQ3_S-1741651430874:latest&amp;quot; and am curious on how to train it to well.. Not speak gibberish.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnaverageuserX"&gt; /u/AnaverageuserX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8ef9z/how_do_i_train_an_untrained_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8ef9z/how_do_i_train_an_untrained_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j8ef9z/how_do_i_train_an_untrained_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-11T00:45:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8egbd</id>
    <title>Openmanus+ollama</title>
    <updated>2025-03-11T00:46:54+00:00</updated>
    <author>
      <name>/u/Choice_Complaint9171</name>
      <uri>https://old.reddit.com/user/Choice_Complaint9171</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone accomplished openmanus ollama and webui on windows &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Choice_Complaint9171"&gt; /u/Choice_Complaint9171 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8egbd/openmanusollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8egbd/openmanusollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j8egbd/openmanusollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-11T00:46:54+00:00</published>
  </entry>
</feed>
