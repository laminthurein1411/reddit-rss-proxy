<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-14T08:07:42+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1io3c9b</id>
    <title>ollama and ollama_host variable</title>
    <updated>2025-02-12T22:12:38+00:00</updated>
    <author>
      <name>/u/DifficultTomatillo29</name>
      <uri>https://old.reddit.com/user/DifficultTomatillo29</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have the most annoying little problem with ollama. I'm on a Mac - and I use it to host ollama for a pc, the pc not having any gpu to speak of. If I kill ollama, export the environment variable OLLAMA_HOST &amp;quot;0.0.0.0:11434&amp;quot; - and run ollama serve - everything works.. but if I run ollama by just clicking on the app - there's no way to inject the environment variable - and I can't find any way to just globally set ollama to always run like that - is there some sort of .config file or something that ollama supports - &lt;/p&gt; &lt;p&gt;ie I don't want it to be possible on my machine to run ollama and _not_ have it listen to other machines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DifficultTomatillo29"&gt; /u/DifficultTomatillo29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io3c9b/ollama_and_ollama_host_variable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io3c9b/ollama_and_ollama_host_variable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1io3c9b/ollama_and_ollama_host_variable/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T22:12:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1io8qdu</id>
    <title>Best Model for Assisting with Novel Writing</title>
    <updated>2025-02-13T02:24:12+00:00</updated>
    <author>
      <name>/u/robinhoodrefugee</name>
      <uri>https://old.reddit.com/user/robinhoodrefugee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, my use case is getting help with writing a full length novel (75,000 - 100,000+ characters). The idea is not to have the LLM write text for me. I want to be feeding my own writing in, along with plot devices, character traits, setting information, conflicts, arcs, themes, etc., so that I can then query it later down the line and ensure I'm consistent in my writing. For example, &amp;quot;when John reveals where the money is, does the location make sense?&amp;quot;&lt;/p&gt; &lt;p&gt;ChatGPT has memory issues remembering this much text so I am turning to offline LLMs. I just installed Ollama. I tried installing deepseek-r1:7b but the install progress kept going up and down and it never completed. It got up to like 2% install (peaked at 130mb out of 4.7gb) and then actually went back down to 0%. It did this multiple times before I finally gave up.&lt;/p&gt; &lt;p&gt;Here are my specs: GPU: Intel UHD Graphics 620 and 1.17TB free of hard disc space out of 1.81TB. I have 32GB of RAM.&lt;/p&gt; &lt;p&gt;Can someone recommend a model that will meet my needs and specs? Again, I want it to be able to remember everything I tell it about my story, so I'm not sure what's going to be appropriate for this use case. I am brand new to LLMs besides ChatGPT which I've been using for less than six months.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robinhoodrefugee"&gt; /u/robinhoodrefugee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io8qdu/best_model_for_assisting_with_novel_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io8qdu/best_model_for_assisting_with_novel_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1io8qdu/best_model_for_assisting_with_novel_writing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T02:24:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1io1hjo</id>
    <title>URL to screenshots server for your self-hosted AI projects (MIT license)</title>
    <updated>2025-02-12T20:54:56+00:00</updated>
    <author>
      <name>/u/gkamer8</name>
      <uri>https://old.reddit.com/user/gkamer8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1io1hjo/url_to_screenshots_server_for_your_selfhosted_ai/"&gt; &lt;img alt="URL to screenshots server for your self-hosted AI projects (MIT license)" src="https://external-preview.redd.it/k5m2RHvy9bfmZbcB7x2-uONoP8GvfJMHaFfCG-IoXcw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3ba3c756d03047423ca7edf67a7a613af44994a" title="URL to screenshots server for your self-hosted AI projects (MIT license)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gkamer8"&gt; /u/gkamer8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/goodreasonai/ScrapeServ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io1hjo/url_to_screenshots_server_for_your_selfhosted_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1io1hjo/url_to_screenshots_server_for_your_selfhosted_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T20:54:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioh4d2</id>
    <title>Questions about context size</title>
    <updated>2025-02-13T11:35:19+00:00</updated>
    <author>
      <name>/u/browndragon456</name>
      <uri>https://old.reddit.com/user/browndragon456</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I apologize in advance for asking this questions but after spending some time searching through I don't think I'm any closer to understanding conclusively. Can you please tell me if there is a context limit that I should be aware of other than the context size of a model? Like if I start using the chat completion end point and I start passing the messages array do I have to worry about passing a particular context window limit or something or will it stick to whatever the model allows it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/browndragon456"&gt; /u/browndragon456 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioh4d2/questions_about_context_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioh4d2/questions_about_context_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ioh4d2/questions_about_context_size/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T11:35:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1inp06h</id>
    <title>AMD 395 can run llama 70B without GPU</title>
    <updated>2025-02-12T11:51:44+00:00</updated>
    <author>
      <name>/u/grigio</name>
      <uri>https://old.reddit.com/user/grigio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Non enough but a good start&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/AMDGPU_/status/1889588690214637747"&gt;https://x.com/AMDGPU_/status/1889588690214637747&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grigio"&gt; /u/grigio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inp06h/amd_395_can_run_llama_70b_without_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inp06h/amd_395_can_run_llama_70b_without_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inp06h/amd_395_can_run_llama_70b_without_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T11:51:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1iokmls</id>
    <title>Running model using api</title>
    <updated>2025-02-13T14:46:34+00:00</updated>
    <author>
      <name>/u/kerneleus</name>
      <uri>https://old.reddit.com/user/kerneleus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any description of how model is loading into memory when you are running api request on it? What will happen if i use two different models on same ollama instance. Will it be unloaded after some time of inactivity?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kerneleus"&gt; /u/kerneleus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iokmls/running_model_using_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iokmls/running_model_using_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iokmls/running_model_using_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T14:46:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1io31bf</id>
    <title>Promptable object tracking robot, built with Moondream &amp; OpenCV Optical Flow (open source)</title>
    <updated>2025-02-12T22:00:02+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1io31bf/promptable_object_tracking_robot_built_with/"&gt; &lt;img alt="Promptable object tracking robot, built with Moondream &amp;amp; OpenCV Optical Flow (open source)" src="https://external-preview.redd.it/dHlrejFmeXk1c2llMTEDy-zmwY-2zxEHn6L-Fnq1X838PMp4mnmxIFCi0bu_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7dd1c2a72530eab6a7d43b6046676f4d888b4d96" title="Promptable object tracking robot, built with Moondream &amp;amp; OpenCV Optical Flow (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vy021eyy5sie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io31bf/promptable_object_tracking_robot_built_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1io31bf/promptable_object_tracking_robot_built_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T22:00:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1iomyy2</id>
    <title>Single core utilization with 4 GPU, could it be better?</title>
    <updated>2025-02-13T16:28:16+00:00</updated>
    <author>
      <name>/u/Puzzleheaded_Wait770</name>
      <uri>https://old.reddit.com/user/Puzzleheaded_Wait770</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iomyy2/single_core_utilization_with_4_gpu_could_it_be/"&gt; &lt;img alt="Single core utilization with 4 GPU, could it be better?" src="https://b.thumbs.redditmedia.com/MXj2r7WWaSu_1hncMJjhlyb62IA53Z4oiw2tZgZGbrM.jpg" title="Single core utilization with 4 GPU, could it be better?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I am trying to use qwen2.5-coder:32b instead of ChatGPT :)&lt;br /&gt; My config are HP DL380 G9 with dual E5-2690 v4, 512GB RAM, Intel NVMe and NVIDIA M10 with 32GB of RAM (it is actually 4 gpus with 8gb of VRAM)&lt;/p&gt; &lt;p&gt;Looks desent, by I've only got &lt;strong&gt;1.63 token/s&lt;/strong&gt;. When I tried to troubleshoot my problem, I found that for some reason, Ollama does not utilize GPU on 100%, even more, it uses only 1 cpu core&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9hvsk54llxie1.png?width=1915&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43dce3476fbe2d6d32ca79722e03412fcebae2dd"&gt;(htop + nvtop during ollama run)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Is there anyway to improve token/s values? I tried to tweak batch size, but it does not help much.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Puzzleheaded_Wait770"&gt; /u/Puzzleheaded_Wait770 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iomyy2/single_core_utilization_with_4_gpu_could_it_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iomyy2/single_core_utilization_with_4_gpu_could_it_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iomyy2/single_core_utilization_with_4_gpu_could_it_be/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T16:28:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1iopgvh</id>
    <title>Python code check</title>
    <updated>2025-02-13T18:12:45+00:00</updated>
    <author>
      <name>/u/engineer_dennis</name>
      <uri>https://old.reddit.com/user/engineer_dennis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: Is there a way to get a wholistic review of a Python project?&lt;/p&gt; &lt;p&gt;I need help with my Python project. Over the years, I‚Äôve changed and updated parts of it, expanding and bug fixing it. At this point, I don‚Äôt remember reasoning behind many decisions that a less experienced me made.&lt;/p&gt; &lt;p&gt;Is there a way to AI review the whole project and get exact steps on improving it? Not just ‚Äúuse type hints‚Äù, but ‚Äú&amp;lt;this function&amp;gt; needs the following type hints, while &amp;lt;that function&amp;gt; can drop half the parameters‚Äù.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/engineer_dennis"&gt; /u/engineer_dennis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iopgvh/python_code_check/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iopgvh/python_code_check/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iopgvh/python_code_check/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T18:12:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1iopr6y</id>
    <title>Reading the response in python to ollama chet gets error Message</title>
    <updated>2025-02-13T18:24:41+00:00</updated>
    <author>
      <name>/u/jrendant</name>
      <uri>https://old.reddit.com/user/jrendant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;response = ollama.chat( model='llama3.2-vision:90b', messages=[{ 'role': 'user', 'content': promptAI, 'images': [os.path.join(root, file)] }] ) here is request to access the content of the response which returns an error - repstr = response['messages']['content'] I am a newbie please help &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jrendant"&gt; /u/jrendant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iopr6y/reading_the_response_in_python_to_ollama_chet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iopr6y/reading_the_response_in_python_to_ollama_chet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iopr6y/reading_the_response_in_python_to_ollama_chet/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T18:24:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1inx1k8</id>
    <title>Ollama on mini PC Intel Ultra 5</title>
    <updated>2025-02-12T17:54:47+00:00</updated>
    <author>
      <name>/u/Parenormale</name>
      <uri>https://old.reddit.com/user/Parenormale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1inx1k8/ollama_on_mini_pc_intel_ultra_5/"&gt; &lt;img alt="Ollama on mini PC Intel Ultra 5" src="https://preview.redd.it/corfx7mcyqie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e5d130a0a99c468aaeed36474580d1582b3e45e" title="Ollama on mini PC Intel Ultra 5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;with arc and ipex-llm I feel like an alien in the AI ‚Äã‚Äãllm context I spent ‚Ç¨600 it's mini it consumes 50w it flies and it's precise, here I published all my tests with the various language models &lt;/p&gt; &lt;p&gt;I think the performance is great for this little GPU accelerated PC.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtube.com/@onlyrottami?si=MSYffeaGo0axCwh9"&gt;https://youtube.com/@onlyrottami?si=MSYffeaGo0axCwh9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Parenormale"&gt; /u/Parenormale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/corfx7mcyqie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inx1k8/ollama_on_mini_pc_intel_ultra_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inx1k8/ollama_on_mini_pc_intel_ultra_5/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T17:54:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioodvn</id>
    <title>Has anyone deployed on Nebius cloud?</title>
    <updated>2025-02-13T17:27:24+00:00</updated>
    <author>
      <name>/u/dizzydes</name>
      <uri>https://old.reddit.com/user/dizzydes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious how they compare to my current stack on GCP as they claim to be fully specialised&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dizzydes"&gt; /u/dizzydes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioodvn/has_anyone_deployed_on_nebius_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioodvn/has_anyone_deployed_on_nebius_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ioodvn/has_anyone_deployed_on_nebius_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T17:27:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioyg55</id>
    <title>Run RAG based on r1 locally, but r1 slow</title>
    <updated>2025-02-14T00:50:24+00:00</updated>
    <author>
      <name>/u/Gold-Independent-792</name>
      <uri>https://old.reddit.com/user/Gold-Independent-792</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyOne&lt;/p&gt; &lt;p&gt;I‚Äôm struggling with &lt;strong&gt;slow inference speeds&lt;/strong&gt; while running DeepSeek-7B/14B on Ollama. Here‚Äôs my setup and what I‚Äôve tried:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU: i7-11th Gen (8 threads)&lt;/li&gt; &lt;li&gt;RAM: 16GB DDR4&lt;/li&gt; &lt;li&gt;GPU: Intel Iris Xe (integrated)&lt;/li&gt; &lt;li&gt;OS: Windows 11&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Current Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Using &lt;strong&gt;Ollama&lt;/strong&gt; (no &lt;code&gt;llama.cpp&lt;/code&gt;) with default settings.&lt;/li&gt; &lt;li&gt;Model: &lt;code&gt;deepseek-7b&lt;/code&gt; (and tried &lt;code&gt;deepseek-14b&lt;/code&gt; but OOM).&lt;/li&gt; &lt;li&gt;Quantization: None (vanilla Ollama installation).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Symptoms:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt;: ~10-15 seconds per token for 7B, unusable for 14B.&lt;/li&gt; &lt;li&gt;RAM maxes out, leading to swapping (disk usage spikes).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Are there &lt;strong&gt;Ollama-specific flags&lt;/strong&gt; to optimize CPU inference?&lt;/li&gt; &lt;li&gt;How to &lt;strong&gt;quantize DeepSeek models&lt;/strong&gt; properly for Ollama?&lt;/li&gt; &lt;li&gt;Can Intel Iris Xe help? I saw &lt;code&gt;OLLAMA_GPU_LAYERS&lt;/code&gt; but unsure if Ollama supports Intel iGPU offloading.&lt;/li&gt; &lt;li&gt;Is 16GB RAM fundamentally insufficient for 7B/14B on Ollama?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And if you have please any suggestion to improve or add something, thank you so much&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gold-Independent-792"&gt; /u/Gold-Independent-792 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioyg55/run_rag_based_on_r1_locally_but_r1_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioyg55/run_rag_based_on_r1_locally_but_r1_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ioyg55/run_rag_based_on_r1_locally_but_r1_slow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T00:50:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ior8yo</id>
    <title>Running vision model or mixed modal model?</title>
    <updated>2025-02-13T19:27:23+00:00</updated>
    <author>
      <name>/u/CaptainCapitol</name>
      <uri>https://old.reddit.com/user/CaptainCapitol</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im trying to learn what I need to run a vision model, to interpret images, as well as just a language model so i can use it for various things. But I am having issues figuring out what I can get away with running the things on.&lt;/p&gt; &lt;p&gt;i don't mind spending some money, but i just can't figure out what I need.&lt;/p&gt; &lt;p&gt;I don't need a hyper modern big setup, but i do want it to answer somewhat fast. &lt;/p&gt; &lt;p&gt;Any suggestions?&lt;/p&gt; &lt;p&gt;I am not US based, so all these microcenter deals or cheap used things, i can't get those. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CaptainCapitol"&gt; /u/CaptainCapitol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ior8yo/running_vision_model_or_mixed_modal_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ior8yo/running_vision_model_or_mixed_modal_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ior8yo/running_vision_model_or_mixed_modal_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T19:27:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iovkc9</id>
    <title>Looking for budget recommendation for GPU 6800xt vs 4060 Ti 16GB vs Quadro RTX 5000</title>
    <updated>2025-02-13T22:33:04+00:00</updated>
    <author>
      <name>/u/Intelligent-Elk-4253</name>
      <uri>https://old.reddit.com/user/Intelligent-Elk-4253</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I recently got up and running with ollama on a Tesla M40 with qwen2.5-coder:32b. I'm pretty happy with the setup but I'd like to be able to help speed things up slightly if possible as right now I'm getting about 7 tokens a second with a 8K context window.&lt;/p&gt; &lt;p&gt;I have a hard limit of $450 and I'm eyeing three card types on ebay. They are the 6800xt, the 4060ti 16GB and the Quadro RTX 5000. On paper the 6800xt looks like it should be the most performant but I understand that AMD's ai support isn't as good as Nvidia. Assuming the 6800xt isn't a good option should I look at the Quadro over the 4060ti?&lt;/p&gt; &lt;p&gt;The end result would be to run whatever card is purchased along side the M40.&lt;/p&gt; &lt;p&gt;Thank you for any insights.&lt;/p&gt; &lt;p&gt;6800 xt specs&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/gpu-specs/radeon-rx-6800-xt.c3694"&gt;https://www.techpowerup.com/gpu-specs/radeon-rx-6800-xt.c3694&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4060 Ti&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/gpu-specs/geforce-rtx-4060-ti-16-gb.c4155"&gt;https://www.techpowerup.com/gpu-specs/geforce-rtx-4060-ti-16-gb.c4155&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quadro RTX 5000&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/gpu-specs/quadro-rtx-5000.c3308"&gt;https://www.techpowerup.com/gpu-specs/quadro-rtx-5000.c3308&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Current server specs&lt;/p&gt; &lt;p&gt;CPU: AMD 5950x&lt;/p&gt; &lt;p&gt;RAM: 64GB DDR 4 32000&lt;/p&gt; &lt;p&gt;OS: Proxmox 8.3&lt;/p&gt; &lt;p&gt;Layout: Physical host ---&amp;gt; Proxmov ---&amp;gt; VM ---&amp;gt; Docker ---&amp;gt; Ollama&lt;/p&gt; &lt;p&gt;\---Tesla M40 ---------------^&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Intelligent-Elk-4253"&gt; /u/Intelligent-Elk-4253 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iovkc9/looking_for_budget_recommendation_for_gpu_6800xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iovkc9/looking_for_budget_recommendation_for_gpu_6800xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iovkc9/looking_for_budget_recommendation_for_gpu_6800xt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T22:33:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iokj6t</id>
    <title>Run Ollama on Intel Core Ultra and GPU using IPEX-LLM portable zip</title>
    <updated>2025-02-13T14:42:28+00:00</updated>
    <author>
      <name>/u/bigbigmind</name>
      <uri>https://old.reddit.com/user/bigbigmind</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using the IPEX-LLM portable zip, it‚Äôs now extremely easy to run Ollama on Intel Core Ultra and GPU: &lt;a href="https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portablze_zip_quickstart.md"&gt;https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portablze_zip_quickstart.md&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Download &amp;amp; unzip&lt;/li&gt; &lt;li&gt;Run `start-ollama.bat`&lt;/li&gt; &lt;li&gt;Run `ollama run deepseek-r1` in a command window&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigbigmind"&gt; /u/bigbigmind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iokj6t/run_ollama_on_intel_core_ultra_and_gpu_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iokj6t/run_ollama_on_intel_core_ultra_and_gpu_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iokj6t/run_ollama_on_intel_core_ultra_and_gpu_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T14:42:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioyiyw</id>
    <title>Ollama 0.5.9 Update make my CPU inference slower</title>
    <updated>2025-02-14T00:54:30+00:00</updated>
    <author>
      <name>/u/Signal_Kiwi_9737</name>
      <uri>https://old.reddit.com/user/Signal_Kiwi_9737</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;Just updated Ollama from 0.5.7 &amp;gt; 0.5.9 and run my favorite LLM and noticed major performance drop on my dual Xeon 6126 setup. Went from ~3 t/s down to ~2 t/s. This is not great for me... Just to be sure this is correct I downgraded Ollama back to 0.5.7 and performance is restored! &lt;/p&gt; &lt;p&gt;Both of my CPUs have AVX512 instructions however it seems that using those instructions can in fact slows down inference performance?? I'm confused on this one... can some one explain this to me :)&lt;/p&gt; &lt;p&gt;My system is a Fujitsu RM2530 M4 1U server, dual Xeon 6126 with 384GB ram, no GPU and NUMA disabled. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Signal_Kiwi_9737"&gt; /u/Signal_Kiwi_9737 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioyiyw/ollama_059_update_make_my_cpu_inference_slower/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioyiyw/ollama_059_update_make_my_cpu_inference_slower/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ioyiyw/ollama_059_update_make_my_cpu_inference_slower/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T00:54:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip54oj</id>
    <title>How do you use console AI?</title>
    <updated>2025-02-14T07:16:51+00:00</updated>
    <author>
      <name>/u/Big-Relative-349</name>
      <uri>https://old.reddit.com/user/Big-Relative-349</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I'm an aspiring comic artist. I‚Äôve been experimenting with various AI models on Ollama to manage my worldbuilding database, but so far, all I‚Äôve gotten are unpredictable responses rather than anything truly useful. The only real takeaway has been learning some basic CMD and PowerShell commands.&lt;/p&gt; &lt;p&gt;My PC can run AI models up to 14B smoothly, but anything from 32B onward starts to lag. I thought my &lt;strong&gt;4060 Ti&lt;/strong&gt; would be the perfect GPU for this, but apparently, I was wrong.&lt;/p&gt; &lt;p&gt;How can I use these AI models in a way that‚Äôs actually useful to me and ensures at least somewhat predictable responses?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big-Relative-349"&gt; /u/Big-Relative-349 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip54oj/how_do_you_use_console_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip54oj/how_do_you_use_console_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ip54oj/how_do_you_use_console_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T07:16:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1iofspy</id>
    <title>Possible 32GB AMD GPU</title>
    <updated>2025-02-13T10:01:44+00:00</updated>
    <author>
      <name>/u/GhostInThePudding</name>
      <uri>https://old.reddit.com/user/GhostInThePudding</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Well this is promising:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=NIUtyzuFFOM"&gt;https://www.youtube.com/watch?v=NIUtyzuFFOM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Leaks show the 9070XT may be a 32GB GPU for under US$1000. Which means if it works well with AI, it could be the ultimate home user GPU available, particularly for Linux users. I hope it doesn't suck!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GhostInThePudding"&gt; /u/GhostInThePudding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iofspy/possible_32gb_amd_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iofspy/possible_32gb_amd_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iofspy/possible_32gb_amd_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T10:01:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip5g9v</id>
    <title>Is there a deepseek r1 zero?</title>
    <updated>2025-02-14T07:40:18+00:00</updated>
    <author>
      <name>/u/wahnsinnwanscene</name>
      <uri>https://old.reddit.com/user/wahnsinnwanscene</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there one that can be used?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wahnsinnwanscene"&gt; /u/wahnsinnwanscene &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip5g9v/is_there_a_deepseek_r1_zero/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip5g9v/is_there_a_deepseek_r1_zero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ip5g9v/is_there_a_deepseek_r1_zero/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T07:40:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioot2m</id>
    <title>OpenThinker:32b</title>
    <updated>2025-02-13T17:45:08+00:00</updated>
    <author>
      <name>/u/GVDub2</name>
      <uri>https://old.reddit.com/user/GVDub2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just loaded up this one. Incredibly complex reasoning process, followed by an extraordinarily terse response. I'll have to go look at the GitHub to see what's going on, as it insists on referring to itself in the third person (&amp;quot;the assistant&amp;quot;). An interesting one, but not a fast response. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GVDub2"&gt; /u/GVDub2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioot2m/openthinker32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioot2m/openthinker32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ioot2m/openthinker32b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T17:45:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1iog9ky</id>
    <title>Challenge! Decode image to JSON</title>
    <updated>2025-02-13T10:35:55+00:00</updated>
    <author>
      <name>/u/dxcore_35</name>
      <uri>https://old.reddit.com/user/dxcore_35</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iog9ky/challenge_decode_image_to_json/"&gt; &lt;img alt="Challenge! Decode image to JSON" src="https://preview.redd.it/3fs8e0uxwvie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7de0101dea023e12442d9a1453e885505037768b" title="Challenge! Decode image to JSON" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dxcore_35"&gt; /u/dxcore_35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3fs8e0uxwvie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iog9ky/challenge_decode_image_to_json/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iog9ky/challenge_decode_image_to_json/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T10:35:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip0rx5</id>
    <title>Best way to self host open source LLM‚Äôs on GCP</title>
    <updated>2025-02-14T02:52:52+00:00</updated>
    <author>
      <name>/u/addimo</name>
      <uri>https://old.reddit.com/user/addimo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have some free credit on google cloud, thinking about using google cloud run with ollama, or vertex ai as they seems to be the simplest to run. But I am not sure if there is a better way on GCP maybe less costly ones‚Ä¶does anyone have experience self hosting on gcp ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/addimo"&gt; /u/addimo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip0rx5/best_way_to_self_host_open_source_llms_on_gcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip0rx5/best_way_to_self_host_open_source_llms_on_gcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ip0rx5/best_way_to_self_host_open_source_llms_on_gcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T02:52:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioyxkm</id>
    <title>How to do proper function calling on Ollama models</title>
    <updated>2025-02-14T01:15:16+00:00</updated>
    <author>
      <name>/u/hervalfreire</name>
      <uri>https://old.reddit.com/user/hervalfreire</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fellow Llamas,&lt;/p&gt; &lt;p&gt;I've been spending some time trying to develop some fully-offline projects using local LLMs, and stumbled upon a bit of a wall. Essentially, I'm trying to use tool calling with a local model, and failing with pretty much all of them.&lt;/p&gt; &lt;p&gt;The test is simple:&lt;/p&gt; &lt;p&gt;- there's a function for listing files in a directory&lt;/p&gt; &lt;p&gt;- the question I ask the LLM is simply how many files exist in the current folder + its parent&lt;/p&gt; &lt;p&gt;I'm using litellm since it helps calling ollama + remote models with the same interface. It also automatically adds instructions around function calling to the system prompt.&lt;/p&gt; &lt;p&gt;The results I got so far:&lt;/p&gt; &lt;p&gt;- Claude got it right every time (there's 12 files total)&lt;/p&gt; &lt;p&gt;- GPT responded in half the time, but was wrong (it hallucinated the number of files and directories)&lt;/p&gt; &lt;p&gt;- tinyllama couldn't figure out how to call the function at all&lt;/p&gt; &lt;p&gt;- mistral hallucinated different functions to try to sum the numbers&lt;/p&gt; &lt;p&gt;- qwen2.5 hallucinated a calculate_total_files that doesn't exist in one run, and got in a loop on another&lt;/p&gt; &lt;p&gt;- llama3.2 get in an infinite loop, calling the same function forever, consistently&lt;/p&gt; &lt;p&gt;- llama3.3 hallucinated a count_files that doesn't exist and failed&lt;/p&gt; &lt;p&gt;- deepseek-r1 hallucinated a list_iles function and failed&lt;/p&gt; &lt;p&gt;I included the code as well as results in a gist here: &lt;a href="https://gist.github.com/herval/e341dfc73ecb42bc27efa1243aaeb69b"&gt;https://gist.github.com/herval/e341dfc73ecb42bc27efa1243aaeb69b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious about everyone's experiences. Has anyone managed to get these models consistently work with function calling?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hervalfreire"&gt; /u/hervalfreire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioyxkm/how_to_do_proper_function_calling_on_ollama_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioyxkm/how_to_do_proper_function_calling_on_ollama_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ioyxkm/how_to_do_proper_function_calling_on_ollama_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T01:15:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioscui</id>
    <title>This is pure genius! Thank you!</title>
    <updated>2025-02-13T20:13:52+00:00</updated>
    <author>
      <name>/u/Apprehensive_Row9873</name>
      <uri>https://old.reddit.com/user/Apprehensive_Row9873</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all. I'm new here, I'm a french engineer. I was searching for a solution to self-host Mistral for days and couldn‚Äôt find the right way to do it correctly with Python and llama.cpp. I just couldn‚Äôt manage to offload the model to the GPU without CUDA errors. After lots of digging, I discovered vLLM and then Ollama. Just want to say THANK YOU! üôå This program works flawlessly from scratch on Docker üê≥, and I‚Äôll now implement it to auto-start Mistral and run directly in memory üß†‚ö°. This is incredible, huge thanks to the devs! üöÄüî•&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apprehensive_Row9873"&gt; /u/Apprehensive_Row9873 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioscui/this_is_pure_genius_thank_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioscui/this_is_pure_genius_thank_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ioscui/this_is_pure_genius_thank_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T20:13:52+00:00</published>
  </entry>
</feed>
