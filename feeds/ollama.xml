<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-05T21:05:47+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ihqub9</id>
    <title>I want to get into Local LLMs for coding, home assistant, and maybe a little conversation. Low token/s is fine to see if I even like it. Which hardware that I have listed could do it? Or do I need a GPU solely for this, even to start off?</title>
    <updated>2025-02-04T19:59:03+00:00</updated>
    <author>
      <name>/u/bigrjsuto</name>
      <uri>https://old.reddit.com/user/bigrjsuto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have the following hardware:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Personal PC (LInux Mint 21) &lt;ul&gt; &lt;li&gt;Ryzen 5800X, 64GB DDR4, 3060 12GB, 1TB NVMe + 2TB NVMe&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Server PC (Proxmox) &lt;ul&gt; &lt;li&gt;Intel 12500T, 128GB DDR4, A4000 16GB (passthrough to Windows 11 VM for Solidworks), 128GB NVMe (boot) + 1TB NVMe (VMs/LXCs), 2x 18TB HDDs&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Gaming/HTPC (Windows) &lt;ul&gt; &lt;li&gt;Intel 10600K, 32GB DDR4, RX 590 8GB, 128GB + 1TB SSD&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;MiniPCs &lt;ul&gt; &lt;li&gt;20x Datto ALTO 3 V2 &lt;ul&gt; &lt;li&gt;Celeron 3865U, 2-16GB DDR4 (Can configure as needed)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;2x Datto S3X2 Dual-NIC &lt;ul&gt; &lt;li&gt;Intel i3-7100U, 2-16GB DDR4&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;4x Optiplex 3040 &lt;ul&gt; &lt;li&gt;Intel i3-6100T, 2-8GB DDR3&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Intel NUC &lt;ul&gt; &lt;li&gt;Intel i7-7567U, 2-16GB DDR4&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Other Hardware &lt;ul&gt; &lt;li&gt;RX 560 4GB LP&lt;/li&gt; &lt;li&gt;Various extra HDDs 160GB - 8TB&lt;/li&gt; &lt;li&gt;Various extra NVMe 32GB - 500GB&lt;/li&gt; &lt;li&gt;A few extra network switches (if needed for clustering)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wanted to add the RX 560 to the server, but after some research, everything I've seen says that 4GB is too little VRAM for even slow output. That's the case, right?&lt;/p&gt; &lt;p&gt;How about a Coral TPU? Or multiple? Each of those Datto ALTO MiniPCs have a A+E keyed m.2 slot, where I could place them and cluster them together.&lt;/p&gt; &lt;p&gt;Could I just run it on my PC? Would the 3060 be good enough to get some output?&lt;/p&gt; &lt;p&gt;I know there's the A4000, but I need it for CAD work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigrjsuto"&gt; /u/bigrjsuto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihqub9/i_want_to_get_into_local_llms_for_coding_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihqub9/i_want_to_get_into_local_llms_for_coding_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihqub9/i_want_to_get_into_local_llms_for_coding_home/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T19:59:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii5ky5</id>
    <title>Facing error while installing deepseek 1.5b</title>
    <updated>2025-02-05T08:31:00+00:00</updated>
    <author>
      <name>/u/The_Arcane19</name>
      <uri>https://old.reddit.com/user/The_Arcane19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ii5ky5/facing_error_while_installing_deepseek_15b/"&gt; &lt;img alt="Facing error while installing deepseek 1.5b" src="https://preview.redd.it/l5hpg8997ahe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=301ecaa1fa019c3b33920f10364ee9848f6749b7" title="Facing error while installing deepseek 1.5b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_Arcane19"&gt; /u/The_Arcane19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l5hpg8997ahe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii5ky5/facing_error_while_installing_deepseek_15b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii5ky5/facing_error_while_installing_deepseek_15b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T08:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1iht2tf</id>
    <title>How To Setup DeepSeek-R1 LLM AI ChatBot Using Ollama On An Ubuntu Linux GPU Cloud Server (VPS)</title>
    <updated>2025-02-04T21:29:42+00:00</updated>
    <author>
      <name>/u/websplaining</name>
      <uri>https://old.reddit.com/user/websplaining</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iht2tf/how_to_setup_deepseekr1_llm_ai_chatbot_using/"&gt; &lt;img alt="How To Setup DeepSeek-R1 LLM AI ChatBot Using Ollama On An Ubuntu Linux GPU Cloud Server (VPS)" src="https://external-preview.redd.it/uNOHV2Maw2LzcoSfFMG6sju1JNogiBy71eR2QYEqjOQ.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbe5a0f4cb1ca54f001e5f84b38b48e405eb752e" title="How To Setup DeepSeek-R1 LLM AI ChatBot Using Ollama On An Ubuntu Linux GPU Cloud Server (VPS)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/websplaining"&gt; /u/websplaining &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/S_JEkuE9EyU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iht2tf/how_to_setup_deepseekr1_llm_ai_chatbot_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iht2tf/how_to_setup_deepseekr1_llm_ai_chatbot_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T21:29:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii6duy</id>
    <title>How do I connect vs code on a client machine to my LLM running on my local server (in Docker)?</title>
    <updated>2025-02-05T09:33:49+00:00</updated>
    <author>
      <name>/u/Serious_Gap_3403</name>
      <uri>https://old.reddit.com/user/Serious_Gap_3403</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Goal: A free, locally hosted and customizable 'co-pilot'.&lt;/p&gt; &lt;p&gt;I have Ollama running in a container in docker on my ubuntu server and want to utilize a code-generating LLM when I use vs code on my windows machine, I am guessing I need to configure a vs code extension to connect to the port of this container somehow? If anyone has tried anything similar any recommendations for such an extension or any guides/resources that were helpful It would be very appreciated, Cheers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Serious_Gap_3403"&gt; /u/Serious_Gap_3403 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6duy/how_do_i_connect_vs_code_on_a_client_machine_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6duy/how_do_i_connect_vs_code_on_a_client_machine_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii6duy/how_do_i_connect_vs_code_on_a_client_machine_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T09:33:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii6jgj</id>
    <title>why Im not able to install</title>
    <updated>2025-02-05T09:46:02+00:00</updated>
    <author>
      <name>/u/Delicious-Ad4105</name>
      <uri>https://old.reddit.com/user/Delicious-Ad4105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ii6jgj/why_im_not_able_to_install/"&gt; &lt;img alt="why Im not able to install" src="https://b.thumbs.redditmedia.com/qdsRLqINGpalRJE3F5p1mZDWIIjWGTMvJ9CLGTer7Gw.jpg" title="why Im not able to install" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/dkmgee4lkahe1.png?width=1453&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02b4979f39589368298d4812e4b4f6234473d89e"&gt;https://preview.redd.it/dkmgee4lkahe1.png?width=1453&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02b4979f39589368298d4812e4b4f6234473d89e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h7shcwtpkahe1.png?width=344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1095f1c38997641dd03e46a1e09933692503a70"&gt;https://preview.redd.it/h7shcwtpkahe1.png?width=344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1095f1c38997641dd03e46a1e09933692503a70&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious-Ad4105"&gt; /u/Delicious-Ad4105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6jgj/why_im_not_able_to_install/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6jgj/why_im_not_able_to_install/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii6jgj/why_im_not_able_to_install/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T09:46:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihibp9</id>
    <title>Is Wikipedia RAG possible entirely locally with a gaming machine?</title>
    <updated>2025-02-04T14:02:46+00:00</updated>
    <author>
      <name>/u/trichofobia</name>
      <uri>https://old.reddit.com/user/trichofobia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all, I'm super super new, so I'm sorry if this is a stupid question, but I just heard what RAG is, I'd like to improve a local model (I'm only really familiar with deepseek, but I understand that ollama is great with RAG) with RAG.&lt;/p&gt; &lt;p&gt;I'd like to download Wikipedia locally, and use that for RAG. I've got a passable gaming laptop I don't use which has 32gb RAM, an RTX 3070 and an i7, along with an SSD.&lt;/p&gt; &lt;p&gt;I know I can download Wikipedia without images and it's something like 12-17gb. Would a local LLM be capable of searching through it automatically and choosing the best 2-3 articles based on my question? Or am I opening a can of worms?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/trichofobia"&gt; /u/trichofobia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihibp9/is_wikipedia_rag_possible_entirely_locally_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihibp9/is_wikipedia_rag_possible_entirely_locally_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihibp9/is_wikipedia_rag_possible_entirely_locally_with_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T14:02:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihnhpl</id>
    <title>Whats the best open source model for video generation?</title>
    <updated>2025-02-04T17:43:10+00:00</updated>
    <author>
      <name>/u/gl2101</name>
      <uri>https://old.reddit.com/user/gl2101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im currently running a 3060 setup but planning to upgrade to a more powerful GPU. &lt;/p&gt; &lt;p&gt;My main goal is to build ai videos but I don‚Äôt know where to start. &lt;/p&gt; &lt;p&gt;Any recommendations are greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gl2101"&gt; /u/gl2101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihnhpl/whats_the_best_open_source_model_for_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihnhpl/whats_the_best_open_source_model_for_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihnhpl/whats_the_best_open_source_model_for_video/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T17:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii7zdv</id>
    <title>Does anyone know a good AI voice-changing model for ollama or any other model that has a similar function?</title>
    <updated>2025-02-05T11:33:18+00:00</updated>
    <author>
      <name>/u/Mallowfanthe4th</name>
      <uri>https://old.reddit.com/user/Mallowfanthe4th</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean as in the replication other people's voices (just for fun).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mallowfanthe4th"&gt; /u/Mallowfanthe4th &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii7zdv/does_anyone_know_a_good_ai_voicechanging_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii7zdv/does_anyone_know_a_good_ai_voicechanging_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii7zdv/does_anyone_know_a_good_ai_voicechanging_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T11:33:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii91b6</id>
    <title>Speech recognition for core EU languages?</title>
    <updated>2025-02-05T12:40:03+00:00</updated>
    <author>
      <name>/u/anderssewerin</name>
      <uri>https://old.reddit.com/user/anderssewerin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the current best model, preferably open, for this? I am testing out Whisper locally and so far it seems pretty solid, but would love to hear experiences from others here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anderssewerin"&gt; /u/anderssewerin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii91b6/speech_recognition_for_core_eu_languages/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii91b6/speech_recognition_for_core_eu_languages/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii91b6/speech_recognition_for_core_eu_languages/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T12:40:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iif4cu</id>
    <title>Is there any way to add my local drive data to a model?</title>
    <updated>2025-02-05T17:13:30+00:00</updated>
    <author>
      <name>/u/billhughes1960</name>
      <uri>https://old.reddit.com/user/billhughes1960</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently using Llama 3.2 Vision with Ollama, and I must say, the experience has been fantastic so far. &lt;/p&gt; &lt;p&gt;However, I've noticed that the model's training data is somewhat outdated, dating back to 2018. One feature that would greatly enhance my interaction with this model is the ability to supplement its knowledge with my own documents and data. Are there any tools or methods available to update or augment the model with user-provided information?&lt;/p&gt; &lt;p&gt;(This post was made better by Ollama!) :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/billhughes1960"&gt; /u/billhughes1960 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iif4cu/is_there_any_way_to_add_my_local_drive_data_to_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iif4cu/is_there_any_way_to_add_my_local_drive_data_to_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iif4cu/is_there_any_way_to_add_my_local_drive_data_to_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T17:13:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiflm1</id>
    <title>Using DeepSeek To Make A Game</title>
    <updated>2025-02-05T17:33:01+00:00</updated>
    <author>
      <name>/u/sveennn</name>
      <uri>https://old.reddit.com/user/sveennn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iiflm1/using_deepseek_to_make_a_game/"&gt; &lt;img alt="Using DeepSeek To Make A Game" src="https://external-preview.redd.it/Jsnuak6K7miLugvD2wowYcZVRD06OvCXYvnQr335CGY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7128c34bc49c26ff56a9bf1a8250bb7747e29153" title="Using DeepSeek To Make A Game" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sveennn"&gt; /u/sveennn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@sveennn/using-deepseek-to-make-a-game-db6932157495"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iiflm1/using_deepseek_to_make_a_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iiflm1/using_deepseek_to_make_a_game/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T17:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihfyi2</id>
    <title>Ollama + DeepSeek + Obsidian = The Ultimate Offline AI Assistant!</title>
    <updated>2025-02-04T11:54:17+00:00</updated>
    <author>
      <name>/u/Kind-Industry-609</name>
      <uri>https://old.reddit.com/user/Kind-Industry-609</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ihfyi2/ollama_deepseek_obsidian_the_ultimate_offline_ai/"&gt; &lt;img alt="Ollama + DeepSeek + Obsidian = The Ultimate Offline AI Assistant!" src="https://external-preview.redd.it/32JgoJVP2Vxa0PebR1pmCtaV_33XwoDfHhsNkStqIjE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49a27f19a9e63b070eb41d43f95929897b33eb6f" title="Ollama + DeepSeek + Obsidian = The Ultimate Offline AI Assistant!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind-Industry-609"&gt; /u/Kind-Industry-609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/qAsGO5N7OCk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihfyi2/ollama_deepseek_obsidian_the_ultimate_offline_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihfyi2/ollama_deepseek_obsidian_the_ultimate_offline_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T11:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iib7um</id>
    <title>Sure way to make them hallucinate - ask them for explicit information like song lyrics. This is Bohemian Rhapsody, according to Llama3.2:3b-instruct-q8_0</title>
    <updated>2025-02-05T14:30:27+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have you tested other models? Are bigger ones more accurate?&lt;/p&gt; &lt;p&gt;Anyway, here's my test:&lt;/p&gt; &lt;p&gt;Note (I'll add my own comments, marked with // as in programming)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;gt;&amp;gt;&amp;gt; Do you know the lyrics of Bohemian Rhapsody?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I do know the lyrics to &amp;quot;Bohemian Rhapsody&amp;quot; by Queen. The song is famous for its complex composition and operatic sections, making it a bit challenging to quote accurately. However, I'll provide you with the main lyrics:&lt;/p&gt; &lt;p&gt;Is this the real life?&lt;/p&gt; &lt;p&gt;Is this just fantasy?&lt;/p&gt; &lt;p&gt;Caught in a landslide,&lt;/p&gt; &lt;p&gt;No escape from reality&lt;/p&gt; &lt;p&gt;Open your eyes, look up to the skies and see&lt;/p&gt; &lt;p&gt;I'm just a poor boy, I need no sympathy&lt;/p&gt; &lt;p&gt;Because I'm easy come, easy go, little high, little low&lt;/p&gt; &lt;p&gt;Any way the wind blows doesn't really matter to me // so far so good&lt;/p&gt; &lt;p&gt;// skipped another &amp;quot;to me&amp;quot;&lt;/p&gt; &lt;p&gt;// skipped the whole section about killing - is this censorship, or natural aversion towards violence?&lt;/p&gt; &lt;p&gt;Mama, life had just begun&lt;/p&gt; &lt;p&gt;But now I've gone and thrown it all away&lt;/p&gt; &lt;p&gt;Mama, oooh (any way the wind blows) // this comes later&lt;/p&gt; &lt;p&gt;Didn't mean to make you cry&lt;/p&gt; &lt;p&gt;If I'm not back again this time tomorrow&lt;/p&gt; &lt;p&gt;Carry on, carry on as if nothing matters // as if nothing really* matters&lt;/p&gt; &lt;p&gt;Too late, my time has come&lt;/p&gt; &lt;p&gt;Sent shivers down my spine, body's aching all the time // sends*&lt;/p&gt; &lt;p&gt;Goodbye, everybody, I've got to go&lt;/p&gt; &lt;p&gt;Gotta leave you all behind and face the truth&lt;/p&gt; &lt;p&gt;Mama, oooh (any way the wind blows) // up to this point it's quite precise&lt;/p&gt; &lt;p&gt;&lt;del&gt;Didn't mean to make you cry&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;// From this point it descends into total chaos and incoherence:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I just walked in to find&lt;/p&gt; &lt;p&gt;Out that the crowd had gathered&lt;/p&gt; &lt;p&gt;There must have been some kind of mistake&lt;/p&gt; &lt;p&gt;They said I was in their way&lt;/p&gt; &lt;p&gt;So they put me down and threw me out&lt;/p&gt; &lt;p&gt;But I won't let them push me around&lt;/p&gt; &lt;p&gt;No, no, no, no, no, no&lt;/p&gt; &lt;p&gt;&lt;strong&gt;// Tries to regain its senses&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;(Operatic section)&lt;/p&gt; &lt;p&gt;Galileo, Galileo, Galileo, Figaro - magnifico&lt;/p&gt; &lt;p&gt;I'm just a poor boy, nobody loves me&lt;/p&gt; &lt;p&gt;Spare him his life from this monstrosity&lt;/p&gt; &lt;p&gt;Easy come, easy go, will you &lt;del&gt;never&lt;/del&gt; let me go?&lt;/p&gt; &lt;p&gt;(Bismillah) No, we will not let you go (Bismillah)&lt;/p&gt; &lt;p&gt;We will not let you go (Let him go)&lt;/p&gt; &lt;p&gt;(Big chorus)&lt;/p&gt; &lt;p&gt;So you think you can stone me and spit in my eye?&lt;/p&gt; &lt;p&gt;So you think you can love me and leave me to die?&lt;/p&gt; &lt;p&gt;Oh, baby, can't do this to me, baby&lt;/p&gt; &lt;p&gt;Just gotta get out, just gotta get right outta here&lt;/p&gt; &lt;p&gt;&lt;strong&gt;// Descends into incoherence again&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ooh, nothing matters, anyone can see&lt;/p&gt; &lt;p&gt;Nothing matters, all are free&lt;/p&gt; &lt;p&gt;Nothing matters, nothing matters&lt;/p&gt; &lt;p&gt;Nothing matters at all&lt;/p&gt; &lt;p&gt;(Big chorus)&lt;/p&gt; &lt;p&gt;(Ooh, nothing matters)&lt;/p&gt; &lt;p&gt;(Bismillah) We will not let you go (Bismillah)&lt;/p&gt; &lt;p&gt;There's a bit more to the song, but these are the main lyrics.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iib7um/sure_way_to_make_them_hallucinate_ask_them_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iib7um/sure_way_to_make_them_hallucinate_ask_them_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iib7um/sure_way_to_make_them_hallucinate_ask_them_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T14:30:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii5o2q</id>
    <title>How do you know which LLM to use and for what use case?</title>
    <updated>2025-02-05T08:37:46+00:00</updated>
    <author>
      <name>/u/hexarthrius</name>
      <uri>https://old.reddit.com/user/hexarthrius</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there, I'm a newbie in local LLMs and maybe AI in general. Is there a guide out there that allows me to assess quickly which LLMs from Ollama are capable of doing which task?&lt;/p&gt; &lt;p&gt;I'd like to leverage AI in my local computer and later maybe branch it out to hosting my own personal service to do most stuff and maybe make an agent of myself to help me with my work (Corporate IT stuff).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hexarthrius"&gt; /u/hexarthrius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii5o2q/how_do_you_know_which_llm_to_use_and_for_what_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii5o2q/how_do_you_know_which_llm_to_use_and_for_what_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii5o2q/how_do_you_know_which_llm_to_use_and_for_what_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T08:37:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii11dz</id>
    <title>Deepseek r1 1.5b thinking about rose flower üåπ</title>
    <updated>2025-02-05T03:39:16+00:00</updated>
    <author>
      <name>/u/False-Woodpecker5604</name>
      <uri>https://old.reddit.com/user/False-Woodpecker5604</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ii11dz/deepseek_r1_15b_thinking_about_rose_flower/"&gt; &lt;img alt="Deepseek r1 1.5b thinking about rose flower üåπ" src="https://preview.redd.it/7axbiaybr8he1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=719cdceebe4b50f16a0fc624b6247f706549c65f" title="Deepseek r1 1.5b thinking about rose flower üåπ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/False-Woodpecker5604"&gt; /u/False-Woodpecker5604 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7axbiaybr8he1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii11dz/deepseek_r1_15b_thinking_about_rose_flower/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii11dz/deepseek_r1_15b_thinking_about_rose_flower/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T03:39:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1iijdrc</id>
    <title>num_ctx parameter does not work</title>
    <updated>2025-02-05T20:05:17+00:00</updated>
    <author>
      <name>/u/Captain21_aj</name>
      <uri>https://old.reddit.com/user/Captain21_aj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried running deepseek-r1:32b but no matter the context length (above 2048) i keep getting input truncate warning with the &lt;code&gt;n_ctx&lt;/code&gt; value stuck to &lt;code&gt;2048&lt;/code&gt;&lt;/p&gt; &lt;p&gt;LLM server log:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llm_load_print_meta: general.name = DeepSeek R1 Distill Qwen 32B llm_load_print_meta: BOS token = 151646 '&amp;lt;ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú&amp;gt;' llm_load_print_meta: EOS token = 151643 '&amp;lt;ÔΩúend‚ñÅof‚ñÅsentence ÔΩú&amp;gt;' llm_load_print_meta: PAD token = 151643 '&amp;lt;ÔΩúend‚ñÅof‚ñÅsentence ÔΩú&amp;gt;' llm_load_print_meta: LF token = 148848 '√Ñƒ¨' llm_load_print_meta: EOG token = 151643 '&amp;lt;ÔΩúend‚ñÅof‚ñÅsentence ÔΩú&amp;gt;' llm_load_print_meta: max token length = 256 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 CUDA devices: Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes llm_load_tensors: ggml ctx size = 0.68 MiB llm_load_tensors: offloading 49 repeating layers to GPU llm_load_tensors: offloaded 49/65 layers to GPU llm_load_tensors: CUDA_Host buffer size = 5312.13 MiB llm_load_tensors: CUDA0 buffer size = 13613.88 MiB llama_new_context_with_model: n_ctx = 32000 llama_new_context_with_model: n_batch = 512 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: flash_attn = 0 llama_new_context_with_model: freq_base = 1000000.0 llama_new_context_with_model: freq_scale = 1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;LLM request log:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;INFO [update_slots] input truncated | n_ctx=2048 n_erase=6660 n_keep=4 n_left=2044 n_shift=1022 tid=&amp;quot;140586800865280&amp;quot; timestamp=1738784475 INFO [update_slots] input truncated | n_ctx=2048 n_erase=3299 n_keep=4 n_left=2044 n_shift=1022 tid=&amp;quot;140586800865280&amp;quot; timestamp=1738784485 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It seems that this issue has been raised since July last year on this &lt;a href="https://github.com/ollama/ollama/issues/5661"&gt;GitHub Issue&lt;/a&gt;. From what I experienced I feel that the model is actually running high context length but the warning still persist, Is there any way to overcome this? Running on arch linux.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Captain21_aj"&gt; /u/Captain21_aj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iijdrc/num_ctx_parameter_does_not_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iijdrc/num_ctx_parameter_does_not_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iijdrc/num_ctx_parameter_does_not_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T20:05:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iikink</id>
    <title>deepseek issues</title>
    <updated>2025-02-05T20:52:09+00:00</updated>
    <author>
      <name>/u/endometrial-disable</name>
      <uri>https://old.reddit.com/user/endometrial-disable</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iikink/deepseek_issues/"&gt; &lt;img alt="deepseek issues" src="https://b.thumbs.redditmedia.com/kR_iZZMo13fP__blaqD7cTXDE-j5kuzriC68OrNoN2Q.jpg" title="deepseek issues" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ieod39jivdhe1.png?width=903&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a616596049c39c83679fb7b1b7430eda761c5643"&gt;https://preview.redd.it/ieod39jivdhe1.png?width=903&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a616596049c39c83679fb7b1b7430eda761c5643&lt;/a&gt;&lt;/p&gt; &lt;p&gt;im trying to run the full deepseek model locally, and i keep running into this issue. am i doing something wrong?&lt;/p&gt; &lt;p&gt;i tried updating my drivers, and there was no difference. what am i missing here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/endometrial-disable"&gt; /u/endometrial-disable &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iikink/deepseek_issues/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iikink/deepseek_issues/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iikink/deepseek_issues/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T20:52:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iikivv</id>
    <title>Trying to use ollama serve</title>
    <updated>2025-02-05T20:52:26+00:00</updated>
    <author>
      <name>/u/No_Whole_4790</name>
      <uri>https://old.reddit.com/user/No_Whole_4790</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;everytime i use ollama serve, it gives me an error saying the port 11434 is in use and its being used by ollama itself. I have restarted the device and switched to a different OS but this is still going. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Whole_4790"&gt; /u/No_Whole_4790 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iikivv/trying_to_use_ollama_serve/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iikivv/trying_to_use_ollama_serve/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iikivv/trying_to_use_ollama_serve/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T20:52:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiklyz</id>
    <title>Quantization help</title>
    <updated>2025-02-05T20:56:04+00:00</updated>
    <author>
      <name>/u/vagaliki</name>
      <uri>https://old.reddit.com/user/vagaliki</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm trying to quantize a model (not LLM yet - that's step 2, just Yolo initially) to run on a DSP. The things I've tried so far (yolo's export with tf's quantizer, qualcomm's AI hub, qualcomm's AIMET) have all not produced very good results (or in the case of AIMET, am getting errors when I try to convert the onnx + encodings to int8). &lt;/p&gt; &lt;p&gt;Would really appreciate if one of you quantization wizards was willing to show me what exactly you are doing and how you're getting the results to be accurate. Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vagaliki"&gt; /u/vagaliki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iiklyz/quantization_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iiklyz/quantization_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iiklyz/quantization_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T20:56:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iikoqb</id>
    <title>Handy scripts for local use</title>
    <updated>2025-02-05T20:59:09+00:00</updated>
    <author>
      <name>/u/Diligent_Property_39</name>
      <uri>https://old.reddit.com/user/Diligent_Property_39</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last few days i created some scripts that might also be interesting for this subreddit users. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;A local chat script for ollama that uses the available llm's from your local install.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Link: &lt;a href="https://github.com/xdep/Ollama-Chat-Client"&gt;https://github.com/xdep/Ollama-Chat-Client&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Another script to test the llm security for any flaws left by its creators. It will try multiple promps to see if it can bypass its default security measures.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Link: &lt;a href="https://github.com/xdep/llm-security-checks"&gt;https://github.com/xdep/llm-security-checks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;*** Screenshots available for both scripts to get an impression of the functionality.&lt;/p&gt; &lt;p&gt;Hope you guys like it :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diligent_Property_39"&gt; /u/Diligent_Property_39 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iikoqb/handy_scripts_for_local_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iikoqb/handy_scripts_for_local_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iikoqb/handy_scripts_for_local_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T20:59:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii6zut</id>
    <title>Optimizing Local LLMs on Mac Mini M4: Seeking Advice for Better Performance</title>
    <updated>2025-02-05T10:20:42+00:00</updated>
    <author>
      <name>/u/Killtec_Gaming</name>
      <uri>https://old.reddit.com/user/Killtec_Gaming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt; community! &lt;/p&gt; &lt;p&gt;We recently purchased a Mac Mini M4 (base model) for our office to run local AI operations. Our primary setup involves n8n for automation workflows integrated with Ollama, using mainly 7B and 14B models. &lt;/p&gt; &lt;p&gt;However, we've noticed that the results from these quantized models are significantly less impressive compared to cloud-based solutions. &lt;/p&gt; &lt;p&gt;We're looking for guidance on: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Are there specific optimization techniques or fine-tuning approaches we should consider? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What settings have you found most effective for 7B/14B models on Apple Silicon? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Would investing in more powerful hardware for running larger models be the only way to achieve cloud-like quality? &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any insights from those running similar setups would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Killtec_Gaming"&gt; /u/Killtec_Gaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6zut/optimizing_local_llms_on_mac_mini_m4_seeking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6zut/optimizing_local_llms_on_mac_mini_m4_seeking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii6zut/optimizing_local_llms_on_mac_mini_m4_seeking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T10:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iijkmy</id>
    <title>Step-by-Step Guide to Running Open Deep Research with smolagents</title>
    <updated>2025-02-05T20:13:05+00:00</updated>
    <author>
      <name>/u/KonradFreeman</name>
      <uri>https://old.reddit.com/user/KonradFreeman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I had heard something about OpenAI's Deep Research&lt;/p&gt; &lt;p&gt;OpenAI‚Äôs Deep Research represents a leap toward AGI by enabling AI to independently discover and synthesize knowledge. While still evolving, its ability to automate expert-level research has transformative potential across industries. For users, however, its current instability and access limitations temper immediate utility, signaling a need for ongoing refinement.&lt;/p&gt; &lt;p&gt;Then I got an email this morning about this new open source project which reverse engineered how Deep Research works:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/blog/open-deep-research"&gt;https://huggingface.co/blog/open-deep-research&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I ran it and wrote a guide on how to run it:&lt;/p&gt; &lt;p&gt;&lt;a href="https://danielkliewer.com/2025/02/05/open-deep-research"&gt;https://danielkliewer.com/2025/02/05/open-deep-research&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You just run this command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;smolagent &amp;quot;{PROMPT}&amp;quot; \ --model-type &amp;quot;HfApiModel&amp;quot; \ --model-id &amp;quot;Qwen/Qwen2.5-Coder-32B-Instruct&amp;quot; \ --imports &amp;quot;pandas numpy&amp;quot; \ --tools &amp;quot;web_search translation&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I was surprised by some of what you can do with it and am interested in applying it and using it as a starting point for some other project.&lt;/p&gt; &lt;p&gt;The interesting thing about it is how it interacts with code instead of JSON and how this increases its accuracy considerably on the benchmarks.&lt;/p&gt; &lt;p&gt;I am planning on adapting this framework to work with Ollama and run local models. I am organizing a Hackathon on the 13th to do just that and more.&lt;/p&gt; &lt;p&gt;The idea is to develop software that benefits humanity using the reverse engineering of ClosedAI's latest model locally using Ollama.&lt;/p&gt; &lt;p&gt;Or whichever inference engine you like.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KonradFreeman"&gt; /u/KonradFreeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iijkmy/stepbystep_guide_to_running_open_deep_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iijkmy/stepbystep_guide_to_running_open_deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iijkmy/stepbystep_guide_to_running_open_deep_research/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T20:13:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii6y3n</id>
    <title>Which model is best for RAG or chatting document?</title>
    <updated>2025-02-05T10:16:59+00:00</updated>
    <author>
      <name>/u/Interesting_Music464</name>
      <uri>https://old.reddit.com/user/Interesting_Music464</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to train a model locally on my Macbook Pro M1 32GB based on a technical standard/specifications that is written in a document format like PDF. Which model would you recommend for this case? I saw that MLX is best for Apple Silicon so that is my only lead on how to properly choose a model aside from choosing the number of parameters and available unified memory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting_Music464"&gt; /u/Interesting_Music464 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6y3n/which_model_is_best_for_rag_or_chatting_document/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6y3n/which_model_is_best_for_rag_or_chatting_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii6y3n/which_model_is_best_for_rag_or_chatting_document/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T10:16:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iihpxs</id>
    <title>Which small models are better than the original ChatGPT (based on GPT 3.5 released in November 2022) ?</title>
    <updated>2025-02-05T18:59:06+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm wondering about those small models that we can run on our PCs. How many parameters do you need to have better performance than the first version of ChatGPT that was released in November 2022?&lt;/p&gt; &lt;p&gt;Are parameters the only measures that count? &lt;strong&gt;&lt;em&gt;Perhaps newer models can achieve the same performance with less parameters?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm asking this because I consider the original ChatGPT to be kind of first serious model. Everything below it, seems like a toy model.&lt;/p&gt; &lt;p&gt;So I'm wondering now in 2025, do we have any models that we can run on PC as good as the first ChatGPT?&lt;/p&gt; &lt;p&gt;If I recall, GPT 3, a predecessor to GPT 3.5, already had 175 billion parameters when it was released in 2020! And it was a long time ago!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iihpxs/which_small_models_are_better_than_the_original/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iihpxs/which_small_models_are_better_than_the_original/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iihpxs/which_small_models_are_better_than_the_original/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T18:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1iifmpr</id>
    <title>qwen 2.5 VL on Ollama</title>
    <updated>2025-02-05T17:34:19+00:00</updated>
    <author>
      <name>/u/mans-987</name>
      <uri>https://old.reddit.com/user/mans-987</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any way to use qwen 2.5 VL with Ollama? The model is open source and can be found here on hugging face: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"&gt;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;also on github: &lt;a href="https://github.com/QwenLM/Qwen2.5-VL"&gt;https://github.com/QwenLM/Qwen2.5-VL&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mans-987"&gt; /u/mans-987 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iifmpr/qwen_25_vl_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iifmpr/qwen_25_vl_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iifmpr/qwen_25_vl_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T17:34:19+00:00</published>
  </entry>
</feed>
